id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
49323f1fd8bad974387a51a2bd7dd13c93dfab25	economic design of exponentially weighted moving average control chart based on measurement error using genetic algorithm	measurement error;economic design;loss function;genetic algorithm;multiple measurement;markov chain	Control charts are one of the most well-known statistical process control tools used for quick detecting changes in the process. One of their types is exponentially weighted moving average control chart. To apply them economically requires determination of corresponding parameters. Because the sampling is used to ensure process stability, the presence of measurement error in control charts seems avoidable. The presence of measurement error in control charts reduces their performance and delays quick reaction to changes. To reduce this effect, taking multiple measurements on quality characteristic of each unit is made. Previous researches do not consider the effect of measurement error on the cost function of control charts. In this paper, considering measurement error and taking multiple measurements, as well as linear and Taguchi loss functions for poor quality products, the cost function of exponentially weighted moving average control chart is modeled. For the proposed function, a numerical example is given, the average run length is computed using Markov chain method, and finally, optimal values of parameters are obtained using genetic algorithm. Finally, sensitivity analysis of parameters is performed, and the results indicate that when the slope of covariate function increases, the role of taking multiple measurement decreases, and in the case of measurement error, the optimum values of the parameters are significantly affected. Copyright © 2013 John Wiley & Sons, Ltd.	chart;genetic algorithm;john d. wiley;loss function;markov chain;numerical analysis;run-length encoding;sampling (signal processing);sensor;taguchi methods	Abbas Saghaei;Seyyed M. T. Fatemi Ghomi;S. Jaberi	2014	Quality and Reliability Eng. Int.	10.1002/qre.1538	econometrics;markov chain;genetic algorithm;computer science;operations management;mathematics;statistics;loss function;observational error	Embedded	28.228880230503385	-19.006332091981182	54210
3b28a311402dc05b49fce3488a1eb32ffd8cf346	a bayesian approach in estimating transition probabilities of a discrete-time markov chain for ignorable intermittent missing data	bayesian;missing data;em algorithm;rejection algorithm;markov chain	To cite this article: Junsheng Ma , Xiaoying Yu , Elaine Symanski , Rachelle Doody & Wenyaw Chan PhD (2014): A Bayesian Approach in Estimating Transition Probabilities of a DiscreteTime Markov Chain for Ignorable Intermittent Missing Data, Communications in Statistics Simulation and Computation, DOI: 10.1080/03610918.2014.911895 To link to this article: http://dx.doi.org/10.1080/03610918.2014.911895	communications in statistics – simulation and computation;elaine m. mcgraw;markov chain;missing data	Junsheng Ma;Xiaoying Yu;Elaine Symanski;Rachelle Doody;Wenyaw Chan	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.911895	econometrics;markov chain;expectation–maximization algorithm;missing data;bayesian probability;data mining;mathematics;statistics	ML	28.08745669122518	-22.655396643203424	54282
947b4f015be012c1bb6d94623f5835ce2da0bcfa	an approach to build process-oriented basis for causation-based multivariate spc and process capability analysis	spc;process oriented basis representation;cluster analysis;factor analysis;principal component analysis;multivariate process capability index	Although various multivariate process monitoring techniques have been developed, they do not diagnose the process for finding the root causes of irregularities during production. There have been recent studies on a new method that involves process-oriented basis representation, which links the process variation to its causes, and thus helps in monitoring and diagnosing a process. However, all the studies done so far focused on its application. In this paper, a method is proposed to build the process-oriented basis for a process irrespective of the number of variables characterizing it. Along with various other statistical techniques, factor analysis and cluster analysis, with customized distance function, are used in developing the method. The built in process-oriented basis is further used for multivariate statistical process control and process capability analysis. Multivariate solder-paste problem from electronics industry is used for illustration. Copyright © 2012 John Wiley & Sons, Ltd.	cluster analysis;correlation does not imply causation;factor analysis;john d. wiley;paste;soldering	Chitta Ranjan;J. Maiti	2013	Quality and Reliability Eng. Int.	10.1002/qre.1475	econometrics;process capability;process capability index;computer science;operations management;mathematics;cluster analysis;factor analysis;statistical process control;statistics;principal component analysis	SE	27.96687156781239	-19.822678757009466	54410
469d9e8770cbe3e7aa253c8c1b9840d713aa0492	joint empirical likelihood confidence regions for a finite number of quantiles under negatively associated samples	qin yongsong li yinghua lei qingzhu 置信区间 经验似然 na样本 两位数 有限数 joint empirical likelihood confidence regions for a finite number of quantiles under negatively associated samples	In this paper, the authors obtain the joint empirical likelihood confidence regions for a finite number of quantiles under negatively associated samples. As an application of this result, the empirical likelihood confidence intervals for the difference of any two quantiles are also developed.		Yongsong Qin;Yinghua Li;Qingzhu Lei	2015	J. Systems Science & Complexity	10.1007/s11424-015-3085-5	econometrics;pattern recognition;mathematics;statistics	Logic	31.479871904030563	-22.43424271279623	54431
737c3a665837da4801fbaeafd7530b7d63920315	noisy kriging-based optimization methods: a unified implementation within the diceoptim package	gaussian processes;active learning;computer experiments;510 mathematics	Kriging-based optimization relying on noisy evaluations of complex systems has recently motivated contributions from various research communities. Five strategies have been implemented in the DiceOptim package. The corresponding functions constitute a user-friendly tool for solving expensive noisy optimization problems in a sequential framework, while offering some flexibility for advanced users. Besides, the implementation is done in a unified environment, making this package a useful device for studying the relative performances of existing approaches depending on the experimental set up. An overview of the package structure and interface is provided, as well as a description of the strategies and some insight about the implementation challenges and the proposed solutions. The strategies are compared to some existing optimization packages on analytical test functions and show promising performances. keywords Computer Experiments; Gaussian Processes; Active learning	active learning (machine learning);complex systems;distribution (mathematics);experiment;gaussian process;kriging;mathematical optimization;optimization problem;performance;usability	Victor Picheny;David Ginsbourger	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.03.018	mathematical optimization;computer experiment;computer science;theoretical computer science;machine learning;gaussian process;mathematics;active learning;statistics	ML	26.25096090181921	-15.073337526500573	54531
57bfe11341b78339e56ebdb4b71bed1fe12d7b73	prediction intervals for integrals of gaussian random fields	spatial average;change of support problem;geostatistics;block average;kriging;bootstrap calibration	Methodology is proposed for the construction of prediction intervals for integrals of Gaussian random fields over bounded regions (called block averages in the geostatistical literature) based on observations at a finite set of sampling locations. Two bootstrap calibration algorithms are proposed, termed indirect and direct, aimed at improving upon plug-in prediction intervals in terms of coverage probability. A simulation study is carried out that illustrates the effectiveness of both procedures, and these procedures are applied to estimate block averages of chromium traces in a potentially contaminated region in Switzerland.		Victor De Oliveira;Bazoumana Kone	2015	Computational statistics & data analysis	10.1016/j.csda.2014.09.013	econometrics;mathematical optimization;mathematics;kriging;statistics;geostatistics	ML	28.937643414224205	-22.031574833826806	54579
94b3c6230297394cd2afb86130b1fba361f0b4c8	a simple procedure for estimating the false discovery rate	false discovery rate;heterodox economics;climate change;bioinformatique;ecological economics;general equilibrium analysis;bioinformatica;bioinformatics	MOTIVATION The most used criterion in microarray data analysis is nowadays the false discovery rate (FDR). In the framework of estimating procedures based on the marginal distribution of the P-values without any assumption on gene expression changes, estimators of the FDR are necessarily conservatively biased. Indeed, only an upper bound estimate can be obtained for the key quantity pi0, which is the probability for a gene to be unmodified. In this paper, we propose a novel family of estimators for pi0 that allows the calculation of FDR.   RESULTS The very simple method for estimating pi0 called LBE (Location Based Estimator) is presented together with results on its variability. Simulation results indicate that the proposed estimator performs well in finite sample and has the best mean square error in most of the cases as compared with the procedures QVALUE, BUM and SPLOSH. The different procedures are then applied to real datasets.   AVAILABILITY The R function LBE is available at http://ifr69.vjf.inserm.fr/lbe   CONTACT broet@vjf.inserm.fr.	estimated;false discovery rate;gene expression profiling;marginal model;mean squared error;microarray;simulation;spatial variability	Cyril Dalmasso;Philippe Broët;Thierry Moreau	2005	Bioinformatics	10.1093/bioinformatics/bti063	biology;econometrics;heterodox economics;false discovery rate;bioinformatics;data mining;mathematics;climate change;ecological economics;statistics	ML	28.833701440791575	-22.047900370870817	54608
46f838689f905def6ac11d837dffeed628cf5f00	empirical bayes estimators of reliability performances using linex loss under progressively type-ii censored samples	empirical bayes;progressively censored samples;asymmetric loss function;62g99;bayes estimator;62j10;empirical bayes estimate;monte carlo simulation;maximum likelihood method;burr xii model	Based on progressively Type-II censored samples, the empirical estimators of reliability performances for Burr XII distribution are researched under LINEX error loss. Firstly, we obtain the Bayes estimators of the reliability performances. Secondly, different from the predecessor, the empirical Bayes estimators of the reliability performances are derived where hyper-parameter is estimated using maximum likelihood method. In the end, in order to investigate the accuracy of estimations, an illustrative example is examined numerically by means of Monte-Carlo simulation. © 2006 IMACS. Published by Elsevier B.V. All rights reserved.	monte carlo method;numerical analysis;performance;simulation	Xiuchun Li;Yimin Shi;Jieqiong Wei;Jian Chai	2007	Mathematics and Computers in Simulation	10.1016/j.matcom.2006.05.002	econometrics;bayes estimator;pattern recognition;mathematics;maximum likelihood;statistics;monte carlo method	AI	29.279578614634822	-21.962582495207513	54895
02d035bd01cec95efe269b34d9520ad9b3488b1d	a general model for start-up demonstration tests	sample size;maximum likelihood estimators start up demonstration test procedure total successes consecutive successes total failures consecutive failures procedure reliability estimation;reliability;confidence level;telecommunication network reliability;generic model;consecutive k out of n systems;telecommunication network reliability failure analysis maximum likelihood estimation;maximum likelihood estimation;data model;failure analysis;waiting time distribution consecutive k out of n systems start up reliability;unit under test;reliability optimization data models mathematical model cognition maximum likelihood estimation;maximum likelihood estimate;waiting time;cognition;mathematical model;waiting time distribution;optimization;start up reliability;data models;maximum like lihood estimation	The commonly known consecutive successes total failures (CSTF) start-up demonstration test procedure is generalized to a total successes consecutive successes total failures consecutive failures (TSCSTFCF) procedure. Accordingly, a unit under test is accepted when either a total number of successful tests or a specified number of consecutive successes of tests are observed before both a total number of failures and any run of specified length of failures; otherwise the unit is rejected. The waiting time until termination (the length of the set of tests) of the general procedure is evaluated. A correct choice of parameters (e.g. the number of successes required for acceptance, or the length of a run of failures causing rejection) may yield a significantly shorter waiting time than that of the relevant CSTF procedure. Also, the new procedure is less sensitive to parameter changes, which is an obvious improvement in case the parameter values are miss-specified. We provide interval confidence bounds for the estimation of the reliability for the start-up demonstration test, which is based on maximum likelihood estimators. Then, we determine the minimum sample size needed in a demonstration test to achieve a certain precision of this reliability at a specified confidence level.	device under test;rejection sampling	Amos E. Gera	2011	IEEE Transactions on Reliability	10.1109/TR.2010.2088730	reliability engineering;econometrics;computer science;mathematics;maximum likelihood;statistics	Theory	29.16594077364524	-18.391358381883034	55199
5bac11329bbcda6744f8819c21c7869a1e2e4f02	fractal measures in control performance assessment	fats;histograms;fractals;standards;indexes;delays;data models	In case of numerous loops control engineers need fast and accurate method finding out loops with the poorest control quality. There exits several measures however, industrial practice reveals their deficiencies. Some of them require introduction of external disturbance into the process (like the step test) that in many cases is considered as a risk or threat. The other group of automatic approaches uses regular process data and is based on the loop Gaussian properties foundation. Analysis of real process data shows that this assumption holds only in a few cases. Both facts initiated research for alternative measures grouped into statistical non-Gaussian indexes and fractal ones based on the properties of R/S plot and Hurst index. The analysis is performed on both simulation and real industrial data revealing interesting properties and proving validity of selected approach.	control flow;fractal;hurst exponent;simulation;threat (computer);volcano plot (statistics)	Pawel D. Domanski	2016	2016 21st International Conference on Methods and Models in Automation and Robotics (MMAR)	10.1109/MMAR.2016.7575177	computer science;artificial intelligence;data mining;statistics	SE	26.852782546206228	-19.059363736708132	55254
20dd80848e4f0e49704444fd3bda8c0e797003c5	a permutation test for determining significance of clusters with applications to spatial and gene expression data	62 07;health research;uk clinical guidelines;hierarchical clustering;classification automatique statistiques;biological patents;cluster algorithm;metodo estadistico;dato observacion;analyse multivariable;valor singular;decomposition valeur singuliere;covariance analysis;theorie approximation;multivariate analysis;analisis datos;europe pubmed central;niveau signification;metodo descomposicion;test permutation;62e17;singular value;singular value decomposition;matrice singuliere;citation search;methode decomposition;variance analysis;statistical method;singular matrix;gene expression data;distribucion estadistica;statistical regression;test significacion;permutation;valeur p;algorithme;discriminant analysis;analyse discriminante;approximation theory;algorithm;decomposition method;data analysis;analisis discriminante;level of detail;analyse covariance;distribution statistique;uk phd theses thesis;62h30;p value;methode statistique;valeur singuliere;analisis variancia;regresion estadistica;estructura datos;62j10;statistical computation;permutacion;calculo estadistico;life sciences;significance level;analisis multivariable;analyse donnee;calcul statistique;test signification;structure donnee;decomposicion valor singular;donnee observation;matriz singular;cluster analysis statistics;analisis covariancia;gene expression data analysis;regression statistique;uk research reports;medical journals;data structure;statistical distribution;observation data;europe pmc;biomedical research;analyse variance;variance;variancia;bioinformatics;algoritmo;significance test;permutation test	Hierarchical clustering is a common procedure for identifying structure in a data set, and this is frequently used for organizing genomic data. Although more advanced clustering algorithms are available, the simplicity and visual appeal of hierarchical clustering has made it ubiquitous in gene expression data analysis. Hence, even minor improvements in this framework would have significant impact. There is currently no simple and systematic way of assessing and displaying the significance of various clusters in a resulting dendrogram without making certain distributional assumptions or ignoring gene-specific variances. In this work, we introduce a permutation test based on comparing the within-cluster structure of the observed data with those of sample datasets obtained by permuting the cluster membership. We carry out this test at each node of the dendrogram using a statistic derived from the singular value decomposition of variance matrices. The p-values thus obtained provide insight into the significance of each cluster division. Given these values, one can also modify the dendrogram by combining non-significant branches. By adjusting the cut-off level of significance for branches, one can produce dendrograms with a desired level of detail for ease of interpretation. We demonstrate the usefulness of this approach by applying it to illustrative data sets.		Peter J. Park;J. Manjourides;Marco Bonetti;Marcello Pagano	2009	Computational statistics & data analysis	10.1016/j.csda.2009.05.031	econometrics;data structure;mathematics;hierarchical clustering;statistical significance;linear discriminant analysis;single-linkage clustering;dendrogram;algorithm;statistics;hierarchical clustering of networks	ML	34.106944105941665	-23.362190665342023	55295
d34ae51acccd74cd52ce9287733dfb074941b971	approximate unconditional test procedure for comparing two ordered multinomials	asymptotic test exact conditional test exact unconditional test approximate unconditional test wilcoxon statistic two ordered multinomials;exact test;multinomial distribution;approximation asymptotique;wilcoxon test;theorie echantillonnage;teoria muestreo;sample size;test statistique;approximate unconditional test;statistical simulation;analisis datos;taux erreur;type i error;tamano muestra;test estadistico;methode approchee;exact conditional test;62f05;statistical test;taille echantillon;loi conditionnelle;metodo aproximado;error type i;ecuesta estadistica;approximate method;ley condicional;polynomial;asymptotic behavior;comportement asymptotique;algorithme;estimation parametrique;two ordered multinomials;algorithm;comportamiento asintotico;conditional test;sample survey;data analysis;simulacion estadistica;grand echantillon;loi multinomiale;simulation statistique;polinomio;test exact;statistical computation;calculo estadistico;test wilcoxon;62d05;asymptotic test;error rate;simulation study;analyse donnee;calcul statistique;large sample;asymptotic approximation;multiplicacion;exact unconditional test;erreur type i;indice error;polynome;multiplication;ley multinomial;wilcoxon statistic;sondage statistique;conditional distribution;asymptotic method;aproximacion asintotica;algoritmo;sampling theory;test conditionnel	The asymptotic and exact conditional methods are widely used to compare two ordered multinomials. The asymptotic method is well known for its good performance when the sample size is sufficiently large. However, Brown et al. (2001) gave a contrary example in which this method performed liberally even when the sample size was large. In practice, when the sample size is moderate, the exact conditional method is a good alternative, but it is often criticised for its conservativeness. Exact unconditional methods are less conservative, but their computational burden usually renders them infeasible in practical applications. To address these issues, we develop an approximate unconditional method in this paper. Its computational burden is successfully alleviated by using an algorithm that is based on polynomial multiplication. Moreover, the proposed method not only corrects the conservativeness of the exact conditional method, but also produces a satisfactory type I error rate. We demonstrate the practicality and applicability of this proposed procedure with two real examples, and simulation studies are conducted to assess its performance. The results of these simulation studies suggest that the proposed procedure outperforms the existing procedures in terms of the type I error rate and power, and is a reliable and attractive method for comparing two ordered multinomials. © 2010 Elsevier B.V. All rights reserved.	approximation algorithm;computation;polynomial ring;rendering (computer graphics);simulation;word lists by frequency	Man-Lai Tang;Wai-Yin Poon;Leevan Ling;Yijie Liao;Hang-Wai Chui	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2010.08.009	sample size determination;conditional probability distribution;econometrics;statistical hypothesis testing;asymptotic analysis;type i and type ii errors;word error rate;survey sampling;wilcoxon signed-rank test;calculus;mathematics;exact test;data analysis;multiplication;multinomial distribution;statistics;polynomial	AI	31.941083242843852	-21.741312931744112	55515
901e5797a779b0cdf20437eb480614e51d1789dd	a novel image secret sharing scheme with meaningful shares	interpolation;image coding;encryption;visualization;coding theory image secret sharing image encryption multimedia security;interpolation method image secret sharing scheme meaningful shares coding method;image coding encoding interpolation encryption visualization;image coding cryptography;encoding	In this paper a novel (t, n) threshold image secret sharing scheme is proposed. Based on the idea that there is close connection between secret sharing and coding theory, coding method on GF(2m) is applied in our scheme instead of the classical Lagrange's interpolation method in order to deal with the fidelity loss problem in the recovery. All the generated share images are meaningful and the size of each share image is the same as the secret image. The analysis proves our scheme is perfect and ideal and also has high security. The experiment results demonstrate that all the shares have high quality and the secret image can be recovered exactly.	coding theory;display resolution;interpolation;secret sharing	Hongliang Cai;Huajian Liu;Qizhao Yuan;Martin Steinebach;Xiaojing Wang	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178274	discrete mathematics;visualization;interpolation;computer science;theoretical computer science;shamir's secret sharing;mathematics;homomorphic secret sharing;secure multi-party computation;internet privacy;secret sharing;verifiable secret sharing;encryption;encoding	Vision	38.91720951788336	-10.701861401836977	55601
476e04cd7ea487b4c159ca2283c76220074dcf35	the comparative study for enhpp software reliability growth model based on mixture coverage function	sse;enhpp;mixture model;test coverage;superposition model	Finite failure NHPP models presented in the literature exhibit either constant. monotonic increasing or monotonic decreasing failure occurrence rates per fault. Accurate predictions of software release times. and estimation of the reliability and availability of a software product require quality of a critical element of the software testing process : test coverage. This model called Enhanced non-homogeneous Poission process(ENHPP). In this paper, exponential coverage and S-type model was reviewed, proposes modified(the superosition and mixture) model, which make out efficiency application for software reliability. Algorithm to estimate the parameters used to maximum likelihood estimator and bisection method. model selection based on SSE statistics for the sake of efficient model, was employed.	software quality;software reliability testing	Hee-Cheul Kim;Hyoung-Keun Park	2011		10.1007/978-3-642-27180-9_23	reliability engineering;econometrics;computer science;statistics	SE	30.12209553498875	-18.64551309020675	55661
5124db97399a17e1ef02471113507acba54a51fc	analyzing and simplifying model uncertainty in fuzzy cognitive maps		Fuzzy Cognitive Mapping (FCM) represents the ‘mental model’ of individuals as a causal network equipped with an inference engine. As individuals may disagree or evidence be insufficient, causal links may be assigned a range rather than one value. When all links have range, the massive search space is a challenge to running simulations. In this paper, we presented, implemented, and evaluated a new approach to identify which ranges are important and simplify models accordingly. Our approach uses a factorial design of experiments, implemented using parallelism to offset its high computational cost. Our implementation (including our new Python library for FCM) is freely available on a third-party repository. Our evaluation on three previously published models shows that our approach can simplify almost half of a model under common settings, and runs within seconds on entry-level hardware for small FCMs. Further research is needed on simplifying the few FCMs having many links.	algorithmic efficiency;bayesian network;causal filter;causality;design of experiments;experiment;fuzzy cognitive map;hyperlink;inference engine;level of detail;mental model;parallel computing;python;simulation;spatial variability;time complexity	Eric A. Lavin;Philippe J. Giabbanelli	2017	2017 Winter Simulation Conference (WSC)	10.1109/WSC.2017.8247923	fuzzy logic;simulation;cognitive map;fuzzy cognitive map;machine learning;computer science;python (programming language);inference engine;offset (computer science);parallel processing;factorial experiment;artificial intelligence	Networks	26.64267344370029	-13.63264672907927	55861
52594c32ce4b743e5a8bf05b15bff5b83680c4d5	coverage error for confidence intervals arising in simulation output analysis	errors;sample size;output;simulation;random variables;asymptotic expansion;confidence interval;asymptotic series;statistical analysis;independent and identically distributed;random variable;technical report;confidence limits;regenerative process;expansion	Coverage error asymptotics for confidence intervals arising in simulation are discussed. Asymptotic expansions, to order O(n<sup>-1</sup>) (n is the sample size), are given for confidence intervals associated with sequences of independent and identically distributed random variables, as well as regenerative processes. Implications for simulation are emphasized.	simulation	Peter W. Glynn	1982			independent and identically distributed random variables;random variable;econometrics;mathematical optimization;confidence interval;confidence distribution;mathematics;cdf-based nonparametric confidence interval;robust confidence intervals;statistics;asymptotic expansion	ML	30.851045987609425	-20.389737196027735	55881
2d4b0d6e8a2f1e45cc8258e704654173943473aa	analysis of variance model, effects of departures from assumptions underlying				Hardeo Sahai;Mohammed I. Ageel;Anwer Khurshid	2011		10.1007/978-3-642-04898-2_224	econometrics;statistics	NLP	30.631602474598406	-22.624347722022502	55946
56ec3eee4d2559aafafec15b8dc5ff4e40298958	optimization of control parameters in genetic algorithms: a stochastic approach	optimisation;liverpool;optimizacion;proceso markov;simulacion numerica;algoritmo genetico;repository;processus markov;stochastic approximation;simulation numerique;approximation stochastique;markov process;estimacion parametro;algorithme genetique;genetic algorithm;optimization;university;aproximacion estocastica;parameter estimation;estimation parametre;numerical simulation	This paper proposes a stochastic approach for optimization of control parameters ( probabilities of crossover and mutation ) in genetic algorithms ( GAs ) . The genetic search can be modelled as a controlled Markovian process, the transition of which depends on the control parameters. A stochastic optimization problem is formed for control of GA parameters, based on a given performance index of populations and analysed as a controlled Markovian process during the genetic search. The optimal values of control parameters can be found from a recursive estimation of control parameters, which is obtained by introducing a stochastic gradient of the performance index and using a stochastic approximation algorithm. The algorithm possesses the capability of finding the stochastic gradient and adapting the control parameters in the direction of descent. A non-stationary Markov model is developed to investigate asymptotic convergence properties of the proposed genetic algorithm. It is proved that the proposed geneti...	genetic algorithm	Y. J. Cao;Q. Henry Wu	1999	Int. J. Systems Science	10.1080/002077299292290	stochastic programming;stochastic approximation;econometrics;mathematical optimization;genetic algorithm;stochastic control;continuous-time stochastic process;stochastic optimization;mathematics;markov process;estimation theory;statistics	AI	30.982192669165546	-10.81066726208475	56082
1e39630f198f15c76a42c08bf694a7f422f83991	rethinking the initialization bias problem in steady-state discrete event simulation	discrete event simulation;deterministic numerical method;warm-up period;so-called warm-up period;random number;initialization bias problem;optimal warm-up period;transient expectation;steady-state discrete event simulation;optimal length;computational modeling;numerical method;steady state;markov processes;markov process;computer model	The state in which a discrete event simulation is started causes the estimators for equilibrium measures obtained from the simulation to be biased, and to reduce this bias, the collection of data is delayed until a so-called warm-up period is completed. In this paper, we determine the optimal warm-up periods for steady-state discrete event simulations. We do this by using deterministic numerical methods, that is, methods not using random numbers. We found that in the systems investigated, transient expectations give no indication regarding the optimal length of the warm-up periods, which is counterintuitive. This requires some re-evaluation of some of commonly held opinions about the factors one should take into account when determining warm-up periods. Such factors will also be discussed.	numerical method;simulation;steady state	Winfried K. Grassmann	2011	Proceedings of the 2011 Winter Simulation Conference (WSC)		computer simulation;econometrics;mathematical optimization;simulation;computer science;discrete event simulation;mathematics;markov process;statistics	Robotics	30.147649458552284	-15.592523007415059	56187
9ea28305284e7d5675ff355097948b27ac73dcf8	a bayes approach to reliability prediction utilizing data from accelerated life tests and field failure observations	bayesian inference;field failure;accelerated life testing;markov chain monte carlo;reliability prediction	A Bayes approach is proposed to improve product reliability prediction by integrating failure information from both the field performance data and the accelerated life testing data. It is found that a product’s field failure characteristic may not be directly extrapolated from the accelerated life testing results because of the variation of field use condition that cannot be replicated in the lab-test environment. A calibration factor is introduced to model the effect of uncertainty of field stress on product lifetime. It is useful when the field performance of a new product needs to be inferred from its accelerated life test results and this product will be used in the same environment where the field failure data of older products are available. The proposed Bayes approach provides a proper mechanism of fusing information from various sources. The statistical inference procedure is carried out through the Markov chain Monte Carlo method. An example of an electronic device is provided to illustrate the use of the proposed method. Copyright © 2008 John Wiley & Sons, Ltd.	accelerated life testing;deployment environment;extrapolation;highly accelerated life test;ibm notes;john d. wiley;markov chain monte carlo;monte carlo method;reliability engineering;test plan;winbugs	Rong Pan	2009	Quality and Reliability Eng. Int.	10.1002/qre.964	reliability engineering;econometrics;accelerated life testing;markov chain monte carlo;engineering;mathematics;bayesian inference;statistics	SE	29.528050437085543	-19.155174446100972	56346
ae7ea8c12ccbf2f3526cdec32d7969d6a0c2345d	co-evolutionary learning in the n-choice iterated prisoner's dilemma with pso algorithm in a spatial environment	strategy evolution coevolutionary learning n choice iterated prisoner dilemma game pso algorithm spatial environment coevolutionary training technique particle swarm optimization interaction topology cost to benefit ratio spatial structure cooperative behavior;spatial structure pso ipd multiple choices co evolution;evolutionary computation;game theory;games educational institutions sociology statistics mathematical model equations sun;particle swarm optimisation cooperative systems cost benefit analysis evolutionary computation game theory learning artificial intelligence;cooperative systems;learning artificial intelligence;particle swarm optimisation;cost benefit analysis	The evolution of strategies in n-choice iterated prisoner's dilemma game is studied on spatial environment. This paper presents and investigates the application of co-evolutionary training techniques based on particle swarm optimization (PSO) to evolve cooperation, and exploring different parameter configurations via numerical simulations. Key model parameters include the size of the population, the interaction topology, the number of choices and the cost-to-benefit ratio. The simulation results reveal that the spatial structure does promote higher levels of cooperative behaviors, the cost-to-benefit ratio and the multiple choices are important factors in determining the strategy evolution.	algorithm;interpupillary distance;iteration;mathematical optimization;numerical analysis;particle swarm optimization;prisoner's dilemma;simulation;the evolution of cooperation	Xiaoyang Wang;HuiYou Chang;Yang Yi;Yibin Lin	2013	2013 IEEE Symposium on Computational Intelligence in Dynamic and Uncertain Environments (CIDUE)	10.1109/CIDUE.2013.6595771	simulation;artificial intelligence;machine learning;mathematics	AI	25.649310414231987	-11.100167002948494	56411
62b385e580c6e9d4c19a8af074456aa0ea2b8579	two-sample location-scale estimation from semiparametric random censorship models	functional delta method;62n03;62f12;62f10;censoring rate;cauchy link;power function;62n02;empirical coverage probability;62f03;gaussian process;62n01;62e20	When two survival functions belong to a location-scale family of distributions, and the available two-sample data are each right censored, the location and scale parameters can be estimated using a minimum distance criterion combined with Kaplan-Meier quantiles. In this paper, it is shown that using the estimated quantiles from a semiparametric random censorship framework produces improved parameter estimates. The semiparametric framework was originally proposed for the one-sample case (Dikta, 1998), and uses a model for the conditional probability that an observation is uncensored given the observed minimum. The extension to the two-sample setting assumes the availability of good fitting models for the group-specific conditional probabilities. When the models are correctly specified for each group, the new location and scale estimators are shown to be asymptotically as or more efficient than the estimators obtained using the Kaplan-Meier based quantiles. Individual and joint confidence intervals for the parameters are developed. Simulation studies show that the proposed method produces confidence intervals that have correct empirical coverage and that are more informative. The proposed method is illustrated using two real data sets.	semiparametric model	Rianka Bhattacharya;Sundarraman Subramanian	2014	J. Multivariate Analysis	10.1016/j.jmva.2014.07.011	econometrics;power function;gaussian process;mathematics;statistics	Metrics	29.60740484414607	-22.26445224935801	56438
c493a650578aab2ee076e33a17ad9d52a1fdb258	fitting manova models with missing continuous or ordinal data using reference priors	62 07;mahalanobis distance;adulto;metodo estadistico;ajustamiento modelo;echantillonnage conditionnel;aplicacion;analisis datos;echantillonnage gibbs;dato que falta;conditional sampling;fonction repartition;gibbs sampling;62h20;data;polychoric correlation;simulacion numerica;prior distribution;hombre;loi conditionnelle;statistical method;sampling distribution;ley condicional;ley a priori;donnee manquante;ajustement modele;funcion distribucion;data analysis;distribution function;missing;methode statistique;muestreo condicional;adult;model matching;simulation numerique;human;distribution echantillonnage;ordinal data;modele hierarchique;62p25;distance mahalanobis;analyse donnee;missing data;missing values;reference prior;estimation statistique;muestreo gibbs;application;estimacion estadistica;distribucion muestreo;60e05;statistical estimation;adulte;conditional distribution;hierarchical model;loi a priori;homme;numerical simulation;donnee ordinale	We present hierarchical MANOVA models for use with continuous or ordinal data in which missing values are readily handled. Full conditional distributions needed for Gibbs sampling are provided and we present two data analyses demonstrating the types of inferences obtainable from these models. The models are fit to an anthropologic data set for the purposes of estimating the Mahalanobis distance between groups of human males based on skeletal measurements, and a bivariate ordinal MANOVA model is fit to cross-classified data describing drinking behavior in healthy adults.	curve fitting;ordinal data	Timothy E. Hanson;Osbjorn M. Pearson	2007	Communications in Statistics - Simulation and Computation	10.1080/03610910701212843	computer simulation;ordinal regression;econometrics;missing data;mathematics;ordinal data;statistics	ML	33.172708080402934	-22.774934355654505	56469
d805c0944636e2c1284dafab155009978417a87d	the effect of different motion types in simple discrete particle systems with quantitative stigmergy	self organisation;swarm intelligence;physarum polycephalum;particle automata;stigmergy;slime mould;emergent behaviour	Discrete particle systems with quantitative stigmergy (ant systems, and particle based simulations of slime mould) are relevant to computational biology and are used as an alternative means to approximate solutions of intractable optimisation problems. The current range of such particle systems exhibits complex behaviour, and particular systems are therefore studied mainly empirically. In contrast, less complex systems, such as cellular automata are better understood and are more amenable to mathematical analysis. To create a bridge between the well-understood area of cellular automata on the one hand and the less understood area of particle systems with quantitative stigmergy on the other hand, this paper proposes to study strongly simplified versions of such particle systems. Eight different motion types are described and evaluated with respect to global system behaviour. The results are analytical as well as empirical. One result is that simple discrete particle systems with quantitative stigmergy perm...	particle system;stigmergy	Gerard Vreeswijk	2017	IJPEDS	10.1080/17445760.2016.1219733	cellular automaton;swarm intelligence;computational physics;simulation;computer science;complex system;particle;stigmergy;distributed computing;particle system	Vision	33.999499011482776	-11.730131437451671	56647
2a81812809d2b430ad5abf58ebb4ab905ccd84b2	choosing the best kernel: performance models for diffusion operators in particle methods	discretization corrected pse;15a23;particle method;kernel choice;15a09;diffusion operator;performance model;15a15;operator parameters;particle strength exchange pse	In scientific simulations of partial differential equations one is often faced with the task of choosing a discretization scheme or tuning the parameters of a discretized differential operator to perform well on a given problem. While this is mostly done through benchmark simulations on test problems, a problem-independent performance model would be desirable. Based on results from numerical analysis, we present a set of problem-independent performance measures for diffusion operators in particle methods. These measures quantify an operator’s accuracy, stability, and computational cost. They can be explicitly derived in closed form, hence enabling comparisons between different operators and operator parameter tuning without the need for running any benchmark simulations. If a small number of benchmarks is available, a regression over the quality measures can be calibrated to absolute CPU time, hence defining predictive performance models for the different operators. We demonstrate this on the example of PSE operators and show the computational savings that can be achieved by operator selection and tuning.		Birte Schrader;Sylvain Reboux;Ivo F. Sbalzarini	2012	SIAM J. Scientific Computing	10.1137/110835815	econometrics;mathematical optimization;simulation;mathematics	HPC	31.333382873977246	-15.965378727153201	56730
307896b0d4488781e2f5847a4bbb2c552b10a42b	effects of spatial structures on evolution of iterated prisoner&#8217;s dilemma game strategies in single-dimensional and two-dimensional grids	genetic operations;genetic operator;game theory;random pairing scheme;neighborhood structures;student members grid computing automata genetic mutations stochastic processes computer science intelligent systems;random processes game theory genetic algorithms iterative methods;spatial structure;iterative methods;cooperative behavior;automata;iterated prisoner s dilemma;stochastic processes;computer experiment;genetic operations iterated prisoner dilemma game strategies random pairing scheme cooperative behavior neighborhood structures mating strategies;random processes;intelligent systems;iterated prisoner dilemma game strategies;student members;mating strategies;genetic algorithms;genetic mutations;computer science;grid computing	We examine the effect of spatial structures on the evolution of iterated prisoner's dilemma (IPD) game strategies through computational experiments in single-dimensional and two-dimensional grid-worlds. Our computational experiments have two characteristic features. One is the use of a random pairing scheme in the IPD game where each player plays against a different randomly chosen opponent at every round of the dilemma game. The random pairing scheme makes it very difficult for players to evolve cooperative behavior. The other characteristic feature is the use of two neighborhood structures, which follows the concept of structured demes. One is for the interaction among players through the IPD game. A player in each cell in a grid-world plays against its neighbors defined by this neighborhood structure. The other is for the mating of strategies by genetic operations. A new strategy for a player is generated by genetic operations from a pair of parent strings, which are selected from its neighbors defined by the second neighborhood structure. It is shown that cooperative behavior is evolved only when the interaction neighborhood is very small and the mating neighborhood is small.	evolution;experiment;interpupillary distance;iterated function;iteration;prisoner's dilemma;randomness;search engine optimization;string (computer science)	Hisao Ishibuchi;Naoki Namikawa;Ken Ohara	2006	2006 IEEE International Conference on Evolutionary Computation	10.1109/CEC.2006.1688416	non-cooperative game;bayesian game;stochastic process;simulation;genetic algorithm;computer experiment;simultaneous game;computer science;artificial intelligence;genetic operator;machine learning;repeated game;mathematics;automaton;strategy;iterative method;screening game;normal-form game;grid computing	Robotics	25.871835062210774	-10.39227033366202	57060
14d4e29eddd7e1b45691c5d10f49fc7ed659bd7f	comparison of adaptive filters for gas turbine performance monitoring	analisis numerico;generalised likelihood ratio;performance monitoring;computacion informatica;matematicas aplicadas;mathematiques appliquees;implementation;gas path analysis;kalman filters;kalman filter;analyse numerique;health monitoring;algorithme;algorithm;gas turbine;numerical analysis;ciencias basicas y experimentales;path analysis;matematicas;likelihood ratio test;covariance matching;implementacion;grupo a;applied mathematics;adaptive filter;adaptive estimation;algoritmo	This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.	portable document format	S. Borguet;O. Léonard	2010	J. Computational Applied Mathematics	10.1016/j.cam.2009.08.075	kalman filter;econometrics;control theory;mathematics;algorithm;statistics	ML	34.12407267316036	-18.21285123561271	57140
8f25513227228bae3938553d4ab97bfab1258e24	an optimal chance constraint multivariate stratified sampling design using auxiliary information		When we are dealing with multivariate problem then we need an allocation which is optimal for all the characteristics in some sense because the individual optimum allocations usually differ widely unless the characteristics are highly correlated. So an allocation called “Compromise allocation” is to be worked out suggested by Cochran. When auxiliary information is also available, it is customary to use it to increase the precision of the estimates. Moreover, for practical implementation of an allocation, we need integer values of the sample sizes. In the present paper the problem is to determine the integer optimum compromise allocation when the population means of various characteristics are of interest and auxiliary information is available for the separate and combined ratio and regression estimates. This paper considers the optimum compromise allocation in multivariate stratified sampling with non-linear objective function and probabilistic non-linear cost constraint. The probabilistic non-linear cost constraint is converted into equivalent deterministic one by using Chance Constrained programming. The formulated multi-objective nonlinear programming problem is solved by Fuzzy Goal programming approach and Chebyshev approximation. Numerical illustration is also given to show the practical utility of the approaches.	approximation algorithm;approximation theory;constraint programming;continuation;dynamic programming;gibbs sampling;goal programming;integer factorization;loss function;nonlinear programming;nonlinear system;numerical analysis;optimization problem;sampling (signal processing);series expansion;stochastic programming;stratified sampling	Neha Gupta;Irfan Ali;Abdul Bari	2014	J. Math. Model. Algorithms in OR	10.1007/s10852-013-9237-5	econometrics;mathematical optimization;mathematics;statistics	AI	29.592123395350914	-14.769260076242734	57602
8a45b6391d0048d2d5b10265ec7771832f86e56e	prediction of rice productivity in kandhamal district of orissa using simulation modeling	empirical distribution;pilot experiment;regression line;prediction interval;simulation modeling	Simulation technique has been employed to predict rice yield of Kandhamal plateaus in Orissa (India) using the data of previous years. Preliminary simulation model has been developed. The test for uniformity and independence has been conducted using Kolmogrov–smironov test and auto-correlation test, respectively. The result obtained has been subjected to testing of hypothesis by using two sided test. Data for five years (1995 to 2000) are used for model validation and the sample size is increased to 12 years i.e., from 1995 to 2007 for prediction up to 2012. Sensitivity analysis is conducted by changing the parameters within feasible limits to find out the effect on the model.	simulation	Banshidhar Panda;Milu Acharya;Dwitikrishna Panigrahi	2014	IJMSSC	10.1142/S1793962314500214	empirical distribution function;econometrics;simulation;prediction interval;engineering;linear regression;simulation modeling;mathematics;statistics	Robotics	26.75823126407303	-21.73813846268233	57633
eac091ad59515ca5fc87a4408d47cbef006cb8ab	a fragile watermarking scheme for image authentication with localization and recovery	tamper detection;watermarking;image processing;tamper localization fragile watermarking image authentication image recovery cyclic redundancy check signature authentication diffie hellman key exchange method;cyclic redundancy check codes;digital signatures;image authentication;fragile watermark image authentication image recovery tamper detection;cryptography;fragile watermark;watermarking authentication cyclic redundancy check cryptography computer science security robustness pixel information management degradation;image recovery;vector quantisation;diffie hellman key exchange;content based retrieval;content based retrieval watermarking digital signatures vector quantisation cryptography cyclic redundancy check codes image processing	A fragile, block-wise, and content-based watermarking for image authentication and recovery is presented. In this scheme, the watermark of each block is an encrypted form of its signature, which includes the block location, a content-feature of another block, and a CRC checksum. While the CRC checksum is to authenticating the signature, the mixture of the location indices of one block with the feature of a randomly selected block complicates the VQ attack. The encryption further strengthens the security. That all security parameters are user dependent and can be computed at both ends individually based on Diffie-Hellman key exchange method makes the scheme not only robust against collage attack but also truly oblivious. The experiments demonstrate that our scheme can detect and localize any tampering of size 8/spl times/8 pixels and above and can recover a 40% damaged image to an intelligible one with 24 dB. As for incidentally manipulated images, our scheme can invalidate all the blocks but does not further degrade the images. Comparing with the scheme of Celik et al. (2002), ours has better tamper localization accuracy while trading off 2-3 dB of the PSNR of watermarked images for recovery.	authentication;checksum;cyclic redundancy check;diffie–hellman key exchange;digital watermarking;encryption;experiment;peak signal-to-noise ratio;pixel;randomness;vector quantization	Phen-Lan Lin;Po-Whei Huang;An-Wei Peng	2004	IEEE Sixth International Symposium on Multimedia Software Engineering	10.1109/MMSE.2004.9	image processing;computer science;cryptography;theoretical computer science;internet privacy;computer security	Security	38.62828273101231	-11.754280320301852	57748
6e1f976f1548de7cf00d50a1e2a2b816a379e7e4	note on comparison of model selection for regression by vladimir cherkassky and yunqian ma	bayes estimation;correlacion;model selection;tecnologia electronica telecomunicaciones;computacion informatica;critical study;information criterion;structural risk minimization;grupo de excelencia;minimisation risque structurale;selection modele;aikaike information criterion;etude critique;statistical regression;estudio critico;vecino mas cercano;estimacion bayes;statistical learning;critere information aikaike;ciencias basicas y experimentales;regresion estadistica;apprentissage statistique;plus proche voisin;nearest neighbour;source code;correlation;tecnologias;grupo a;regression statistique;bayesian information criterion;estimation bayes	While Cherkassky and Ma (2003) raise some interesting issues in comparing techniques for model selection, their article appears to be written largely in protest of comparisons made in our book, Elements of Statistical Learning (2001). Cherkassky and Ma feel that we falsely represented the structural risk minimization (SRM) method, which they defend strongly here. In a two-page section of our book (pp. 212213), we made an honest attempt to compare the SRM method with two related techniques, Aikaike information criterion (AIC) and Bayesian information criterion (BIC). Apparently, we did not apply SRM in the optimal way. We are also accused of using contrived examples, designed to make SRM look bad. Alas, we did introduce some careless errors in our original simulationerrors that were corrected in the second and subsequent printings. Some of these errors were pointed out to us by Cherkassky and Ma (we supplied them with our source code), and as a result we replaced the assessment SRM performs poorly overall with a more moderate the performance of SRM is mixed (p. 212). These and other corrections can be seen in the errata section on-line at http://www-stat.stanford.edu/ElemStatLearn.	alas1 wt allele;aicardi's syndrome;bayesian information criterion;ephedra sinica;genetic selection;model selection;online and offline;providing (action);published erratum;source code;structural risk minimization	Trevor J. Hastie;Robert Tibshirani;Jerome H. Friedman	2003	Neural Computation	10.1162/089976603321891765	psychology;econometrics;structural risk minimization;computer science;artificial intelligence;machine learning;mathematics;bayesian information criterion;correlation;model selection;regression analysis;statistics;source code	ML	25.82266794272262	-19.810530975882255	57813
0b50ca2517d6dd2c31e34f1e76b6cfbfb5ab0b2c	a review of data hiding in digital images.	data hiding;digital watermark;copyright protection;digital image	In this paper we will overview the use of data hiding techniques in digital images. In particular we will describe how one can use Steganography to hide information in a digital image. Steganography is related to cryptography and is the basis for many of the digital watermarking techniques currently being developed. The interest in data hiding has risen with the recent activity in digital copyright protection schemes. One way to protect the ownership of a digital image is to secretly embed data in the content of the image identifying the owner. This paper will review recent developments in data hiding, specifically as it pertains to copyright protection of digital images.	cryptography;digital image;digital watermarking;steganography	Eugene T. Lin;Edward J. Delp	1999			digital watermarking;watermark	Graphics	37.28210026754954	-12.530850139104864	57858
df7882de875934ec6226cffe806120e85480f5b6	on marshall-olkin type distribution with effect of shock magnitude	marshall olkin distribution;computacion informatica;dependence;pqd;shock models;ciencias basicas y experimentales;matematicas;nqd;grupo a	In classical Marshall–Olkin type shock models and their modifications a system of two or more components is subjected to shocks that arrive from different sources at random times and destroy the components of the system. With a distinctive approach to the Marshall–Olkin type shock model, we assume that if the magnitude of the shock exceeds some predefined threshold, then the component, which is subjected to this shock, is destroyed; otherwise it survives. More precisely, we assume that the shock time and the magnitude of the shock are dependent random variables with given bivariate distribution. This approach allows to meet requirements of many real life applications of shock models, where the magnitude of shocks is an important factor that should be taken into account. A new class of bivariate distributions, obtained in this work, involve the joint distributions of shock times and their magnitudes. Dependence properties of new bivariate distributions have been studied. For different examples of underlying bivariate distributions of lifetimes and shock magnitudes, the joint distributions of lifetimes of the components are investigated. The multivariate extension of the proposed model is also discussed.	marshall rose;marshalling (computer science)	C. Murat Ozkut;Ismihan Bayramoglu	2014	J. Computational Applied Mathematics	10.1016/j.cam.2014.04.002	econometrics;calculus;mathematics;statistics	Theory	31.53354362868513	-18.8729167553746	58004
ffb906aad8ec8a4e992b0e706705540b3c59cf1f	contribution to the sample mean plot for graphical and numerical sensitivity analysis	modelizacion;analisis sensibilidad;optimisation;test statistique;metodo monte carlo;optimizacion;importance measure;test estadistico;statistical test;global sensitivity analysis;methode monte carlo;state dependence;analyse globale;permutation;modelisation;systeme incertain;uncertainty analysis;sensitivity analysis;monte carlo method;articles in periodicals and books;permutacion;dependencia del estado;analyse sensibilite;state dependent parameter;optimization;monte carlo simulation;sistema incierto;modeling;uncertain system;nuclear waste;global analysis;dependance de l etat;permutation test	The contribution to the sample mean plot, originally proposed by Sinclair, is revived and further developed as practical tool for global sensitivity analysis. The potentials of this simple and versatile graphical tool are discussed. Beyond the qualitative assessment provided by this approach, a statistical test is proposed for sensitivity analysis. A case study that simulates the transport of radionuclides through the geosphere from an underground disposal vault containing nuclear waste is considered as a benchmark. The new approach is tested against a very efficient sensitivity analysis method based on state dependent parameter meta-modelling. & 2009 Elsevier Ltd. All rights reserved.	benchmark (computing);graphical user interface;numerical analysis	Ricardo Bolado-Lavin;William Castaings;Stefano Tarantola	2009	Rel. Eng. & Sys. Safety	10.1016/j.ress.2008.11.012	econometrics;calculus;mathematics;statistics;monte carlo method	SE	28.07213643499421	-15.837696885581913	58140
4573791f64a4accbd121aa03e2ec37b207b347ff	shape-based scenario generation using copulas	copulas;hb economic theory;tail dependence;portfolio management;stochastic programming;scenario generation	The purpose of this article is to show how the multivariate structure (the “shape” of the distribution) can be separated from the marginal distributions when generating scenarios. To do this we use the copula. As a result, we can define combined approaches that capture shape with one method and handle margins with another. In some cases the combined approach is exact, in other cases, the result is an approximation. This new approach is particularly useful if the shape is somewhat peculiar, and substantially different from the standard normal elliptic shape. But it can also be used to obtain the shape of the normal but with margins from different distribution families, or normal margins with for example tail dependence in the multivariate structure. We provide an example from portfolio management. Only one-period problems are discussed.	approximation;marginal model;sampling (signal processing)	Michal Kaut;Stein W. Wallace	2011	Comput. Manag. Science	10.1007/s10287-009-0110-y	financial economics;stochastic programming;econometrics;mathematical optimization;copula;mathematics;statistics;project portfolio management	Theory	27.00385151984653	-20.94165570659751	58182
1623cd208b8ba422e00fffdac0ffcd91d933dac3	computer-generated hologram authentication via optical correlation	ciphertext computer generated hologram authentication optical correlation cgh based authentication approach phase only mask;iterative decoding;decoding;authentication;optical imaging;image processing cryptography;correlation;encoding;adaptive optics;optical imaging iterative decoding decoding correlation adaptive optics authentication encoding	Computer-generated hologram (CGH) has attracted much attention over the past decades, and has been widely applied in many areas, such as display, security and optical tweezer. In this paper, CGH-based authentication approach is presented by using optical correlation. After the original image is sparsified, phase-only mask is iteratively extracted as ciphertext based on CGH system. It is illustrated that the decoded information can be effectively and correctly authenticated by using optical correlation.	algorithm;authentication;ciphertext;code;computer-generated holography;modulation;simulation;spatial light modulator;variable shadowing	Wen Chen	2016	2016 IEEE 14th International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2016.7819220	computer vision;telecommunications;computer science;theoretical computer science	EDA	38.54744354395916	-10.114917985601826	58218
bd46a65d6d96580eeba5c80b74709f9e36814512	normal probability plots with confidence for the residuals in linear regression	graphical tests;normal distribution;normal probability plot;linear regression model;residuals;power;simultaneous inference	Normal probability plots for a simple random sample and normal probability plots for residuals from linear regression are not treated differently in statistical text books. In the statistical literature, 1 − α simultaneous probability intervals for augmenting a normal probability plot for a simple random sample are available. The first purpose of this article is to demonstrate that the tests associated with the 1 − α simultaneous probability intervals for a simple random sample may have a size substantially different from α when applied to the residuals from linear regression. This leads to the second purpose of this article: construction of four normal probability plot-based tests for residuals, which have size α exactly. We then compare the powers of these four graphical tests and a non-graphical test for residuals in order to assess the power performances of the graphical tests and to identify the ones that have better power. Finally, an example is provided to illustrate the methods.		Wanpen Chantarangsi;Wei Liu;Frank Bretz;Seksan Kiatsupaibul;Anthony J. Hayter	2018	Communications in Statistics - Simulation and Computation	10.1080/03610918.2016.1165840	normal distribution;probability distribution;probability plot;econometrics;probability mass function;linear regression;power;mathematical statistics;mathematics;linear probability model;method of mean weighted residuals;joint probability distribution;rankit;normal probability plot;statistics;q–q plot	Crypto	29.951347212977858	-20.657063220742117	58458
4ea2b66b878b712f68a71e5011431d6a7b6f5927	using common random numbers and control variates in multiple-comparison procedures	analisis estadistico;intervalo confianza;stochastic simulation;simulation;simulation stochastique;random number;statistical analysis variance reduction;confidence interval;statistical analysis;control variates;simulacion estocastica;multiple comparisons;intervalle confiance;comparacion multiple;analyse statistique;comparaison multiple;nombre aleatoire;multiple comparison;random numbers;numero aleatorio;statistics multiple comparisons;varianza;variance	This paper considers the determination of the relative merits of two or more system designs via stochastic simulation experiments by constructing simultaneous interval estimates of certain differences in expected performance. Tukey's all-pairwise-comparisons procedure, Hsu's multiple-comparisons-with-the-best procedure, and Dunnett's multiple-comparisons-with-a-control procedure are standard methods for making such comparisons. We propose refinements for all three procedures through the use of two variance reduction techniques: common random numbers and control variates. We show that the proposed procedures are better than the standard multiple-comparison procedures in the sense that they have a larger probability of containing the true difference and, at the same time, not containing zero when a difference exists.	control variates	Wei-Ning Yang;Barry L. Nelson	1991	Operations Research	10.1287/opre.39.4.583	random variate;econometrics;combinatorics;mathematics;multiple comparisons problem;statistics;variance reduction;control variates	Theory	31.590662616123268	-21.422710118179424	58609
d8a642cd47a04c736b5ce1a7559c2d1d289c22ed	a robust cusum scheme with a weighted likelihood ratio to monitor an overdispersed counting process		Abstract The Poisson cumulative sum (CUSUM) control chart is a method to monitor count data, which are commonly modeled by a Poisson distribution. However, the assumption of a Poisson distribution may not be valid in practice and the robustness of the Poisson CUSUM chart is shown to be poor when the volatility is greater than expected, a phenomenon known as overdispersion. Motivated by this, we propose an improved weighted Poisson CUSUM control chart based on the weighted log-likelihood ratio. For the purpose of discounting the unexplained volatility, smaller weights are assigned to the larger shifts. The continuity, monotonicity and convergence of the cumulative statistic proved in this paper make the design more practical. Compared with the conventional Poisson CUSUM and the Winsorized Poisson CUSUM control charts, the new weighted Poisson CUSUM scheme performs most robustly under overdispersion. In addition, the fast initial response (FIR) feature makes the chart more sensitive to start-up cases. A real-data example of monitoring daily orders on a start-up E-commerce company illustrates the superiorities of the proposed scheme.		Miaomiao Yu;Chunjie Wu;Zhijun Wang;Fugee Tsung	2018	Computers & Industrial Engineering	10.1016/j.cie.2018.09.029	count data;mathematical optimization;engineering;statistics;overdispersion;chart;control chart;poisson distribution;phenomenon;cusum;winsorized mean	AI	28.070596554013967	-19.144049962017352	58610
b840a8ee430c9f7b3cb576dd835b3df5f38cc27e	robust testing procedures in heteroscedastic linear models	test hypothese;estimacion sesgada;metodo estadistico;analisis numerico;test statistique;estimator robustness;small sample;metodo monte carlo;theorie approximation;aplicacion;65c05;62j05;test lineaire;stochastic method;fonction repartition;wald test;test hipotesis;62f35;linear test;test estadistico;62f05;62e17;simulacion numerica;modele lineaire;error sistematico;statistical test;shrinkage estimator;methode monte carlo;statistical method;modelo lineal;distribucion estadistica;asymptotic behavior;small samples;loi asymptotique;comportement asymptotique;heteroscedastic linear model;analyse numerique;estimation parametrique;approximation theory;comportamiento asintotico;funcion distribucion;distribution function;robustez estimador;numerical analysis;distribution statistique;heteroscedasticidad;estimateur retrecissement;bias;methode statistique;biais asymptotique;monte carlo method;heteroscedasticite;simulation numerique;linear model;contiguous alternatives;methode stochastique;robustness;testing procedures;pequena muestra;asymptotic distribution;heteroscedasticity;monte carlo simulation;application;60e05;m estimates;biased estimation;statistical distribution;estimation biaisee;test wald;petit echantillon;testing procedures 62f35;erreur systematique;62e20;numerical simulation;asymptotic bias;robustesse estimateur;hypothesis test;metodo estocastico	There exist many studies which treat the robust tests in homoscedastic linear models. However, the robust testing procedure in heteroscedastic linear models has not been examined. In this article, three classes of testing procedures for testing subhypothesis in heteroscedastic linear models are developed. These are Wald-type, score-type, and drop-in dispersion tests. The asymptotic distributions of these tests are obtained under the null hypothesis and contiguous alternatives. For a robustness criterion, the maximum asymptotic bias of the level of the test for distributions in a shrinking contamination neighborhood is used and the most-efficient robust test is derived. Finally, the performance of these tests in small sample is studied by Monte Carlo simulation.	linear model	Jin Zhao;Jinde Wang	2009	Communications in Statistics - Simulation and Computation	10.1080/03610910802468666	computer simulation;econometrics;statistical hypothesis testing;asymptotic analysis;calculus;mathematics;statistics;monte carlo method	ML	32.75141832998389	-22.360781504610515	58687
16a675ee1c15d088176df622ee9ffb0147ddf296	the allocation of computer time in comparing simulation experiments	simulation experiment	This paper investigates the problem of efficiently allocating computer time between two simulation experiments when the objective is to make a statistical comparison of means. For a given level of accuracy our results show that significantly less computer time is required when the sample sizes are determined according to a certain rule than when the sample sizes are equal. A graphical analysis suggests that small errors in estimating the population parameters of the allocation rule do not significantly affect the efficient allocation of time. The influence that the degree of autocorrelation has on the time allocation is also investigated; results show that small differences in the autocorrelation functions are important when each process is highly autocorrelated. Positively correlated samples for the two experiments are examined and incorporated into the efficient allocation rule. It is shown that their use leads to a saving in computer time. A two-stage procedure is described wherein initial estimates of...	experiment;simulation	George S. Fishman	1968	Operations Research	10.1287/opre.16.2.280	econometrics;mathematical optimization;simulation;computer experiment;computer science;mathematics;statistics	ML	29.711446867013628	-16.03123055577216	58809
b3f85632b5748b5ed2c54138e4f0d816d49793f5	technical note - a method for combining three-valued predictions	value prediction	"""This note shows a method for finding readily the distribution of the sum or product of variables for which predictions are available in three-valued form: a most likely value together with high and low values that each have specified probabilities of occurrence. It is assumed that the variables have a Weibull distribution. Using dimensionless graphs entered with an """"asymmetry quotient"""" found arithmetically from the original prediction values, the first, second, and third moments of each Weibull distribution are found. Then the moments of the individual distributions are combined to find the three moments of the overall distribution. If desired, these combined moments can be converted to the three-value form of the original estimates-a low, most likely, and high figure. The method described can be used instead of Monte Carlo methods, avoids their possible uncertainties, and does not require a computer. It is generally more accurate than another alternative, the approximations used in summing PERT critical paths."""		William H. Sutherland	1974	Operations Research	10.1287/opre.22.5.1104	econometrics;mathematical optimization;mathematics;statistics	Robotics	30.43174634801115	-16.997250474578028	58834
619c73a3ef6de5c62f995fc86071dc2770c2998d	rare event simulation for rough energy landscapes	rare event simulation;standard monte carlo method;asymptotically optimal estimator;fast oscillating coefficient;small noise limit;potential function;small noise stochastic differential;rough energy landscape;asymptotically optimal importance;fast oscillating parameter;fast oscillating function;importance sampling;monte carlo methods;energy landscape;monte carlo method;differential equations;estimation;mathematical model;simulation;stochastic differential equation;oscillations;noise;functional model;proteins;helium	A rough energy landscape can be modeled by a potential function superimposed by another fast oscillating function. Modeling motion in such a rough energy landscape by a small noise stochastic differential equation with fast oscillating coefficients, we construct asymptotically optimal importance sampling schemes for the study of rare events. Standard Monte Carlo methods perform poorly for these kind of problems in the small noise limit, even without the added difficulties of the fast oscillating function. We study the situation in which the fast oscillating parameter goes to zero faster than the intensity of the noise. We identify an asymptotically optimal estimator in the sense of variance minimization using the subsolution approach. Examples and simulation results are provided.	asymptotically optimal algorithm;coefficient;empirical risk minimization;importance sampling;monte carlo method;rare events;rough set;sampling (signal processing);simulation	Paul Dupuis;Konstantinos Spiliopoulos;Hui Wang	2011	Proceedings of the 2011 Winter Simulation Conference (WSC)		econometrics;mathematical optimization;mathematics;statistics;monte carlo method	Robotics	33.137548419016035	-15.970801303378575	58961
f31803e1f52cbe4e9c40f3853cb0f58fa0e9a249	optimal reliability in design for fatigue life	60g55;49q10;linear elasticity;reliability statistics;optimal design;optimal shapes for integral cost functionals;schauder estimates	The failure of a component often is the result of a degradation process that originates with the formation of a crack. Fatigue describes the crack formation in the material under cyclic loading. Activation and deactivation operations of technical units are important examples in engineering where fatigue and especially low-cycle fatigue (LCF) play an essential role. A significant scatter in fatigue life for many materials results in the necessity of advanced probabilistic models for fatigue. Moreover, optimization of reliability is of vital interest in engineering, where with respect to fatigue the cost functionals are motivated by the predicted probability for the integrity of the component after a certain number of load cycles. The natural mathematical language to model failure, here understood as crack initiation, is the language of spatio-temporal point processes and their first failure times. The local crack formation intensities thereby need to be modeled as a function of local stress states and thus...		Hanno Gottschalk;Sebastian Schmitz	2014	SIAM J. Control and Optimization	10.1137/120897092	optimal design;vibration fatigue;reliability;mathematics;schauder estimates;linear elasticity;statistics	Theory	36.28193207006444	-16.972616060544347	59239
bd6eae69253fea3c5df2ef5693eb550bc4bacfc9	dynamic model averaging for practitioners in economics and finance: the edma package		Raftery, Kárnỳ, and Ettler (2010) introduce an estimation technique referred to as Dynamic Model Averaging (DMA). In their application, DMA is used to the problem of predicting the output strip thickness for a cold rolling mill, where the output is measured with a time delay. Recently, DMA has also shown to be very useful in macroeconomic and financial applications. In this paper, we present the eDMA package for DMA estimation in R, which is especially suited for practitioners in economics and finance. Our implementation proves to be up to 133 times faster then a standard implementation on a single–core CPU. With the help of this package, practitioners are able perform DMA on a standard PC without resorting to large clusters, which are not easily available to all researchers. We demonstrate the usefulness of this package through simulation experiments and an empirical application using quarterly U.S. inflation data.	broadcast delay;central processing unit;ensemble learning;experiment;simulation;thickness (graph theory)	Leopoldo Catania;Nima Nonejad	2016	CoRR		computer science;econometrics;inflation;financial economics	ML	26.16347827048914	-20.85511758713706	59324
ecf1f4364291ed3dc8b7971d3f0fb97b8d57144c	a computer program to estimate the parameters of covariate dependent higher order markov model	computer program;higher order;s plus;markov model;covariate dependent;longitudinal data	This paper presents a computer program developed in S-plus to estimate the parameters of covariate dependent higher order Markov Chain and related tests. The program can be applied for two states Markov Chain with any order and any number of covariates depending on the PC capabilities. The program provides the maximum likelihood estimates of the parameters, together with their estimated standard error, t-value and significance level. It also produces the test results for likelihood ratio and model chi-square. To illustrate the program we have used a longitudinal data set on maternal morbidity of rural women in Bangladesh. The occurrences of haemorrhage, convulsion, or fits at different follow-ups were used as outcome variable. Economic status, wanted pregnancy, ages at marriage, and education of women were used as covariates.	computer program;estimated;fits;hemorrhage;malignant fibrous histiocytoma;markov chain;markov model;morbidity - disease rate;s-plus;likelihood ratio;significance level	Rafiqul Islam Chowdhury;M. Ataharul Islam;Makhdoom Ali Shah;Naser Al-Enezi	2005	Computer methods and programs in biomedicine	10.1016/j.cmpb.2004.10.003	econometrics;higher-order logic;covariate;computer science;data science;markov model;statistics	ML	31.186295720257412	-21.090651548651213	59351
435295edb89548da8c8b60a3b6671ee977cc750d	graphical methods for the design and analysis of simulation experiments	experimental design discussion;simulation setting;fractional design;simulation experiment;common random number stream;design technique;standardized simulation output plot;simulation run;experiment design;graphical method;general topic;design of experiment;random variable;experience design;multidimensional systems;experimental design;computational modeling;industrial engineering;sample size determination;computer simulation;design methodology	In this paper we develop some intuitive graphical methods for designing, running, and analyzing simulation experiments. We first concentrate on the analysis of output plots from a single run; the concept of a standardized simulation output plot is presented and its use illustrated. The remainder of the paper involves the design of experiments that may involve several (sequential) simulation runs. Experimental design discussions in simulation often focus on special topics unique to the field. Control over random variables permits design techniques such as antithetic variates and common random number streams. Yet many of the general topics of experiment design including confounding, fractional designs, and sample size determination are important in a simulation setting. The second part of this tutorial will focus on these general topics and present some graphical tools for generating experiment designs: causal diagrams and multidimensional point plots.	experiment;simulation	Russell R. Barton;Lee W. Schruben	1989		10.1109/WSC.1989.718662	computer simulation;econometrics;simulation;computer science;mathematics;design of experiments;statistics	Robotics	29.03711392086283	-15.549253716892881	59447
6e9f06b099dc03acb5101f4f74daa250eedf0265	theoretical and numerical study of the performance of 'pincus' optimisation method		In this work, we are interested in the numerical and theoretical study of the so called u0027Pincus algorithmu0027, which is a stochastic optimisation method. It is based on a representation of the optimum as a limit of a ratio of two expectations, computed using a Monte Carlo approximation. First, we theoretically study the convergence of Pincus algorithm. Then, numerical computations are performed: we show the algorithm advantages and limits, in comparison with other methods in terms of robustness, speed and dependence on the dimension.	mathematical optimization;numerical analysis	Majed Chemkhi;Mohamed Jebalia;Azmi Makhlouf	2016	IJMNO	10.1504/IJMMNO.2016.10002830	mathematical optimization;mathematics;robustness (computer science);computation;monte carlo method;law of large numbers;convergence (routing)	Robotics	31.270538676226106	-14.387267615845468	59506
de081e5e32f6125048e4498f364ddbf1038f52b2	stability of approximations of average run length of risk-adjusted cusum schemes using the markov approach: comparing two methods of calculating transition probabilities	performance measure;experimental design;numerical stability;modele risque;metodo estadistico;population finie;matriz transicion;aplicacion;loi probabilite;ley probabilidad;05bxx;transition probability;estabilidad numerica;aproximacion;simulacion numerica;plan experiencia;estimacion promedio;risk adjusted cusum;statistical method;calculo automatico;maillage;markov property;finite population;methode calcul;computing;transition matrix;approximation;modele discret;moyenne;calcul automatique;metodo calculo;methode cusum;cusum method;adverse outcomes;smoothing methods;average run length;62k99;risk adjustment;celdarada;plan experience;methode statistique;smoothing;metodo cusum;promedio;methode lissage;probability distribution;simulation numerique;probabilidad transicion;62d05;alisamiento;65f35;grid pattern;average;medical monitoring;stabilite numerique;mean estimation;secondary j20;risk model;estimation moyenne;primary 62;application;lissage;probabilite transition;computing method;poblacion finita;arl;matrice transition;numerical simulation	Risk-adjusted CUSUM schemes are designed to monitor the number of adverse outcomes following a medical procedure. An approximation of the average run length (ARL), which is the usual performance measure for a risk-adjusted CUSUM, may be found using its Markov property. We compare two methods of computing transition probability matrices where the risk model classifies patient populations into discrete, finite levels of risk. For the first method, a process of scaling and rounding off concentrates probability in the centre of the Markov states, which are non-overlapping sub-intervals of the CUSUM decision interval, and, for the second, a smoothing process spreads probability uniformly across the Markov states. Examples of risk-adjusted CUSUM schemes are used to show, if rounding is used to calculate transition probabilities, the values of ARLs estimated using the Markov property vary erratically as the number of Markov states vary and, on occasion, fail to converge for mesh sizes up to 3,000. On the other hand, if smoothing is used, the approximate ARL values remain stable as the number of Markov states vary. The smoothing technique gave good estimates of the ARL where there were less than 1,000 Markov states.	approximation algorithm;converge;financial risk modeling;image scaling;markov chain;markov model;markov property;population;rounding;run-length encoding;smoothing	Ronald A. Webster;Anthony N. Pettitt	2007	Communications in Statistics - Simulation and Computation	10.1080/03610910701208361	computer simulation;probability distribution;econometrics;markov chain;markov kernel;computing;markov property;continuous-time markov chain;balance equation;approximation;calculus;mathematics;additive markov chain;markov process;markov model;numerical stability;statistics;smoothing;variable-order markov model	ML	35.82462552408416	-21.18567861495297	59667
462386476d6f1ed259cad51bb8109ce8be8f72ef	estimator of population total using rao, hartley and cochran's scheme using optional randomized response technique in multi-character surveys		It becomes very difficult to collect reliable data related to some social problem, which are sensitive in nature. When resistance is encountered, the usual modification of the survey method is simply an added effort on the part of the interviewer to gain the confidence of the interviewee. Singh and Joarder (14) propose an Optional Randomized Response Technique (ORRT) in SRSWR, which is always efficient than Eichhorn and Hayre (5) multiplicative scrambled response method. Keeping this in view, we propose a set of alternative estimators in multi-character surveys using ORRT to estimate the population total corresponding to usual estimators in probability proportional to size without replacement sampling design. and derived the expressions for MSE under the model. An empirical study has also been carried out to examine their performance.	hartley (unit);randomized algorithm	Sammy Sidhu;M. L. Bansal	2008	MASA		econometrics;calculus;mathematics;statistics	Robotics	29.624193548306167	-21.80672772878164	60094
8000338fda1fa55acb5572516bd4a4fdd3fe50af	looking at markov samplers through cusum path plots: a simple diagnostic idea	summary statistic;1 dimensional;cusum path plot;markov sampler;mixing;convergence diagnostic;sequential plot;markov chain	In this paper, we propose to monitor a Markov chain sampler using the cusum path plot of a chosen 1-dimensional summary statistic. We argue that the cusum path plot can bring out, more e ectively than the sequential plot, those aspects of a Markov sampler which tell the user how quickly or slowly the sampler is moving around in its sample space, in the direction of the summary statistic. The proposal is then illustrated in four examples which represent situations where the cusum path plot works well and not well. Moreover, a rigorous analysis is given for one of the examples. We conclude that the cusum path plot is an e ective tool for convergence diagnostics of a Markov sampler and for comparing di erent Markov samplers.	markov chain;sampling (signal processing)	Bin Yu;Per Mykland	1998	Statistics and Computing	10.1023/A:1008917713940	econometrics;markov chain;summary statistics;machine learning;one-dimensional space;mathematics;mixing;statistics	ML	33.31161708794891	-17.807277653846523	60132
3d6010345da1f72b550d3fb11fd0bc653b206fe0	a bayesian nonparametric procedure for comparing algorithms		A fundamental task in machine learning is to compare the performance of multiple algorithms. This is usually performed by the frequentist Friedman test followed by multiple comparisons. This implies dealing with the well-known shortcomings of null hypothesis significance tests. We propose a Bayesian approach to overcome these problems. We provide three main contributions. First, we propose a nonparametric Bayesian version of the Friedman test using a Dirichlet process (DP) based prior. We show that, from a Bayesian perspective, the Friedman test is an inference for a multivariate mean based on an ellipsoid inclusion test. Second, we derive a joint procedure for the multiple comparisons which accounts for their dependencies and which is based on the posterior probability computed through the DP. The proposed approach allows verifying the null hypothesis, not only rejecting it. Third, as a practical application we show the results in our algorithm forracing, i.e. identifying the best algorithm among a large set of candidates sequentially assessed. Our approach consistently outperforms its frequentist counterpart.	algorithm;characterization test;machine learning;verification and validation;whole earth 'lectronic link	Alessio Benavoli;Giorgio Corani;Francesca Mangili;Marco Zaffalon	2015			econometrics;frequentist inference;machine learning;statistical power;mathematics;statistics	ML	28.98929188401787	-23.928637782445815	60284
a94cdc5fe94f7ed26dab5dc14adc828b4cdcc0e5	alternatives to the least squares solution to peelle's pertinent puzzle	peelle s puzzle;approximate bayesian computation using density estimation;mean squared error	Peelle’s Pertinent Puzzle (PPP) was described in 1987 in the context of estimating fundamental parameters that arise in nuclear interaction experiments. In PPP, generalized least squares (GLS) parameter estimates fell outside the range of the data, which has raised concerns that GLS is somehow flawed and has led to suggested alternatives to GLS estimators. However, there have been no corresponding performance comparisons among methods, and one suggested approach involving simulated data realizations is statistically incomplete. Here we provide performance comparisons among estimators, introduce approximate Bayesian computation (ABC) using density estimation applied to simulated data realizations to produce an alternative to the incomplete approach, complete the incompletely specified approach, and show that estimation error in the assumed covariance matrix cannot always be ignored.	approximation algorithm;bayesian network;computation;data compression;error detection and correction;experiment;generalized least squares;hoc (programming language);kernel density estimation;next-generation network;relevance	Tom Burr;Todd L. Graves;Nicolas W. Hengartner;Toshihiko Kawano;Feng Pan;Patrick Talou	2011	Algorithms	10.3390/a4020115	econometrics;mathematical optimization;computer science;machine learning;mathematics;mean squared error;statistics	ML	29.192130042074787	-22.94733281516994	60294
bb2facbe334c6441a169c669522c1f981dd74d8d	configuring and enhancing measurement systems for damage identification	applied computing;data interpretation;measurement system;system identification;sensor placement;multiple model;global optimization;structural health monitoring;article;maximum entropy;damage identification	Engineers often decide to measure structures upon signs of damage to determine its extent and its location. Measurement locations, sensor types and numbers of sensors are selected based on judgment and experience. Rational and systematic methods for evaluating structural performance can help make better decisions. This paper proposes strategies for supporting two measurement tasks related to structural health monitoring – (1) installing an initial measurement system and (2) enhancing measurement systems for subsequent measurements once data interpretation has occurred. The strategies are based on previous research into system identification using multiple models. A global optimization approach is used to design the initial measurement system. Then a greedy strategy is used to select measurement locations with maximum entropy among candidate model predictions. Two bridges are used to illustrate the proposed methodology. First, a railway truss bridge in Zangenberg, Germany is examined. For illustration purposes, the model space is reduced by assuming only a few types of possible damage in the truss bridge. The approach is then applied to the Schwandbach bridge in Switzerland, where a broad set of damage scenarios is evaluated. For the truss bridge, the approach correctly identifies the damage that represents the behaviour of the structure. For the Schwandbach bridge, the approach is able to significantly reduce the number of candidate models. Values of candidate model parameters are also useful for planning inspection and eventual repair.	entropy (information theory);experiment;full scale;global optimization;greedy algorithm;information visualization;iteration;mathematical optimization;sensor;switzerland;system identification;system of measurement	Prakash Kripakaran;Ian F. C. Smith	2009	Advanced Engineering Informatics	10.1016/j.aei.2009.06.002	structural engineering;simulation;system identification;engineering;artificial intelligence;principle of maximum entropy;system of measurement;forensic engineering;data analysis;statistics;global optimization	AI	25.986767638749843	-18.263689194545492	60528
4ef6d5d6095970386884d6ce6d5b41a1f174f22f	a novel estimation approach for mixture transition distribution model in high-order markov chains	optimisation sous contrainte;constrained optimization;high order temporal dependence;metodo estadistico;analisis numerico;medio ambiente;chaine markov;statistical simulation;cadena markov;algoritmo busqueda;aplicacion;modelo markov;melange loi probabilite;maximum likelihood;stochastic method;fonction repartition;quasi newton;algorithme recherche;simulacion numerica;search algorithm;maximum vraisemblance;65kxx;mixed distribution;optimization method;statistical method;maximum likelihood estimation;metodo optimizacion;65c40;analyse numerique;mixture transition distribution 65c40;optimizacion con restriccion;49xx;funcion distribucion;distribution function;markov model;simulacion estadistica;numerical analysis;maximum likelihood estimate;or algorithm;mathematical programming;methode statistique;simulation statistique;environment;simulation numerique;62m05;algorithme qr;methode optimisation;distributed models;mixture transition distribution;methode stochastique;mezcla ley probabilidad;simulation study;genetic algorithm;genetic algorithms;global optimization;environnement;60j10;evolutionary algorithm;modele markov;62p12;application;60e05;programmation mathematique;programacion matematica;maxima verosimilitud;hybrid algorithm;markov chains;numerical simulation;markov chain;metodo estocastico	A transformation is proposed to convert the nonlinear constraints of the parameters in the mixture transition distribution (MTD) model into box-constraints. The proposed transformation removes the difficulties associated with the maximum likelihood estimation (MLE) process in the MTD modeling so that the MLEs of the parameters can be easily obtained via a hybrid algorithm from the evolutionary algorithms and/or quasi-Newton algorithms for global optimization. Simulation studies are conducted to demonstrate MTD modeling by the proposed novel approach through a global search algorithm in R environment. Finally, the proposed approach is used for the MTD modelings of three real data sets.	bayesian information criterion;mtd-f;markov chain;nonlinear programming;r language;simulation	D. G. Chen;Yuhlong Lio	2009	Communications in Statistics - Simulation and Computation	10.1080/03610910802715009	computer simulation;econometrics;constrained optimization;evolutionary algorithm;calculus;mathematics;maximum likelihood;statistics;global optimization	ML	30.71903178159272	-11.205455472757487	60616
767fc370d0149eebd39196d75a93692a8726aa9d	scale and rotation invariant optical id tags for automatic vehicle identification and authentication	verification;homeland security;metodo correlacion;object recognition;automatic vehicle identification;authorisation;correlation method;optical character recognition;real time;authentication;digital signatures;stored reference;reconnaissance objet;id tags;area of interest;authentification;distortion invariance;remote automatic vehicle identification distortion invariance id tags;vehiculo;automatic vehicle detection and identification systems;rotation invariance;automatic recognition;autenticacion;remote vehicle inventory control scale invariant optical id tag rotation invariant optical id tag automatic vehicle identification in plane rotation authentication distortion invariance multiplexing id tag a priori information optical code encrypted signature dual phase encoding technique degradation sources hidden signature authorized vehicle transportation homeland security;vehicule;cryptography;a priori information;traffic engineering computing;vehicles;verificacion;traffic engineering computing vehicles authorisation encoding cryptography digital signatures;vehicle;remote automatic vehicle identification;encoding;inventory control;authentication optical distortion encoding optical noise cryptography vehicle detection proposals topology information security optical design;reconocimiento automatico;methode correlation;reconnaissance automatique	We present a scale and/or rotation invariant ID tag to achieve real time automatic vehicle identification for inventory or security purposes. We focus our attention to achieve invariance with respect to scale variations and in-plane rotations of the ID tag. Both distortions must be taken into account in the design of a vehicle identification system operating from above the area of interest (e.g., aerial detection and authentication). In our proposal, distortion invariance is achieved by both multiplexing the ID tag a priori information, and developing an appropriate topology for encoding the information on the ID tag. To increase security, the designed ID tag consists of an optical code containing an encrypted signature which identifies the vehicle. The applied encryption procedure follows the double phase encoding technique, which provides robustness to different degradation sources such as noise, occlusion, scratches, etc. that may affect the ID tag. Once the ID tag is captured by the processor, decryption of the hidden signature is carried out, and its correlation with a previously stored reference signal allows either the identification or the rejection of the authorized vehicle. Numerical results are provided to show the feasibility of the system. The proposed system may have broad applications in transportation, homeland security, and remote vehicle inventory control.	additive white gaussian noise;aerial photography;authentication;authorization;automatic number plate recognition;digital signature;distortion;encode;edge detection;elegant degradation;encryption;gradient;inventory control;multiplexing;real-time computing;rejection sampling;signal-to-noise ratio;tag cloud;turbulence;utility functions on indivisible goods	Elisabet Pérez-Cabre;Bahram Javidi	2005	IEEE Transactions on Vehicular Technology	10.1109/TVT.2005.851358	telecommunications;computer science;engineering;authentication;world wide web;computer security	Mobile	37.74189768709158	-13.9809506657691	60680
a2d5507af0eb5b677aba6cfadc673d69cd998e34	lossless secret image sharing based on generalized: lsb replacement	secret sharing;chinese remainder theorem;image reconstruction;lossless data hiding	In traditional secret sharing schemes a secret is shared within a group of users such that a subset of the group must come together and collaborate in order to reconstruct the secret. In general image secret sharing schemes, the cover image, within which a secret share is hidden, cannot be restored back to its original form. This loss is unaffordable in situations dealing with sensitive images, for example, medical, military and legal industries. In this paper we propose a secret image sharing scheme by which after secret reconstruction, the cover image can also be restored back to its original form without any bit loss. In the proposed scheme, the secret share generation and secret image reconstruction are based on the concept of the Chinese Remainder Theorem. Hiding of the secret image shares and the cover image retrieval are done utilizing a Generalized Least Significant Bit Replacement method. Implementation results show that the proposed method compares favorably to a recent method for lossless secret sharing.	image retrieval;iterative reconstruction;least significant bit;lossless compression;secret sharing	Ruchira Naskar;Rajat Subhra Chakraborty	2012		10.1145/2401603.2401668	shared secret;theoretical computer science;shamir's secret sharing;mathematics;homomorphic secret sharing;secure multi-party computation;internet privacy;proactive secret sharing;secret sharing;computer security;verifiable secret sharing	Crypto	38.9211940937651	-10.757559316452367	61225
3ec390aa18c61569c9b0504e88039d89d0331a37	analytic expressions for rate and cv of a type i neuron driven by white gaussian noise	use;expression analytique;gaussian noise;espiga positiva;intensidad;statistique;intervalo;neurone;moment;quadrature;type;tecnologia electronica telecomunicaciones;coefficient of variation;densite;computacion informatica;stochastic process;tipo;approximation numerique;momento;seuil;analytical expression;parametre;sistema;nudo;simulacion numerica;influencia;ruido gaussiano;additive noise;error sistematico;ruido aditivo;forma normal;grupo de excelencia;threshold;leaky integrate and fire;bruit additif;densidad;cuadratura;time;result;intervalle;input;utilizacion;aproximacion numerica;pointe positive;type i neuron;influence;spike;utilisation;interpretacion;parametro;parameter;interval;gaussian white noise;neurona;cv;temps;bias;neuron type i;ciencias basicas y experimentales;system;coefficientof variation;relacion;simulation numerique;bruit gaussien;entree ordinateur;processus stochastique;statistics;distancia;normal form;resultado;ruido blanco;interpretation;potencial accion;forme normale;white gaussian noise;formula cuadratura;resultat;first passage time;numerical approximation;umbral;systeme;density;noeud;stochastic model;quadrature formula;reseau neuronal;tecnologias;entrada ordenador;proceso estocastico;grupo a;bruit blanc;formule quadrature;action potential;relation;modelo estocastico;node;red neuronal;white noise;modele stochastique;neuron;distance;intensity;intensite;potentiel action;erreur systematique;estadistica;neural network;numerical simulation;tiempo	We study the one-dimensional normal form of a saddle-node system under the influence of additive gaussian white noise and a static bias current input parameter, a model that can be looked upon as the simplest version of a type I neuron with stochastic input. This is in contrast with the numerous studies devoted to the noise-driven leaky integrate-and-fire neuron. We focus on the firing rate and coefficient of variation (CV) of the interspike interval density, for which scaling relations with respect to the input parameter and noise intensity are derived. Quadrature formulas for rate and CV are numerically evaluated and compared to numerical simulations of the system and to various approximation formulas obtained in different limiting cases of the model. We also show that caution must be used to extend these results to the neuron model with multiplicative gaussian white noise. The correspondence between the first passage time statistics for the saddle-node model and the neuron model is obtained only in the Stratonovich interpretation of the stochastic neuron model, while previous results have focused only on the Ito interpretation. The correct Stratonovich interpretation yields CVs that are still relatively high, although smaller than in the Ito interpretation; it also produces certain qualitative differences, especially at larger noise intensities. Our analysis provides useful relations for assessing the distance to threshold and the level of synaptic noise in real type I neurons from their firing statistics. We also briefly discuss the effect of finite boundaries (finite values of threshold and reset) on the firing statistics.	abstract interpretation;additive white gaussian noise;anatomic node;approximation;biasing;biological neuron model;coefficient of variance;concurrent versions system;first-hitting-time model;gaussian quadrature;image scaling;large;newton–cotes formulas;node - plant part;normal statistical distribution;numerical analysis;parameter (computer programming);population parameter;simulation;small;synaptic package manager;test scaling;utility functions on indivisible goods;white noise	Benjamin Lindner;André Longtin;Adi R. Bulsara	2003	Neural Computation	10.1162/08997660360675035	stochastic process;telecommunications;calculus;mathematics;white noise;parameter;artificial neural network;statistics	ML	35.60239141036032	-21.676569557830945	61315
a37707969132c98417ded61bb6aba69f67686c57	performance of some ridge parameters for probit regression: with application to swedish job search data	sample size;ridge regression;mse;maximum likelihood;social sciences;job search;probit regression;secondary 62j02;economics and business;mean square error;samhallsvetenskap;ekonomi och naringsliv;monte carlo simulation;multicollinearity;primary 62j07	In ridge regression the estimation of the ridge parameter is an important issue. This paper generalizes some methods for estimating the ridge parameter for probit ridge regression (PRR) model based on the work of Kibria et al. (2011). The performance of these new estimators are judged by calculating the mean square error (MSE) using Monte Carlo simulations. In the design of the experiment we chose to vary the sample size and the number of regressors. Furthermore, we generate explanatory variables that are linear combinations of other regressors, which is a common situation in economics. In an empirical application regarding Swedish job search data we also illustrate the benefits of the new method.	mean squared error;monte carlo method;probit model;production rule representation;simulation	Håkan Locking;Kristofer Månsson;Ghazi Shukur	2013	Communications in Statistics - Simulation and Computation	10.1080/03610918.2011.654032	sample size determination;probit model;econometrics;multicollinearity;data mining;mathematics;mean squared error;maximum likelihood;tikhonov regularization;statistics;monte carlo method	AI	29.024945747392714	-22.755405141668415	61497
3b82dc15b9aa7e2e02937a70c76d31fc4be2330f	evolutionary robotics: coping with environment change	environmental change;evolutionary robotics	In this paper an evolutionary method consisting of encoding a set of local adaptation rules that synapses obey while a robot freely moves in the environment is compared to standard evolution of fixed-weight control networks. The results show that evolutionary adaptive controllers can adapt online without additional evolutionary training to strong environmental changes where instead the performance of evolutionary fixedweight controllers is significantly degraded. Two cases are described: transfer of evolved controllers from simulated to real robots and across different robotic platforms that vary in size, shape, and sensor response profile. In both cases evolved adaptive controllers autonomously and quickly adjust synaptic weights to successfully accomplish the task in the new conditions.	autonomous robot;control system;evolution;evolutionary robotics;game controller;genetic algorithm;high- and low-level;hoc (programming language);interaction;order of approximation;simulation;synapse;synaptic weight	Joseba Urzelai;Dario Floreano	2000				Robotics	24.97431777269958	-12.68720866053528	61683
fedc53df273d9ca0641a612d43a6aee425983986	the nature of sensitivity in monotone missing not at random models	linear mixed model;49k40;metodo estadistico;analisis sensibilidad;donnee longitudinale;likelihood ratio;computacion informatica;analisis datos;estadistica test;informacion incompleta;statistique test;statistical method;incomplete information;incomplete data;data analysis;pattern mixture models;methode statistique;ciencias basicas y experimentales;sensitivity analysis;missing at random;matematicas;statistical computation;information incomplete;likelihood ratio test;calculo estadistico;local influence;statistics;analyse sensibilite;influence locale;analyse donnee;ignorability;calcul statistique;truncation;grupo a;dropout process;rapport vraisemblance;test razon verosimilitud;test rapport vraisemblance;longitudinal data;categorical data;missing not at random;test statistic;inference;relacion verosimilitud;clinical trials	Models for incomplete longitudinal data under missingness not at random have gained some popularity. At the same time, cautionary remarks have been issued regarding their sensitivity to often unverifiable modeling assumptions. Consequently, there is evidence for a shift towards using ignorable methodology, supplemented with sensitivity analyses to explore the impact of potential deviations of this assumption in the direction of missingness at random. One such tool is local influence. It is shown that local influence tends to pick up a lot of different anomalies in the data at hand, not just deviations in the MNAR mechanism. This particular behavior is described and insight offered in terms of the non-standard behavior of the likelihood ratio test statistic for MAR missingness versus MNAR missingness within a model of the Diggle and Kenward type. © 2004 Elsevier B.V. All rights reserved.	missing data;random number generation;monotone	Ivy Jansen;Niel Hens;Geert Molenberghs;Marc Aerts;Geert Verbeke;Michael G. Kenward	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2004.10.009	econometrics;likelihood-ratio test;data mining;mathematics;statistics	AI	32.40434813538875	-21.97971727495228	61746
7bd1abb1fbbb8db1ba353e82baa3f5954f5fcb2f	goodness of fit test for discrete random variables	empirical process gof test;discrete random variables;continuous extension of discrete distribution;estimation effect;khmaladze transformation;vasicek s test;goodness of fit test;maximum entropy test	In this paper, a goodness of fit (gof) test for discrete random variables is studied. For the test, the empirical process gof test constructed based on the Khmaladze transformation method is considered to remove the parameter estimation effect. Further, the approach of the continuous extension of discrete random variables introduced in Denuit and Lambert (2005) is adopted. It is shown that under regularity conditions, the transformed empirical process weakly converges to a standard Brownianmotion. As a gof test based on this result, the maximum entropy type test designed by Lee et al. (2011) is considered. As with the empirical process gof test, Vasicek’s entropy test is also considered and a properlymodified version, whose limiting distribution is unaffected by the choice of parameter estimates, is provided. Simulation results are provided for illustration. © 2013 Published by Elsevier B.V.	estimation theory;simulation	Sangyeol Lee	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.07.026	econometrics;discrete mathematics;mathematics;goodness of fit;khmaladze transformation;statistics	AI	31.29339421846022	-19.83401023290566	61760
e8620a7a8d06c3489bf4cbce487c95377ac250f7	overlapping batch means: something for nothing?	confidence interval;technical report;asymptotic variance	Nonoverlapping batch means (NOLBM) is a we11-known approach For estimating the variance of the sample mean. In this paper we consider an overlapping batch means (OLBM) estimator that, based on the same assumptions and batch size as NOLBM, has essentially the same mean and only 2/3 the asymptotic variance of NOLBM. Confidence interval procedures for the mean based on NOLBM and OLBM are discussed. Both estimators are compared to the classical estimator of the variance of the mean based on sums of covariances.		Marc S. Meketon;Bruce W. Schmeiser	1984			econometrics;mathematical optimization;delta method;confidence interval;computer science;technical report;mathematics;bias of an estimator;world wide web;one-way analysis of variance;statistics	ML	31.137129725798147	-21.990185403642258	61806
d78eca754e49d96dc1350a84344c29d3afa8a0fc	stratified sampling in the simplex with applications to estimating distributions	operations research;random variable;monte carlo;variance reduction;stratified sampling	Many problems in statistics and operations research reduce to the evaluation of the distribution of a random variable, called the response, known to be a complicated function of a number, d say, of independent uniform variables. Monte Carlo estimation is often used for this purpose if the distribution is analytically intractable. Often the response possesses symmetry properties with respect to its arguments. It is then possible to restrict sampling to simplex regions of the sample space. This can be easily combined with stratified sampling to give variance reduction of order O(d) compared with normal stratified sampling. The theory of such methods is discussed and a simple stratified sampling scheme is applied to two examples giving a two to five fold reduction in the variance.	monte carlo method;operations research;sampling (signal processing);stratified sampling;variance reduction	Russell C. H. Cheng;Teresa R. Davenport	1988		10.1145/318123.318250	systematic sampling;random variable;sampling;econometrics;mathematical optimization;simple random sample;sampling design;balanced repeated replication;importance sampling;slice sampling;mathematics;rejection sampling;cluster sampling;stratified sampling;umbrella sampling;monte carlo integration;poisson sampling;statistics;variance reduction;monte carlo method	Theory	31.24533790157679	-17.243507552361898	61865
413991eb0042445b15b05b581a160cdae8aecc13	statistical analysis of sample-size effects in ica	sample size;statistical analysis	Independent component analysis (ICA) solves the blind sour ce separation problem by evaluating higher-order statistics . In a first approximation this implies using thirdand fourth-order t ms. Indeed, many ICA algorithms often moreover assume symmetri c sources and hence rely on fourth-order cumulants or kurtosi s only, estimated by sample kurtosis. In this contribution we discu s the effect of the sample size and the true value of the kurtosis on the estimation quality. For a family of distribution with a larg e range of kurtoses we present analytical results of the estimation quality, both in the Cramér-Rao sense as well as for the best known est imators. We further provide results from simulations that su pport the relevance of the analytical results.	algorithm;independent computing architecture;independent component analysis;order of approximation;relevance;simulation	J. Michael Herrmann;Fabian J. Theis	2007		10.1007/978-3-540-77226-2_43	sample size determination;independent component analysis;econometrics;computer science;pattern recognition;mathematics;statistics	ML	30.5582698676657	-22.484065378501576	61964
a060b3a694d86aafa948bb2e476b826904c7b613	implementing refined descriptive sampling into three-phase discrete-event simulation models	90b99;65c05;65c10;68u20;sampling;activity cycle diagram;modeling;monte carlo methods;discrete event simulation	ABSTRACTThis article examines the behavior of refined descriptive sampling (RDS) on a civil engineering problem and focuses on the statistical comparison of random sampling (RS) and RDS. The problem considered is the aggregates quarry of Bejaia city. A discrete event simulation model is built using the activity cycle diagram and an application is then developed under the simulation system PSim, in Turbo Pascal programming language. The simulation of aggregates quarry is carried out to measure its performance using both sampling methods to generate input variables. Variance reduction and efficiency are deduced. The results prove the efficiency of RDS over RS.	sampling (signal processing);simulation	Latifa Baghdali-Ourbih;Megdouda Ourbih-Tari;Abdelnasser Dahmani	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1085557	sampling;systems modeling;discrete event simulation;mathematics;activity cycle diagram;algorithm;statistics;monte carlo method	AI	34.5920881315575	-16.71078911116649	62085
f6c9a5eef77a6d98d8d71dd1e4fd5dc151fa2164	model-based optimization of sulfur recovery units	recuperacion energia;water vapor;modelizacion;desulfuration;recuperation energie;optimisation;claus process;concepcion ingenieria;engineering design;waste heat boiler;optimizacion;kinetic model;modele cinetique;conception ingenierie;modelo cinetico;energy recovery;production combinee;fabrica productos quimicos;vapeur eau;horno;reacteur;energie totale;energia total;heat recovery;modelisation;caldera recuperacion;total plant optimization;recuperacion calor;produccion combinada;cinetique;cogeneration;furnace;desulfuracion;optimization;desulfurization;usine chimique;recuperation chaleur;chemical plant;acid gas;chaudiere recuperation;kinetics;detailed kinetics;total energy;modeling;cinetica;reactor;vapor agua;four	Multi-scale process modeling is very appealing methodology for process optimization since it highlights certain issues that remain unexplored with conventional methodologies and debottlenecks certain potentialities that remain unexploited in chemical plants. In this work, a kinetic model with 2400 reactions and 140 species is implemented in a proper reactor network to characterize the thermal furnace and the waste heat boiler of sulfur recovery units; the network with detailed kinetics is the kernel of a Claus process simulation that includes all the unit operations and the catalytic train. By doing so, reliable estimation of acid gas conversion, elemental sulfur recovery, and steam generation is achieved with the possibility to carry out an integrated process-energy optimization at the total plant scale.	mathematical optimization	Flavio Manenti;Davide Papasidero;Giulia Bozzano;Eliseo Ranzi	2014	Computers & Chemical Engineering	10.1016/j.compchemeng.2014.01.019	energy recovery;mathematical optimization;energy;systems modeling;chemistry;flue-gas desulfurization;environmental engineering;chemical plant;heat recovery ventilation;engineering;claus process;water vapor;mineralogy;waste management;acid gas;engineering design process;kinetics;waste heat recovery unit	DB	33.05166351533063	-10.015938697913825	62225
ec1c66777bae9cb2677dc5d64f9d2f81209dda74	power calculations for global and local moran's i	software;punto silla;moran s i;approximation normale;power analysis;test statistique;spatial dependence;analisis estadistico;spatial data;analisis datos;logiciel;analisis espacial;fonction repartition;implementation;point col;test estadistico;loi exacte;statistical test;processus autoregressif;methode col;distribucion estadistica;saddle point method;funcion distribucion;data analysis;test power;distribution function;estimation erreur;statistical analysis;distribution statistique;autoregressive processes;error estimation;statistical computation;analyse statistique;estimacion error;calculo estadistico;software package;logicial;analyse donnee;calcul statistique;progiciel;point selle;spatial analysis;estimation statistique;potencia test;implementacion;exact distribution;estimacion estadistica;puissance test;60e05;statistical estimation;statistical distribution;paquete programa;normal approximation;analyse spatiale;62e15;metodo punto en puerto;saddle point	As in any statistical test, a power analysis can help in assessing the outcomes of whether global or local spatial dependencies exist. This point was briefly addressed with respect to global Moran’s I, but it has not been widely used. One reason may be that the most commonly used spatial analysis and GIS software packages do not support power analysis. Thus, apart from using the code for saddle-point approximation, applications have been restricted to employing normal approximations. An implementation of the exact distributions for global and local Moran’s I, which are integrated into the R-package spdep, is presented. Furthermore, assuming a simultaneous autoregressive spatial data generating scheme, substantial cases are provided, demonstrating the drawbacks and potential flaws of using the normal approximation in power calculations. The results confirm that, particularly for localMoran’s I, due to the smallness of sets of neighborhoods, this practicemay potentially lead to errors of inference. An example concernedwithUpperAustrian migration, where using the exact distribution leads to different conclusions, is presented as well. © 2009 Published by Elsevier B.V.	approximation;autoregressive model;computer cluster;geographic information system;hotspot (wi-fi);neural coding;spatial analysis;truncation	Roger Bivand;Werner G. Müller;Markus Reder	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.07.021	econometrics;calculus;mathematics;spatial analysis;statistics	DB	33.80345962491874	-22.327401167431557	62534
88127339cf4caaaa253b59073b31e1f849fd5c09	estimation of weibull parameters from wind measurement data by comparison of statistical methods	weibull parameters;statistical test;statistical methods;weibull distribution;wind speed shape frequency measurement method of moments weibull distribution wind energy;wind power method of moments parameter estimation weibull distribution;mean wind speed;statistical test mean wind speed weibull distribution statistical methods weibull parameters;power density method weibull parameter estimation wind measurement data wind speed characteristics macedonia republic ad 2015 03 to 2012 08 wind resource wind power wind energy conversion systems weibull wind speed distribution wind energy analysis method of moments empirical method maximum likelihood method	In this study, we investigate the wind speed characteristics in the northern part of Republic of Macedonia, using measured wind speed data in the period from August 2012 to March 2015. The two parameter Weibull distribution is widely used for modeling, characterizing and predicting wind resource and wind power, as well as assessing the optimal performance of wind energy conversion systems. Therefore it is very important to precisely estimate the Weibull parameters for any candidate site. We compare four methods for calculating the parameters of Weibull wind speed distribution for wind energy analysis: Method of moments, Empirical method, Maximum likelihood method and Power density method. We apply each method on a sample of hourly averaged wind speed data and estimate its accuracy using statistical methods of analysis. Finaly, we conclude that the Maximum likelihood method gives best results in calculating the shape parameter - k and scale parameter - c for the particular site. Once the Weibull parameters are calculated with high accuracy, further research can be done in using the kinetic wind energy.	chi;parameter (computer programming);performance	Maja Celeska;Krste Najdenkoski;Vlatko Stoilkov;Aneta Buchkovska;Zhivko Kokolanski;Vladimir Dimchev	2015	IEEE EUROCON 2015 - International Conference on Computer as a Tool (EUROCON)	10.1109/EUROCON.2015.7313684	reliability engineering;weibull distribution;econometrics;statistical hypothesis testing;weibull modulus;statistics	Visualization	29.58612262664844	-18.41509155383303	62640
e88af3fc0fb7149894b4e1497d996ac9be628a63	phase ii monitoring of multivariate multiple linear regression profiles	multiple linear regression;statistical process control;profile monitoring;phase ii;average run length arl;change point;multivariate multiple linear regression profiles	In certain cases, the quality of a process or a product can be effectively characterized by two or more multiple linear regression profiles in which response variables are correlated. This structure can be modeled as multivariate multiple linear regression profiles. When linear profiles are monitored separately, then correlation between response variables is ignored and misleading results could be expected. To overcome this problem, the use of methods that consider the multivariate structure between response variables is inevitable. In this paper, we propose four methods to monitor this structure in Phase II. The performance of the methods is compared through simulation studies in terms of the average run length criterion. Furthermore, a method based on likelihood ratio approach is developed to determine the location of shifts and a numerical simulation is used to evaluate the performance of the proposed method. Finally, the use of the methods is illustrated by a numerical example. Copyright © 2010 John Wiley & Sons, Ltd.	computer simulation;frequency response;john d. wiley;numerical analysis;run-length encoding	M. Eyvazian;Rassoul Noorossana;Abbas Saghaei;Amirhossein Amiri	2011	Quality and Reliability Eng. Int.	10.1002/qre.1119	segmented regression;econometrics;multivariate statistics;mathematical optimization;proper linear model;multivariate adaptive regression splines;linear regression;linear predictor function;bayesian multivariate linear regression;linear model;mathematics;design matrix;regression analysis;statistical process control;statistics;cross-sectional regression;general linear model	AI	28.250854131672487	-19.685950298073433	62664
db6ffe649847e16eae25759cd1b16d607a387d24	moment matching approximation of asian basket option prices	moment adaptation;moment matching;metodo momento;analisis numerico;asian basket option;computacion informatica;matematicas aplicadas;moment method;mathematiques appliquees;stochastic method;normal distribution;variable aleatoire;numerical method;pricing;mathematics and statistics;aproximacion;60j65;variable aleatoria;65k15;curva gauss;fijacion precios;arithmetique;analyse numerique;approximation;65c20;numerical analysis;aritmetica;metodo numerico;arithmetics;basket option;ciencias basicas y experimentales;methode moment;matematicas;independent random variables;numero de condicionamiento;algebra lineal numerica;random variable;algebre lineaire numerique;condition number;65f35;loi normale;sum of non independent random variables;methode stochastique;15a12;numerical linear algebra;geometric mean;grupo a;applied mathematics;91b28;fixation prix;indice conditionnement;gaussian distribution;methode numerique;management science;log extended skew normal;metodo estocastico	In this paper we propose some moment matching pricing method s for European-style discrete arithmetic Asian basket opti ons n a Black & Scholes framework. We generalize the approach of [5 ] and of [8] in several ways. We create a framework that allows for a whole class of conditioning random variables which are normally distributed. We moment match not only with a lognor mal random variable but also with a log-extended-skew-normal r andom variable. We also improve the bounds of [9]. Numerical esults are included and on the basis of our numerical tests, we expla in which method we recommend depending on moneyness and time-to-maturity.	approximation algorithm;black–scholes model;capability maturity model;numerical analysis;numerical method	Griselda Deelstra;Ibrahima Diallo;Michèle Vanmaele	2010	J. Computational Applied Mathematics	10.1016/j.cam.2009.03.004	normal distribution;econometrics;lattice model;numerical analysis;calculus;mathematics;statistics	ML	34.128174569951675	-19.770837449967527	62900
6a8158e1925b02173f44602cfe9e8f86e20b78ba	moving blocks bootstrap control chart for dependent multivariate data	sample size;statistical process control control charts principal component analysis;real industrial data dependent multivariate data moving blocks bootstrap control chart principal component analysis independent and identically distributed assumption empirical average run length adopting simulation data;statistical process control;false alarm rate;control chart;average run length;independent and identically distributed;control charts;control charts industrial control statistical analysis principal component analysis statistical distributions sampling methods parametric statistics weight control size control distributed computing;principal component analysis;block bootstrap;multivariate data	A method for constructing moving blocks bootstrap control chart for dependent multivariate data was proposed. In this method the statistics of the control charts were firstly obtained by principal component analysis, then a modified MBB was used to determine their control limits which can not be based on independent and identically distributed (IID) assumption. Empirical average run length and false alarm rate were proposed to evaluate the control charts. Two examples, adopting simulation data and real industrial data respectively, were given to illustrate the method, in which some results about factors' impact on MBB, such as sample size , moving block size, block number, et al., were obtained. The results show MBB-based method is superior to PCA-based method under certain conditions and is an available way to establish control charts for weakly dependent multivariate data.	block size (cryptography);chart;minimum bounding box;principal component analysis;run-length encoding;simulation	Yu-ming Liu;Jun Liang;Ji-xin Qian	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1401016	econometrics;control chart;computer science;pattern recognition;shewhart individuals control chart;u-chart;statistics	Robotics	28.209630568860586	-20.134176277079558	62967
3ef9840db0d063946c1329199a6c2f8a6d2f8083	sequential stopping rules for fixed-sample acceptance tests	stopping rule	This paper discusses optimal stopping rules for fixed-sample acceptance tests where the observations with time delays are obtained sequentially. It studies two cases, one with a known prior distribution and the other one without a prior distribution, discusses Bayes-optimal and minimax stopping rules, and considers a stopping rule using the maximum likelihood estimate of I¸, the probability of a single success. An example is given assuming that the prior distribution is a beta distribution. It is shown that the Bayes-optimal stopping rule thus obtained approaches the stopping rule using the maximum likelihood estimate when the beta parameters, α and β, approach zero.	acceptance testing	R. C. Chang;S. Ehrenfeld	1974	Operations Research	10.1287/opre.22.1.100	sequential estimation;econometrics;mathematical optimization;likelihood principle;optimal stopping;stopping time;optional stopping theorem;mathematics;statistics	Theory	31.09091714846749	-20.08684474094663	63512
e85d492958a341d36628048d0771dac265400994	nonparametric approach to identifying narx systems	α mixing;recursive estimation;nonparametric;kernel function;journal;narx;fixed point;recursive estimate;strong consistency;theoretical analysis;stochastic approximation;growth rate;geometrically ergodic;geometric ergodicity;qijiang song han fu chen x系统 nar 非参数 非线性自回归 非线性函数 随机逼近 强一致性 数值模拟 nonparametric approach to identifying narx systems;mixing;strongly consistent;markov chains;numerical simulation;markov chain	This paper considers identification of the nonlinear autoregression with exogenous inputs (NARX system). The growth rate of the nonlinear function is required be not faster than linear with slope less than one. The value of f(·) at any fixed point is recursively estimated by the stochastic approximation (SA) algorithm with the help of kernel functions. Strong consistency of the estimates is established under reasonable conditions, which, in particular, imply stability of the system. The numerical simulation is consistent with the theoretical analysis.	nonlinear autoregressive exogenous model	Qi-jiang Song;Han-Fu Chen	2010	J. Systems Science & Complexity	10.1007/s11424-010-9268-1	computer simulation;stochastic approximation;econometrics;markov chain;mathematical optimization;mathematics;statistics	Logic	30.288654412939287	-23.292886164169396	63745
f91cdb51654c32fb4ac579873f80ff4415d08bdc	recognition of electro-magnetic information leakage of computer based on multi-image blind deconvolution		The security problem of screen image leakage from a display unit has become serious with the rapid speed of signal transmission technology. This paper presents a novel investigation on the characteristics of the power line compromising channel. Moreover, a measurement system has been actually developed for the leakage signal analyzing and image reconstruction. In order to overcome the degradation of reconstructed motion images and enhance the reconstructed image quality, a multi-image blind deconvolution method was proposed and test experiments were carried out to verify the effectiveness of the multi-image blind deconvolution algorithm based on the conducted signal from the power line.	blind deconvolution;information leakage;spectral leakage	Shanjing Yang;Jianlin Hu;Weiqing Huang	2017		10.1007/978-3-319-60033-8_77	iterative reconstruction;computer vision;transmission (telecommunications);computer science;information leakage;distributed computing;image quality;leakage (electronics);artificial intelligence;blind deconvolution;communication channel;multi-image	Crypto	37.27989792905357	-10.25446165334056	63772
17a67d95411a234dde3692da3adbe062a7906bbd	drug risk assessment with determining the number of sub-populations under finite mixture normal models	62 07;bayes estimation;modele risque;schwarz criterion;selection problem;model selection;population finie;medicament;problema seleccion;bayes model;relacion dosis respuesta;analisis datos;melange loi probabilite;maximum likelihood;recuento poblacion;maximum vraisemblance;bayes factor;mixed distribution;dose activity relation;selection modele;curva gauss;finite population;effectif population;62f07;data analysis;estimacion bayes;methode selection;maximum likelihood estimate;seleccion modelo;modele bayes;62f15;statistical computation;62f03;calculo estadistico;62d05;loi normale;facteur bayes;algorithme em;mixture normal models;population number;bic;risk assessment;mezcla ley probabilidad;analyse donnee;medicamento;calcul statistique;algoritmo em;dose response;drug;selection method;risk model;finite mixture model;em algorithm;62p10;gaussian distribution;maxima verosimilitud;poblacion finita;62h12;relation dose reponse;estimation bayes;finite mixture;probleme selection	In some biological experiments, it is quite common that laboratory subjects may be different in their patterns of susceptibility to a treatment. In these situations, finite mixture model analysis becomes a useful tool. Under a finite mixture normal model, drug risk assessment problems are studied under the assumption that the population at risk consists of multiple sub-types of different susceptibilities. The EM algorithm and its extension are utilized to obtain the maximum likelihood estimates of the model. Particular discussion is given to the model selection problem based on Bayes factor, which is approximated by the Schwarz criterion. Meantime, the practical significance of the proposed method is illustrated with an actual dose–response data set.	population;risk assessment	Jian Tao;Ning-Zhong Shi;S.-Y. Sik-Yum Lee	2004	Computational Statistics & Data Analysis	10.1016/j.csda.2003.09.006	econometrics;calculus;mathematics;maximum likelihood;statistics	ML	33.02701576482231	-22.98648674607294	64300
5e34236f7cc720c14ae69589d7a153b388cab4f7	optimization under uncertainty of a composite fabrication process using a deterministic one-stage approach	computacion informatica;materials processing;uncertainty;polymer matrix composite;osop;grupo de excelencia;process design;sampling;objective function;optimization problem;optimization under uncertainty;ciencias basicas y experimentales;probability distribution;quimica;optimization;differential algebraic equation;computational efficiency;non linear system;pultrusion	Process design under uncertainty has received considerable attention in recent years, and has led to the development of several modeling and solution approaches. These approaches are broadly categorized under stochastic formulations (model parameters with probability distributions), multiperiod formulations (where uncertain parameters are discretized into a number of deterministic realizations), and parametric programming formulations. This paper presents an application of the one-stage optimization problem (OSOP), a multiperiod method, to find optimal cure temperature cycle design under uncertainty for polymer-matrix composites fabrication using the pultrusion process. The process is governed by a highly non-linear system of partial differential-algebraic equations. The OSOP method is also systematically compared with a sampling-based approach in terms of computational efficiency and solution quality. Most work done so far using such deterministic methods has focused on problems where the performance objective function (often cost) and process constraints are analytic/algebraic in nature. In contrast, in materials processing simulations, evaluation of the objective function and the process constraints are based on the solution of differential-algebraic equations (DAE). © 2006 Elsevier Ltd. All rights reserved.	categorization;computation;deterministic algorithm;differential algebraic equation;discretization;linear algebra;linear system;mathematical optimization;nonlinear system;optimization problem;parametric programming;polymer;program optimization;sampling (signal processing);semiconductor device fabrication;simulation	Charles Acquah;Ivan V. Datskov;Andryas Mawardi;Feng Zhang;Luke E. K. Achenie;Ranga Pitchumani;Eugene Santos	2006	Computers & Chemical Engineering	10.1016/j.compchemeng.2005.12.015	probability distribution;optimization problem;process design;sampling;econometrics;mathematical optimization;uncertainty;pultrusion;differential algebraic equation;mathematics;statistics	AI	31.566042166064122	-15.03457713505353	64361
42a0791a7edfa8abaf4cf128ff9f8c720e36539a	bayesian prediction intervals and their relationship to tolerance intervals	bayesian prediction;system reliability;hierarchical linear model;normal distribution;gibbs sampling;variance components model;markov chain monte carlo;gamma distribution;predictive density	We consider Bayesian prediction intervals that contain a proportion of a finite number of observations with a specified probability. Such intervals arise in numerous applied contexts and are closely related to tolerance intervals. Several examples are provided to illustrate this methodology, and simulation studies are used to demonstrate potential pitfalls of using tolerance intervals when prediction intervals are required.		Michael S. Hamada;Val Johnson;Leslie M. Moore;Joanne Wendelberger	2004	Technometrics	10.1198/004017004000000518	normal distribution;gamma distribution;econometrics;gibbs sampling;markov chain monte carlo;multilevel model;mathematics;credible interval;statistics	NLP	30.423132757569583	-19.97845563596598	64575
855fceaaee885a8b4ce9001dc5ca1217fc3889e8	accelerating the parameters identifiability procedure: set by set selection	parameters identifiability;scarce experimental data;info eu repo semantics article;parameters estimation;binary search;info ar repo semantics articulo;info eu repo semantics publishedversion	In this paper, a numerical procedure based on the binary search is proposed for accelerating the parameters identifiability procedure. Basically, the parameters are selected set by set using a given criterion for ranking the parameters. Since parameters identifiability procedures are strongly dependent on the initial estimates of parameters values, simultaneous parameters re-estimation step has been proposed in this paper. Two examples were used to evaluate the performance of the proposed criterion. In both cases, a significant reduction of the computational time was observed, and the results regard to the model fit are similar to those criteria based on the selection of parameters one by one, as usually presented in the literature.		Kese P. F. Alberton;André Luís Alberton;Jimena Andréa Di Maggio;María Soledad Díaz;Argimiro R. Secchi	2013	Computers & Chemical Engineering	10.1016/j.compchemeng.2013.04.014	econometrics;mathematical optimization;computer science;data mining;statistics;binary search algorithm	Logic	29.58564131286056	-13.06495798616767	64870
354b85df78791ea74d194b81686832039a876eef	comparison of resampling schemes for particle filtering	monte carlo methods;particle filtering (numerical methods);signal sampling;bootstrap filter algorithm;multinomial resampling approach;particle filtering;resampling schemes;residual methods;sequential monte carlo methods;stratified methods;particle filter;central limit theorem	This contribution is devoted to the comparison of various resampling approaches that have been proposed in the literature on particle filtering. It is first shown using simple arguments that the so-called residual and stratified methods do yield an improvement over the basic multinomial resampling approach. A simple counter-example showing that this property does not hold true for systematic resampling is given. Finally, some results on the large-sample behavior of the simple bootstrap filter algorithm are given. In particular, a central limit theorem is established for the case where resampling is performed using the residual approach.	automated theorem proving;multinomial logistic regression;particle filter;peterson's algorithm;resampling (statistics);sampling (signal processing);stratified sampling;subatomic particle	Randal Douc;Olivier Cappé;Eric Moulines	2005	ISPA 2005. Proceedings of the 4th International Symposium on Image and Signal Processing and Analysis, 2005.		econometrics;mathematical optimization;particle filter;auxiliary particle filter;central limit theorem;mathematics;statistics;monte carlo method	Arch	30.33929534850083	-22.12853158459376	65196
87905d233b1de1462f72fe71e4997a65ad357e8e	parameter estimates of general failure rate model: a bayesian approach		The failure rate function plays an important role in studying the lifetime distributions in reliability theory and life testing models. A study of the general failure rate model $r(t)=a+bt^{\theta-1}$, under squared error loss function taking $a$ and $b$ independent exponential random variables has been analyzed in the literature. In this article, we consider $a$ and $b$ not necessarily independent. The estimates of the parameters $a$ and $b$ under squared error loss, linex loss and entropy loss functions are obtained here.		Asok K. Nanda;Sudhansu S. Maiti;Chanchal Kundu;Amarjit Kundu	2019	J. Computational Applied Mathematics	10.1016/j.cam.2018.11.002	econometrics;mathematics;statistics	ML	30.771067247957355	-19.24465137247706	65487
d6a0363f02e829752c74ccea1465741e49228f89	a new variance-based global sensitivity analysis technique	variance ratio function;w;sobol s indices;model emulation;variance based sensitivity analysis;monte carlo simulation	A new set of variance-based sensitivity indices, called W -indices, is proposed. Similar to the Sobol’s indices, both main and total effect indices are defined. The W -main effect indices measure the average reduction of model output variance when the ranges of a set of inputs are reduced, and the total effect indices quantify the average residual variance when the ranges of the remaining inputs are reduced. Geometrical interpretations show that theW -indices gather the full information of the variance ratio function, whereas, Sobol’s indices only reflect the marginal information. Then the double-loop-repeated-set Monte Carlo (MC) (denoted as DLRSMC) procedure, the double-loop-single-set MC (denoted as DLSSMC) procedure and the model emulation procedure are introduced for estimating the W -indices. It is shown that the DLRS MC procedure is suitable for computing all theW -indices despite its highly computational cost. The DLSS MC procedure is computationally efficient, however, it is only applicable for computing low order indices. Themodel emulation is able to estimate all theW -indices with low computational cost as long as the model behavior is correctly captured by the emulator. The Ishigami function, a modified Sobol’s function and two engineering models are utilized for comparing the W and Sobol’s indices and verifying the efficiency and convergence of the three numerical methods. Results show that, for even an additive model, the W -total effect index of one input may be significantly larger than its W -main effect index. This indicates that there may exist interaction effects among the inputs of an additive model when their distribution ranges are reduced. © 2013 Elsevier B.V. All rights reserved.	additive model;algorithmic efficiency;approximation;computation;doppler effect;emulator;marginal model;monte carlo method;numerical method;rpgツクールvx value! +ツクールシリーズ素材集 和;utility functions on indivisible goods;verification and validation	Pengfei Wei;Zhenzhou Lu;Jingwen Song	2013	Computer Physics Communications	10.1016/j.cpc.2013.07.006	econometrics;mathematical optimization;variance-based sensitivity analysis;mathematics;statistics;monte carlo method	DB	28.890423344415257	-18.85872309635851	65577
904251f957a6e794fafc0854788007164359f1b3	some tests for detecting trends based on the modified baumgartner-weiß-schindler statistics	baumgartner weiis schindler test;baumgartner weis schindler test;e mp value;nonparametric statistics;e m p value;img class imglazyjsb inlineimage height 12 width 55 alt view the mathml source style margin top 5px;exact tests;test for trend;vertical align middle title view the mathml source src sd grey_pxl gif data inlimgeid 1 s2 0 s0167947312002514 si67 gif e m p;unconditional test	We propose a modified nonparametric Baumgartner-Weiβ-Schindler test and investigate its use in testing for trends among K binomial populations. Exact conditional and unconditional approaches to p-value calculation are explored in conjunction with the statistic in addition to a similar test statistic proposed by Neuhäuser (2006), the unconditional approaches considered including the maximization approach (Basu, 1977), the confidence interval approach (Berger and Boos, 1994), and the E + M approach (Lloyd, 2008). The procedures are compared with regard to actual Type I error and power and examples are provided. The conditional approach and the E + M approach performed well, with the E + M approach having an actual level much closer to the nominal level. The E + M approach and the conditional approach are generally more powerful than the other p-value calculation approaches in the scenarios considered. The power difference between the conditional approach and the E + M approach is often small in the balance case. However, in the unbalanced case, the power comparison between those two approaches based on our proposed test statistic show that the E+ M approach has higher power than the conditional approach.	confidence intervals;expectation–maximization algorithm;manuscripts;nominal level;ordinal position;ordinal data;p-value;population;sample statistic;schindler disease, type i;sensor;statistic (data);thomas baumgartner;unbalanced circuit;whole earth 'lectronic link	Guogen Shan;Changxing Ma;Alan D. Hutson;Gregory E. Wilding	2013	Computational statistics & data analysis	10.1016/j.csda.2012.04.021	nonparametric statistics;econometrics;cochran–armitage test for trend;mathematics;algorithm;statistics	SE	30.847276932902133	-21.918081054802943	65943
6321d5bcc73ccedc0cc42013e8df6c7fb4579bb0	lossless watermarking for image authentication: a new framework and an implementation	watermarking authentication image reconstruction distortion digital signatures image storage image coding availability image converters humans;public key cryptography;filigranage numerique;protection information;zero distortion reconstruction;analisis imagen;digital watermarking;controle acces;watermarking;evaluation performance;image storage;invertible authentication;image coding;performance evaluation;data integrity;forgery detection;image processing;data compression;lossless generalized least significant bit data embedding;availability;computer graphics;image converters;integrite donnee;implementation;evaluacion prestacion;localization;circuit sans perte;authentication;data embedding;digital signatures;tamper localization forgery detection invertible authentication lossless compression reversible data embedding;procesamiento imagen;lossless generalized least significant bit data embedding lossless watermarking image authentication zero distortion reconstruction image reconstruction public key authentication;cle publique;lossless compression;localizacion;traitement image;patents as topic;authentification;lossless watermarking;tamper localization;computer security;signal processing computer assisted;distortion;reconstruction image;reversible data embedding;least significant bit;image authentication;image enhancement;autenticacion;localisation;public key;image interpretation computer assisted;proteccion informacion;reconstruccion imagen;image reconstruction;information protection;filigrana digital;llave publica;product labeling;lossless circuit;algorithms;image analysis;pattern recognition automated;humans;access control;message authentication;public key cryptography image coding message authentication image reconstruction watermarking;algorithms computer graphics computer security data compression image enhancement image interpretation computer assisted patents as topic pattern recognition automated product labeling signal processing computer assisted	We present a novel framework for lossless (invertible) authentication watermarking, which enables zero-distortion reconstruction of the un-watermarked images upon verification. As opposed to earlier lossless authentication methods that required reconstruction of the original image prior to validation, the new framework allows validation of the watermarked images before recovery of the original image. This reduces computational requirements in situations when either the verification step fails or the zero-distortion reconstruction is not needed. For verified images, integrity of the reconstructed image is ensured by the uniqueness of the reconstruction procedure. The framework also enables public(-key) authentication without granting access to the perfect original and allows for efficient tamper localization. Effectiveness of the framework is demonstrated by implementing the framework using hierarchical image authentication along with lossless generalized-least significant bit data embedding.	algorithm;arabic numeral 0;authentication;computation;digital watermarking;distortion;embedding;least significant bit;lossless compression;most significant bit;public-key cryptography;requirement;verification of theories;watermark (data file)	Mehmet Utku Celik;Gaurav Sharma;A. Murat Tekalp	2006	IEEE Transactions on Image Processing	10.1109/TIP.2005.863053	computer vision;image analysis;image processing;digital watermarking;computer science;theoretical computer science;authentication;public-key cryptography;computer security	Vision	38.479613240439036	-11.469214057089882	66170
599ba447344a2fb52f2d49a17e26daa42269fccb	appraisal of statistical practices in hri vis-a-vis the t-test for likert items/scales		Likert items and scales are often used in human subject studies to measure subjective responses of subjects to the treatment levels. In the field of human-robot interaction (HRI), with few widely accepted quantitative metrics, researchers often rely on Likert items and scales to evaluate their systems. However, there is a debate on what is the best statistical method to evaluate the differences between experimental treatments based on Likert item or scale responses. Likert responses are ordinal and not interval, meaning, the differences between consecutive responses to a Likert item are not equally spaced quantitatively. Hence, parametric tests like ttest, which require interval and normally distributed data, are often claimed to be statistically unsound in evaluating Likert response data. The statistical purist would use non-parametric tests, such as the Mann-Whitney U test, to evaluate the differences in ordinal datasets; however, non-parametric tests sacrifice the sensitivity in detecting differences a more conservative specificity – or false positive rate. Finally, it is common practice in the field of HRI to sum up similar individual Likert items to form a Likert scale and use the t-test or ANOVA on the scale seeking the refuge of the central limit theorem. In this paper, we empirically evaluate the validity of the ttest vs. the Mann-Whitney U test for Likert items and scales. We conduct our investigation via Monte Carlo simulation to quantify sensitivity and specificity of the tests.	computer science;design of experiments;experiment;human–robot interaction;monte carlo method;ordinal data;robot;sensitivity and specificity;sensor;simulation	Matthew C. Gombolay;Ankit Shah	2016				HCI	27.863881880098464	-20.625568321762803	66199
6d7082fc0fde2eecb66ef10967106bc066ecc1fd	digital images authentication scheme based on bimodal biometric watermarking in an independent domain	biometric watermarking;iris recognition;multibiometrics;biometric verification;image authentication;independent component analysis ica;fingerprint recognition;image watermarking	We propose a new biometric-based bimodal watermarking scheme.Two independent watermarks based on fingerprint and iris biometrics are used.Our algorithm explores an independent domain of the cover image.Authentication procedure is based on fusion of two extracted biometric watermarks. With the growing accessibility and usability of internet there is a growing concern over content protection of digital images. Recently, to eliminate the traditional use of passwords and to ensure that the access to the image is restricted only to legitimate users, security solutions are increasingly combined with biometrics. Consequently, biometric-based watermarking algorithms, that involve embedding the identity of the owner, are proposed to solve ownership disputes. This paper presents a new scheme for protecting and authenticating invisibly watermarked digital images. It applies Independent Component Analysis to the cover image and enables the insertion of two independent watermarks based on fingerprint and iris biometrics. In this approach biometric techniques are used for watermarks generation and for owners authentication. The main advantage of proposed algorithm is construction of ICA based watermarking domain to enable insertion of two independent watermarks, that improve authentication accuracy and makes scheme more robust.	authentication;biometrics	Wioletta Wójtowicz;Marek R. Ogiela	2016	J. Visual Communication and Image Representation	10.1016/j.jvcir.2016.02.006	computer vision;digital watermarking;computer science;iris recognition;internet privacy;computer security;fingerprint recognition	Vision	37.86900145450572	-11.773721403569345	66764
d6db01ca1b56e35836344f4a40925a0257ecf1a6	testing independence in time series via universal distributions of permutations and words	goodness of fit;empirical distribution;time series;atrial fibrillation;probability distribution;permutations;chi square test;binary words;recursive algorithm;symbolic dynamics;symbolic analysis;ecg analysis	We study probability distributions of permutations and binary words, which arise in symbolic analysis of time series and their differences. Under the assumptions that the series is stationary and independent we show that these probability distributions are universal and we derive a recursive algorithm for computing the distribution of binary words. This provides a general framework for performing chi square tests of goodness of fit of empirical distributions versus universal ones. We apply these methods to analyze heartbeat time series; in particular we measure the extent to which atrial fibrillation can be modeled as an independent sequence.	algorithm;recursion (computer science);stationary process;time series	Camillo Cammarota;Enrico Rogora	2005	I. J. Bifurcation and Chaos	10.1142/S0218127405012788	empirical distribution function;probability distribution;symbolic dynamics;combinatorics;discrete mathematics;chi-square test;time series;mathematics;permutation;symbolic data analysis;goodness of fit;statistics;recursion	ML	31.985671786817658	-20.559772001377212	66955
2996558c9d0d5f849e5b694c2ca34788749449b5	evaluations of evidence combination rules in terms of statistical sensitivity and divergence	sensitivity;belief networks;belief functions;inference mechanisms;statistical analysis;divergence;evaluation criteria;mean square error;information fusion;mse;evidence combination rules;uncertainty reasoning;evidence combination;evaluation;statistical sensitivity;dempster's rule;sensor fusion;mean square error methods;robustness;estimation;uncertainty;noise	The theory of belief functions is one of the most important tools in information fusion and uncertainty reasoning. Dempster's rule of combination and its related modified versions are used to combine independent pieces of evidence. However, until now there is still no solid evaluation criteria and methods for these combination rules. In this paper, we look on the evidence combination as a procedure of estimation and then we propose a set of criteria to evaluate the sensitivity and divergence of different combination rules by using for reference the mean square error (MSE), the bias and the variance. Numerical examples and simulations are used to illustrate our proposed evaluation criteria. Related analyses are also provided.	cohesion (computer science);computer performance;estimation theory;focal (programming language);mean squared error;monte carlo;numerical linear algebra;numerical method;simulation;trusted computer system evaluation criteria	Deqiang Han;Jean Dezert;Yi Yang	2014	17th International Conference on Information Fusion (FUSION)		econometrics;pattern recognition;mathematics;statistics	SE	26.780132012142904	-19.07384362399678	67278
ef12d5b5ff566fa18e15f8c6864a78402bd15818	fuzzy heaping mechanism for heaped count data with imprecision		In genetic association studies, the traits of interest may sometimes be collected from the reported data. Since subjects report exact responses and/or rounded responses, the histogram of data frequently exhibits spikes at particular values. This phenomenon, known as heaping, can cause difficulties in performing the association test via standard modeling approaches. Recently, several models have been proposed to identify the true unobservable underlying distribution from heaped data. However, all of these methods depend on probabilistic assumptions regarding the heaping mechanism. Unfortunately, probabilistic models cannot represent heaped data effectively, because heaping can be caused by imprecisely reported values. This type of imprecision is different from probabilistic uncertainty, which is described well by a probabilistic model. In this paper, we propose a fuzzy heaping model to identify genetic variants for the heaped count data. Our fuzzy model uses a mixture of likelihood functions for precisely and imprecisely reported data, treating heaped data as imprecise data represented by fuzzy sets. Moreover, since reported count data may include excess zeros, as well as heaped data, we extend our fuzzy heaping model to handle excess zeros. Through simulation studies, we show that the proposed fuzzy heaping model controls type I errors effectively and has great power to identify causal variants. We illustrate the proposed fuzzy heaping model through a study of the identification of genetic variants associated with the number of cigarettes smoked per day.	count data	Hye-Young Jung;Heawon Choi;Taesung Park	2018	Soft Comput.	10.1007/s00500-017-2641-4	fuzzy logic;econometrics;count data;statistics;fuzzy set;probabilistic logic;statistical model;phenomenon;histogram;mathematics	NLP	26.59846173963847	-22.180613092724972	67440
aebfcd5749f3c2b668ead060867411874ba153cc	monitoring the shape parameter of a weibull regression model in phase ii processes	profile monitoring;extreme value model;control charts;weibull model;relative log likelihood ratio	In this paper, the interest is focused on monitoring profiles with Weibull distributed-response and common shape parameter in phase II processes. The monitoring of such profiles is completely possible by taking the natural logarithm of the Weibull-distributed response. This is equivalent to characterize the correspondent process by an extreme value linear regression model with common scale parameter D 1. It was found out that from the monitoring of the common log-scale parameter of the extreme value linear regression model, with the help of a simple scheme, it can be obtained important information about the deterioration of the entire process assuming the ˇ coefficients as nuissance parameters that do not have to be known but stable. Control charts are based on the relative log-likelihood ratio statistic defined for the log-scale parameter of the log-transformation of the Weibull-distributed response and its respective signed square root. It was also found out that some existing adjustments are needed in order to improve the accuracy of using the distributional properties of the monitoring statistics for relatively small and moderate sample sizes. Simulation studies suggest that resulting charts have appealing properties and work fairly acceptable when non-large enough samples are available at discrete sampling moments. Detection abilities of the studied corrected control schemes improve when sample size increases. Copyright © 2014 John Wiley & Sons, Ltd.	chart;coefficient;in-phase and quadrature components;john d. wiley;maxima and minima;sampling (signal processing);simulation	Carlos Arturo Panza;José Alberto Vargas	2016	Quality and Reliability Eng. Int.	10.1002/qre.1740	weibull distribution;econometrics;control chart;engineering;operations management;mathematics;statistics	AI	29.01446270098772	-20.50997232720737	67507
6f3830de478f806e5d176ac8fe14984ce2e43831	on the performance of phase i dispersion control charts for process monitoring	signaling probability;dispersion parameter;control charts;parametric non parametric;phase i;normality non normality	Control charts are usually implemented in two phases: the retrospective phase (phase I) and the monitoring phase (phase II). The performance of any phase II control chart structure depends on the preciseness of the control limits obtained from the phase I analysis. In statistical process control, the performance of phase I dispersion charts has mainly been investigated for normal or contaminated normal distributions of the quality characteristic of interest. Little work has been carried out to investigate the performance of a wide range of possible phase I dispersion charts for processes following non-normal distributions. The current study deals with the proper choice of a control chart for the evaluation of process dispersion in phase I. We have analyzed the performance of a wide range of dispersion control charts, including two distribution-free structures. The performance of the control charts is evaluated in terms of probability to signal, under normal and non-normal process setups. These results will be useful for quality control practitioners in their selection of a phase I control chart. Copyright © 2014 John Wiley & Sons, Ltd.	chart;in-phase and quadrature components;john d. wiley	Saddam Akber Abbasi;Muhammad Riaz;Arden Miller;Shabbir Ahmad	2015	Quality and Reliability Eng. Int.	10.1002/qre.1703	reliability engineering;econometrics;control chart;engineering;operations management;statistics	Arch	28.305983556010858	-19.478271425974672	67613
8281b4b73bfe9c7325aeaad667415af27ac651f2	a novel accurate source number estimation method based on gbsa-mdl algorithm		Several classical source number estimation methods have been proposed in the past based on information theoretic criteria such as minimum description length (MDL). However, in most known real applications there is a scenario in which the number of sensors goes to infinity at the same speed as the number of snapshots (general asymptotic case) which yields to a blind performance for the classical MDL and results in an inaccurate source number estimation. Accordingly, in this work, the Galaxy Based Search Algorithm (GBSA) is modified and applied with the MDL criteria in order to optimize and correct the detection of source number under such sample-starving case. Simulation results show that the proposed GBSA-MDL based method gives reliable results compared to several used source number estimation methods.	algorithm;mdl (programming language)	Taha Bouras;Di He;Fei Wen;Peilin Liu;Wenxian Yu	2017		10.1007/978-3-319-78139-6_39	computer science;artificial intelligence;pattern recognition	EDA	24.75659595248468	-23.17381451689182	67759
1339a5e9c0baaa7377ce3bf9b4b458c0829dc241	nonlinear interpolation of multivariable functions by the monte carlo method	random sampling;higher order;monte carlo method;monte carlo	The nonlinear interpolation of functions of very many variables is discussed. Deterministic termwise assessment of a prohibitively large number of terms naturally leads to a choice of random sampling from these numerous terms. After introduction of an appropriate higher order interpolation formula, a working algorithm is established by the Monte Carlo method. Numerical examples are also given.	algorithm;interpolation;monte carlo method;nonlinear system;numerical method;sampling (signal processing)	Takao Tsuda;Kozo Ichida	1970	J. ACM	10.1145/321592.321595	quantum monte carlo;monte carlo method in statistical physics;quasi-monte carlo method;econometrics;mathematical optimization;diffusion monte carlo;dynamic monte carlo method;hybrid monte carlo;particle filter;markov chain monte carlo;interpolation;slice sampling;monte carlo molecular modeling;mathematics;kinetic monte carlo;rejection sampling;parallel tempering;monte carlo integration;monte carlo algorithm;statistics;monte carlo method;monte carlo method for photon transport	Graphics	32.20274834566365	-16.213861710378616	68329
746bd9df350b472a76e7199a0263eb95feb706c4	non-parametric information-theoretic measures of one-dimensional distribution functions from continuous time series	distribution function;kullback leibler;anomaly detection;kolmogorov smirnov;probability distribution function;functional form	We study non-parametric measures for the problem of comparing distributions, which arise in anomaly detection for continuous time series. Non-parametric measures take two distributions as input and produce two numbers as output: the difference between the input distributions and the statistical significance of this difference. Some of these measures, such as Kullback-Leibler measure, are defined for comparing probability distribution functions (PDFs) and some others, such as Kolmogorov-Smirnov measure, are for cumulative distribution functions (CDFs). We first show how to adapt the PDF based measures to compare CDFs, resulting in a total of 23 CDF based measures. We then provide a unified functional form that subsumes all these measures. We present our methodology to determine the significance (of the measures) by simulations only. Finally, we evaluate these measures for the anomaly detection in continuous time series.	anomaly detection;higher-order function;kullback–leibler divergence;portable document format;simulation;theory;time series	Paolo D'Alberto;Ali Dasdan	2009		10.1137/1.9781611972795.59	pattern recognition;inverse-gamma distribution;noncentral chi-squared distribution;artificial intelligence;probability distribution;inverse-chi-squared distribution;statistics;reciprocal distribution;kolmogorov–smirnov test;normal-gamma distribution;beta prime distribution;mathematics	ML	31.84331867233039	-20.27761606242321	68351
49645458165e7d78c194de5d97a0dd959d65ce62	fast simulation of rare events in queueing and reliability models	likelihood ratio;asymptotic optimality;unbiased estimator;reliability modeling;discrete time;jackson network;rare event;buffer overflow;waiting time;probability distribution;tree structure;queueing system;importance sampling;asynchronous transfer mode	This paper surveys efficient techniques for estimating, via simulation, the probabilities of certain rare events in queueing and reliability models. The rare events of interest are long waiting times or buffer overflows in queueing systems, and system failure events in reliability models of highly dependable computing systems. The general approach to speeding up such simulations is to accelerate the occurrence of the rare events by using importance sampling. In importance sampling, the system is simulated using a new set of input probability distributions, and unbiased estimates are recovered by multiplying the simulation output by a likelihood ratio. Our focus is on describing asymptotically optimal importance sampling techniques. Using asymptotically optimal importance sampling, only a fixed number of samples are required to get accurate estimates, no matter how rare the event of interest is. In practice, this means that the required run lengths can be reduced by many orders of magnitude, compared to standard simulation. The queueing systems studied include simple queues (e.g., GI/GI/1) and discrete time queues with multiple autocorrelated arrival processes that arise in the analysis of Asynchronous Transfer Mode communications switches. References for results on Jackson networks and and tree structured networks of ATM switches are given. Both Markovian and non-Markovian reliability models are treated.	rare events;simulation	Philip Heidelberger	1993		10.1007/BFb0013853	jackson network;probability distribution;econometrics;mathematical optimization;discrete time and continuous time;buffer overflow;likelihood-ratio test;importance sampling;computer science;asynchronous transfer mode;layered queueing network;tree structure;bias of an estimator;statistics	Metrics	30.159386251740344	-15.525679606282107	68558
3414aa44d0b609c958ff5d05fda8269cfc91fde4	relevance vector machines based modelling and optimisation for collaborative control parameter design: a case study	non linear effect;bayes estimation;modelizacion;control optimo;economies d energie;gas;design optimisation;control optimo matematicas;ahorros energia;ga;methode noyau;etude experimentale;control parameter design;cooperation;agricultural equipment;collaborative control parameters design;efecto no lineal;aprendizaje probabilidades;probabilistic approach;algoritmo genetico;cooperacion;optimal control;modelisation;relevance vector machines;controle optimal;estimacion bayes;control proceso;materiel agricole;analisis regresion;equipo agricola;epandeuse;rvm;enfoque probabilista;approche probabiliste;commande optimale;metodo nucleo;relevance vector machine;process control;algorithme genetique;optimal control mathematics;energy savings;analyse regression;apprentissage probabilites;kernel method;genetic algorithm;fertiliser spreaders;genetic algorithms;regression analysis;effet non lineaire;collaborative design;modeling;estudio experimental;commande processus;esparcidora;probability learning;spreader;estimation bayes;fertiliser spreader	A new collaborative control parameter design strategy is proposed for economic plant control process. The relevance vector machines (RVMs) and genetic algorithms (GAs) are combined to generate the optimal control index table for controllers. More specifically, the probabilistic model based on RVMs is utilised to describe the non-linear behaviours according to the experimental dataset. The evolution-based optimisation model based on GAs is used for collaborative design of the optimum control parameter combinations. A variable-rate fertilising system is presented as an application case for collaborative generation of control index table with the combined accuracy, energy saving and fertilising-consistency optimisation objectives. The experimental results show the effectiveness of the proposed hybrid approach.	genetic algorithm;mathematical optimization;nonlinear system;optimal control;relevance;sampling (signal processing);software release life cycle;statistical model	Jin Yuan;Chengliang Liu;Xuan F. Zha	2009	IJCAT	10.1504/IJCAT.2009.028043	genetic algorithm;computer science;engineering;artificial intelligence;machine learning;process control;operations research	Robotics	31.493746213924126	-10.963893843020367	68701
b0bab36f4524d1747801b7f301157e8bc3775be9	nonparametric age replacement: bootstrap confidence intervals for the optimal cost	fiabilidad;reliability;problema reemplazo;envejecimiento;bootstrap;estrategia optima;intervalo confianza;simulation;estimation non parametrique;replacement problem;renewal theory;simulacion;probleme remplacement;algorithme;optimal strategy;algorithm;non parametric estimation;replacement renewal confidence interval for the optimal cost in age replacement;confidence interval;theorie renouvellement;fiabilite;intervalle confiance;ageing;statistics;vieillissement;estimacion no parametrica;estimation confidence interval for the optimal cost in age replacement;strategie optimale;algoritmo;teoria renovacion	Bootstrap confidence intervals for the actual cost of using a given nonparametric estimate of the optimal age replacement strategy are shown to have the claimed coverage probability. A numerical algorithm is given to obtain these confidence intervals in practice. The small sample behavior of these confidence intervals is illustrated by simulations. Finally, comparisons are made with the confidence interval obtained from asymptotic normal theory. We show that the bootstrap confidence interval is the one to use in age replacement problems.		Christian Léger;Robert Cléroux	1992	Operations Research	10.1287/opre.40.6.1062	ageing;renewal theory;econometrics;confidence interval;artificial intelligence;confidence distribution;confidence region;reliability;mathematics;confidence and prediction bands;cdf-based nonparametric confidence interval;credible interval;robust confidence intervals;statistics	Theory	32.39012464155529	-21.623088201372298	68723
f9ec9904d06c35569bf050166f3838bf9d75eb92	detecting an interaction between treatment and a continuous covariate: a comparison of two approaches	probabilidad error;62f40;numerical stability;efficacite traitement;metodo polinomial;treatment effect;effet traitement;ucl;bootstrap;51e24;analisis datos;loi probabilite;ley probabilidad;treatment efficiency;interaction;type i error;estabilidad numerica;type 1 error;continuous variable;simulation;discovery;simulacion;clinical trial;theses;plan randomise;conference proceedings;error type i;polynomial;continuous covariates;stability;algorithme;algorithm;data analysis;ensayo clinico;plan aleatorizado;digital web resources;estimation erreur;model building;randomized trial;polynomial method;error estimation;ucl discovery;randomized design;polinomio;probability distribution;open access;statistical computation;estimacion error;calculo estadistico;eficacia tratamiento;covariate;analyse donnee;ucl library;calcul statistique;covariable;error probability;interaccion;stabilite numerique;essai clinique;book chapters;open access repository;fractional polynomials;erreur type i;methode polynomiale;polynome;62p10;probabilite erreur;sliding window;algoritmo;ucl research;clinical trials	In clinical trials, there is considerable interest in investigating whether a treatment effect is similar in all patients, or that some prognostic variable indicates a differential response to treatment. To examine this, a continuous predictor is usually categorized into groups according to one or more cutpoints. The treatment/covariate interaction is then analyzed in factorial fashion using multiplicative terms. The use of cutpoints raises several difficult issues for the analyst. It is preferable to keep continuous variables continuous in such a model. To achieve this, the MFP algorithm for multivariable model-building with fractional polynomials was recently extended to a new algorithm called multivariable fractional polynomial interaction (MFPI). With the latter, covariates may be binary, categorical or continuous, and cutpoints are avoided. MFPI is compared with a graphical technique, the subpopulation treatment-effect pattern plot or subpopulation treatment effect pattern plot (STEPP). Differences between MFPI and STEPP are illustrated by re-analysis of a randomized trial in kidney cancer. The stability of the two procedures is investigated by using the bootstrap. The Type I error probability of MFPI to 'detect' spurious interactions is estimated by simulation. MFPI and STEPP are found to exhibit similar treatment/covariate interactions. The tail-oriented variant of STEPP is found to give more stable and interpretable results than the sliding window variant. The type 1 error probabilty of MFPI is found to be close to its nominal value.	sensor	Willi Sauerbrei;Patrick Royston;Karina Zapien Arreola	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.12.041	econometrics;type i and type ii errors;clinical trial;mathematics;algorithm;statistics	ML	32.251682950430514	-21.87104663180935	69222
021de3425def4b8e153a1c8eaa7d934be7dcad5d	multiple break detection in the correlation structure of random variables	multiple change point detection;correlations;binary segmentation;cusum statistics;financial returns	Correlations between random variables play an important role in applications, e.g. in financial analysis. More precisely, accurate estimates of the correlation between financial returns are crucial in portfolio management. In particular, in periods of financial crisis, extreme movements in asset prices are found to be more highly correlated than small movements. It is precisely under these conditions that investors are extremely concerned about changes on correlations. A binary segmentation procedure to detect the number and position of multiple change points in the correlation structure of random variables is proposed. The procedure assumes that expectations and variances are constant and that there are sudden shifts in the correlations. It is shown analytically that the proposed algorithm asymptotically gives the correct number of change points and the change points are consistently estimated. It is also shown by simulation studies and by an empirical application that the algorithm yields reasonable results. © 2013 Elsevier B.V. All rights reserved.		Pedro Galeano;Dominik Wied	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.02.031	econometrics;mathematics;correlation function;statistics	AI	27.4766478926681	-18.80671678035295	69286
58471dd413b4824c9bc03151cb935a75625fe469	evolving controllers for real robots: a survey of the literature	dk atira pure researchoutput researchoutputtypes contributiontojournal article;lifelong adaptation by evolution;mobile robot;training;simulation;genetics;evolutionary robotics;physical robots;evaluation criteria;survey methods;evolutionary computing	For many years, researchers in the field of mobile robotics have been investigating the use of genetic and evolutionary computation (GEC) to aid the development of mobile robot controllers. Alongside the fundamental choices of the GEC mechanism and its operators, which apply to both simulated and physical evolutionary robotics, other issues have emerged which are specific to the application of GEC to physical mobile robotics. This article presents a survey of recent methods in GEC-developed mobile robot controllers, focusing on those methods that include a physical robot at some point in the learning loop. It simultaneously relates each of these methods to a framework of two orthogonal issues: the use of a simulated and/or a physical robot, and the use of finite, training phase evolution prior to a task and/or lifelong adaptation by evolution during a task. A list of evaluation criteria are presented and each of the surveyed methods are compared to them. Analyses of the framework and evaluation criteria suggest several possibilities; however, there appear to be particular advantages in combining simulated, training phase evolution (TPE) with lifelong adaptation by evolution (LAE) on a physical robot.	common criteria;ethernet over twisted pair;evolutionary algorithm;evolutionary computation;evolutionary robotics;least absolute deviations;mobile robot	Joanne H. Walker;Simon M. Garrett;Myra S. Wilson	2003	Adaptive Behaviour	10.1177/1059712303113003	mobile robot;simulation;computer science;artificial intelligence;evolutionary robotics;survey methodology;evolutionary computation	Robotics	25.573269650359187	-13.184553836797313	69485
68ade78161a439e8d0d967a033ea390b1c145932	new models in durability tool-testing: pseudo-weibull distribution		"""The problem of tool durability is considered to be a most important factor in mechanical engineering. In the last three decades various models for tool life have been proposed. Among these models the statistical ones play a major role, since almost all tool characteristics are regarded as random variables. Classical distribution functions — as for instance log-normal, exponential or Weibull ones — have proved their utility in this matter, but the large variety of tools themselves have imposed the need to find other distributions to describe the behaviour of tool life. Two Soviet engineers (Druzhinin [1] and Katzev [2]) have introduced the socalled """"Alpha"""" distribution, initially to describe merely the time for performing a given operation. A detailed statistical analysis of the Alpha model has been perfor­ med by the present author in [3]. In the present paper we shall derive a new distribution function which has proved its utility in durability tool-testing."""	durability (database systems);random graph;time complexity	Viorel Gh. Voda	1989	Kybernetika		mathematics;mathematical optimization;weibull distribution;durability	ML	31.556965677110036	-18.358661711465228	69538
6602a06036069d352cdf1af3b2530d3fa13b91d5	assessment of fault-detection processes: an approach based on reliability techniques	developpement logiciel;ley poisson;exponential distribution;estimacion imperfeccion;maximum likelihood;porcentaje falla;stopping rule;estimation defaut;maximum vraisemblance;process assessment;moyenne temps bon fonctionnement;software fault tolerance;taux defaillance;tiempo medio buen funcionamiento;maximum likelihood estimation;mean time between failures;reliability theory;defect valuation;regla parada;detection defaut;desarrollo logicial;fault detection;hazard rate;software development;estimacion parametro;failure rate;reliability theory software fault tolerance parameter estimation exponential distribution poisson distribution maximum likelihood estimation;unconditional distribution parameter estimation fault detection processes reliability techniques software artifact software development exponential distribution hazard rate poisson distribution maximum likelihood;fiabilite logiciel;parameter estimation;estimation parametre;fiabilidad logicial;loi poisson;programming switches fault detection inspection testing software reliability maximum likelihood detection maximum likelihood estimation software quality software measurement;software reliability;deteccion imperfeccion;maxima verosimilitud;poisson distribution;regle arret;defect detection	Two major factors influence the number of faults uncovered by a fault-detection process applied to a software artifact (e.g., specification, code): ability of the process to uncover faults, quality of the artifact (number of existing faults). These two factors must be assessed separately, so that one can: switch to a different process if the one being used is not effective enough, or stop the process if the number of remaining faults is acceptable. The fault-detection process assessment model can be applied to all sorts of artifacts produced during software development, and provides measures for both the 'effectiveness of a fault-detection process' and the 'number of existing faults in the artifact'. The model is valid even when there are zero defects in the artifact or the fault-detection process is intrinsically unable to uncover faults. More specifically, the times between fault discoveries are modeled via reliability-based techniques with an exponential distribution. The hazard rate is the product of 'effectiveness of the fault-detection process' and 'number of faults in the artifact'. Based on general hypotheses, the number of faults in an artifact follows a Poisson distribution. The unconditional distribution, whose parameters are estimated via maximum likelihood, is obtained.		Sandro Morasca	1996	IEEE Trans. Reliability	10.1109/24.556586	reliability engineering;econometrics;engineering;mathematics;maximum likelihood;statistics	Embedded	29.482715104600885	-18.202098579231826	69762
03d453e51dafe5da1860f5d977d0ea8f4bc3a6ff	robustness and ambiguity in continuous time	continuous time;hidden markov model;prior distribution;discrete time;decision maker;ambiguity robustness hidden markov model likelihood function entropy statistical detection error smooth ambiguity;likelihood function;state transition	We use statistical detection theory in a continuous-time environment to provide a new perspective on calibrating a concern about robustness or an aversion to ambiguity. A decision maker repeatedly confronts uncertainty about state transition dynamics and a prior distribution over unobserved states or parameters. Two continuous-time formulations are counterparts of two discrete-time recursive specifications of Hansen and Sargent (2007) [16]. One formulation shares features of the smooth ambiguity model of Klibanoff et al. (2005) and (2009) [24,25]. Here our statistical detection calculations guide how to adjust contributions to entropy coming from hidden states as we take a continuous-time limit. © 2011 Elsevier Inc. All rights reserved. JEL classification: C61; D81; D83	detection theory;recursion;risk aversion;state transition table	Lars Peter Hansen;Thomas J. Sargent	2011	J. Economic Theory	10.1016/j.jet.2011.01.004	econometrics;decision-making;discrete time and continuous time;prior probability;mathematics;likelihood function;hidden markov model;statistics	AI	27.387473582622988	-22.82978251297636	70001
5988f4e0f5de62cc992cd2af4502a17b68fd2e0a	improving predictive accuracy of logistic regression model using ranked set samples	62j12;rare characteristics;ranked set sampling;separation of likelihood;62d05;logistic regression model	Improving Predictive Accuracy of Logistic Regression Model Using Ranked Set Samples Mr. Kevin Carl P. Santos M.S. & Prof. Erniel B. Barrios Ph.D. To cite this article: Mr. Kevin Carl P. Santos M.S. & Prof. Erniel B. Barrios Ph.D. (2015): Improving Predictive Accuracy of Logistic Regression Model Using Ranked Set Samples, Communications in Statistics Simulation and Computation, DOI: 10.1080/03610918.2014.955113 To link to this article: http://dx.doi.org/10.1080/03610918.2014.955113	communications in statistics – simulation and computation;logistic regression	Kevin Carl P. Santos;Erniel B. Barrios	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.955113	econometrics;data mining;mathematics;logistic regression;statistics	ML	28.29341740554018	-22.617322239184627	70232
2a59bd6ed631bc3b675cb5383880ad52db87b130	a fragile invertible watermarking technique for the authentication of medical images	patient entry date;watermarking;archiving system;medical image retrieval;communication system;medical images watermarking dct;image coding;medical images;radiologist;authorisation;image restoration fragile invertible watermarking technique medical image authentication archiving system communication system radiologist public network decimal digit number patient entry date file id signature extraction erasable embedding process medical image retrieval frequency domain watermarking method;biomedical imaging;medical image processing authorisation feature extraction image coding image restoration image retrieval image watermarking;image restoration;artificial neural networks;medical image;discrete cosine transforms;cryptography;feature extraction;medical image processing;fragile invertible watermarking technique;frequency domain watermarking method;signature extraction;head;image watermarking;medical image authentication;frequency domain;public network;digital number;file id;erasable embedding process;decimal digit number;dct;biomedical imaging head watermarking artificial neural networks discrete cosine transforms cryptography image restoration;image retrieval	This paper deals with a fragile watermarking technique for the authentication of medical images. Medical images are stored in archiving and communication systems that are accessed by the radiologists for diagnosis. The proposed method insure the integrity of the medical image data that is being transferred over public network. Any modification to the watermarked image can be detected using the proposed watermarking method. A unique 14 decimal digits number (signature) representing patient's entry date and file ID embedded inside the patient's medical image in an imperceptible way without increasing the data size that need to be transferred. This signature can be extracted at the radiologist viewer work stations and used for the authentication while the modified data is restored back to original if the image found to be authentic. This kind of erasable embedding process would ensure medical image retrieval without any modification to the image data after the authentication process. The proposed frequency domain watermarking method go along with the unique need of medical images for diagnosis.	algorithm;archive;authentication;digital watermarking;embedded system;image retrieval;medical imaging;radiology;requirement;topography;watermark (data file)	Ahmed Al-Gindy	2010	The 10th IEEE International Symposium on Signal Processing and Information Technology	10.1109/ISSPIT.2010.5711776	image restoration;computer vision;feature extraction;image retrieval;computer science;cryptography;theoretical computer science;machine learning;multimedia;head;frequency domain;artificial neural network	EDA	38.19512083341669	-10.897970588227826	70250
1e67dbcb2490cdd0c94bcc0903ab62026f4f0a41	accelerated degradation test planning using the inverse gaussian process	stress analysis gaussian processes life testing relaxation;accelerated degradation test planning stress relaxation data random effects ig process model adt planning optimal constant stress accelerated degradation tests degradation analysis ig process models inverse gaussian process;random effects accelerated degradation test lower quantile inverse gaussian process;degradation stress planning life estimation reliability data models time measurement	The IG process models have been shown to be an important family in degradation analysis. In this paper, we are interested in optimal constant-stress accelerated degradation tests (ADTs) planning when the underlying degradation follows the inverse Gaussian (IG) process. We first consider ADT planning for the IG process without random effects. Asymptotic variance of the estimate of a lower quantile is derived, and the objective of the planning is to minimize this variance by properly choosing the testing stresses, and the number of samples allocated to each stress. Next, ADT planning for a random-effects IG process model is considered. We then applied the IG process to fit the stress relaxation data of a component, and use the developed methods to help with the optimal ADT design.	automated planning and scheduling;elegant degradation;gaussian process;linear programming relaxation;process modeling;random effects model;stress ball;test plan	Zhi-Sheng Ye;Liangpeng Chen;Loon Ching Tang;Min Xie	2014	IEEE Transactions on Reliability	10.1109/TR.2014.2315773	econometrics;mathematical optimization;engineering;statistics	Robotics	30.60382936754402	-18.776709327129	70360
cd3cd9d5f2eebc71031b53a5964695abda39d1d9	a linear bayesian stochastic approximation to update project duration estimates	bayes estimation;modelizacion;dependencia dato;belief networks;metodo pert;reseau croyance;factor riesgo;linear estimation;approximation lineaire;data dependency;risk factor;real time;estimacion lineal;modele lineaire;linear approximation;modelo lineal;probabilistic approach;stochastic approximation linear bayesian method bayesian belief network project duration estimation;facteur risque;bayesian method;modelisation;reseau bayes;estimation lineaire;risk factors;estimacion bayes;red bayes;chemin critique;enfoque probabilista;approche probabiliste;critical path;bayesian belief network;temps reel;stochastic approximation;complecion;linear model;approximation stochastique;aproximacion lineal;prediction accuracy;bayes network;dependance donnee;bayesian estimator;tiempo real;completitud;aproximacion estocastica;completeness;linear bayesian method;completude;project duration estimation;modeling;completion;recorrido critico;methode pert;estimation bayes;pert	By relaxing the unrealistic assumption of probabilistic independence on activity durations in a project, this paper develops a hierarchical linear Bayesian estimation model. Statistical dependence is established between activity duration and the amount of resource, as well as between the amount of resource and the risk factor. Upon observation or assessment of the amount of resource required for an activity in near completion, the posterior expectation and variance of the risk factor can be directly obtained in the Bayesian scheme. Then, the expected amount of resources required for and the expected duration of upcoming activities can be predicted. We simulate an application project in which the proposed model tracks the varying critical path activities on a real time basis, and updates the expected project duration throughout the entire project. In the analysis, the proposed model improves the prediction accuracy by 38.36% compared to the basic PERT approach.	stochastic approximation	Sungbin Cho	2009	European Journal of Operational Research	10.1016/j.ejor.2008.04.019	stochastic approximation;econometrics;artificial intelligence;bayesian network;mathematics;risk factor;statistics	Robotics	25.56613190379373	-19.62662870503984	70516
145587c4e4194caf1fc7c233eeb96f557f74a9bb	optimization of noisy computer experiments with tunable precision	sequential design;expected improvement;optimal method;stochastic simulation;stochastic models kriging noise precision accuracy and precision design of experiments doe statistical experimental design sed nuclear sequential methods;grupo de excelencia;510 mathematics;computer experiment;ciencias basicas y experimentales;stochastic simulators;matematicas;statistics;kriging	This article addresses the issue of kriging-based optimization of stochastic simulators. Many of these simulators depend on factors that tune the level of precision of the response, the gain in accuracy being at a price of computational time. The contribution of this work is two-fold: firstly, we propose a quantile-based criterion for the sequential design of experiments, in the fashion of the classical Expected Improvement criterion, which allows an elegant treatment of heterogeneous response precisions. Secondly, we present a procedure for the allocation of the computational time given to each measurement, allowing a better distribution of the computational effort and increased efficiency. Finally, the optimization method is applied to an original application in nuclear criticality safety. This article has supplementary material online.		Victor Picheny;David Ginsbourger;Yann Richet;Gregory Caplin	2011	Technometrics	10.1080/00401706.2012.707580	econometrics;computer experiment;sequential analysis;stochastic simulation;mathematics;kriging;statistics	ML	29.017371763218524	-16.38828764746541	70744
f007a4b9504138ef73e0f953681bd9b260d1062a	using path control variates in activity network simulation	interval estimation;exponential distribution;point estimation;active network;confidence interval;control variates;critical path;stochastic activity networks;technical report;variance reduction;covariance matrix	In the simulation of a stochastic activity network (SAN), the usual objective is to obtain point and confidence-interval estimators of the mean completion time for the network. This paper presents a new procedure for using path control variates to improve the efficiency of such estimators. Because each path control is the duration of an associated path in the network, the vector of selected path controls has both a known mean and a known covariance matrix. All of this information is incorporated into point- and interval-estimation procedures for both normal and nonnormal responses. To evaluate the performance of these procedures experimentally, we compare actual versus predicted reductions in point-estimator variance and confidence-interval half-length for a set of SANs in which the following characteristics are systematically varied: (a) the size of the network (number of nodes and activities); (b) the topology of the network; (c) the relative dominance (criticality index) of the critical path; and (d) the percentage of activities with exponentially distributed durations. The experimental results indicate that large variance reductions can be achieved with these estimation procedures in a wide variety of networks.	control variates;critical path method;emoticon;estimation theory;experiment;self-organized criticality;simulation;stochastic process;variance reduction	Sekhar Venkatraman;James R. Wilson	1985		10.1145/21850.253156	exponential distribution;econometrics;active networking;covariance matrix;mathematical optimization;antithetic variates;interval estimation;confidence interval;technical report;critical path method;point estimation;mathematics;statistics;variance reduction;control variates	ML	29.221481061074073	-16.856334954238132	71017
4ab345e43282a68debe3aa937ecf0a88b833e52e	synthetic charts to control bivariate processes with autocorrelated data	vertical align middle title view the mathml source src sd grey_pxl gif data inlimgeid 1 s2 0 s0360835216301085 si1 gif x;bivariate process;synthetic;x ź charts;side sensitive;img class imglazyjsb inlineimage height 13 width 15 alt view the mathml source style margin top 5px;steady state	We consider the monitoring of bivariate processes with autocorrelated data.The use of two side-sensitive X ź charts compete well with the synthetic T2 chart.The Markov chain approach was used to obtain the steady-state properties.The synthetic rules always improve the performance of the control charts. In this study, we propose the use of simultaneous X ź charts to control bivariate processes with autocorrelated data. The first set of X ź charts is side-sensitive with regard to the same variable (SV X ź charts) and the second one is side-sensitive with regard to both variables (BV X ź charts). The Markov chain approach was used to obtain the steady-state properties of the X ź charts. In comparison with the standard synthetic T2 chart, the SV and the BV charts signal faster in a wide variety of disturbances, except when the variables are high correlated. The BV charts are simpler and signal faster than the SV charts.	autocorrelation;bivariate data;chart;synthetic intelligence	Felipe Domingues Simões;Roberto Campos Leoni;Marcela Aparecida Guerreiro Machado;Antonio Fernando Branco Costa	2016	Computers & Industrial Engineering	10.1016/j.cie.2016.04.005	econometrics;computer science;operations management;steady state;statistics	SE	27.006471055112424	-20.75231719187251	71234
39c50947d78da0d35753d2444a44875728ede830	selection of model discrepancy priors in bayesian calibration	bayesian calibration;model uncertainty;validation;identifiability	In the Kennedy and O'Hagan framework for Bayesian calibration of physics models, selection of an appropriate prior form for the model discrepancy function is a challenging issue due to the lack of physics knowledge regarding model inadequacy. Aiming to address the uncertainty arising from the selection of a particular prior, this paper first conducts a study on possible formulations of the model discrepancy function. A first-order Taylor series expansion-based method is developed to investigate the potential redundancy caused by adding a discrepancy function to the original physics model. Further, we propose a three-step (calibration, validation, and combination) approach in order to inform the decision on the construction of model discrepancy priors. In the validation step, a reliability-based metric is used to evaluate posterior model predictions in the validation domain. The validation metric serves as a quantitative measure of how well the discrepancy formulation captures the missing physics in the model. In the combination step, the posterior distributions of model parameters and discrepancy corresponding to different priors are combined into a single distribution based on the probabilistic weights derived from the validation step. The combined distribution acknowledges the uncertainty in the prior formulation of model discrepancy function.	discrepancy function	You Ling;Joshua Mullins;Sankaran Mahadevan	2014	J. Comput. Physics	10.1016/j.jcp.2014.08.005	discrepancy function;econometrics;mathematical optimization;identifiability;mathematics;statistics	Theory	27.664563353633845	-17.672097283770647	71452
225897664a552e5e21fd185d3c55f169bb5f8d4b	block bootstrap estimation of the distribution of cumulative outdoor degradation	normal distribution;bepress selected works;periodic dependent time series;time series;probability of failure;confidence interval;periodic dependent time series normal distribution central limit theorem;central limit theorem;block bootstrap;simulation study;cumulant	An interesting prediction problem involving degradation of materials exposed to outdoor environments (weathering) is the estimation of the distribution of future cumulative degradation using smallto moderatesize degradation data sets. This distribution, which arises as a result of the uncertainty/variability in the weather, can be expressed mathematically as the distribution of the sum of a periodic dependent time series, and is approximately normal by the Central Limit Theorem. The estimation of this distribution is thus equivalent to estimating the mean and the variance of the distribution. We propose a block-bootstrap-based approach to estimate the distribution, and a novel technique to estimate the variance of the distribution. An example involving the degradation of a solar reflector material is provided. We also present the results of a simulation study, which will show that in particular the proposed variance estimator is superior to the sample variance in estimating the variance of the distribution.	approximation algorithm;elegant degradation;simulation;spatial variability;time series	Victor Chan;Soumendra Nath Lahiri;William Q. Meeker	2004	Technometrics	10.1198/004017004000000266	normal distribution;half-normal distribution;empirical distribution function;econometrics;mathematical optimization;q-function;confidence interval;normal-gamma distribution;central limit theorem;student's t-distribution;time series;inverse-chi-squared distribution;log-normal distribution;mathematics;variance-gamma distribution;compound probability distribution;asymptotic distribution;chi-squared distribution;infinite divisibility;statistics;cumulant;three-point estimation;distribution fitting	ML	29.104633387022663	-20.9022133237734	71478
6d56e62a7a654c92b4879724c008810aaafebde7	design of a control chart using a modified ewma statistic	normal distribution;control chart;average run length;ewma statistic	In this paper, the design of a control chart is given using a modified exponentially weighted moving average statistic under the assumption that the quality characteristic of interest follows the normal distribution. The structure of the proposed control chart is developed, and the necessary measures are derived to find the average run length for in-control and out-of-control processes. The efficiency of the proposed chart is compared with two existing control charts in terms of the average run length. The results are explained with the help of industrial example. Copyright © 2016 John Wiley u0026 Sons, Ltd.		Nasrullah Khan;Aslam Muhammad;Chi-Hyuck Jun	2017	Quality and Reliability Eng. Int.	10.1002/qre.2102	normal distribution;ewma chart;reliability engineering;control chart;control limits;operations management;mathematics;x-bar chart;statistics	HCI	28.275291738819806	-19.29160453351443	71645
6923bebf401f1f8edb72d15dc469fdc20c541136	algorithm 808: arfit - a matlab package for the estimation of parameters and eigenmodes of multivariate autoregressive models	model identification;ar model;oscillations;eigenmodes;computacion informatica;high dimensionality;confidence intervals;grupo de excelencia;time series;autoregressive model;multivariate time series;confidence interval;least squares;ciencias basicas y experimentales;principal oscillation pattern;matematicas;least square;time series data;dynamic characteristic;parameter estimation;matlab;order selection	ARfit is a collection of Matlab modules for modeling and analyzing multivariate time series with autoregressive (AR) models. ARfit contains modules to given time series data, for analyzing eigen modes of a fitted model, and for simulating AR processes. ARfit estimates the parameters of AR models from given time series data with a stepwise least squares algorithm that is computationally efficient, in particular when the data are high-dimensional. ARfit modules construct approximate confidence intervals for the estimated parameters and compute statistics with which the adequacy of a fitted model can be assessed. Dynamical characteristics of the modeled time series can be examined by means of a decomposition of a fitted AR model into eigenmodes and associated oscillation periods, damping times, and excitations. The ARfit module that performs the eigendecomposition of a fitted model also constructs approximate confidence intervals for the eigenmodes and their oscillation periods and damping times.	algorithmic efficiency;approximation algorithm;autoregressive model;dynamical system;eigen (c++ library);least squares;matlab;normal mode;simulation;stepwise regression;time series	Tapio Schneider;Arnold Neumaier	2001	ACM Trans. Math. Softw.	10.1145/382043.382316	econometrics;confidence interval;machine learning;time series;star model;mathematics;autoregressive model;least squares;statistics	ML	35.031267741941555	-22.02433761081887	71911
55b4c67acff8cf92668245506cc2b452faea8505	optimal allocation and splitting among designs in rare event simulation	operations research;dissertation	OPTIMAL ALLOCATION AND SPLITTING AMONG DESIGNS IN RARE EVENT SIMULATION Ben W. Crain, Ph.D. George Mason University, 2013 Dissertation Director: Prof. John F. Shortle This dissertation develops efficient algorithms, in theory and in implementation, for selecting, via simulation, the best design, or system, from a set of designs, where “best” is the design with the smallest probability of some (generally undesirable) outcome. Compared to standard techniques these algorithms improve the efficiency of simulation when the (undesirable) outcomes have very small probabilities, on the order of 1.0E-6, or smaller. Such outcomes are “rare events”. The algorithms could also be used to estimate non-rare probabilities, although, in that case, their advantages over techniques not geared toward rare events diminish. The designs in question differ in construction, or in the values of their parameters, but are such that their operations can be simulated as stochastic processes which terminate in a non-rare event (a set of non-rare outcomes), or a rare event (a set of rare outcomes). The task is to estimate, efficiently, the probabilities of the rare events, in order to select the design with the smallest one. Efficient algorithms are those which produce estimators of the rare event probabilities with acceptable variances, within acceptable computational times. I do not define “acceptable variances”. The focus is on algorithms which achieve better results, compared to standard methods, for a given amount of computational time (a given computing budget constraint). Better results are achieved by (approximately) maximizing the Probability of Correct Selection (PCS) of an algorithm, subject to a computing budget constraint. PCS is the probability that the algorithm will correctly identify the best design. Simulation algorithms against which the new algorithms are compared include simple Monte Carlo (MC), Optimal Computing Budget Allocation (OCBA), fixed-effort Splitting, and the Optimal Splitting Technique for Rare-Event Simulation (OSTRE). These algorithms are reviewed, prior to the development of the two new algorithms: Single Optimization and OCBA+OSTRE. The major contribution of this dissertation is the theoretical development and practical implementation of these new algorithms. The mathematical equivalence of these two new methods (in the sense that they attain, in theory, the same maximum PCS) is proven, and their computational complexities are compared. Numerical testing illustrates that they can out-perform standard techniques, and suggests that OCBA+OSTRE is better, in practice, than Single Optimization.	algorithm;analysis of algorithms;computation;extreme value theory;mason;mathematical optimization;monte carlo method;optimal computing budget allocation;program optimization;rare events;simulation;stochastic process;terminate (software);time complexity;turing completeness	Ben Crain	2013			simulation;computer science;operations management;management science	Graphics	30.03690182845638	-16.003351562945436	72035
4cf8851eed081032397c51e80219b6b09291aad4	indirect estimation of elliptical stable distributions	loi student;institutional repositories;indirect inference;analyse multivariable;finite sample;analisis numerico;estimacion;multivariate;emerging market;metodo monte carlo;fedora;theorie approximation;multivariate analysis;65c05;analisis datos;stochastic method;fonction repartition;elliptical distribution;ley n variables;62h10;62e17;echantillon fini;methode monte carlo;monte carlo study;elliptic function;distribucion estadistica;analyse numerique;vital;stock markets;60e07;approximation theory;marche valeurs;ley student;funcion distribucion;data analysis;distribution function;numerical analysis;distribution statistique;estimation;monte carlo method;indexation;statistical computation;loi elliptique;calculo estadistico;asymptotic properties;methode stochastique;elliptical;analisis multivariable;multivariate distribution;analyse donnee;calcul statistique;student distribution;vtls;funcion eliptica;stable distribution;loi n variables;high dimension;60e05;statistical distribution;ils;fonction elliptique;stable;metodo estocastico	We present an indirect estimation approach for elliptical stable distributions which relies on the use of a multivariate t distribution as auxiliary model. This distribution is also elliptical and we show that its parameters have a one-to-one relationship with those of the elliptical stable, therefore making the proposed indirect approach especially suitable. Standard asymptotic properties are also shown and we analyze the finite sample behavior of the estimators via a comprehensive Monte Carlo study. An application to 27 emerging markets stock indexes concludes the paper.	ergodicity;gradient;monte carlo method;one-to-one (data model);simulation;stationary process;time series;via c3	Marco J. Lombardi;David Veredas	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.04.035	econometrics;elliptical distribution;stable distribution;calculus;mathematics;statistics	ML	33.205754618564725	-22.277137634590233	72076
9743131484f6cfe01352bbb2c6a7f10bdd1c2a72	estimation of survival and capture probabilities in open population capture-recapture models when covariates are subject to measurement error	simex;mark capture recapture;error in variables;little penguins;cormack jolly seber model;regression calibration	Predictor variables (or covariates) are frequently used in a capture–recapture analysis when estimating demographic quantities such as population size or survival probabilities. If these predictor variables are measured with error and subsequently used in the analysis, then estimates of the model parameters may be biased. Several approaches have been proposed to account for error-in-variables in capture–recapture models, however these methods generally assume the population is closed; hence quantities of interest for open populations such as the survival probabilities do not appear in the likelihood. To account for measurement error in environmental time-varying covariates for open population capture–recapture data, the well-known Cormack–Jolly–Seber model and two statistical methods are considered: (1) simulation–extrapolation; and (2) regression calibration, as well as a new method which accounts for correlation (arising from measurement error) between the survival and capture probabilities. Several simulation studies are conducted to examine the method performances, and a case study is presented which uses capture–recapture data on the Little Penguin Eudyptulaminor and sea-surface temperature data as an environmental covariate to model their survival and capture probabilities. © 2015 Elsevier B.V. All rights reserved.	extrapolation;kerrison predictor;mark and recapture;performance;population;simulation;whole earth 'lectronic link	Jakub Stoklosa;Peter Dann;Richard M. Huggins;Wen-Han Hwang	2016	Computational Statistics & Data Analysis	10.1016/j.csda.2015.10.010	econometrics;demography;computer science;statistics	AI	27.09483397570177	-22.010249792192248	72107
2083537343e9a7e3793c7519f6bb111863b48912	nonparametric independence tests: space partitioning and kernel approaches	empirical distribution;limit distribution;ucl;discovery;theses;conference proceedings;multi dimensional;digital web resources;strong consistency;ucl discovery;open access;random variable;ucl library;book chapters;open access repository;large deviation;tests of independence;ucl research	Three simple and explicit procedures for testing the independence of two multi-dimensional random variables are described. Two of the associated test statistics (L1, log-likelihood) are defined when the empirical distribution of the variables is restricted to finite partitions. A third test statistic is defined as a kernel-based independence measure. All tests reject the null hypothesis of independence if the test statistics become large. The large deviation and limit distribution properties of all three test statistics are given. Following from these results, distributionfree strong consistent tests of independence are derived, as are asymptotically α-level tests. The performance of the tests is evaluated experimentally on benchmark data. Consider a sample of R ×Rd′-valued random vectors (X1, Y1), . . . , (Xn, Yn) with independent and identically distributed (i.i.d.) pairs defined on the same probability space. The distribution of (X, Y ) is denoted by ν, while μ1 and μ2 stand for the distributions of X and Y , respectively. We are interested in testing the null hypothesis that X and Y are independent, H0 : ν = μ1 × μ2, while making minimal assumptions regarding the distribution. We consider two main approaches to independence testing. The first is to partition the underlying space, and to evaluate the test statistic on the resulting discrete empirical measures. Consistency of the test must then be verified as the partition is refined for increasing sample size. Previous multivariate hypothesis tests in this framework, using the L1 divergence measure, include homogeneity tests (to determine whether two random variables have the same distribution, by Biau and Györfi [1]); and goodness-of-fit tests (for whether a random variable has a particular distribution, by Györfi and van der Meulen [2], and Beirlant et al. [3]). The log-likelihood has also been employed on discretised spaces as a statistic for goodness-of-fit testing [4]. We provide generalizations of both the L1 and loglikelihood based tests to the problem of testing independence, representing to our knowledge the first application of these techniques to independence testing. 2 Gretton and Györfi We obtain two kinds of tests for each statistic: strong consistent tests based on large deviation bounds, which make no assumptions about the distribution; and tests based on the asymptotic distribution of the test statistic, which assume only that the distribution is nonatomic. We also present a conjecture regarding the form taken by an asymptotic test based on the Pearson χ statistic, using the goodness-of-fit results in [4] (further related test statistics include the power divergence family of Read and Cressie [6], although we do not study them here). The advantage of our test procedures is that, besides being explicit and easy to carry out, they require very few assumptions on the partition sequences, are consistent, and have distribution-independent thresholds. Our second approach to independence testing is kernel-based. In this case, our test statistic has a number of different interpretations: as an L2 distance between Parzen window estimates [7], as a smoothed difference between empirical characteristic functions [8, 9], or as the Hilbert-Schmidt norm of a crosscovariance operator mapping between functions of the random variables [10, 11]. Each test differs from the others regarding the conditions required of the kernels: the Parzen window statistic requires the kernel bandwidth to decrease with increasing sample size, and has a different limiting distribution to the remaining two statistics; while the Hilbert-Schmidt approach uses a fixed bandwidth, and can be thought of as a generalization of the characteristic function-based test. We provide two new results: a strong consistent test of independence based on a tighter large deviation bound than that in [10], and an empirical comparison of the limiting distributions of the kernel-based statistic. Additional independence testing approaches also exist in the statistics literature. For d = d = 1, an early nonparametric test for independence, due to Hoeffding, Blum, Kiefer, and Rosenblatt [12, 13], is based on the notion of differences between the joint distribution function and the product of the marginals. The associated independence test is consistent under appropriate assumptions. Two difficulties arise when using this statistic in a test, however. First, quantiles of the null distribution are difficult to estimate. Second, and more importantly, the quality of the empirical distribution function estimates becomes poor as the dimensionality of the spaces R and R ′ increases, which limits the utility of the statistic in a multivariate setting. Further approaches to independence testing can be used when particular assumptions are made on the form of the distributions, for instance that they should exhibit symmetry. We do not address these approaches in the present study. The paper is organized as follows. Section 1 describes the large deviation and limit distribution properties of the L1-test statistic. The large deviation result is used to formulate a distribution-free strong consistent test of independence, which rejects the null hypothesis if the test statistic becomes large. The limit distribution is used in an asymptotically α-level test, which is consistent when the distribution is nonatomic. Both a distribution-free strong consistent test 3 A strong consistent test means that both on H0 and on its complement the test makes a.s. no error after a random sample size. This concept is close to the definition of discernability introduced by Dembo and Peres [5]. See [1] for further discussion. Nonparametric Independence Tests 3 and an asymptotically α-level test are presented for the log-likelihood statistic in Section 2, and a conjecture for an asymptotically α-level test based on the Pearson χ statistic is described in Section 3. Section 4 contains a review of kernel-based independence statistics, and the associated hypothesis tests for both the fixed-bandwidth and variable-bandwidth cases. Finally, a numerical comparison between the tests is given in Section 5. More detailed proofs and further discussion may be found in an associated technical report [14]. 1 L1-based statistic Denote by νn, μn,1 and μn,2 the empirical measures associated with the samples (X1, Y1), . . . , (Xn, Yn), X1, . . . , Xn, and Y1, . . . , Yn, respectively, so that νn(A × B) = n#{i : (Xi, Yi) ∈ A × B, i = 1, . . . , n}, μn,1(A) = n #{i : Xi ∈ A, i = 1, . . . , n}, and μn,2(B) = n #{i : Yi ∈ B, i = 1, . . . , n}, for any Borel subsets A and B. Given the finite partitions Pn = {An,1, . . . , An,mn} of R and Qn = {Bn,1, . . . , Bn,m′n} of R ′ , we define the L1 test statistic comparing νn and μn,1 × μn,2 as Ln(νn, μn,1 × μn,2) = ∑	benchmark (computing);blum axioms;characteristic function (convex analysis);characterization test;cross-covariance;discretization;experiment;frank rosenblatt;kernel density estimation;numerical analysis;peres–horodecki criterion;schmidt decomposition;simplicial complex;smoothing;space partitioning;taxicab geometry;usb;window function	Arthur Gretton;László Györfi	2008		10.1007/978-3-540-87987-9_18	empirical distribution function;random variable;econometrics;chi-square test;data mining;mathematics;statistics;strong consistency	ML	31.141034423578393	-22.22947786742911	72368
884ddae7729a10ec40e45c9494f422d7411ea86f	does a gibbs sampler approach to spatial poisson regression models outperform a single site mh sampler?	autocorrelacion;count data;association statistique;sampler;metropolis algorithm;seguro;metodo estadistico;analyse multivariable;data augmentation;chaine markov;statistical simulation;cadena markov;metodo monte carlo;finance;multivariate analysis;analisis datos;gibbs sampler;melange loi probabilite;echantillonnage gibbs;gibbs sampling;melangeage;algorithme metropolis hastings;metropolis hastings;62h20;modele lineaire;methode monte carlo;statistical association;mixed distribution;regression model;statistical method;modelo lineal;statistical regression;62jxx;data analysis;modelo regresion;asociacion estadistica;simulacion estadistica;assurance;sciences actuarielles;heterogeneidad;methode statistique;poisson regression model;algorithme metropolis;algoritmo metropolis hastings;simulation statistique;regresion estadistica;modele regression;monte carlo method;37a25;statistical computation;linear model;calculo estadistico;modele simulation;spatial effect;mezcla ley probabilidad;simulation study;analisis multivariable;analyse donnee;muestreador;calcul statistique;62p05;modelo simulacion;mixing;regression statistique;muestreo gibbs;echantillonneur;mezclado;simulation model;ddc 510;metropolis hastings algorithm;insurance companies;insurance;finanzas;heterogeneity;heterogeneite;autocorrelation;algoritmo metropolis;markov chain;sonderforschungsbereich 386	A Gibbs sampler for a Poisson regression model including spatial effects is presented and evaluated. The approach is based on that a Poisson regression model can be transformed into an approximate normal linear model by data augmentation using the introduction of two sequences of latent variables. It is shown how this methodology can be extended to spatial Poisson regression models and details of the resulting Gibbs sampler are given. In particular, the influence of model parameterisation and different update strategies on the mixing of the MCMC chains is discussed. The developed Gibbs samplers are analysed in two simulation studies and applied to model the expected number of claims for policyholders of a German car insurance company. The mixing of the Gibbs samplers depends crucially on the model parameterisation and the update schemes. The best mixing is achieved when collapsed algorithms are used, reasonable low autocorrelations for the spatial effects are obtained in this case. For the regression effects however, autocorrelations are rather high, especially for data with very low heterogeneity. For comparison a single component Metropolis–Hastings algorithms is applied which displays very good mixing for all components. Although the Metropolis–Hastings sampler requires a higher computational effort, it outperforms the Gibbs samplers which would have to be run considerably longer in order to obtain the same precision of the parameters. c © 2008 Elsevier B.V. All rights reserved.	approximation algorithm;autocorrelation;convolutional neural network;gibbs sampling;latent variable;linear model;markov chain monte carlo;metropolis;metropolis–hastings algorithm;modified huffman coding;poisson regression;sampling (signal processing);simulation	Susanne Gschlößl;Claudia Czado	2008	Computational Statistics & Data Analysis	10.1016/j.csda.2008.02.020	metropolis–hastings algorithm;econometrics;gibbs sampling;mathematics;statistics	AI	33.56030004770288	-22.747843149220667	72527
dc821edf09641d9101d64a3022c892525d47e49f	modeling the dependent competing risks with multiple degradation processes and random shock using time-varying copulas	frechet hoeffding bounds multiple degradation process time varying copulas s dependent competing risk model random shocks dependence structure time scaled covariate factor copula method fatal shock degradation rate acceleration system reliability estimation distribution bounds;system reliability;time varying;time scale;degradation;reliability;electric shock;covariance analysis;fatal shocks;random variables;joints;reliability theory;acceleration;copula method;multiple degradation;competing risks model;multiple degradation copula method dependent competing risks fatal shocks;random processes;random variable;mathematical model;dependence structure;dependent competing risks;kendall s tau;reliability theory covariance analysis random processes;competing risks;degradation electric shock reliability joints mathematical model random variables acceleration	We develop s-dependent competing risk model for systems subject to multiple degradation processes and random shocks using time-varying copulas. The proposed model allows for a more flexible dependence structure between risks in which (a) the dependent relationship between random shocks and degradation processes is modulated by a time-scaled covariate factor, and (b) the dependent relationship among various degradation processes is fitted using the copula method. Two types of random shocks are considered in the model: fatal shocks, which fails the system immediately; and nonfatal shocks, which does not. In a nonfatal shock situation there are two impacts towards the degradation processes: sudden increment jumps, and degradation rate accelerations. The comparison results of the system reliability estimation from both constant and time-varying copulas are illustrated in the numerical examples to demonstrate the application of the proposed model. The modified joint distribution bounds in terms of Kendall's tau and Spearman's rho provide an improvement to Frechet-Hoeffding bounds for estimating the possible system reliability range.	akaike information criterion;bayesian information criterion;elegant degradation;embedded system;emoticon;estimation theory;financial risk modeling;kendall tau distance;modulation;numerical analysis;numerical partial differential equations	Yaping Wang;Hoang Pham	2012	IEEE Transactions on Reliability	10.1109/TR.2011.2170253	random variable;stochastic process;econometrics;engineering;mathematics;forensic engineering;statistics	ML	31.058406994560155	-18.801726642789184	72545
5dc1f8b6b25b658d5e6038151b8a8e42e2e29aa3	comparison of the andersen-gill model with poisson and negative binomial regression on recurrent event data	survival function;poisson process;metodo estadistico;estimator robustness;intervalo confianza;analisis datos;loi probabilite;ley probabilidad;taux erreur;negative binomial distribution;type i error;biometrie;62f25;estimation non parametrique;biometrics;sobrevivencia;biometria;donnee censuree;independent increment;clinical trial;statistical method;loi binomiale negative;error type i;medical science;statistical regression;62jxx;standard error;funcion sobrevivencia;estimacion insesgada;estimation parametrique;non parametric estimation;increment independant;probabilite couverture;data analysis;ensayo clinico;confidence interval;prevention;62n02;robustez estimador;poisson regression;estimation erreur;fonction survie;censored data;ciencia medica;coverage probability;62g15;heterogeneidad;error estimation;methode statistique;ley binomial negativa;negative binomial regression;regresion estadistica;probability distribution;intervalle confiance;statistical computation;estimacion error;calculo estadistico;recurrent events;62n99;error rate;survie;analyse donnee;proceso poisson;calcul statistique;62nxx;essai clinique;estimacion no parametrica;unbiased estimation;parameter estimation;estimation statistique;survival;60g51;regression statistique;erreur type i;indice error;estimacion estadistica;statistical estimation;proportional hazard;62p10;estimation sans biais;heterogeneity;processus poisson;heterogeneite;otitis media;science medicale;prevencion;robustesse estimateur	Many generalizations of the Cox proportional hazard method have been elaborated to analyse recurrent event data. The Andersen–Gill model was proposed to handle event data following Poisson processes. This method is compared with non-survival approaches, such as Poisson and negative binomial regression. The comparison is performed on data simulated according to various event-generating processes and differing in subject heterogeneity. When robust standard error estimates are applied, for Poisson processes the Andersen–Gill approach is comparable to a negative binomial regression, whereas the poisson regression has comparable coverage probabilities of confidence intervals, but increased type I error rates; however, none of the methods can generate unbiased parameter estimates with data violating the independent increment assumption. These findings are illustrated by data from a clinical trial of the efficacy of a new pneumococcal vaccine for prevention of otitis media. © 2008 Elsevier B.V. All rights reserved.	poisson regression	Antje Jahn-Eimermacher	2008	Computational Statistics & Data Analysis	10.1016/j.csda.2008.04.009	econometrics;poisson binomial distribution;preventive healthcare;calculus;count data;zero-inflated model;mathematics;poisson regression;poisson distribution;negative binomial distribution;statistics	ML	32.903074024484006	-22.368088444758733	72772
b8fdaa819a500b734cc2d1b7646001318148efee	to blank or not to blank? a comparison of the effects of disclosure limitation methods on nonlinear regression estimates	selection model;contraste;statistical disclosure limitation;mascara;eleccion;model based reasoning;recorte prensa;sample selection;errors in variables in nonlinear models;raisonnement base sur modele;regression non lineaire;metodo monte carlo;recoleccion dato;data gathering;data collection;non linear regression;statistical databases;methode monte carlo;econometria;non linear model;regresion no lineal;modele non lineaire;decoupage presse;reduction donnee;semi parametric selection;base donnee statistique;modelo no lineal;data privacy;blanking;monte carlo method;selectividad;selectivity;reduccion datos;etalonnage;modele donnee;inproceedings;data reduction;econometrics;masque;data quality;disclosure limitation;parameter estimation;errors in variables;selectivite;monte carlo;choix;collecte donnee;confidentialite donnee;mask;semi parametric selection models;calibration;choice;econometrie;nonlinear regression;data models;data generation process;nonlinear model;empirical research	Statistical disclosure limitation is widely used by data collecting institutions to provide safe individual data. However, the choice of the disclosure limitation method severely affects the quality of the data. In particular, estimators for nonlinear models based on data which are masked by standard disclosure limitation techniques such as blanking or noise addition lead to inconsistent parameter estimates. This paper investigates to what extent appropriate econometric techniques can obtain parameter estimates of the true data generating process, if the data are masked by noise addition or blanking. Comparing three different estimators – calibration method, the SIMEX method and a semiparametric sample selectivity estimator – we produce Monte-Carlo evidence on how the reduction of data quality by masking can be minimized. JEL classification: C21, J24, J31	data quality;estimation theory;maxima and minima;monte carlo method;newton's method;nonlinear system;selectivity (electronic);semiparametric model;structural equation modeling;truncation	Sandra Lechner;Winfried Pohlmeier	2004		10.1007/978-3-540-25955-8_15	econometrics;information privacy;computer science;artificial intelligence;mathematics;nonlinear regression;statistics;monte carlo method;data collection	DB	32.61600328675341	-23.408385017355293	72906
453fdde4f6e5ecd666b549f39c5da9d99ae8eb03	image encryption using binary key-images	bit plane;image encryption;plaintext attack image encryption key image bit plane edge map brute force attack chipertext attack;cryptography videos pixel security image edge detection usa councils protection streaming media logic multimedia systems;image coding;edge map;secure communication binary key images bit plane edge map lossless image encryption algorithm brute force attack ciphertext attack plaintext attack multimedia application security;multimedia application;secure communication;mobile phone;multimedia computing;cryptography;brute force attack;plaintext attack;chipertext attack;real time application;key image;multimedia computing cryptography image coding	This paper introduces a new concept for image encryption using a binary “key-image”. The key-image is either a bit plane or an edge map generated from another image, which has the same size as the original image to be encrypted. In addition, we introduce two new lossless image encryption algorithms using this key-image technique. The performance of these algorithms is discussed against common attacks such as the brute force attack, ciphertext attacks and plaintext attacks. The analysis and experimental results show that the proposed algorithms can fully encrypt all types of images. This makes them suitable for securing multimedia applications and shows they have the potential to be used to secure communications in a variety of wired/wireless scenarios and real-time application such as mobile phone services.	algorithm;bit plane;brute-force attack;brute-force search;cipher;ciphertext;cryptanalysis;distortion;edge detection;encryption;key space (cryptography);lossless compression;mobile phone;plaintext;real-time computing;real-time transcription;secure communication;security token;sensor	Yicong Zhou;Karen Panetta;Sos S. Agaian	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5346780	multiple encryption;secure communication;bit plane;40-bit encryption;plaintext-aware encryption;computer science;cryptography;theoretical computer science;symmetric-key algorithm;link encryption;filesystem-level encryption;on-the-fly encryption;internet privacy;malleability;deterministic encryption;computer security;encryption;probabilistic encryption;ciphertext;56-bit encryption;attribute-based encryption	Vision	38.06570553747177	-11.20573529457085	73092
63a49f70a9c12d61351302b985f10008a7a46737	a review and some new results on permutation testing for multivariate problems	finite sample consistency;robust testing;permutation tests;tests for survival functions;nonparametric combination;multi sided tests	In recent years permutation testing methods have increased both in number of applications and in solving complex multivariate problems. When available permutation tests are essentially of an exact nonparametric nature in a conditional context, where conditioning is on the pooled observed data set which is often a set of sufficient statistics in the null hypothesis. Whereas, the reference null distribution of most parametric tests is only known asymptotically. Thus, for most sample sizes of practical interest, the possible lack of efficiency of permutation solutions may be compensated by the lack of approximation of parametric counterparts. There are many complex multivariate problems, quite common in empirical sciences, which are difficult to solve outside the conditional framework and in particular outside the method of nonparametric combination (NPC) of dependent permutation tests. In this paper we review such a method and its main properties along with some new results in experimental and observational situations (robust testing, multi-sided alternatives and testing for survival functions).		Fortunato Pesarin;Luigi Salmaso	2012	Statistics and Computing	10.1007/s11222-011-9261-0	econometrics;combinatorics;mathematics;statistics	Theory	28.26840618289666	-23.447381283676094	73330
11ee7780494404ce80b6a57605651af8f05779c5	a novel framework for robust lossless data hiding	histogram;jpeg2000;robust data hiding;lossless data hiding	Histogram-based lossless data hiding (LDH) has been recognized as an effective and efficient way in the field of data hiding. Recently, a LDH method using the statistical quantity histogram (SQH) has been a hotspot for its good performance, which can extract secret correctly even if stego-image has some extent distortion. However, this method has some shortcoming which limits its application in practice. For this purpose, we develop a novel framework, which consists of the following three components:1) In order to overcome the shortcomings of traditional SQH, proposing a pre-processing method to construct a suitable carrier for information embedding; 2)Applying LDH method to restore the bitmap information ,which will be used for reversibly recovering the host image; 3) Presenting a robust lossless hiding algorithm, which guarantees host images can be recovered losslessly in the case of stego-image remains intact, on the contrary, the secret information can also be resistant to a certain degree of JPEG2000 attacks. Thorough experiments over different kinds of images demonstrate the effectiveness of the proposed framework.	lossless compression	Li-hong Zhu;Quan Zhou	2013	JCP	10.4304/jcp.8.11.2809-2815	computer science;theoretical computer science;data mining;jpeg 2000;histogram;algorithm;statistics	Vision	39.02520467830019	-11.842373271270713	73355
6190964f2a69bae1ba8b905c44c2beb89cf9f4e7	ranking and selection with covariates		We consider a new ranking and selection problem in which the performance of each alternative depends on some observable random covariates. The best alternative is thus not constant but depends on the values of the covariates. Assuming a linear model that relates the mean performance of an alternative and the covariates, we design selection procedures producing policies that represent the best alternative as a function in the covariates. We prove that the selection procedures can provide certain statistical guarantee, which is defined via a nontrivial generalization of the concept of probability of correct selection that is widely used in the conventional ranking and selection setting.	experiment;linear model;numerical analysis;observable;selection algorithm	Haihui Shen;L. Jeff Hong;Xiaowei Zhang	2017	2017 Winter Simulation Conference (WSC)	10.1109/WSC.2017.8247946	simulation;statistics;linear model;computer science;observable;ranking;covariate	ML	29.725726798150756	-22.215387328725598	73371
b62e1f73878e5ede0a0023a9d2775122afff2d11	the additive weibull-geometric distribution: theory and applications	maximum likelihood;additive weibull distribution;geometric distribution;moments	In this paper, we introduce a new class of lifetime distributions which is called the additive Weibull geometric (AWG) distribution. This distribution obtained by compounding the additive Weibull and geometric distributions. The new distribution has a number of well-known lifetime special sub-models such as modified Weibull geometric, Weibull geometric, exponential geometric, among several others. Some structural properties of the proposed new distribution are discussed. We propose the method of maximum likelihood for estimating the model parameters and obtain the observed information matrix. A real data set is used to illustrate the importance and flexibility of the new distribution.	additive model;formation matrix;geometric median;observed information;time complexity;utility functions on indivisible goods	I. Elbatal;Mohamed M. Mansour;Mohammad Ahsanullah	2016	JSTA	10.2991/jsta.2016.15.2.3	gumbel distribution;half-normal distribution;exponential distribution;noncentral chi-squared distribution;weibull distribution;beta-binomial distribution;econometrics;geometric distribution;posterior predictive distribution;weibull modulus;exponentiated weibull distribution;univariate distribution;log-cauchy distribution;inverse-chi-squared distribution;log-normal distribution;ratio distribution;compound probability distribution;maximum spacing estimation;uniform distribution;statistics;distribution fitting	Theory	31.088186519805596	-20.83051103858418	73495
67106712cdac6e95b3442fb29b60711516993894	bayesian reliability estimation for deteriorating systems with limited samples using the maximum entropy approach	deteriorating system;fuzzy numbers;bayesian inference;maximum entropy method;small sample sizes	In this paper the combinations of maximum entropy method and Bayesian inference for reliability assessment of deteriorating system is proposed. Due to various uncertainties, less data and incomplete information, system parameters usually cannot be determined precisely. These uncertainty parameters can be modeled by fuzzy sets theory and the Bayesian inference which have been proved to be useful for deteriorating systems under small sample sizes. The maximum entropy approach can be used to calculate the maximum entropy density function of uncertainty parameters more accurately for it does not need any additional information and assumptions. Finally, two optimization models are presented which can be used to determine the lower and upper bounds of systems probability of failure under vague environment conditions. Two numerical examples are investigated to demonstrate the proposed method.	elegant degradation;fuzzy number;fuzzy set;heart rate variability;mathematical optimization;numerical analysis;numerical method;principle of maximum entropy;vagueness	Ning-Cong Xiao;Yanfeng Li;Zhonglai Wang;Weiwen Peng;Hong-Zhong Huang	2013	Entropy	10.3390/e15125492	econometrics;mathematical optimization;binary entropy function;maximum entropy probability distribution;principle of maximum entropy;fuzzy number;mathematics;bayesian inference;maximum entropy spectral estimation;statistics	ML	30.37723315293051	-18.91824710176131	73560
6e7b13e2806c1934b952b04afc32f34c0720c5c2	on the selection of the bandwidth parameter for the k-chart	one class classification;support vector data description;process monitoring	The k-chart, based on support vector data description, has received recent attention in the literature. We review four different methods for choosing the bandwidth parameter, s, when the k-chart is designed using the Gaussian kernel. We provide results of extensive Phase I and Phase II simulation studies varying the method of choosing the bandwidth parameter along with the size and distribution of sample data. In very limited cases, the k-chart performed as desired. In general, we are unable to recommend the k-chart for use in a Phase I or Phase II process monitoring study in its current form. Copyright © 2017 John Wiley u0026 Sons, Ltd.		Maria L. Weese;Waldyn G. Martinez;L. Allison Jones-Farmer	2017	Quality and Reliability Eng. Int.	10.1002/qre.2123	econometrics;gaussian function;support vector machine;statistics;one-class classification;chart;computer science;bandwidth (signal processing)	DB	28.12723587982158	-20.331522426171105	73909
461c0bd0e45ff61e61e825e50bcde507e538f6e6	reliability tests for weibull distribution with varying shape-parameter, based on complete data	interval estimation;sample size;reliability;spl chi sup 2 approximation weibull distribution varying shape parameter complete data reliability tests lifetime distribution production lot acceptance criterion consumer risks producer risks acceptable mttf rejectable mttf interval estimates rejectable distribution acceptable distribution wilson hilferty approximation;risk management weibull distribution reliability testing failure analysis;risk management;mean time to failure;testing;indexing terms;satisfiability;failure analysis;weibull distribution;indexation;shape parameter;weibull distribution shape maximum likelihood estimation production industrial engineering standards publication fitting life testing exponential distribution	The Weibull distribution indexed by scale and shape parameters is generally used as a distribution of lifetime. In determining whether or not a production lot is accepted, one wants the most effective sample size and the acceptance criterion for the specified producer and consumer risks. (/spl mu//sub 0/ /spl equiv/ acceptable MTTF; /spl mu//sub 1/ /spl equiv/ rejectable MTTF). Decide on the most effective reliability test satisfying both constraints: Pr{reject a lot | MTTF = /spl mu//sub 0/} /spl les/ /spl alpha/, Pr{accept a lot | MTTF = /spl mu//sub 1/} /spl les/ /spl beta/. /spl alpha/, /spl beta/ are the specified producer, consumer risks. Most reliability tests for assuring MTTF in the Weibull distribution assume that the shape parameter is a known constant. Thus such a reliability test for assuring MTTF in Weibull distribution is concerned only with the scale parameter. However, this paper assumes that there can be a difference between the shape parameter in the acceptable distribution and in the rejectable distribution, and that both the shape parameters are respectively specified as interval estimates. This paper proposes a procedure for designing the most effective reliability test, considering the specified producer and consumer risks for assuring MTTF when the shape parameters do not necessarily coincide with the acceptable distribution and the rejectable distribution, and are specified with the range. This paper assumes that /spl alpha/ < 0.5 and /spl beta/ < 0.5. This paper confirms that the procedure for designing the reliability test proposed here applies is practical.		Koji Hisada;Ikuo Arizino	2002	IEEE Trans. Reliability	10.1109/TR.2002.801845	sample size determination;reliability engineering;weibull distribution;econometrics;failure analysis;mean time between failures;interval estimation;index term;risk management;engineering;reliability;mathematics;software testing;shape parameter;statistics;satisfiability	Embedded	30.026418324404517	-18.43390667575444	74296
eca07afddb4e3373aafd5ce5a78d9e7b75b6a39f	a bayesian analysis of moving average processes with time-varying parameters	bayes estimation;time average;moving average process;forecasting;processus gauss;moyenne mobile;theorie filtrage;modelo prevision;stochastic process;lme;theorie approximation;analisis datos;gaussian processes;fonction repartition;modelo multiplicativo;62m20;60g15;loi gamma;62e17;time varying parameter;modele multiplicatif;estimacion promedio;prior distribution;promedio temporal;time series;distribucion estadistica;prix;ley a priori;bayesian method;analyse bayesienne;forecast model;estimation parametrique;multiplicative model;approximation theory;funcion distribucion;ley gama;data analysis;estimacion bayes;distribution function;london metal exchange;moving average;posterior distribution;forecasting theory;prediction theory;distribution statistique;loi beta;ley beta;62f15;beta distribution;statistical computation;calculo estadistico;promedio movil;processus stochastique;62e10;processus gaussien;ley a posteriori;theorie prevision;moving average processes;multiple model;gamma distribution;analyse donnee;processus moyenne mobile;calcul statistique;gaussian process;mean estimation;estimation statistique;proceso estocastico;theorie prediction;locally stationary processes;estimation moyenne;proceso gauss;estimacion estadistica;bayesian models;60e05;price;bayesian analysis;loi a posteriori;statistical estimation;statistical distribution;locally stationary process;filtering theory;precio;loi a priori;moyenne temporelle;estimation bayes;bayesian model;modele prevision;time varying parameters	A new Bayesian method is proposed for estimation and forecasting with Gaussian moving average (MA) processes with time-varying parameters. The focus is placed on MA models of order one, but a general result is given for an MA process of an arbitrary known order. A multiplicative model for the evolution of the squares of the parameters is introduced following Bayesian conjugacy through beta and truncated gamma distributions and a discount factor. Two new distributions are proposed providing the prior and posterior distributions of the parameters of the model and the one-step forecast distribution of the process. Several well-known distributional results are extended by replacing the gamma distribution with the truncated gamma distribution. The proposed methodology is illustrated with two examples consisting of simulated data and of aluminium spot prices of the London metal exchange.		Kostas Triantafyllopoulos;Guy P. Nason	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2007.04.001	dirichlet distribution;inverse distribution;stochastic process;econometrics;inverse-gamma distribution;bayesian probability;calculus;gaussian process;mathematics;bayesian linear regression;bayesian hierarchical modeling;generalized gamma distribution;statistics	ML	32.2961892189459	-21.659389858282047	74321
7fe4486b9d961aa62bcf1edcb99db5bf036842ff	mediaeval 2015 drone protect task: privacy protection in surveillance systems using false coloring	benchmarking;video surveillance;privacy protection;false coloring	In this paper, we share our results for privacy protection using false coloring in surveillance systems in the Drone Protect Task. The aim is obscuring sensitive regions that are privacy related without sacrificing intelligibility and pleasantness. The idea in false coloring is transforming the colors of an image using a color palette into a different set of colors in which private information is harder to recognize. The method can be applied globally to the whole frame or to a given region of interest (ROI). The privacy protected output has a pleasant look, and if desired, it can be reversed to obtain a close approximation to the original. Benchmarking evaluations on the Mini-drone dataset show promising results especially for intelligibility and pleasantness criteria.	approximation;color;graph coloring;intelligibility (philosophy);palette (computing);personally identifiable information;privacy;region of interest;unmanned aerial vehicle	Serdar Çiftçi;Pavel Korshunov;Ahmet Oguz Akyüz;Touradj Ebrahimi	2015			engineering;advertising;internet privacy;computer security	AI	37.48397846486134	-13.677702171441853	74510
b42c63dde6a2e25b8d72ffdb2c024334662a12de	accelerated simulation of hybrid biological models with quasi-disjoint deterministic and stochastic subnets		Computational biological models are indispensable tools for in silico hypothesis testing. But with the increasing complexity of biological systems, traditional simulators become inefficient to tackle emerging computational challenges. Hybrid simulation, which combines deterministic and stochastic parts, is a promising direction to deal with such challenges. However, currently existing algorithms of hybrid simulation are impractical for implementing real and complex biological systems. One reason for such limitation is that the performance of hybrid simulation not only relies on the number of stochastic events, but also on the type as well as the efficiency of the deterministic solver. In this paper, a new method is proposed for improving the performance of hybrid simulators by reducing the frequent reinitialisation of the deterministic solver. The proposed approach works well with models that contain a substantial number of stochastic events and higher numbers of continuous variables with limited connections between the deterministic and stochastic regimes. We tested these improvements on a number of case studies and it turns out that, for certain examples, the amended algorithm is ten times faster than the exact method.	simulation;stochastic optimization;subnetwork	Mostafa Herajy;Monika Heiner	2016		10.1007/978-3-319-47151-8_2	statistical hypothesis testing;mathematical optimization;dependency graph;in silico;disjoint sets;computer science;solver	Theory	33.477637581946695	-12.673476819765677	74536
be96276d3906bda48a9a88767a0722299c158ddf	reversible fragile watermarking for locating tampered blocks in 2d vector maps	authentication;fragile watermarking;tamper localization;2d vector map;reversible data hiding	For 2D vector maps, obtaining good tamper localization performance and original content recovery with existing reversible fragile watermarking schemes is a technically challenging problem. Using an improved reversible watermarking method and a fragile watermarking algorithm based on vertex insertion, we propose a reversible fragile watermarking scheme that detects and locates tampered blocks with high accuracy while ensuring recovery of the original content. In particular, we propose dividing the features of the vector map into different blocks, calculating the block authentication watermarks and embedding the watermarks with different watermarking schemes. While the block division ensures superior accuracy of tamper localization, the reversible watermarking method and the fragile watermarking algorithm based on vertex insertion provide recovery of the original content. Experimental results show that the proposed scheme could detect and locate malicious attacks such as vertex/feature modification, vertex/feature addition, and vertex/feature deletion.	algorithm;authentication;digital watermarking;insertion sort;user-generated content;vector map;vertex (graph theory)	Nana Wang;Chaoguang Men	2012	Multimedia Tools and Applications	10.1007/s11042-012-1333-4	computer science;theoretical computer science;authentication;internet privacy;computer security	EDA	38.66264078065016	-11.097283053535497	74551
db2b5721c22546cb564c193cfbe99504f9dfba42	bayesian estimation based on progressively type-ii censored samples from compound rayleigh distribution		This paper considers inference under progressive type-II censoring scheme with a compound Rayleigh failure time distribution. The maximum likelihood (ML) and the Bayes estimators for the two unknown parameters of the compound Rayleigh distribution (CRD) distribution are derived. A Bayesian approach using Markov chain Monte Carlo (MCMC) method to generate from the posterior distributions and in turn computing the Bayes estimators are developed. Point estimation and confidence intervals based on maximum likelihood and bootstrap methods are also proposed. The approximate Bayes estimators have been obtained under the assumptions of informative and non-informative priors. An example with the real data is discussed to illustrate the proposed methods. Finally, we made comparisons between the maximum likelihood and different Bayes estimators using a Monte Carlo simulation study.	approximation algorithm;censoring (statistics);information;markov chain monte carlo;monte carlo method;rayleigh–ritz method;resampling (statistics);simulation	Rashad Mohamed El-Sagheer;Mohammad Ahsanullah	2015	JSTA	10.2991/jsta.2015.14.2.1	marginal likelihood;econometrics;bayesian linear regression;statistics;maximum a posteriori estimation;bayes factor;bayesian hierarchical modeling;bayes estimator;bayesian inference;gibbs sampling;mathematics	ML	30.127265638588277	-22.552589260121156	74681
6cdbef029650ad1a8f5e94e7f0e565eba7cdc34a	a comparative study of memory-type control charts under normal and contaminated normal environments	robust estimators;contaminated environment;efficiency;memory type charts;universiteitsbibliotheek;average run length;diffuse symmetric disturbances;percentiles	Cumulative sum (CUSUM) and exponentially weighted moving average (EWMA) control charts are commonly used to detect small changes in the parameters of production processes. Recently, a new control structure was introduced, named as mixed EWMA–CUSUM control chart, which combined both charts. The current study provides a detailed comparison of these three types of control charts when the process parameters are unknown under normal and contaminated normal environments. Performance measures average run length and different percentiles of run length distribution are used for comparison purposes. We investigate six different location estimators with the structures of the three memory charts and study their robustness properties. Copyright © 2015 John Wiley & Sons, Ltd.	chart;control flow;john d. wiley;optimal control;run-length encoding	Hafiz Zafar Nazir;Nasir Abbas;Muhammad Riaz;Ronald J. M. M. Does	2016	Quality and Reliability Eng. Int.	10.1002/qre.1835	percentile;econometrics;engineering;operations management;mathematics;efficiency;statistics	HCI	28.34503556698963	-19.680956948373566	74747
caf777d8b03372fc40d91dda5c0e5620b4a5db31	a class of models for uncorrelated random variables	random distribution;05e05;metodo estadistico;rho spearman;analyse multivariable;sums of random variables;produit variable aleatoire;farlie gumbel morgenstern;correlacion rango;multivariate analysis;random variable product;coeficiente correlacion;independance;symmetric function;variable aleatoire;fonction repartition;ley n variables;convolution;funcion densidad probabilidad;probability density function;62h20;loi conjointe;funcion simetrica;correlation rang;62h10;fonction symetrique;variable aleatoria;distribution aleatoire;statistical method;convolucion;dependence;ley 2 variables;distribucion estadistica;fonction caracteristique;independence;fonction densite probabilite;funcion distribucion;distribution function;spearman rho;distribution statistique;random variable sum;marginal distribution;independencia;sub independence;methode statistique;convolution dependence farlie gumbel morgenstern kendall s tau mutual information spearman s rho stochastic equivalence sub independence;94a17;random variable;characteristic function;spearman s rho;stochastic equivalence;ley conjunta;ley marginal;mutual information;analisis multivariable;multivariate distribution;bivariate distribution;60e10;tau kendall;estimation statistique;correlation coefficient;kendall s tau;suma variable aleatoria;estimacion estadistica;loi n variables;distribucion aleatoria;60e05;statistical estimation;rank correlation;coefficient correlation;funcion caracteristica;somme variable aleatoire;statistical distribution;loi 2 variables;kendall tau;62b10;loi marginale;62h12;62e15;joint distribution;producto variable aleatoria	Multivariate Analysis. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in We consider the class of multivariate distributions that gives the distribution of the sum of uncorrelated random variables by the product of their marginal distributions. This class is defined by a representation of the assumption of sub-independence, formulated previously in terms of the characteristic function and convolution, as a weaker assumption than independence for derivation of the distribution of the sum of random variables. The new representation is in terms of stochastic equivalence and the class of distributions is referred to as the summable uncorrelated marginals (SUM) distributions. The SUM distributions can be used as models for the joint distribution of uncorrelated random variables, irrespective of the strength of dependence between them. We provide a method for the construction of bivariate SUM distributions through linking any pair of identical symmetric probability density functions. We also give a formula for measuring the strength of dependence of the SUM models. A final result shows that under the condition of positive or negative orthant dependence, the SUM property implies independence.	autocorrelation;bivariate data;broadcast delay;characteristic function (convex analysis);control system;convolution;curve fitting;eset nod32 antivirus;kendall tau distance;marginal model;mutual information;portable document format;return statement;simulation;turing completeness	Nader Ebrahimi;G. G. Hamedani;Ehsan S. Soofi;Hans Volkmer	2010	J. Multivariate Analysis	10.1016/j.jmva.2010.03.011	probability distribution;marginal distribution;inverse distribution;econometrics;kendall tau rank correlation coefficient;illustration of the central limit theorem;stability;convolution of probability distributions;heavy-tailed distribution;sum of normally distributed random variables;statistical parameter;calculus;mathematics;normally distributed and uncorrelated does not imply independent;joint probability distribution;total sum of squares;statistics	Theory	33.16155576509574	-20.494787462757582	74811
e8b724295a51b1c87991ea79ba181972ebceec55	efficient securing of iris image template by using grayscale level incremented/decremented method	plain image;iiris tempalte;image coding;t technology;cipher image;security of data data encapsulation entropy image coding image colour analysis;data encapsulation;qa75 electronic computers computer science;iris recognition entropy cryptography gray scale authentication;image colour analysis;entropy iris image template securing grayscale level incremented method grayscale level decremented method casia dataset image hiding;iris image;entropy;plain image iiris tempalte iris image entropy cipher image;security of data	The proposed techniques in the paper is based on grayscale level incremented/decremented method. In this paper we used CASIA dataset for experimentation. Efficient results were achieved with respect to image hiding validated by entropy. Graphical results are also shown at the end to visualize our results.	grayscale	Maqsood Mahmud;Muhammad Waqar;Abdulrahman Abdulkarim Mirza;Abdul Hanan Bin Abdullah	2012	2012 Ninth International Conference on Information Technology - New Generations	10.1109/ITNG.2012.131	computer vision;entropy;feature detection;u-matrix;computer science;theoretical computer science;standard test image;computer graphics (images)	EDA	38.65918303911912	-10.589071411945103	75017
8e86598b8d1c8a68d6dd71e94d08e626c32111e4	modeling heterogeneity for bivariate survival data by shared gamma frailty regression model		In the analysis of survival data with parametric models, it is well known that the Weibull model is not suitable for modeling survival data where the hazard rate is non-monotonic. For such cases, where hazard rates are bathtub-shaped or unimodal (or hump-shaped), log-logistic, lognormal, Birnbaun-Saunders, and inverse Gaussian models are used for the computational simplicity and popularity among users. When models are inadequate and inappropriate, compound Rayleigh, arctangent, generalized Weibull, and Weibull-Pareto composite models are also used. Out of these models log-logistic (LL) model is frequently used. The log-logistic distribution (LLD) has the advantage of having simple algebraic expressions for its survivor and hazard functions and a closed form for its distribution function. In this paper, we consider gamma distribution as frailty distribution and LLD as baseline distribution for bivariate survival times. The problem of analyzing and estimating parameters of bivariate LLD with shared gamma frailty is of interest and the focus of this paper. We introduce Bayesian estimation procedure using Markov Chain Monte Carlo (MCMC) technique to estimate the parameters involved in the proposed model. We present a simulation study and two real data examples to compute Bayesian estimates of the parameters and their standard errors and then compare the true values of the parameters with the estimated values for different sample sizes. A search of the literature suggests there is currently no work has been done for bivariate log-logistic regression model with shared gamma frailty using Bayesian approach.	baseline (configuration management);bathtub curve;bivariate data;burn-in;censoring (statistics);computation;estimation theory;failure rate;gamma correction;gibbs sampling;interval arithmetic;iteration;ll parser;linear algebra;logistic regression;markov chain monte carlo;mathematical model;monte carlo method;newton's method;parametric polymorphism;pareto efficiency;rayleigh–ritz method;sampling (signal processing);simulation;stationary process	David D. Hanagal;Richa Sharma	2013	MASA	10.3233/MAS-130259	econometrics;statistics	ML	30.407524323792583	-23.009201912518932	75288
75a83d6e461d2ef5284808a9e9dce4ebb7ef9224	list of publications of endre csáki		[1] On two modifications of the Wilcoxon test, Magyar Tud. Akad. Mat. Kutató Int. Közl., 4 (1959), 313–319 (in Hungarian). [2] On some problems connected with the Galton test, Magyar Tud. Akad. Mat. Kutató Int. Közl., 6 (1961), 97–109. (I. Vincze) [3] On the number of intersections in the one-dimensional random walk, Magyar Tud. Akad. Mat. Kutató Int. Közl., 6 (1961), 281–286. [4] Methods of mathematical statistics for evaluating electric breakdown measuring series, Period. Polyt. Electr. Eng., 7 (1963), 9–35. (E. Németh) [5] On some combinatorial relations concerning the symmetric random walk, Acta Sci. Math. (Szeged), 24 (1963), 231–235. (I. Vincze) [6] Two joint distribution laws in the theory of order statistics, Mathematica (Cluj), 5 (1963), 27–37. (I. Vincze) [7] On some distributions connected with the arcsine law, Magyar Tud. Akad. Mat. Kutató Int. Közl., 8 (1963), 281–291. (I. Vincze) [8] Statistical methods of reliability, Hı́radástechnika, 15 (1964), 332–339 (in Hungarian). (A. Balogh, K. Sarkadi) [9] Nonparametric methods, Magyar Tud. Akad. Mat. Kutató Int. Közl., 11 (1965), 188–202. [10] On the statistical problems of life tests of electrical components, Wissl. Zeitschrift der Humboldt Univ., 16 (1967), 6–8. [11] An iterated logarithm law for semimartingales and its application to empirical distribution function, Studia Sci. Math. Hungar., 3 (1968), 287–292. [12] On the number of intersections between two empirical distribution functions, Studies in Mathematical Statistics, Akadémiai Kiadó, Budapest, 1968, 27–32. [13] On the number of intersections and the ballot theorem, Period. Math. Hungar., 2 (1972), 5–13. (G. Tusnády) [14] Some notes on the law of the iterated logarithm for empirical distribution function, Limit Theorems of Probability Theory, Colloquia Math. Soc. J. Bolyai, Vol. 11, North-Holland, Amsterdam, 1975, 47–58. [15] Investigations concerning the empirical distribution function, Magyar Tud. Akad. Mat. Fiz. Oszt. Közl., 22 (1977), 239–327 (in Hungarian). Translation: Selected Transl. Math. Statist. Probab., 15 (1981), 229–317. [16] A lemma on real functions and its application in the nonparametric theory, Proc. Symp. to honour J. Neyman, Warsaw, 1977, 71–80. (I. Vincze) [17] The law of the iterated logarithm for normalized empirical distribution function, Z. Wahrscheinlichkeitstheorie und Verw. Gebiete, 38 (1977), 147–167.	electronic component;iterated function;iteration;math-matic;registered jack;wolfram mathematica		2010	Periodica Mathematica Hungarica	10.1007/s10998-010-3023-1		ML	36.139542415968585	-16.080410534358116	75445
61ff1f429d29a79136147fed1091b09f86e6f39d	a comparison of the hosmer-lemeshow, pigeon-heyse, and tsiatis goodness-of-fit tests for binary logistic regression under two grouping methods	binary logistic regression;tsiatis;goodness of fit;hosmer lemeshow;deciles of risk;62h10;pigeon heyse;partition the covariate space	Algebraic relationships between Hosmer-Lemeshow   HL , Pigeon-Heyse   2 J , and Tsiatis   T goodness-of-fit statistics for binary logistic regression models with continuous covariates were investigated, and their distributional properties and performances studied using simulations. Groups were formed under deciles-of-risk (DOR) and partitionD ow nl oa de d by [ R M IT U ni ve rs ity L ib ra ry ] at 0 1: 48 1 9 M ar ch 2 01 6	horseland;logistic regression;performance;simulation	Jana D. Canary;Leigh Blizzard;Ronald P. Barry;David W. Hosmer;Stephen Quinn	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1017583	hosmer–lemeshow test;econometrics;mathematics;logistic regression;goodness of fit;statistics	ML	31.23484624539024	-22.87235547375989	75692
393c2c6fff06dd8536bcd88336bddb0a1de51ddf	spatial partition-based particle filtering for data assimilation in wildfire spread simulation		This article develops a spatial partition-based particle filtering framework to support data assimilation for large-scale wildfire spread simulation effectively. The developed spatial partition-based particle filtering framework breaks the system state and observation data into smaller spatial regions and then carries out localized particle filtering based on these spatial regions. Particle Filters (PFs) hold great promise to support data assimilation for spatial temporal simulations, such as wildfire spread simulation, to achieve more accurate simulation or prediction results. However, PFs face major challenges to work effectively for complex spatial temporal simulations due to the high-dimensional state space of the simulation models, which typically cover large areas and have a large number of spatially dependent state variables. The developed framework exploits the spatial locality property of system state and observation data and employs the divide-and-conquer principle to reduce state dimension and data complexity. This framework is especially developed for a discrete event cellular space model (the wildfire simulation model), which significantly differs from prior works that use numerical models specified by partial differential equations (PDEs) with continuous variables. Within this framework, a two-level automated spatial partitioning method is presented to provide automated and balanced spatial partitions with fewer boundary sensors. The developed framework is applied to a wildfire spread simulation and achieved improved results compared to using standard PF-based data assimilation methods.	computer simulation;correspondence problem;crowd simulation;data assimilation;experiment;locality of reference;numerical partial differential equations;pf (firewall);particle filter;principle of locality;real-time clock;resampling (statistics);sensor;space partitioning;state space;wildfire modeling	Yuan Long;Xiaolin Hu	2017	ACM Trans. Spatial Algorithms and Systems	10.1145/3099471	artificial intelligence;simulation modeling;machine learning;particle filter;state variable;partial differential equation;state space;partition (number theory);data assimilation;computer science;space partitioning	Graphics	37.793209067058335	-23.612665133823956	75727
c1377b3ab07ca7219274054a907bd6f296f7c208	estimation of pharmacokinetic parameters by orthogonal regression: comparison of four algorithms	measurement error model;estudio cohorte;fonction orthogonale;medicament;regression non lineaire;coefficient of variation;coeficiente variacion;pharmacocinetique;estimation method;etude cohorte;estudio comparativo;root mean square error;non linear regression;simulation;hombre;regresion no lineal;coefficient variation;methode calcul;methodological bias;metodo calculo;etude comparative;accuracy;precision;first order;rate constant;variation coefficient;modelo compartimental;compartmental model;human;comparative study;estimacion parametro;biais methodologique;pharmacokinetics;degree of approximation;orthogonal function;medicamento;orthogonal regression;drug;parameter estimation;estimation parametre;errors in variables;funcion ortogonal;modele compartimental;computing method;cohort study;farmacocinetica;ruta metodologica;homme	The contribution of non-linear orthogonal regression for estimation of individual pharmacokinetic parameters when drug concentrations and sampling times are subject to error was studied. The first objective was to introduce and compare four numerical approaches that involve different degrees of approximation for parameter estimation by orthogonal regression. The second objective was to compare orthogonal with non-orthogonal regression. These evaluations were based on simulated data sets from 300 'subjects', thereby enabling precision and accuracy of parameter estimates to be determined. The pharmacokinetic model was a one-compartment open model with first-order absorption and elimination rates. The inter-individual coefficients of variation (CV) of the pharmacokinetic parameters were in the range 33-100%. Eight measurement-error models for times and concentrations (homo- or heteroscedastic with constant CV) were considered. Accuracy of the four algorithms was very close in almost all instances (typical bias, 1-4%). Precision showed three expected trends: root mean squared error (RMSE) increased when the residual error was larger or the number of observations was smaller, and it was highest for the absorption rate constant and common error variance. Overall, RMSE ranged from 5 to 40%. It was found that the simplest algorithm for othogonal regression performed as well as the more complicated approaches. Errors in sampling time resulted in an increased bias and imprecision in individual parameter estimates (especially for k(a) in our example) and in common error variance when the estimation method did not take into account these errors. In this situation, use of orthogonal regression resulted in smaller bias and better precision.	algorithm;algorithmic efficiency;anatomical compartments;coefficient;computation;estimated;estimation theory;evaluation;excretory function;first-order predicate;ka band;large;mean squared error;multi-compartment model;nonlinear system;numerical analysis;order of approximation;population parameter;regression testing;sample variance;sampling (signal processing);sampling - surgical action;small;total least squares	Michel Tod;Azzedine Aouimer;Olivier Petitjean	2002	Computer methods and programs in biomedicine	10.1016/S0169-2607(00)00148-6	econometrics;calculus;errors-in-variables models;mathematics;accuracy and precision;coefficient of variation;statistics	ML	35.152282007012644	-23.849433054019222	75978
2f5de294e0d4c81bc9b847283424c9453a2595ca	series of randomized complete block experiments with non-normal data	62 07;experimental design;block design;metodo estadistico;analyse multivariable;medio ambiente;60k37;repeated measurement;test statistique;statistical simulation;covariancia;multivariate analysis;analisis datos;05bxx;randomized complete block;social sciences;repeated measures;test estadistico;aproximacion;plan experiencia;statistical test;covariance;statistical method;plan randomise;mesure repetee;plan bloc;symetrie;symmetry;approximation;plan bloque;data analysis;plan aleatorizado;fixed effect model;62h05;simulacion estadistica;62k99;split plot design;plan experience;methode statistique;randomized design;simulation statistique;modele effet mixte;randomized complete block design;environment;statistical computation;calculo estadistico;experimental design multi environment trial multivariate analysis repeated measures split plot design;simulation study;analisis multivariable;analyse donnee;environnement;calcul statistique;random effects model;plant breeding;multi environment trial;simetria;62p12;covariance structure;medida repetida;62k10;62kxx;plan en tiroirs	Randomized complete block designs are common in agricultural and other experiments. In this manuscript, we derive asymptotic procedures as well as finite approximations, for the analysis of data arising from series of such experiments. We do not assume normality of the data, and the within-block covariance structures can be arbitrary (no restriction to compound symmetry). The methods are specifically designed for trials with many environments and few blocks per environment, such as multi-environment trials in variety testing and plant breeding. We consider fixed and random effects models for the environment factor. The methodology takes advantage of multivariate notation, and the questions of interest are formulated as profile analysis problems. Finite performance of the proposed procedures is examined in a simulation study, and application is demonstrated using data from a series of crop variety trials. © 2010 Elsevier B.V. All rights reserved.	approximation;cooperative breeding;experiment;random effects model;randomized algorithm;simulation	Arne C. Bathke;Solomon W. Harrar;Haiyan Wang;Ke Zhang;Hans-Peter Piepho	2010	Computational Statistics & Data Analysis	10.1016/j.csda.2010.02.007	econometrics;statistical hypothesis testing;block design;repeated measures design;fixed effects model;plant breeding;covariance;restricted randomization;approximation;mathematics;symmetry;multivariate analysis;natural environment;data analysis;completely randomized design;design of experiments;algorithm;randomized block design;statistics;random effects model	AI	34.12326455562994	-22.546560246288482	76033
22c63cecff671d41ffc40ccb5f7d8c29630c0482	transformations for accelerating mcmc simulations with broken ergodicity	original landscape;mcmc approach;complex system;bpf value;energy landscape;mcmc simulation;broken ergodicity;new approach;modified bpf;energy well;metropolis algorithm;markov processes;markov chain monte carlo;boltzmann equation;monte carlo methods;monte carlo;partition function;statistical mechanics	"""A new approach for overcoming broken ergodicity in Markov Chain Monte Carlo (MCMC) simulations of complex systems is described. The problem of broken ergodicity is often present in complex systems due to the presence of deep """"energy wells"""" in the energy landscape. These energy wells inhibit the efficient sampling of system states by the Metropolis Algorithm thereby making estimation of the Boltzmann Partition Function (BPF) more difficult. The approach described here uses transformation functions to create a family of modified or smoothed energy landscapes. This permits the Metropolis Algorithm and the MCMC approach to sample system states in a way that leads to accurate estimates of a modified BPF (mBPF). Theoretical results show how it is then possible to extrapolate from this mBPF to the BPF value associated with the original landscape with a small absolute error. Computational examples are provided."""	approximation error;complex systems;computation;ergodicity;extrapolation;markov chain monte carlo;metropolis;metropolis–hastings algorithm;monte carlo method;sampling (signal processing);simulation;smoothing	Mark Fleischer	2007	2007 Winter Simulation Conference		mathematical optimization;statistical mechanics;mathematics;statistics;monte carlo method	EDA	33.59940847812911	-15.574575264363157	76113
2c76a40ae89f22920524817fe7327ae9b93326fa	copyright management using progressive image delivery, visible watermarking and robust fingerprinting	watermarking;image coding;management system;authorisation;low resolution;copyright;watermarking authorisation copyright cryptography image coding;mark removal progressive image delivery visible watermarking robust fingerprinting copyright management system digital images authorization;watermarking robustness fingerprint recognition engineering management digital images image resolution internet multimedia systems computer science packaging;cryptography;fingerprinting copyright management progressive image delivery visible watermarking;digital image	A copyright management system for online trade of digital images is proposed in which low-resolution images with a visible mark are freely downloadable. An authorized user receives a package containing the details of the image he/she buys together with a key for mark removing. Using a downloaded client-end routine, the user removes the mark, reconstructs the high-quality image and embeds a robust fingerprint in the image simultaneously.	digital watermarking;radio fingerprinting	Chuan Qin;Wei-Bin Lee;Xinpeng Zhang;Shuozhong Wang	2006		10.1109/ICARCV.2006.345394	digital watermarking alliance;computer science;cryptography;management system;multimedia;internet privacy;computer security;digital image	HCI	37.754498390120446	-12.278227597642228	76273
1cd4f4a8befbe851929d520b40e571aa38a40274	a simple univariate outlier identification procedure designed for large samples	experimental design;metodo estadistico;probability;aplicacion;loi probabilite;ley probabilidad;05bxx;fonction repartition;loi gamma;62f35;simulacion numerica;plan experiencia;outlier;statistical method;weibull;curva gauss;valor critico;robust;observacion aberrante;funcion distribucion;ley gama;distribution function;grand echantillon;62k99;plan experience;normal;62h30;methode statistique;probability distribution;probabilidad;simulation numerique;probabilite;gamma;62e10;loi normale;gamma distribution;masking;observation aberrante;large sample;valeur critique;application;critical value;60e05;boxplot;gaussian distribution;numerical simulation	A simple univariate outlier identification procedure is presented for the detection of multiple outliers in large and moderate sized data sets. This procedure is a modification of the well-known boxplot outlier-labeling rule. Critical values are easy to obtain for the large sample case for a variety of useful distributions, including the normal, t, gamma, and Weibull. Simple adjustment formulas and graphs are provided for handling smaller samples. Basic probability properties are obtained mathematically and through simulation. Two data sets illustrate the procedure's application as a simple and effective screening tool for both moderate and large-sized univariate samples.		Sharmila Banerjee;Boris Iglewicz	2007	Communications in Statistics - Simulation and Computation	10.1080/03610910601161264	normal distribution;computer simulation;probability distribution;gamma distribution;weibull distribution;econometrics;outlier;normal;critical value;distribution function;calculus;gamma correction;box plot;masking;probability;mathematics;design of experiments;statistics	Vision	33.02306548386614	-22.610195302796484	76823
a3d7f48ff81c8bf1bf16a1f360fe4fc83f4975f1	on global sensitivity indices: monte carlo estimates affected by random errors	monte carlo	Global sensitivity indices for sensitivity analysis of model output are usually estimated by Monte Carlo or quasi-Monte Carlo methods. In this paper, the bias in sensitivity indices produced by random errors in model evaluation is studied.	monte carlo method	Ilya M. Sobol;Boris Shukhman	2007	Monte Carlo Meth. and Appl.	10.1515/MCMA.2007.005	monte carlo method in statistical physics;econometrics;fourier amplitude sensitivity testing;dynamic monte carlo method;hybrid monte carlo;markov chain monte carlo;mathematics;rejection sampling;monte carlo integration;statistics;monte carlo method;control variates	ML	30.761810273573982	-17.710930215212098	76827
b9c6f946ccf5d69b04511b0b4859f731a3ea28ef	use of displays with packaged statistical programs	regression program;general form;various input variable;independent variable;regression function;input variable;new variable;dependent variable;stated function;regression equation;statistical program	During the past few years there has been a great increase in the use of packaged statistical programs. These programs are prepared in a general form. For example, a regression program will allow the user to specify:  the number of variables being introduced as a data set  the number of cases  the choice from this data set of the dependent variable the choice of some subset of the input variables to be the independent variables  the type of input (cards vs. tapes, etc.)  the transformation of any variable in the data set  the construction of new variables for some stated function of the input variables  the stepwise computation of regression function  the priority with which variables may be considered for introduction into the regression equation  the plotting of residuals vs. various input variables.	computation;stepwise regression	W. J. Dixon	1967		10.1145/1465611.1465673	statistical data type;variables;econometrics;linear predictor function;regression diagnostic;mathematics;path coefficient;design matrix;factor analysis;algorithm;regression analysis;statistics	Theory	30.302442574599432	-20.847968365932683	76951
4807e7c2db1bf63a2f884a491835ee9826b16607	the application of item response theory for analyzing the negotiators' accuracy in defining their preferences			item response theory	Ewa Roszkowska;Tomasz Wachowicz	2016		10.1007/978-3-319-52624-9_1	econometrics;statistics	ECom	28.935937168091094	-23.317092694437115	77022
ef59d09cdf081f642adb1aad64f33fa2cdab58fd	mixed model regression mapping for qtl detection in experimental crosses	linear mixed model;software;false discovery rate;quantitative trait loci;ajustamiento modelo;test multiple;likelihood ratio;statistical simulation;genotype by environment interaction;analisis datos;logiciel;random effects;variance component;interaction;reml;qtl mapping;modele lineaire;modele mixte;spatial variation;multiple test;inbred line;regression model;modelo lineal;methode blup;field experiment;componente variancia;statistical regression;interval maps;62jxx;ajustement modele;data analysis;modelo regresion;mmrm;conditional model;simulacion estadistica;efecto aleatorio;metodo blup;composante variance;mixed model;random effect;simulation statistique;weighted sums;regresion estadistica;modele regression;model matching;variacion espacial;62j10;statistical computation;linear model;likelihood ratio test;calculo estadistico;linkage group;covariate;turning point;asreml;variation spatiale;logicial;simulation study;analyse donnee;calcul statistique;covariable;interaccion;composite interval mapping;best linear unbiased predictor;modelo mixto;regression statistique;62p12;rapport vraisemblance;effet aleatoire;test razon verosimilitud;test rapport vraisemblance;asremi;variance;variancia;first fit;relacion verosimilitud	This paper presents mixed model regression mapping (MMRM) as a method for mapping quantitative trait loci (QTL) in backcross and F2 data arising from crosses of inbred lines. It is related to interval mapping, composite interval mapping and other regression approaches but differs in that it tests for QTL presence in each linkage group before conditionally modeling QTL location. The three key ideas presented are to promote use of a Likelihood Ratio type of test for the presence of QTL in linkage groups before searching for QTL as a method of controlling false discovery rate, to present an alternative QTL profile to the LOD score for identifying the possible location of a QTL, and to promote the use of a local smoother to identify turning points in a profile based on evaluation at marker points rather than directly predicting all intermediate points. MMRM requires fitting a short series of models to locate and then evaluate putative QTL. Assuming marker covariates are allocated to linkage groups, MMRM first fits all the markers as independent random effects with common variance within the linkage groups. If there is no significant variance component associated with a linkage group, there is no evidence for a QTL associated with that group. Otherwise a QTL profile is predicted as a weighted sum of the marker BLUPs from which to postulate the most likely position of the QTL. A putative QTL covariate for that position is then calculated from flanking markers and added to the model. If this does not explain all the marker variance, the model is refined. Since MMRM is based on a linear mixed model, the model is easily extended to include extraneous sources of variation such as spatial variation in field experiments, to handle multiple QTL and to test for genotype by environment interactions. It is expounded using two simple examples analysed in the ASReml linear models software. Two simulation studies show that MMRM identifies QTL as reliably as but more directly than other common methods.	mixed model	A. R. Gilmour	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.12.031	mixed model;econometrics;likelihood-ratio test;mathematics;quantitative trait locus;inclusive composite interval mapping;regression analysis;statistics;random effects model	ML	35.09940268584196	-22.56826569199989	77144
828b493d843acb79bc4ebab4d149887ca75ffbc9	a modified weibull distribution	model identification;exponentiated weibull;weibull distribution shape parameter estimation maximum likelihood estimation distribution functions probability density function systems engineering and theory mechanical engineering equations;integrable model;modified weibull distribution;probability density function;graphical approach;computer science software engineering;modeling data;weibull probability plot;bathtub shaped hazard rate function;parameter estimation weibull distribution reliability theory;indexing terms;maximum likelihood estimation;competing models;journal article;reliability theory;weibull extension;systems engineering and theory;bathtub shape;mechanical engineering;weibull distribution;family;shape;beta integrated model;extreme value;lifetime distribution;hazard rate;3 parameter generalization;functional model;parameter estimation modified weibull distribution lifetime distribution bathtub shaped hazard rate function beta integrated model type 1 extreme value distribution 3 parameter generalization weibull probability paper plot model identification exponentiated weibull weibull extension competing models;distribution functions;parameter estimation;weibull probability paper plot;engineering electrical electronic;type 1 extreme value distribution;hazard rate function;computer science hardware architecture	A new lifetime distribution capable of modeling a bathtub-shaped hazard-rate function is proposed. The proposed model is derived as a limiting case of the Beta Integrated Model and has both the Weibull distribution and Type I extreme value distribution as special cases. The model can be considered as another useful 3-parameter generalization of the Weibull distribution. An advantage of the model is that the model parameters can be estimated easily based on a Weibull probability paper (WPP) plot that serves as a tool for model identification. Model characterization based on the WPP plot is studied. A numerical example is provided and comparison with another Weibull extension, the exponentiated Weibull, is also discussed. The proposed model compares well with other competing models to fit data that exhibits a bathtub-shaped hazard-rate function.	bathtub curve;maxima and minima;numerical analysis;system identification;volcano plot (statistics);web hosting service	C. D. Lai;Min Xie;D. N. Prabhakar Murthy	2003	IEEE Trans. Reliability	10.1109/TR.2002.805788	reliability engineering;data modeling;weibull distribution;econometrics;probability density function;generalized extreme value distribution;weibull modulus;index term;system identification;reliability theory;exponentiated weibull distribution;shape;function model;distribution function;extreme value theory;mathematics;maximum likelihood;hazard ratio;estimation theory;statistics	Metrics	31.07332470614109	-19.553373961273497	77351
54159b9b59a5340eb2a6edb4b6cd35a35d7a3f04	lossless and efficient polynomial-based secret image sharing with reduced shadow size		Thien-and-Lin’s polynomial-based secret image sharing (PSIS) is utilized as the basic method to achieve PSISs with better performances, such as meaningful shares, two-in-one property and shares with different priorities. However, this (k, n) threshold PSIS cannot achieve lossless recovery for pixel values more than 250. Furthermore, current solutions to lossless recovery for PSIS have several natural drawbacks, such as large computational costs and random pixel expansion. In this paper, a lossless and efficient (k, n) threshold PSIS scheme with reduced shadow size is presented. For lossless recovery and efficiency, two adjacent pixels are specified as a secret value, the prime in the sharing polynomial is replaced with 65,537, and then the additional screening operation can ensure each shared value in the range [0, 65,535]. To reduce shadows size and improve security, only the first k − 1 coefficients are embedded with secret values and the last coefficient is assigned randomly. To prevent the leakage of secrets, generalized Arnold permutation with special key generating strategy is performed on the secret image prior to sharing process without key distribution. Both theoretical analyses and experiments are conducted to demonstrate the effectiveness of the proposed scheme.	arnold;coefficient;conceptualization (information science);data curation;digital curation;embedded system;experiment;key distribution;lossless compression;performance;pixel;polynomial;randomness;spectral leakage	Xuan Zhou;Yuliang Lu;Xuehu Yan;Yongjie Wang;Lintao Liu	2018	Symmetry	10.3390/sym10070249	combinatorics;discrete mathematics;mathematics;polynomial;image sharing;shadow;lossless compression	ML	38.85539670151771	-10.179307144351926	77426
1e071e4eb517ff140c094d6c1bc6775b6e326040	how should prey animals respond to uncertain threats?	health research;constrained optimization;uk clinical guidelines;biological patents;escape decision;europe pubmed central;uncertainty;citation search;agent based modeling;probabilistic automata;objective function;uk phd theses thesis;life sciences;uk research reports;medical journals;computer simulation;europe pmc;biomedical research;bioinformatics;evolution;in silico	A prey animal surveying its environment must decide whether there is a dangerous predator present or not. If there is, it may flee. Flight has an associated cost, so the animal should not flee if there is no danger. However, the prey animal cannot know the state of its environment with certainty, and is thus bound to make some errors. We formulate a probabilistic automaton model of a prey animal's life and use it to compute the optimal escape decision strategy, subject to the animal's uncertainty. The uncertainty is a major factor in determining the decision strategy: only in the presence of uncertainty do economic factors (like mating opportunities lost due to flight) influence the decision. We performed computer simulations and found that in silico populations of animals subject to predation evolve to display the strategies predicted by our model, confirming our choice of objective function for our analytic calculations. To the best of our knowledge, this is the first theoretical study of escape decisions to incorporate the effects of uncertainty, and to demonstrate the correctness of the objective function used in the model.	computer simulation;correctness (computer science);decision theory;loss function;optimization problem;population;prey;probabilistic automaton;threat (computer)	Joel Zylberberg;Michael Robert DeWeese	2011		10.3389/fncom.2011.00020	psychology;computer simulation;biology;constrained optimization;simulation;uncertainty;computer science;bioinformatics;artificial intelligence;machine learning;evolution;operations research	AI	32.541447296921646	-11.58122919438146	77554
b5c6af225c1fed9bdb2fac4fb3f773a3dc94630f	an exponential damage model for strength of fibrous composite materials	composite material;tensile testing;birnbaum saunders distribution;weibull distribution exponential damage model fibrous composite material tensile strength test statistical distribution cumulative damage model parameter estimation gaussian distribution;cumulative damage;weibull distribution composite materials failure analysis gaussian distribution parameter estimation tensile strength tensile testing;inverse gaussian distribution;failure analysis;weibull distribution;size effect;composite materials conducting materials stress aerospace materials aircraft manufacture aluminum statistical distributions predictive models gaussian distribution maximum likelihood estimation;weibull distribution birnbaum saunders distribution cumulative damage inverse gaussian distribution size effect tensile strength;tensile strength;composite materials;parameter estimation;cumulant;statistical distribution;gaussian distribution	When modeling experimental data observed from carefully performed tensile strength tests, statistical distributions are typically used to describe the strength of composite specimens. Recently, cumulative damage models derived for predicting tensile strength have been shown to be superior to other models when used to fit composite strength data. Here, an alternative model is developed which is based on an exponential cumulative damage approach. The model is shown to exhibit a similar structural form to the other models in the literature so that previous theory for cumulative damage models can be utilized to find parameter estimates.	composite pattern;energy citations database;explanatory combinatorial dictionary;simultaneous equations model;time complexity;utility functions on indivisible goods	William J. Owen	2007	IEEE Transactions on Reliability	10.1109/TR.2007.903353	normal distribution;probability distribution;weibull distribution;econometrics;failure analysis;tensile testing;mathematics;forensic engineering;ultimate tensile strength;estimation theory;inverse gaussian distribution;statistics;cumulant	Comp.	31.34864492070483	-20.548663636567394	77578
b819fb60d4d203f09f4a05bb3d03fbf6df32f82d	winding stairs: a sampling tool to compute sensitivity indices	small sample size;winding stairs sampling scheme;global sensitivity analysis;first order indices;total sensitivity indices;model evaluation;first order;nonlinear modelling;sensitivity analysis;simulation study;sobol lp _τ sequences	Sensitivity analysis aims to ascertain how each model input factor influences the variation in the model output. In performing global sensitivity analysis, we often encounter the problem of selecting the required number of runs in order to estimate the first order and/or the total indices accurately at a reasonable computational cost. The Winding Stairs sampling scheme (Jansen M.J.W., Rossing W.A.H., and Daamen R.A. 1994. In: Gasman J. and van Straten G. (Eds.), Predictability and Nonlinear Modelling in Natural Sciences and Economics. pp. 334–343.) is designed to provide an economic way to compute these indices. The main advantage of it is the multiple use of model evaluations, hence reducing the total number of model evaluations by more than half. The scheme is used in three simulation studies to compare its performance with the classic Sobol’ LPτ . Results suggest that the Jansen Winding Stairs method provides better estimates of the Total Sensitivity Indices at small sample sizes.	algorithmic efficiency;boson sampling;chan's algorithm;computation;interaction;morris method;nonlinear modelling;numerical analysis;sampling (signal processing);simulation	Karen Chan;Andrea Saltelli;Stefano Tarantola	2000	Statistics and Computing	10.1023/A:1008950625967	econometrics;mathematical optimization;first-order logic;mathematics;sensitivity analysis;statistics	Vision	27.491444904920147	-17.318258443419996	77721
d346772292d85546c4d3cf8efcff263d5e876b99	a parametric bootstrap solution to two-way manova without interaction under heteroscedasticity: fixed and mixed models	heteroscedastic two way monova;secondary 62j10;primary 62f03;期刊论文;unbalanced data;bootstrap re sampling	In this article, we propose a parametric bootstrap (PB) test for heteroscedastic two-way multivariate analysis of variance without Interaction. For the problem of testing equal main effects of factors, we obtain a PB approach and compare it with existing modified Brown–Forsythe (MBF) test and approximate Hotelling T2 (AHT) test by an extensive simulation study. The PB test is a symmetric function in samples, and does not depend on the chosen weights used to define the parameters uniquely. Simulation results indicate that the PB test performs satisfactorily for various cell sizes and parameter configurations when the homogeneity assumption is seriously violated, and tends to outperform the AHT test for moderate or larger samples in terms of power and controlling size. The MBF test, the AHT test, and the PB test have similar robustness to violations of underlying assumptions. It is also noted that the same PB test can be used to test the significance of random effect vector in a two-way multivariate mixed e...	mixed model	Liwen Xu;Lingli Yuan	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.930906	econometrics;mathematics;exact test;statistics	ECom	30.250606559566904	-22.737723042460487	77743
bdf81dcd76fa96e980e24cdd1d912b772b801f44	localization of jpeg double compression through multi-domain convolutional neural networks		When an attacker wants to falsify an image, in most of cases she/he will perform a JPEG recompression. Different techniques have been developed based on diverse theoretical assumptions but very effective solutions have not been developed yet. Recently, machine learning based approaches have been started to appear in the field of image forensics to solve diverse tasks such as acquisition source identification and forgery detection. In this last case, the aim ahead would be to get a trained neural network able, given a to-be-checked image, to reliably localize the forged areas. With this in mind, our paper proposes a step forward in this direction by analyzing how a single or double JPEG compression can be revealed and localized using convolutional neural networks (CNNs). Different kinds of input to the CNN have been taken into consideration, and various experiments have been carried out trying also to evidence potential issues to be further investigated.	artificial neural network;authentication;convolutional neural network;digital signature forgery;discrete cosine transform;experiment;jpeg 2000;lossy compression;machine learning;portable network graphics;sensor	Irene Amerini;Tiberio Uricchio;Lamberto Ballan;Roberto Caldelli	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.233	computer vision;convolutional neural network;frequency domain;transform coding;artificial neural network;computer science;jpeg;machine learning;lossless jpeg;compression (physics);artificial intelligence	Vision	36.369985769808316	-13.122426609996719	77810
87d7ff500ce9d122e419396b9f6df654d3f19cf1	evolution and incremental learning in the iterated prisoner's dilemma	minimisation;social network services;modelizacion;dilema social;iterated prisoner dilemma;search problems convergence of numerical methods evolutionary computation game theory iterative methods learning artificial intelligence minimisation probability;complex dynamics;adaptability;adaptabilite;learning algorithm;prisoner dilemma game;convergence game theory australia feedback learning systems genetic algorithms humans environmental economics social network services;mise a jour;convergence;evolutionary computation;game theory;probability;bucle multiple;jeu dilemme prisonnier;feedback learning mechanism;helium;prisoner s dilemma;convergence of numerical methods;disparity minimization;juego dilema prisionero;multiple loop;social dilemma;prisoner s dilemma evolution genetic algorithm ga incremental learning il;disparity;search strategy;intelligence artificielle;algorithme apprentissage;probabilistic approach;algoritmo genetico;classification;disparidad;adaptabilidad;double loop incremental learning scheme;actualizacion;modelisation;learning systems;iterative methods;probability evolutionary computation iterated prisoner dilemma memetic adaptation framework directed search eventual convergence disparity minimization double loop incremental learning scheme classification component feedback learning mechanism;incremental learning il;eventual convergence;feedback;iterated prisoner s dilemma;retroaccion;incremental learning;retroaction;memetic adaptation framework;enfoque probabilista;approche probabiliste;environmental economics;strategie recherche;feedback regulation;algorithme genetique;artificial intelligence;algorithme evolutionniste;genetic algorithm;genetic algorithms;algoritmo evolucionista;humans;search problems;inteligencia artificial;evolutionary algorithm;classification component;evolutionary process;learning artificial intelligence;direct search;evolutionary learning;directed search;algoritmo aprendizaje;modeling;learning strategies;dilemme social;disparite	This paper examines the comparative performance and adaptability of evolutionary, learning, and memetic strategies to different environment settings in the iterated prisoner's dilemma (IPD). A memetic adaptation framework is developed for IPD strategies to exploit the complementary features of evolution and learning. In the paradigm, learning serves as a form of directed search to guide evolving strategies to attain eventual convergence towards good strategy traits, while evolution helps to minimize disparity in performance among learning strategies. Furthermore, a double-loop incremental learning scheme (ILS) that incorporates a classification component, probabilistic update of strategies and a feedback learning mechanism is proposed and incorporated into the evolutionary process. A series of simulation results verify that the two techniques, when employed together, are able to complement each other's strengths and compensate for each other's weaknesses, leading to the formation of strategies that will adapt and thrive well in complex, dynamic environments.	binocular disparity;evolution;experience;interpupillary distance;iterated function;iteration;memetic algorithm;memetics;prisoner's dilemma;programming paradigm;simulation;software release life cycle;synergy	Hanyang Quek;Kay Chen Tan;Chi Keong Goh;Hussein A. Abbass	2009	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2008.2003009	game theory;mathematical optimization;simulation;genetic algorithm;computer science;artificial intelligence;machine learning;evolutionary algorithm;mathematics;algorithm;evolutionary computation	Robotics	25.31294833891288	-11.215934230731522	77904
b37b5224cdf807380fc31bbc31b3569374482333	comparing approaches for evolving high-level robot control based on behaviour repertoires		Evolutionary robotics approaches have traditionally been focused on monolithic controllers. Recent studies on the evolution of hierarchical control have, however, yielded promising results. Hierarchical approaches typically rely on a repertoire of behaviour primitives (which themselves can be the result of an evolutionary process), and an evolved top-level arbitrator that continually executes primitives from the repertoire to solve a given task. In this paper, we compare different controller architectures for the evolution of top-level arbitrators. We propose two new methods, one based on neural networks and another based on decision trees induced by genetic programming. We compare the new approaches with existing ones, namely neural network regressors and non-hierarchical control, in a challenging simulated maze navigation task that requires a broad diversity of primitives. Based on empirical results, we draw a number of conclusions regarding the strengths and limitations of each of the studied approaches.	artificial neural network;decision tree;evolutionary robotics;genetic programming;language primitive;monolithic system;robot control	Jorge C. Gomes;Anders Lyhne Christensen	2018	2018 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2018.8477699	control theory;machine learning;task analysis;artificial intelligence;artificial neural network;decision tree;genetic programming;robot control;computer science;robot kinematics;evolutionary robotics	Robotics	25.201908580842186	-13.007041473821225	77945
db1446bff6cff76cbd3df1ca15aa640f82d677ff	a general framework for evolutionary multiobjective optimization via manifold learning	evolutionary computation;regularity;principal curve;manifold learning;laplacian eigenmaps;multiobjective optimization	Under certain mild condition, the Pareto-optimal set (PS) of a continuous multiobjective optimization problem, with m objectives, is a piece-wise continuous (m 1)-dimensional manifold. This regularity property is important, yet has been unfortunately ignored in many evolutionary multiobjective optimization (EMO) studies. The first work that explicitly takes advantages of this regularity property in EMO is the regularity model-based multiobjective estimation of distribution algorithm (RM-MEDA). However, its performance largely depends on its model parameter, which is problem dependent. Manifold learning, also known as nonlinear dimensionality reduction, is able to discover the geometric property of a low-dimensional manifold embedded in the high-dimensional ambient space. This paper presents a general framework that applies advanced manifold learning techniques in EMO. At each generation, we first use a principal curve algorithm to obtain an approximation of the PS manifold. Then, the Laplacian eigenmaps algorithm is employed to find the low-dimensional representation of this PS approximation. Afterwards, we identify the neighborhood relationship in this low-dimensional representation, which is also applicable for the original high-dimensional data. Based on the neighborhood relationship, we can interpolate new candidate solutions that obey the geometric property of the PS manifold. Empirical results validate the effectiveness of our proposal. & 2014 Elsevier B.V. All rights reserved.	approximation;embedded system;estimation of distribution algorithm;interpolation;mathematical optimization;multi-objective optimization;nonlinear dimensionality reduction;nonlinear system;optimization problem;ps-algol;pareto efficiency;rm-odp	Ke Li;Sam Kwong	2014	Neurocomputing	10.1016/j.neucom.2014.03.070	mathematical optimization;combinatorics;computer science;multi-objective optimization;machine learning;mathematics;nonlinear dimensionality reduction;evolutionary computation;manifold alignment	AI	29.25982074433906	-10.462290418701986	78502
3a986821cedf95ca7c5ca7e4ab041f340e884cc9	charted metropolis light transport		In this manuscript, inspired by a simpler reformulation of primary sample space Metropolis light transport, we derive a novel family of general Markov chain Monte Carlo algorithms called charted Metropolis-Hastings, that introduces the notion of sampling charts to extend a given sampling domain and make it easier to sample the desired target distribution and escape from local maxima through coordinate changes. We further apply the novel algorithms to light transport simulation, obtaining a new type of algorithm called charted Metropolis light transport, that can be seen as a bridge between primary sample space and path space Metropolis light transport. The new algorithms require to provide only right inverses of the sampling functions, a property that we believe crucial to make them practical in the context of light transport simulation.	chart;light transport theory;markov chain monte carlo;maxima and minima;metropolis light transport;metropolis–hastings algorithm;monte carlo method;sampling (signal processing);simulation	Jacopo Pantaleoni	2017	ACM Trans. Graph.	10.1145/3072959.3073677	econometrics;mathematical optimization;calculus;mathematics;metropolis light transport;statistics	Graphics	33.69532690724209	-15.619201973639598	78811
3768a5a7771f2403042a2d3fd502c03a99fd942f	spatial multi-taper spectrum estimation for nuclear reactor modelling	parametric model;47a10;stochastic process;methode parametrique;nuclear reactor;analisis datos;metodo parametrico;echantillonnage;parametric method;spatial variation;regression model;spectrum;62m15;modele parametrique;statistical regression;62jxx;sampling;data analysis;modelo regresion;transfer function;funcion traspaso;regresion estadistica;modele regression;variacion espacial;statistical computation;calculo estadistico;analyse spectrale;processus stochastique;superficie;variation spatiale;analyse donnee;analisis espectral;surface;spectral properties;fonction transfert;calcul statistique;spectral estimation;temperature measurement;estimation statistique;proceso estocastico;spectral analysis;muestreo;regression statistique;estimacion estadistica;statistical estimation;47a11;propriete spectrale;propiedad espectral	Multi-taper univariate and cross-spectral analysis is used to investigate the structure of spatial variation in the temperatures measured across the surface of a nuclear reactor. The construction of the spatial tapers over the approximate circular reactor surface is described, along with derivation and sampling properties of the spectral estimates. It is shown how the analysis can be used to identify regular patterns in the temperature measurements, and the spatial transfer function between the reactor temperatures and fuel characteristics. The results are used to specify a parametric regression model for the temperature measurements.	reactor (software);spectral density estimation	Carl John Scarrott;Granville Tunnicliffe Wilson	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2009.06.007	stochastic process;econometrics;calculus;mathematics;regression analysis;statistics	ML	34.65051585111422	-22.594840351524756	78848
1728573a3fb984ab82f1e2b99bc943aad174f9e4	modeling data with multiple time dimensions	pinon pine;systeme sante;seguro;ajustamiento modelo;biological system;62g05;environmental impact;analisis datos;05c05;62m10;62m20;health system;hombre;multiple time series;non linear model;time series;retail lending;non linear modeling;fonction caracteristique;ajustement modele;soin;data analysis;dendrochronology;systeme biologique;analyse serie temporelle;assurance;forecasting theory;digital media;time series analysis;tree rings;portfolio forecasting;douglas fir;global climate change;sistema salud;search for extraterrestrial intelligence;model matching;scenario based forecasting;care;statistical computation;serie temporelle;calculo estadistico;human;35l67;el malpais;characteristic function;serie temporal;generalized additive model;theorie prevision;biological systems;analyse donnee;dual time dynamics;calcul statistique;62p05;60e10;ponderosa pine;generalized additive models;62p12;cuidado;funcion caracteristica;large classes;62p10;human resource management;sistema biologico;insurance;tree ring;homme;non linear dynamics;seti home;health care	A large class of problems in time series analysis can be represented by a set of overlapping time series with different starting times. These time series may be treated as different probes of the same underlying process. Such probes may follow a characteristic lifecycle as a function of the time since the series began. They may also be subject to environmental shocks according to calendar time. In addition, the calibration of each probe may be unknown such that each series may show a different magnitude of response to the underlying lifecycles and environmental impacts. This paper describes an approach to analyzing these multiple time series as a single set such that the underlying lifecycles and calendar-based shocks may be measured. Simultaneously, the individual calibrations of the time series are also measured. This technique is referred to as dual-time dynamics, and it applies to many important business problems. Applications to tree ring analysis, the SETI@home project, and retail loan portfolio forecasting are provided. Other areas of possible application include digital media services, insurance, human resource management, health care, and biological systems to name a few.		Joseph L. Breeden	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2007.01.023	econometrics;simulation;human resource management;mathematics;dendrochronology;statistics	ML	35.0002660943089	-22.528389113629604	78876
094c6dae2bae3d9d394041c78471e70887d5740e	a bayesian monte carlo-based algorithm for the estimation of small failure probabilities of systems affected by uncertainties	gaussian processes;uncertainties;bayesian monte carlo;small failure probability	The estimation of system failure probabilities in presence of uncertainties may be a difficult task when the values involved are very small, so that sampling-based Monte Carlo methods may become computationally impractical, especially if the computer codes used to model the system response require large computational efforts, both in terms of time and memory. In this work, we propose to exploit the Bayesian Monte Carlo (BMC) approach to the estimation of definite integrals for developing a new, efficient algorithm for estimating small failure probabilities. The Bayesian framework allows an effective use of all the information available, i.e. the computer code evaluations and the input uncertainty distributions, and, at the same time, the analytical formulation of the Bayesian estimator guarantees the construction of a computationally lean algorithm. The proposed method is first satisfactorily tested with reference to an analytic, two-dimensional case study of literature, offering satisfactory results; then, it is applied to a realistic case study of a natural convection-based cooling system of a gas-cooled fast reactor, operating under a post-loss-of-coolant accident (LOCA), showing performances comparable to those of other efficient alternative methods of literature.	algorithm;bayesian network;code;computer cooling;monte carlo method;performance;reactor (software);sampling (signal processing)	Francesco Cadini;A. Gioletta	2016	Rel. Eng. & Sys. Safety	10.1016/j.ress.2016.04.003	bayesian average;econometrics;simulation;hybrid monte carlo;computer science;monte carlo molecular modeling;gaussian process;mathematics;kinetic monte carlo;bayesian statistics;monte carlo integration;statistics;monte carlo method	ML	31.148013434787885	-16.405856808311007	78958
46cb0a5e5d71d35a38521d4aab9ed83e09baca3d	field data analyses with additional after-warranty failure data	reliability;after warranty data;weibull distribution;maximum likelihood estimate;asymptotic properties;simulation study;field data;reporting probability;expectation and maximization;em algorithm;article	This paper proposes methods of estimating the lifetime distribution for situations where additional field data can be gathered after the warranty expires in a parametric time to failure distribution. For satisfactory inference about parameters involved, it is desirable to incorporate these after-warranty data in the analysis. It is assumed that after-warranty data are reported with probability  p  1 (<1), while within-warranty data are reported with probability 1. Methods of obtaining maximum likelihood estimators are outlined, their asymptotic properties are studied, and specific formulas for Weibull distribution are obtained. An estimation procedure using the expectation and maximization algorithm is also proposed when the reporting probability is unknown. Simulation studies are performed to investigate the properties of the estimates.		Y. S. Oh;Do Sun Bai	2001	Rel. Eng. & Sys. Safety	10.1016/S0951-8320(00)00056-9	reliability engineering;weibull distribution;econometrics;expectation–maximization algorithm;inverse-chi-squared distribution;reliability;mathematics;maximum likelihood;joint probability distribution;statistics	DB	29.584595985753328	-20.561408507494942	79000
06ffe2e6248a453a49267b2eadc38e4de82533fe	estimating the percentage of food expenditure in small areas using bias-corrected p-spline based estimators	model design;mse;design oriented bootstrap bias correction;regression model;spanish household budget survey;small area estimation;p;mixed model;bias correction;p spline models;penalized splines;household budget survey	Small area estimators based on a penalized spline regression model approximating a non-linear but smooth relationship between a response and a given covariate are obtained. In each small area, individual curves are fitted using penalized splines with B-spline bases, exploiting the mixed model representation of the P-splines for inferential purposes. To account for possible bias, a design-oriented bootstrap correction is proposed. The mean squared error of the bias-corrected estimator is also provided. The methods are used to estimate the percentage of food expenditure for alternative household sizes at provincial level in Spain using the 2006 Spanish Household Budget Survey.	spline (mathematics);t-spline	Ana F. Militino;T. Goicoa;María Dolores Ugarte	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2012.01.009	mixed model;small area estimation;p;econometrics;mathematics;regression analysis;statistics	ML	27.390504746207018	-22.579138762261827	79159
96810a37571cb379c8c2b32048274fdb765705e0	a new method for goodness-of-fit testing based on type-ii right censored samples	order statistics;mathematics;order statistic;parametric statistics;distributed computing;type ii right censored samples;empirical distribution function;random variables;testing;indexing terms;maximum likelihood estimation;uniform distribution goodness of fit testing type ii right censored samples order statistics;statistical distributions;statistical analysis;testing statistical distributions statistical analysis parametric statistics random variables distribution functions maximum likelihood estimation mathematics distributed computing;goodness of fit test;simulation study;uniform distribution empirical distribution function edf statistics goodness of fit testing monte carlo simulation order statistics type ii censoring;distribution functions;monte carlo simulation;goodness of fit testing;empirical distribution function edf statistics;right censoring;uniform distribution;type ii censoring	We present a simple method for testing goodness-of-fit based on type-II right censored samples. Applying the property of order statistics due to Malmquist, we can transform any conventional type-II right censored sample of size r out of n from a uniform distribution to a complete sample of size r from a uniform distribution. This result is used to develop the proposed goodness-of-fit test procedure. The simulation studies reveal that the proposed approach provides as good or better overall power than the method of Michael & Schucany.	censoring (statistics);earliest deadline first scheduling;simulation;type inference	Chien-Tai Lin;Yen-Lung Huang;N. Balakrishnan	2008	IEEE Transactions on Reliability	10.1109/TR.2008.2005860	econometrics;order statistic;pattern recognition;mathematics;statistics	Visualization	30.836374029318105	-20.384313324395613	79163
68d026990ded606ed71152f532e5ab21dfb96c05	modeling high frequency data using hawkes processes with power-law kernels		Those empirical properties exhibited by high frequency financial data, such as time-varying intensities and self-exciting features, make it a challenge to model appropriately the dynamics associated with, for instance, order arrival. To capture the microscopic structures pertaining to limit order books, this paper focuses on modeling high frequency financial data using Hawkes processes. Specifically, the model with power-law kernels is compared with the counterpart with exponential kernels, on the goodness of fit to the empirical data, based on a number of proposed quantities for statistical tests. Based on one-trading-day data of one representative stock, it is shown that Hawkes processes with power-law kernels are able to reproduce the intensity of jumps in the price processes more accurately, which suggests that they could serve as a realistic model for high frequency data on the level of microstructure.	book;norm (social);time complexity	Changyong Zhang	2016		10.1016/j.procs.2016.05.366	econometrics;computer science;artificial intelligence;statistics	AI	24.727117812337767	-21.22928826762318	79273
7c5ee260317635187b691c8dcb6cddc85b4c1f76	a cusum change-point detection algorithm for non-stationary sequences with application to data network surveillance	time varying;stationary sequence;screening;cusum algorithm;line detection;root cause analysis;on line detection;network surveillance;change point detection	We adapt the classic cusum change-point detection algorithm to handle non-stationary sequences that are typical with network surveillance applications. The proposed algorithm uses a defined timeslot structure to take into account time varying distributions, and uses historical samples of observations within each timeslot to facilitate a nonparametric methodology. Our proposed solution includes an on-line screening feature that fully automates the implementation of the algorithm and eliminates the need for manual oversight up until the point where root cause analysis begins.	algorithm;computer and network surveillance;stationary process	Veronica Montes De Oca;Daniel R. Jeske;Qi Zhang;Carlos Rendón;Mazda Marvasti	2010	Journal of Systems and Software	10.1016/j.jss.2010.02.006	simulation;root cause analysis;computer science;data mining;stationary sequence;change detection;statistics	ML	30.756373508368952	-13.47399112074873	79499
ebd62290221aa54c869739777dc80ba9f9d71c97	optimal observation time points in stochastic chemical kinetics		Wet-lab experiments, in which the dynamics within living cells are observed, are usually costly and time consuming. This is particularly true if single-cell measurements are obtained using experimental techniques such as flow-cytometry or fluorescence microscopy. It is therefore important to optimize experiments with respect to the information they provide about the system. In this paper we make a priori predictions of the amount of information that can be obtained from measurements. We focus on the case where the measurements are made to estimate parameters of a stochastic model of the underlying biochemical reactions. We propose a numerical scheme to approximate the Fisher information of future experiments at different observation time points and determine optimal observation time points. To illustrate the usefulness of our approach, we apply our method to two interesting case studies.	approximation algorithm;design of experiments;experiment;fisher information;kinesiology;numerical analysis	Charalampos Kyriakopoulos;Verena Wolf	2014		10.1007/978-3-319-27656-4_5	econometrics;simulation;mathematics;statistics	ML	33.985346476220464	-15.45062744292647	79563
8d102a174efcd4a1532f063bc41b56e5ade9e4c0	a control model for markovian genetic regulatory networks	conference_paper;continuous variable;genetic regulatory network;book_chapter;control problem;genetic network;probability distribution;stochastic dynamic programming	In this paper, we study a control model for gene intervention in a genetic regulatory network. At each time step, a finite number of controls are allowed to drive to some target states (i.e, some specific genes are on, and some specific genes are off) of a genetic network. We are interested in determining a minimum amount of control cost on a genetic network over a certain period of time such that the probabilities of obtaining such target states are as large as possible. This problem can be formulated as a stochastic dynamic programming model. However, when the number of genes is n, the number of possible states is exponentially increasing with n, and the computational cost of solving such stochastic dynamic programming model would be very huge. The main objective of this paper is to approximate the above control problem and formulate as a minimization problem with integer variables and continuous variables using dynamics of states probability distribution of genes. Our experimental results show that our proposed formulation is efficient and quite effective for solving control gene intervention in a genetic network.	gene regulatory network	Michael K. Ng;Shuqin Zhang;Wai-Ki Ching;Tatsuya Akutsu	2006		10.1007/11790105_4	stochastic programming;probability distribution;mathematical optimization;machine learning;genetic representation;mathematics;mathematical economics;statistics	Logic	33.38125505971193	-10.999823340645975	80081
d3f905b6f0cb90467961650a05b885aab4698d35	l1-penalised ordinal polytomous regression estimators with application to gene expression studies		Qualitative but ordered random variables, such as severity of a pathology, are of paramount importance in biostatistics and medicine. Understanding the conditional distribution of such qualitative variables as a function of other explanatory variables can be performed using a specific regression model known as ordinal polytomous regression. Variable selection in the ordinal polytomous regression model is a computationally difficult combinatorial optimisation problem which is however crucial when practitioners need to understand which covariates are physically related to the output and which covariates are not. One easy way to circumvent the computational hardness of variable selection is to introduce a penalised maximum likelihood estimator based on some well chosen non-smooth penalisation function such as, e.g., the l_1-norm. In the case of the Gaussian linear model, the l_1-penalised least-squares estimator, also known as LASSO estimator, has attracted a lot of attention in the last decade, both from the theoretical and algorithmic viewpoints. However, even in the Gaussian linear model, accurate calibration of the relaxation parameter, i.e., the relative weight of the penalisation term in the estimation cost function is still considered a difficult problem that has to be addressed with caution. In the present paper, we apply l_1-penalisation to the ordinal polytomous regression model and compare several hyper-parameter calibration strategies. Our main contributions are: (a) a useful and simple l_1 penalised estimator for ordinal polytomous regression and a thorough description of how to apply Nesterovu0027s accelerated gradient and the online Frank-Wolfe methods to the problem of computing this estimator, (b) a new hyper-parameter calibration method for the proposed model, based on the QUT idea of Giacobino et al. and (c) a code which can be freely used that implements the proposed estimation procedure.	polytomy	Stéphane Chrétien;Christophe Guyeux;Serge Moulin	2018		10.4230/LIPIcs.WABI.2018.17	estimator;lasso (statistics);combinatorics;statistics;regression analysis;linear model;ordinal number;polytomous rasch model;feature selection;computer science;conditional probability distribution	ML	27.615950341711862	-23.717830843479852	80370
21b2ab440c04e817d7b797815a8f5bf89fd34992	partially parametric interval estimation of $pr\{y>x\}$	approximation asymptotique;fonction vraisemblance;finite sample;62f12;statistical simulation;methode empirique;methode parametrique;asymptotic normality;intervalo confianza;analisis datos;stress strength problem;variable independante;maximum likelihood;variable aleatoire;metodo parametrico;62f25;metodo empirico;parametric method;echantillon fini;maximum vraisemblance;empirical method;62g20;variable aleatoria;area under the roc curve;funcion verosimilitud;vraisemblance empirique;estimation parametrique;data analysis;confidence interval;62fxx;simulacion estadistica;parametric estimation;62g15;simulation statistique;intervalle confiance;statistical computation;calculo estadistico;random variable;estimacion parametro;analyse donnee;calcul statistique;variable independiente;asymptotic approximation;parameter estimation;estimation parametre;empirical likelihood;likelihood function;maxima verosimilitud;normalidad asintotica;independent variable;normalite asymptotique;receiver operating characteristic roc curve;aproximacion asintotica;62e20	Let X and Y   be two independent continuous random variables. Three techniques to obtain confidence intervals for ρ=Pr{Y>X}ρ=Pr{Y>X} are discussed in a partially parametric framework. One method relies on the asymptotic normality of an estimator for ρρ; the remaining methods involve empirical likelihood and combine it with maximum likelihood estimation and with full parametric likelihood, respectively. Finite-sample accuracy of the confidence intervals is assessed through a simulation study. An illustration is given using a data set on the detection of carriers of Duchenne Muscular Dystrophy.		Gianfranco Adimari;Monica Chiogna	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2005.12.007	variables;random variable;econometrics;confidence interval;calculus;mathematics;maximum likelihood;likelihood function;asymptotic distribution;estimation theory;data analysis;empirical research;statistics	ML	32.70491748857593	-22.566553535395922	80963
85c65fce94308ece82f8b53ca67eb7abe7ba3ba4	spatial counts under differential privacy mechanism on changing spatial scales		With a spatial statistical database covering a large region, how to publish differential privacy protected information is a challenge. In previous works, information was published using large fixed spatial cells. In this paper, we develop novel flexible methods to publish the spatial information, which allows the users to freely move around the large region, zoom in and zoom out at arbitrary locations, and obtain information over spatial areas both large and small. We develop two methods to publish the spatial information protected under differential privacy. First the region is divided into the smallest spatial cells, where each cell does not observe an event happening more than once. Given repeated measurements, such as multiple day data, the noise added Bernoulli probabilities are computed for all the smallest spatial cells. For larger spatial cells of high interests to users, the noisy Bernoulli probabilities are combined into noisy Poisson-Binomial distributions which also satisfy differential privacy requirement. We use the New York Taxi data in the experiments to demonstrate how our methods work. We show that both of our methods are accurate, while the noisy count probabilities directly obtained from fixed large spatial cells often generate the spatial counts much smaller than the true values. © 2017 Elsevier Ltd. All rights reserved.	bernoulli polynomials;differential privacy;experiment;noisy-storage model;spatial scale;statistical database	Jun Jiang;Bowei Xi;Murat Kantarcioglu	2018	Computers & Security	10.1016/j.cose.2017.11.018	internet privacy;data mining;differential privacy;statistical database;poisson binomial distribution;spatial analysis;computer science;publication;test data generation;zoom	ML	36.26543276524893	-18.702848755628015	81363
26a47f98ad3cb5f0b4e89c4a561f0a1e922e3530	local bootstrap approaches for fractional differential parameter estimation in arfima models	intervalo tiempo;62f40;estimacion sesgada;erreur moyenne;bootstrap;intervalo confianza;analisis datos;62f25;simulation;metodo semiparametrico;erreur quadratique moyenne;error sistematico;estimacion promedio;semiparametric estimation;mean error;simulacion;methode semiparametrique;error medio;time interval;resolucion problema;periodogramme;simulation experiment;data analysis;confidence interval;fractionally integrated arma process;estimation erreur;time series analysis;bias;62g15;error estimation;mean square error;intervalle confiance;statistical computation;estimacion error;calculo estadistico;semiparametric method;estimacion parametro;fractional integral;analyse donnee;periodogram;calcul statistique;mean estimation;parameter estimation;error medio cuadratico;estimation parametre;estimation moyenne;biased estimation;estimation biaisee;problem solving;resolution probleme;erreur systematique;intervalle temps	In this paper we investigate bootstrap techniques applied to the estimation of the fractional differential parameter in ARFIMA models, d. The novelty is the focus on the local bootstrap of the periodogram function. The approach is then applied to three different semiparametric estimators of d, known from the literature, based upon the periodogram function. By means of an extensive set of simulation experiments, the bias and mean square errors are quantified for each estimator and the efficacy of the local bootstrap is stated in terms of low bias, short confidence intervals, and low CPU times. Finally, a real data set is analyzed to demonstrate that the methodology may be quite effective in solving real problems.	autoregressive fractionally integrated moving average;bootstrapping (statistics);estimation theory	E. M. Silva;Glaura C. Franco;Valderio Anselmo Reisen;Frederico R. B. Cruz	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2005.10.007	econometrics;calculus;mathematics;mean squared error;statistics	ML	33.19128230051067	-22.127926558315874	81487
24896811388f1cd227d396165e49a6b63d14e37f	a study on image electronic money based on watermarking technique	image processing;image compression;image watermarking;digital image	This study introduces a technology that utilizes digital images as electronic money by inserting watermark into the images. Watermark technology assigns contents-ID to images and inserts the ID into the images in an unnoticeable way. The server that manages the issue and the usage of image electronic money (called ‘WaterCash' hereafter) stores contents-IDs to database and manage them as electronic money. WaterCash guarantees anonymity and prevents the forgery and modification of WaterCash based on semi-fragile watermarking technique. In addition, WaterCash is transferable and the illegal use of WaterCash can be prevented based on the watermarking technology. Because the watermarking technology used in this paper was designed to be robust to image compression but vulnerable to intentional or unintentional image processing, WaterCash is applied to JPEG-compressed images.		Jung-Soo Lee;Jong-Weon Kim;Kyu-Tae Kim;Jong-Uk Choi;Whoi-Yul Kim	2004		10.1007/978-3-540-30543-9_64	computer vision;image processing;image compression;computer science;digital image processing;multimedia;internet privacy;computer security;digital image	EDA	38.227853386543615	-11.94365525705625	81643
ef366665ebec550c0a1e553924c5875a24f7f745	a new rbdo method using adaptive response surface and first-order score function for crashworthiness design	bayesian metric;gaussian process;bias correction function;adaptive response surface;reliability based design optimization	This study presents a new Reliability-based Design Optimization method using adaptive response surface and first-order score function analysis for complex system design optimization considering the variability of design variables. The adaptive response surface using Bayesian metric and Gaussian process based model bias correction method, is developed to represent the true performance functions and replace the true limit state function. First-order score function analysis is exploited to compute the sensitivities of probabilistic responses with respect to the design variables, which are the mean values of the random variables. Numerical results indicate that the proposed methods can produce the best response surface and estimate the sensitivities of probabilistic responses accurately. The proposed methodology is demonstrated by a vehicle crashworthiness design optimization problem with full frontal and offset frontal impacts.	first-order predicate;response surface methodology	Lei Shi;Shih-Po Lin	2016	Rel. Eng. & Sys. Safety	10.1016/j.ress.2016.07.007	probabilistic-based design optimization;econometrics;mathematical optimization;gaussian process;mathematics;statistics	EDA	29.589450390736612	-16.336719252746374	82239
d5fa019541a85294aa788f93b3db773bfa4897b7	nhpp-based software reliability models using equilibrium distribution	em algorithm;software reliability	Non-homogeneous Poisson processes (NHPPs) have gained much popularity in actual software testing phases to estimate the software reliability, the number of remaining faults in software and the software release timing. In this paper, we propose a new modeling approach for the NHPP-based software reliability models (SRMs) to describe the stochastic behavior of software fault-detection processes. The fundamental idea is to apply the equilibrium distribution to the fault-detection time distribution in NHPP-based modeling. We also develop efficient parameter estimation procedures for the proposed NHPP-based SRMs. Through numerical experiments, it can be concluded that the proposed NHPP-based SRMs outperform the existing ones in many data sets from the perspective of goodness-of-fit and prediction performance. key words: software reliability, equilibrium distribution, EM algorithm, NHPP, real data analysis	estimation theory;expectation–maximization algorithm;experiment;list of software reliability models;markov chain;numerical analysis;software quality;software release life cycle;software reliability testing;software testing	Xiao Xiao;Hiroyuki Okamura;Tadashi Dohi	2012	IEICE Transactions		econometrics;expectation–maximization algorithm;computer science;software reliability testing;machine learning;software quality;statistics	ML	30.27414566964958	-18.60197614532865	82390
b3e1b99cd53f4a7deb152509711e8c3780e5f4da	parallel computing method of valuing for multi-asset european option	european option;convergence rate;monte carlo method;parallel computer;quasi monte carlo method;high dimension	A critical problem in Finance Engineering is to value the option and other derivatives securities correctly. The Monte Carlo method (MC) is an important one in the computation for the valuation of multiasset European option. But its convergence rate is very slow. So various quasi Monte Carlo methods and there relative parallel computing method are becoming an important approach to the valuing of multi-asset European option. In this paper, we use a number-theoretic method, which is a H-W method, to generate identical distributed point set in order to compute the value of the multi-asset European option. It turns out to be very effective, and the time of computing is greatly shortened. Comparing with other methods, the method computes less points and it is especially suitable for high dimension problem.	parallel computing	Weimin Zheng;Jiwu Shu;Xiaotie Deng;Yonggen Gu	2003		10.1007/3-540-44862-4_1	quasi-monte carlo method;econometrics;mathematical optimization;computer science;mathematics;rate of convergence;monte carlo integration;monte carlo methods for option pricing;statistics;monte carlo method	HPC	33.57992397551708	-16.722245280596475	82738
1af4288cec0ff4b939c1e08c455f06fc0a804631	weighted empirical likelihood estimates and their robustness properties	metodo estadistico;fonction vraisemblance;estimator robustness;erreur moyenne;methode parametrique;estimacion densidad;analisis datos;weighted likelihood;variable independante;maximum likelihood;normal distribution;variable aleatoire;fonction repartition;metodo parametrico;estimation densite;root mean square error;parametric method;estimation non parametrique;maximum vraisemblance;variable aleatoria;contaminated normal distribution;estimacion promedio;mean error;statistical method;error medio;curva gauss;funcion verosimilitud;vraisemblance empirique;non parametric estimation;density estimation;funcion distribucion;data analysis;distribution function;robustez estimador;estimation erreur;error estimation;methode statistique;statistical computation;estimacion error;calculo estadistico;random variable;loi normale;analyse donnee;calcul statistique;variable independiente;estimacion no parametrica;mean estimation;estimation moyenne;empirical likelihood;60e05;maximum likelihood method;statistical estimation;likelihood function;gaussian distribution;maxima verosimilitud;independent variable;robustesse estimateur	Maximum likelihood methods are by far the most popular methods for deriving statistical estimators. However, parametric likelihoods require distributional specifications. The empirical likelihood is a nonparametric likelihood function that does not require such distributional assumptions, but is otherwise analogous to its parametric counterpart. Both likelihoods assume that the random variables are independent with a common distribution. A nonparametric likelihood function for data that are independent, but not necessarily identically distributed is introduced. The contaminated normal density is used to compare the robustness properties of weighted empirical likelihood estimators to empirical likelihood estimators. It is shown that as the contamination level of the sample increases, the root mean squared error of the empirical likelihood estimator for the mean increases. Conversely, the root mean squared error of the weighted empirical likelihood estimator for the mean remains closer to the theoretical root mean squared error. © 2006 Elsevier B.V. All rights reserved.	mean squared error	N. L. Glenn;Yichuan Zhao	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.07.032	normal distribution;efficient estimator;econometrics;likelihood principle;score test;likelihood-ratio test;marginal likelihood;calculus;m-estimator;mathematics;mean squared error;restricted maximum likelihood;maximum likelihood;likelihood function;quasi-maximum likelihood;maximum likelihood sequence estimation;estimation theory;statistics	ML	32.352561719617746	-22.793613907367995	82848
237864974bcbcc21c14760a5e49b0cef03998dcb	simulated annealing model search for subset selection in screening experiments	model selection;nonregular factorial design;linear regression;grupo de excelencia;statistics design of experiments;ciencias basicas y experimentales;matematicas;facorial designs linear regression modeling selection stochastic models	The analysis of screening experiments based on nonregular designs can lead to a model selection problem in which the number of variables is large, the number of trials is small, and there are constraints on model structure. Common subset selection methods do not perform well in this setting. We propose a new approach particularly well suited to screening. The method uses an intentionally nonconvergent stochastic search to generate a large set of well-fitting models, each with the same number of variables. Model selection is then viewed as a feature extraction problem from this set. An easy-to-use graphical method and an automatic approach are proposed to determine the best models. Computer code and additional supplementary materials are available online.	experiment;simulated annealing	Mark A. Wolters;Derek Bingham	2011	Technometrics	10.1198/TECH.2011.08157	econometrics;mathematical optimization;linear regression;mathematics;model selection;statistics	NLP	28.99499536335907	-15.354579727535718	82914
7cbac9bf72a758b9889aa49c8826ea8fdb0a2091	reject option with multiple thresholds	estimation error	"""To this end, the so-called Bayes decision rule assigns each pattern x to the class for which the a posteriori probability P(u i Dx) is maximum. An error probability lower than the one provided by the above Bayes rule can be obtained using the so-called `rejecta option. Namely, the patterns that are the most likely to be misclassi""""ed are rejected (i.e., they are not classi""""ed); they are then handled by more sophisticated procedures (e.g., a manual classi""""cation is performed). However, handling high reject rates is usually too time-consuming for application purposes. Therefore, a trade-o! between error and reject is mandatory. The formulation of the best error-reject trade-o! and the related optimal reject rule was given by Chow [1]. According to Chow's rule, a pattern x is rejected if"""	best practice;rule 90	Giorgio Fumera;Fabio Roli;Giorgio Giacinto	2000	Pattern Recognition	10.1016/S0031-3203(00)00059-5	econometrics;computer science;statistics	ML	28.19190928494397	-19.15216349170298	83064
9b232ac01ccb2368d1ae1aabd1e94b82c67525d2	a numerical investigation of the accuracy of parametric bootstrap for discrete data	exact test;bootstrap;selected works;bepress;nuisance parameters;tests of non inferiority	Standard first order tests have size error that decreases as m - 1 / 2 where m is a measure of sample size. Parametric bootstrap tests use an exact calculation of the P -value, assuming nuisance parameters equal their null maximum likelihood estimates. It is commonly believed that their performance is driven by asymptotics, notwithstanding some confusion in the literature on asymptotic error rates.For simple discrete models, parametric bootstrap tests can be calculated explicitly rather than simulated. Moreover, their accuracy can also be calculated exactly as a function of the nuisance parameter. This article reports the results of an intensive numerical investigation of the accuracy of first order and parametric bootstrap based tests, firstly of treatment effect in clinical trials, and secondly of association in simple logistic regression. We conclude that bootstrap tests have asymptotic size error that decreases at rate O ( m - 1 ) but that their excellent small sample performance has little to do with asymptotics.	bootstrapping (statistics);discrete mathematics;numerical analysis	Chris J. Lloyd	2013	Computational Statistics & Data Analysis	10.1016/j.csda.2012.09.015	econometrics;mathematical optimization;mathematics;exact test;statistics	ML	30.1079356746047	-21.391611015204496	83248
915b1cedfab5521c94609368eff3fa4ea1313159	experimental study on the impact of robust watermarking on iris recognition accuracy	watermarking;robust watermarking;biometrics;iris recognition	Watermarking has been suggested as a means to improve security of biometric systems or to add additional functionalities to such systems. We experimentally investigate the impact of applying a set of blind robust watermarking schemes on the recognition performance of two iris recognition algorithms. We find that different watermarking schemes result in a very different amount of impact rendering the choice of a particular watermarking scheme an important issue to be considered in the investigated context.	algorithm;biometrics;digital watermarking;experiment;iris recognition;watermark (data file)	Jutta Hämmerle-Uhl;Karl Raab;Andreas Uhl	2010		10.1145/1774088.1774405	computer vision;digital watermarking;computer science;iris recognition;internet privacy;computer security;biometrics	EDA	37.1807824794692	-11.455106430065129	83327
5b0384a259f91e6e724256dfcbd62895e8b4453c	multinomial selection for comparison with a standard	multinomial distribution;metodo estadistico;selection problem;problema seleccion;small sample;probability;multinomial;methode parametrique;loi probabilite;ley probabilidad;metodo parametrico;parametric method;simulacion numerica;simulation;estadistica rango;statistical method;output analysis;estimation parametrique;62f07;grand echantillon;62k99;loi multinomiale;methode statistique;90b50;probability distribution;probabilidad;simulation numerique;probabilite;rank statistic;statistique rang;pequena muestra;large sample;ley multinomial;petit echantillon;comparison with a standard;numerical simulation;probleme selection	The multinomial selection problem is considered under the formulation of comparison with a standard, where each system is required to be compared to a single system, referred to as a “standard,” as well as to other alternative systems. The goal is to identify systems that are better than the standard, or to retain the standard when it is equal to or better than the other alternatives in terms of the probability to generate the largest or smallest performance measure. We derive new multinomial selection procedures for comparison with a standard to be applied in different scenarios, including exact small-sample procedure and approximate large-sample procedure. Empirical results and the proof are presented to demonstrate the statistical validity of our procedures. The tables of the procedure parameters and the corresponding exact probability of correct selection are also provided.	multinomial logistic regression	Shing Chih Tsai;Jung Wei	2011	Communications in Statistics - Simulation and Computation	10.1080/03610918.2011.560729	computer simulation;econometrics;calculus;mathematics;multinomial distribution;statistics	Web+IR	31.531905797199578	-21.32963641219601	83358
e5fab71dcb5e8626ebf767db462833e671853c72	the mathematical modeling of heuristics	time complexity;mathematical model	Two models of heuristics have been suggested in previous studies of A* tree-searching. The models are used to determine what mathematical properties heuristics must have if A* is to have polynomial, versus exponential, average asymptotic time complexity. In the EC model polynomial A* complexity is associated with logarithmic heuristic error. In the NC model it is associated with a concentration ofh-values near a rapidly growing central function; logarithmic clustering is adequate but more deviation is allowed if the central function grows fast enough. This paper introduces a third model based on approximating heuristic values with normally distributed random variables — the ND model. The ND model predicts polynomial A* complexity when the mean ofh-values is rapidly growing and variance is logarithmic. The three models are compared. They are tested by attempting to explain the success of weighting heuristics to reduce A* complexity. The EC model is found inadequate; the NC and ND models reveal situations in which weighting changes time complexity from exponential to polynomial. These results are largely corroborated by statistical data from a particular problem domain.	a* search algorithm;asymptotic computational complexity;cluster analysis;heuristic (computer science);mathematical model;norsk data;polynomial;problem domain;time complexity	Henry W. Davis;Stephen V. Chenoweth	1992	Annals of Mathematics and Artificial Intelligence	10.1007/BF01543476	time complexity;mathematical optimization;combinatorics;discrete mathematics;computer science;machine learning;mathematical model;mathematics	AI	28.74183040450689	-12.194209386045074	83494
cd6bd28d5d455f58c8421fdfb63a3f733c4b5371	a new weibull-pareto distribution: properties and applications	moment;62f10;pareto distribution;weibull g class;likelihood estimation;hazard function;62n05;60e05	Many distributions have been used as lifetime models. In this article, we propose a new threeparameter Weibull–Pareto distribution, which can produce the most important hazard rate shapes, namely, constant, increasing, decreasing, bathtub, and upsidedown bathtub. Various structural properties of the new distribution are derived including explicit expressions for the moments and incomplete moments, Bonferroni and Lorenz curves, mean deviations, mean residual life, mean waiting time, and generating and quantile functions. The Rényi and q entropies are also derived. We obtain the density function of the order statistics and their moments. The model parameters are estimated by maximum likelihood and the observed information matrix is determined. The usefulness of the new model is illustrated by means of two real datasets on Wheaton river flood and bladder cancer. In the two applications, the new model provides better fits than the Kumaraswamy– Pareto, beta-exponentiated Pareto, beta-Pareto, exponentiated Pareto, and Pareto models.	fits;formation matrix;lorenz cipher;observed information;pareto efficiency	M. H. Tahir;Gauss M. Cordeiro;Ayman Alzaatreh;Muhammad Mansoor;Muhammad Zubair	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.948190	econometrics;mathematical optimization;exponentiated weibull distribution;pareto distribution;failure rate;mathematics;moment;statistics	ML	31.408226962185935	-19.509143951278357	83553
19b20b02653c7c49059afe267f8d02b8af066e84	global sensitivity analysis of nonlinear mathematical models — an implementation of two complementing variance-based algorithms		A new approach for a global sensitivity analysis of nonlinear mathematical models is presented using the information provided by two complementing variance-based methods. As a first step, the model is evaluated applying a shared sampling strategy for both methods based on Sobol's quasi-random sequences. Then, total sensitivity indices are estimated in a second step using the Sobol'-Saltelli method whereas first-order sensitivity indices are concurrently computed using a modified version of the well-known Fourier Amplitude Sensitivity Test. Although the analysis is focused on the calculation of total sensitivity indices, first-order sensitivity indices and thus information about the main effects of model input parameters can be obtained at no extra computational cost. Another advantage of this approach is that data of previous model evaluations can be reused for a new, more precise sensitivity analysis. The capability and performance of the method is investigated using an analytical test function.	algorithm;algorithmic efficiency;computation;distribution (mathematics);first-order predicate;low-discrepancy sequence;mathematical model;nonlinear system;sampling (signal processing);whole earth 'lectronic link	Thomas Henkel;Heike Wilson;Wilfried Krug	2012	Proceedings Title: Proceedings of the 2012 Winter Simulation Conference (WSC)			EDA	27.52397499300851	-17.244513515303964	83562
32dd3f9c393e58d2758398c03026c66e63ccb187	extension of the noise propagation matrix method for higher mode solutions	noise propagation;higher mode;monte carlo	The noise propagation matrix method (NPMM) has been extended to get higher mode solutions. Previous studies show that the NPMM can be used to compute the dominance ratio of a system. It is essentially the same as the Coarse Mesh Projection Method (CMPM), both of which use the noise propagation matrix (NPM) to determine the dominance ratio, either after finishing the Monte Carlo simulation or on-the-fly during the simulation. Since only the fundamental fission source information is explicitly utilized while the higher mode information is implicitly contained in the statistical noises, the NPMM can usually only give an approximate estimation of the dominance ratio after thousands of cycles. In this study, the NPMM is extended by simulating the higher modes explicitly, so that the dominance ratio estimation can be more accurate and efficient. Besides, the higher mode solutions can be obtained at the same time with good accuracy and efficiency.	matrix method;software propagation	Peng Zhang;Hyunsuk Lee;Deokjung Lee	2017	J. Comput. Physics	10.1016/j.jcp.2017.05.007	econometrics;mathematical optimization;simulation;mathematics;monte carlo method	Theory	32.442526190269106	-15.191401220422247	83653
502668a4a74bd90e58ca260cbc2ae2ad0920f594	adaptive survey designs for sampling rare and clustered populations	complete allocation stratified sampling;adaptive cluster sampling;adaptive two stage sequential sampling;stratified sampling;two phase sampling	Designing an efficient large-area survey is a challenge, especially in environmental science when many populations are rare and clustered. Adaptive and unequal probability sampling designs are appealing when populations are rare and clustered because survey effort can be targeted to subareas of high interest. For example, higher density subareas are usually of more interest than lower density areas. Adaptive and unequal probability sampling offer flexibility for designing a long term survey because they can accommodate changes in survey objectives, changes in underlying environmental habitat, and changes in species-habitat models. There are many different adaptive sampling designs including adaptive cluster sampling, two-phase stratified sampling, two-stage sequential sampling, and complete allocation stratified sampling. Sample efficiency of these designs can be very high compared with simple random sampling. Large gains in efficiency can be made when survey effort is targeted to the subareas of the study site where there are clusters of individuals from the underlying population. These survey methods work by partitioning the study area in some way, into strata, or primary sample units, or in the case of adaptive cluster sampling, into networks. Survey effort is then adaptively allocated to the strata or primary unit where there is some indication of higher species counts. Having smaller, and more numerous, strata improves efficiency because it allows more effective targeting of the adaptive, second-phase survey effort.	adaptive sampling;habitat;population;sampling (signal processing);stratified sampling;two-phase commit protocol	Jennifer A. Brown;M. Salehi MohammadSalehi;Mohammad Moradi;Bardia Panahbehagh;David R. Smith	2013	Mathematics and Computers in Simulation	10.1016/j.matcom.2012.09.008	sampling;econometrics;mathematical optimization;simple random sample;nonprobability sampling;mathematics;cluster sampling;stratified sampling;lot quality assurance sampling;statistics	HCI	26.097036295103276	-16.795085854558863	83654
942023500c27597d06b1263d8abaaa371ded6799	sampling and concentration values of incomplete bibliographies	infometrie;scientometrics;informetrics;measurement techniques;concentracion;loi;echantillonnage;bibliografia;bibliography;bibliographies;inequality mathematics;law;sampling;mathematical formulas;scientometria;bibliographie;scientometrie;infometria;muestreo;concentration;stratified sampling	This paper studies concentration aspects of bibliographies. More in particular we study the impact of incompleteness of such a bibliography on its concentration values (i.e. its degree of inequality of production of its sources). Incompleteness is modelled by sampling in the complete bibliography. The model is general enough to comprise truncation of a bibliography as well as a perfectly stratified sample on sources or items. In all cases we prove that the sampled bibliography (or incomplete one) has a higher concentration value than the complete one. These models hence shed some light on the measurement of production inequality in incomplete bibliographies. 'Permanent address Acknowledgement : the author is grateful to prof Dr. R. Rousseau for interesting discussions on the topic of this paper.	bibliographic index;gibbs sampling;sampling (signal processing);social inequality;stratified sampling;truncation	Leo Egghe	2002	JASIST	10.1002/asi.10033	sampling;formula;scientometrics;computer science;bibliography;stratified sampling;concentration;law;statistics	DB	35.14869421076158	-18.94897661258825	83668
0396fd4005761956b17a48a9ca5cc99656245692	extensions: an efficient importance sampling method for rare event simulation in large scale tandem	transition probability;rare event simulation;large scale system;large scale;tandem queue;importance sampling	In this paper, we present a variance minimization (VM) procedure for rare event simulation in tandem queueing networks. We prove that the VM method can produce a zero variance. The VM method is suitable to compute optimal importance sampling (IS) parameters for small scale tandem networks. For large scale tandem networks we propose a sub-optimal IS (SOIS) method, which projects the optimal biased transition probabilities of the corresponding small scale system into those of a large scale system. In other words, we establish an efficient IS method for a large scale system by zooming into a small scale system and then projecting our findings into the large scale system. The numerical results show that our SOIS method can produce accurate results with very short CPU time, while many other methods often require much longer.	central processing unit;computation;extreme value theory;importance sampling;markov chain;numerical analysis;quality of service;sampling (signal processing);simulation;software development;state space;tandem computers	Lei Wei;Honghui Qi	2002			markov chain;simulation;importance sampling;database;mathematics;statistics	HPC	30.286612461374038	-15.259134101138436	83729
73046b0c525314d088e6fca1f5ebfb213722ef70	delivery: an open-source model-based bayesian seismic inversion program	computadora;tratamiento datos;computers;roche magasin;reservoirs;software;markov chain analysis;computer languages;seismic methods;geostatistique;saturacion;geoestadistica;probability;sand;arena;logiciel;ordinateur;log analysis;hydrocarbure;methode monte carlo;data processing;geostatistics;traitement donnee;seismic response;problema inverso;computer programs;inverse problem;markov chain monte carlo;markov chain monte carlo methods;inverse analysis;reservoir rocks;probabilidad;bruit;sable;metodo sismico;cluster system;probabilite;hydrocarbons;langage programmation;analyse chaine markov;reponse sismique;roca reservorio;stochastic model;sand bodies;programa computador;methode sismique;probleme inverse;likelihood function;saturation;corps sableux;modele stochastique;reservoir;monte carlo analysis;programme ordinateur;hidrocarburo;noise;open source;stochastic models	We introduce a new open–source toolkit for model–based Bayesian seismic inversion called Delivery. The prior model in Delivery is a trace–local layer stack, with rock physics information taken from log analysis and layer times initialised from picks. We allow for uncertainty in both the fluid type and saturation in reservoir layers: variation in seismic responses due to fluid effects are taken into account via Gassman’s equation. Multiple stacks are supported, so the software implicitly performs a full AVO inversion using approximate Zoeppritz equations. The likelihood function is formed from a convolutional model with specified wavelet(s) and noise level(s). Uncertainties and irresolvabilities in the inverted models are captured by the generation of multiple stochastic models from the Bayesian posterior (using Markov Chain Monte Carlo methods), all of which acceptably match the seismic data, log data, and rough initial picks of the horizons. Post-inversion analysis of the inverted stochastic models then facilitates the answering of commercially useful questions, e.g. the probability of hydrocarbons, the expected reservoir volume and its uncertainty, and the distribution of net sand. Delivery is written in java, and thus platform independent, but the SU data backbone makes the inversion particularly suited to Unix/Linux environments and cluster systems.	approximation algorithm;astrophysical virtual observatory;clustered file system;internet backbone;java;linux;log analysis;markov chain monte carlo;monte carlo method;open-source software;stack overflow;stochastic process;unix;wavelet	James Gunning;Michael E. Glinsky	2004	Computers & Geosciences	10.1016/j.cageo.2003.10.013	seismic inversion;simulation;data processing;artificial intelligence;stochastic modelling;mathematics;statistics;geostatistics;reservoir	ML	36.6540699689234	-22.002160051893895	83793
53d8544e4f784f43a58af177b32bc8ce9a4206c3	using simulation to study statistical tests for arrival process and service time models for service systems	queueing theory;service industries;statistical testing;stochastic processes;simulation	When fitting queueing models to service system data, it can be helpful to perform statistical tests to confirm that the candidate model is appropriate. The Kolmogorov-Smirnov (KS) test can be used to test whether a sample of interarrival times or service times can be regarded as a sequence of i.i.d. random variables with a continuous cdf, and also to test a nonhomogeneous Poisson Process (NHPP). Using extensive simulation experiments, we study the power of various alternative KS tests based on data transformations. Among available alternative tests, we find the one with the greatest power in testing a NHPP. Furthermore, we devise a new method to test a sequence of i.i.d. random variables with a specified continuous cdf; it first transforms a given sequence to a rate-1 Poisson process (PP) and then applies the existing KS test of a PP. We show that it has greater power than direct KS tests.	experiment;kolmogorov complexity;queueing theory;simulation;windows legacy audio components	Song-Hee Kim;Ward Whitt	2013	2013 Winter Simulations Conference (WSC)		stochastic process;econometrics;statistical hypothesis testing;computer science;mathematics;queueing theory;statistics	Metrics	31.238602314918896	-18.901496949195693	83858
bff227a18041156b81220739bf51660831b439b2	interactive genetic algorithms with variational population size	building block;evolutionary design;population size;user fatigue;clustering;interactive genetic algorithm;building blocks;interactive genetic algorithms;evolutionary process;variational population size;human computer interface	Traditional interactive genetic algorithms often have a small population size because of a limited human-computer interface and user fatigue, which restrict these algorithms’ performances to some degree. In order to improve these algorithms’ performances and alleviate user fatigue effectively, we propose an interactive genetic algorithm with variational population size in this paper. In the algorithm, the whole evolutionary process is divided into two phases, i.e. fluctuant phase and stable phase of the user’s cognition. In fluctuant phase, a large population is adopted and divided into several coarse clusters according to the similarity of individuals. The user only evaluates these clusters’ centers, and the other individuals’ fitness is estimated based on the acquired information. In stable phase, the similarity threshold changes along with the evolution, resulting in refined clustering of the population. In addition, elitist individuals are reserved to extract building blocks. The offspring is generated based on these building blocks, leading to a reduced population size. The proposed algorithm was applied to a fashion evolutionary design system, and the results validated its efficiency.	calculus of variations;cluster analysis;cognition;continuous design;genetic algorithm;human–computer interaction;in-game advertising;interactive evolutionary computation;performance;the offspring;variational principle;virtual private server	Jie Ren;Dun-Wei Gong;Xiaoyan Sun;Jie Yuan;Ming Li	2009		10.1007/978-3-642-04020-7_8	mathematical optimization;evolutionary music;population size;simulation;interactive evolutionary computation;cultural algorithm;computer science;bioinformatics;artificial intelligence;machine learning;cluster analysis	AI	26.16858370080041	-10.898028285876252	84094
5f4ac3fa6d1d5a25b0aac64eb535fdc00985a12f	efficient importance sampling for reduced form models in credit risk	last section;large loss;theoretical finding;efficient importance;stochastic credit risk model;numerical result;asymptotically optimal importance;reduced form model;specific asymptotic regime;logarithmic asymptote;estimation theory;importance sampling;credit risk;risk management;stochastic processes;probability;finance	In this paper we study the problem of estimating probability of large losses in the framework of doubly stochastic credit risk models. We derive a logarithmic asymptote for the probability of interest in a specific asymptotic regime and propose an asymptotically optimal importance sampling algorithm for efficiently estimating the same. The numerical results in the last section corroborate our theoretical findings.	asymptote;asymptotically optimal algorithm;doubly stochastic model;financial risk modeling;importance sampling;numerical analysis;sampling (signal processing)	Achal Bassamboo;Sachin Jain	2006	Proceedings of the 2006 Winter Simulation Conference		stochastic process;econometrics;risk management;credit risk;importance sampling;probability;mathematics;estimation theory;applied probability;statistics	ML	32.51610277388961	-17.170113334713005	84169
9772b18b0f237aaa23fdd2d5d8680dc1ca9948b5	stochastic processes via the pathway model	data analysis;pathway idea;model building;pathway model solar neutrinos;input output type stochastic models;thicker or thinner tailed models	After collecting data from observations or experiments, the next step is to analyze the data to build an appropriate mathematical or stochastic model to describe the data so that further studies can be done with the help of the model. In this article, the input-output type mechanism is considered first, where reaction, diffusion, reaction-diffusion, and production-destruction type physical situations can fit in. Then techniques are described to produce thicker or thinner tails (power law behavior) in stochastic models. Then the pathway idea is described where one can switch to different functional forms of the probability density function through a parameter called the pathway parameter. The paper is a continuation of related solar neutrino research published previously in this journal.	continuation;experiment;gene regulatory network;stochastic process;tails	Arak M. Mathai;Hans J. Haubold	2015	Entropy	10.3390/e17052642	econometrics;simulation;model building;artificial intelligence;mathematics;data analysis;physics;quantum mechanics;statistics	SE	34.39151502285832	-15.676268504320198	84227
1c52a774565d5466fcc7d34210ada65b6ef82b80	estimating the approximation error when fixing unessential factors in global sensitivity analysis	dimensionalidad;modelizacion;analisis sensibilidad;optimisation;sensitivity analysis by groups;analisis factorial;approximation error;optimizacion;numerical method;fonction analytique;analytical solution;global sensitivity analysis;dimensionality;error aproximacion;analyse globale;solucion analitica;modelisation;analyse factorielle;estimation erreur;metodo numerico;factor analysis;error estimation;sensitivity analysis;dimensionnalite;estimacion error;sensitivity indices;analyse sensibilite;funcion analitica;sobol method;optimization;solution analytique;analytic solution;modeling;analytical function;methode numerique;erreur approximation;global analysis	One of the major settings of global sensitivity analysis is that of fixing non influential factors, in order to reduce the dimensionality of a model. However, this is usually done without knowing the magnitude of the approximation error being produced. This paper presents and proves a new theorem for the estimation of the average approximation error generated when fixing non influential factors. A simple function where analytical solutions are available is used as test case to illustrate the theorem; furthermore this function is used to show the theorem applicability to sensitivity analysis by groups of factors. Improved formulas for the estimation of sensitivity indices are presented; such formulas allow for more accurate estimations at a lower computational cost with respect to the original method of Sobol&#8217;. URI: Authors: SOBOL' Ilya TARANTOLA Stefano [1] GATELLI Debora KUCHERENKO Sergei MAUNTZ Wolfgang Publication Year: 2007 Type: Articles in Journals Publisher: ELSEVIER SCI LTD DOI: 10.1016/j.ress.2006.07.001 [2] Citation: RELIABILITY ENGINEERING & SYSTEM SAFETY p. 957-960 vol. 92 Source URL: https://ec.europa.eu/jrc/en/publication/articles-journals/estimating-approximation-error-when-fixing-unessential-factors -global-sensitivity-analysis Links [1] https://ec.europa.eu/jrc/en/person/stefano-tarantola [2] http://dx.doi.org/10.1016/j.ress.2006.07.001	algorithmic efficiency;approximation error;reliability engineering;test case;uniform resource identifier	Ilya M. Sobol;Stefano Tarantola;Debora Gatelli;Sergei S. Kucherenko;Wolfgang Mauntz	2007	Rel. Eng. & Sys. Safety	10.1016/j.ress.2006.07.001	econometrics;closed-form expression;approximation error;calculus;mathematics;statistics	AI	32.264249007090285	-19.29244545750478	84339
e2e247fc0a98afb7b840cff9f4e748166ee48a57	first-order uncertainty analysis using algorithmic differentiation of morphodynamic models	morphodynamics;algorithmic differentiation;uncertainty analysis;sensitivity analysis;monte carlo simulation;process based modelling	We present here an efficient first-order second moment method using Algorithmic Differentiation (FOSM/AD) which can be applied to quantify uncertainty/sensitivities in morphodynamic models. Changes with respect to variable flow and sediment input parameters are estimated with machine accuracy using the technique of Algorithmic Differentiation (AD). This method is particularly attractive for process-based morphodynamic models like the Telemac-2D/Sisyphe model considering the large number of input parameters and CPU time associated to each simulation. The FOSM/AD method is applied to identify the relevant processes in a trench migration experiment (van Rijn, 1987). A Tangent Linear Model (TLM) of the Telemac-2D/Sisyphe morphodynamic model (release 6.2) was generated using the AD-enabled NAG Fortran compiler. One single run of the TLM is required per variable input parameter and results are then combined to calculate the total uncertainty. The limits of the FOSM/AD method have been assessed by comparison with Monte Carlo (MC) simulations. Similar results were obtained assuming small standard deviation of the variable input parameters. Both settling velocity and grain size have been identified as the most sensitive input parameters and the uncertainty as measured by the standard deviation of the calculated bed evolution increases with time. & 2016 Published by Elsevier Ltd.	automatic differentiation;central processing unit;compiler;first-order predicate;first-order second-moment method;fortran;linear model;material point method;monte carlo method;nonlinear system;parameter (computer programming);python;simulation;spatial variability;velocity (software development);velocity obstacle	Catherine Villaret;Rebekka Kopmann;David Wyncoll;Jan Riehme;Uwe Merkel;Uwe Naumann	2016	Computers & Geosciences	10.1016/j.cageo.2015.10.012	econometrics;mathematical optimization;simulation;uncertainty analysis;computer science;mathematics;sensitivity analysis;statistics;monte carlo method	SE	31.295134620918667	-15.978019494862147	84507
dade1425a6139c100b0d4802058d217978f5e2b4	a complex mathematical modeling method for biological objects. modeling the tundra community	imitational approach;tundra community model;complex study;tundra animal population;biological object;complete set;animal population fluctuation;complex mathematical modeling method;huge model;biological information;ecologo-biological system;computational study	We consider a method for mathematical modeling of ecologo–biological systems based on computational studies that unites formal and informal, analytic and imitational approaches. The method is based on complex studies that include a complete set of operations, from filtering biological information to constructing a set of interrelated models, including simplified ones, that admit an analytic (parametric) study. This lets us overcome the disadvantages of purely imitational approaches: they are restricted by numerical experiments and often have huge models. The proposed approach has been used to analyze animal population fluctuations with the tundra community model “vegetation–lemmings–arctic foxes.” As a result of our studies, we formulate hypotheses on leading mechanisms that determine the fluctuations of tundra animal populations. DOI: 10.1134/S0005117913020069	biological system;computation;experiment;lemmings;mathematical model;numerical analysis;population dynamics	V. N. Glushkov;D. A. Sarancha	2013	Automation and Remote Control	10.1134/S0005117913020069	mathematical optimization;simulation;computer science;engineering;artificial intelligence;mathematics;statistics	Comp.	34.625354877158465	-14.28441714495723	84932
b4a1d8b9df7a1b15bf7c560ed38b25eab5137f77	two-sided tolerance intervals in the exponential case: corrigenda and generalizations	bayes estimation;theorie echantillonnage;teoria muestreo;parametre dispersion;sample size;exponential distribution;cuantila;scale parameter;intervalo confianza;analisis datos;bayesian approach;ley exponencial;censored sample;fonction repartition;loi exponentielle;tamano muestra;62f25;estimation non parametrique;donnee censuree;taille echantillon;ecuesta estadistica;ley weibull;satisfiability;statistical model;estimation parametrique;echantillon censure;non parametric estimation;weibull distribution;funcion distribucion;sample survey;data analysis;confidence interval;estimacion bayes;distribution function;censored data;62g15;tolerancia;intervalle confiance;statistical computation;calculo estadistico;62d05;modele statistique;62n99;tolerance interval;analyse donnee;modelo estadistico;calcul statistique;tolerance;quantile;intervalle tolerance;estimacion no parametrica;estimation statistique;estimacion estadistica;60e05;statistical estimation;loi weibull;sondage statistique;62n01;estimation bayes;muestra censurada;parametro dispersion;sampling theory	Exact two-sided guaranteed-coverage tolerance intervals for the exponential distribution which satisfy the traditional ''equal-tailedness'' condition are derived in the failure-censoring case. The available empirical information is provided by the first r ordered observations in a sample of size n. A Bayesian approach for the construction of equal-tailed tolerance intervals is also proposed. The degree of accuracy of a given tolerance interval is quantified. Moreover, the number of failures needed to achieve the desired accuracy level is predetermined. The Bayesian perspective is shown to be superior to the frequentist viewpoint in terms of accuracy. Extensions to other statistical models are presented, including the Weibull distribution with unknown scale parameter. An alternative tolerance interval which coincides with an outer confidence interval for an equal-tailed quantile interval is also examined. Several important computational issues are discussed. Three censored data sets are considered to illustrate the results developed.	time complexity	Arturo J. Fernández	2010	Computational Statistics & Data Analysis	10.1016/j.csda.2009.07.016	sample size determination;exponential distribution;statistical model;weibull distribution;econometrics;quantile;confidence interval;scale parameter;bayesian probability;survey sampling;distribution function;calculus;mathematics;tolerance interval;data analysis;credible interval;censoring;statistics;satisfiability	Theory	32.288136148154166	-21.86293114665042	85056
e19a6576c731538552234b34d6c47b0059402e85	parallel exact sampling and evaluation of gaussian markov random fields	parallel computing;linear algebra;algoritmo paralelo;15xx;sampler;metropolis algorithm;gaussian field;campo gaussiano;symmetric positive definite;chaine markov;cadena markov;parallel algorithm;metodo monte carlo;gaussian markov random fields;65c05;modelo markov;analisis datos;algorithme metropolis hastings;metropolis hastings;echantillonnage;62e17;methode monte carlo;gaussian markov random field;calculo automatico;matriz simetrica;65c40;computing;algorithme parallele;champ gaussien;symmetric matrix;sampling;calcul automatique;speed;data analysis;60g60;enrejado;62m40;markov model;campo aleatorio;matrice definie positive;positive definite matrix;treillis;markov chain monte carlo;algorithme metropolis;algoritmo metropolis hastings;vitesse deplacement;algebre lineaire;monte carlo method;velocidad desplazamiento;statistical computation;calculo estadistico;parallel computer;algebra lineal;analyse donnee;matrice symetrique;muestreador;calcul statistique;06bxx;60j10;modele markov;estimation statistique;matriz definida positiva;muestreo;estimacion estadistica;echantillonneur;statistical estimation;metropolis hastings algorithm;spatial gaussian markov random field model;exact sampling;champ aleatoire;algoritmo metropolis;lattice;markov chain;random field	Markov chain Monte Carlo algorithms are computationally expensive for large models. Especially, the so-called one-block Metropolis–Hastings (M–H) algorithm demands large computational resources, and parallel computing seems appealing. A parallel one-block M–H algorithm for latent Gaussian Markov random field (GMRF) models is introduced. Important parts of this algorithm are parallel exact sampling and evaluation of GMRFs. Parallelisation is achieved with parallel algorithms from linear algebra for sparse symmetric positive definite matrices. The parallel GMRF sampler is tested for GMRFs on lattices and irregular graphs, and gives both good speed-up and good scalability. The parallel one-block M–H algorithm is used to make inference for a geostatistical GMRF model with a latent spatial field of 31,500 variables. © 2006 Elsevier B.V. All rights reserved.	analysis of algorithms;computation;computational resource;linear algebra;markov chain monte carlo;markov random field;metropolis;monte carlo method;parallel algorithm;parallel computing;sampling (signal processing);scalability;sparse matrix	Ingelin Steinsland	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.01.013	metropolis–hastings algorithm;combinatorics;linear algebra;calculus;mathematics;statistics	AI	34.52734127137581	-23.24378717152083	85092
2b1fb39231e5f2bc20c7427e5d72eb64264debad	restricted subset selection and its application	jos;simulation;conceptual modellling;journal of simulation;output analysis;design and analysis of simulation experimetns;input modelling;applications of simulation and modelling;simulation visualisation;subset selection;simulation tutorials;parallel and distributed simulation;discrete event simulation	This paper develops procedures for selecting a set of normal populations with unknown means and unknown variances such that the final subset of selected populations satisfies the following requirement: with probability at least P*, the selected subset will contain a population or ‘only and all’ of those populations whose mean or means are within a value of d* from the smallest mean. The size of the selected subset is random; however, at most, m populations will be chosen. A restricted subset attempts to exclude populations that deviate more than d* from the smallest mean. Here P*, d*, and m are user-specified parameters. These procedures can be used when the unknown variances across populations are unequal. We then extend the sequentialized procedure to perform a selection with constraints. An experimental performance evaluation demonstrates the validity and efficiency of these restricted-subset-selection procedures. Journal of Simulation (2010) 4, 116–127. doi:10.1057/jos.2009.18; published online 13 November 2009	apply;boyce–codd normal form;computation;mi1;marginal model;performance evaluation;population;requirement;simulation	E. J. Chen	2010	J. Simulation	10.1057/jos.2009.18	simulation;computer science;discrete event simulation;machine learning	Vision	30.16272092668178	-21.01386835870072	85142
8a3c0f766c683c9ed76df73afd3673f52fe1de12	streamflow rating uncertainty: characterisation and impacts on model calibration and performance	nadaraya watson;uncertainty;stage discharge;journal article;ensemble;rating curve;heteroscedasticity;calibration	Common streamflow gauging procedures require assumptions about the stage-discharge relationship (the ‘rating curve’) that can introduce considerable uncertainties in streamflow records. These rating uncertainties are not usually considered fully in hydrological model calibration and evaluation yet can have potentially important impacts. We analysed streamflow gauge data and conducted two modelling experiments to assess rating uncertainty in operational rating curves, its impacts on modelling and possible ways to reduce those impacts. We found clear evidence of variance heterogeneity (heteroscedasticity) in streamflow estimates, with higher residual values at higher stage values. In addition, we confirmed the occurrence of streamflow extrapolation beyond the highest or lowest stage measurement in many operational rating curves, evenwhen these were previously flagged as not extrapolated. The first experiment investigated the impact on regional calibration/evaluation of: (i) using two streamflow data transformations (logarithmic and square-root), compared to using non-transformed streamflow data, in an attempt to reduce heteroscedasticity and; (ii) censoring the extrapolated flows, compared to no censoring. Results of calibration/evaluation showed that using a square-root transformed streamflow (thus, compromising weight on high and low streamflow) performed better than using non-transformed and log-transformed streamflow. Also, surprisingly, censoring extrapolated streamflow reduced rather than improved model performance. The second experiment investigated the impact of rating curve uncertainty on catchment calibration/evaluation and parameter estimation. A Monte-Carlo approach and the nonparametric Weighted Nadaraya-Watson (WNW) estimator were used to derive streamflow uncertainty bounds. These were later used in calibration/evaluation using a standard Nash-Sutcliffe Efficiency (NSE) objective function (OBJ) and a modified NSE OBJ that penalised uncertain flows. Using square-root transformed flows and the modified NSE OBJ considerably improved calibration and predictions, particularly for mid and low flows, and there was an overall reduction in parameter	bowyer–watson algorithm;censoring (statistics);code;discharger;estimation theory;experiment;extrapolation;incidence matrix;informatics;kerry mitchell;loss function;ming li;monte carlo method;nash equilibrium;network search engine;occam's razor;operational system;optimization problem;typset and runoff;warren abstract machine;wavefront .obj file;while	Jorge L. Peña-Arancibia;Yongqiang Zhang;Daniel E. Pagendam;Neil R. Viney;Julien Lerat;Albert I. J. M. van Dijk;Jai Vaze;Andrew J. Frost	2015	Environmental Modelling and Software	10.1016/j.envsoft.2014.09.011	ensembl;econometrics;rating curve;calibration;uncertainty;hydrology;mathematics;heteroscedasticity;statistics	AI	26.282715369823343	-17.940448335877196	85210
2c2303161bf7bc5c3b8e848a31d7808f9875ba50	phase i distribution-free analysis of multivariate data	statistical process control;multivariate signed ranks;nonparametric methods;control charts;change point detection;lasso	AbstractIn this study, a new distribution-free Phase I control chart for retrospectively monitoring multivariate data is developed. The suggested approach, based on the multivariate signed ranks, can be applied to individual or subgrouped data for detection of location shifts with an arbitrary pattern (e.g. isolated, transitory, sustained, progressive, etc.). The procedure is complemented with a LASSO-based post-signal diagnostic method for identification of the shifted variables. A simulation study shows that the method compares favorably with parametric control charts when the process is normally distributed, and largely outperforms other multivariate nonparametric control charts when the process distribution is skewed or heavy-tailed. An R package can be found in the supplementary material.		Giovanna Capizzi;Guido Masarotto	2017	Technometrics	10.1080/00401706.2016.1272494	econometrics;control chart;lasso;data mining;mathematics;change detection;statistical process control;statistics	Theory	28.18770462611563	-20.249522945556944	85230
ef799f5c091b5030c89e381dc7f6eb1ba0ea8aba	modelling credit grade migration in large portfolios using cumulative t-link transition models	logistic;cumulative link;heavy tailed;markov processes;probit	For a credit portfolio, we are often interested in modelling the migration of accounts between credit grades over time. For a large retail portfolio, data on credit grade migration may be available only in the form of a series of (typically monthly) population transition matrices representing the gross flow of accounts between each pair of credit grades in the given time period. The challenge is to model the transition process on the basis of these aggregate flow matrices. Each row of an observed transition matrix represents a sample from an ordinal probability distribution. Following Malik and Thomas (2012), Feng et al (2008) and McNeil and Wendin (2006), we assume a cumulative link model for these ordinal distributions. Common choices of link function are based on the normal (probit link) or logistic distributions, but the fit to observed data can be poor. In this paper, we investigate the fit of alternative link specifications based on the t-distribution. Such distributions arise naturally when modelling data which arise through aggregating an inhomogeneous sample of obligors, by combining a simple structural-type model for credit migration at the obligor level, with a suitable mixing distribution to model the variability between obligors.		Jonathan J. Forster;Matteo Buzzacchi;Agus Sudjianto;Risa Nagao	2016	European Journal of Operational Research	10.1016/j.ejor.2016.03.017	financial economics;logistics;econometrics;economics;mathematics;markov process;probit;statistics	AI	26.396565345126913	-21.198743381283336	85250
c3cdd9b378edf743f7740fa56524cc0239b907ee	an extended class of minimax generalized bayes estimators of regression coefficients	modelo lineal generalizado;minimaxity;bayes estimation;hypergeometric function;62j12;metodo estadistico;analyse multivariable;ridge regression;multivariate analysis;shrinkage estimation;coeficiente regresion;modele lineaire generalise;regresion ridge;perte quadratique;fonction monotone;metodo minimax;primary;modele lineaire;minimax method;shrinkage estimator;shrinkage estimators;statistical method;modelo lineal;generalized bayes estimators;statistical regression;62jxx;primary 62c20;teoria decision;funcion monotona;bayes estimator;decision estadistica;estimacion bayes;regression pseudo orthogonale;regression;secondary 62j07;estimation erreur;estimateur retrecissement;unknown variance;error estimation;methode statistique;theorie decision;regression minimaxity shrinkage estimators generalized bayes estimators invariant loss unknown variance hypergeometric function;regresion estadistica;secondary;decision theory;linear model;estimacion error;invariant loss;methode minimax;monotonic function;quadratic loss;invariante;general linear model;analisis multivariable;62c20;generalized linear model;statistical decision;regression statistique;62j07;regression coefficient;decision statistique;invariant;coefficient regression;estimation bayes	We derive minimax generalized Bayes estimators of regression coefficients in the general linear model with spherically symmetric errors under invariant quadratic loss for the case of unknown scale. The class of estimators generalizes the class considered in Maruyama and Strawderman (2005) to include non-monotone shrinkage functions. AMS subject classification: Primary 62C20, secondary 62J07	coefficient;general linear model;minimax;vhdl-ams;monotone	Yuzo Maruyama;William E. Strawderman	2009	J. Multivariate Analysis	10.1016/j.jmva.2009.06.001	shrinkage estimator;econometrics;mathematical optimization;hypergeometric function;regression;bayes estimator;decision theory;monotonic function;linear regression;invariant;linear model;generalized linear model;mathematics;multivariate analysis;tikhonov regularization;regression analysis;statistics;general linear model	ML	32.683804874745555	-23.39751345732916	85653
9deca52f2f0b281c831b05b6924ab462069a4f7e	algorithm 39: correlation coefficients with matrix multipliation	system analysis;computer simulation of stochastic processes;monte carlo;correlation coefficient;simulation of human learning		algorithm;coefficient	Papken Sassouni	1961	Commun. ACM	10.1145/366199.366266	direct simulation monte carlo;monte carlo method in statistical physics;econometrics;mathematical optimization;dynamic monte carlo method;hybrid monte carlo;monte carlo molecular modeling;kinetic monte carlo;system analysis;statistics;monte carlo method	Theory	32.65803647826273	-16.50172588763592	85773
f35679850a58b2bc9638e9535773d609e049d2e2	highly efficient and secure multi-secret image sharing scheme	symmetric;secure;boolean operation;multi secret image sharing	This paper presents a novel highly efficient secret image sharing scheme that is symmetric, Boolean-based, secure, and enables multi-secret image sharing. The proposed (n, n) scheme involves sharing n secret images among n shared images and recovers all n secret images from n shared images, and losing any shared image prevents recovering any secret image. We propose a novel symmetric sharing-recovery function (SSRF) for performing sharing and recovery. The scheme is based on Boolean operations to attain low computational complexity and is more secure than previous Boolean-based schemes; it exhibits more randomness in each shared image, more randomness between two or more shared images, and higher shared image sensitivity. The experimental results showed that a similar CPU computation time is required for both generating shared images from secret images, and recovering secret images from shared images. Furthermore, the computation time of the SSRF is proportional to the number of secret images.	boolean algebra;boolean operations on polygons;central processing unit;computation;computational complexity theory;exclusive or;hash function;pixel;randomness;rendering (computer graphics);time complexity	Chien-Chang Chen;Wei-Jie Wu;Jun-Long Chen	2015	Multimedia Tools and Applications	10.1007/s11042-015-2634-1	shared secret;theoretical computer science;shamir's secret sharing;distributed computing;homomorphic secret sharing;secure multi-party computation;symmetry;proactive secret sharing;secret sharing;pre-shared key;verifiable secret sharing	Crypto	38.614744090841555	-10.078987396014835	85817
51c8aca7694e61028d06b4919b0420cf4e282984	new indices for the evaluation of the statistical properties of bayesian $\bar{x}$ control charts for short runs	economic design;statistical process control;statistical properties;adaptive bayesian control charts;quality management	In this paper we evaluate the statistical performance of various adaptive control schemes, which are implemented for monitoring the process mean in finite production runs. We develop formulas for the computation of a variety of indices, which reflect the statistical properties of the quality control mechanism of a production process, which is conducted through several one-sided, Bayesian x¯ control charts. Moreover, through our numerical study we evaluate the statistical performance of the economically designed adaptive x¯ control charts, in comparison with their static counterpart. Surprisingly, we find out that very often Bayes-nx¯ chart – with adaptive sample sizes but constant sampling intervals – performs better than the other Bayesian control charts, from a statistical point of view.	bayesian network;chart;intelligent control;mathematical optimization;numerical analysis;optimal control;sampling (signal processing)	Yiannis Nikolaidis;George Tagaras	2017	European Journal of Operational Research	10.1016/j.ejor.2016.09.039	econometrics;quality management;economics;computer science;data mining;statistical process control;statistics	Vision	28.820373636416743	-19.22052490960301	85925
50b984ff66c4b8f43232832e7c7b8eb9b3d90a55	copula density estimation by total variation penalized likelihood	metodo regularizacion;bayes estimation;medida aleatoria;factor riesgo;metodo estadistico;analisis numerico;62p20;problema mal planteado;regularisation;metodo monte carlo;theorie approximation;finance;aplicacion;65c05;estimacion densidad;selected works;risk factor;stochastic method;variable aleatoire;information criterion;copulas;fonction repartition;economic sciences;penalized likelihood;econometric model;62h20;estimation densite;loi conjointe;regularization method;62e17;simulacion numerica;probleme mal pose;estimation non parametrique;random measure;methode monte carlo;variable aleatoria;metodo penalidad;methode regularisation;copula;62g07;econometria;statistical method;distribucion estadistica;facteur risque;analyse numerique;regularization;critere information;approximation theory;non parametric estimation;density estimation;ciencias economicas;funcion distribucion;estimacion bayes;distribution function;penalty method;methode penalite;numerical analysis;distribution statistique;sciences actuarielles;marginal distribution;methode statistique;modele econometrique;monte carlo method;ill posed problem;simulation numerique;algebra lineal numerica;random variable;algebre lineaire numerique;ley conjunta;ley marginal;methode stochastique;65f22;bepress;total variation;62p05;regularizacion;sciences economiques;dependence modeling;econometrics;numerical linear algebra;estimacion no parametrica;estimation statistique;mesure aleatoire;application;estimacion estadistica;60e05;modelo econometrico;statistical estimation;statistical distribution;loi marginale;econometrie;finanzas;joint distribution;60g57;estimation bayes;numerical simulation;metodo estocastico	A copula density is the joint probability density function (PDF) of a random vector with uniform marginals. An approach to bivariate copula density estimation is introduced that is based on a maximum penalized likelihood estimation (MPLE) with a total variation (TV) penalty term. The marginal unity and symmetry constraints for copula density are enforced by linear equality constraints. The TV-MPLE subject to linear equality constraints is solved by an augmented Lagrangian and operator-splitting algorithm. It offers an order of magnitude improvement in computational efficiency over another TV-MPLE method without constraints solved by log-barrier method for second order cone program. A data-driven selection of the regularization parameter is through K-fold cross-validation (CV). Simulation and real data application show the effectiveness of the proposed approach. The MATLAB code implementing the methodology is available online.	algorithm;augmented lagrangian method;bivariate data;computation;cross-validation (statistics);interior point method;linear equation;matlab;marginal model;portable document format;second-order cone programming;simulation	Leming Qu;Yi Qian	2009	Communications in Statistics - Simulation and Computation	10.1080/03610910903168587	computer simulation;econometrics;copula;calculus;mathematics;copula;statistics	ML	33.02744652340116	-23.132294687214014	86041
7a1294fb0f92eb76ad254c35d10ba4d6371c7a81	on the use of error propagation for statistical validation of computer vision software	photogrammetry;metodo estadistico;vision ordenador;software engineering computer vision program verification statistical testing parameter estimation;test statistique;index terms statistical analysis;image databank;analisis forma;test estadistico;algorithms artificial intelligence computer simulation data interpretation statistical image enhancement image interpretation computer assisted information storage and retrieval models statistical pattern recognition automated software software validation;software engineering index terms statistical analysis multivariate hypothesis testing 3d parameter estimation error propagation software validation;multi image photogrammetric resection calculations error propagation statistical validation computer vision software statistical testing parameter estimation software buildings vertices 3d geometric information;statistical test;image multiple;statistical method;parameter estimation error;imagen multiple;multivariate hypothesis testing;program verification;indexing terms;software engineering;fotogrametria;multiple image;coordinate measuring machine;computer vision;propagation erreur;computer errors computer vision parameter estimation software engineering software testing software algorithms uncertainty statistical analysis image databases spatial databases;lines of code;statistical analysis;error propagation;methode statistique;banco imagen;banque image;estimacion parametro;genie logiciel;vision ordinateur;pattern analysis;statistical testing;maquina medida coordenada;propagacion error;parameter estimation;estimation parametre;growth of error;experimental methodology;3d parameter estimation;software validation;photogrammetrie;ingenieria informatica;analyse forme;hypothesis test;machine mesure coordonnee	Computer vision software is complex, involving many tens of thousands of lines of code. Coding mistakes are not uncommon. When the vision algorithms are run on controlled data which meet all the algorithm assumptions, the results are often statistically predictable. This renders it possible to statistically validate the computer vision software and its associated theoretical derivations. In this paper, we review the general theory for some relevant kinds of statistical testing and then illustrate this experimental methodology to validate our building parameter estimation software. This software estimates the 3D positions of buildings vertices based on the input data obtained from multi-image photogrammetric resection calculations and 3D geometric information relating some of the points, lines and planes of the buildings to each other.	algorithm;appendix;arabic numeral 0;computer vision;emoticon;estimated;estimation theory;excision;fbn2 wt allele;haplogroup pre-jt;heterojunction;hypertensive disease;jt (visualization format);mathematical optimization;maxima and minima;multiplication;null value;numerous;photogrammetry;population parameter;propagation of uncertainty;quantity;rendering (computer graphics);singular;software propagation;source lines of code;statistical test;trnt1 gene;vertex (geometry)	Xufei Liu;Tapas Kanungo;Robert M. Haralick	2005	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2005.203	computer vision;statistical hypothesis testing;software sizing;computer science;theoretical computer science;machine learning;mathematics;software metric;statistics	Vision	37.148350440222075	-22.383627056224014	86113
939bfb3efd365c93fa30f6e3e42506b6341c3ad5	new better than renewal-used classes of life distributions	reliability engineering;reliability;electric shock;probability;stochastic dominance;maintenance;probability density function;homogeneous shock model process;poisson shock model better than renewal used life distributions class stochastic dominance economics conditional life distribution homogeneous shock model process aging properties increasing failure rate;random variables;increasing failure rate;aging;conditional life distribution;aging properties;failure analysis;stochastic processes;poisson shock model;poisson distribution reliability failure analysis probability ageing;aging stochastic processes reliability engineering instruction sets electric shock maintenance distribution functions probability density function random variables cities and towns;ageing;cities and towns;economics;distribution functions;life distributions class;better than renewal used;poisson distribution;survival probability;instruction sets	The concept of stochastic dominance in economics is used to compare the conditional life distribution of a used device that has lived until time 0 and its life distributions at time = 0. This comparison gives two classes of life distributions namely, the NBRU class of life distribution and its dual. Relations of these classes to existing criteria of aging and some of their closure properties under reliability operations are studied. The preservation of these aging properties for the survival probability under the homogeneous shock model process are investigated.		Abdul-Rahman M. Abouammoh;Isa S. Qamber	2003	IEEE Trans. Reliability	10.1109/TR.2003.808743	ageing;reliability engineering;stochastic process;mathematics;statistics	Metrics	31.47473731126296	-18.577685177020474	86137
03491abfaaecf449e8e3519f7eb51c51859c73d2	continuous models of epidemic spreading in heterogeneous dynamically changing random networks		Modeling spreading processes in complex random networks plays an essential role in understanding and prediction of many real phenomena like epidemics or rumor spreading. The dynamics of such systems may be represented algorithmically by Monte-Carlo simulations on graphs or by ordinary differential equations (ODEs). Despite many results in the area of network modeling the selection of the best computational representation of the model dynamics remains a challenge. While a closed form description is often straightforward to derive, it generally cannot be solved analytically; as a consequence the network dynamics requires a numerical solution of the ODEs or a direct Monte-Carlo simulation on the networks. Moreover, Monte-Carlo simulations and ODE solutions are not equivalent since ODEs produce a deterministic solution while MonteCarlo simulations are stochastic by nature. Despite some recent advantages in Monte-Carlo simulations, particularly in the flexibility of implementation, the computational cost of an ODE solution is much lower and supports accurate and detailed output analysis such as uncertainty or sensitivity analyses, parameter identification etc. In this paper we propose a novel approach to model spreading processes in complex random heterogeneous networks using systems of nonlinear ordinary differential equations. We successfully apply this approach to predict the dynamics of HIV-AIDS spreading in sexual networks, and compare it to historical data.	algorithm;algorithmic efficiency;approximation;cloud computing;computation;data model;degree distribution;flow network;graph (discrete mathematics);markov switching multifractal;mathematical optimization;monte carlo method;network interface device;nonlinear system;numerical analysis;numerical partial differential equations;population;simulation;time complexity	Sergey V. Ivanov;Alexander Boukhanovsky;Peter M. A. Sloot	2012	CoRR		mathematical optimization;combinatorics;simulation;computer science;artificial intelligence	AI	33.67216081288427	-12.762158763103955	86325
a9f29904b192b16aadda6758d41ff18c3c4f263e	power calculation for the likelihood ratio-test when comparing two dependent intraclass correlation coefficients	intraclass correlation coefficient;likelihood ratio test;power;reproducibility	Comparing the reproducibility level of two devices with continuous outcome on a unique sample of subjects (each subject being assessed several times with both devices) comes down to compare two dependent intraclass correlation coefficients (ICCs). When planning such a reproducibility study, one has to specify both the number of subjects to be included and the number of replicates per subject associated to each device. We propose SAS and S-plus macros, which allow power calculations by implementing a simulation study where dependent ICCs are compared by means of a likelihood ratio-test.	coefficient;s-plus;sas;simulation;likelihood ratio	B. Giraudeau;Raphaël Porcher;J. Y. Mary	2005	Computer methods and programs in biomedicine	10.1016/j.cmpb.2004.10.001	econometrics;likelihood-ratio test;reproducibility;power;pattern recognition;mathematics;intraclass correlation;statistics	AI	30.282749225603027	-21.38331021876107	86336
214fe353437ff5390aaffcfb7d05473fec9e2a39	a sequential experiment design for input uncertainty quantification in stochastic simulation		When we use simulations to estimate the performance of a stochastic system, simulations are often driven by input distributions that are estimated from real-world data. There is both input and simulation uncertainty in the performance estimates. Non-parametric sampling approaches, e.g., the bootstrap, could be used to generate samples of input distributions quantifying both input model and parameter uncertainty. In this paper, a sequential experiment design is proposed to efficiently propagate the input uncertainty to output mean and deliver a percentile confidence interval to quantify the impact of input uncertainty on the system performance. Compared to the classical equal allocation, it could assign more computational budget to samples of input distributions that contribute most to the percentile confidence interval estimation. Our approach is supported by rigorous theoretical and empirical study.	analysis of algorithms;bootstrapping (statistics);computation;design of experiments;sampling (signal processing);simulation;stochastic process;uncertainty quantification	Yuan Yi;Wei Xie;Enlu Zhou	2015	2015 Winter Simulation Conference (WSC)		data modeling;econometrics;mathematical optimization;uncertainty analysis;uncertainty;computer science;resource management;computational model;sensitivity analysis;statistics	Robotics	29.54289242866507	-17.080727536593017	86554
934a8e3cc97ef5ad2256cb68e17e6f3d9787b1da	continual coevolution through complexification	evolving neural networks;coevolution;continuous improvement	In competitive coevolution, the goal is to establish an “arms race” that will lead to increasingly sophisticated strategies. However, in practice, the process often leads to idiosyncrasies rather than continual improvement. Applying the NEAT method for evolving neural networks to a competitive simulated robot duel domain, we will demonstrate that (1) as evolution progresses the networks become more complex, (2) complexification elaborates on existing strategies, and (3) if NEAT is allowed to complexify, it finds dramatically more sophisticated strategies than when it is limited to fixed-topology networks. The results suggest that in order to realize the full potential of competitive coevolution, genomes must be allowed to complexify as well as optimize over the course of evolution.	artificial neural network;coat of arms;cooperative coevolution;evolution;horner's method;neat chipset;neuroevolution of augmenting topologies	Kenneth O. Stanley;Risto Miikkulainen	2002			simulation;coevolution;artificial intelligence	ML	25.243050197592122	-12.88587970190627	86646
94c43e0cc64a687a59bc4f809e178217e3df55f5	known mean, unknown maxima? testing the maximum knowing only the mean	simulation;62g32;62g10;quantitative group tests;sample maximum;sample mean;correlation;62p10	In the quantitative group testing problem, the use of the group mean to identify if the group maximum is greater than a prefixed threshold (infected group) is analyzed, using n independent and identically distributed individuals. Under these conditions, it is shown that the information of the mean is sufficient to classify each group as infected or healthy with low probability of misclassification when the underline distribution is a unilateral heavy-tailed distribution.	maxima	Ricardo A. T. Santos;João Paulo Martins;Miguel Felgueiras	2015	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.773345	econometrics;sample mean and sample covariance;combinatorics;arithmetic mean;mathematics;truncated mean;correlation;statistics;mean	ECom	30.901670901344385	-21.35750086613723	86749
8b51ec147f684c44a1d65271e200e2b00a39b682	estimating the mixing proportion in a semiparametric mixture model	classification automatique statistiques;cluster algorithm;densite probabilite;analyse multivariable;multivariate analysis;probability density;analisis datos;melange loi probabilite;fonction repartition;melangeage;mixed distribution;clustering semiparametric mixture model;metodo secuencial;sequential method;densidad probabilidad;algorithme;discriminant analysis;analyse discriminante;algorithm;funcion distribucion;data analysis;analisis discriminante;distribution function;mixture model;62h30;clustering;37a25;statistical computation;calculo estadistico;methode sequentielle;semiparametric model;mezcla ley probabilidad;analisis multivariable;analyse donnee;calcul statistique;semiparametric mixture model;cluster analysis statistics;estimation statistique;mixing;estimacion estadistica;60e05;mezclado;statistical estimation;modele semi parametrique;algoritmo	In this paper, we investigate methods of estimating the mixing proportion in the case when one of the probability densities is not specified analytically in a mixture model. The methodologywe propose ismotivated by a sequential clustering algorithm. After a sequential clustering algorithm finds the center of a cluster, the next step is to identify observations belonging to that cluster. If we assume that the center of the cluster is known and that the distribution of observations not belonging to the cluster is unknown, the problem of identifying observations in the cluster is similar to the problem of estimating the mixing proportion in a special two-component mixture model. The mixing proportion can be considered as the proportion of observations belonging to the cluster.We propose two estimators for parameters in the model and compare the performance of these two estimators in several different cases. © 2010 Elsevier B.V. All rights reserved.	algorithm;cluster analysis;mixture model;semiparametric model	Seongjoo Song;Dan L. Nicolae;Jongwoo Song	2010	Computational Statistics & Data Analysis	10.1016/j.csda.2010.04.007	econometrics;probability density function;distribution function;calculus;mixture model;mathematics;mixing;multivariate analysis;cluster analysis;linear discriminant analysis;data analysis;semiparametric model;statistics	ML	33.27097827827685	-22.976536470957573	87081
e938ab335271ab529bb1712219ce19ee59cdb5e3	a refined jensen's inequality in hilbert spaces and empirical approximations	espace hilbert;metodo estadistico;analyse multivariable;test statistique;regression function;62g05;absolute continuity;probability;order statistic;espacio hilbert;multivariate analysis;fonctionnelle;fonction regression;loi probabilite;nonparametric estimation;ley probabilidad;fonction repartition;funcion regresion;attente;supporting hyperplane;46cxx;test estadistico;aproximacion;statistique ordre;estimation non parametrique;62g08;statistical test;convexite;empirical distribution function;statistical method;distribucion estadistica;loi asymptotique;statistical regression;62jxx;integral;convexidad;60e15 62g08 jensen s inequality supporting hyperplane empirical measure convex regression function linearly ordered classes of sets pettis integral;approximation;funcional;hilbert space;non parametric estimation;funcion distribucion;distribution function;convex regression function;distribution statistique;functional;60b05;mesure probabilite;convex function;methode statistique;regresion estadistica;jensen s inequality;probability distribution;probabilidad;integrale;linearly ordered classes of sets;probabilite;estadistica orden;fonction repartition empirique;expectation;empirical measure;60e15;analisis multivariable;62g30;estimacion no parametrica;convexity;estimation statistique;generalized convexity;regression statistique;estimacion estadistica;60e05;fonction convexe;statistical estimation;probability measure;statistical distribution;medida probabilidad;expectacion;funcion convexa;62e20;settore secs p 05 econometria;pettis integral	Let f : X→ R be a convex mapping and X a Hilbert space. In this paper we prove the following refinement of Jensen’s inequality: E(f |X ∈ A) ≥ E(f |X ∈ B ) for every A,B such that E(X |X ∈ A) = E(X |X ∈ B ) and B ⊂ A. Expectations of Hilbert space valued random elements are defined by means of the Pettis integrals. Our result generalizes a result of Karlin and Novikov (1963), who derived it for X = R. The inverse implication is also true if P is an absolutely continuous probability measure. A convexity criterion based on the Jensen-type inequalities follows and we study its asymptotic accuracy when the empirical distribution function based on a n−dimensional sample approximates the unknown distribution function. Some statistical applications are addressed, such as nonparametric estimation and testing for convex regression functions or other functionals.	approximation;convex function;hilbert space;jensen's inequality;refinement (computing);social inequality	S. Leorato	2009	J. Multivariate Analysis	10.1016/j.jmva.2008.10.003	probability distribution;mathematical analysis;jensen's inequality;calculus;mathematics;statistics	DB	32.29249238636417	-23.060929802493632	87119
858ea32a199679541f27f7f5238e43b62f36a776	conditional simulation for moving average processes with discrete or continuous values	asset prices;moving average process;two dimensions;tail probability;simulation technique;random variable;variance reduction	A conditional simulation technique has previously been presented for variance reduction when estimating tail probabilities, particularly extreme ones, for a wide class of moving-average processes. Here, we generalize the technique from continuous to discrete random variables. Two distinct approaches to this generalization are presented and compared. We describe some of the empirical properties of the preferred method in simple examples, and present some more general examples including autoregressive moving-average processes in one and two dimensions. We show that the technique performs well for processes with a wide range of structures, provided the tail probability to be estimated is not too large. We discuss briefly the application of this technique in investigating volatility in financial models of, for example, asset prices.	simulation	Raffaella Settimi;Paul G. Blackwell	1998	Statistics and Computing	10.1023/A:1008986117574	random variable;econometrics;mathematical optimization;two-dimensional space;conditional variance;moving-average model;mathematics;statistics;variance reduction	Logic	31.00371935001811	-19.573150494754476	87316
3e18351966861d60d455a6af9d6fc9c797db46c5	efficient algorithms to derive maximum-likelihood estimates for finite exponential and weibull mixtures	efficient algorithm;maximum likelihood estimate	in this study. three algorithms whose convergence can be proved are devised and implemented. Since implementation of these algorithms is not laborious, they are attractive for researchers dealing with mixture models. The algorithms are compared using the total number of basic operations as the performance measure. Computational experimentation indicates that substantial improvement can be achieved utilizing the two-phase method.	algorithm;computation;horner's method;mixture model;time complexity;two-phase locking;vergence	Ali Riza Kaylan;Carl M. Harris	1981	Computers & OR	10.1016/0305-0548(81)90037-X	econometrics;mathematical optimization;probabilistic analysis of algorithms;computer science;mathematics;maximum likelihood;statistics	Theory	30.708169529517658	-16.423406318441334	87497
4d0a348d155bcf34790f7af4c61fe580812a48ce	bunea, c., charitos, t., cooke, r.m., becker, g. two-stage bayesian models - application to zedb project: reliability engineering and system safety, 2005, 90 (2-3); 123-30	bayesian model		donald becker;reliability engineering;system safety	Jussi K. Vaurio	2007	Rel. Eng. & Sys. Safety	10.1016/j.ress.2006.01.004	reliability engineering;engineering;mathematics;bayesian inference;operations research;statistics	SE	29.232771364541673	-19.75315888134978	87586
f40b0ed8a9cabfcc0a8f8f25b170cce463a60704	on stochasticity and turbulence in the federal funds market	dynamic systems theory;structure function;power spectrum;fractional brownian motion;space time;stochastic model	This paper uses various tests from statistics and dynamical systems theory to support stochastic models to explain the behaviour of the federal funds rate. In particular, a fractional Brownian motion is supported by the power spectrum of the series, the structure function test and the Hurst test. A coloured noise model is supported by the signal differentiation test and the space-time separation test. Evidence against the turbulent behaviour hypothesis is also provided.	stochastic process;turbulence	Ioannis Andreadis;Apostolos Serletis	2001	Int. J. Systems Science	10.1080/00207720118727	kolmogorov structure function;dynamical systems theory;actuarial science;stochastic modelling;space time;mathematics;fractional brownian motion;mathematical economics;spectral density;statistics	Logic	36.32155179816674	-20.158401033491586	87669
87aba6bc9efbcd23005d65be0b665d174c60b1d9	on nonparametric signal detectors	nonparametric detection;false alarm rate	This paper is a survey and summary of some of the simpler one-input and two-input detection schemes. Particular emphasis is placed on those systems where less than a complete statistical description of the input is required. Such nonparametric detectors offer the advantages of ease of instrumentation and insensitivity to changes in the input statistics while yielding a fixed maximum false alarm rate. For both the one-input and twoinput cases, the paper begins with a review of well-known parametric detectors, proceeds in the direction of increasing departure from the parametric noise model, and introduces corresponding nonparametric techniques.	sensor	Jack W. Carlyle;John B. Thomas	1964	IEEE Trans. Information Theory	10.1109/TIT.1964.1053663	econometrics;computer science;constant false alarm rate;mathematics;statistics	Vision	29.609343734227622	-20.61100569366052	87766
a0b3538a487bfa628cf696608fcf21547b97ffc7	multivariate generalized poisson geometric process model with scale mixtures of normal distributions	multivariate log t distribution;62j12;multivariate log t;62hxx;62m10;geometric process;62f15;generalized poisson distribution;62p25;scale mixtures of normals;markov chain monte carlo algorithm	This paper proposes a new model named as the multivariate generalized Poisson log-t geometric process (MGPLTGP) model to study multivariate time-series of counts with overdispersion or underdispersion, non-monotone trends within each time-series and positive or negative correlation between pairs of time-series. This model assumes that the multivariate counts follow independent generalized Poisson distributions with an additional parameter to adjust for different degrees of dispersion including overdispersion and underdispersion. Their means after discounting the trend effect geometrically by ratio functions form latent stochastic processes and follow a multivariate log-t distribution with a flexible correlation structure to capture both positive correlation and negative correlation. By expressing the multivariate Student’s t-distribution in scale mixtures of normals, the model can be implemented through Markov chain Monte Carlo algorithms via the user-friendly WinBUGS software. The applicability of the MGPLTGP model is illustrated through an analysis of the possession and/or use of two illicit drugs, amphetamines and narcotics in New South Wales, Australia.	process modeling	Jennifer So-Kuen Chan;Wai-Yin Wan	2014	J. Multivariate Analysis	10.1016/j.jmva.2014.02.002	econometrics;multivariate statistics;mathematical optimization;normal-wishart distribution;mathematics;multivariate analysis;multivariate stable distribution;statistics;multivariate t-distribution	Theory	31.17668153867203	-20.913488600735395	87920
d7f7e091e01be79e6ddca7c273021d7dae136fdf	decomposability of high-dimensional diversity measures: quasi-u-statistics, martingales and nonstandard asymptotics	dna;second order;genetique;92d20;genomics;theorie echantillonnage;metodo estadistico;teoria muestreo;analyse multivariable;sample size;covariance analysis;probability;subgrupo;high dimensionality;subgroup;multivariate analysis;independance;categorical data dependence dna genomics hamming distance orthogonal system permutation measure second order asymptotics second order decomposability;ecologia;loi probabilite;genetica;ley probabilidad;martingale;divergence;tamano muestra;mesure martingale;primary;variance analysis;taille echantillon;ecologie;60g48;32xx;statistical method;dependence;ecuesta estadistica;second order asymptotics;sous groupe;ecology;statistical regression;genetics;data model;independence;sample survey;martingale measure;hamming distance;analyse covariance;marginal distribution;statistique u;medida martingala;independencia;u statistic;secondary 62g20;methode statistique;analisis variancia;regresion estadistica;primary 62g10;probability distribution;secondary;probabilidad;62j10;distance hamming;estadistica u;probabilite;62d05;ley marginal;analisis multivariable;analisis covariancia;second order decomposability;regression statistique;orthogonal system;high dimension;categorical data;distancia hamming;sondage statistique;loi marginale;divergencia;analyse variance;permutation measure;sampling theory	In complex diversity analysis, specially arising in genetics, genomics, ecology and other high-dimensional (and sometimes low sample size) data models, typically subgroup-decomposability (analogous to ANOVA decomposability) arises. In groupdivergence of diversity measures in a high-dimension low sample size scenario, it is shown that Hamming distance-type statistics lead to a general class of quasi Ustatistics having a martingale (array) property, providing key to the study of general (nonstandard) asymptotics. Neither the stochastic independence nor homogeneity of the marginal probability laws play a basic role. A genomic MANOVA model is presented as an illustration.	anova–simultaneous component analysis;data model;ecology;hamming distance;marginal model;stochastic process	Aluísio Pinheiro;Pranab Kumar Sen;Hildete Prisco Pinheiro	2009	J. Multivariate Analysis	10.1016/j.jmva.2009.01.007	u-statistic;probability distribution;sample size determination;marginal distribution;independence;econometrics;genomics;hamming distance;martingale;analysis of variance;categorical variable;analysis of covariance;data model;survey sampling;calculus;probability;mathematics;subgroup;multivariate analysis;divergence;second-order logic;dna;regression analysis;statistics	ML	33.21815470105502	-21.27343748101456	87970
19162d3064d196e72ac027ed1a887786917a8cb9	a comparative study of monte carlo methods to compute rare event probabilities of failure in reliability models	markov chain monte carlo methods mcmc;metropolis hasting mh method;differential evaluation and adaptive metropolis dream;delayed rejection and adaptive metropolis dram;hamiltonian or hybrid monte carlo hybrid method;probability of failure;reliability models;limit state equation	This paper presents a comparative study of the performance of different Monte Carlo Simulation methods in the computation of rare event probabilities in Reliability Theory. We evaluate the performance of 4 well known Markov Chain Monte Carlo methods (MCMC), namely Metropolis-Hasting (MH), Hamiltonian or Hybrid Monte Carlo (HYBRID), Delayed Rejection and Adaptive Metropolis (DRAM), and Differential Evolution Adaptive Metropolis (DREAM), for computing the Probability of Failure using the Reliability Theory framework. We also compared the results of simulations with an approximate analytical method called First Order Reliability Method (FORM). The study shows that while both HYBRID and DREAM produce more accurate results, contrary to intuition, HYBRID method was very slow in performance.	approximation algorithm;computation;differential evolution;dynamic random-access memory;extreme value theory;form;first-order reliability method;hamiltonian (quantum mechanics);hybrid monte carlo;markov chain monte carlo;metropolis;metropolis–hastings algorithm;modified huffman coding;monte carlo method;monte carlo methods for option pricing;newton's method;rejection sampling;simulation	Gopichand Agnihotram;Hari M. Koduvely	2013		10.1145/2522548.2522600	monte carlo method in statistical physics;quasi-monte carlo method;mathematical optimization;dynamic monte carlo method;multiple-try metropolis;hybrid monte carlo;markov chain monte carlo;computer science;monte carlo molecular modeling;kinetic monte carlo;rejection sampling;parallel tempering;monte carlo integration;statistics;monte carlo method	HPC	32.4649563969037	-15.29948311315311	88024
8cfafca31c9be864f26fe4b6962bc288a59add4c	the exact run length distribution and design of the shewhart x̅ chart with estimated parameters based on median run length	estimated parameters;62p30;statistical process control;swinburne;shewhart chart;average run length;median run length;run length distribution;62e15	This paper investigates the Shewhart chart with estimated parameters. When parameters are unknown, we show that the average run length is a confusing performance measure and that the median run length, which is a better representation of the central tendency associated with different shapes of the run length distribution, is more intuitive. A new design model for the Shewhart chart with estimated parameters is developed. With this proposed design model, the Shewhart chart for both known and estimated parameters, even with a small number of Phase-I samples, will have an almost similar sensitivity for a specified shift.	run-length encoding	Wei Lin Teoh;Michael B. C. Khoo;Philippe Castagliola;Ming Ha Lee	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.889158	econometrics;control chart;mathematics;x-bar chart;statistical process control;statistics	Networks	29.993079601364148	-20.511928168730613	88222
343bacb00eac51eb64c0f5f009daea524d42c681	on asymptotic approximations to the distributions of statistics constructed from samples with random sizes		Due to the stochastic character of the intensities of information flows in high performance information systems, the size of data available for the statistical analysis can be often regarded as random. In the paper general theorem concerning the asymptotic expansions of the distribution function of the statistics based on the sample of random size was proved. Some examples are presented for the cases where the sample size has the negative binomial or discrete Pareto distributions.		Vladimir Bening;Victor Korolev;Alexander I. Zeifman	2017		10.7148/2017-0661	statistics;mathematics;asymptotic analysis	Theory	32.904302700537365	-18.84758841949658	88344
8a1e204c255ee79502ff035fb1302e13161fea3f	bibliography on estimation of misclassification	pattern classification bibliographies estimation nonparametric estimation;nonparametric estimation;bibliographies;estimation;pattern classification	Articles, books, and technical reports on the theoretical and experimental estimation of probability of misclassification are listed for the case of correctly labeled or preclassified training data. By way of introduction, the problem of estimating the probability of misclassification is discussed in order to characterize the contributions of the literature.		Godfried T. Toussaint	1974	IEEE Trans. Information Theory	10.1109/TIT.1974.1055260	econometrics;bayes classifier;estimation;computer science;pattern recognition;mathematics;statistics	Vision	25.44217102971091	-23.273756697024115	88464
35603a143edd612a3f6ca150bb924ef83ac33200	compressed sensing with probabilistic measurements: a group testing solution	sampling methods;compressed sensing;group testing solution;infected population identification;partially unknown sampling procedure;probabilistic measurements;reconstruction algorithm;sampling process;classical groups;world war ii;probabilistic model;group testing	Detection of defective members of large populations has been widely studied in the statistics community under the name “group testing”, a problem which dates back to World War II when it was suggested for syphilis screening. There, the main interest is to identify a small number of infected people among a large population using collective samples. In viral epidemics, one way to acquire collective samples is by sending agents inside the population. While in classical group testing, it is assumed that the sampling procedure is fully known to the reconstruction algorithm, in this work we assume that the decoder possesses only partial knowledge about the sampling process. This assumption is justified by observing the fact that in a viral sickness, there is a chance that an agent remains healthy despite having contact with an infected person. Therefore, the reconstruction method has to cope with two different types of uncertainty; namely, identification of the infected populapteioopnle and the partially unknown sampling procedure. In this work, by using a natural probabilistic model for “viral infections”, we design non-adaptive sampling procedures that allow successful identification of the infected population with overwhelming probability 1 − o(1). We propose both probabilistic and explicit design procedures that require a “small” number of agents to single out the infected individuals. More precisely, for a contamination probability p, the number of agents required by the probabilistic and explicit designs for identification of up to k infected members is bounded by m = O(k2(log n)/p2) and m = O(k2 (log2 n)/p2), respectively. In both cases, a simple decoder is able to successfully identify the infected population in time O(mn).	adaptive sampling;algorithm;binary logarithm;codec;compressed sensing;population;sampling (signal processing);statistical model	Mahdi Cheraghchi;Ali Hormati;Amin Karbasi;Martin Vetterli	2009	2009 47th Annual Allerton Conference on Communication, Control, and Computing (Allerton)		statistical model;sampling;group testing;simulation;sparse matrix;machine learning;mathematics;contamination;software testing;probabilistic logic;compressed sensing;statistics;world war ii	ML	31.871049963102198	-12.644998412761312	88472
0c66328c473d3e24bfca628189fb44e275d47a97	reliable and robust transmission and storage techniques for medical images with patient information	digital watermarking;digital signal processing;digital watermark;reed solomon codes;cryptographic protocol;turbo codes;information integration;error correction code;medical image;advanced encryption standard aes;advanced encryption standard;quality of service;turbo code;additive white gaussian noise awgn	There is an increased emphasis on the use of digital techniques in all aspects of human life today. Broadcast radio and television, cellular phone services, consumer and entertainment electronics etc are increasingly using digital signal processing techniques to improve the quality of service. Transmission and storage of documentation and images pertaining to patient records cannot remain an exception to this global trend. Hence, patient records (text and image information) are increasingly stored and processed in digital form. Currently, text and image information, which constitute two separate pieces of data are handled as different files. Thus, there is a possibility of the text and message information, pertaining to different patients, being interchanged and thus mishandled. This can be avoided by merging text and image information in such a manner that the two can be separated without perceptible damage to information contained in either file. Digital watermarking techniques can be used to interleave patient information with medical images. In this work, we have employed digital watermarking along with strong cryptographic protocols and powerful error correcting codes. This reduces the probability of sensitive patient information falling into the wrong hands and ensures information integrity when it is conveyed over noisy channels.	accidental falls;cellular phone;code;contain (action);cryptographic protocol;cryptography;digital signal processing;digital watermarking;embedding;encryption;error detection and correction;forward error correction;medical records;medical records systems, computerized;medical imaging;patients;protocols documentation;quality of service;radio broadcasting;real-time computing;real-time recovery;reed–solomon error correction;signal-to-noise ratio;storage medium;supernumerary mandibular right primary canine;television;turbo code	Myagmarbayar Nergui;U. Sripati Acharya;U. Rajendra Acharya;Wenwei Yu	2009	Journal of Medical Systems	10.1007/s10916-009-9332-3	turbo code;telecommunications;digital watermarking;computer science;theoretical computer science;digital image processing;computer security;statistics	Graphics	38.818557076079436	-14.031670866143351	88696
1375ba64f326e3cd3482ace20a807ffb9e388eb0	exploiting data-distribution patterns in modeling tuple selectivities in a database	modelizacion;estimacion;base donnee;analisis textura;interrogation base donnee;database;calcul erreur;interrogacion base datos;base dato;segmentation;data distribution;modelisation;error analysis;texture analysis;estimation;pattern recognition;calculo error;reconnaissance forme;reconocimiento patron;modeling;analyse texture;database query;segmentacion	To estimate the number of tuples satisfying a certain query, a data-distribution model is proposed. The model is based on a discrete approximation of the data space and belongs to the class of nonparametric models. Using texture-analysis techniques applied to the multidimensional data space, it is proposed that a segmentation of this space be obtained as a means of obtaining a discrete approximation. Thus the space is divided into a number of homogeneous regions which can be later queried to obtain good estimates of the size of the response set. To obtain this segmentation, an adaptation of a hierarchical segmentation method from pattern recognition is proposed to extend its applicability from three dimensions to D dimensions. In order to flatten the hierarchical segmentation while maintaining a high accuracy model, the homogeneity of the bit patterns of the segments is assessed. A sampling method is proposed to assess the homogeneity of these bit patterns. The accuracy of this sampling technique is analyzed and in particular, it is shown that only a small sample is needed to provide reasonably accurate estimates.	approximation;bitmap;cpu cache;cluster analysis;computation;database;dataspaces;iteration;logical equality;olap cube;pattern recognition;sampling (signal processing);steady state;xfig	Nabil Kamel;Roger King	1993	Inf. Sci.	10.1016/0020-0255(93)90038-N	estimation;systems modeling;computer science;data mining;database;mathematics;segmentation;statistics	DB	36.12239782030733	-23.8423471719942	88745
f6d47fbe34abf477471019c978d1b1ef01523095	an empirical likelihood-based method for comparison of treatment effects - test of equality of coefficients in linear models	ji cuadrado;efficacite traitement;theorie echantillonnage;fonction vraisemblance;teoria muestreo;sample size;likelihood ratio;medicament;treatment effect;modele empirique;methode empirique;effet traitement;analisis datos;normal distribution;treatment efficiency;test comparacion;tamano muestra;equality test;metodo empirico;simulation;modele lineaire;empirical method;taille echantillon;simulacion;ecuesta estadistica;modelo lineal;curva gauss;khi deux;funcion verosimilitud;vraisemblance empirique;simulation experiment;chi square;sample survey;data analysis;test egalite;estimation erreur;test igualdad;error estimation;statistical computation;linear model;estimacion error;calculo estadistico;eficacia tratamiento;62d05;loi normale;empirical model;analyse donnee;modelo empirico;medicamento;calcul statistique;drug;estimation statistique;comparison test;rapport vraisemblance;estimacion estadistica;empirical likelihood;statistical estimation;likelihood function;sondage statistique;gaussian distribution;test comparaison;sampling theory;relacion verosimilitud	To compare two treatment effects, which can be described as the difference of the parameters in two linear models, we propose an empirical likelihood-based method to make inference for the difference. Our method is free of the assumptions of normally distributed and homogeneous errors, and equal sample sizes. The empirical likelihood ratio for the difference of the parameters of interest is shown to be asymptotically chi-squared. Simulation experiments illustrate that our method outperforms the published ones. Our method is used to analyze a data set from a drug study.		Haiyan Su;Hua Liang	2010	Computational statistics & data analysis	10.1016/j.csda.2009.10.018	normal distribution;econometrics;calculus;mathematics;quasi-maximum likelihood;empirical modelling;statistics	ML	32.65041900807821	-22.4880415542925	88827
b7e5ab26ddd3e8a25ff44f85ea6c1f5e963fa5c9	dimension reduction in discrete time portfolio optimization with partial information	93e11;fast mean reversion;filtering;dimension reduction;93e20;partial information;portfolio optimization;approximate dynamic programming;93c70	This paper considers the problem of portfolio optimization in a market with partial information and discretely observed price processes. Partial information refers to the setting where assets have unobserved factors in the rate of return and the level of volatility. Standard filtering techniques are used to compute the posterior distribution of the hidden variables, but there is difficulty in finding the optimal portfolio because the dynamic programming problem is non-Markovian. However, fast time scale asymptotics can be exploited to obtain an approximate dynamic program (ADP) that is Markovian and is therefore much easier to compute. Of consideration is a model where the latent variables (also referred to as hidden states) have fast mean reversion to an invariant distribution that is parameterized by a Markov chain θt, where θt represents the regime-state of the market and reverts to its own invariant distribution over a much longer time scale. Data and numerical examples are also presented, and there appears to be evidence that unobserved drift results in an information premium.	dimensionality reduction	Andrew Papanicolaou	2013	SIAM J. Financial Math.	10.1137/120897596	filter;financial economics;econometrics;mathematical optimization;economics;finance;portfolio optimization;mathematics;statistics;dimensionality reduction	Theory	26.26324202497277	-20.633834751376103	88924
f41bd0971e226602b9213b3637da68e121a12b5c	an efficient variant of the product and ratio estimators in double sampling		In this paper we have suggested a double sampling version of Sahai (10) estimator using simple random sampling without replacement (SRSWOR) at both phases and its properties are studied. The estimator is also compared with simple mean per unit for a given cost of the survey. An empirical study is carried out to demonstrate the performance of suggested estimator over usual estimators.	sampling (signal processing)	Housila P. Singh;Gajendra K. Vishwakarma	2006	MASA		efficient estimator;minimax estimator;econometrics;mathematical optimization;minimum-variance unbiased estimator;estimator;mathematics;statistics	ML	29.663231374508314	-22.50435868683211	88926
0360b2c0012f59af3a38b6e3aef91003c592e270	r package to handle archimax or any user-defined continuous copula construction: acopula		We introduce acopula package (run under R) that aims at researchers as well as practitioners in the field of modelling stochastic dependence. Description of tools with examples are given, namely several probability related functions, estimation and testing procedures, and two utility functions.		Tomás Bacigál	2013		10.1007/978-3-642-39165-1_11	econometrics;data mining;statistics	Logic	30.339062808495914	-19.265643961861972	89791
be29cf5667bdb67ea46e4965d25f1cbbcdf98a0b	optimal checkpointing and rollback strategies with media failures: statistical estimation algorithms	poisson process;nonparametric statistics software fault tolerance system recovery statistical analysis stochastic processes;nonparametric statistics;checkpointing stochastic systems testing statistical analysis statistical distributions;software fault tolerance;total time on test optimal checkpointing rollback strategies statistical estimation algorithms stochastic models file recovery checkpoint generations system failure homogeneous poisson process statistical nonparametric algorithms;system recovery;statistical analysis;consistent estimator;stochastic processes;stochastic model;statistical estimation;renewal process	This paper considers two stochastic models for a file recovery action with checkpoint generations when two kinds of failures; system failure and media failure, occur according to a homogeneous Poisson process and a renewal process, respectively. For the unknown media failure time distribution, we develop statistical nonparametric algorithms to estimate the optimal checkpoint intervals which maximize the system availabilities. The algorithms proposed are based on the corresponding total time on test (TTT) statistics to the media failure time distribution, and can provide strongly consistent estimates from its sample data.	algorithm;application checkpointing;estimation theory;statistical model	Tadashi Dohi;Shunji Osaki;Naoto Kaio	1999		10.1109/PRDC.1999.816225	nonparametric statistics;reliability engineering;stochastic process;renewal theory;poisson process;continuous-time stochastic process;computer science;stochastic modelling;consistent estimator;software fault tolerance;statistics	ML	30.549114314624703	-18.82824434462634	89804
d9f02b7233116a8fd29980102d967101f628282d	analysis of binary longitudinal data with time-varying effects	probit mixed model;repeated measures;nonparametric regression;partial collapse;longitudinal data	This paper considers the analysis of longitudinal data where a binary response variable is observed repeatedly for each subject over time. In analyzing such data, regression coefficients are commonly assumed constant over time, which may not properly account for the time-varying effects of some subject characteristics on a sequence of binary outcomes. This paper proposes a Bayesian method for the analysis of binary longitudinal data with time-varying regression coefficients and random effects to account for nonlinear subject-specific effects over time as well as between-subject variation. The proposed method facilitates posterior computation via the method of partial collapse and accommodates spatially inhomogeneous smoothness of nonparametric functions without overfitting via a basis search technique. The proposed method is illustrated with a simulated study and the binary longitudinal data from the German socioeconomic panel study.		Seonghyun Jeong;Minjae Park;Taeyoung Park	2017	Computational Statistics & Data Analysis	10.1016/j.csda.2017.03.007	econometrics;repeated measures design;data mining;mathematics;nonparametric regression;statistics	ML	30.171285573097535	-23.668986550509363	90332
77b4ce9bda5e227da8cb9e031ebe027009281c0f	on joint determination of the number of states and the number of variables in markov-switching models: a monte carlo study	experimental design;metodo estadistico;analisis numerico;metodo monte carlo;theorie approximation;aplicacion;65c05;modelo markov;stochastic method;modelo autorregresivo;05bxx;140302;62j02;62e17;simulacion numerica;plan experiencia;methode monte carlo;processus autoregressif;econometric and statistical methods;monte carlo study;statistical method;information criteria;distribucion estadistica;journal article;analyse numerique;autoregressive model;approximation theory;griffith business school;markov switching model;markov model;numerical analysis;62k99;distribution statistique;plan experience;autoregressive processes;methode statistique;monte carlo method;simulation numerique;62m05;methode stochastique;modele markov;monte carlo;modele autoregressif;application;statistical distribution;numerical simulation;metodo estocastico	In this paper we examine the performance of two newly developed procedures that jointly select the number of states and variables in Markov-switching models by means of Monte Carlo simulations. They are Smith, Naik and Tsai (2006) and Psaradakis and Spagnolo (2006), respectively. The former develops Markov switching criterion (MSC) designed specifically for Markov-switching models, while the latter recommends the use of standard complexitypenalised information criteria (BIC, HQC, & AIC). The Monte Carlo evidence shows that BIC outperforms MSC while MSC and HQC are preferable over AIC. URL: http://mc.manuscriptcentral.com/lssp E-mail: comstat@univmail.cis.mcmaster.ca Communications in Statistics Simulation and Computation	bayesian information criterion;communications in statistics – simulation and computation;hannan–quinn information criterion;markov chain;monte carlo method	Thatphong Awirothananon;Wai-Kong Cheung	2009	Communications in Statistics - Simulation and Computation	10.1080/03610910903121982	computer simulation;econometrics;deviance information criterion;calculus;mathematics;statistics;monte carlo method	AI	32.54908958842253	-22.385172570199725	90425
40d67b8b7d88cc2ef91f37d7a6725bd86034596a	computationally efficient comparison of experimental designs for system reliability studies with binomial data	experimental design;system reliability;design of experiments doe bayesian methods systems analysis reliability markov chains monte carlo methods;resource allocation;grupo de excelencia;design of experiments;markov chain monte carlo;ciencias basicas y experimentales;matematicas;complex systems	Computationally Efficient Comparison of Experimental Designs for System Reliability Studies With Binomial Data Jessica L. Chapman a , Max D. Morris c & Christine M. Anderson-Cook b a Department of Mathematics, Computer Science and Statistics , St. Lawrence University , Canton , NY , 13617 b Los Alamos National Laboratory , Los Alamos , NM , 87545 c Department of Statistics , Iowa State University , Ames , IA , 50011 Accepted author version posted online: 29 May 2012.Published online: 28 Nov 2012.	computer science	Jessica L. Chapman;Max D. Morris;Christine M. Anderson-Cook	2012	Technometrics	10.1080/00401706.2012.694784	econometrics;complex systems;mathematics;design of experiments;statistics	DB	33.65155419425468	-17.54025872156875	90506
15c7ac72cfbf9f60479b0269a720d3678b9540f1	distributed block-dependent watermarking method for jpeg2000 image authentication	discrete wavelet transforms;distributed block dependent watermarking method;watermarking;image coding;discrete wavelet transform;chaos;crop and replacement;authentication;fragile watermarking;transform coding;counterfeiting;watermarking discrete wavelet transforms image coding;accuracy;image authentication;block dependent;block dependent image authentication fragile watermarking;discrete wavelet transform distributed block dependent watermarking method jpeg2000 image authentication malicious tampers crop and replacement;malicious tampers;watermarking authentication transform coding counterfeiting discrete wavelet transforms image coding data engineering electronic mail image recognition distributed databases;jpeg2000 image authentication	Two essential benefits, the resistance to the counterfeiting attack and the tamper localization accuracy, of the fragile watermarking methods are mutually restricted. In this paper, a novel distributed block-dependent watermarking method is proposed to gain both benefits by inserting the block watermark into other chosen blocks. Further, the proposed method can be easily implemented in the DWT domain for authenticating JPEG2000 images. Experimental results show that the method can resist the counterfeiting attack effectively as other block-dependent watermarking methods, while sustain the high localization accuracy as the block-independent watermarking method. It is also demonstrated that the proposed method can successfully authenticate the JPEG2000 images while recognize the malicious tampers, such as crop-and-replacement, deletion, and so on.	authentication;chaos theory;coefficient;digital watermarking;discrete wavelet transform;jpeg 2000;watermark (data file)	Shewen Sun;Zhaohong Li	2009	2009 First International Workshop on Database Technology and Applications	10.1109/DBTA.2009.108	transform coding;watermarking attack;digital watermarking;computer science;theoretical computer science;authentication;mathematics;accuracy and precision;internet privacy;discrete wavelet transform;computer security;statistics	EDA	39.133251138126326	-11.942809478097638	91016
6a9d79d3bf38210557db3b94170220e3de3b48f2	correlation and the time interval in multiple regression models	intervalo tiempo;time correlation;partial regression coefficient;coeficiente correlacion;coeficiente regresion;correlation temporelle;regression model;time series;time interval;modelo regresion;correlacion temporal;analisis regresion;correlation time;regresion multiple;multiple regression model;modele regression;serie temporelle;analyse correlation;serie temporal;temps correlation;analyse regression;regression analysis;correlation coefficient;regression coefficient;regression multiple;article;coefficient correlation;coefficient regression;analisis correlacion;correlation analysis;tiempo correlacion;intervalle temps;multiple regression	In this paper we investigate the time interval effect of multiple regression models in which some of the variables are additive and some are multiplicative. The effect on the partial regression and correlation coefficients is influenced by the selected time interval. We find that the partial regression and correlation coefficients between two additive variables approach one-period values as n increases. When one of the variables is multiplicative, they will approach zero in the limit. We also show that the decreasing speed of the n-period correlation coefficients between both multiplicative variables is faster than others, except that a one-period correlation has a higher positive value. The results of this paper can be widely applied in various fields where regression or correlation analyses are employed.		Rong Jea;Jin-Lung Lin;Chao-Ton Su	2005	European Journal of Operational Research	10.1016/j.ejor.2003.07.020	segmented regression;ecological correlation;econometrics;regression dilution;linear regression;partial correlation;linear predictor function;calculus;standardized coefficient;regression diagnostic;mathematics;partial least squares regression;path coefficient;correlation function;regression analysis;statistics;cross-sectional regression	Vision	33.65172287060293	-22.324183201967188	91073
e7c7fe476802e04d2677b2180559cbac80df6acb	generation of correlated parameters for statistical circuit simulation	simulation ordinateur;fabrication;modelizacion;gaussian domain;concepcion asistida;computer aided design;analyse multivariable;model parameters;p variate normal density function;response surface methodology;metodo monte carlo;p variate normal density function correlated parameters statistical circuit simulation gaussian domain gaussian distribution multivariate statistics monte carlo simulations correlation matrix deterministic tracking model parameters isodensity contour;parametro circuito;multivariate analysis;integrated circuit;statistical analysis circuit analysis computing correlation methods matrix algebra monte carlo methods;circuit parameter;statistical independence;manufacturing process;methode monte carlo;circuito integrado;deterministic tracking;matrix algebra;correlation methods;circuit simulation power system modeling monte carlo methods response surface methodology integrated circuit modeling predictive models manufacturing processes fabrication gaussian distribution density functional theory;parametre circuit;modelisation;density functional theory;circuit simulation;manufacturing processes;deterministic tracking method;statistical analysis;procedimiento fabricacion;statistical circuit simulation;monte carlo method;integrated circuit modeling;isodensity contour;conception assistee;analisis multivariable;multivariate statistics;predictive models;simulacion computadora;estimation statistique;power system modeling;correlation matrix;procede fabrication;monte carlo simulation;circuit analysis computing;correlated parameters;estimacion estadistica;modeling;statistical estimation;monte carlo simulations;computer simulation;density functional;gaussian distribution;monte carlo methods;circuit integre	Standard transformations for converting non-Gaussian raw data to the Gaussian domain are reviewed. Once the appropriate transformation is determined, conversion formulas are developed so that only the raw data's mean and variance are required to convert any future data points to the Gaussian domain. With the input parameters represented in terms of the Gaussian distribution, techniques from multivariate statistics are used to generate statistically correlated sets of parameters. The method used to generate input parameters for Monte Carlo simulations is examined. The technique is based on the relatively easy generation of statistically independent observations and their relationship to the more general case of statistically dependent observations by way of the parameters' correlation matrix. A method called deterministic tracking of model parameters is developed. Using the definition of an isodensity contour for a p-variate normal density function, an expression is derived to calculate the most probable setting of input parameters, given that a subset of these parameters was first fixed to specific values. A simple but realistic three-variable example of the methods developed is shown. >	electronic circuit simulation	Kevin S. Eshbaugh	1992	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.170985	computer simulation;econometrics;mathematical optimization;computer science;computer aided design;mathematics;statistics;monte carlo method	EDA	35.895435544846364	-19.924787263899184	91191
5b870beb4b75e2e334c9aeb90b756b3c7e575ac8	on the testability of coarsening assumptions: a hypothesis test for subgroup independence	technische reports;ddc 510	Since coarse(ned) data naturally induce set-valued estimators, analysts often assume coarsening at random (CAR) to force them to be single-valued. Focusing on a coarse categorical response variable and a precisely observed categorical covariate, we first re-illustrate the impossibility to test CAR and then contrast it to another type of coarsening called subgroup independence (SI). It turns out that – depending on the number of subgroups and categories of the response variable – SI can be point-identifying as CAR, but testable unlike CAR. A main goal of this paper is the construction of the likelihood-ratio test for SI. All issues are similarly investigated for the here proposed generalized versions, gCAR and gSI, thus allowing a more flexible application of this hypothesis test. The results are illustrated by the data of the German Panel Study “Labour Market and Social Security” (PASS).		Julia Plass;Marco E. G. V. Cattaneo;Georg Schollmeyer;Thomas Augustin	2017	Int. J. Approx. Reasoning	10.1016/j.ijar.2017.07.014	econometrics;mathematics;algorithm;statistics	ML	30.78081032920972	-22.84532201292818	91882
618b27f0a4ba39fd54bbb95632b9f11ffeea59c6	on the influence of the number of algorithms, problems, and independent runs in the comparison of evolutionary algorithms	friedman test;crs4eas;nemenyi test;multiple comparison	When conducting a comparison between multiple algorithms on multiple optimisation problems it is expected that the number of algorithms, problems and even the number of independent runs will affect the final conclusions. Our question in this research was to what extent do these three factors affect the conclusions of standard Null Hypothesis Significance Testing (NHST) and the conclusions of our novel method for comparison and ranking the Chess Rating System for Evolutionary Algorithms (CRS4EAs). An extensive experiment was conducted and the results were gathered and saved of k = 16 algorithms on N = 40 optimisation problems over n = 100 runs. These results were then analysed in a way that shows how these three values affect the final results, how they affect ranking and which values provide unreliable results. The influence of the number of algorithms was examined for values k = {4, 8, 12, 16}, number of problems for values N = {5, 10, 20, 40}, and number of independent runs for values n = {10, 30, 50, 100}. We were also interested in the comparison between both methods – NHST’s Friedman test with post-hoc Nemenyi test and CRS4EAs – to see if one of them has advantages over the other. Whilst the conclusions after analysing the values of k were pretty similar, this research showed that the wrong value of N can give unreliable results when analysing with the Friedman test. The Friedman test does not detect any or detects only a small number of significant differences for small values of N and the CRS4EAs does not have a problem with that. We have also shown that CRS4EAs is an appropriate method when only a small number of independent runs n are available. © 2017 Elsevier B.V. All rights reserved.	evolutionary algorithm;hoc (programming language);mathematical optimization;nat friedman;while	Niki Vecek;Matej Crepinsek;Marjan Mernik	2017	Appl. Soft Comput.	10.1016/j.asoc.2017.01.011	econometrics;mathematics;nemenyi test;multiple comparisons problem;algorithm;friedman test;statistics	ML	29.316252365967518	-21.02059103818736	92063
a8ed3c8dd2086e6e43da79b86b866fffd90d4cf8	ak-sys: an adaptation of the ak-mcs method for system reliability	system reliability;classification;sampling;monte carlo simulation;kriging;ak mcs	A lot of research work has been proposed over the last two decades to evaluate the probability of failure of a structure involving a very time-consuming mechanical model. Surrogate model approaches based on Kriging, such as the Efficient Global Reliability Analysis (EGRA) or the Active learning and Kriging-based Monte-Carlo Simulation (AK-MCS) methods, are very efficient and each has advantages of its own. EGRA is well suited to evaluating small probabilities, as the surrogate can be used to classify any population. AK-MCS is built in relation to a given population and requires no optimization program for the active learning procedure to be performed. It is therefore easier to implement and more likely to spend computational effort on areas with a significant probability content. When assessing system reliability, analytical approaches and first-order approximation are widely used in the literature. However, in the present paper we rather focus on sampling techniques and, considering the recent adaptation of the EGRA method for systems, a strategy is presented to adapt the AK-MCS method for system reliability. The AK-SYS method, “Active learning and Kriging-based SYStem reliability method”, is presented. Its high efficiency and accuracy are illustrated via various examples.	multi categories security	William Fauriat;Nicolas Gayton	2014	Rel. Eng. & Sys. Safety	10.1016/j.ress.2013.10.010	reliability engineering;sampling;econometrics;simulation;biological classification;engineering;mathematics;kriging;statistics;monte carlo method	SE	28.39202532687927	-17.177906274615424	92193
aed3fa7305b6eda78a73eac2109f432e4576b28b	measured quantity value estimator for multiplicative nonlinear measurement models	computers;uncertainty;random variables;measurement uncertainty;estimation;temperature measurement;soil measurements	An estimate of a measurand using a nonlinear function of uncorrelated input quantities can be done by either applying the nonlinear function to the means of the input quantities (Method 1) or calculating the mean of a set of values obtained from propagating individual measurement values through the nonlinear function (Method 2). This paper proposes an improvement over the standard Method 2 procedures when the input quantities are assumed to be statistically independent and the nonlinear function has a general sum-of-product form, which covers many common measurement models. This paper shows that the proposed new approach (called Method 2S), if applicable, always produces a mean-squared error smaller than that of the conventional Method 2 procedures. The proposed approach improves the efficiency of Type-A evaluation as well as the Monte Carlo method. It also well complements the mainstream practices in the measurement uncertainty evaluation.	disjunctive normal form;mean squared error;monte carlo method;nonlinear system	Ye Chow Kuang;Arvind Rajan;Melanie Po-Leen Ooi;Serge N. Demidenko	2017	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2017.2658000	random variable;econometrics;mathematical optimization;estimation;uncertainty;temperature measurement;mathematics;statistics;measurement uncertainty	Vision	28.175875267611044	-18.090319861513144	92438
1ee67725291cdc9b77b9a07d139ccc52fd00e4a3	characterizing branching processes from sampled data	graph characterization;sampling and estimation;mcmc;branching processes	Branching processes model the evolution of populations of agents that randomly generate offspring (children). These processes, more patently Galton-Watson processes, are widely used to model biological, social, cognitive, and technological phenomena, such as the diffusion of ideas, knowledge, chain letters, viruses, and the evolution of humans through their Y-chromosome DNA or mitochondrial RNA. A practical challenge of modeling real phenomena using a Galton-Watson process is the choice of the offspring distribution, which must be measured from the population. In most cases, however, directly measuring the offspring distribution is unrealistic due to lack of resources or the death of agents. So far, researchers have relied on informed guesses to guide their choice of offspring distribution. In this work we propose two methods to estimate the offspring distribution from real sampled data. Using a small sampled fraction of the agents and instrumented with the identity of the ancestors of the sampled agents, we show that accurate offspring distribution estimates can be obtained by sampling as little as 14% of the population.	exact algorithm;markov chain monte carlo;population;randomness;sampling (signal processing);the offspring;thomas j. watson research center	Fabricio Murai;Bruno F. Ribeiro;Donald F. Towsley;Krista Gile	2013		10.1145/2487788.2488053	econometrics;markov chain monte carlo;mathematics;statistics	AI	31.83927572656969	-12.680893573997867	92687
1db66cd67f1347ccc512fec022032dc2b863cb28	how to generate regularly behaved production data? a monte carlo experimentation on dea scale efficiency measurement	scale efficiency;modelizacion;regularite;correlacion;production function;evaluation performance;distance function;gestion multi article;analisis envolvimiento datos;parametric distance function dea technical efficiency scale efficiency monte carlo experiments;sample size;entrada salida;small sample;rendement echelle;metodo monte carlo;performance evaluation;methode parametrique;regularidad;methode non parametrique;retorno de escala;non parametric test;monte carlo experiments;tamano muestra;metodo parametrico;evaluacion prestacion;parametric method;regularity;gestion muti articulo;methode monte carlo;taille echantillon;econometria;produccion pequena serie;dea;small series production;satisfiability;small samples;return to scale;input output;modelisation;metodo no parametrico;technical efficiency;data envelopment analysis;monte carlo experiment;monte carlo method;prueba no parametrica;test non parametrique;non parametric method;multi item management;pequena muestra;econometrics;correlation;monte carlo;efficiency measurement;modeling;production petite serie;petit echantillon;econometrie;entree sortie;analyse enveloppement donnee;parametric distance function;returns to scale;regularity condition	Monte Carlo experimentation is a well-known approach used to test the performance of alternative methodologies under different hypotheses. In the frontier analysis framework, whatever the parametric or non-parametric methods tested, experiments to date have been developed assuming single output multi-input production functions. The data generated have mostly assumed a Cobb-Douglas technology. Among other drawbacks, this simple framework does not allow the evaluation of DEA performance on scale efficiency measurement. The aim of this paper is twofold. On the one hand, we show how reliable two-output two-input production data can be generated using a parametric output distance function approach. A variable returns to scale translog technology satisfying regularity conditions is used for this purpose. On the other hand, we evaluate the accuracy of DEA technical and scale efficiency measurement when sample size and output ratios vary. Our Monte Carlo experiment shows that the correlation between true and estimated scale efficiency is dramatically low when DEA analysis is performed with small samples and wide output ratio variations.	experiment;monte carlo method	Sergio Perelman;Daniel Santín	2009	European Journal of Operational Research	10.1016/j.ejor.2008.11.013	returns to scale;econometrics;operations management;data envelopment analysis;mathematics;statistics;monte carlo method	ML	32.678348785444264	-21.04221020088512	92757
30b6b98d38c286d9f8484b8848ae9ce3e9b6318b	verbal probabilities: linear or logistic? - a regression analysis approach	verbal probabilities;numerical values;regression analysis	Verbal probability expressions are quite intuitive and used in a variety of clinical important issues like adverse drug causality appraisal. However, there is insufficient evidence of their numerical meaning and whether they have a linear or logistic relationship with them. We aimed at contributing to answering these questions by means of a comparative regression analysis based on a sample of N=683 participants between 10 and 82 years (mean age 20.33±11.77; median: 18 years) who were asked to numerically rate a given set of sixteen verbal probability phrases on a visual analogue scale. With respect to the explained variance, we found an R2 for the linear model of 0.574 while R2 for the logistic model was only 0.392 indicating a superiority of linear model compared to the logistic model. Although we were able to show that ranked verbal phrases are more likely to behave in a linear that in a logistic way other regression options like the double logistic model should be taken into consideration for further research.	analog;causality;linear model;logistic regression;numerical analysis;phrases;probability;sample variance;sixteen	Thomas Ostermann;Hannah Vogel;Sebastian Appelbaum	2018	Studies in health technology and informatics	10.3233/978-1-61499-896-9-117	statistics;regression analysis;mathematics	NLP	26.471376602601794	-22.208681653863053	92833
bfa86d94f87f9e03c3c15709fd34bca2a356c0ef	essential secret image sharing scheme with the same size of shadows	derivative polynomial;secret image sharing;birkhoff interpolation;essential shadow;threshold secret sharing	Secret image sharing is a method to decompose a secret image into shadow images (shadows) so that only qualified subset of shadows can be used to reconstruct the secret image. Usually all shadows have the same importance. Recently, an essential SIS (ESIS) scheme with different importance of shadows was proposed. All shadows are divided into two group: essential shadows and non-essential shadows. In reconstruction, the involved shadows should contain at least a required number of shadows, including at least a required number of essential shadows. However, there are two problems in previous ESIS scheme: unequal size of shadows and concatenation of sub-shadow images. These two problems may lead to security vulnerability and complicate the reconstruction. In this paper, we propose a novel ESIS scheme based on derivative polynomial and Birkhoff interpolation. A single shadow with the same-size is generated for each essential and non-essential participant. The experimental results demonstrate that our scheme can avoid above two problems effectively. An essential secret image sharing scheme is proposed based on derivative polynomial.The essential shadows are more important than non-essential shadows.All shadows have the same size.No concatenation operation is needed in generation of shadows.	birkhoff interpolation;concatenation;polynomial;polynomial-time reduction;vulnerability (computing)	Peng Li;Ching-Nung Yang;Zhili Zhou	2016	Digital Signal Processing	10.1016/j.dsp.2015.12.004	combinatorics;discrete mathematics;birkhoff interpolation;mathematics;geometry	Theory	38.470097552309085	-10.009874436057881	93021
257079ac426f0a3f98b908758f87dc496771bc8d	generating gamma variates by a modified rejection technique	exponential distribution;probability density;normal distribution;random variable;gamma distribution;random numbers;acceptance rejection method	A suitable square root transformation of a gamma random variable with mean a ≥ 1 yields a probability density close to the standard normal density. A modification of the rejection technique then begins by sampling from the normal distribution, being able to accept and transform the initial normal observation quickly at least 85 percent of the time (95 percent if a ≥ 4). When used with efficient subroutines for sampling from the normal and exponential distributions, the resulting accurate method is significantly faster than competing algorithms.	algorithm;gamma correction;rejection sampling;sampling (signal processing);subroutine;time complexity	Joachim H. Ahrens;Ulrich Dieter	1982	Commun. ACM	10.1145/358315.358390	quantile function;normal distribution;random variate;exponential distribution;random variable;gamma distribution;econometrics;mathematical optimization;inverse-gamma distribution;probability density function;q-function;folded normal distribution;normal-gamma distribution;mixture distribution;student's t-distribution;logit-normal distribution;inverse transform sampling;log-normal distribution;mathematics;variance-gamma distribution;rejection sampling;convolution random number generator;exponentially modified gaussian distribution;chi-squared distribution;standard normal table;statistics;probability integral transform	Graphics	31.538957875215843	-17.56722761702074	93192
302a7898950299761c291b021b9f657adb356824	two-stage multinomial logit model	decision tree;disability pension;statistical model;interaction effect;multi classification;accuracy rate;multinomial logit model	We suggest a two-stage multinomial logit model (TMLM) for incorporating and interpreting both the interaction and main effects in the model for multi-categorized responses. TMLM combines the robustness of multinomial logit model (MLM) with the good properties of decision tree (DT), which makes it possible to cluster homogeneous subjects and thus to incorporate the interaction effects of explanatory variables in MLM. In the first step of TMLM, DT is applied to determine the most influential interaction effects and to create a cluster variable that represents categories with best splits for optimal tree. In the second step, the cluster variable is involved in MLM as an explanatory variable. With TMLM, it is possible to interpret not only the interactions among explanatory variables, but also the main effects. It is also possible to cluster and characterize homogeneous subjects; these would not be possible with MLM. This model also improves the accuracy rate in multi-classification for multi-categorized responses. We apply TMLM to the national pension data of disability pensioners in Korea and compare the results with two types of MLM models. TMLM is suggested as a statistical model for characterizing both the interaction and main effects of explanatory variables and also for improving accuracy rates comparing to MLM.	categorical variable;interaction;multinomial logistic regression;reason;statistical model	Jin-Hyung Kim;Mijung Kim	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.11.057	statistical model;econometrics;interaction;computer science;machine learning;decision tree;mathematics;multinomial probit;multinomial logistic regression;statistics	NLP	25.89515067267691	-22.377918408939202	93389
1a2d5df46e24a2fe00d883b0512fd76e615db914	eliciting conditional and unconditional rank correlations from conditional probabilities	elicitation;arbre graphe;bayes estimation;modelizacion;processus gauss;fiabilidad;reliability;bayesian belief nets;belief;entropia;correlacion rango;high dimensionality;analisis estadistico;bayesian statistics;tree graph;securite;proceso markov;probabilidad condicional;correlation rang;probabilite conditionnelle;probabilistic approach;diagramme influence;modelisation;systeme incertain;estimacion bayes;croyance;uncertainty analysis;statistical analysis;hybrid method;marginal distribution;enfoque probabilista;approche probabiliste;processus markov;fiabilite;entropie;safety;analyse statistique;probability theory;markov process;random variable;graphical model;ley marginal;theorie probabilite;teoria probabilidad;elicitacion;dependent random variables;entropy;gaussian process;reseau neuronal;creencia;arbol grafo;proceso gauss;sistema incierto;seguridad;modeling;conditional probability;diagrama influencia;rank correlation;uncertain system;influence diagram;red neuronal;loi marginale;estimation bayes;neural network	Causes of uncertainties may be interrelated and may introduce dependencies. Ignoring these dependencies may lead to large errors. A number of graphical models in probability theory such as dependence trees, vines and (continuous) Bayesian belief nets [Cooke RM. Markov and entropy properties of tree and vine-dependent variables. In: Proceedings of the ASA section on Bayesian statistical science, 1997; Kurowicka D, Cooke RM. Distribution-free continuous Bayesian belief nets. In: Proceedings of mathematical methods in reliability conference, 2004; Bedford TJ, Cooke RM. Vines—a new graphical model for dependent random variables. Ann Stat 2002; 30(4):1031–68; Kurowicka D, Cooke RM. Uncertainty analysis with high dimensional dependence modelling. New York: Wiley; 2006; Hanea AM, et al. Hybrid methods for quantifying and analyzing Bayesian belief nets. In: Proceedings of the 2005 ENBIS5 conference, 2005; Shachter RD, Kenley CR. Gaussian influence diagrams. Manage Sci 1998; 35(5) [15].] have been developed to capture dependencies between random variables. The input for these models are various marginal distributions and dependence information, usually in the form of conditional rank correlations. Often expert elicitation is required. This paper focuses on dependence representation, and dependence elicitation. The techniques presented are illustrated with an application from aviation safety. r 2007 Elsevier Ltd. All rights reserved.	bayesian network;graphical model;influence diagram;john d. wiley;marginal model;markov chain;rm-odp;ruby document format	O. Morales;Dorota Kurowicka;A. L. C. Roelen	2008	Rel. Eng. & Sys. Safety	10.1016/j.ress.2007.03.020	econometrics;entropy;artificial intelligence;mathematics;artificial neural network;statistics	AI	26.360302654658526	-18.964604372805265	93977
a3c23aee2ecb1c98f20d291e169549d8e219e46d	technique for authenticating h.264/svc streams in surveillance applications	surveillance h 264 svc authentication;video surveillance;video streaming;image resolution;data compression;surveillance;authentication;code standards;static var compensators authentication feature extraction scalability surveillance vectors streaming media;video coding;image enhancement;vectors;streaming media;cryptography;feature extraction;video surveillance code standards cryptography data compression image enhancement image resolution image sequences video coding video streaming;computation complexity h 264 scalable video coding video surveillance sequence video streaming video enhancement layer video quality video resolution temporal scalability terminal device hybrid authentication scheme ausss authenticating svc surveillance stream cryptographic based authentication content based authentication video compression;static var compensators;scalability;image sequences	Surveillance streams coded by H.264/SVC (scalable video coding), which consist of one base layer and one or multiple enhancement layers, support flexible and various quality, resolution, and temporal scalabilities such that clients with different network bandwidth and terminal devices can seamlessly access them. In this paper, based on characteristics of SVC layer and surveillance sequences, we present an efficient and hybrid authentication scheme to ensure the integrity of SVC surveillance streams, named AUSSS (Authenticating SVC Surveillance Streams). Distinguished from the existing schemes, AUSSS exploits both cryptographic-based authentication and content-based authentication. Compared to existing authentication schemes, experimental results demonstrate that AUSSS causes less computation complexity and smaller compression overhead.	authentication;computation;cryptography;data compression;digital watermarking;key frame;overhead (computing);scalability;scalable video coding	Zhuo Wei;Robert H. Deng;Jialie Shen;Yongdong Wu;Xuhua Ding;Swee-Won Lo	2013	2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2013.6618259	data compression;scalability;image resolution;feature extraction;computer science;cryptography;authentication;internet privacy;computer security;statistics;computer network	DB	38.778365857937665	-12.967308835524502	94336
3a4aa3b2a3bdb164cd335fe5bed482b071105ae7	model-based experimental screening for doc parameter estimation	d optimal design;multivariate data analysis;parameter estimation;diesel oxidation catalyst;engine rig experiments	In the current study a parameter estimation method based on data screening by sensitivity analysis is presented. The method applied Multivariate Data Analysis (MVDA) on a large transient data set to select different subsets on which parameters estimation was performed. The subset was continuously updated as the parameter values developed using Principal Component Analysis (PCA) and D-optimal onion design. The measurement data was taken from a Diesel Oxidation Catalyst (DOC) connected to a full scale engine rig and both kinetic and mass transport parameters were estimated. The methodology was compared to a conventional parameter estimation method and it was concluded that the proposed method achieved a 32% lower residual sum of squares but also that it displayed less tendencies to converge to a local minima. The computational time was however significantly longer for the evaluated method.	estimation theory	Björn Lundberg;Jonas Sjöblom;Åsa Johansson;Björn Westerberg;Derek Creaser	2015	Computers & Chemical Engineering	10.1016/j.compchemeng.2015.01.004	diesel particulate filter;econometrics;simulation;engineering;optimal design;mathematics;multivariate analysis;estimation theory;statistics	DB	26.009401869636477	-17.529010890675522	94527
1be1113df266b1846428f10872f96d647ceb50f2	a method of h.264 video watermarking robust to attack on i and p frames by removal	watermarking;video watermarking discrete cosine transforms;psnr;watermarking robustness discrete cosine transforms psnr vectors streaming media transcoding;vectors;video on demand watermark video h 264 b frames high profile;streaming media;discrete cosine transforms;robustness;discrete cosine transfoms h 264 video watermarking p frames i frames h 264 high profile video low frequency dct residuals motion vectors transcoding video on demand systems;transcoding	The authors consider two new attacks on watermarked video specific to HD 60 fps H.264 High and Main profiles: selective I/P frames' removal and zeroing of residuals in B frames. We propose the method of embedding of a readable watermark for H.264 High Profile video robust to such attacks. The video stream is partially decoded removing entropy coding, then a watermark is embedded into low-frequency DCT residuals and motion vectors of B frames, so that it could be later extracted in an uncompressed domain even if almost all I and P frames were removed by the attacker during transcoding. The method is nonblind, robust to new attacks and to such attacks as transcoding, down-scale, and post processing filters due to robustness of low frequency DCT coefficients. The method consists of two stages: computationally intensive preprocessing stage performed to each video and low complexity payload embedding stage performed to each copy. So it is specifically suitable for fingerprinting of Video-on-Demand (VoD) systems, where a unique watermark should be embedded in each video's copy.	code;coefficient;digital video;digital watermarking;discrete cosine transform;discrete logarithm;download;embedded system;entropy encoding;file synchronization;fingerprint (computing);gaussian blur;group of pictures;h.264/mpeg-4 avc;preprocessor;streaming media;test suite;video compression picture types	Alexandra Afanasyeva;Denis Ivanov;Eugeny Linsky;Dmitry Ryzhov;Sergey Salishev	2014	2014 4th International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2014.7001951	reference frame;computer vision;transcoding;peak signal-to-noise ratio;digital watermarking;computer science;multimedia;motion compensation;video post-processing;statistics;robustness;computer graphics (images)	EDA	39.02642827095433	-12.6432227897791	94654
9d1dfdd44f1972cbcffc60ef8451b28f64a94bda	novelty search for soft robotic space exploration	cppn neat;soft robotics;space exploration;locomotion;cppn;voxcad;novelty search	The use of soft robots in future space exploration is still a far-fetched idea, but an attractive one. Soft robots are inherently compliant mechanisms that are well suited for locomotion on rough terrain as often faced in extra-planetary environments. Depending on the particular application and requirements, the best shape (or body morphology) and locomotion strategy for such robots will vary substantially. Recent developments in soft robotics and evolutionary optimization showed the possibility to simultaneously evolve the morphology and locomotion strategy in simulated trials. The use of techniques such as generative encoding and neural evolution were key to these findings. In this paper, we improve further on this methodology by introducing the use of a novelty measure during the evolution process. We compare fitness search and novelty search in different gravity levels and we consistently find novelty-based search to perform as good as or better than a fitness--based search, while also delivering a greater variety of designs. We propose a combination of the two techniques using fitness-elitism in novelty search to obtain a further improvement. We then use our methodology to evolve the gait and morphology of soft robots at different gravity levels, finding a taxonomy of possible locomotion strategies that are analyzed in the context of space-exploration.	galaxy morphological classification;mathematical optimization;planetary scanner;requirement;robot;robotic spacecraft;soft robotics;taxonomy (general)	Georgios Methenitis;Daniel Hennes;Dario Izzo;Arnoud Visser	2015		10.1145/2739480.2754731	simulation;artificial intelligence;space exploration;bio-inspired robotics	Robotics	25.483595007948576	-12.622968661022766	94684
43697341e7741edfc8e05b7cb5b98840e67095f5	a comparison of the maximum likelihood estimators under ranked set sapling some of its modifications	sample size;precision relative asymptotique;maximum likelihood;tamano muestra;random sampling;maximum vraisemblance;taille echantillon;estadistica rango;informacion fisher;ranked set sampling;estimator;estimador;asymptotic relative precision;maximum likelihood estimate;extreme ranked set sampling;muestreo aleatorio;percentile ranked set sampling;rank statistic;statistique rang;median ranked set sampling;echantillonnage aleatoire;simple random sampling;information fisher;maxima verosimilitud;fisher information;estimateur	This paper compares the maximum likelihood estimators (MLEs) of the parameters of the location-scale parameter family of distributions, under the methods of ranked set sampling (S.L. Stokes, Annals of the Institute of Statistical Mathematics 47 (3) (1995) 465), median ranked set sampling and extreme ranked set sampling (A.-B Shaibu, H.A. Muttlak, Annals of the Institute of Statistical Mathematics, submitted), and percentile ranked set sampling. Several distributions have been considered and the estimators are compared based on their asymptotic relative precision with respect to the corresponding simple random sample estimators.		A.-B. Shaibu;Hassen A. Muttlak	2002	Applied Mathematics and Computation	10.1016/S0096-3003(01)00055-8	econometrics;pattern recognition;m-estimator;mathematics;maximum likelihood;statistics	ML	31.08882393420211	-22.212976309668804	94692
0c7464899956d7988d8ba543c0456d94549c110e	inference of s-system models of genetic networks using a genetic local search	model based reasoning;system modeling;inference mechanisms;genetics;large scale;genetics genomics bioinformatics gene expression inference algorithms search methods differential equations large scale systems nonlinear equations kinetic theory;genetic network;genetic algorithms;search problems;search problems physiological models genetic algorithms inference mechanisms model based reasoning;local search;physiological models;search operators s system models genetic networks genetic local search genetic network inference problem	In this paper, w e propose a new me thod for the in fe rence of S-system mode l s of large-scale genet ic ne tworks . Th i s inethod employs a technique to decompose the genetic network inference p r o h l e m into several snhprohlenis, then appl ies a genetic local search t o each of the subproblems. A local search method utilizing the f e a t u r e of the S-system model is u s e d as o n e of t h e search operators in this gene t i c local search. Finally, the effectiveness of the proposed m e t h o d is verified th rough a gene t i c network inference problem.	field electron emission;gene regulatory network;local search (optimization)	Shuhei Kimura;Mariko Okada;Akihiko Konagaya	2003		10.1109/CEC.2003.1299635	quality control and genetic algorithms;mathematical optimization;systems modeling;genetic algorithm;adaptive neuro fuzzy inference system;computer science;bioinformatics;artificial intelligence;local search;model-based reasoning;genetic operator;machine learning;genetic representation;data mining;mathematics	AI	26.59964898949137	-10.410050747848212	94695
f0f405c6068ed2307be9d90bfc59b183ba90bfd8	a simulation technique for estimation in perturbed stochastic activity networks	monte carlo experiments;what if analysis;stochastic network;simulation technique;stochastic activity networks;pert	"""We are concerned with what-if analysis in estimating the expected value and the distribution function of completion time in stochastic activity networks. Widely used Monte Carlo simulation models of stochastic networks are often subject to errors caused by the estitnated parameter(s) of underlying input distribution function. """" What-if' analysis is needed to establish confidence with respect to small changes in the parameters of the input distributions. However, traditional """"what-if'analysis requires a separate simulation run for each input value. Recently, a method based on Likelihood Ratio (LR) for estimating performance function far several scenarios using a single-run extrapolation has been presented. In this paper, we experiment the use of this LR method in a network with exponential arc durations. We also consider the method with a nonlinear control random variate (NCRV) and compare it to crude Monte Carlo. The results show that the NCRV method induces variance reduction and is an effeetive filter t..."""		V. G. Adlakha;Hossein Arsham	1992	Simulation	10.1177/003754979205800406	stochastic neural network;econometrics;mathematical optimization;dynamic monte carlo method;hybrid monte carlo;program evaluation and review technique;computer science;systems engineering;stochastic tunneling;statistics	EDA	29.688920829916803	-16.696340637497485	94915
81bee2fb8aff0b2053d2fc2153b77ad5ddd78fde	state and parameter estimation of microbial growth processes	monod model;microbial systems;observers;state estimation;sensitivity analysis;parameter estimation;microbial growth	Based on a second order nonlinear model of microbial growth, two distinct problems-state estimation and parameter estimation-are considered. The first case assumes only one state is available from measurement and an asymptotic estimate of the other state is required. The second problem assumes both states are available, and considers the design of an estimator to provide asymptotic estimates of the growth system parameters.	estimation theory	S. Aborhey;Darrell Williamson	1978	Automatica	10.1016/0005-1098(78)90008-0	econometrics;bacterial growth;control theory;mathematics;estimation theory;sensitivity analysis;statistics	ML	33.63120362604762	-19.120989057907398	95237
6bc6b6030ee8b060099bd6949230990d15ecf21d	bootstrapping simultaneous confidence bands	mathematics computing;regression analysis;sampling methods;asymptotic method;bootstrap resampling;confidence band;regression line	Once a regression has been fitted to data, it is usually necessary to add confidence intervals to indicate the accuracy of the fitted regression line. This can easily be done for individual explanatory variable values. However sometimes confidence limits are needed simultaneously for the whole range of explanatory variable values of interest. In other words the problem is to construct a confidence band within which the entire unknown true regression line lies with given confidence. This article discusses computer intensive methods for doing this. The advantage of such methods compared with classical asymptotic methods is that accurate coverages can be obtained quite easily using bootstrap resampling.	resampling (statistics)	Russell C. H. Cheng	2005	Proceedings of the Winter Simulation Conference, 2005.		segmented regression;bootstrapping;econometrics;confidence interval;confidence distribution;confidence region;pattern recognition;mathematics;confidence and prediction bands;cdf-based nonparametric confidence interval;robust confidence intervals;statistics	Robotics	29.971489968660876	-21.8229074248797	95428
e167cdc2948c507bea4245fae2046b36b299c8cc	the asymptotic convexity of the negative likelihood function of garch models	garch;approximation asymptotique;fonction vraisemblance;optimisation;garch model;statistical simulation;convergence;computacion informatica;algorithm performance;optimizacion;analisis datos;maximum likelihood;probabilities mathematical statistics;maximum vraisemblance;65kxx;convexite;optimization method;numerical optimization;maximum likelihood estimation;metodo optimizacion;iterative algorithm;convergence numerique;convexidad;funcion verosimilitud;algorithme;49xx;numerical convergence;algorithm;data analysis;modele garch;iteraccion;simulacion estadistica;maximum likelihood estimate;resultado algoritmo;ciencias basicas y experimentales;simulation statistique;matematicas;statistical computation;calculo estadistico;performance algorithme;foreign exchange rates;methode optimisation;statistics;iteration;foreign exchange rate;taux change;exchange rate;simulation study;analyse donnee;calcul statistique;optimization;asymptotic approximation;convexity;grupo a;convergencia numerica;likelihood function;maxima verosimilitud;tasa cambio;aproximacion asintotica;algoritmo	We prove the convexity of the negative likelihood function in the asymptotic sense for GARCH models. This property provides assurance for the convergence of numerical optimization algorithms for maximum likelihood estimation of GARCH. A simulation study is conducted in order to compare the performance of several different iteration algorithms. An example based on the log-returns of foreign exchange rates is also given. © 2004 Elsevier B.V. All rights reserved.	algorithm;asymptote;foreign exchange service (telecommunications);iteration;mathematical optimization;numerical analysis;simulation	Wai-Cheung Ip;Heung Wong;J. Z. Pan;D. F. Li	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2004.08.012	econometrics;calculus;mathematics;maximum likelihood;quasi-maximum likelihood;statistics	AI	33.308317687780935	-23.368593930602735	95929
ce90e8c314892eb809e5dea3a2ac37ef1408fd23	binning for efficient stochastic multiscale particle simulations	continuous time markov process;65c05;coagulation;60j28;atmospheric aerosol;60j27;monte carlo;stochastic simulation algorithm	Gillespie’s Stochastic Simulation Algorithm (SSA) is an exact procedure for simulating the evolution of a collection of discrete, interacting entities, such as coalescing aerosol particles or reacting chemical species. The high computational cost of SSA has motivated the development of more efficient variants, such as Tau-Leaping, which sacrifices the exactness of SSA. For models whose interacting entities can be characterized by a continuous parameter, such as a measure of size for aerosol particles, we analyze strategies for accelerating these algorithms by aggregating particles of similar size into bins. We show that for such models an appropriate binning strategy can dramatically enhance efficiency, and in particular can make SSA computationally competitive without sacrificing exactness. These strategies are especially effective for highly multiscale problems. We formulate binned versions of both the SSA and Tau-Leaping algorithms and analyze and demonstrate their performance.	algorithmic efficiency;computation;computer simulation;entity;gillespie algorithm;interaction;product binning;tau-leaping	M. D. Michelotti;M. T. Heath;Matthew West	2013	Multiscale Modeling & Simulation	10.1137/130908038	econometrics;mathematical optimization;simulation;mathematics;statistics;monte carlo method	AI	33.60358570781474	-14.995740730180355	96027
72bc1db098f450b30a075e023cc7b024a9114269	security analysis of a key based color image watermarking vs. a non-key based technique in telemedicine applications		Recently, digital watermarking has become an important technique to preserve patients’ privacy in telemedicine applications. Since, medical information are highly sensitive, security of watermarked medical images becomes a critical issue in telemedicine applications. In this paper, two targeted attacks have been proposed against a key based color image watermarking scheme and also a non-key based one, in order to evaluate their security in telemedicine applications. The target schemes are SVD-based and QR-based color image watermarking algorithms, which their embedding procedures are quit the same. The proposed attacks exploit the prior knowledge of the watermarking algorithms to make changes in the exact embedding spaces. Thus, these changes would cause disruption in extraction procedure. Our experimental results show that the key based watermarking scheme is more secure than the non-key based one. This is because the proposed targeted attack needs to distort the key based watermarked images more than non-key based ones to remove the embedded watermarks. Our proposed targeted attacks also have more efficient performance in removing watermarks than other general attacks such as JPEG compression, Gaussian noise and etc. Finally, these attacks have been proposed to show the vulnerabilities of watermarking schemes in order to help the designers to implement more secure schemes.	algorithm;ct scan;color image;denial-of-service attack;digital watermarking;distortion;elegant degradation;embedded system;experiment;jpeg;kirchhoff's theorem;median filter;neural correlates of consciousness;peak signal-to-noise ratio;scheme;singular value decomposition;structural similarity;watermark (data file)	Pegah Nikbakht Bideh;Mojtaba Mahdavi;Shahram Etemadi Borujeni;Sima Arasteh	2018	Multimedia Tools and Applications	10.1007/s11042-018-6218-8	computer vision;computer science;gaussian noise;artificial intelligence;digital watermarking;security analysis;color image;exploit;jpeg;telemedicine	EDA	38.78495328255403	-11.480871511489742	96044
c27570cd230f858fadd8853fcacb048d54c18c74	simple genetic algorithm with &alpha -selection - intrinsic system model, fixed points and the fixed point graph	system modeling;fixed point	Genetic algorithms (GA) are instances of random heuristic search (RHS) which mimic biological evolution and molecular genetics in simplified form. These random search algorithms can be theoretically described with the help of a deterministic dynamical system model by which the stochastic trajectory of a population can be characterized using a deterministic heuristic function and its fixed points. For practical problem sizes the determination of the fixed points is unfeasible even for the simple genetic algorithm (SGA). The recently introduced simple genetic algorithm with α-selection allows the analytical calculation of the unique fixed points of the dynamical system model. In this paper, an overview of the theoretical results for the simple genetic algorithm withα-selection and its corresponding intrinsic system model is given. Further, the connection to the fixed point graph is illustrated which describes the asymptotic behavior of the simple genetic algorithm. In addition to the theoretical analysis experimental results for the simple genetic algorithm with α-selection, uniform crossover and bitwise mutation are presented.	bitwise operation;crossover (genetic algorithm);dynamical system;evolution;fixed point (mathematics);genetic algorithm;heuristic (computer science);random search;search algorithm;steady state;synthetic genetic array;turing completeness	André Neubauer	2010				AI	27.407025665577137	-10.240000255052761	96156
9789297c159562723adff7ab66b4e1b03ceee4ac	cross-calibration of categorical variables: an evaluation of the genetic algorithm approach		Abstract This paper highlights the importance of the cross-calibration of categorical variables, models cross-calibration as the forecast of a joint probability distribution, and proposes a non-traditional method that can be applied to any observed sample of joint data points. The sample is generally distorted due to measurement errors and differences among raters. The approach uses a genetic algorithm that predicts the true joint probability of two categorical variables. Unlike existing methods, the proposed approach does not explicitly account for any prior knowledge, does not impose any constraint, does not define a specific agreement, and does not specify the type of dependence that exists between the variables. However, the approach produces good logical estimates of the probability forecast both at a specific point in time and longitudinally across time. The computational investigation quantifies this performance using different scoring measures and provides computational evidence of its validity and superiority.		Rym M'Hallah;Suja Aboukhamseen	2019	Appl. Soft Comput.	10.1016/j.asoc.2018.10.009	mathematics;machine learning;genetic algorithm;observational error;algorithm;artificial intelligence;data point;categorical variable;joint probability distribution;cross calibration	Logic	24.983949155767434	-20.839028664172517	96249
afbc2e66c971ba99505b04adcd02a56948bf49cf	efficient global optimisation for black-box simulation via sequential intrinsic kriging		Efficient Global Optimization (EGO) is a popular method that searches sequentially for the global optimum of a simulated system. EGO treats the simulation model as a black-box, and balances local and global searches. In deterministic simulation, EGO uses ordinary Kriging (OK), which is a special case of universal Kriging (UK). In our EGO variant we use intrinsic Kriging (IK), which eliminates the need to estimate the parameters that quantify the trend in UK. In random simulation, EGO uses stochastic Kriging (SK), but we use stochastic IK (SIK). Moreover, in random simulation, EGO needs to select the number of replications per simulated input combination, accounting for the heteroscedastic variances of the simulation outputs. A popular selection method uses optimal computer budget allocation (OCBA), which allocates the available total number of replications over simulated combinations. We derive a new allocation algorithm. We perform several numerical experiments with deterministic simulations and random simulations. These experiments suggest that (1) in deterministic simulations, EGO with IK outperforms classic EGO; (2) in random simulations, EGO with SIK and our allocation rule does not differ significantly from EGO with SK combined with the OCBA allocation rule.		Ehsan Mehdad;Jack P. C. Kleijnen	2018	JORS	10.1080/01605682.2017.1409154		Metrics	28.33119475999638	-12.785293875254013	96357
896b02dc63e77e1bc8435a2a8753566352823969	multiplicative-binomial distribution: some results on characterization, inference and random data generation	profile likelihood;zero inflated data;likelihood ratio test;binomial distribution;over dispersion	Multiplicative-binomial distribution is one of the distributions that allows for over-dispersion, and underdispersion relative to the standard binomial distribution. It will be shown that the multiplicative-binomial distribution can be a very useful model for these situations. Moreover, the confidence interval for the parameters of the multiplicative-binomial distribution is investigated by the profile likelihood methods. The first four moments and simulation procedures for generating data from the multiplicative-binomial distribution using R-software are given. By using four applications to simulated and real data it is shown that the multiplicative-binomial distribution is the same as or outperforming the standard binomial and beta-binomial distributions.	line mode browser;norm (social);simulation	Elsayed A. H. Elamir	2013	JSTA	10.2991/jsta.2013.12.1.8	gumbel distribution;beta-binomial distribution;econometrics;geometric distribution;quasi-likelihood;combinatorics;posterior predictive distribution;beta negative binomial distribution;heavy-tailed distribution;univariate distribution;binomial distribution;index of dispersion;count data;inverse-chi-squared distribution;mathematics;compound probability distribution;sampling distribution;negative binomial distribution;binomial proportion confidence interval;binomial test;multinomial distribution;statistics;continuity correction;distribution fitting	Theory	30.80737966919157	-21.513269341144486	96470
6e25c88fa27ff86279afbf5b074271f5feb9d736	nonparametric modeling and break point detection for time series signal of counts		This paper considers the problem of flexible modeling as well as break point detection for time series signal of counts. In particular, the Poisson Generalized Autoregressive Moving Average (GARMA) models paired with radial basis expansions are used to fit such signals. A genetic algorithm is developed to find the possible breaks and the best fitting model derived from the minimum description length principle. The empirical performance of the proposed methodology is illustrated via a simulation study and a practical analysis of the bursts in the BATSE gamma ray data. Lastly, the consistency of the estimated break points and the model parameters is established under some regularity conditions.	time series	Qi Gao;Thomas C. M. Lee;Chun Yip Yau	2017	Signal Processing	10.1016/j.sigpro.2017.03.034	econometrics;mathematical optimization;mathematics;statistics	ML	29.08967522341399	-21.472405380342682	96476
a9d7ad1c6e0ee8f9185e7f6fc8266068fd0388b0	identifiability of compartmental models: algorithms to solve an actual problem by means of symbolic calculus	compartmental model;mathematical model	The problem considered in the present paper concerns a structural identifiability test for an appealing class of mathematical models, the so called compartmental models, which are widely employed in several fields of biology, medicine, ecology, etc.	algorithm;ecology;mathematical model;multi-compartment model	Annalisa Bossi;Livio Colussi;Claudio Cobelli;Giorgio Romanin-Jacur	1980	ACM SIGSAM Bulletin	10.1145/1089235.1089239	econometrics;mathematical optimization;machine learning;mathematical model;mathematics	SE	34.447655860543236	-14.426791869957112	96551
1f39640271d7444b7321f3c68923e05e88bdfb54	an asymptotic expansion for the incomplete beta function	loi student;distribution;ji cuadrado;student s distribution;asymptotic expansion;gamma function;khi deux;loi f;developpement asymptotique;funcion beta;beta function;ley f;ley student;chi square;desarrollo asintotico;chi square distribution;numerical methods;f distribution;american mathematical society;incomplete beta function;fonction gamma;gamma function ratio;student distribution;funcion gama;methode numerique;fonction beta	A new asymptotic expansion is derived for the incomplete beta function I(a, b, x), which is suitable for large a, small b and x > 0.5. This expansion is of the form I(a, b, x) ∼ Q(b,−γ log x) + Γ(a+ b) Γ(a)Γ(b) x ∞ ∑	emoticon	B. G. S. Doman	1996	Math. Comput.	10.1090/S0025-5718-96-00729-6	beta function;calculus;mathematics;statistics;asymptotic expansion	Logic	33.92761334776511	-20.677164795587238	96852
89ae2d466feee8863774ca0401d8ce31ed187b1d	an evaluation of the bootstrap for model validation in mixture models	nonparametric bootstrap;regression mixture models;model validation;finite mixture models;leave k out cross validation	Bootstrapping has been used as a diagnostic tool for validating model results for a wide array of statistical models. Here we evaluate the use of the non-parametric bootstrap for model validation in mixture models. We show that the bootstrap is problematic for validating the results of class enumeration and demonstrating the stability of parameter estimates in both finite mixture and regression mixture models. In only 44% of simulations did bootstrapping detect the correct number of classes in at least 90% of the bootstrap samples for a finite mixture model without any model violations. For regression mixture models and cases with violated model assumptions, the performance was even worse. Consequently, we cannot recommend the non-parametric bootstrap for validating mixture models. The cause of the problem is that when resampling is used influential individual observations have a high likelihood of being sampled many times. The presence of multiple replications of even moderately extreme observations is shown to lead to additional latent classes being extracted. To verify that these replications cause the problems we show that leave-k-out cross-validation where sub-samples taken without replacement does not suffer from the same problem.		Thomas Jaki;Ting-Li Su;Minjung Kim;M. Lee Van Horn	2018	Communications in statistics: Simulation and computation	10.1080/03610918.2017.1303726	econometrics;bootstrap aggregating;mathematics;regression model validation;statistics	ML	28.98013261224931	-23.883865994971913	97111
48f81daec5378cfcbe6c839d06b25570ccbfb959	reliability analysis of stochastic structural systems using theory of random renewal pulse process	reliability analysis		reliability engineering	W. Shiraki;M. Tsunekuni;S. Matsuho	1993			reliability engineering;econometrics;statistics	Robotics	29.71184038364497	-19.124420946266078	97247
437a282e4a4e7c62ca21f13a32b2066004cb3e13	weibull component reliability-prediction in the presence of masked data	forecasting;iterative method;prevision;fiabilidad;reliability;intervalo confianza;maximum likelihood;operant conditioning;operating conditions;maximum vraisemblance;ley weibull;maximum likelihood estimation;reliability theory;metodo iterativo;failure analysis;iterative methods;weibull distribution;confidence interval;masquage;competing risks model;maximum likelihood estimate;condition operatoire;iterative methods reliability theory failure analysis weibull distribution maximum likelihood estimation;operating system;methode iterative;enmascaramiento;fiabilite;intervalle confiance;defaillance;system failure weibull component reliability prediction masked data likelihood function iterative procedure maximum likelihood estimates confidence intervals life distribution parameters;masking;failures;condicion operatoria;loi weibull;fallo;likelihood function;maxima verosimilitud;maximum likelihood estimation life estimation assembly systems failure analysis data analysis life testing system testing degradation manufacturing closed form solution	Analysts are often interested in obtaining component reliabilities by analyzing system-life data. This is generally done by making a series-system assumption and applying a competing-risks model. These estimates are useful because they reflect component reliabilities after assembly into an operational system under true operating conditions. The fact that most new systems under development contain a large proportion of old technology also supports the approach. In practice, however, this type of analysis is often confounded by the problem of masking (the exact cause of system failure is unknown). This paper derives a likelihood function for the masked-data case and presents an iterative procedure (IMLEP) for finding maximum likelihood estimates and confidence intervals of Weibull component life-distribution parameters. The approach is illustrated with a simple numerical example.		J. S. Usher	1996	IEEE Trans. Reliability	10.1109/24.510806	reliability engineering;econometrics;mathematics;maximum likelihood;statistics	Embedded	29.566300994545525	-18.19429081809289	97495
bd2ca569a680a4791722787acd60793285b1d1b2	a test for convex dominance with respect to the exponential class based on an $l^1$ distance	exponential distribution;harmonic new better than used in expectation;exponential distributions;bridges;l 1 distance;random variables;aging;reliability theory;exponential class;trajectory;hnbue;random processes;asymptotic test;asymptotic properties;l 1 norm;empirical distributions;statistical testing;convex dominance;distribution functions;null hypothesis;nonnegative random variable;article;convex order;test statistic;ageing classes of distributions;ageing distributions;harmonic analysis	We consider the problem of testing if a non-negative random variable is dominated, in the convex order, by the exponential class. Under the null hypothesis, the variable is harmonic new better than used in expectation (HNBUE), a well-known class of ageing distributions in reliability theory. As a test statistic, we propose the L1 norm of a suitable distance between the empirical and the exponential distributions, and we completely determine its asymptotic properties. The practical performance of our proposal is illustrated with simulation studies, which show that the asymptotic test has a good behavior and power, even for small sample sizes. Finally, three real data sets are analyzed.		Amparo Baíllo;Javier Cárcamo;Sofia Nieto	2015	IEEE Transactions on Reliability	10.1109/TR.2014.2355534	exponential distribution;random variable;stochastic process;mathematical optimization;statistical hypothesis testing;combinatorics;test statistic;reliability theory;trajectory;distribution function;harmonic analysis;mathematics;null hypothesis;statistics	Metrics	31.836434899861853	-19.095393686056664	97623
8c11695fcb4ec1079e06e026ead14283a209454a	a non-gaussian kalman filter with application to the estimation of vehicular speed	modelo lineal generalizado;modelo dinamico;conjunto recursivo;modelizacion;ecoulement trafic;forecasting;processus gauss;moyenne ponderee;prevision;product service design;filtro kalman;modele lineaire generalise;competitividad;road traffic;linear models gaussian curve state space models estimation forecasting closed loop;gestion trafic;filtre kalman;weighting;dynamic model;espace etat;volume;estimacion promedio;grupo de excelencia;armonica;kalman filter;ensemble recursif;harmonic;aprendizaje probabilidades;ruido no gaussiano;traffic flow;curva gauss;traffic management;ponderacion;non gaussian noise;filtre kalman etendu;scenario;modelisation;non gaussian kalman filter;volumen;trafic routier;reciprocal inverse gaussian distribution;harmonique;route;state space method;argumento;methode espace etat;weighted average;ciencias basicas y experimentales;state space;matematicas;modele dynamique;script;inferencia;coste;competitiveness;loi normale;gestion trafico;promedio pondero;carretera;apprentissage probabilites;filtro kalman extendido;trafico carretera;recursive set;highway;generalized linear model;ponderation;gaussian process;mean estimation;espacio estado;extended kalman filter;state space model;estimation moyenne;proceso gauss;competitivite;modeling;flujo trafico;article;gaussian distribution;inference;probability learning;dynamic generalized linear model;metodo espacio estado;cout;bruit non gaussien	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;kalman filter;primary source	Baibing Li	2009	Technometrics	10.1198/TECH.2009.0017	normal distribution;kalman filter;route;econometrics;active traffic management;ensemble kalman filter;systems modeling;input/output;forecasting;weighted arithmetic mean;state space;scenario;state-space representation;calculus;traffic flow;harmonic;generalized linear model;gaussian process;weighting;mathematics;extended kalman filter;volume;recursive set;statistics	Robotics	35.74907608412121	-19.584761233892653	98131
b507f6d03c18c7aae3250a4657c011a8b3b3b186	normal distribution based pseudo ml for missing data: with applications to mean and covariance structure analysis	estimacion sesgada;error medida;linear combination;metodo estadistico;fonction vraisemblance;analyse multivariable;62f12;covariance analysis;covariancia;measurement error;coefficient beta 2 pearson;multivariate analysis;maximum likelihood;normal distribution;best approximation;dato que falta;fonction repartition;data;aproximacion;maximum vraisemblance;variance analysis;coefficient of kurtosis;estimacion promedio;matrice covariance;missing completely at random;covariance;statistical method;matriz covariancia;curva gauss;distribucion estadistica;statistical regression;statistical model;funcion verosimilitud;data model;approximation;donnee manquante;erreur mesure;estimating equation;funcion distribucion;distribution function;estimation erreur;factor model;analyse covariance;consistent estimator;distribution statistique;sandwich type covariance matrix;marginal distribution;missing;combinacion lineal;factor analysis;error estimation;methode statistique;analisis variancia;regresion estadistica;62p15;missing at random;modele factoriel;62j10;estimacion error;62h25;62e10;loi normale;mejor aproximacion;ley marginal;analisis multivariable;coefficient of skewness;missing data;coefficient beta 1 pearson;mean estimation;parameter estimation;skewness;analisis covariancia;not missing at random;estimation statistique;regression statistique;estimation moyenne;estimacion estadistica;60e05;asymptotic normal;statistical estimation;biased estimation;covariance structure;statistical distribution;likelihood function;estimation biaisee;consistency;gaussian distribution;maxima verosimilitud;loi marginale;combinaison lineaire;analyse variance;covariance matrix;62e20;asymptotic bias consistency estimating equation factor analysis sandwich type covariance matrix not missing at random;asymptotic bias;meilleure approximation	When missing data are either missing completely at random (MCAR) or missing at random (MAR), the maximum likelihood (ML) estimation procedure preserves many of its properties. However, in any statistical modeling, the distribution specification for the likelihood function is at best only an approximation to the real world. In particular, since the normal-distribution-based ML is typically applied to data with heterogeneous marginal skewness and kurtosis, it is necessary to know whether such a practice still generates consistent parameter estimates. When the manifest variables are linear combinations of independent random components and missing data are MAR, this paper shows that the normal-distribution-based MLE is consistent regardless of the distribution of the sample. Examples also show that the consistency of the MLE is not guaranteed for all nonnormally distributed samples. When the population follows a confirmatory factor model, and data are missing due to the magnitude of the factors, the MLE may not be consistent even when data are normally distributed. When data are missing due to the magnitude of measurement errors/uniqueness, MLEs for many of the covariance parameters related to the missing variables are still consistent. This paper also identifies and discusses the factors that affect the asymptotic biases of the MLE when data are not missing at random. In addition, the paper also shows that, under certain data models and MAR mechanism, the MLE is asymptotically normally distributed and the asymptotic covariance matrix is consistently estimated by the commonly used sandwich-type covariance matrix. The results indicate that certain formulas and/or conclusions in the existing literature may not be entirely correct.	missing data	Ke-Hai Yuan	2009	J. Multivariate Analysis	10.1016/j.jmva.2009.05.001	normal distribution;estimation of covariance matrices;econometrics;missing data;covariance;calculus;mathematics;factor analysis;imputation;statistics	Metrics	32.780151986381775	-22.497228297567297	98141
47d1dbe6500500ace74b6b1cea5d70b4825b64ed	importance sampling based defuzzification for general type-2 fuzzy sets	type 2 fuzzy sets;computational complexity importance sampling defuzzification type 2 fuzzy sets type 2 fuzzy logic systems ubiquitous uncertainty;ubiquitous uncertainty;monte carlo methods fuzzy sets uncertainty probability distribution random variables testing frequency selective surfaces;fuzzy set;uncertainty;type 2 fuzzy logic systems;random variables;testing;uncertainty handling;fuzzy set theory;uncertainty handling computational complexity fuzzy logic fuzzy set theory importance sampling;fuzzy sets;fuzzy logic;computational complexity;probability distribution;type 2 fuzzy set;fuzzy logic system;importance sampling;probability distribution function;monte carlo methods;defuzzification;frequency selective surfaces	General type-2 fuzzy logic systems (T2 FLS) constitute a powerful tool for coping with ubiquitous uncertainty in many engineering applications. However, the immense computational complexity associated with defuzzification of general T2 fuzzy sets still remains an unresolved issue and prohibits its practical use. This paper proposes a novel importance sampling based defuzzification method for general T2 FLS. Here, a subset from the domain of all embedded fuzzy sets is randomly sampled using a specific probability distribution function. The algorithm is compared with the previously published uniform sampling defuzzification method. Experimental results demonstrate that importance sampling substantially reduces the variance of the sampling defuzzification method. Comparison of T2FLS output surfaces showed that smoother and more stable response can be achieved with the proposed importance sampling based defuzzification method.	algorithm;computational complexity theory;defuzzification;embedded system;formal system;free library of springfield township;fuzzy logic;fuzzy set;importance sampling;randomness;sampling (signal processing);type-2 fuzzy sets and systems	Ondrej Linda;Milos Manic	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584256	mathematical optimization;discrete mathematics;defuzzification;computer science;machine learning;mathematics;fuzzy set;statistics	Robotics	26.3746843712097	-16.38192728949763	98434
6ffc3c4548ad03e70224a7009c8032e289e0e568	effects of parameter estimation on the multivariate distribution-free phase ii sign ewma chart	msewma chart;statistical process control;average run length;moving average ma chart;multivariate control chart;nonparametric procedure	Multivariate nonparametric control charts can be very useful in practice and have recently drawn a lot of interest in the literature. Phase II distribution-free (nonparametric) control charts are used when the parameters of the underlying unknown continuous distribution are unknown and can be estimated from a sufficiently large Phase I reference sample. While a number of recent studies have examined the in-control (IC) robustness question related to the size of the reference sample for both univariate and multivariate normal theory (parametric) charts, in this paper, we study the effect of parameter estimation on the performance of the multivariate nonparametric sign exponentially weighted moving average (MSEWMA) chart. The in-control average run-length (ICARL) robustness and the out-of-control shift detection performance are both examined. It is observed that the required amount of the Phase I data can be very (perhaps impractically) high if one wants to use the control limits given for the known parameter case and maintain a nominal ICARL, which can limit the implementation of these useful charts in practice. To remedy this situation, using simulations, we obtain the “corrected for estimation” control limits that achieve a desired nominal ICARL value when parameters are estimated for a given set of Phase I data. The out-of-control performance of the MSEWMA chart with the correct control limits is also studied. The use of the corrected control limits with specific amounts of available reference sample is recommended. Otherwise, the performance the MSEWMA chart may be seriously affected under parameter estimation. Copyright © 2016 John Wiley & Sons, Ltd.	code;estimation theory;gantt chart;john d. wiley;real life;run-length encoding;simulation;step detection;supercomputer;technical support	Y. H. Dovoedo;Subha Chakraborti	2017	Quality and Reliability Eng. Int.	10.1002/qre.2019	ewma chart;\bar x and s chart;reliability engineering;econometrics;control limits;computer science;operations management;mathematics;shewhart individuals control chart;u-chart;x-bar chart;\bar x and r chart;radar chart;statistical process control;statistics;multi-vari chart	ML	28.245337092642824	-19.516849646942777	98844
0f774d987052e1be92a91f97403844d38e1cc3f4	analysis of social learning strategies when discovering and maintaining behaviours inaccessible to incremental genetic evolution		It has been demonstrated that social learning can enable agents to discover and maintain behaviours that are inaccessible to incremental genetic evolution alone. However, previous models investigating the ability of social learning to provide access to these inaccessible behaviours are often limited. Here we investigate teacher-learner social learning strategies. It is often the case that teachers in teacher-learner social learning models are restricted to one type of agent, be it a parent or some fit individual; here we broaden this exploration to include a variety of teachers to investigate whether these social learning strategies are also able to demonstrate access to, and maintenance of, behaviours inaccessible to incremental genetic evolution. In this work new agents learn from either a parent, the fittest individual, the oldest individual, a random individual or another young agent. Agents are tasked with solving a river crossing task, with new agents learning from a teacher in mock evaluations. The behaviour necessary to successfully complete the most difficult version of the task has been shown to be inaccessible to incremental genetic evolution alone, but achievable using a combination of social learning and noise in the Genotype-Phenotype map. Here we show that this result is robust in all of the teacher-learner social learning strategies explored here.	color gradient;evolution;hill climbing;mock object;parallels desktop for mac	Ben P. Jolley;James M. Borg;Alastair Channon	2016		10.1007/978-3-319-43488-9_26	knowledge management;artificial intelligence;management science	AI	24.885561409548306	-10.66650357687025	99222
5c89b2e839c1a1486e7194d40cc779611a46a887	phase-i risk-adjusted geometric control charts to monitor health-care systems	risk adjusted control charts;likelihood ratio test;change point;phase i	Because of the importance of health-care processes to people life, researchers attempted to reduce death rates using riskadjusted control charts. In this paper, the number of patients survived at least 30days after a surgery is monitored using a novel risk-adjusted geometric control chart. In this chart, the patient risk is modeled using a logistic regression. The new scheme is proposed to be used in Phase-I where a likelihood ratio test derived from a change-point model is employed. The application of the proposed chart is demonstrated in a case study. Furthermore, through simulation studies, it is shown that the proposed control chart is more effective in terms of power than the chart with a binary random variable. Copyright © 2014 John Wiley & Sons, Ltd.	binary data;chart;john d. wiley;logistic regression;simulation	Fatemeh Mohammadian;Seyed Taghi Akhavan Niaki;Amirhossein Amiri	2016	Quality and Reliability Eng. Int.	10.1002/qre.1722	reliability engineering;likelihood-ratio test;computer science;operations management;mathematics;statistics;multi-vari chart	ML	28.21586586765302	-19.41345970449454	99560
bf9a1db0283081791aae0be727a426de0c00a506	efficient aging-aware failure probability estimation using augmented reliability and subset simulation			subset simulation	Hiromitsu Awano;Takashi Sato	2017	IEICE Transactions		mathematics;machine learning;theoretical computer science;subset simulation;monte carlo method;artificial intelligence	Visualization	24.62337535364382	-16.251697560480572	99615
8b729d59132076a785b150d14247265bc093904c	guidelines for using variable selection techniques in data envelopment analysis	model specification;efficiency estimation;data envelopment analysis;data envelopment analysis model specification efficiency estimation	Model misspecification has significant impacts on Data Envelopment Analysis (DEA) efficiency estimates. This paper discusses the four most widely-used approaches to guide variable specification in DEA. We analyze Efficiency Contribution Measure (ECM), Principal Component Analysis (PCA-DEA), a regression-based test, and bootstrapping for variable selection via Monte Carlo simulations to determine each approach’s advantages and disadvantages. For a three input, one output production process, we find that: PCA-DEA performs well with highly correlated inputs (greater than 0.8) and even for small data sets (less than 300 observations); both the regression and ECM approaches perform well under low correlation (less than 0.2) and relatively larger data sets (at least 300 observations); and bootstrapping performs relatively poorly. Bootstrapping requires hours of computational time whereas the three other methods require minutes. Based on the results, we offer guidelines for effectively choosing among the four selection methods.	computation;data envelopment analysis;feature selection;monte carlo method;principal component analysis;simulation;time complexity	Niranjan R. Nataraja;Andrew L. Johnson	2011	European Journal of Operational Research	10.1016/j.ejor.2011.06.045	econometrics;data mining;data envelopment analysis;mathematics;specification;statistics	Metrics	27.43398707456383	-23.71191097873434	99761
24192dbd8ca25323e794e6776b80b5969749af19	bayesian causal effects in quantiles: accounting for heteroscedasticity	bayes estimation;experimental design;garch;stock return;metodo estadistico;analisis numerico;test statistique;international stock markets;garch model;cuantila;chaine markov;cadena markov;metodo monte carlo;skewness measure;theorie approximation;finance;65c05;analisis datos;causal effect;stochastic method;05bxx;rentabilite boursiere;fonction repartition;quantile regression;citations;impact factor;regression quantile;test estadistico;62e17;plan experiencia;statistical test;methode monte carlo;regression model;statistical method;h index;distribucion estadistica;statistical regression;62jxx;65c40;analyse numerique;stock markets;approximation theory;marche valeurs;bayesian;funcion distribucion;data analysis;modelo regresion;estimacion bayes;distribution function;modele garch;numerical analysis;62k99;distribution statistique;sciences actuarielles;plan experience;heteroscedasticidad;markov chain monte carlo;methode statistique;regresion estadistica;modele regression;monte carlo method;mesure asymetrie;heteroscedasticite;stock returns;statistical computation;calculo estadistico;rankings;methode stochastique;analyse donnee;calcul statistique;62p05;quantile;60j10;skewed laplace distribution;economics;heteroscedasticity;causal relation;estimation statistique;regression statistique;estimacion estadistica;60e05;statistical estimation;statistical distribution;finanzas;research impact;research production;estimation bayes;laplace distribution;granger non causality in quantiles;markov chain;metodo estocastico	Testing for Granger non-causality over varying quantile levels could be used to measure and infer dynamic linkages, enabling the identification of quantiles for which causality is relevant, or not. However, dynamic quantiles in financial application settings are clearly affected by heteroscedasticity, as well as the exogenous and endogenous variables under consideration. GARCH-type dynamics are added to the standard quantile regression model, so as to more robustly examine quantile causal relations between dynamic variables. An adaptive Bayesian Markov chain Monte Carlo scheme, exploiting the link between quantile regression and the skewed-Laplace distribution, is designed for estimation and inference of the quantile causal relations, simultaneously estimating and accounting for heteroscedasticity. Dynamic quantile linkages for the international stock markets in Taiwan and Hong Kong are considered over a range of quantile levels.	causal filter;causality;markov chain monte carlo;monte carlo method	Cathy W. S. Chen;Richard Gerlach;D. C. M. Wei	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.12.014	autoregressive conditional heteroskedasticity;econometrics;mathematics;regression analysis;statistics	AI	33.032415395564335	-22.041008527149817	99816
b5ee6d86c7647423e5314f4b75c8fe9ac1a48b0d	simple tests for peakedness, fat tails and leptokurtosis based on quantiles	test hypothese;classification automatique statistiques;stock return;approximation asymptotique;variation journaliere;theorie echantillonnage;statistical moment;metodo estadistico;teoria muestreo;analyse multivariable;sample size;estimator robustness;cuantila;metodo monte carlo;empirical relation;theorie approximation;multivariate analysis;fat tail;loi probabilite;ley probabilidad;variable aleatoire;rentabilite boursiere;fonction repartition;estadistica test;statistique test;tamano muestra;test hipotesis;moment statistique;62e17;methode monte carlo;variable aleatoria;taille echantillon;probabilidad cola;return distribution;outlier;statistical method;tail probability;heavy tail;α robustness;ecuesta estadistica;normality test;curva gauss;distribucion estadistica;loi asymptotique;test normalite;discriminant analysis;analyse discriminante;approximation theory;observacion aberrante;funcion distribucion;sample survey;momento estadistico;analisis discriminante;distribution function;heavy tails;robustez estimador;estimation erreur;distribution statistique;62h30;error estimation;methode statistique;monte carlo method;robustesse;probability distribution;relation empirique;estructura datos;test normalidad;estimacion error;probabilite queue;random variable;62d05;loi normale;observation aberrante;analisis multivariable;robustness;structure donnee;asymptotic distribution;quantile;relacion empirica;asymptotic approximation;variacion diaria;monte carlo simulation;2000;data structure;validity robustness;statistical distribution;sondage statistique;gaussian distribution;test statistic;daily variation;aproximacion asintotica;selector statistics;62e20;robustez;sampling theory;robustesse estimateur;hypothesis test	The conventional test for leptokurtosis is based on the fourth moment of the standardized sample. This test su,ers from various weaknesses: It cannot account for peakedness and fat tails separately and it is extremely sensitive with respect to outliers resulting in a lack of robustness concerning the error of the 1rst kind. We suggest alternative tests for fat tails, peakedness and leptokurtosis. They are based on selector statistics as introduced by Hogg. Quantiles of the test statistics under normality for sample sizes of up to 2000 are derived by Monte-Carlo simulation. The asymptotic distribution is derived analytically as well. We apply the tests to daily, weekly and monthly asset returns of three German stocks. One can conclude that the new tests give a much more subtle picture of the structure of leptokurtosis in the data than the conventional test. c © 2003 Elsevier Science B.V. All rights reserved.	file allocation table;monte carlo method;simulation;tails	Friedrich Schmid;Mark Trede	2003	Computational Statistics & Data Analysis	10.1016/S0167-9473(02)00170-6	probability distribution;econometrics;data structure;calculus;mathematics;linear discriminant analysis;statistics;monte carlo method	ML	32.8500169392935	-22.148779283922856	99853
99c6f7518078df26ff245eb1a03ce165bb81d612	an efficient test allocation for software reliability estimation	bayes estimation;sequential design;sampling scheme;software reliability	We propose an efficient sampling scheme in the software reliability estimation. In contrast to fixed sampling schemes, where the proportion of test cases taken from each partition is determined before reliability testing begins, we make allocation decisions dynamically throughout the testing process. We then refine these estimates in an iterative manner as we sample. We also compare the result from the accelerated sampling scheme with the best fixed sampling scheme and demonstrate its superiority over the best fixed sampling scheme in terms of the expected loss incurred when the overall reliability is estimated by its Bayes estimator both theoretically and through Monte Carlo simulations. 2013 Elsevier Inc. All rights reserved.	iterative method;monte carlo method;reliability engineering;sampling (signal processing);simulation;software quality;software reliability testing;test case	Kamel Rekab;Herbert Thompson;Wei Wu	2013	Applied Mathematics and Computation	10.1016/j.amc.2013.06.013	econometrics;mathematical optimization;computer science;sequential analysis;mathematics;software quality;statistics	Metrics	28.827552916183095	-18.341513552094227	99972
f0cd9044ed4b904437fe54b5b317f44ca021b39e	a process model to develop an internal rating system: sovereign credit ratings	modelizacion;analyse risque;capital;paises en desarrollo;country risk;banking;analisis estadistico;pays en developpement;commande modele interne;principio modelo interno;support vector machines;bolsa valores;methode noyau;risk analysis;processes;hd28 management industrial management;basel ii;aprendizaje probabilidades;secteur bancaire;credit;logistic regression;internal rating system;bourse valeurs;stock exchange;modelisation;analisis riesgo;regresion logistica;institucion financiera;analisis regresion;statistical analysis;control modelo interno;credito;internal model control;financial institutions;regression logistique;metodo nucleo;machine exemple support;analyse statistique;portfolio management;principe modele interne;model;institution financiere;analyse regression;apprentissage probabilites;developing country;kernel method;internal model principle;regression analysis;financial institution;gestion cartera;modele donnee;process model;credit rating;support vector machine;maquina ejemplo soporte;vector support machine;gestion portefeuille;modeling;internal ratings based;logistic regression model;probability learning;developing countries;data models;sovereign ratings	The Basel II capital accord encourages financial institutions to develop rating systems for assessing the risk of default of their credit portfolios in order to better calculate the minimum regulatory capital needed to cover unexpected losses. In the internal ratings based approach, financial institutions are allowed to build their own models based on collected data. In this paper, a generic process model to develop an advanced internal rating system is presented in the context of country risk analysis of developed and developing countries. In the modelling step, a new, gradual approach is suggested to augment the well-known ordinal logistic regression model with a kernel based learning capability, hereby yielding models which are at the same time both accurate and readable. The estimated models are extensively evaluated and validated taking into account several criteria. Furthermore, it is shown how these models can be transformed into user-friendly and easy to understand scorecards. D 2005 Elsevier B.V. All rights reserved.	human-readable medium;it risk management;logistic regression;ordered logit;ordinal data;process modeling;sports rating system;usability;whole earth 'lectronic link	Tony Van Gestel;Bart Baesens;Peter Van Dijcke;Joao Garcia;Johan A. K. Suykens;Jan Vanthienen	2006	Decision Support Systems	10.1016/j.dss.2005.10.001	support vector machine;developing country;computer science;operations management;machine learning;logistic regression;economy;operations research;project portfolio management	AI	25.429341869304096	-19.508311414575044	100147
be7e0a99c993cf0d01becb23a1ce22657bcf341a	bayesian estimation of defects based on defect decay model: bayesed3m	bayesian approach;maximum likelihood estimate	The estimation of the total number of defects at early stages of the testing process helps managers to make resource allocation and deadline decisions. The use of nonbayesian approaches has proven to be accurate but presents a certain latency to achieve a reasonable accuracy. Here we describeBayesED3M , a bayesian estimator construct upon an existing MLE (Maximum Likelihood Estimator) named asED3M . The case studies presented in this paper show thatBayesED3M outperformsED3M whenever reasonable historical data is available.	capability maturity model integration;converge;in the beginning... was the command line;noise (electronics);software bug	Syed Waseem Haider;João W. Cangussu	2006				Metrics	28.909814340192128	-18.42788027782686	100273
ac93f93573fec0946ad7f02193e486f1cf362435	model criticism of bayesian networks with latent variables	bayesian network;networks;bayesian statistics;latent variable;mathematical models;artificial intelligence;cognitive tests	AUTHOR Williamson, David M.; Mislevy, Robert J.; Almond, Russell G. TITLE Model Criticism of Bayesian Networks with Latent Variables. PUB DATE 2001-04-00 NOTE 26p.; Paper presented at the Annual Meeting of the National Council on Measurement in Education (Seattle, WA, April 11-13, 2001). PUB TYPE Reports Descriptive (141) Speeches/Meeting Papers (150) EDRS PRICE MF01/PCO2 Plus Postage. DESCRIPTORS *Artificial Intelligence; *Bayesian Statistics; *Cognitive Tests; *Mathematical Models; *Networks IDENTIFIERS *Latent Variables	bayesian network;edge enhancement;latent variable;observable;sensor	David M. Williamson;Russell G. Almond;Robert J. Mislevy	2000			latent class model;latent variable;econometrics;computer science;artificial intelligence;machine learning;cognitive test;bayesian network;mathematical model;bayesian statistics;probabilistic latent semantic analysis;statistics	AI	26.451460289520476	-23.45009934126806	100710
22ab8391ac7fa0574c14e15e1779cfd747091493	selecting and estimating regular vine copulae and application to financial returns	model selection;multivariate copula;minimum spanning tree;regular vines	Regular vine distributions which constitute a flexible class of multivariate dependence models are discussed. Since multivariate copulae constructed through pair-copula decompositions were introduced to the statistical community, interest in these models has been growing steadily and they are finding successful applications in various fields. Research so far has however been concentrating on so-called canonical and D-vine copulae, which are more restrictive cases of regular vine copulae. It is shown how to evaluate the density of arbitrary regular vine specifications. This opens the vine copula methodology to the flexible modeling of complex dependencies even in larger dimensions. In this regard, a new automated model selection and estimation technique based on graph theoretical considerations is presented. This comprehensive search strategy is evaluated in a large simulation study and applied to a 16-dimensional financial data set of international equity, fixed income and commodity indices which were observed over the last decade, in particular during the recent financial crisis. The analysis provides economically well interpretable results and interesting insights into the dependence structure among these indices.	graph theory;model selection;simulation;vine toolkit	J. Dißmann;Eike C. Brechmann;Claudia Czado;Dorota Kurowicka	2013	Computational Statistics & Data Analysis	10.1016/j.csda.2012.08.010	econometrics;minimum spanning tree;mathematics;model selection;statistics	Metrics	26.174263853555296	-21.279640062462132	100932
6b13f8b1042d052abc8059db89e5bd4e3ffa5f5a	bootstrap method for central and intermediate order statistics under power normalization			bootstrapping (statistics);resampling (statistics)	H. M. Barakat;E. M. Nigm;O. M. Khaled	2015	Kybernetika	10.14736/kyb-2015-6-0923	normalization (statistics);order statistic;econometrics;mathematics;bootstrapping (electronics);statistics;weak consistency	DB	31.02473970613421	-22.18348724054427	101117
27c92ae626c1f46e7186fd79327a7c60f9bd0357	the exponentiated exponential mixture and non-mixture cure rate model in the presence of covariates	exponentiated weibull distribution;non mixture lifetime model;mixture lifetime model;long term data	This paper presents estimates for the parameters included in long-term mixture and non-mixture lifetime models, applied to analyze survival data when some individuals may never experience the event of interest. We consider the case where the lifetime data have a two-parameters exponentiated exponential distribution. The two-parameter exponentiated exponential or the generalized exponential distribution is a particular member of the exponentiated Weibull distribution introduced by [31]. Classical and Bayesian procedures are used to get point and confidence intervals of the unknown parameters. We consider a general survival model where the scale, shape and cured fraction parameters of the exponentiated exponential distribution depends on covariates.	confidence intervals;estimated;population parameter;exponential	Josmar Mazucheli;Emílio Augusto Coelho Barros;Jorge Alberto Achcar	2013	Computer methods and programs in biomedicine	10.1016/j.cmpb.2013.06.015	econometrics;exponentiated weibull distribution;mathematics;statistics	ML	30.863049689047024	-21.070953594771726	101137
fe74cafdd70aa106fd773685ee714c5c93842547	statistical analysis of load demand distribution at banaras hindu university, india	statistical analysis;probability distribution;load demand;goodness of fit test	There are various segments of energy sources like coal, oil, gas and renewable energy to generate electricity. Owing to the reason of being carbon free energy sources, utilization of renewable energy resources are increasing exponentially. It is clear from the recent past, role of renewable distributed generation technologies are the most demanded one. Optimal utilization of renewable energy sources and continuous electricity supply at any location needs statistical analysis to predict the randomly distributed electricity load demand of same location. For the statistical analysis, we have collected hourly load demand data of Banaras Hindu University (BHU), India region for the year 2014 from the Electricity & Water Supply Service (EWSS) centre, BHU. In this research work, we have considered Lognormal, Weibull and Rayleigh probability distributions to estimate the randomly distributed load demand data of BHU region. We had taken help of STATISTICA software for the statistical analysis and comparative study between these three probability distributions. With the help of Kolmogorov-Smirnov, Anderson-Darling and Chi-Square Goodness-of-fit tests, we identified that Lognormal distribution is the best fitted one.	chi;randomness;rayleigh–ritz method;statistica	Manish Kumar;Cherian Samuel	2016	2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2016.7732400	probability distribution;econometrics;engineering;mathematics;goodness of fit;statistics	Metrics	29.70746220218745	-18.29812333034133	101227
1b70c235f0115c9c7557f089adb6e29e845dc231	the specification of the propensity score in multilevel observational studies	causal inferences;estimacion sesgada;c21 cross sectional models;propensity score matching;hierarchical structure;treatment effect models;fixed effect;causal inference multilevel studies propensity score unconfoundedness;analisis datos;causal effect;variable aleatoire;hierarchized structure;03d55;propensity score;error sistematico;variable aleatoria;hierarchical data;unconfoundedness;structure hierarchisee;causal inference;data analysis;spatial models;social science;efecto aleatorio;observational study;effet fixe;bias;random effect;estructura datos;statistical computation;calculo estadistico;random variable;c01 econometrics;covariate;modele simulation;analyse donnee;quantile regressions;calcul statistique;covariable;structure donnee;modelo simulacion;estimation statistique;monte carlo;effet aleatoire;multilevel model;estimacion estadistica;multilevel studies;data structure;statistical estimation;biased estimation;simulation model;estimation biaisee;estructura jerarquizada;erreur systematique	Propensity Score Matching (PSM) has become a popular approach to estimation of causal effects. It relies on the assumption that selection into a treatment can be explained purely in terms of observable characteristics (the “unconfoundedness assumption”) and on the property that balancing on the propensity score is equivalent to balancing on the observed covariates. Several applications in social sciences are characterized by a hierarchical structure of data: units at the first level (e.g., individuals) clustered into groups (e.g., provinces). In this paper we explore the use of multilevel models for the estimation of the propensity score for such hierarchical data when one or more relevant cluster-level variables is unobserved. We compare this approach with alternative ones, like a single level model with cluster dummies. By using Monte Carlo evidence we show that multilevel specifications usually achieve reasonably good balancing in cluster level unobserved covariates and consequently reduce the omitted variable bias. This is also the case for the dummy model.	algorithm;causal filter;causal inference;data structure;monte carlo method;multi-level governance;multilevel model;observable;propensity score matching	Bruno Arpino;Fabrizia Mealli	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2010.11.008	econometrics;data structure;causal inference;multilevel model;machine learning;propensity score matching;mathematics;statistics;random effects model	ML	33.49108402418713	-22.998495506109123	101228
e5e7bd8b6209b27e92ed76d207ec92ce303f0203	a neuro-genetic algorithm for heteroskedastic time-series processes empirical tests on global asset returns	time series;genetics;genetic algorithm;hybrid algorithm;evolutionary computing	The paper proposes a new neuro-genetic hybrid algorithm (NGHA) for coping with ill-conditioned timeseries processes. Extensive testing and comparisons to various heteroskedastic models indicate that the neurogenetic algorithm may be a useful device for modelling complicated time series. NGHA is used to model a factor price series corresponding to the European factor of a representative set of global asset returns. NGHA provides a platform for adapting evolutionary computation to the search for suitable networks for observed time series.	condition number;evolutionary computation;genetic algorithm;hybrid algorithm;time series	Ralf Östermark	1999	Soft Comput.	10.1007/s005000050071	econometrics;mathematical optimization;genetic algorithm;hybrid algorithm;computer science;artificial intelligence;machine learning;time series;statistics;evolutionary computation	ML	30.119985303649553	-11.038979040875407	101252
b2ab3844131b32f1aa3e9a7d3c1b25b5e89714df	a note on bimodality in the log-likelihood function for penalized spline mixed models	bayes estimation;fonction vraisemblance;spline;log likelihood;chaine markov;cadena markov;covariance analysis;metodo monte carlo;41a15;approximation numerique;analisis datos;melange loi probabilite;maximum likelihood;fonction repartition;data smoothing;log vraisemblance;melangeage;esplin;variance component;profile likelihood;maximum vraisemblance;variance analysis;modele mixte;methode monte carlo;prior distribution;metodo penalidad;mixed distribution;ley a priori;componente variancia;smoothing parameter;statistical regression;aproximacion numerica;funcion verosimilitud;funcion distribucion;data analysis;estimacion bayes;aproximacion esplin;distribution function;penalty method;smoothing methods;methode penalite;composante variance;analyse covariance;mixed model;65d07;spline approximation;approximation spline;analisis variancia;regresion estadistica;methode lissage;monte carlo method;37a25;62j10;statistical computation;calculo estadistico;vraisemblance profil;canonical transformation;lissage donnees;mezcla ley probabilidad;analyse donnee;calcul statistique;numerical approximation;modele melange;analisis covariancia;mixing model;modelo mixto;mixing;regression statistique;60e05;alisadura datos;mezclado;likelihood function;parametre lissage;smoothing spline;penalized splines;maxima verosimilitud;loi a priori;analyse variance;estimation bayes;variance;variancia;markov chain	For a smoothing spline or general penalized spline model, the smoothing parameter can be estimated using residual maximum likelihood (REML) methods by expressing the spline in the form of a mixed model. The possibility of bimodality in the profile log-likelihood function for the smoothing parameter of these penalized spline mixed models is demonstrated. A canonical transformation into independent observations is used to provide efficient evaluation of the log-likelihood function and gives insight into the incompatibilities between the model and data that cause bimodality. This transformation can also be used to assess the influence of different frequency components in the data on the estimated smoothing parameter. It is demonstrated that, where bimodality occurs in the log-likelihood, Bayesian penalized spline models may show poor mixing in MCMC chains and be sensitive to the choice of prior distributions for variance components. © 2008 Elsevier B.V. All rights reserved.	algorithm;bayesian network;computation;cooperative breeding;markov chain monte carlo;mathematical optimization;mixed model;random effects model;smoothing spline;software developer;spline (mathematics)	Sue J. Welham;Robin Thompson	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.10.031	spline;mixed model;canonical transformation;econometrics;markov chain;prior probability;analysis of variance;smoothing spline;analysis of covariance;likelihood-ratio test;distribution function;calculus;penalty method;mathematics;maximum likelihood;likelihood function;mixing;variance;data analysis;regression analysis;statistics;smoothing;monte carlo method	AI	32.94903674388885	-23.075734766009266	101633
69d07417a8c08bac9adf21f4c5c0ff8d409e356e	palmhash code for palmprint verification and protection	databases;security of data data privacy feature extraction image coding image fusion image texture palmprint recognition;image coding;encryption;palmhash code;multi orientation score level fusion;image fusion;feature extraction gabor filters encoding databases encryption;palmprint protection;gabor filters;image texture;data privacy;feature extraction;palmprint recognition;perpendicular orientation transposition;cancelable biometrics palmprint verification palmprint protection biometric template privacy biometric template security biometric technology texture coding method cancelable palmprint coding scheme texture feature palmhash code generation mechanism perpendicular orientation transposition multiorientation score level fusion;palmprint protection palmhash code perpendicular orientation transposition multi orientation score level fusion palmprint verification;encoding;security of data;palmprint verification	The security and privacy of biometric template has become a hot issue with the development of biometric technology. Palmprint verification has a high accuracy thank to texture coding methods, so it is urgent to develop nice cancelable palmprint coding scheme based on texture feature. PalmHash Code is proposed as a secure and effective cancelable palmprint coding framework in this paper. Three generation mechanisms of PalmHash Code are introduced and compared. Beside, two measures, perpendicular orientation transposition and multi-orientation score level fusion, are used to improve the performance of PalmHash Codes. The experimental results and analysis confirm that PalmHash Code enhanced by the two measures meet the four objectives of cancelable biometric satisfactorily.	biometrics;fingerprint;qr code	Lu Leng;Jiashu Zhang	2012	2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2012.6334853	image texture;computer vision;speech recognition;information privacy;feature extraction;computer science;pattern recognition;image fusion;encryption;encoding	SE	36.50679227824345	-11.010512514359506	102063
0c33685d68c8185c54421e09bcc417b74f8cec4d	controlled sequential bifurcation for software reliability study	software reliability study;stochastic simulation response;reliability analysis;software reliability analysis;simulation experiment;new screening method;csb procedure;controlled sequential bifurcation;type i error;screening experiment;screening result;computer simulation;error control;simulation;normal distribution;bifurcation;stochastic simulation;software systems;software reliability;hypothesis test	Computer simulation is an appealing approach for the reliability analysis of structure-based software systems as it can accommodate important complexities present in realistic systems. When the system is complicated, a screening experiment to quickly identify important factors (components) can significantly improve the efficiency of analysis. The challenge is to guarantee the correctness of the screening results with stochastic simulation responses. Control Sequential Bifurcation (CSB) is a new screening methods for simulation experiments. With appropriate hypothesis testing procedures, CSB can simultaneously control the Type I error and power. The past research, however, has focused on responses with normal distributions. This paper proposes a CSB procedure with binary responses for software reliability analysis. A conditional sequential test for equality of two proportions is implemented to guarantee the the desired error control.	bifurcation theory;boyce–codd normal form;collection of computer science bibliographies;computer simulation;correctness (computer science);error detection and correction;experiment;software quality;software reliability testing;software system	Jun Xu;Feng Yang;Hong Wan	2007	2007 Winter Simulation Conference		normal distribution;computer simulation;reliability engineering;statistical hypothesis testing;type i and type ii errors;computer science;theoretical computer science;stochastic simulation;mathematics;software quality;statistics;software system	SE	30.985809488356878	-15.415831696277104	102108
03a35234d091e2a734c3876d2fe05ca076a1dd58	approximate estimation of system reliability via fault trees	approximation asymptotique;arbol defecto;fault tree;system reliability;diagrama binaria decision;fiabilite systeme;diagramme binaire decision;repairable system;fiabilidad sistema;analyse arbre defaut;binary decision diagrams;arbre defaut;rare event;asymptotic approximation;sistema reparable;systeme reparable;lower bound;fault tree analysis;aproximacion asintotica;binary decision diagram	In this article, we show how fault tree analysis, carried out by means of binary decision diagrams (BDD), is able to approximate reliability of systems made of independent repairable components with a good accuracy and a good efficiency. We consider four algorithms: the Murchland lower bound, the Barlow-Proschan lower bound, the Vesely full approximation and the Vesely asymptotic approximation. For each of these algorithms, we consider an implementation based on the classical minimal cut sets/rare events approach and another one relying on the BDD technology. We present numerical results obtained with both approaches on various examples.	fault tree analysis	Yves Dutuit;Antoine Rauzy	2005	Rel. Eng. & Sys. Safety	10.1016/j.ress.2004.02.008	reliability engineering;fault tree analysis;engineering;mathematics;algorithm;statistics	EDA	27.764560933595877	-15.450049859633369	102126
078307198f52127d18010c24be1978e32c39db80	control charts for monitoring the mean and percentiles of weibull processes with variance components			chart;random effects model	Francis Pascual;Changsoon Park	2018	Quality and Reliability Eng. Int.	10.1002/qre.2252		SE	28.776236928479154	-20.539716119822437	102202
b90af25baa689e36a59433159bbda8187a5c4c12	synthetic-np chart for attributes	synthetic chart;syn np chart;fraction nonconforming synthetic np chart large shift detection;average number of defective syn np chart synthetic chart np chart;control charts process control monitoring steady state optimization mathematical model equations;average number of defective;charts;np chart	While the np chart is powerful for detecting large shifts in fraction nonconforming p, the synthetic chart is more effective for detecting small p shifts. In this article, the salient features of the np chart and synthetic chart are hybridized to produce the Syn-np chart. The results of the performance studies indicate that the new Syn-np chart significantly outperforms the np chart and synthetic chart by 73% and 31%, respectively, in terms of Average Number of Defectives (AND) across a wide range of p shifts under different circumstances.	sensor;synthetic data;synthetic intelligence	Salah Haridy;Zhang Wu	2011	2011 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2011.6118161	reliability engineering;economics;np-chart;operations management;chart;mathematics;algorithm;statistics	DB	27.480768141920027	-18.762403569395527	102248
d89a3a23036d37c104d3c6d02e97486d637b96d8	general saddlepoint approximations: application to the anderson-darling test statistic	ordre approximation;goodness of fit;cumulative distribution function;metodo estadistico;secondary 62g30;approximation normale;limit distribution;theorie approximation;aplicacion;developpement mathematique;saddlepoint approximation;fonction repartition;saddlepoint approximation primary 62e17;estadistica test;loi limite;statistique test;62e17;simulacion numerica;orden aproximacion;anderson darling test;statistical method;curva gauss;distribucion estadistica;loi asymptotique;mathematical expansion;60f17;higher order;approximation theory;funcion distribucion;distribution function;distribution statistique;62g10;methode statistique;approximation order;simulation numerique;primary 62e17 62g10;loi normale;application;60e05;statistical distribution;normal approximation;gaussian distribution;test statistic;ley limite;62e20;numerical simulation;expansion	We consider the relative merits of various saddlepoint approximations for the c.d.f. of a statistic with a possibly non-normal limit distribution. In addition to the usual Lugannani-Rice approximation we also consider approximations based on higher-order expansions, including the case where the base distribution for the approximation is taken to be non-normal. This extends earlier work by Wood et al. (1993). These approximations are applied to the distribution of the Anderson-Darling test statistic. While these generalizations perform well in the middle of the distribution’s support, a conventional normal-based Lugannani-Rice approximation (Giles, 2001) is superior for conventional critical regions.	approximation;rice's theorem	Qian Chen;David E. Giles	2008	Communications in Statistics - Simulation and Computation	10.1080/03610910701730141	normal distribution;computer simulation;probability distribution;econometrics;test statistic;higher-order logic;cumulative distribution function;distribution function;calculus;mathematics;goodness of fit;asymptotic distribution;chi-squared distribution;anderson–darling test;statistics;approximation theory	ML	33.44670196280202	-21.141573089543012	102836
1ec765c5ba7decbd0b9a17a7761ca00ffc11886e	arc refractor methods for adaptive importance sampling on large bayesian networks under evidential reasoning	bayes estimation;modelizacion;multicriteria analysis;metodo adaptativo;raisonnement probabiliste;bayesian network;error muestreo;computacion informatica;prior probability;distribution network;reseau distribution;loi probabilite;ley probabilidad;algoritmo adaptativo;probabilidad condicional;uncertainty;sampling error;probabilite conditionnelle;probabilite a priori;prior distribution;prise de decision;programmation stochastique;methode adaptative;intelligence artificielle;aprendizaje probabilidades;posterior probability;probabilistic approach;ley a priori;bayesian method;approximate bayesian inference;red distribucion;evidential reasoning;modelisation;reseau bayes;adaptive algorithm;systeme incertain;estimacion bayes;posterior distribution;algorithme adaptatif;razonamiento por prueba;razonamiento probabilidad;red bayes;ciencias basicas y experimentales;enfoque probabilista;probabilidad a priori;approche probabiliste;probability distribution;adaptive method;probabilite a posteriori;echantillonnage importance;conditional probability table;probabilidad a posteriori;bayes network;ley a posteriori;muestreo optimo;apprentissage probabilites;artificial intelligence;refinacion;refining;analisis multicriterio;inteligencia artificial;analyse multicritere;optimal sampling;probabilistic logic;importance sampling;grupo a;stochastic programming;toma decision;sistema incierto;logique probabiliste;modeling;conditional probability;loi a posteriori;uncertain system;programacion estocastica;echantillonnage optimal;probability learning;probabilistic reasoning;raisonnement par preuve;erreur echantillonnage;loi a priori;raffinage;estimation bayes;variance;variancia;bayesian networks	Approximate Bayesian inference by importance sampling derives probabilistic statements from a Bayesian network, an essential part of evidential reasoning with the network and an important aspect of many Bayesian methods. A critical problem in importance sampling on Bayesian networks is the selection of a good importance function to sample a network's prior and posterior probability distribution. The initially optimal importance functions eventually start deviating from the optimal function when sampling a network's posterior distribution given evidence, even when adaptive methods are used that adjust an importance function to the evidence by learning. In this article we propose a new family of Refractor Importance Sampling (RIS) algorithms for adaptive importance sampling under evidential reasoning. RIS applies ''arc refractors'' to a Bayesian network by adding new arcs and refining the conditional probability tables. The goal of RIS is to optimize the importance function for the posterior distribution and reduce the error variance of sampling. Our experimental results show a significant improvement of RIS over state-of-the-art adaptive importance sampling algorithms.	bayesian network;importance sampling;sampling (signal processing)	Haohai Yu;Robert A. van Engelen	2010	Int. J. Approx. Reasoning	10.1016/j.ijar.2010.04.006	econometrics;importance sampling;machine learning;bayesian network;mathematics;bayesian linear regression;statistics	AI	25.690507850011453	-19.631135881886603	102996
14fe75e4229b78d678d84830de12f4ba26ce7170	a simple iterative model accurately captures complex trapline formation by bumblebees across spatial scales and flower arrangements	animals;flight animal;bees;models biological;journal article;appetitive behavior;databases factual;computational biology;flowers	Pollinating bees develop foraging circuits (traplines) to visit multiple flowers in a manner that minimizes overall travel distance, a task analogous to the travelling salesman problem. We report on an in-depth exploration of an iterative improvement heuristic model of bumblebee traplining previously found to accurately replicate the establishment of stable routes by bees between flowers distributed over several hectares. The critical test for a model is its predictive power for empirical data for which the model has not been specifically developed, and here the model is shown to be consistent with observations from different research groups made at several spatial scales and using multiple configurations of flowers. We refine the model to account for the spatial search strategy of bees exploring their environment, and test several previously unexplored predictions. We find that the model predicts accurately 1) the increasing propensity of bees to optimize their foraging routes with increasing spatial scale; 2) that bees cannot establish stable optimal traplines for all spatial configurations of rewarding flowers; 3) the observed trade-off between travel distance and prioritization of high-reward sites (with a slight modification of the model); 4) the temporal pattern with which bees acquire approximate solutions to travelling salesman-like problems over several dozen foraging bouts; 5) the instability of visitation schedules in some spatial configurations of flowers; 6) the observation that in some flower arrays, bees' visitation schedules are highly individually different; 7) the searching behaviour that leads to efficient location of flowers and routes between them. Our model constitutes a robust theoretical platform to generate novel hypotheses and refine our understanding about how small-brained insects develop a representation of space and use it to navigate in complex and dynamic environments.	approximation algorithm;bees;bombus terrestris;flowers;heuristic;instability;iterative and incremental development;iterative method;rewards;schedule (document type);self-replicating machine;spatial scale;travelling salesman problem	Andrew M. Reynolds;Mathieu Lihoreau;Lars Chittka	2013		10.1371/journal.pcbi.1002938	computational biology;biology;simulation;artificial intelligence;bees algorithm;ecology	ML	25.368650133224936	-10.679124794536436	103091
68c266af34a3db57beceac2a7284e391d20679ac	stereo matching based stereo image watermarking for tamper detection and recovery	tamper detection;three dimensional television;stereo matching;stereo image watermarking	AbstractDigital watermarking is an effective way for authentication of stereo images when they are transmitted over networks. Since the left and right images of a stereo image pair are not independent but similar to each other, their watermarking design should be different from that of mono-images. This paper thus proposes a stereo matching based stereo image watermarking scheme for tamper detection and recovery. One-to-one mappings between left and right image blocks are established, and tamper detection bits generated from each block and the recovery bits of the mapped block are embedded back to the block itself. To keep the consistency between left and right images, a stereo image pair is usually tampered symmetrically. That is, the tampered areas in left and right images are similar to each other, thus the combination of tamper detection results of left and right images can facilitate more accurate tamper detection results. In general, a tampered block in one image of stereo image pair can be recovere...	computer stereo vision;digital watermarking	Ting Luo;Gangyi Jiang;Mei Yu;Feng Shao;Zongju Peng;Yo-Sung Ho	2014	Int. J. Comput. Intell. Syst.	10.1080/18756891.2014.889843	computer stereo vision;computer vision;theoretical computer science;mathematics;computer graphics (images)	Vision	39.03669631343801	-11.252263830797625	103354
fbd90701b3729da95d0448822d7a1c6a2cc8bba4	classification of eeg signals using a novel genetic programming approach	globally prime;emperical mode decomposition;genetic programming	In this paper, we present a new method for classification of electroencephalogram (EEG) signals using Genetic Programming (GP). The Empirical Mode Decomposition (EMD) is used to extract the features of EEG signals which served as an input for the GP. In this paper, new constructive crossover and mutation operations are also produced to improve GP. In these constructive crossover and mutation operators hill climbing search is integrated to remove the destructive nature of these operators. To improve GP, we apply constructive crossover on all the individuals which remain after reproduction. A new concept of selecting the global prime off-springs of the generation is also proposed. The constructive mutation approach is applied to poor individuals who are left after selecting globally prime off-springs. Improvement of the method is measured against classification accuracy, training time and the number of generations for EEG signal classification. As we show in the results section, the classification accuracy can be estimated to be 98.69% on the test cases, which is better than classification accuracy of Liang and coworkers method which was published in 2010.	electroencephalography;genetic programming;hilbert–huang transform;hill climbing;statistical classification;test case	Arpit Bhardwaj;Aruna Tiwari;M. Vishaal Varma;M. Ramesh Krishna	2014		10.1145/2598394.2609851	artificial intelligence;machine learning;mathematics;algorithm	ML	27.099063882803613	-11.23822420675614	103514
824fb14f1ab57eca733c8c461b7557b2fc0e3975	bonferroni and gini indices and recurrence relations for moments of progressive type-ii right censored order statistics from marshall-olkin exponential distribution	order statistics;progressive type ii censored sampling;gini index;marshall olkin exponential distri bution;bonferroni index;bonferroni curve;lorenz curve	In this paper, we derive explicit expressions for Bonferroni Curve (BC), Bonferroni index (BI), Lorenz Curve (LC) and Gini index (GI) for the Marshall-Olikn Exponential (MOE) distribution, which have mainly concern with some aspects like poverty, welfare, decomposability, reliability, sampling and inference. We also establish several recurrence relations satisfied by the single and the product moments of progressive Type-II right censored order statistics from MOE distribution, to enable one to evaluate the single and product moments of all order in a simple recursive way.	censoring (statistics);lorenz cipher;moe;marshalling (computer science);recurrence relation;recursion;sampling (signal processing)	Narinder Pushkarna;Jagdish Saran;Rashmi Tiwari	2013	JSTA	10.2991/jsta.2013.12.3.7	econometrics;mathematics;mathematical economics;statistics	ML	32.902684433850744	-19.582496609040074	103783
5e12b2766dd63c021b2800e1efcf9c68df708d8f	mixture periodic autoregressive conditional heteroskedastic models	iterative method;classification automatique statistiques;statistical moment;ajustamiento modelo;analyse multivariable;modelo arch;statistical simulation;arch model;algorithm performance;theorie approximation;coefficient beta 2 pearson;multivariate analysis;analisis datos;melange loi probabilite;maximum likelihood;modelo autorregresivo;maximization;attente;moment statistique;62e17;maximum vraisemblance;coefficient of kurtosis;processus autoregressif;autocovariance;mixed distribution;loi conditionnelle;outlier;ley condicional;distribucion estadistica;segundo momento;autoregressive model;metodo iterativo;algorithme;ajustement modele;discriminant analysis;analyse discriminante;approximation theory;observacion aberrante;algorithm;momento estadistico;data analysis;analisis discriminante;simulacion estadistica;maximum likelihood estimate;distribution statistique;autoregressive processes;62h30;resultado algoritmo;methode iterative;simulation statistique;autocovarianza;model matching;statistical computation;calculo estadistico;analyse correlation;expectation maximization algorithm;second moment;modele simulation;performance algorithme;expectation;observation aberrante;mezcla ley probabilidad;simulation study;analisis multivariable;analyse donnee;calcul statistique;modelo simulacion;modele autoregressif;modele arch;extreme event;simulation model;maximizacion;autoregressive conditional heteroskedasticity;statistical distribution;conditional distribution;maxima verosimilitud;expectacion;analisis correlacion;maximisation;correlation analysis;algoritmo	Mixture Periodically Correlated Autoregressive Conditionally Heteroskedastic (MPARCH) model, which extends the ARCH model, is proposed. The primary motivation behind this extension is to make the model consistent with high kurtosis, outliers and extreme events, and at the same time, able to capture the periodicity feature exhibited by the autocovariance structure. The second and the fourth moment periodically stationary conditions and their closed-forms are derived. Maximum likelihood estimation is obtained via the iterative Expectation Maximization algorithm and the performance of this algorithm is shown via a simulation studies and the MPARCH models are fitted to a real data set.	autoregressive model	M. Bentarzi;F. Hamdi	2008	Computational Statistics & Data Analysis	10.1016/j.csda.2008.06.019	autoregressive conditional heteroskedasticity;econometrics;calculus;mathematics;maximum likelihood;linear discriminant analysis;statistics	ML	33.4220283057121	-22.768144511809993	103887
7a0fc4721eb328d0b56bf926b202284eab65f53c	perovskite classification: an excel spreadsheet to determine and depict end-member proportions for the perovskite- and vapnikite-subgroups of the perovskite supergroup		Abstract Perovskite mineral oxides commonly exhibit extensive solid-solution, and are therefore classified on the basis of the proportions of their ideal end-members. A uniform sequence of calculation of the end-members is required if comparisons are to be made between different sets of analytical data. A Microsoft Excel spreadsheet has been programmed to assist with the classification and depiction of the minerals of the perovskite- and vapnikite-subgroups following the 2017 nomenclature of the perovskite supergroup recommended by the International Mineralogical Association (IMA). Compositional data for up to 36 elements are input into the spreadsheet as oxides in weight percent. For each analysis, the output includes the formula, the normalized proportions of 15 end-members, and the percentage of cations which cannot be assigned to those end-members. The data are automatically plotted onto the ternary and quaternary diagrams recommended by the IMA for depiction of perovskite compositions. Up to 200 analyses can be entered into the spreadsheet, which is accompanied by data calculated for 140 perovskite compositions compiled from the literature.	city of heroes;spreadsheet	Andrew J. Locock;Roger H. Mitchell	2018	Computers & Geosciences	10.1016/j.cageo.2018.01.012	compositional data;statistics;ternary operation;computer science;supergroup;perovskite;mineralogy	ML	37.10925689439721	-19.326342016899773	104122
b70152834d426353e7d96208f7679b82b3164136	asynergistic regression based on maximized rank correlation	modelo lineal generalizado;experimental design;62j12;association statistique;metodo estadistico;analyse multivariable;medicament;correlacion rango;aplicacion;multivariate analysis;62k20;modele lineaire generalise;62h20;correlation rang;simulacion numerica;metodo semiparametrico;modele lineaire;plan experiencia;62g08;statistical association;monotonic index models;statistical method;methode semiparametrique;modelo lineal;statistical regression;62jxx;algorithme;surface reponse;algorithm;asociacion estadistica;side effect;plan experience;drug interactions;drug therapy;methode statistique;regresion estadistica;indexation;simulation numerique;linear model;semiparametric method;analyse correlation;superficie respuesta;semiparametric model;analisis multivariable;medicamento;generalized linear model;plan surface reponse;dose response;drug interaction;synergy;drug;regression statistique;application;response surface;rank correlation;synergy 62g08;modele semi parametrique;analisis correlacion;numerical simulation;correlation analysis;algoritmo	The property of synergy and its detection are discussed. A response surface is said to possess synergy if it is monotone in each argument and its level curves are convex. Detecting this property is particularly useful in the study of combination drug therapies where the goal is enhanced response or diminished side effect. One way to detect synergy is to fit a surface with linear level curves under the assumption of asynergy and observe the residuals. We explore an algorithm to accomplish this asynergistic regression via a reduction in dimensionality and connections to semiparametric monotonic linear index models. We see that the asynergistic model is a confounded version of the monotonic linear index model where the	algorithm;convex function;non-monotonic logic;response surface methodology;semiparametric model;synergy;monotone	Michael C Donohue;Ian Abramson;Anthony Gamst	2008	Communications in Statistics - Simulation and Computation	10.1080/03610910701723716	computer simulation;synergy;association;econometrics;response surface methodology;dose–response relationship;calculus;linear model;generalized linear model;mathematics;multivariate analysis;drug interaction;design of experiments;rank correlation;side effect;regression analysis;semiparametric model;statistics	AI	33.45948002671877	-23.156778540486485	104177
096f4abbd2f23ed44433807539c110136c7e52e7	an adaptive cusum chart with single sample size for monitoring process mean and variance	statistical process control;variable sample size and sampling intervals vssi;journal article;average extra quadratic loss aeql;quality control	This article proposes an adaptive absolute cumulative sum chart (called the adaptive ACUSUM chart) for statistical process control. The new development includes the variable sampling interval (VSI), variable sample size (VSS) and VSS and interval (VSSI) versions, all of which are highly effective for monitoring the mean and variance of a variable x by inspecting the absolute sample shift (where μ0 is the in-control mean or target value of x). While the adaptive ACUSUM chart is a straightforward extension of the ABS CUSUM chart developed by Wu, et al., it is much more effective than all other adaptive CUSUM charts. Noteworthily, the superiority of VSI ACUSUM chart over the best adaptive CUSUM chart in literature is about 35% from an overall viewpoint. Moreover, the design and implementation of the adaptive ACUSUM chart are much simpler than that of all other adaptive CUSUM schemes. All these desirable features of the adaptive ACUSUM chart may be attributable to the use of a single sample size (n = 1). Another quite interesting finding is that the simpler VSI ACUSUM chart works equally well as the more complicated VSSI ACUSUM chart. Copyright © 2012 John Wiley & Sons, Ltd.		Yanjing Ou;Zhang Wu;Ka Man Lee;Kan Wu	2013	Quality and Reliability Eng. Int.	10.1002/qre.1454	econometrics;quality control;computer science;engineering;operations management;mathematics;shewhart individuals control chart;x-bar chart;statistical process control;statistics	HCI	28.09349425262683	-19.395989267616926	104216
4bb54c29342075beb0d905a6eeb0f94bbc5a6329	halftone visual cryptography via direct binary search	direct binary search;hvs model halftone visual cryptography direct binary search halftoning method dbs halftoning method secret binary image halftone images secret pixels perceptual errors human visual system model;binary image;visual perception cryptography image processing;visual cryptography;human visual system;image quality;visualization cryptography satellite broadcasting image reconstruction silicon simulation	This paper considers the problem of encoding a secret binary image SI into n shares of meaningful halftone images within the scheme of visual cryptography (VC). Secret pixels encoded into shares introduce noise to the halftone images. We extend our previous work on halftone visual cryptography [1] and propose a new method that can encode the secret pixels into the shares via the direct binary search (DBS) halftoning method. The perceptual errors between the halftone shares and the continuous-tone images are minimized with respect to a human visual system (HVS) model [2]. The secret image can be clearly decoded without showing any interference with the share images. The security of our method is guaranteed by the properties of VC. Simulation results show that our proposed method can improve significantly the halftone image quality for the encoded shares compared with previous algorithms.	binary image;binary search algorithm;code;color image;direct-broadcast satellite;display resolution;encode;encryption;human visual system model;image quality;interference (communication);pixel;simulation;visual cryptography	Zhongmin Wang;Gonzalo R. Arce;Giovanni Di Crescenzo	2006	2006 14th European Signal Processing Conference		computer vision;computer science;theoretical computer science;error diffusion;computer graphics (images)	Vision	39.06031500566882	-10.720179227320621	104220
f77e471c76d7eaf9ea3ae3693f3f4d4c35ee43ba	robust estimation for parameters of the extended burr type iii distribution	the extended burr type iii distribution;62f10;quantile estimator;62f35;outliers;robust regression estimator	We consider various robust estimators for the extended Burr Type III (EBIII) distribution for complete data with outliers. The considered robust estimators are M-estimators, least absolute deviations, Theil, Siegel's repeated median, least trimmed squares, and least median of squares. Before we perform the aforementioned estimators for the EBIII, we adapt the quantiles method to the estimation of the shape parameter k of the EBIII. The simulation results show that the considered robust estimators generally outperform the existing estimation approaches for data with upper outliers, with certain of them retaining a relatively high degree of efficiency for small sample sizes.		Yeliz Mert Kantar;Vural Yildirim	2015	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.839032	robust statistics;econometrics;outlier;estimator;least trimmed squares;trimmed estimator;robust measures of scale;m-estimator;mathematics;robust regression;statistics	Theory	29.305614372351712	-22.87261984863463	104411
e86ac072f7841c94d980bff8a18241f33cdce6be	reweighted kernel density estimation	estimacion sesgada;integrated squared bias;mean integrated square error;kernels;finite sample;41a15;estimacion densidad;analisis datos;metodo reduccion;methode noyau;implementation;estimation densite;echantillon fini;error sistematico;62g07;estimateur noyau;kernel estimation;density estimation;data analysis;aproximacion esplin;65d07;bias;noyau mathematiques;spline approximation;approximation spline;metodo nucleo;estimation noyau;statistical computation;calculo estadistico;kernel density estimate;mean integrated squared error;analyse donnee;kernel method;methode reduction;calcul statistique;esplin cubico;spline cubique;estimation statistique;implementacion;biased bootstrap;estimacion estadistica;bias reduction;empirical likelihood;reduction method;statistical estimation;biased estimation;estimation biaisee;kernel estimator;leave one out;erreur systematique;cubic spline	Standard kernel density estimation is subject to bias that can mask structure by flattening peaks and filling in troughs in the density. A number of methods of bias reduction have been proposed including approaches based on reweighting the contributions from the individual data points. We explore the potential of bias reduction by reweighting, and propose a new type of reweighted kernel density estimator in which the weights are defined by a cubic spline on the logit scale. The free parameters of this spline are optimized with respect to a leave-one-out performance criterion. Technical aspects of the implementation of our estimator are discussed and its finite sample performance is analysed through experiments with simulated and real data. The results are very encouraging, and suggest that our new methodology is capable of significantly greater bias reduction than existing reweighted density estimators.	kernel density estimation	Martin L. Hazelton;Berwin A. Turlach	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.02.002	kernel density estimation;econometrics;mathematical optimization;multivariate kernel density estimation;mathematics;variable kernel density estimation;statistics	ML	32.823702183413964	-23.766599505857204	104682
cdfd4023bfa62280c1ed27541c5ed45d9a84864e	bayes approach in rdt using accelerated and long-term life data	reliability;posterior risk;bayesian approach;accelerated testing;reliability modeling;prior distribution;estimation of failure rate;censored data;accelerated life testing;failure rate;bayes analysis	A common problem of reliability demonstration testing (RDT) is the magnitude of total time on test required to demonstrate reliability to the consumer’s satisfaction, particularly in the case of high reliability components. One solution is the use of accelerated life testing (ALT) techniques. Another is to incorporate prior beliefs, engineering experience, or previous data into the testing framework. This may have the effect of reducing the amount of testing required in the RDT in order to reach a decision regarding conformance to the reliability specification. It is in this spirit that the use of a Bayesian approach can, in many cases, significantly reduce the amount of testing required. We demonstrate the use of this approach to estimate the acceleration factor in the Arrhenius reliability model based on long-term data given by a manufacturer of electronic components (EC). Using the Bayes approach we consider failure rate and acceleration factor to vary randomly according to some prior distributions. Bayes approach enables for a given type of technology the optimal choice of test plan for RDT under accelerated conditions when exacting reliability requirements must be met. These requirements are given by a hypothetical consumer by two different ways. The calculation of posterior consumer’s risk is demonstrated in both cases. The test plans are optimum in that they take into account Var{ l udata}, posterior risk, E{l udata}, Medianl or other percentiles of l at data observed at the accelerated conditions. The test setup assumes testing of units with time censoring. q 1999 Elsevier Science Ltd. All rights reserved.	accelerated life testing;censoring (statistics);conformance testing;electronic component;failure rate;randomness;remote digital terminal;requirement;test plan	Radim Bris	2000	Rel. Eng. & Sys. Safety	10.1016/S0951-8320(99)00025-3	reliability engineering;econometrics;prior probability;accelerated life testing;bayesian probability;engineering;failure rate;reliability;mathematics;censoring;statistics	SE	29.377785930004315	-18.954171838521713	104683
40b5bbe70f2bed3cfd4944d53baeec398fcfdce6	small-sample improvements in the statistical analysis of seasonally cointegrated systems	iterative method;metodo estadistico;finite sample;small sample;metodo monte carlo;analisis datos;echantillon fini;methode monte carlo;cointegracion;estadistica rango;statistical method;cointegration;small samples;universiteitsbibliotheek;metodo iterativo;data analysis;analisis regresion;statistical analysis;methode statistique;methode iterative;seasonality;monte carlo method;statistical computation;calculo estadistico;reduced rank regression;settore secs s 03 statistica economica;cointegration saisonniere;cointegration analysis;analyse regression;rank statistic;analyse donnee;statistique rang;pequena muestra;regression analysis;calcul statistique;monte carlo simulation;seasonal cointegration;petit echantillon	This paper proposes new iterative reduced-rank regression procedures for seasonal cointegration analysis. The suggested methods are motivated by the idea that modelling the cointegration restrictions jointly at different frequencies may increase efficiency in finite samples. Monte Carlo simulations indicate that the new tests and estimators perform well with respect to already existing statistical procedures. JEL classification: C32	iterative method;monte carlo method;simulation	Gianluca Cubadda;Pieter Omtzigt	2005	Computational Statistics & Data Analysis	10.1016/j.csda.2004.05.016	econometrics;calculus;mathematics;statistics;monte carlo method	AI	33.25794451044428	-22.581197699388017	104769
38c6411fe84342a275115203382ade8db83701bc	fast fitting of convolutions using rational approximations	weibull distribution;numerical integration	Models of response time distributions are powerful models because they capture mean response times as well as variability and asymmetry. Some of these models are expressed with a closed-form equation (e.g. the Weibull distribution). However, a large number of others are not available in closed form, including convolutions. Fitting such distributions with the likelihood method is still possible with the help of numerical integration techniques. However, the time required to fit a single subject is a matter of days. We present an alternative to numerical integration based on rational approximations. This approach is 100 times faster so that fitting a single subject can be done in less than an hour. As shown with simulations, the approach can be fully automatized, and is both reliable and accurate.	approximation;computation;convolution;curve fitting;heart rate variability;numerical analysis;numerical integration;remez algorithm;response time (technology);simulation;speedup;wolfram mathematica	Denis Cousineau	2004			numerical integration;asymmetry;response time;mathematical optimization;weibull distribution;convolution;computer science;polynomial and rational function modeling;mean and predicted response	Metrics	31.493454891309607	-16.390719389317166	104821
642b50fffb27a2fa316f29ca5a5ae97e4e7d0315	a simulation-based approach to the study of coefficient of variation of dividend yields	dividend policy;modelizacion;estimacion sesgada;empirical study;point estimation;chaine markov;cadena markov;coefficient of variation;metodo monte carlo;methode empirique;analisis estadistico;stock market;dividende;maximum likelihood;bolsa valores;normal distribution;echantillonnage gibbs;gibbs sampling;chine;metodo empirico;maximum vraisemblance;empirical method;methode monte carlo;sampling distribution;curva gauss;stock markets;bourse valeurs;stock exchange;modelisation;marche valeurs;maximum likelihood estimate;statistical analysis;dividend yields;asie;loi beta;markov chain monte carlo;ley beta;beta distribution;monte carlo method;analyse statistique;distribution echantillonnage;loi normale;dividend;hong kong;china;muestreo gibbs;distribucion muestreo;modeling;biased estimation;estimation biaisee;gaussian distribution;maxima verosimilitud;asia;markov chain	Existing empirical studies of dividend yields and dividend policies either make no assumption or the normal distribution of the dividend yields data. The statistical results will be biased because they cannot reflect the finite support set property of dividend yields which can only range from 0 to 1. We posit that the assumption that dividend yields follow a beta distribution is more appropriate. The coefficient of variation (CV) is used to measure the stability of dividend yields. If we assume dividend yields follow a normal distribution, then the maximum likelihood estimate for coefficient of variation is given by s x. This only gives us a point estimate, which cannot depict the full picture of the sampling distribution of the coefficient of variation. A simulation-based approach is adopted to estimate CV under the beta distribution. This approach will give us a point estimate as well as the empirical sampling distribution of CV. With this approach, we study the stability of dividend yields of the Hang Seng index and its sub-indexes of the Hong Kong stock market and compare the results with the traditional approach. 2007 Elsevier B.V. All rights reserved.	bayesian approaches to brain function;boson sampling;coefficient;sampling (signal processing);simulation;theory	Wan-Kai Pang;Bosco Wing-Tong Yu;Marvin D. Troutt;Shui-Hung Hou	2008	European Journal of Operational Research	10.1016/j.ejor.2007.05.032	normal distribution;financial economics;econometrics;economics;mathematics;maximum likelihood;empirical research;statistics	AI	33.06296461152569	-21.978450563116155	104886
d835fc814610fe2864f1006ea444b401126f9a79	two-stage sensitivity-based group screening in computer experiments	experimental design;design of experiments doe latin squares inputs input output analysis sensitivity analysis computer models screening gaussian curve;qa mathematics;grupo de excelencia;low impact input;design of experiments;ciencias basicas y experimentales;matematicas;active factor;latin hypercube design;gaussian process;total sensitivity index	Sophisticated computer codes that implement mathematical models of physical processes can take hours to produce a single response. Screening to determine the most active inputs is critical for understanding the input-output relationship. This paper presents new methodology based on two-stage group screening. In stage 1, groups of inputs are screened out and, at stage 2, individual active inputs are sought. Inputs are evaluated through their total effect sensitivity indices (TSIs) which are compared with a benchmark null TSI distribution. Examples show that, in comparison with with one-stage procedures, the proposed method provides accurate screening while reducing computational effort.	algorithmic efficiency;benchmark (computing);code;column (database);computation;computer simulation;gaussian process;mathematical model;minimax;nonlinear system;projection screen;schmidt decomposition;sensitivity and specificity;time-slot interchange	Hyejung Moon;Angela M. Dean;Thomas J. Santner	2012	Technometrics	10.1080/00401706.2012.725994	econometrics;mathematics;elementary effects method;design of experiments;statistics	Arch	29.82684975290009	-15.792083834099092	104980
5560160e83b91a38851cdb3d4fe97c63871ac385	xor based continuous-tone multi secret sharing for store-and-forward telemedicine		Traditional k out of n visual cryptography scheme has been proposed to encrypt single secret image into n shares where only k or more shares can decode the secret image. Many existing schemes on visual cryptography are restricted to consider only binary images as secret which are not appropriate for many important applications. Store-and-Forward telemedicine is one such application where medical images are transmitted from one site to another via electronic medium to analyze the patient’s clinical health status. The main objective of Store-and-Forward telemedicine is to provide remote clinical services via two-way communication between the patient and the healthcare provider using electronic medical image, audio and video means. In this paper, a new XOR based Continuous-tone Multi Secret Sharing scheme suitable for store-and-forward telemedicine is proposed to securely transmit the medical images. It also eliminates basic security constraints of VC like pixel expansion in shares/recovered secret images, random pattern of shares, explicit codebook requirement, lossy recovery of secret and limitation on number of secret and shares. Proposed approach is n out of n multi secret sharing scheme which is able to transmit n secret images simultaneously. All secrets could be revealed only after some computations with all n shares and one master share. Master share has been created with the secret key at encoding phase and it can be regenerated at the time of decoding using same secret key. Here all shares are meaningful in continuous-tone which may provide confidentiality to medical images during transmission. Proposed approach not only preserves all basic characteristics of traditional VC but also increases the capacity of secret image sharing. From the experiments we found that irrespective of visible contents of the shares, the probability of getting back the pixel values of respective original secret images at the receiver end is very high.	algorithm;binary image;code;codebook;computation;confidentiality;encryption;exclusive or;experiment;key (cryptography);least significant bit;lossy compression;online and offline;pixel;secret sharing;store and forward;visual cryptography	Shivendra Shivani;Rajitha Bakthula;Suneeta Agarwal	2016	Multimedia Tools and Applications	10.1007/s11042-016-4012-z	shared secret;telecommunications;computer science;theoretical computer science;shamir's secret sharing;homomorphic secret sharing;secure multi-party computation;internet privacy;proactive secret sharing;secret sharing;world wide web;key distribution;computer security;verifiable secret sharing	Crypto	38.339586485498785	-10.742815192407217	105042
c49e167b0d8eba5e3e2403fc969362e540b63f5d	sequential optimal experiment design for neural networks using multiple linearization	nonlinear mapping;point estimation;multi layer perceptron network;normal distribution;nonlinear parameter estimation;probability density function;system identification;optimal experiment design;markov chain monte carlo methods;parameter estimation;prediction error method;conditional probability;density functional;neural network;experience design	Design of an optimal input signal in system identification using a multi-layer perceptron network is treated. Neural networks of the same structure differing only in parameter values are able to approximate various nonlinear mappings. To ensure high quality of network parameter estimates, it is crucial to find a suitable input signal. It is shown that utilizing the conditional probability density function of parameters for design of the input signal provides better results than currently used procedures based on parameter point estimates only. The conditional probability density function of parameters is unknown and hence it is estimated using the Gaussian sum approach approximating arbitrary probability density function by a sum of normal distributions. This approach is less computationally demanding than the Markov Chain Monte Carlo method and achieves better results in comparison with the commonly used local prediction error methods. The properties of the proposed input signal designs are illustrated in numerical examples.	approximation algorithm;artificial neural network;design of experiments;display resolution;markov chain monte carlo;monte carlo method;multilayer perceptron;neural network software;nonlinear system;numerical analysis;numerical method;optimal design;system identification	Pavel Hering;Miroslav Simandl	2010	Neurocomputing	10.1016/j.neucom.2010.04.004	normal distribution;econometrics;mathematical optimization;cluster-weighted modeling;probability density function;density estimation;conditional probability;scale parameter;system identification;experience design;machine learning;point estimation;mathematics;estimation theory;artificial neural network;statistics	ML	28.227882698809562	-23.74057653119066	105235
320d29ddd058ff029e59e787846dc1987926554e	digital video watermark optimization for detecting replicated two-dimensional barcodes		Two-dimensional barcodes (2D codes) have become common as an authentification way of e-tickets and e-coupons. However, easy replication by using other mobile phone camera is apprehended. This paper proposes a method for detecting copied 2D code by a semi-fragile digital video watermark which can be destroyed by replication. Compared to digital image watermarking, it is more difficult to copy 2D code in which the time-varying watermark is embedded without destroying the watermark. In the proposed method, the video watermarking scheme is optimized to maximize the difference of the watermark extraction degree between valid and replicated 2D code videos. The optimization is performed using a novel self-adaptive differential evolution algorithm with actual mobile devices rather than simulation. Experimental results have shown that the proposed method successfully designs a video watermarking scheme which allows authenticity determination of 2D codes displayed on mobile device screen.	barcode;digital video	Takeru Maehara;Ryo Ikeda;Satoshi Ono	2015		10.1007/978-3-319-31960-5_16	digital image;computer science;theoretical computer science;artificial intelligence;computer vision;watermark;differential evolution;mobile phone;digital watermarking;authentication;mobile device	Vision	38.01213234902356	-11.789422577545597	105524
05138e9ab35ac03629124f1f4e4d5caa93909ef9	incorporating scatter search and threshold accepting in finding maximum likelihood estimates for the multinomial probit model	modelizacion;fonction vraisemblance;model specification;optimisation;multinomial probit scatter search threshold accepting maximum likelihood estimation;scatter search;threshold accepting;optimizacion;maximum likelihood;modelo probit;maximum vraisemblance;maximum likelihood estimation;algoritmo genetico;funcion verosimilitud;modelisation;maximum likelihood estimate;specification modele;especificacion modelo;multinomial probit;algorithme genetique;probit model;algorithme evolutionniste;genetic algorithm;global optimization;algoritmo evolucionista;optimization;modele donnee;modele probit;synthetic data;evolutionary algorithm;numerical experiment;computational efficiency;modeling;likelihood function;maxima verosimilitud;data models	This paper presents a procedure that incorporates scatter search and threshold accepting to find the maximum likelihood estimates for the multinomial probit (MNP) model. Scatter search, widely used in optimization-related studies, is a type of evolutionary algorithm that uses a small set of solutions as the selection pool for mating and generating new solutions to search for a globally optimal solution. Threshold accepting is applied to the scatter search to improve computational efficiency while maintaining the same level of solution quality. A set of numerical experiments, based on synthetic data sets with known model specifications and error structures, were conducted to test the effectiveness and efficiency of the proposed framework. The results indicated that the proposed procedure enhanced performance in terms of likelihood function value and computational efficiency for MNP model estimation as compared to the original scatter search framework.	multinomial logistic regression;probit model	Yu-Hsin Liu	2011	European Journal of Operational Research	10.1016/j.ejor.2010.10.038	econometrics;mathematical optimization;computer science;evolutionary algorithm;mathematics;maximum likelihood;statistics;global optimization	ML	30.454299923828746	-11.259580436742967	105572
a70c40191f2a4c6f931de4f74a3e24bf3a655d61	multi-population genetic algorithm quality assessment implementing intuitionistic fuzzy logic	estimation theory;ethanol;genetic algorithms estimation theory fuzzy logic;genetic algorithms computational modeling linear programming parameter estimation mathematical model estimation ethanol;fuzzy logic;mpga multipopulation genetic algorithm quality assessment intuitionistic fuzzy logic s cerevisiae fedbatch cultivation model parameters;computational modeling;estimation;linear programming;mathematical model;genetic algorithms;parameter estimation	Intuitionistic fuzzy logic has been implemented in this investigation aiming to derive intuitionistic fuzzy estimations of S. cerevisiae fed-batch cultivation model parameters obtained using multi-population genetic algorithm (MpGA). Performances of the examined algorithm have been tested before and after the application of the procedure for purposeful model parameters genesis for three different values of generation gap which is the most sensitive genetic algorithms parameter toward convergence time. Results obtained after the implementation of intuitionistic fuzzy logic for the algorithm performance assessment have been compared and MpGA with GGAP = 0.1 after the purposeful model parameters genesis procedure application has been distinguished as the fastest and the most reliable one.	fastest;fuzzy logic;genesis;genetic algorithm;mathematical optimization;performance	Maria Angelova;Krassimir T. Atanassov;Tania Pencheva	2012	2012 Federated Conference on Computer Science and Information Systems (FedCSIS)		mathematical optimization;computer science;linear programming;machine learning;mathematics;estimation theory;algorithm;statistics	EDA	26.160883964783952	-10.147513448184718	105629
5935d26591d4ce53f67983616172cf20af5cc93b	evolution and stability of an autonomous system subjected to interdependent decisionmakers	biophysics;stability organisms evolution biology random processes open systems biological system modeling genetic mutations humans biophysics sampling methods;organisms;decay rate;biological system modeling;autonomic system;stability;evolution biology;random processes;open system;humans;genetic mutations;sampling methods;open systems	Time evolution and stability of an autonomous system which is subject to many interdependent decisionmakers have been investigated. The autonomous system evolves toward a goal at which the irreversible decay rate of the system is minimized, where the autonomous system is supposed to be an open system subjected to various irreversible flows from outside, and where each of the interdependent decisionmakers is supposed to make and to revise his decisions at his own will by referring to the previous decisions made by both himself and others. The most stable and asymptotic structure with the least irreversible decay rate is the one which can restore itself even if the decisions of the interdependent decisionmakers are perturbed slightly. The surviving structure is determined as an organism holding a lasting memory of itself.	autonomous robot;autonomous system (internet);interdependence	Koichiro Matsuno	1978	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1978.4310017	stochastic process;simulation;artificial intelligence;control theory;open system;statistics	Embedded	26.04726135357094	-12.00069612629208	105631
e4b0c9bbbb139a866d7961596d4ad06657aef331	bootstrapping analysis of lifetime data with subsampling	weibull distribution;bootstrapping;percentile estimates;bias reduction;subsampling	Due to cost and time limitation, reliability experiments usually contain subsampling which is a restriction on randomization. Two-stage approach can analyze right censored Weibull distributed reliability data with subsampling. However, the confidence intervals of low percentiles cannot be obtained by two-stage approach. Moreover, the shape parameter is overestimated. In this paper, we present a two-stage bootstrapping approach and an unbiasing factor approach to solve the aforementioned problems. An example is provided to illustrate the proposed method. In addition, the proposed method is compared with current method through simulations. The results show that the proposed method performs well in terms of relative bias and root mean square error in low percentiles.		Guodong Wang;Zhanwen Niu;Shanshan Lv;Liang Qu;Zhen He	2016	Quality and Reliability Eng. Int.	10.1002/qre.1925	weibull distribution;econometrics;computer science;data mining;mathematics;bootstrapping;statistics	SE	29.228421110124494	-22.280689124856632	105996
2f9179483419f998770fca25d51e809590ebec73	forensics investigations of multimedia data: a review of the state-of-the-art	digital forensics;computer forensics;forgery multimedia communication visualization digital images image color analysis digital cameras;steganography and steganalysis;forgery;data recovery approaches for multimedia files;digital camera;digital forensics forensics investigations multimedia data;multimedia systems;source identification;computer security;fragment identification;digital cameras;visualization;multimedia systems computer forensics;content forgery;environment classification;image color analysis;state of the art;multimedia data;multimedia communication;forensics investigations;steganography and steganalysis digital forensics state of the art multimedia data survey source identification environment classification content classification content forgery data recovery approaches for multimedia files fragment identification;digital image;content classification;survey;digital images	Digital forensics is one of the cornerstones to investigate criminal activities such as fraud, computer security breaches or the distribution of illegal content. The importance and relevance of this research fields attracted various research institutes leading to substantial progress in the area of digital investigations. One essential piece of evidence is multimedia data. For this reason this paper provides an overview of the state-of-the-art in the forensic investigation of multimedia data, the relationship between the various research fields and further potential research activities.	computer security;relevance	Rainer Poisel;Simon Tjoa	2011	2011 Sixth International Conference on IT Security Incident Management and IT Forensics	10.1109/IMF.2011.14	computer science;multimedia;internet privacy;world wide web;computer forensics	DB	37.08315298427846	-12.696801764428495	106095
266a59c44afe3a284adb7b09bc53f39ec91b26cc	the decomposition of quadratic forms under skew normal settings		In this paper, the decomposition properties of noncentral skew chi-square distribution is studied. A given random variable U having a noncentral skew chi-square distribution with (ku003e1) degrees of freedom, can be partitioned into the sum of two independent random variables (U_1) and (U_2) such that (U_1) has a noncentral skew chi-square distribution with 1 degree of freedom and (U_2) has the noncentral chi-square distribution with (k-1) degrees of freedom. Also if (ku003e2), this partition can be modified into (U=U_1+U_2), where (U_1) has a noncentral skew chi-square distribution with 2 degrees of freedom and (U_2) has a central chi-square distribution with (k-2) degrees of freedom. The densities of noncentral skew chi-square distributions with 1 degree of freedom, 2 degrees of freedom, and (ku003e2) degrees of freedom are derived, and their graphs are presented. For illustration of our main results, the linear regression model with skew normal errors is considered as an application.		Ziwei Ma;Weizhong Tian;Baokun Li;Tonghui Wang	2018		10.1007/978-3-319-70942-0_15	mathematical analysis;partition (number theory);degrees of freedom (statistics);linear regression;skew;quadratic form;mathematics;graph;random variable	HCI	31.37621542186114	-23.077500033669704	106195
2a9a37f01210fdcabc153cc424d6f0858a023ec3	a stratified simulation scheme for inference in bayesian belief networks	new simulation scheme;simulation scheme;exponential runtime;average runtime;promising simulation scheme;predictable runtime;likelihood weighting scheme;bayesian belief network;exact algorithm;stratified simulation scheme;stratified scheme;likelihood weighting;mathematics	Simulation schemes for probabilistic infer­ ence in Bayesian belief networks offer many advantages over exact algorithms; for ex­ ample, these schemes have a linear and thus predictable runtime while exact algo­ rithms have exponential runtime. Exper­ iments have shown that likelihood weight­ ing is one of the most promising simulation schemes. In this paper, we present a new simulation scheme that generates samples more evenly spread in the sample space than the likelihood weighting scheme. We show both theoretically and experimentally that the stratified scheme outperforms likelihood weighting in average runtime and error in estimates of beliefs.	algorithm;bayesian network;exptime;experiment;lifting scheme;simulation;time complexity	Remco R. Bouckaert	1994			econometrics;computer science;machine learning;bayesian network;mathematics;bayesian statistics;statistics	AI	24.924741891739238	-16.95902363421261	106207
a8437da16242d1c9b73e944ddfe47d051e1e06be	lattice rules and randomized quasi-monte carlo	randomized quasi monte carlo;high dimensionality;unbiased estimator;confidence interval;large scale simulation;monte carlo method;lattice rules;error estimate;multivariate integration	High-dimensional multivariate integration is a difficult problem for which the Monte Carlo method is often the only viable approach. This method provides an unbiased estimator of the integral, together with a probabilistic error estimate (e.g., in the form of a confidence interval). The aim of randomized quasi-Monte Carlo (QMC) methods is to provide lower-variance unbiased estimators, also with error estimates. This talk will concentrate on one class of randomized QMC methods: randomized lattice rules. We will explain how these methods fit into QMC methods in general and why they are interesting, how to choose their parameters, and how they can be used for medium and large-scale simulations. Numerical examples will be given to illustrate their effectiveness. V.N. Alexandrov et al. (Eds.): ICCS 2001, LNCS 2073, p. 9, 2001. c © Springer-Verlag Berlin Heidelberg 2001	lecture notes in computer science;monte carlo method;numerical linear algebra;quantum monte carlo;quasi-monte carlo method;randomized algorithm;simulation;springer (tank)	Pierre L'Ecuyer	2001		10.1007/3-540-45545-0_7	quantum monte carlo;quasi-monte carlo method;econometrics;mathematical optimization;confidence interval;hybrid monte carlo;stein's unbiased risk estimate;monte carlo molecular modeling;mathematics;bias of an estimator;monte carlo integration;statistics;monte carlo method;control variates	ML	32.18067766699916	-16.20410045604089	106227
f97aeda62488f2f2965bbec2ddfb89962066d567	convergence of a new evolutionary computing algorithm in continuous state space	optimisation;chaine markov;cadena markov;convergence;optimizacion;raw materials;transition probability;espace etat;continuous system;algoritmo genetico;operateur evolution;systeme continu;convergencia;sistema continuo;continuous space;state space;espace continu;similartaxis operator;algorithme genetique;evolution operator;algorithme evolution;genetic algorithm;optimization;evolutionary algorithm;espacio estado;operador evolucion;operateur dissimilation;evolutionary computing;markov chain;dissimilation operator	A Markov chain on a new evolutionary computing algorithm is analyzed in continuous state space. By establishing transition probability density, the convergence of the similartaxis operator is proved. Meanwhile, the local property of the similartaxis operator is shown. To avoid its prematurity, a dissimilation operator need to be introduced. With the concept of P-absorbing field and P-optimal state, the convergence of the dissimilation operator is proved. We apply this new algorithm to a difficult problem for the accurate mixture ratio of raw materials of cement processing and make a comparison between GAs and the new algorithm. Finally, the functions of similartaxis and dissimilation operators are analyzed in a practical view.	algorithm;evolutionary computation;state space	Chuan-Long Wang;Ke-Ming Xie	2002	Int. J. Comput. Math.	10.1080/00207160211915	markov chain;artificial intelligence;evolutionary algorithm;calculus;mathematics;algorithm;statistics	Theory	30.923812523956528	-10.75754686010421	106331
17f43a3eb82509f047dac90cd416399cd1c0825e	an evaluation of change-point estimators for a sequence of normal observations with unknown parameters		ABSTRACTPerformance of maximum likelihood estimators (MLE) of the change-point in normal series is evaluated considering three scenarios where process parameters are assumed to be unknown. Different shifts, sample sizes, and locations of a change-point were tested. A comparison is made with estimators based on cumulative sums and Bartlettu0027s test. Performance analysis done with extensive simulations for normally distributed series showed that the MLEs perform better (or equal) in almost every scenario, with smaller bias and standard error. In addition, robustness of MLE to non-normality is also studied.		Jorge Garza-Venegas;Victor Tercero-Gomez;Alvaro Cordero-Franco;María del Carmen Temblador-Pérez;Mario Beruvides	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1115524	econometrics;statistics	ML	29.354797929321695	-20.828568086330225	106367
e3af70e73af646f9f86209bd3f141fca890b62a2	investigation of the widely applicable bayesian information criterion	marginal likelihood;journal article;power posteriors;widely applicable bayesian information criterion;machine learning;statistics;evidence	The widely applicable Bayesian information criterion (WBIC) is a simple and fast approximation to the model evidence that has received little practical consideration. WBIC uses the fact that the log evidence can be written as an expectation, with respect to a powered posterior proportional to the likelihood raised to a power t∗ ∈ (0, 1), of the log deviance. Finding this temperature value t∗ is generally an intractable problem. We find that for a particular tractable statistical model that the mean squared error of an optimally-tuned version of WBIC with correct temperature t∗ is lower than an optimally-tuned version of thermodynamic integration (power posteriors). However in practice WBIC uses the a canonical choice of t = 1/ log(n). Here we investigate the performance of WBIC in practice, for a range of statistical models, both regular models and singular models such as latent variable models or those with a hierarchical structure for which BIC cannot provide an adequate solution. Our findings are that, generally WBIC performs adequately when one uses informative priors, but it can systematically overestimate the evidence, particularly for small sample sizes.		Nial Friel;James P. McKeone;Chris J. Oates;Anthony N. Pettitt	2017	Statistics and Computing	10.1007/s11222-016-9657-y	econometrics;marginal likelihood;machine learning;pattern recognition;mathematics;statistics	ML	29.73493644135195	-21.19830004268842	106492
543f7cce1a2311f3eb7a6fe48d9a7463030b9bc2	normal deviate [s14] (algorithm 442)	normal distribution inverse;normal distribution;probit transform;taylor series approximation;taylor series	Submittal of an algorithm for consideration for publication in Communications of the ACM implies unrestricted use of the algorithm within a computer is permissible. Description This procedure evaluates the n-dimensional integral iv f? v(x) dx .... (a,b) cal ~a2 n by the Monte Carlo method. The variance reduction scheme used here is a form of stratified sampling. The advantages of stratified sampling are well known [I], and the concept of optimum stratification is discussed in most text books on Monte Carlo methods [2, 3, 4]. The advantages of adaptive quadrature are also well known, and many such algorithms have been published in Communications and elsewhere [5, 6, 7]. Combining adaptive quadrature with stratified sampling is a straightforward process [8, 9]. The workings of this procedure are somewhat similar to Algorithm 303 I6]. Algorithm 303 is one-dimensional, and while it can be used for multidimension integrals by recursive calls, for more than approximately six dimensions the number of evaluations of the integrand becomes intolerable. The goal of the algorithm given here is to try to overcome this defect of Algorithm 303 and other algorithms like it. The procedure works as follows: 1. A set of samples is taken, uniformly stratified throughout the entire volume being integrated. 2. Based on the variance in these samples, a decision is made as to whether more samples are needed. 3. If more samples are needed, the volume is cut in half and the Copyright @ 1973, Association for Computing Machinery, Inc. General permission to republish, but not for profit, an algorithm is granted, provided that reference is made to this publication, to its date of issue, and to the fact that reprinting privileges were granted by permission of the Association for Computing Machinery. entire procedure (but with fewer samples) is repeated on each half, recursively, the halvings being repeated as required. The choice of axis for the halving is based on samples of the gradient. The result of this process is that the overall stratification is not uniform, but approaches optimum as more and more samples are taken, since more halvings (thus more samples) are taken in the regions of high variance. A certain amount of caution must be used in the choice of the input parameter m (m + n is the number of samples taken initially). If the function being integrated is reasonably smooth, relatively low values of m (say 5 to 10) …	adaptive quadrature;apache axis;book;communications of the acm;division by two;emoticon;genetic algorithm;gradient;monte carlo method;parameter (computer programming);recursion (computer science);sampling (signal processing);software bug;stratified sampling;variance reduction	Geoffrey W. Hill;A. W. Davis	1973	Commun. ACM	10.1145/361932.361949	econometrics;taylor series;calculus;truncated normal distribution;mathematics;generalized normal distribution;statistics	Theory	31.60335341023692	-17.49653012281322	106947
96b7a9f96969f1b6f3c45b3807297d120ffc0ec1	on the preconditioned quasi-monte carlo algorithm for matrix computations		In this paper we present a quasi-Monte Carlo Sparse Approximate Inverse (SPAI) preconditioner. In contrast to the standard deterministic SPAI preconditioners that use the Frobenius norm, Monte Carlo and quasi-Monte Carlo preconditioners rely on stochastic and hybrid algorithms to compute a rough matrix inverse (MI). The behaviour of the proposed algorithm is studied. Its performance is measured and compared with the standard deterministic SPAI and MSPAI (parallel SPAI) approaches and with the Monte Carlo approach. An analysis of the results is also provided.	computation;monte carlo algorithm;monte carlo method;preconditioner;quasi-monte carlo method	Vassil N. Alexandrov;Oscar A. Esquivel-Flores;Sofiya Ivanovska;Aneta Karaivanova	2015		10.1007/978-3-319-26520-9_17	monte carlo method;preconditioner;computation;mathematics;matrix (mathematics);algorithm;matrix norm;quasi-monte carlo method	HPC	32.45972100587059	-15.78545842647978	107083
954f69c20c4a11280bc4db48ded753283c20a8b6	extensions of stability selection using subsamples of observations and covariates	stability selection;variable selection;subsampling	We introduce extensions of stability selection, a method to stabilise variable selection methods introduced by Meinshausen and Buhlmann (J R Stat Soc 72:417---473, 2010). We propose to apply a base selection method repeatedly to random subsamples of observations and subsets of covariates under scrutiny, and to select covariates based on their selection frequency. We analyse the effects and benefits of these extensions. Our analysis generalizes the theoretical results of Meinshausen and Buhlmann (J R Stat Soc 72:417---473, 2010) from the case of half-samples to subsamples of arbitrary size. We study, in a theoretical manner, the effect of taking random covariate subsets using a simplified score model. Finally we validate these extensions on numerical experiments on both synthetic and real datasets, and compare the obtained results in detail to the original stability selection method.		Andre Beinrucker;Ürün Dogan;Gilles Blanchard	2016	Statistics and Computing	10.1007/s11222-015-9589-y	econometrics;mathematical optimization;mathematics;feature selection;statistics	ML	29.379152838691862	-23.87297966647707	107292
0a92d42da3e2e1b5ae9905c8f3ab69c0e6954f74	a shrinkage approach for failure rate estimation of rare events	shrinkage estimator;failure data analysis;rare events;highly reliable systems	Quality and Reliability Engineering International#R##N#Early View (Online Version of Record published before inclusion in an issue)	failure rate;rare events	Xun Xiao;Min Xie	2016	Quality and Reliability Eng. Int.	10.1002/qre.1732	shrinkage estimator;econometrics;data mining;mathematics;statistics	Metrics	29.082175314209646	-20.297398809068522	107317
2dc8dab8c2423bf3f2577aad65a9faa0f77adbf1	a weighted prediction-based selection criterion for response surface designs	compound design criteria;logistic regression;design of experiments;v optimality	Abstract#R##N##R##N#The goal of response surface designs is typically to make precise predictions. A commonly used prediction-based design selection criterion isitV-optimality, which seeks designs that minimize the average prediction variance over the entire experimental region. We propose an alternative criterion, which seeks designs that yield small prediction variances particularly in those parts of the experimental region where a response is expected to be interesting, important, or desirable. The new criterion is a weighted V-optimality criterion, which attaches higher weights to areas with such interesting outcomes. The weights in the new criterion are derived from a logistic regression model. We illustrate the value of the new criterion using an example from the automotive industry. Copyright © 2011 John Wiley & Sons, Ltd.	response surface methodology	M. Emmett;Peter Goos;E. C. Stillman	2011	Quality and Reliability Eng. Int.	10.1002/qre.1231	econometrics;engineering;machine learning;mathematics;logistic regression;design of experiments;bayesian information criterion;statistics	EDA	27.54497453892071	-19.732523055490347	107399
b40bee78b5037c6a8cfa4bbedbef50d32af836d1	convergence of random k-nearest-neighbour imputation	28cxx;k;distance function;sample size;convergence;analisis datos;loi probabilite;ley probabilidad;dato que falta;fonction repartition;tamano muestra;taille echantillon;probabilite reponse;algorithme;donnee manquante;algorithm;funcion distribucion;vecino mas cercano;data analysis;convergencia;distribution function;probabilidad respuesta;probability distribution;statistical computation;calculo estadistico;62d05;plus proche voisin;analyse donnee;nearest neighbour;survey data;calcul statistique;28bxx;k nearest neighbour;missing data;missing values;synthetic data;hot deck;response probability;60e05;imputation;algoritmo	Random k-nearest-neighbour (RKNN) imputation is an established algorithm for filling in missing values in data sets. Assume that data are missing in a random way, so that missingness is independent of unobserved values (MAR), and assume there is a minimum positive probability of a response vector being complete. Then RKNN, with k equal to the square root of the sample size, asymptotically produces independent values with the correct probability distribution for the ones that are missing. An experiment illustrates two different distance functions for a synthetic data set. © 2006 Elsevier B.V. All rights reserved.	algorithm;experiment;geo-imputation;missing data;synthetic data	Fredrik A. Dahl	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.11.007	econometrics;missing data;mathematics;imputation;statistics	ML	32.77381178724153	-22.772064942263082	107648
6a0e694d7151b73381d08fe49d3d6bc98c9a5fbf	bayesian experimental design for the active nitridation of graphite by atomic nitrogen	kullback leibler divergence;k nearest neighbor;random variable;sensitivity analysis;information design;experimental design;optimal design;mutual information;bayesian analysis;data collection;nitrogen;information gain;uncertainty quantification	The problem of optimal data collection to efficiently learn the model parameters of a graphite nitridation experiment is studied in the context of Bayesian analysis using both synthetic and real experimental data. The paper emphasizes that the optimal design can be obtained as a result of an information theoretic sensitivity analysis. Thus, the preferred design is where the statistical dependence between the model parameters and observables is the highest possible. In this paper, the statistical dependence between random variables is quantified by mutual information and estimated using a k nearest neighbor based approximation. It is shown, that by monitoring the inference process via measures such as entropy or Kullback-Leibler divergence, one can determine when to stop the data collection process. The methodology is applied to select the most informative designs on both a simulated data set and on an experimental data set, previously published in the literature. It is also shown that the sequential Bayesian analysis used in the experimental design can also be useful in detecting conflicting information ∗Corresponding author: Gabriel Terejanu. E-mail address: terejanu@ices.utexas.edu. Address: ICES, The University of Texas at Austin, 1 University Station, C0200 Austin, TX 78712, USA. Preprint submitted to Experimental Thermal and Fluid Science July 8, 2011 ar X iv :1 10 7. 14 45 v1 [ ph ys ic s. da ta -a n] 7 J ul 2 01 1 between measurements and model predictions.	approximation;bayesian experimental design;bayesian network;design of experiments;graphite;kullback–leibler divergence;mutual information;observable;optimal design;sensor;synthetic intelligence;theory	Gabriel Terejanu;Rochan R. Upadhyay;Kenji Miki	2011	CoRR		bayesian experimental design;kullback–leibler divergence	ML	26.041793341698202	-18.35176863115443	107854
47a42e7315c8b8a505ebe60eaedcfeacaa9d33cb	eag: edge adaptive grid data hiding for binary image authentication	image coding;data encapsulation;image watermarking;message authentication	This paper proposes a novel data hiding method for authenticating binary images through establishing dense edge adaptive grids (EAG) for invariantly selecting good data carrying pixel locations (DCPL). Our method employs dynamic system structure with carefully designed local content adaptive processes (CAP) to iteratively trace new contour segments and to search for new DCPLs. By maintaining and updating a location status map, we re-design the fundamental content adaptive switch and a protection mechanism is proposed to preserve the local CAPs' contexts as well as their corresponding outcomes. Different from existing contour-based methods, our method addresses a key interference issue and has unprecedentedly demonstrated to invariantly select a same sequence of DCPLs for an arbitrary binary host image and its marked versions for our contour-tracing based hiding method. Comparison also shows that our method achieves better trade-off between large capacity and good perceptional quality as compared with several prior works for representative binary text and cartoon images.	adaptive mesh refinement;algorithm;authentication;binary data;binary image;distortion;dynamical system;experiment;extended affix grammar;fits;interference (communication);interlaced video;noise (electronics);pixel;protection mechanism	Hong Cao;Alex ChiChung Kot	2012	Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference		computer science;data mining;internet privacy;world wide web	Vision	38.574336659728516	-10.909425753942724	107907
3efd3b85acded36d36434525922f78949441ee15	an efficient and secure multi-secret image sharing scheme with general access structure	image encryption;reliability engineering;image storage;multi secret;image coding;information security;information science;niobium;data mining;cryptography protection information security computer science reliability engineering internet data security image storage image reconstruction information science;polynomials;protection;internet;cryptography;image reconstruction;pixel;secure multisecret image sharing scheme;multi use;transforms;n threshold secret sharing scheme;t;general access structure;general access structure t n threshold secret sharing scheme multi secret multi use share;computer science;secret sharing scheme;image coding cryptography;share;image encryption secure multisecret image sharing scheme general access structure;data security	In 2008, Shi et al. proposed a new (t, n)-threshold secret sharing scheme for image encryption. This paper indicates that their scheme is insecure because it is possible to find the secret image from the public image. In addition, it is inflexible since the width must be equal to the height and the size of the secret images must be the same upon sharing multi-image. Moreover, their scheme is not efficient enough. For these reasons, we propose two new schemes to avoid secret images that can be found from the public image and the width and height of secret image can be decided flexibly. Finally, the proposed schemes not only achieve some properties such as multi-secret images, multiuse, and the number of shares that are held by each participant is just one but also can be used for any general access structure in this paper.	access structure;encryption;lossless compression;secret sharing	Ching-Fan Lee;Justie Su-tzu Juan	2009	2009 Third IEEE International Conference on Secure Software Integration and Reliability Improvement	10.1109/SSIRI.2009.22	niobium;information science;computer science;cryptography;information security;theoretical computer science;shamir's secret sharing;homomorphic secret sharing;data security;secure multi-party computation;internet privacy;proactive secret sharing;secret sharing;computer security;verifiable secret sharing;pixel	Vision	38.97112585694504	-10.717526644536502	107947
b0be9ed7b4ee3d2f46b5a51fa4d96a9b07b62642	dynamic portfolio choices by simulation-and-regression: revisiting the issue of value function vs portfolio weight recursions	dynamic portfolio choices;simulation and regression;portfolio optimization;least squares monte carlo;approximate dynamic programming	Simulation-and-regression methods have been recently proposed to solve multi-period, dynamic portfolio choice problems. In the constant relative risk aversion (CRRA) framework, the “value function recursion vs portfolio weight recursion” issue was previously examined in van Binsbergen and Brandt (2007) and Garlappi and Skoulakis (2009). We revisit this issue in the context of an alternative simulation-andregression algorithmic approach which does not rely on Taylor series approximations of the value function. We find that, in this context, the portfolio weight recursion variant of the algorithm provides very precise results, is more reliable, and should be preferred to the value function recursion variant, especially for problems with long maturities and large risk-aversion levels. Acknowledgments: The first author acknowledges financial support from NSERC. Les Cahiers du GERAD G–2015–75 1	algorithm;approximation;bellman equation;benchmark (computing);bivariate data;cholesky decomposition;clenshaw–curtis quadrature;coefficient;computable function;exptime;emoticon;expected utility hypothesis;gaussian quadrature;interpolation;isoelastic utility;iteration;les trophées du libre;loop invariant;mathematical optimization;optimization problem;polynomial;qr decomposition;recursion;risk aversion;simulation	Michel Denault;Jean-Guy Simonato	2017	Computers & OR	10.1016/j.cor.2016.09.022	post-modern portfolio theory;econometrics;mathematical optimization;portfolio optimization;mathematics;statistics	AI	30.967980108497656	-15.067772993985784	108058
7a025ffe63bc83e7645d205da3fdea17d7d28c2f	instrumental weighted variables under heteroscedasticity, part i - consistency				Jan Ámos Vísek	2017	Kybernetika	10.14736/kyb-2017-1-0001	econometrics;statistics	NLP	30.364125859647885	-23.524766884220742	108111
5baca7191a853572af17b1a90c6bac6da1c266c0	destination-directed trajectory modeling and prediction using conditionally markov sequences		In some problems there is information about the destination of a moving object. An example is an airliner flying from an origin to a destination. Such problems have three main components: an origin, a destination, and motion in between. To emphasize that the motion trajectories end up at the destination, we call them destination-directed trajectories. The Markov sequence is not flexible enough to model such trajectories. Given an initial density and an evolution law, the future of a Markov sequence is determined probabilistically. One class of conditionally Markov (CM) sequences, called the CML sequence (including the Markov sequence as a special case), has the following main components: a joint endpoint density (i.e., an initial density and a final density conditioned on the initial) and a Markov-like evolution law. This paper proposes using the CML sequence for modeling destination-directed trajectories. It is demonstrated how the CML sequence enjoys several desirable properties for destination-directed trajectory modeling. Some simulations of trajectory modeling and prediction are presented for illustration.		Reza Rezaie;X. Rong Li	2018	2018 IEEE Western New York Image and Signal Processing Workshop (WNYISPW)			ML	34.67489911247771	-15.53201394801715	108217
2e1948872caef525e8aa5fa8f18c1255c2cd2f07	efficiency and robustness in monte carlo sampling of 3-d geophysical inversions with obsidian v0.1.2: setting up for success		The rigorous quantification of uncertainty in geophysical inversions is a challenging problem. Inversions are often ill-posed and the likelihood surface may be multimodal; properties of any single mode become inadequate uncertainty measures, and sampling methods become inefficient for irregular posteriors or high-dimensional parameter spaces. We explore the influences of different choices made by the practitioner on the efficiency and accuracy of Bayesian geophysical inversion methods that rely on Markov chain Monte Carlo sampling to assess uncertainty, using a multi-sensor inversion of the three-dimensional structure and composition of a region in the Cooper Basin of South Australia as a case study. The inversion is performed using an updated version of the Obsidian distributed inversion software. We find that the posterior for this inversion has complex local covariance structure, hindering the efficiency of adaptive sampling methods that adjust the proposal based on the chain history. Within the context of a parallel-tempered Markov chain Monte Carlo scheme for exploring high-dimensional multi-modal posteriors, a preconditioned Crank-Nicholson proposal outperforms more conventional forms of random walk. Aspects of the problem setup, such as priors on petrophysics or on 3-D geological structure, affect the shape and separation of posterior modes, influencing sampling performance as well as the inversion results. Use of uninformative priors on sensor noise can improve inversion results by enabling optimal weighting among multiple sensors even if noise levels are uncertain. Efficiency could be further increased by using posterior gradient information within proposals, which Obsidian does not currently support, but which could be emulated using posterior surrogates. Copyright statement. TEXT		R. Scalzo;David H Kohn;Hugo K. H. Olierook;Gregory A. Houseman;Rohitash Chandra;Mark Girolami;Sally Cripps	2018	CoRR		sampling (statistics);geophysics;prior probability;monte carlo method;markov chain monte carlo;inversion (meteorology);computer science;covariance;adaptive sampling;weighting	AI	27.13159651224051	-18.00897509081177	108301
f4dc4f061f5fcfa527b5855ec1203b1493006171	developing a bivariate spatial association measure: an integration of pearson's r and moran's i	moran s i;distribucion espacial;spatial dependence;coeficiente correlacion;point to point;spatial pattern;spatial correlation;statistical analysis;spatial distribution;analyse statistique;sda;geografia;distribution spatiale;spatial smoothing;correlation coefficient;geographie;spatial association;article;coefficient correlation;key words spatial association;geography	This research is concerned with developing a bivariate spatial association measure or spatial correlation coefficient, which is intended to capture spatial association among observations in terms of their point-to-point relationships across two spatial patterns. The need for parameterization of the bivariate spatial dependence is precipitated by the realization that aspatial bivariate association measures, such as Pearson's correlation coefficient, do not recognize spatial distributional aspects of data sets. This study devises an L statistic by integrating Pearson's r as an aspatial bivariate association measure and Moran's I as a univariate spatial association measure. The concept of a spatial smoothing scalar (SSS) plays a pivotal role in this task.	bivariate data	Sang-Il Lee	2001	Journal of Geographical Systems	10.1007/s101090100064	econometrics;spatial correlation;common spatial pattern;spatial dependence;point-to-point;calculus;mathematics;moran's i;statistics	Vision	34.58066144610565	-21.9039364638054	108429
8e7447dfaa7dd6bc3c234f359d4bdea163e49e11	a probabilistic approach for representation of interval uncertainty	chevauchement;modelizacion;epistemic uncertainty;moment matching;metodo momento;optimisation;fiabilidad;reliability;methode empirique;temps polynomial;moment method;optimizacion;loi probabilite;ley probabilidad;efficient algorithm;metodo empirico;empirical method;problema np duro;programmation stochastique;probabilistic approach;overlap;imbricacion;imperfect information;segundo momento;interval data;upper bound;modelisation;np hard problem;systeme incertain;uncertainty analysis;johnson distribution;probleme np difficile;continuous optimization;enfoque probabilista;approche probabiliste;methode moment;fiabilite;probability distribution;second moment;polynomial time;borne inferieure;informacion imperfecta;optimization;aritmetica intervalo;upper and lower bounds;interval arithmetic;multiple valued;borne superieure;arithmetique intervalle;stochastic programming;sistema incierto;modeling;uncertain system;programacion estocastica;lower bound;interval uncertainty;cota superior;cota inferior;information imparfaite;tiempo polinomial	In this paper, we propose a probabilistic approach to represent interval data for input variables in reliability and uncertainty analysis problems, using flexible families of continuous Johnson distributions. Such a probabilistic representation of interval data facilitates a unified framework for handling aleatory and epistemic uncertainty. For fitting probability distributions, methods such as moment matching are commonly used in the literature. However, unlike point data where single estimates for the moments of data can be calculated, moments of interval data can only be computed in terms of upper and lower bounds. Finding bounds on the moments of interval data has been generally considered an NP-hard problem because it includes a search among the combinations of multiple values of the variables, including interval endpoints. In this paper, we present efficient algorithms based on continuous optimization to find the bounds on second and higher moments of interval data. With numerical examples, we show that the proposed bounding algorithms are scalable in polynomial time with respect to increasing number of intervals. Using the bounds on moments computed using the proposed approach, we fit a family of Johnson distributions to interval data. Furthermore, using an optimization approach based on percentiles, we find the bounding envelopes of the family of distributions, termed as a Johnson p-box. The idea of bounding envelopes for the family of Johnson distributions is analogous to the notion of empirical p-box in the literature. Several sets of interval data with different numbers of intervals and type of overlap are presented to demonstrate the proposed methods. As against the computationally expensive nested analysis that is typically required in the presence of interval variables, the proposed probabilistic representation enables inexpensive optimization-based strategies to estimate bounds on an output quantity of interest. & 2010 Elsevier Ltd. All rights reserved.	analysis of algorithms;approximation algorithm;continuous optimization;interval arithmetic;mathematical optimization;np-hardness;numerical analysis;scalability;time complexity;unified framework	Kais Zaman;Sirisha Rangavajhala;Mark P. McDonald;Sankaran Mahadevan	2011	Rel. Eng. & Sys. Safety	10.1016/j.ress.2010.07.012	mathematical optimization;calculus;mathematics;continuous optimization;upper and lower bounds;statistics	ML	27.803606953329442	-15.266907734053925	108804
fe83681659685ca182291a489f749809b0c10a7c	extended visual cryptography for general access structures using random grids	color;visualization cryptography image reconstruction color stacking gray scale algorithm design and analysis;gray scale;visualization;stacking;cryptography;image reconstruction;security general access structure random grids noise like random shares secret image hiding innocent looking cover image extended visual cryptography scheme pixel expansion extensive codebook design;image coding cryptography data encapsulation;algorithm design and analysis	Noise-like random shares successfully hide secret images, but suffer management problem: dealers themselves cannot differentiate between the shares. This problem is solved by the extended visual cryptography scheme that stamps different innocent looking cover image on each share. In this paper, we propose an extended visual cryptography scheme using random grids which can accommodate general access structures. Random grids help us get rid of pixel expansion and extensive codebook designs. The experimental results validate the correctness of the algorithms from security perspective.	algorithm;codebook;color image;correctness (computer science);experiment;grayscale;pixel;random access;residential gateway;veritas cluster server;visual cryptography	Sonu K. Mishra;Kumar Biswaranjan	2015	2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2015.7275899	visualization;computer science;cryptography;theoretical computer science;stacking;mathematics;internet privacy;computer security	Vision	38.72075056862959	-10.027708154009373	108903
d3a89aa22c2493717b707adea4ac24681b7a2e0c	stochastic petri nets in systems biology	stochastic petri net;computational biology bioinformatics;system biology;algorithms;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	Stochastic Petri Nets are a well-known class of models that has been extensively used by computer scientists to analyse distributed and parallel systems. The Petri Net theory as it was developed by Adam Petri [5] offer means to analyse topological properties of the net such as the existence of deadlocks or invariants. With their stochastic extension, Petri Nets can also be used to model the dynamics of stochastic systems. The underlying Markov Process and information about its steady state behavior can easily be obtained.	computer scientist;deadlock;steady state;stochastic petri net;stochastic process;systems biology	Ole Schulz-Trieglaff	2005	BMC Bioinformatics	10.1186/1471-2105-6-S3-P25	computational biology;biology;dna microarray;stochastic petri net;computer science;bioinformatics;systems biology	Logic	34.163590745233165	-10.642557792914145	108961
777808e3a1e320aeb4e673a384793d4a55fd7322	health assessment method for electronic components subject to condition monitoring and hard failure		In this paper, a health assessment method for electronic components subject to condition monitoring (CM) is presented, which can be used to estimate the conditional reliability characteristics given the current operational age and the corresponding degradation state. The degradation process is characterized by a continuous-time Markov chain, which is incorporated into a Cox’s proportional hazard model to describe the hazard rate of the time to failure. The two main challenges encountered in the health assessment of electronic components subject to CM and hard failure, i.e., the large number of degradation states and the general deteriorating transition mechanism, can be properly addressed by the proposed method. To illustrate the effectiveness of the proposed method, a case study of power metal-oxide-semiconductor field-effect transistors is performed as a representative example of a general electronic application. The method for the model parameter estimation is developed and applied using the degradation and failure data obtained from an accelerated testing experiment, based on which the conditional reliability function, the mean residual life, and other health characteristics can be explicitly calculated using the proposed discretization technique. The excellent accuracy of the health assessment demonstrates the effectiveness and advantages of the proposed method applied to a real electronic component subject to CM and hard failure.		Shuai Zhao;Viliam Makis;Shaowei Chen;Yong Li	2019	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2018.2839938	control engineering;proportional hazards model;residual;condition monitoring;electronic component;hazard ratio;estimation theory;discretization;mathematics;markov chain	SE	28.879825098866984	-17.995980404676096	109056
ec873e15943c3f1dce6afec3ed24a3ed951572b2	em-based algorithms for autoregressive models with t-distributed innovations		ABSTRACTThis paper considers the estimation of parameters of AR(p) models for time series with t-distribution via EM-based algorithms. The paper develops asymptotic properties for the estimation to show that the estimators are efficient. Also testing theory for the estimators is considered. The robustness of the estimators and various tests to deviations from an assumed model is investigated. The study shows that the algorithms have equal estimation efficiency even if the error distribution is miss-specified or perturbed by outliers. Interestingly, the estimators from these algorithms performed better than that of the Modified Maximum Likelihood (MML) considered in Tiku et al. (2000).	algorithm;autoregressive model	Uchenna Chinedu Nduka	2018	Communications in Statistics - Simulation and Computation	10.1080/03610918.2017.1280164	star model	ML	29.63044951486851	-22.628666168979848	109214
9711fc185d1427508fb702ceb4c19e1e0e84a6c0	computational procedure of performance assessment of lifetime index of normal products with fuzzy data under the type ii right censored sampling plan	process capability analysis;normal distribution;type ii right censored sample;fuzzy statistical hypothesis testing	Process capability analysis has been widely applied in the field of quality control to monitor the performance of industrial processes. In practice, process capability index Cpk is a popular means to assess target-the-best type quality performance. Normal distribution is an important assumption in process capability analysis. In product quality testing experiments, the experimenter may not always be in a position to observe the quality data of all the products (or items) put on test. Therefore, censored samples may arise in practice. Moreover, observations with coarse scales; measurement error that is not quantified accurately. Therefore, imprecise data also may arise in practice. Our study purposes to utilize the process capability index Cpk in assessing the quality performance of products more generally and accurately. A new approach of analyzing normal, censored and imprecise data is proposed in our study. The new approach will apply a fuzzy statistical estimator of Cpk to develop a new fuzzy statistical hypothesis testing procedure under the normal distribution with the type II right censored sample, imprecise data and large sample. The new fuzzy statistical hypothesis testing procedure can handle normal, censored, imprecise and large sample quality data. Moreover, the purchasers can then employ the new fuzzy statistical hypothesis testing procedure to determine whether the quality performance of products adheres to the required level. The manufactures also can utilize the new fuzzy statistical hypothesis testing procedure to enhance product process capability.	censoring (statistics);computation;estimation theory;experiment;fuzzy logic;sampling (signal processing)	Wen-Chuan Lee;Ching-Wen Hong;Jong-Wuu Wu	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-141463	normal distribution;econometrics;process capability;data mining;mathematics;statistics	SE	28.36471974344795	-19.17899224601974	109222
053c3d0358cebcf75c381abf92da86d091031bd9	sparse reconstruction based watermarking for secure biometric authentication	digital watermarking;biometric security;qim	This paper presents a robust watermarking method to enhance the security of multimodal biometric authentication system. A compact representation of face is first generated by downsampling the raw image with a high ratio. Then, the face feature is employed as watermark and embedded into fingerprint image with a blind SS-QIM scheme. Under the framework of sparse reconstruction, the credibility of fingerprint data can be verified by checking the validity of extracted pattern in the authentication stage. Furthermore, if the extracted face watermark is valid, it can provide additional identity information for multimodal biometric authentication. Experimental results demonstrate that the face watermark can effectively verify the credibility of fingerprint images while not affecting their recognition performance. Plus, fusing valid face watermark with host fingerprint can also increase the reliability of authentication.	authentication;biometrics;decimation (signal processing);digital watermarking;embedded system;fingerprint;multimodal interaction;raw image format;sparse approximation;sparse matrix	Bin Ma;Chunlei Li;Zhaoxiang Zhang;Yunhong Wang	2011		10.1007/978-3-642-25449-9_31	computer vision;computer science;internet privacy;computer security	Vision	38.548117703971364	-11.808026045431857	109246
11654884defd253d958d66892248043a70d58a0d	stochastic modeling and analysis of multiple nonlinear accelerated degradation processes through information fusion	copulas;general wiener process;accelerated degradation testing;nonlinearity;multiple performance parameters;s dependency	Accelerated degradation testing (ADT) is an efficient technique for evaluating the lifetime of a highly reliable product whose underlying failure process may be traced by the degradation of the product's performance parameters with time. However, most research on ADT mainly focuses on a single performance parameter. In reality, the performance of a modern product is usually characterized by multiple parameters, and the degradation paths are usually nonlinear. To address such problems, this paper develops a new s-dependent nonlinear ADT model for products with multiple performance parameters using a general Wiener process and copulas. The general Wiener process models the nonlinear ADT data, and the dependency among different degradation measures is analyzed using the copula method. An engineering case study on a tuner's ADT data is conducted to demonstrate the effectiveness of the proposed method. The results illustrate that the proposed method is quite effective in estimating the lifetime of a product with s-dependent performance parameters.	aicardi's syndrome;elegant degradation;estimated;estimation theory;incremental funding methodology;marginal model;nonlinear system;population parameter;reliability engineering;sensor;stochastic modelling (insurance);tv tuner card;androsterone glucuronide;emotional dependency;glutamyl-trna(gln) amidotransferase complex location	Fuqiang Sun;Le Liu;Xiaoyang Li;Haitao Liao	2016		10.3390/s16081242	nonlinear system;engineering;copula;physics;statistics	SE	29.89774366650283	-18.53544632768743	109396
56b022a59e5061e68c93964b0bf0f5b92d7a9fbe	size and power properties of some tests in the birnbaum-saunders regression model	test hypothese;metodo estadistico;fonction vraisemblance;birnbaum saunders distribution fatigue life distribution gradient test lifetime data likelihood ratio test local power score test wald test;analyse multivariable;finite sample;analisis numerico;duree vie fatigue;likelihood ratio;parametre forme;test statistique;failure time;metodo monte carlo;theorie approximation;62h15;multivariate analysis;65c05;analisis datos;stochastic method;statistique score;fonction repartition;estadistica test;wald test;statistique test;temps defaillance;test hipotesis;test estadistico;score statistics;62e17;simulation;echantillon fini;statistical test;methode monte carlo;tiempo vida;simulacion;regression model;statistical method;birnbaum saunders distribution;asymptotic expansion;ddistribucion duracion vida;fatigue life;distribucion estadistica;loi asymptotique;statistical regression;62jxx;developpement asymptotique;analyse numerique;funcion verosimilitud;approximation theory;funcion distribucion;data analysis;modelo regresion;distribution function;numerical analysis;lifetime;life distribution;distribution statistique;desarrollo asintotico;methode statistique;regresion estadistica;modele regression;monte carlo method;test score;statistical computation;likelihood ratio test;calculo estadistico;fatigue life distribution;gradient test;score test;methode stochastique;shape parameter;analisis multivariable;analyse donnee;calcul statistique;asymptotic distribution;longevidad fatiga;lifetime data;duree vie;regression statistique;rapport vraisemblance;monte carlo simulation;test razon verosimilitud;test rapport vraisemblance;60e05;local power;statistical distribution;likelihood function;test wald;test statistic;62e20;distribution duree vie;relacion verosimilitud;hypothesis test;metodo estocastico	The Birnbaum–Saunders distribution has been used quite effectively to model times to failure for materials subject to fatigue and for modeling lifetime data. In this paper we obtain asymptotic expansions, up to order n and under a sequence of Pitman alternatives, for the nonnull distribution functions of the likelihood ratio, Wald, score and gradient test statistics in the Birnbaum–Saunders regression model. The asymptotic distributions of all four statistics are obtained for testing a subset of regression parameters and for testing the shape parameter. Monte Carlo simulation is presented in order to compare the finite-sample performance of these tests. We also present an empirical application.	gradient;monte carlo method;simulation	Artur J. Lemonte;Silvia L. P. Ferrari	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2010.09.008	econometrics;statistical hypothesis testing;likelihood-ratio test;calculus;regression diagnostic;mathematics;regression analysis;statistics;monte carlo method	ML	32.86456418960777	-22.284839130957486	109497
83ec55eddbb133ed41f3729aec17b4e58465c935	sparse power-law network model for reliable statistical predictions based on sampled data		A. P. Kartun-Giles, D. Krioukov, J. P. Gleeson, Y. Moreno, 5, 6, 7 and G. Bianconi School of Mathematical Sciences, Queen Mary University of London, London, United Kingdom Departments of Physics, Mathematics, and Electrical & Computer Engineering, Northeastern University, Boston, United States MACSI, Department of Mathematics and Statistics, University of Limerick, Limerick, Ireland Institute for Biocomputation and Physics of Complex Systems (BIFI), University of Zaragoza, Zaragoza, Spain Department of Theoretical Physics, Faculty of Sciences, University of Zaragoza, Zaragoza, Spain ISI Foundation, Turin, Italy Complexity Science Hub Vienna, Vienna, Austria Abstract A projective network model is a model that enables predictions to be made based on a subsample of the network data, with the predictions remaining unchanged if a larger sample is taken into consideration. An exchangeable model is a model that does not depend on the order in which nodes are sampled. Despite a large variety of non-equilibrium (growing) and equilibrium (static) sparse complex network models that are widely used in network science, how to reconcile sparseness (constant average degree) with the desired statistical properties of projectivity and exchangeability is currently an outstanding scientific problem. Here we propose a network process with hidden variables which is projective and can generate sparse power-law networks. Despite the model not being exchangeable, it can be closely related to exchangeable uncorrelated networks as indicated by its information theory characterization and its network entropy. The use of the proposed network process as a null model is here tested on real data, indicating that the model offers a promising avenue for statistical network modelling.		Alexander P. Kartun-Giles;Dmitri V. Krioukov;James P. Gleeson;Yamir Moreno;Ginestra Bianconi	2018	Entropy	10.3390/e20040257	network science;hidden variable theory;complex network;statistics;mathematics;network model;information theory;power law;projective test;null model	ML	25.099667501864317	-23.911085273590867	109557
85150d9631f05d200fb4a4eab376cbf1a90d5975	parameter inference for asynchronous logical networks using discrete time series	experimental design;regulatory network;discrete time;time series;chemotaxis;tumor;ggh model;salmonella typhimurium accumulation;common property	This paper is concerned with the dynamics of asynchronous logical models of regulatory networks as introduced by R. Thomas. Available knowledge about the dynamics of a regulatory network is often limited to a sequence of snapshots in the form of a discrete time series. Using CTL formulas together with the concept of partially monotone paths, a methodology is elaborated to investigate the compatibility of a given time series and a Thomas model. The approach can be used to revise the model, but also to evaluate the given data. Additionally, suggestions are made to analyze a model pool for common properties regarding component behavior and interaction types, aiming at results exploitable for experimental design.	computation;curve fitting;design of experiments;discretization;fits;irma board;time series;monotone	Hannes Klarner;Heike Siebert;Alexander Bockmayr	2011		10.1145/2037509.2037528	econometrics;discrete time and continuous time;simulation;computer science;artificial intelligence;time series;mathematics;chemotaxis;design of experiments;algorithm;statistics	AI	25.64768371346869	-22.898477858177554	109777
1546cea8937ca20ac9942cf9d80b9ab9676b11f9	creating discrete joint densities from continuous ones: the moment matching-maximum entropy approach	treatment planning;population pharmacokinetic modeling;discrete distribution;moment matching;plan tratamiento;metodo momento;individualized drug therapy;optimisation;pharmacocinetique;metodo entropia maxima;moment method;optimizacion;systeme discret;individual specificity;specificite individuelle;pairing;continuous system;plan traitement;satisfiability;dual problem;systeme continu;drug therapy;distribution densite;sistema continuo;methode moment;pharmacokinetics;multiple model;distribucion densidad;optimization;emparejamiento;multiple model control;sistema discreto;methode entropie maximum;appariement;especificidad individual;method of maximum entropy;maximum entropy;conversion;density distribution;discrete system;farmacocinetica	An approach for converting continuous densities into discrete ones with a pre-assigned set of support points is developed. The algorithm performs this conversion in a most skeptical, least-informative way, by finding the discrete distribution with the maximum entropy that satisfies a set of moment constraints derived from the stated continuous distribution, such as means and variances. Individualized drug therapies based on multiple model control rely on the availability of discrete distributions to generate the underlying model set. The methods developed herein are especially compatible with individualized drug therapies based on multiple model control that rely on the availability of discrete densities to generate the model set.		Mark H. Milman;F. Jiang;Roger W. Jelliffe	2001	Computers in biology and medicine	10.1016/S0010-4825(00)00035-4	pharmacotherapy;probability distribution;econometrics;duality;principle of maximum entropy;discrete system;calculus;pairing;mathematics;pharmacokinetics;statistics;satisfiability	Graphics	35.944087082165275	-23.050973435718227	109859
215b82b61bb5559671f10868220870a84a0b9ae1	key-dependent compressed domain audio hashing	databases;message authentication audio coding cryptography;audio retrieval;chaos;audio content identification;content preserving operations;post randomization;audio coding;cryptography;feature extraction;security issue;signal processing;indexation;content preserving operations audio hashing audio content identification audio retrieval audio indexing security issue hash generation random feature selection post randomization;hash generation;robustness;feature selection;hash function;message authentication;audio indexing;security;feature extraction robustness information retrieval authentication chaos signal processing algorithms fingerprint recognition data security national security intelligent systems;audio hashing;random feature selection	Perceptual audio hashing has been applied in audio content identification, retrieval and indexing for its accuracy and perceptual robustness. Security issue becomes important when perceptual hashing is applied in audio content authentication. A key-dependent compressed domain perceptual audio hashing algorithm for audio content authentication is proposed. A randomization scheme controlled by a random seed is applied in hash generation for random feature selection and post randomization. The hash function is key-dependent and collision free. Meanwhile, the randomization has little effect on robustness. The hash value is invariant to content preserving operations.	algorithm;authentication;feature selection;hash function;monkey's audio;perceptual hashing;random seed;robustness (computer science);sound card	Yuhua Jiao;Mingyu Li;Qiong Li;Xiamu Niu	2008	2008 Eighth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2008.294	feature hashing;hash table;double hashing;hash function;linear hashing;perfect hash function;extendible hashing;dynamic perfect hashing;feature extraction;open addressing;computer science;cryptography;consistent hashing;machine learning;signal processing;universal hashing;k-independent hashing;cuckoo hashing;locality preserving hashing;internet privacy;suha;feature selection;world wide web;2-choice hashing;information retrieval;locality-sensitive hashing;robustness	Robotics	37.906126145824736	-12.709269325878095	109924
5f82e96d76f956c9198756674235016eda23b7ec	adaptive selection of the optimal order of linear regression models using learning automata	stochastic automaton;metodo adaptativo;selection problem;optimisation;problema seleccion;variable structure;optimizacion;learning;linear regression model;reforzamiento;reinforcement;automata estocastico;regression model;methode adaptative;learning automata;model order selection;aprendizaje;automate stochastique;modelo regresion;apprentissage;loss function;modele regression;probability distribution;adaptive method;renforcement;optimization;probleme selection	This paper concerns the adaptive selection of the optimal order of linear regression models using a variable-structure stochastic learning automaton. The Alaike criterion is derived for stationary and non-stationary cases, and it is shown that the optimal order minimizes a loss function corresponding to the evaluation of this criterion. The order of the regression model belongs to a finite set. Each order value is associated with an action of the automaton. The Bush-Mosteller reinforcement scheme with normalized automaton input is used to adjust the probability distribution. Simulation results illustrate the feasibility and performance of this model order selection approach	automata theory;learning automata	Alexander S. Poznyak;Kaddour Najim;Enso Ikonen	1996	Int. J. Systems Science	10.1080/00207729608929199	probability distribution;reinforcement;linear regression;artificial intelligence;machine learning;mathematics;regression analysis;statistics;loss function	Logic	31.221606288812687	-11.04650969469451	110412
9603ea0b403255cdf37176c9fc66ed2a49734a54	a new approach to setting control limits of cumulative count of conforming charts for high-yield processes	ccc r chart;negative binomial distribution;average run length;cumulant	Cumulative count of conforming (CCC-r) charts are usually used to monitor nonconforming fraction p in high-yield processes. Existing approaches to setting the control limits may cause non-maximal or biased in-control average run length (ARL). Non-maximal in-control ARL implies that the chart might not quickly detect the upward shift of p from its nominal value p0. On the other hand, biased in-control ARL means that both the in-control and out-of-control ARLs are inflated. This paper develops a new approach to setting control limits for CCC-r charts with near-maximal and near-unbiased in-control ARL. Experimental results show that the proposed approach is effective in terms of the maximization and unbiasedness of in-control ARL. Copyright © 2009 John Wiley & Sons, Ltd.	chart;expectation–maximization algorithm;john d. wiley;maximal set;run-length encoding	Jung-Tai Chen	2009	Quality and Reliability Eng. Int.	10.1002/qre.1015	econometrics;computer science;operations management;mathematics;negative binomial distribution;statistics;cumulant	AI	27.92809385393235	-19.0772698847722	110530
4fea2f367f715c2566c6e1e501e779f9967d24c8	visual cryptography scheme with meaningful shares based on qr codes		"""In the past few years, visual cryptography scheme (VCS) has aroused much research attention and developed rapidly because of its simple decryption. However, meaningless shares remain a continuing challenge of VCS to its practical applications. In this paper, we propose a (<inline-formula> <tex-math notation=""""LaTeX"""">$k$ </tex-math></inline-formula>, <inline-formula> <tex-math notation=""""LaTeX"""">$n$ </tex-math></inline-formula>)-VCS combining with QR codes. To enlarge the allowable maximum size of secret image, a probabilistic sharing model is utilized. Based on it, a secret sharing method is presented with high relative difference. Furthermore, we embed the initial shares into cover QR codes by using encoding redundancy. After that, each share is meaningful and can be read by any standard QR code reader. Different from previous work, error correction capacities of the covers are perfectly preserved. We also highlight that our scheme can be used to authenticate the security of QR codes from some uncertain sources. Finally, experimental results and comparisons are provided to show the feasibility and advantages of the proposed scheme."""		Zhengxin Fu;Yuqiao Cheng;Bin Yu	2018	IEEE Access	10.1109/ACCESS.2018.2874527	redundancy (engineering);error detection and correction;cryptography;distributed computing;visual cryptography;visualization;probabilistic logic;computer science;secret sharing;information science	Crypto	38.64334180159984	-11.222637187484558	110540
7167ad21fc5e3f0539d613caf6fbe5977fe14399	on the range of heterogeneous samples	random distribution;order statistics;metodo estadistico;analyse multivariable;monotone regression;likelihood ratio;exponential distribution;order statistic;multivariate analysis;variable independante;selected works;variable aleatoire;fonction repartition;relacion orden;random sampling;62h20;variable aleatoria;estimacion promedio;tiempo vida;distribution aleatoire;ordering;copula;statistical method;proportional hazard model;statistical regression;62jxx;moyenne;dispersive ordering;funcion distribucion;relation ordre;distribution function;cebysev s sum inequality;lifetime;methode statistique;regresion estadistica;promedio;hazard rate;likelihood ratio ordering;random variable;60e15;bepress;analisis multivariable;average;variable independiente;62g30;mean estimation;duree vie;regression statistique;rapport vraisemblance;estimation moyenne;proportional hazards model;distribucion aleatoria;60e05;proportional hazard;monotone regression dependence;independent variable;range;relacion verosimilitud;cebysev s sum inequality copula dispersive ordering exponential distribution likelihood ratio ordering monotone regression dependence order statistics proportional hazards model range	"""Let R""""n be the range of a random sample X""""1,...,X""""n of exponential random variables with hazard rate @l. Let S""""n be the range of another collection Y""""1,...,Y""""n of mutually independent exponential random variables with hazard rates @l""""1,...,@l""""n whose average is @l. Finally, let r and s denote the reversed hazard rates of R""""n and S""""n, respectively. It is shown here that the mapping t@?s(t)/r(t) is increasing on (0,~) and that as a result, R""""n=X""""(""""n"""")-X""""(""""1"""") is smaller than S""""n=Y""""(""""n"""")-Y""""(""""1"""") in the likelihood ratio ordering as well as in the dispersive ordering. As a further consequence of this fact, X""""(""""n"""") is seen to be more stochastically increasing in X""""(""""1"""") than Y""""(""""n"""") is in Y""""(""""1""""). In other words, the pair (X""""(""""1""""),X""""(""""n"""")) is more dependent than the pair (Y""""(""""1""""),Y""""(""""n"""")) in the monotone regression dependence ordering. The latter finding extends readily to the more general context where X""""1,...,X""""n form a random sample from a continuous distribution while Y""""1,...,Y""""n are mutually independent lifetimes with proportional hazard rates."""		Christian Genest;Subhash C. Kochar;Maochao Xu	2009	J. Multivariate Analysis	10.1016/j.jmva.2009.01.001	econometrics;order statistic;calculus;mathematics;proportional hazards model;statistics	Theory	33.31717494071488	-20.421903207530146	110805
b72e0831a19531c8f7a26592fb471079588658fb	asymptotic behavior of reliability function for multidimensional aggregated weibull type reliability indices		We derive asymptotic approximation of high risk probability (ruin probability) for multidimensional aggregated reliability index which is a linear combination of single independent indexes, whose reliability functions (distribution tails) behave like Weibull tails.		Julia Farkas;Enkelejd Hashorva;Vladimir I. Piterbarg	2017		10.1007/978-3-319-71504-9_22	asymptotic analysis;linear combination;statistics;weibull distribution;mathematics	DB	31.964486935914067	-19.378070409635935	110997
d362fecbc982dfdc68289a3e2bba4f059408e984	a method to calculate the exit time in stochastic simulations		A novel method is presented to compute the exit time for the stochastic simulation algorithm. The method is based on the addition of a series of random variables and is derived using the convolution theorem. The final distribution is derived and approximated in the frequency domain. The distribution for the final time is transformed back to the real domain and can be sampled from in a simulation. The result is an approximation of the classical stochastic simulation algorithm that requires fewer random variates. An analysis of the error and speedup compared to the stochastic simulation algorithm is presented.	approximation algorithm;computer simulation;convolution;gillespie algorithm;speedup;stochastic gradient descent	Basil S. Bayati	2015	CoRR		mathematical optimization;combinatorics;discrete mathematics;stochastic optimization;stochastic simulation;mathematics	ML	32.23286131590141	-15.929297341541398	111094
c6438ca79673efcadb7b17a65f9e4d18818e25bf	convergencia de estimadores a mínimo de entropía robustos: aplicaciones en instrumentación y al pds	monte carlo simulation;entropy;regression	IN THIS PAPER WE PROPOSE TO CONTINUE IN THE SAME RESEARCH LINE INITIATED BY PRONZATO AND THIERRY (PRONZATO ET AL, 2000A), (PRONZATO ET AL, 2000B), (PRONZATO ET AL, 2001), RECENT WORKS INSPIRED IN THE MINIMUM-ENTROPY ESTIMATION HAVE BEEN PUBLISHED BY DE LA ROSA AND FLEURY (DE LA ROSA ET AL, 2002), (DE LA ROSA ET AL, 2003) IN THE INSTRUMENTATION FRAMEWORK. AN STATISTICAL MODEL HAS BEEN ESTABLISHED TO REPRESENT SOME INSTRUMENTAL SIGNALS, SIMILARLY, SOME LIMITED HYPOTHESIS OVER SUCH A MODEL HAVE BEEN MADE. IN FACT, WE ASSUME LIMITED KNOWLEDGE OF THE NOISE OR EXTERNAL PERTURBATIONS DISTRIBUTION THAT INTERACT INTO THE SYSTEM. THE USE OF ROBUST ESTIMATORS IN SUCH SITUATIONS IS VERY HELPFUL, SINCE THE REAL SYSTEMS ARE ALWAYS EXPOSED TO CONTINUOUS PERTURBATIONS OF UNKNOWN NATURE. SOME APPLICATIONS WHERE THE LAST IS TRUE ARE: MEDICAL INSTRUMENTATION, INDUSTRIAL PROCESSES, IN TELECOMMUNICATIONS AMONG OTHERS. SOME RESULTS OF NEW MINIMUM-ENTROPY ESTIMATORS FOR LINEAR AND NONLINEAR MODELS ARE PRESENTED, SUCH RESULTS COMPLEMENT THOSE PRESENTED BY PRONZATO AND THIERRY.		José Ismael de la Rosa Vargas	2006	Computación y Sistemas		econometrics;geography;cartography;statistics	Crypto	33.58616569328757	-19.542272607419555	111205
9390f6581f4c7d95a6ff47e7d206c470342f7d11	csimmdmv: a parallel program for stochastic characterization of multi-dimensional, multi-variant, and multi-scale distribution of heterogeneous reservoir rock properties from well log data	csimmdmv;acf;random medium;stochastic simulation;linux cluster;three dimensional;multi dimensional;natural gas;parallel kriging;level of detail;monte carlo method;model matching;gas hydrate;psdf;reservoir heterogeneity;northwest territories;software package;pdf;parallel implementation;parallel programs;monte carlo simulation;3d 3v;conditional simulation	We present CSimMDMV, a software package to simulate twoand three-dimensional, multi-variant heterogeneous reservoir models from well logs at different characteristic scales. Based on multi-variant conditional stochastic simulation, this software is able to parameterize multi-dimensional heterogeneities and to construct heterogeneous reservoir models with multiple rock properties. The models match the well logs at borehole locations, simulate heterogeneities at the level of detail provided by well logging data elsewhere in the model space, and simultaneously honor the correlations present in various rock properties. It provides a versatile environment in which a variety of geophysical experiments can be performed. This includes the estimation of petrophysical properties and the study of geophysical response to the heterogeneities. This paper describes the theoretical basis of the approach and provides the details of the parallel implementation on a Linux cluster. A case study on the assessment of natural gas hydrate amount in Northwest Territories, Canada is provided. We show that the combination of rock physics theory with multiple realizations of three-dimensional and three-variant (3D–3V) gas hydrate reservoir petrophysical models enable us to estimate the average amount of gas hydrate and associated uncertainties using Monte Carlo method. & 2010 Elsevier Ltd. All rights reserved.	computer cluster;experiment;level of detail;linux;monte carlo method;simulation;whole earth 'lectronic link	Jun-Wei Huang;Gilles Bellefleur;Bernd Milkereit	2011	Computers & Geosciences	10.1016/j.cageo.2010.11.012	econometrics;simulation;hydrology;computer science;mathematics;statistics;monte carlo method	SE	36.543020233006374	-22.01691856232464	111243
32a6a26adae12742c87234edeb445e30a0ad6356	information and random censoring	medida informacion;theorie statistique;divergence;mesure information;information measure;measures of information;teoria estadistica;theorie information;information theory;divergencia;statistical theory;teoria informacion;hollander proschan sconing approach	"""Fisher and divergence type measures of information in the area of random censoring are introduced and compared with the measures of Hollander, Proschan, and Sconing. The basic properties of statistical information theory are established for these measures of information. The winners are the classical measures of information. 1. I N T R O D U C T I O N In recent years, censoring has been at the center of statistical research activity. Statistical information theory could not escape the trend. Censored data arise naturally in a number of fields, particularly in problems of reliability and survival analysis. There are several types of censoring. Here we shall confine ourselves to the right randomly censoring type. L e t X 1 , X : . . . . . X n be independent and identically distributed (iid) random variables, a r a n d o m sample from X, corresponding, for example, to the true lifetimes of items on test. With every X i there corresponds a Y, independent of X/. The Y/'s, called censoring variables, are also iid random variables, a random sample from Y. The observations consist of the iid pairs ( Z i, 6 i ) , i = 1, 2 . . . . . n , where Z i = min(Xi, Y/) and 6 i = I~ x ' ~ y,). If •i = 1, i.e., Z i =X~, the ith observation is uncensored with probability p = P ( 6 i = 1) = f ~ _ ~ f ( x ) G ( x ) d x , and if 8 i =0 , the ith observation is censored with probability i.e. q = P ( 6 i = O) ~ _ _ = f _ ~ g ( x ) F ( x ) dx, p + q = 1, where F and G are the cdf's for X and Y, respectively, and F and G the corresponding survival functions. The 6i 's are the censoring indicators (or status variINFORMATION SCIENCES 92, 159-174 (1996) © Elsevier Science Inc, 1996 0020-0255/96/$15.00 655 Avenue of the Americas, New York, NY 10010 SSDI 0020-0255(96)00055-2 160 CH. TSAIRIDIS ET AL. ables). This is the randomly right censored model. The variable Y may be considered as a confounding variable which inhibits the experimenter's ability to observe X. If Y= 2, we have no censoring. It is interesting to note that if proportionality between hazard rates of X and Y hold, i.e., if G ( t ) = ( F ( t ) ) ¢, t~(0,zc) for some /3>0, then Z and S are independent random variables (cf. Abdushukurov and Kim [1]). There are other types of censoring, e.g., type I censoring, type II, etc., where observations, as they are collected, are missed either because the """"experiment must terminate"""" or because the design of the experiment imposes the selection of a part of the ordered statistic of the original observations. Note that random censoring includes type I censoring by simply setting Y~ = t c, where tc is some (preassigned) fixed number called fixed censoring time. For more details on censored models, see Miller [18], Lawless [14], and Nelson [19]. In the last decade, several papers have appeared in the literature dealing with measures of information and censoring. They do not deal with the foundation of measures of information in statistical information theory, but they use these measures indirectly for the solution of statistical problems. The main papers in this area are those of Sconing [25], Goel [8], Hollander, Proschan, and Sconing [10, 11], Abdushukurov and Kim [1], Kim [12], Baxter [2], and Stute [27]. With the exception of Sconing [25], Goel [8], and Hollander, Proschan, and Sconing [10, 11], these papers do not deal with the issue of how to define information in the censored data case, but they introduce functionals for the study of some statistical problems such as maximum likelihood estimation in the censoring case. Hollander, Proschan, and Sconing [10] confine themselves only to define entropy type measures of information. In 1990, the same authors considered measures of information based on Csiszar's divergence between X and Z, an approach which has been criticized by Goel [8]. Here we study this problem in the most general case. At the same time, we study the foundational properties of these measures along the lines of the work of Papaioannou and Kempthorne [21], Papaioannou [20], and Ferentinos and Papaioannou [6, 7]. In Section 2, we critically review the Hollander, Proschan, and Sconing approach and examine their entropy based measure in terms of the basic properties of statistical information theory, which are nonnegativity, additivity (strong or weak), maximal information, and sufficiency. To these properties we add the """"acid test"""" properties appropriate for random censoring introduced by the same authors. We also consider the analogous measure based on the divergence. In Section 3 and 4, we define the Fisher type and divergence type measures of information in a randomly censored setup, and in Section 5, we examine and study the properties of these measures. The conclusion of this investigation is that in random censoring, INFORMATION AND RANDOM CENSORING 161 the classical measures of information based on the distribution(s) of (Z, 6) are more appropriate for statistical information theory than the Hollander, Proschan, and Sconing or Stute type measures based on entropy, ~entropy, and Kullback-Leibler or ~divergences. 2. THE HOLLANDER, PROSCHAN, AND SCONING APPROACH The emphasis of the Hollander, Proschan, and Sconing approach is on the following two """"acid test"""" properties: (1) E[Information(X)]>~E[Information(Z, 6)] for every X , Y or a kind of maximal information property. (2) E[lnformation(Z 1, 31)] <~E[Information(Z 2, 62)] for every X, where (Zi, 3i) is the censored variable associated with Y/, i = 1,2 and Y1 ~ t Y2. Property (2) states that as censoring increases, information decreases. It is easy to check that Y1 ~st Y2 implies that the probability of censoring for (X, YI) is larger than the corresponding probability for (X, Y2). The Hollander, Proschan, and Sconing measure of information for the randomly censored model, when X and Y have discrete probability distributions Pi = P(X = i), qi = P(Y = i), is defined to be"""	baxter (robot);binary prefix;censoring (statistics);emoticon;hazard analysis;information theory;kullback–leibler divergence;maximal set;quantities of information;quantum;randomness;reliability engineering;terminate (software);vergence	Ch. Tsairidis;K. Ferentinos;T. Papaioannou	1996	Inf. Sci.	10.1016/0020-0255(96)00055-2	econometrics;statistical theory;information theory;mathematics;divergence;statistics	ML	32.36372731448229	-18.12318138693162	111307
f5c499cf739237354ee766ebb08a55489b7f744e	window switching strategy based semi-fragile watermarking for mp3 tamper detection	tamper detection;mp3;semi-fragile watermarking;window switching	MP3 is a promising carrier format for covert communication, and how to secure its fidelity and integrity is a significant problem. In this paper, we propose a semi-fragile MP3 watermarking method for tamper detection by exploiting the rule of window switching during encoding. The method carries out embedding by establishing a mapping relationship between the type of window and the MD5 of authentication information. Moreover, we design a tamper detection method by analysing the watermark bits sequence. The experimental results show the effectiveness of the proposed method in terms of accuracy for tamper detection, time efficiency, imperceptibility and advisable robustness with compressed MP3 audio.	advanced audio coding;authentication;digital watermarking;md5;mp3;semiconductor industry	Zhaopin Su;Lejie Chang;Guofu Zhang;Jianguo Jiang;Feng Yue	2016	Multimedia Tools and Applications	10.1007/s11042-016-3539-3	computer science;internet privacy;watermark;world wide web;computer security	EDA	39.0036371443802	-11.988537007273289	111370
c7f5bc2982d8b8c127830ca951ca4aabc80c24d2	limitations of augustinsson plots	microordenador;reaccion enzimatica;representation graphique;regression non lineaire;analisis datos;computerized processing;tratamiento informatico;representacion grafica;non linear regression;parametro cinetico;representation augustinsson;regresion no lineal;microordinateur;microcomputer;kinetic parameter;parametre cinetique;data analysis;constante michaelis;enzymatic reaction;analyse donnee;reaction enzymatique;traitement informatique;graphics;michaelis constant	The Augustinsson plot transforms the Michaelis-Menten equation by graphing velocity, v, as a function of velocity/substrate, v/s. The plot is linear with a negative slope for a single class of saturable sites and curvilinear for multiple classes of sites. Statistically, Augustinsson plots suffer from the limitation that there is no true independent variable. Experimental errors form a Gaussian distribution that rests neither parallel to the v axis nor v/s axis, but rather along a line emanating from the origin. Widening the range of s values produces increasingly large errors in the calculated Km and Vm values, especially when multiple sites are present. Exact data containing no error produces accurate constants using either the Michaelis-Menten or Augustinsson equation. With Km and Vm constants previously derived by fitting data with the statistically valid Michaelis-Menten equation, the Augustinsson plot may be used accurately to display multiple site data.		George W. Dombi	1992	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/8.5.475	computer science;graphics;michaelis–menten kinetics;calculus;mathematics;microcomputer;data analysis;algorithm;nonlinear regression;statistics	ML	35.4274443686729	-22.167167039552197	111414
6fea6aae4812e7ca0e71a8da78e3e908b85afebf	likelilood ratio gradient estimation: an overview	likelihood ratio;simulation;stochastic system;statistical analysis;setting adjusting;stochastic processes;mathematical programming;monte carlo method;state space;gradient estimate;gradients;rates;algorithms;optimization;technical report;monte carlo;determinants mathematics;estimates;steady state;discrete event simulation;markov chain	The likelihood ratio method for gradient estimation is briefly surveyed. Two applications settings are described, namely Monte Carlo optimization and statistical analysis of complex stochastic systems. Steady-state gradient estimation is emphasized, and both regenerative and non-regenerative approaches are given. The paper also indicates how these methods apply to general discrete-event simulations; the idea is to view such systems as general state space Markov chains.	decision theory;ibm notes;markov chain;mathematical optimization;monte carlo method;numerical aperture;simulation;state space;stochastic gradient descent;stochastic process	Peter W. Glynn	1987		10.1145/318371.318612	stochastic process;econometrics;mathematical optimization;computer science;mathematics;statistics;monte carlo method	ML	32.75527698158349	-16.995395845499907	111466
2f7c254260468c651456134d4550ac6ae2aaf84b	products, and ratios for a bivariate gamma distribution	matematicas aplicadas;point pourcentage;mathematiques appliquees;fonction repartition;loi gamma;fonction speciale;ley 2 variables;percentage point;funcion especial;funcion distribucion;ley gama;distribution function;special function;gamma distribution;special functions;bivariate distribution;applied mathematics;loi 2 variables	We derive the distributions of P = XY and W = X/(X + Y) and the corresponding moment properties when X and Y follow a bivariate gamma distribution. The expressions turn out to involve several special functions. We also provide extensive tabulations of the percentage points associated with the two distributions. These tables—obtained using intensive computing power—will be of use to practitioners of the bivariate gamma distribution. 2005 Elsevier Inc. All rights reserved.	bivariate data;classical xy model	Saralees Nadarajah	2005	Applied Mathematics and Computation	10.1016/j.amc.2005.01.070	gamma distribution;inverse-gamma distribution;mathematical analysis;percentage point;distribution function;calculus;mathematics;joint probability distribution;generalized gamma distribution;special functions;statistics	ECom	34.01823450329381	-20.744261139699763	111640
3f646c9bd4dda8726499f3398ca3ad31a926f024	stochastic degradation process modeling and remaining useful life estimation with flexible random-effects		Models with random-effects are generally used in the field of degradation modeling and remaining useful life (RUL) estimation for describing unit-to-unit variability. The wide employment of parameters, which is assumed to be subjected to normal distribution to capture this variability, may disaccord with actual industrial conditions, and will introduce misspecifications. Such misspecification can affect the accuracy of RUL estimation and the subsequent inference results. In this paper, we propose a degradation model with flexible random-effects, which makes it flexible to choose distributions to portray the unit-to-unit variability according to the available information. To do so, the mixture of normal distributions, as a distribution describing random-effects, is incorporated into a class of diffusion process based degradation models whose drift coefficient is a linear combination of some time-dependent functions with known forms. The combination coefficients of each function are treated as random variables drawn from the mixture of normal distributions. An analytical approximated probability density function (PDF) of the RUL is derived under the concept of first passage time (FPT). To identify the model parameters, a framework for parameter estimation is presented based on stochastic expectation maximization (SEM) algorithm. Finally, simulation studies are provided to demonstrate the superiority of the normal mixture over the individual normal distribution for describing random effects in RUL estimation.	elegant degradation;process modeling;stochastic process	Zhengxin Zhang;Chang-Hua Hu;Xiaosheng Si;Jianxun Zhang;Jian-Fei Zheng	2017	J. Franklin Institute	10.1016/j.jfranklin.2016.06.039	econometrics;engineering;statistics	Robotics	30.181053983802087	-19.46920379649242	111811
5e95a1dd62eccdd683b5a823dfc950b94e110563	reduction of bias and skewness with applications to second order accuracy	second order;sample size;accurate inference;satisfiability;parametric;central limit theorem;skewness reduction;bias reduction	Supposêθ is an estimator of θ in R that satisfies the central limit theorem. In general, inferences on θ are based on the central limit approximation. These have error O(n−1/2), where n is the sample size. Many unsuccessful attempts have been made at finding transformations which reduce this error to O(n−1). The variance stabilizing transformation fails to achieve this. We give alternative transformations that have bias O(n−2), and skewness O(n−3). Examples include the binomial, Poisson, chi-square and hypergeometric distributions.	approximation;automated theorem proving	Christopher S. Withers;Saralees Nadarajah	2011	Statistical Methods and Applications	10.1007/s10260-011-0167-y	sample size determination;econometrics;combinatorics;central limit theorem;mathematics;parametric statistics;second-order logic;statistics;satisfiability	ML	31.58581335247799	-22.116055620211853	111834
37ffdcac9ff444037fc1c5896c1a312d9758001e	space-varying regression models: specifications and simulation	bayes estimation;autocorrelacion;hyperparameters;association statistique;correlacion;mcmc algorithm;model specification;analyse multivariable;chaine markov;cadena markov;metodo monte carlo;metropolis hastings algo;multivariate analysis;coeficiente regresion;62j05;fonction repartition;hyperparametre;ley n variables;algorithme metropolis hastings;62h20;echantillonnage;bayesian inference;specification;simulation;modele lineaire;markov random fields;linear regression;methode monte carlo;statistical association;prior distribution;simulacion;regression model;spatial structure;modelo lineal;ley a priori;statistical regression;62jxx;markov random field;sampling;algorithme;estimation parametrique;algorithm;bayesian;funcion distribucion;modelo;modelo regresion;estimacion bayes;distribution function;asociacion estadistica;campo aleatorio;posterior distribution;especificacion;specification modele;markov chain monte carlo;especificacion modelo;regresion estadistica;62f15;modele regression;monte carlo method;linear model;regresion lineal;modele simulation;ley a posteriori;analisis multivariable;multivariate distribution;modele;modelo simulacion;correlation;estimation statistique;muestreo;regression statistique;regression coefficient;estimacion estadistica;loi n variables;loi a posteriori;statistical estimation;simulation model;discussion paper;models;metropolis hastings algorithm;sampling schemes;regression lineaire;coefficient regression;loi a priori;champ aleatoire;hyper parameter;autocorrelation;estimation bayes;algoritmo;markov chain;random field	Space-varying regression models are generalizations of standard linear models where the regression coe4cients are allowed to change in space. The spatial structure is speci%ed by a multivariate extension of pairwise di6erence priors, thus enabling incorporation of neighboring structures and easy sampling schemes. Bayesian inference is performed by incorporation of a prior distribution for the hyperparameters. This approach leads to an untractable posterior distribution. Inference is approximated by drawing samples from the posterior distribution. Di6erent sampling schemes are available and may be used in an MCMC algorithm. They basically di6er in the way they handle blocks of regression coe4cients. Approaches vary from sampling each location-speci%c vector of coe4cients to complete elimination of all regression coe4cients by analytical integration. These schemes are compared in terms of their computation, chain autocorrelation, and resulting inference. Results are illustrated with simulated data and applied to a real dataset. Related prior speci%cations that can accommodate the spatial structure in di6erent forms are also discussed. The paper concludes with a few general remarks. c © 2002 Elsevier Science B.V. All rights reserved.	approximation algorithm;autocorrelation;computation;linear model;markov chain monte carlo;sampling (signal processing);simulation	Dani Gamerman;Ajax R. B. Moreira;Håvard Rue	2003	Computational Statistics & Data Analysis	10.1016/S0167-9473(02)00211-6	econometrics;hyperparameter;linear regression;calculus;mathematics;bayesian linear regression;specification;regression analysis;statistics	AI	32.60241624027471	-23.156806337254366	111844
2e5f1059d54d8ae5de88dc92e1d97370c712300b	recursive computation of piecewise constant volatilities	parametric model;volatility;prior knowledge;dynamic program;gaussian white noise;indexation;stock returns;simulation study;long range;heteroskedasticity	Returns of risky assets are often modelled as the product of a volatility function and standard Gaussian white noise. Long range data cannot be adequately approximated by simple parametric models. The choice is between retaining simple models and segmenting the data, or to use a non-parametric approach. There is not always a clear dividing line between the two approaches. In particular, modelling the volatility as a piecewise constant function can be interpreted either as segmentation based on the simple model of constant volatility, or as an approximation to the observed volatility by a simple function. A precise concept of local approximation is introduced and it is shown that the sparsity problem of minimizing the number of intervals of constancy under constraints can be solved using dynamic programming. The method is applied to the daily returns of the German DAX index. In a short simulation study it is shown that the method can accurately estimate the number of breaks for simulated data without prior knowledge of this number.	computation;recursion (computer science)	Laurie Davies;Christian Höhenrieder;Walter Krämer	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2010.06.027	forward volatility;econometrics;implied volatility;parametric model;volatility;mathematics;white noise;stochastic volatility;heteroscedasticity;statistics	Theory	26.27273828519995	-20.687067344393366	111908
7e7932de7c36b6252e5e5580b31696671103b925	products of gaussians and probabilistic minor component analysis	processus gauss;densite probabilite;mca;analisis componente principal;sistema experto;analisis factorial;covariancia;probability density;estimacion densidad;gaussian pancake;estimation densite;covariance;analyse composante mineure;probabilistic approach;product;crepe gauss;densidad probabilidad;density estimation;analyse factorielle;producto;factor analysis;poe model;enfoque probabilista;approche probabiliste;principal component analysis;product of experts model;analyse composante principale;component analysis;produit;gaussian process;systeme expert;proceso gauss;minor component analysi;covariance structure;product of experts;expert system	Recently, Hinton introduced the products of experts architecture for density estimation, where individual expert probabilities are multiplied and renormalized. We consider products of gaussian pancakes equally elongated in all directions except one and prove that the maximum likelihood solution for the model gives rise to a minor component analysis solution. We also discuss the covariance structure of sums and products of gaussian pancakes or one-factor probabilistic principal component analysis models.	normal statistical distribution;principal component analysis;probability;product of experts	Christopher K. I. Williams;Felix V. Agakov	2002	Neural Computation	10.1162/089976602753633439	product;econometrics;probability density function;density estimation;computer science;covariance;product of experts;calculus;gaussian process;mathematics;factor analysis;expert system;statistics;principal component analysis	ML	32.85348579943833	-23.86566483113046	112216
42cb910e68c2c27e0d02c2038be8624a35b3de38	on robustness analysis of stochastic biochemical systems by probabilistic model checking		This report proposes a novel framework for a rigorous robustness analysis of stochastic biochemical systems. The technique is based on probabilistic model checking. We adapt the general definition of robustness introduced by Kitano to the class of stochastic systems modelled as continuous time Markov Chains in order to extensively analyse and compare robustness of biological models with uncertain parameters. The framework utilises novel computational methods that enable to effectively evaluate the robustness of models with respect to quantitative temporal properties and parameters such as reaction rate constants and initial conditions. The framework is applied to gene regulation as an example of a central biological mechanism where intrinsic and extrinsic stochasticity plays crucial role due to low numbers of DNA and RNA molecules. Using our methods we have obtained a comprehensive and precise analysis of stochastic dynamics under parameter uncertainty. Furthermore, we apply our framework to compare several variants of twocomponent signalling networks from the perspective of robustness with respect to intrinsic noise caused by low populations of signalling components. We succeeded to extend previous studies performed on deterministic models (ODE) and show that stochasticity may significantly affect obtained predictions. Our case studies demonstrate that the framework can provide deeper insight into the role of key parameters in maintaining the system functionality and thus it significantly contributes to formal methods in computational systems biology.	cell signaling;computation;formal methods;initial condition;markov chain;model checking;non-deterministic turing machine;population;statistical model;stochastic process;systems biology	Lubos Brim;Milan Ceska;Sven Drazan;David Safránek	2013	CoRR		econometrics;computer science;machine learning;statistics	Comp.	33.47567727607894	-12.521250617421787	112261
cb2863aebdad680b2970d9a21acdd7e347d10d91	improved mean and variance estimating formulas for pert analyses	standards;project evaluations;scheduling forecasting statistics probability optimization project evaluations project scheduling;measurement uncertainty;statistical distributions optimisation parameter estimation probability project management;forecasting statistics probability;accuracy;shape;scheduling;approximation methods accuracy shape standards measurement uncertainty estimation error;project progress controlling activity program evaluation and review technique pert mean variance estimation formula probability weights parameter estimation estimation error minimization pearson distribution system bell shaped beta distributions j shaped beta distributions project settings;optimization;project scheduling;approximation methods;estimation error	The program evaluation and review technique (PERT) is a popular method for measuring and controlling activity progress in projects. Its structure is simple, and the result is fairly accurate as long as none of its base assumptions is violated. Many authors have challenged these assumptions and suggested improvements to the mean and variance formulas. This paper quantifies the accuracy of a wide range of PERT mean-variance estimation formulas. In addition, we develop a new PERT variant using common percentiles. The proposed method uses three points for estimation, just like the classical PERT. However, it provides options for the selection of the three points. It provides different set of probability weights by the selection of the three points and what parameter to estimate, i.e., mean or variance, which minimizes the estimation error. We compare the accuracy of our approach with existing methods using the Pearson distribution system. Our use of the Pearson system allows us to systematically compare different PERT methods over a wider range of distribution shapes than has previously been considered. This analysis shows that, despite its simple structure, our new method outperforms existing methods when estimating means and variances of most bell-shaped and J-shaped beta distributions. We also demonstrate how practitioners could use our new methods in actual project settings.	mean squared error;optimistic concurrency control;p5 (microarchitecture);program evaluation and review technique;second level address translation	Seong Dae Kim;Robert K. Hammond;J. Eric Bickel	2014	IEEE Transactions on Engineering Management	10.1109/TEM.2014.2304977	econometrics;mathematical optimization;shape;mathematics;accuracy and precision;scheduling;schedule;statistics;measurement uncertainty	Visualization	29.148587856499606	-14.096306934277093	112389
133ead17f18f4ad7d510af277a8ceed20f646211	general balance, large data sets and extensions to unbalanced treatment structures	experimental design;large data sets;unbalanced designs;general balance;analysis of variance;analysis of variance algorithms;multiple regression	General balance defines an important class of designs covering virtually all the traditional experimental designs and, in particular, those with several error terms. In these designs the block terms are mutually orthogonal, the treatment terms are also mutually orthogonal, and the contrasts of each treatment term have equal efficiency factors in each of the strata where they are estimated. Generally balanced designs can efficiently be analysed by an algorithm which involves a series of sweep operations for each stratum. Each sweep estimates the effects of a treatment term (by its means divided by the efficiency factor), and subtracts them from the current working vector which then becomes the working vector for the next sweep. Workspace requirements are considerably less than those of multiple-regression style algorithms, so the algorithm has great advantages particularly with large treatment models. With large data sets, however, it may be more difficult to fulfill all the conditions of general balance. One possibility is to retain orthogonal block structure, but allow unbalanced treatment structures. It is shown that the algorithm can successfully be adapted to this situation, but it now requires an iterative sequence of sweeps to fit the treatment model in each stratum.	unbalanced circuit	Roger W. Payne	2003	Computational Statistics & Data Analysis	10.1016/S0167-9473(03)00094-X	econometrics;mathematical optimization;analysis of variance;linear regression;mathematics;design of experiments;statistics	ML	28.53117736495755	-21.81752459710221	112733
f6d3e08c7647fb96ccc682dcc19a3ab187287f85	designing a screening experiment with a reciprocal weibull degradation rate	sample size;termination time;lognormal distribution;reciprocal weibull distribution;degradation rate;weibull distribution;optimal design;resolution iii design;simulation study;inspection frequency;design of experiment	Degradation tests and design of experiments are powerful techniques to improve the reliability of highly reliable products. With respect to a resolution III experiment with a linearized degradation model where the degradation rate follows a lognormal distribution, Yu and Chiao [Yu, H. F., & Chiao, C. H. (2002). An optimal designed degradation experiment for reliability improvement. IEEE Transactions on Reliability, 51 (4) 427–433] addressed the problem of how to determine the optimal settings of decision variables such as the inspection frequency, the sample size, and the termination time for each run, which are influential to the precision of identifying significant factors and the experimental cost. In practical applications, Weibull and lognormal distributions are much alike. They may fit the lifetime data well. However, their predictions may lead to a significant difference. In this paper, we will deal with the optimal design of a resolution III experiment with a linearized degradation model where the degradation rate follows a reciprocal Weibull distribution. First, an intuitively appealing identification rule is proposed. Next, under the constraints of a minimum probability of correct decision and a maximum probability of incorrect decision of the proposed identification rule, the optimal test plan is derived by using the criterion of minimizing the total cost of experiment. An example is provided to demonstrate the proposed method. Finally, a simulation study is also provided to discuss the effects of mis-specification between the models of Yu and Chiao (2002) and the present paper on identification efficiency. 2007 Elsevier Ltd. All rights reserved.	decision theory;design of experiments;elegant degradation;experiment;optimal design;simulation;test plan	Hong-Fwu Yu	2007	Computers & Industrial Engineering	10.1016/j.cie.2006.09.004	sample size determination;reliability engineering;weibull distribution;econometrics;engineering;optimal design;log-normal distribution;mathematics;design of experiments;statistics	AI	28.99021650129693	-19.125139875534966	112770
3df1a8077784c8b7252d05a292845949bf0c043a	linear shrinkage estimator of scale parameter of morgenstern type bivariate logistic distribution using ranked set sampling		Ranked set sampling is applicable whenever ranking of a set of sampling units can be done easily by a judgement method or based on the measurement of an auxiliary variable on the units selected. In this paper, we have suggested some linear shrinkage estimators of the scale parameter of Morgenstern type bivariate logistic distribution (MTBLD) using ranked set sampling alongwith their properties. Relevant conditions are obtained under which the suggested estimators are better than Chacko and Thomas (6) estimators and the minimum mean squared error (MMSE) estimators. Numerical illustrations are given in support of the present study.	bivariate data;sampling (signal processing)	Housila P. Singh;Vishal Mehta	2014	MASA	10.3233/MAS-140301	econometrics;mathematical optimization;mathematics;statistics	HCI	30.26872968517338	-22.87652646221799	112829
f684b80fd7e23da7ff43d393553558a448c0896d	a structural mixed model to shrink covariance matrices for time-course differential gene expression studies	bayes estimation;association statistique;correlacion;configuracion;metodo estadistico;analyse multivariable;structural model;repeated measurement;ridge regression;covariance analysis;covariancia;decomposition;multivariate analysis;cholesky decomposition;analisis datos;bayesian approach;time course;05bxx;regresion ridge;methode cholesky;metodo cholesky;estadistica test;statistique test;metodo descomposicion;62h20;ley gran numero;loi grand nombre;methode decomposition;variance analysis;modele mixte;shrinkage estimator;statistical association;matrice covariance;differential gene expression;covariance;statistical method;mesure repetee;matriz covariancia;natural extension;eigenvalues;statistical regression;eigenvector;modele structure;eigenvalue;vector propio;law of large numbers;decomposition method;data analysis;estimacion bayes;asociacion estadistica;regression pseudo orthogonale;analyse covariance;mixed model;estimateur retrecissement;methode statistique;covariance matrices;analisis variancia;regresion estadistica;valor propio;62j10;statistical computation;calculo estadistico;analyse correlation;modelo estructura;analisis multivariable;analyse donnee;valeur propre;calcul statistique;microarray;correlation;descomposicion;analisis covariancia;false positive;modelo mixto;cholesky method;puce a adn;regression statistique;configuration;62j07;medida repetida;vecteur propre;test statistic;analisis correlacion;analyse variance;eigenvectors;covariance matrix;estimation bayes;variance;variancia;correlation analysis	Time-course microarray studies require a particular modelling of covariance matrices when measures are repeated on the same individuals. Taking into account the within-subject correlation in the test statistics for differential gene expression involves, however, a large number of parameters when a gene-specific approach is used, which often results in a lack of power due to the small number of individuals usually considered in microarray experiments. Shrinkage approaches can improve this detection power in differential gene expression studies by reducing the number of parameters, while offering a good flexibility and a small rate of false positives. The aim of this presentation is to propose several other F-type statistics for timecourse microarray studies based on an extension of the structural mixed model presented by [1] to the multivariate case. We will focus on two main decompositions of the empirical gene-bygene covariance matrices: eigenvalue/eigenvector and Cholesky decomposition. A structural mixed model will be used in three configurations to shrink i) the eigenvalues, ii) the innovation variances, iii) both the innovation variances and antedependence parameters. The F-statistics based on these shrunk covariance matrices were compared to a gene-by-gene analysis, an homogeneous covariance model and the modified Limma F-test both on simulated and real data sets. The proposed methods were found to perform well compared to other empirical Bayesian approaches, and outperformed the gene-specific or common-covariance methods in many cases. Computing the denominator degrees of freedom of these F-statistics under H0 still remains, however, an open question.	cholesky decomposition;experiment;microarray databases;mixed model	Guillemette Marot;Jean-Louis Foulley;Florence Jaffrézic	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.04.018	econometrics;eigenvalues and eigenvectors;calculus;mathematics;statistics	ML	33.350481618692534	-23.023777364688495	113037
f4ac9296725cdb8b0770dc7e222e7e48db1a2397	bootstrap method for characterizing the effect of uncertainty in shear strength parameters on slope reliability	bootstrap method;reliability index;uncertainty;slope reliability;probability distribution	This paper aims to propose a bootstrap method for characterizing the effect of uncertainty in shear strength parameters on slope reliability. The procedure for a traditional slope reliability analysis with fixed distributions of shear strength parameters is presented first. Then, the variations of the mean and standard deviation of shear strength parameters and the Akaike Information Criterion values associated with various distributions are studied to characterize the uncertainties in distribution parameters and types of shear strength parameters. The reliability of an infinite slope is presented to demonstrate the validity of the proposed method. The results indicate that the bootstrap method can effectively model the uncertain probability distributions of shear strength parameters. The uncertain distributions of shear strength parameters have a significant influence on slope reliability. With the bootstrap method, the slope reliability index is represented by a confidence interval instead of a single fixed index. The confidence interval increases with increasing factor of slope safety. Considering both the uncertainties in distribution parameters and distribution types of shear strength parameters leads to a higher variation and a wider confidence interval of reliability index. & 2015 Elsevier Ltd. All rights reserved.	akaike information criterion;booting;bootstrapping (statistics);reliability engineering;resampling (statistics)	Dian-Qing Li;Xiao-Song Tang;Kok-Kwang Phoon	2015	Rel. Eng. & Sys. Safety	10.1016/j.ress.2015.03.034	probability distribution;reliability engineering;econometrics;uncertainty;mathematics;statistics	ML	28.977444401388983	-19.20837807512273	113262
88c8854e9e3c8c325516cb3df9dd05127be40a92	comparison of selection methods in on-line distributed evolutionary robotics	neural networks;selection methods;multi agent systems;evolutionary robotics;artificial intelligence	In this paper, we study the impact of selection methods in the context of on-line on-board distributed evolutionary algorithms. We propose a variant of the mEDEA algorithm in which we add a selection operator, and we apply it in a task-driven scenario. We evaluate four selection methods that induce different intensity of selection pressure in a multi-robot navigation with obstacle avoidance task and a collective foraging task. Experiments show that a small intensity of selection pressure is sufficient to rapidly obtain good performances on the tasks at hand. We introduce different measures to compare the selection methods, and show that the higher the selection pressure, the better the performances obtained, especially for the more challenging food foraging task.	collective intelligence;evolutionary algorithm;evolutionary robotics;experiment;fitness function;obstacle avoidance;on-board data handling;online and offline;performance;robotic mapping;swarm	Iñaki Fernández Pérez;Amine M. Boumaza;François Charpillet	2014	CoRR	10.7551/978-0-262-32621-6-ch046	simulation;computer science;artificial intelligence;machine learning;multi-agent system;evolutionary robotics;artificial neural network	NLP	25.423230389636473	-12.138222530981244	113394
8ec7fde00942c203590cf191dcf61bc0dd03111b	latent time-series motifs	time series;motifs;repeated patterns	Motifs are the most repetitive/frequent patterns of a time-series. The discovery of motifs is crucial for practitioners in order to understand and interpret the phenomena occurring in sequential data. Currently, motifs are searched among series sub-sequences, aiming at selecting the most frequently occurring ones. Search-based methods, which try out series sub-sequence as motif candidates, are currently believed to be the best methods in finding the most frequent patterns.  However, this paper proposes an entirely new perspective in finding motifs. We demonstrate that searching is non-optimal since the domain of motifs is restricted, and instead we propose a principled optimization approach able to find optimal motifs. We treat the occurrence frequency as a function and time-series motifs as its parameters, therefore we learn the optimal motifs that maximize the frequency function. In contrast to searching, our method is able to discover the most repetitive patterns (hence optimal), even in cases where they do not explicitly occur as sub-sequences. Experiments on several real-life time-series datasets show that the motifs found by our method are highly more frequent than the ones found through searching, for exactly the same distance threshold.	approximation algorithm;experiment;frequency response;gradient descent;hill climbing;local optimum;loss function;mathematical optimization;optimization problem;real life;sequence motif;time series;times ascent	Josif Grabocka;Nicolas Schilling;Lars Schmidt-Thieme	2016	TKDD	10.1145/2940329	bioinformatics;machine learning;time series;data mining;mathematics;statistics	ML	29.61201188119792	-11.840445711017296	113438
25f462b688cdb6143481a8a7080cf9afaf05c35c	k-adic similarity coefficients for binary (presence/absence) data	dice sorenson coefficient;rand index;association statistique;jaccard coefficient;analyse multivariable;computacion informatica;multivariate analysis;analisis datos;62h20;statistical association;resemblance measures;simple matching coefficient;data analysis;asociacion estadistica;ciencias basicas y experimentales;global order equivalence;matematicas;indexation;donnee binaire;analisis multivariable;analyse donnee;dato binario;binary data;grupo a;indices of association	k-Adic formulations (for groups of objects of size k) of a variety of 2adic similarity coefficients (for pairs of objects) for binary (presence/absence) data are presented. The formulations are not functions of 2-adic similarity coefficients. Instead, the main objective of the the paper is to present k-adic formulations that reflect certain basic characteristics of, and have a similar interpretation as, their 2adic versions. Two major classes are distinguished. The first class is referred to as Bennani-Heiser similarity coefficients, which contains all coefficients that can be defined using just the matches, the number of attributes that are present and that are absent in k objects, and the total number of attributes. The coefficients in the second class can be formulated as functions of Dice’s association indices.	british undergraduate degree classification;coefficient;first-class function	Matthijs J. Warrens	2009	J. Classification	10.1007/s00357-009-9032-1	association;econometrics;rand index;calculus;mathematics;multivariate analysis;data analysis;jaccard index;statistics	ML	35.3640520760223	-20.229043652046062	113570
aaa33c26c060e061aab7f7d352088be12fadf281	generalized convergence models for tournament- and (mu, lambda)-selection	order statistic	This paper analyzes the eeect of noise on diierent selection mechanisms for genetic algorithms. Models for several selection scheme are developed that successfully predict the convergence characteristics of genetic algorithms within noisy environments. The selection schemes modeled in this paper include proportionate selection, tournament selection,-selection, and linear ranking selection. These models are shown to accurately predict the convergence rate of genetic algorithms under a wide range of noise levels.	genetic algorithm;rate of convergence;tournament selection	Thomas Bäck	1995			econometrics;combinatorics;mathematics;statistics	AI	28.38477198632015	-12.489996735176925	113571
4d10a5f1905f7e075c89185d402804f55b40dcb9	on the efficiency evaluation of a novel scheme based on daubechies wavelet for watermarking in 5g		Nowadays the Internet infrastructure enables the transfer of massive amounts of large data sets throughout the Web. Over this infrastructure, the risks of infringement of the owner's rights and the incapability to defend their data from attacks are important. There are several demands for novel and smart watermarking schemes to save those rights from an illegal copying of the digital products. The Digital watermarking is a technology used to protect the copyrights of the digital media. Image watermarking scheme is used to limit the chances of piracy. Various approaches were proposed to be used with an image watermarking scheme, such as wavelet transforms, principle component analysis and singular value decomposition. In this work, the proposed watermarking scheme is based on the Daubechies family wavelets, Daubechies-5 and Daubechies-7 wavelet transform. This wavelet family approach is highly robust against various types of attacks, prohibiting the piracy and authentication of the digital data. The experimental results depict that this proposed scheme allows protection at a higher level compared with other existing frameworks and schemes.	authentication;daubechies wavelet;digital data;digital image;digital media;digital watermarking;internet;peak signal-to-noise ratio;principal component analysis;scheme;singular value decomposition;wavelet transform;world wide web	Tamara K. Al-Shayea;Constandinos X. Mavromoustakis;George Mastorakis;Jordi Mongay Batalla;Evangelos K. Markakis;Evangelos Pallis	2018	2018 IEEE 23rd International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD)	10.1109/CAMAD.2018.8514968	digital data;computer science;digital media;real-time computing;wavelet transform;wavelet;digital watermarking;theoretical computer science;copying;daubechies wavelet;authentication	EDA	38.84419488512968	-11.92049242771019	114051
b74434a112529a36a3effd420942f8a9832455c9	scalable approximate bayesian inference for outlier detection under informative sampling		Government surveys of business establishments receive a large volume of submissions where a small subset contain errors. Analysts need a fast-computing algorithm to flag this subset due to a short time window between collection and reporting. We offer a computationallyscalable optimization method based on non-parametric mixtures of hierarchical Dirichlet processes that allows discovery of multiple industry-indexed local partitions linked to a set of global cluster centers. Outliers are nominated as those clusters containing few observations. We extend an existing approach with a new “merge” step that reduces sensitivity to hyperparameter settings. Survey data are typically acquired under an informative sampling design where the probability of inclusion depends on the surveyed response such that the distribution for the observed sample is different from the population. We extend the derivation of a penalized objective function to use a pseudo-posterior that incorporates sampling weights that “undo” the informative design. We provide a simulation study to demonstrate that our approach produces unbiased estimation for the outlying cluster under informative sampling. The method is applied for outlier nomination for the Current Employment Statistics survey conducted by the Bureau of Labor Statistics. c ©2016 Terrance D. Savitsky.	algorithm;anomaly detection;computer cluster;information;loss function;mathematical optimization;optimization problem;sampling (signal processing);simulation;undo	Terrance D. Savitsky	2016	Journal of Machine Learning Research		frequentist inference;pattern recognition;statistics	ML	29.269802226560888	-21.20074826991336	114077
62213f45ec144174fc1b59d3798948900f7d4633	parametric bootstrap inferences for unbalanced panel data models	secondary 62j05;bootstrap resampling;parametric bootstrap pivotal variable;primary 62f40;coverage probability;missing data	This paper presents parametric bootstrap (PB) approaches for hypothesis testing and interval estimation for the regression coefficients of panel data regression models with incomplete panels. Some simulation results are presented to compare the performance of the PB approaches with the approximate inferences. Our studies show that the PB approaches perform satisfactorily for various sample sizes and parameter configurations, and the performance of PB approaches is mostly better than the approximate methods with respect to the coverage probabilities and the type I error rates. The PB inferences have almost exact coverage probabilities and Type I error rates. Furthermore, the PB procedure can be simply carried out by a few simulation steps, and the derivation is easier to understand and to be extended to the multi-way error component regression models with unbalanced panels. Finally, the proposed approaches are illustrated by using a real data example.	bootstrapping (statistics);data model;panel data;unbalanced circuit	Liwen Xu;Dengkui Wang	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2016.1248567	bootstrapping;econometrics;missing data;data mining;mathematics;statistics	DB	29.456660928886592	-22.55448927583649	114213
cfeb814f17baece8253d50187c890643332353ed	generalized levinson-durbin sequences, binomial coefficients and autoregressive estimation	metodo cuadrado menor;processus ponctuel;association statistique;correlacion;metodo estadistico;methode moindre carre;analyse multivariable;processus temps discret;multivariate analysis;temps polynomial;least squares method;modelo autorregresivo;62m10;analyse tendance;62h20;coefficient binomial;60g12;37c25;point process;erreur quadratique moyenne;summation;estimacion promedio;trend analysis;processus autoregressif;statistical association;60g55;punto fijo;statistical method;partial correlations;sumacion;satisfiability;autoregressive model;fixed point;estimacion insesgada;asociacion estadistica;levinson durbin sequence;autoregressive processes;methode statistique;point fixe;mean square error;partial correlation;least square;analyse correlation;correlation function;polynomial time;yule walker estimator;coeficiente binomial;least squares estimate;11b65;proceso puntual;analisis multivariable;05a10;binomial coefficients;binomial coefficient;least squares estimator;mean estimation;unbiased estimation;error medio cuadratico;correlation;estimation moyenne;modele autoregressif;discrete time processes;sommation;fix point;estimation sans biais;analisis correlacion;generalized levinson durbin sequence;binomial coefficients generalized levinson durbin sequence least squares estimator levinson durbin sequence partial correlations yule walker estimator;analisis tendencia;correlation analysis;tiempo polinomial	For { t y } a discrete time, second-order stationary process, the Levinson-Durbin recursion is used to determine the coefficients , jk α j=1, … , k, of the best linear predictor of 1 + k y , , ˆ 1 1 1 y y y kk k k k α α − − − = + L best in the sense of minimizing the mean square error. The coefficients jk α determine a Levinson-Durbin sequence. A generalized Levinson-Durbin sequence, a special case of a sequence of generalized binomial coefficients, is studied. Binomial coefficients form a generalized Levinson-Durbin sequence, and all generalized Levinson-Durbin sequences are shown to obey some summation formulas which generalize summations satisfied by binomial coefficients. The summation formulas are expressed in terms of the partial correlation sequence. Levinson-Durbin sequences arise in the construction of autoregressive model coefficient estimates for the Yule-Walker, tapered Yule-Walker, Burg and Kay estimators. The least squares autoregressive estimator, though, does not give rise to a Levinson-Durbin sequence. However, least squares fixed point processes, which yield least squares estimates of the coefficients unbiased to order 1/T, where T is the sample length, can be combined to construct Levinson-Durbin sequences. By contrast, analogous Yule-Walker fixed point processes do not combine to construct Levinson-Durbin sequences. The least squares fixed point processes are studied when the mean of the process is a polynomial time trend that is estimated by least squares. For each degree of polynomial trend, the fixed point processes form a sequence of projections from an infinite order fixed point process. The correlation functions and spectral 2 densities of these infinite order fixed point processes are derived for polynomial trends of degree up to 5.	amiga walker;autoregressive model;coefficient;fixed point (mathematics);kerrison predictor;least squares;levinson recursion;mean squared error;point process;polynomial;stationary process;time complexity	Paul Shaman	2010	J. Multivariate Analysis	10.1016/j.jmva.2010.01.004	generalized least squares;econometrics;partial correlation;calculus;mathematics;non-linear least squares;least squares;binomial coefficient;statistics;recursive least squares filter	Theory	34.167379382428784	-22.579211815473226	114349
3e5cd32aa5616df020aa7a5e789f1ad5566bdddf	monte carlo methods for american options	metodo cuadrado menor;suite aleatoire;modelizacion;methode moindre carre;least squares approximations;quasi random sequences american options least squares monte carlo method;metodo monte carlo;least squares method;random sequences;simulation;methode monte carlo;monte carlo methods cost accounting security least squares methods upper bound mathematics performance analysis optimization methods stochastic processes differential equations;simulacion;sucesion aleatoria;stock markets;modelisation;monte carlo method;random sequence;least squares monte carlo;random sequences stock markets monte carlo methods least squares approximations;american option;monte carlo;modeling;monte carlo methods	We review the basic properties of American options and the difficulties of applying Monte Carlo valuation to American options. Recent progress on the Least Squares Monte Carlo (LSM) method is described, including the use of quasi-random sequences in LSM. A particle approach to evaluation of American options is formulated. Conclusions and prospects for future research are discussed.	least squares;low-discrepancy sequence;monte carlo method;monte carlo methods in finance;value (ethics)	Russel E. Caflisch;Suneal Chaudhary	2004	Proceedings of the 2004 Winter Simulation Conference, 2004.	10.1109/WSC.2004.1371513	econometrics;mathematical optimization;computer science;mathematics;monte carlo methods for option pricing;statistics;monte carlo method	AI	33.8077364371976	-16.80060486732179	114448
662bfa7793f7caebbe52b07ee82c4483523d10a8	case study on statistically estimating minimum makespan for flow line scheduling problems	kolmogorov smirnov;confidence interval estimation;objective function;extreme value theory;confidence interval;scheduling;least square;scheduling problem;heuristics;parameter estimation;statistical estimation;lower bound;flexible flow line	Lower bounds are typically used to evaluate the performance of heuristics for solving combinatorial minimization problems. In the absence of tight analytical lower bounds, optimal objective-function values may be estimated statistically. In this paper, extreme value theory is used to construct confidence-interval estimates of the minimum makespan achievable when scheduling nonsimilar groups of jobs on a two-stage flow line. Experimental results based on randomly sampled solutions to each of 180 randomly generated test problems revealed that (i) least-squares parameter estimators outperformed standard analytical estimators for the Weibull approximation to the distribution of the sample minimum makespan; (ii) to evaluate each Weibull fit reliably, both the Anderson–Darling and Kolmogorov–Smirnov goodnessof-fit tests should be used; and (iii) applying a local improvement procedure to a large sample of randomly generated initial solutions improved the probability that the resulting Weibull fit yielded a confidence interval covering the	approximation;extreme value theory;heuristic (computer science);least squares;loss function;makespan;maxima and minima;procedural generation;randomness;scheduling (computing)	Amy D. Wilson;Russell E. King;James R. Wilson	2004	European Journal of Operational Research	10.1016/S0377-2217(02)00910-4	kolmogorov–smirnov test;job shop scheduling;econometrics;mathematical optimization;confidence interval;heuristics;extreme value theory;mathematics;upper and lower bounds;estimation theory;least squares;scheduling;statistics	AI	28.965612552747274	-16.16903258284643	114793
6b9df5b92515e18ba31b3b4a8cd92cdedb595b66	a lossless (2, 8)-chaos-based secret image sharing scheme	image reconstruction gray scale three dimensional displays color cryptography visualization;color images lossless 2 8 chaos based secret image sharing scheme chaos based image encryption encrypted shares secret image reconstruction image share binary images grayscale images;image reconstruction chaos cryptography image coding image colour analysis;image encryption secret image sharing chaotic system	This paper introduces a new (2, 8)-secret image sharing scheme integrating the chaos-based image encryption with secret image sharing. It divides the secret image into 8 encrypted shares. Combining any two or more shares is able to completely reconstruct the secret image without any distortion. Each image share is only one pixel larger than the secret image in row and column directions. The proposed scheme is able to directly process the secret images with various formats such as the binary, grayscale, and color images. Experimental and comparison results demonstrate the excellent performance of the proposed scheme.	distortion;encryption;grayscale;high-level programming language;lossless compression;pixel	Long Bao;Yicong Zhou;C. L. Philip Chen	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974422	computer vision;feature detection;color image;binary image;u-matrix;theoretical computer science;digital image processing;internet privacy;digital image	Vision	38.960864540707995	-10.01763971004467	114890
c5907b932eeee9e077b65de999189c2cb74f8dbb	genetic programming of a microcontrolled water bath plant	modelizacion;genetic program;control algorithm;control difusa;programme commande;sintesis control;funcion no lineal;programacion automatica;linear genetic programming;ingenierie connaissances;fuzzy control;logique floue;non linear function;logica difusa;intelligence artificielle;automatic programming;algoritmo genetico;higher order;linear functionals;program optimization;fuzzy logic;optimal control;modelisation;assembly;performance programme;programacion lineal;control program;microregisseur;synthese commande;neuro fuzzy;fonction non lineaire;linear programming;mathematical model;algorithme genetique;programmation lineaire;control system design;programa mando;artificial intelligence;genetic algorithm;eficacia programa;montage;optimisation programme;program performance;inteligencia artificial;microcontrolador;reseau neuronal;montaje;modeling;red neuronal;control synthesis;programmation automatique;microcontroller;commande floue;neural network;optimizacion programa;knowledge engineering	Typically, control system design leads to a higher-order non-linear function of the system's state variables. As a result, it is very hard to find a satisfactory mathematical solution. On the other hand, considering a microcontroller based implementation, another difficulty is to program it to carry out the desired control algorithm. This paper presents the application of linear genetic programming in the automatic synthesis of a microcontroller assembly program, which performs an optimized control of a water bath plant. The synthesis starts from the plant's mathematical modeling and supplies directly a assembly code for the microcontroller platform. When comparing the control performance of the synthesized program with that of a neuro-fuzzy based controller, the synthesized program proved to perform slightingly better.	genetic programming	Douglas Mota Dias;Marco Aurélio Cavalcanti Pacheco;José Franco Machado do Amaral	2006		10.1007/11893011_40	fuzzy logic;microcontroller;systems modeling;genetic algorithm;higher-order logic;optimal control;computer science;linear programming;artificial intelligence;neuro-fuzzy;program optimization;knowledge engineering;mathematical model;assembly;artificial neural network;algorithm	Robotics	32.055061499588824	-10.668251901164412	114906
987b35c529cca6baac5c0edb9f9a12c7cf3289e8	a robust video watermarking based on feature regions and crowdsourcing		Video watermarking technique aims at resolving insecurity problems. Recently, many approaches have been proposed in order to satisfy the new constraints of video applications such as robustness to collusion attacks, high level of security and signature invisibility. In this paper, a new video watermarking approach based on feature regions is proposed. The originality of this approach is to use crowdsourcing technique in order to detect feature regions. First, video summary is generated. This summary is then used to detect the first type of feature regions based on crowdsourcing technique. On the other hand, mosaic is generated from original video to detect the second type of feature region browsed by the moving objects. Finally, the signature is embedded into the mosaic generated after merging these two types of feature regions using multi-frequential watermarking scheme. Experimental results have shown a high level of invisibility thanks to the efficient choice of the embedded target. Moreover, the proposed approach is robust against several attacks especially to collusion attacks.	crowdsourcing;digital video;digital watermarking;elegant degradation;embedded system;high-level programming language;ncsa mosaic;user interface;zero suppression	Asma Kerbiche;Saoussen Ben Jabra;Ezzeddine Zagrouba;Vincent Charvillat	2018	Multimedia Tools and Applications	10.1007/s11042-018-5888-6	collusion;computer vision;robustness (computer science);computer science;pattern recognition;merge (version control);digital watermarking;artificial intelligence;originality;crowdsourcing;invisibility	EDA	38.214479572498064	-13.078496024981378	114971
0b9eeb4e05d88f82b1793cf78d7830bd3d706f54	gradient estimation for applied monte carlo analyses	performance function;simulation;linear response;sensitivities;random variable;gradient estimate;gradients;system analysis;monte carlo;monte carlo simulation;approximations	For many practical applications of system analysis by Monte Carlo simulation, an important piece of information is the sensitivity of the outcome to input information. One matter of interest is the sensitivity of the outcome of the variables controlling it, that is the gradients with respect to the (usually implicit) performance function. Typically, analytical first (and higher) derivatives are not available and the usual numerical approach requires major computational effort. The present paper proposes estimation of gradients using a simplified (linear) response surface, which is a function of the random variables, and which is constructed after the Monte Carlo simulation has been performed. It uses all, or a subset of, the simulation run outcomes. No extra simulation runs are required. There are few restrictions on the form of the performance function and the gradients are estimated at small computational cost. The present paper is restricted to normal random variables.	gradient;monte carlo method	Robert E. Melchers;Mukshed Ahammed	2002	Rel. Eng. & Sys. Safety	10.1016/S0951-8320(02)00172-2	direct simulation monte carlo;quantum monte carlo;monte carlo method in statistical physics;quasi-monte carlo method;econometrics;mathematical optimization;diffusion monte carlo;dynamic monte carlo method;hybrid monte carlo;markov chain monte carlo;computer science;monte carlo molecular modeling;mathematics;kinetic monte carlo;rejection sampling;monte carlo integration;statistics;monte carlo method	ML	31.350588087107376	-16.496186511784295	114990
01cc00cfc4ef2e1f1752372310823ab4ba534156	testing estimator's credibility - part ii: other tests*	dentistry;statistical testing mean square error methods;performance evaluation;mse bias credibility testing filters;mse bias credibility testing;automatic testing;filters;statistical test;state estimation;covariance matrix estimation error state estimation dentistry automatic testing terminology parameter estimation filters nasa;estimation;mean square error methods;terminology;statistical testing;estimation error;parameter estimation;statistical test performance evaluation estimation credibility;credibility;nasa;covariance matrix	For pt.I see ibid., p.Z001330-7 (2006). Many estimators and filters provide assessments of their own estimation error. Are these self-assessments trustable? What is the degree to which they are trustable? This is Part II of a two-part series that provides answers to some of these questions, referred to as the credibility of the estimators. It proposes several tests for credibility and a test-based solution to the problem of comparing estimators' credibility. Numerical examples are provided to illustrate the utility and effectiveness of the proposed tests		X. Rong Li;Zhanlue Zhao	2006	2006 9th International Conference on Information Fusion	10.1109/ICIF.2006.301740	econometrics;computer science;data mining;statistics	Robotics	27.295811331158784	-19.388985030322864	115146
c6107643af0a532f9ea7e47c8480fb56c4a096ac	remaining useful life estimation for systems with time-varying mean and variance of degradation processes	degradation data;reliability;remaining useful life;fluctuation;gyro	This paper focuses on the problem of how to estimate remaining useful life (RUL) for a class of systems with high fluctuating dagradation caused by time-varying mean and variance. In engineering practice, the fluctuation of the degradation data can reflect the system stability, and hence it can serve as an additional indicator for the system’s health state in addition to the degradation observations. To estimate the RUL for this class of systems, three issues should be considered jointly: (i) how to model the degradation data with high fluctuation, (ii) how to define an indicator denoting the fluctuation’s level, and (iii) how to combine the degradation path and the fluctuation for the RUL estimating task. In responses to the above issues, this paper characterizes the degree of the fluctuation by the standard deviation of the stochastic degradation process and defines the standard deviation as an additional performance variable. Moreover, a stochastic degradation model is proposed to reflect the high fluctuating degradation data, in which the standard deviation is time dependent. To obtain the marginal distribution of the RUL derived by the standard deviation, the failure threshold of the standard deviation is given according to its influence on the degradation process. Another marginal distribution derived by the degradation data is also obtained by the time distribution of the degradation process crossing the degradation threshold. Then, through deriving the correlation between the two marginal distributions based on the probability theory, the joint distribution of the RUL is obtained. Finally, a practical case study for gyro is conducted. The results demonstrate the feasibility and applicability of the proposed model. Copyright © 2014 John Wiley & Sons, Ltd.	elegant degradation;gyro;john d. wiley;marginal model;quantum fluctuation;stochastic process	Jian-Xun Zhang;Chang-Hua Hu;Xiaosheng Si;Zhi-Jie Zhou;Dang-Bo Du	2014	Quality and Reliability Eng. Int.	10.1002/qre.1705	econometrics;engineering;operations management;reliability;mathematics;statistics	ML	30.13823616258768	-18.69675556586967	115315
3d2e01f835327fb6d404e36919d1c5cc99850a9e	analysis of agricultural production in asia and measurement of technical efficiency using copula-based stochastic frontier quantile model		The purpose of this paper is to evaluate the efficiency of agricultural production in Asia and analyze the production function of Asian countries. Methodologically, we employ the stochastic frontier model with the concern about dependency between two-sided error term and one-sided inefficiency. Likewise, we try to improve the performance of the standard stochastic frontier model by applying quantile regression to the frontier production function. Therefore, this paper introduces the model called Copula-based stochastic frontier quantile model as an alternative tool for this issue. The accuracy of this model is proved through a simulation study before applying to the agricultural production data of Asia.		Varith Pipitpojanakarn;Paravee Maneejuk;Woraphon Yamaka;Songsak Sriboonchitta	2016		10.1007/978-3-319-49046-5_59	copula (linguistics);econometrics;quantile;inefficiency;quantile regression;agricultural productivity;production function;frontier	Robotics	26.202761030506554	-21.146877552468023	115395
fdac44c9bc830fea57f9ea49caac32235a9780dc	particle swarm optimization approaches to coevolve strategies for the iterated prisoner's dilemma	swarm intelligence;jeu somme non nulle;evolutionary computation;game theory;neural nets;juego suma no nula;non zero sum game;particle swarm optimization pso;binary pso particle swarm optimization coevolve strategies iterated prisoner s dilemma coevolutionary training technique nonzero sum problem neural network function approximator;coevolution;approximation fonction;iterative methods;iterated prisoner s dilemma ipd;iterated prisoner s dilemma;particle swarm optimizer;function approximation;optimizacion enjambre particula;optimisation essaim particule;algorithme evolutionniste;algoritmo evolucionista;neural networks nns;evolutionary algorithm;reseau neuronal;particle swarm optimization pso coevolution iterated prisoner s dilemma ipd neural networks nns;particle swarm optimisation;coevolucion;red neuronal;game theory particle swarm optimisation iterative methods evolutionary computation neural nets function approximation;particle swarm optimization neural networks working environment noise game theory computer science mathematical model environmental economics mathematics information technology africa;neural network	This paper presents and investigates the application of coevolutionary training techniques based on particle swarm optimization (PSO) to evolve playing strategies for the nonzero sum problem of the iterated prisoner's dilemma (IPD). Three different coevolutionary PSO techniques are used, differing in the way that IPD strategies are presented: A neural network (NN) approach in which the NN is used to predict the next action, a binary PSO approach in which the particle represents a complete playing strategy, and finally, a novel approach that exploits the symmetrical structure of man-made strategies. The last technique uses a PSO algorithm as a function approximator to evolve a function that characterizes the dynamics of the IPD. These different PSO approaches are compared experimentally with one another, and with popular man-made strategies. The performance of these approaches is evaluated in both clean and noisy environments. Results indicate that NNs cooperate well, but may develop weak strategies that can cause catastrophic collapses. The binary PSO technique does not have the same deficiency, instead resulting in an overall state of equilibrium in which some strategies are allowed to exploit the population, but never dominate. The symmetry approach is not as successful as the binary PSO approach in maintaining cooperation in both noisy and noiseless environments-exhibiting selfish behavior against the benchmark strategies and depriving them of receiving almost any payoff. Overall, the PSO techniques are successful at generating a variety of strategies for use in the IPD, duplicating and improving on existing evolutionary IPD population observations.	32-bit;64-bit computing;algorithm;angular defect;artificial neural network;benchmark (computing);channel (communications);experiment;game theory;interaction;interpupillary distance;iteration;machine learning;mathematical optimization;particle swarm optimization;phase-shift oscillator;population;prisoner's dilemma;weak value	Nelis Franken;Andries Petrus Engelbrecht	2005	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2005.856202	mathematical optimization;function approximation;coevolution;computer science;artificial intelligence;machine learning;mathematics;iterative method;zero-sum game;artificial neural network	AI	25.527482285172795	-11.161361851159976	115517
b51439ef1329c2efad1aa6f8e53d67f66fdf474b	forward search outlier detection in data envelopment analysis	production function;linear regression;outlier detection;data envelopment analysis dea;super efficiency;data envelope analysis;forward search	In this paper we tackle the problem of outlier detection in data envelopment analysis (DEA). We propose a procedure where we merge the super-efficiency DEA and the forward search. Since DEA provides efficiency scores which are not parameters to fit the model to the data, we introduce a distance, to be monitored along the search. This distance is obtained through the integration of a regression model and the super-efficiency DEA. We simulate a Cobb–Douglas production function and we compare the super-efficiency DEA and the forward search analysis in both uncontaminated and contaminated settings. For inference about outliers, we exploit envelopes obtained through Monte Carlo simulations.	anomaly detection;data envelopment analysis	Tiziano Bellini	2012	European Journal of Operational Research	10.1016/j.ejor.2011.07.023	econometrics;anomaly detection;computer science;linear regression;data mining;production function;data envelopment analysis;mathematics;statistics	ML	28.5005719335326	-23.734287498393016	116182
04585dacdfd66f28921b6817901fdf7f8a37cee9	comparison of linear shrinkage estimators of a large covariance matrix in normal and non-normal distributions	normal distribution;double shrinkage;non normal distribution;shrinkage;risk function;large sample;high dimension;linear shrinkage estimator;covariance matrix	The problem of estimating the large covariance matrix of both normal and nonnormal distributions is addressed. In convex combinations of the sample covariance matrix and the identity matrix multiplied by a scalor statistic, we suggest a new estimator of the optimal weight based on exact or approximately unbiased estimators of the numerator and denominator of the optimal weight in non-normal cases. It is also demonstrated that the estimators given in the literature have secondorder biases. It is numerically shown that the proposed estimator has a good risk performance.	numerical analysis	Yuki Ikeda;Tatsuya Kubokawa;Muni S. Srivastava	2016	Computational Statistics & Data Analysis	10.1016/j.csda.2015.09.011	normal distribution;scatter matrix;estimation of covariance matrices;matrix t-distribution;shrinkage estimator;econometrics;covariance matrix;mathematical optimization;multivariate normal distribution;covariance;mathematics;shrinkage;statistics;covariance function	ML	30.90068465193639	-23.364478547519134	116335
a8b96d2d47718f404ecd1ebd523ae414980158e0	constrained optimization of black-box stochastic systems using a novel feasibility enhanced kriging-based method		Abstract Stochastically constrained simulation optimization problems are challenging because the inherent noise terms to a black-box system lead to the need of considering the uncertainty at both the objective and the constraint function. To address this problem, we propose a Kriging-based optimization framework, which uses stochastic Kriging models to approximate the objective and the constraint functions and an adaptive sampling approach to sequentially search for the next point that is promising to have a better objective. The main contribution of this work is to incorporate a feasibility-enhanced term to the infill sampling criterion, which improves the Kriging-based algorithmu0027s capabilities of returning a truly feasible near-optimal solution for stochastic systems. The efficacy of the Kriging-based algorithm is demonstrated with eight benchmark problems and a case study from the pharmaceutical manufacturing domain.	black box;constrained optimization;kriging;mathematical optimization;stochastic process	Zilong Wang;Marianthi Ierapetritou	2018	Computers & Chemical Engineering	10.1016/j.compchemeng.2018.07.016	black box (phreaking);mathematical optimization;sampling (statistics);mathematics;constrained optimization;adaptive sampling;kriging;optimization problem	EDA	28.680118052044477	-14.233568448942384	116361
1a1098ce654c5a956e90ae092b7fbed1c38cd239	mixed models for longitudinal left-censored repeated measures	linear mixed model;hiv infection;longitudinal study;corresponding author;repeated measures;sas proc nlmixed;human immunodeficiency virus;standard error;viral load;mixed model;left censoring;detection limit	Longitudinal studies could be complicated by left-censored repeated measures. For example, in Human Immunodeficiency Virus infection, there is a detection limit of the assay used to quantify the plasma viral load. Simple imputation of the limit of the detection or of half of this limit for left-censored measures biases estimations and their standard errors. In this paper, we review two likelihood-based methods proposed to handle left-censoring of the outcome in linear mixed model. We show how to fit these models using SAS Proc NLMIXED and we compare this tool with other programs. Indications and limitations of the programs are discussed and an example in the field of HIV infection is shown.	autoregressive model;bivariate data;brownian motion;censor;clinical trial censoring;gaussian process;geo-imputation;hiv infections;information;mind;missing data;mixed model;naivety;normal statistical distribution;normality unit;numerical analysis;numerical integration;plasma active;random effects model;sas;viral load measurement;slope	Rodolphe Thiébaut;Hélène Jacqmin-Gadda	2004	Computer methods and programs in biomedicine	10.1016/j.cmpb.2003.08.004	mixed model;econometrics;medicine;virology;mathematics;statistics	NLP	30.729232015371977	-21.608496605530657	116441
b09985949304bb1f0af23b13decef3171198fd27	a scalable signature scheme for video authentication	secret sharing;streaming video;video authentication;authentication signature;signature scheme;digital signature;digital video	This paper addresses the problem of ensuring the integrity of a digital video and presents a scalable signature scheme for video authentication based on cryptographic secret sharing. The proposed method detects spatial cropping and temporal jittering in a video, yet is robust against frame dropping in the streaming video scenario. In our scheme, the authentication signature is compact and independent of the size of the video. Given a video, we identify the key frames based on differential energy between the frames. Considering video frames as shares, we compute the corresponding secret at three hierarchical levels. The master secret is used as digital signature to authenticate the video. The proposed signature scheme is scalable to three hierarchical levels of signature computation based on the needs of different scenarios. We provide extensive experimental results to show the utility of our technique in three different scenarios—streaming video, video identification and face tampering.	algorithm;authentication;computation;cryptography;diffie–hellman key exchange;digital signature;digital video;error detection and correction;forward error correction;frame (video);key frame;malware;scalability;secret sharing;streaming media;video processing	Pradeep K. Atrey;Wei-Qi Yan;Mohan S. Kankanhalli	2006	Multimedia Tools and Applications	10.1007/s11042-006-0074-7	digital signature;computer science;video tracking;smacker video;internet privacy;secret sharing;blind signature;motion compensation;world wide web;computer security	Security	38.65073568739048	-12.903577999308657	116530
ca87fe9dcb91cc4b3ab391df311c664fe3571668	goodness-of-fit testing in growth curve models: a general approach based on finite differences	goodness of fit;courbe croissance;parametric model;metodo estadistico;growth curve;test statistique;62p20;test ajustement;theorie approximation;finance;methode parametrique;estimacion densidad;ecologia;analisis datos;accroissement population;economic sciences;biometrie;metodo parametrico;estimation densite;test estadistico;62e17;parametric method;simulation;estimation non parametrique;biometrics;biometria;statistical test;ecologie;62g07;econometria;simulacion;statistical method;finite difference;biology;biologia;medical science;dinamica poblacion;demografia;distribucion estadistica;ecology;modele parametrique;curva crecimiento;approximation theory;non parametric estimation;density estimation;ciencias economicas;data analysis;population dynamic;distribution statistique;sciences actuarielles;ciencia medica;poblacion crecimiento;methode statistique;growth curve goodness of fit finite difference;statistical computation;growth curve model;calculo estadistico;goodness of fit test;population dynamics;growth mechanism;analyse donnee;calcul statistique;population growth;62p05;sciences economiques;econometrics;growth pattern;estimacion no parametrica;prueba ajuste;estimation statistique;dynamique population;estimacion estadistica;demography;statistical estimation;statistical distribution;62p10;econometrie;biologie;finanzas;growth model;demographie;science medicale	Growth curve models are routinely used in various fields such as biology, ecology, demography, population dynamics, finance, econometrics, etc. to study the growth pattern of different populations and the variables linked with them. Many different kinds of growth patterns have been used in the literature to model the different types of realistic growth mechanisms. It is generally a matter of substantial benefit to the data analyst to have a reasonable idea of the nature of the growth pattern under study. As a result, goodness-of-fit tests for standard growth models are often of considerable practical value. In this paper we develop some natural goodness-of-fit tests based on finite differences of the size variables under consideration. The method is general in that it is not limited to specific parametric forms underlying the hypothesized model so long as an appropriate finite difference of some function of the size variables can be made to vanish. In addition it allows the testing process to be carried out under a set up which manages to relax most of the assumptions made by Bhattacharya et al. (2009); these assumptions are generally reasonable but not guaranteed to hold universally. Thus our proposed method has a very wide scope of application. The performance of the theory developed is illustrated numerically through several sets of real data and through simulations.	finite difference	A. Mandal;W. T. Huang;S. K. Bhandari;A. Basu	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2010.09.003	econometrics;calculus;mathematics;population dynamics;goodness of fit;statistics	ML	33.52091397463445	-22.17865874398768	116659
dd5ec69fa19fa976f753f1a89f201509513f6d78	multilevel monte carlo simulation of the eddy current problem with random parameters	eddy currents electromagnetism monte carlo methods;convergence;uncertainty;wires;monte carlo methods wires convergence computational efficiency steel electron tubes uncertainty;steel;computational efficiency;electron tubes;multilevel monte carlo simulation eddy current problem electromagnetism field spatial resolution;monte carlo methods	The multilevel Monte Carlo method is applied to an academic example in the field of electromagnetism. The method exhibits a reduced variance by assigning the samples to multiple models with a varying spatial resolution. For the given example it is found that the main costs of the method are spent on the coarsest level.	monte carlo method;simulation	Armin Galetzka;Zeger Bontinck;Ulrich Römer;Sebastian Schöps	2017	2017 International Applied Computational Electromagnetics Society Symposium - Italy (ACES)	10.23919/ROPACES.2017.7916062	direct simulation monte carlo;statistical physics;quantum monte carlo;monte carlo method in statistical physics;quasi-monte carlo method;econometrics;mathematical optimization;dynamic monte carlo method;hybrid monte carlo;markov chain monte carlo;monte carlo molecular modeling;kinetic monte carlo;parallel tempering;monte carlo integration;physics;monte carlo method;monte carlo method for photon transport	Theory	32.98421616975348	-15.683954826688549	116737
e9a5125f5debf2fee4242e635e69831636367caa	random grid-based visual secret sharing with multiple decryptions	threshold;multiple decryptions;progressive visual secret sharing;pixel expansion;visual secretsharing;random grids;visual cryptographic scheme;lossless recovery	Visual cryptographic scheme (VCS) with multiple decryptions has the features of both OR-based VCS (OVCS) and XOR-based VCS (XVCS), which can be applied in wider applications. In this paper, a threshold VCS with the abilities of OR and XOR decryptions is proposed based on random grids (RG). If a copy machine is not available, the secret could be reconstructed by stacking. On the other hand, if a device having XOR operation is available, the visual quality of the recovered secret image is improved as well as the secret can be reconstructed losslessly for (k,n)(k,n) threshold (case) when n shares are collected. Experiments are conducted to evaluate the security and efficiency of the proposed scheme.	secret sharing	Xuehu Yan;Shen Yun Wang;Xiamu Niu;Ching-Nung Yang	2015	J. Visual Communication and Image Representation	10.1016/j.jvcir.2014.11.003	arithmetic;theoretical computer science;mathematics;proactive secret sharing;computer security	Vision	38.8363472296024	-10.463483844880127	116753
fa4708e4a995cc5498be6bc352565fde86a2ffb6	methods of sampling based on exhaustive and evolutionary search		The development of mathematical software for training sampling is considered. Exhaustive and evolutionary sampling methods are developed. Criteria for selection, censoring, and pseudoclustering of instances are introduce in these methods. This makes it possible to speed up the sampling process and to ensure the compliance of the samples with the limited size. The proposed methods allow for the automatic allocation of a subset of instances with the minimal size from the original sample. The subset contains the most important instances for the model’s construction. The complexity estimates of the developed methods are defined. Experiments to determine the practical applicability of the methods are conducted. The use of the proposed estimates and identified dependences makes it possible to take into account the available computer resources during the sampling.	censoring (statistics);experiment;mathematical software;sampling (signal processing)	Sergei A. Subbotin	2013	Automatic Control and Computer Sciences	10.3103/S0146411613030073	mathematical optimization;data mining;mathematics;statistics	AI	29.690135263758133	-15.687507506223133	116921
a036307c46ab8795047ffbe7e18ca79cd99edef7	analysis of filtering and smoothing algorithms for lévy-driven stochastic volatility models	experimental design;filtering;theorie filtrage;model specification;filtrage;analisis numerico;covariance analysis;metodo monte carlo;stochastic process;theorie approximation;filtro kalman;65c05;analisis datos;nonparametric estimation;stochastic method;05bxx;fonction repartition;62m20;62e17;filtrado;filtre kalman;estimation variance;estimation non parametrique;modele lineaire;variance analysis;espace etat;plan experiencia;levy process;methode monte carlo;stochastic volatility model;kalman filter;non linear model;modele non lineaire;modelo lineal;distribucion estadistica;statistical regression;analyse numerique;algorithme;approximation theory;algorithm;non parametric estimation;funcion distribucion;article letter to editor;data analysis;distribution function;modelo no lineal;smoothing methods;numerical analysis;62k99;prediction theory;analyse covariance;distribution statistique;plan experience;state space method;marginal distribution;specification modele;methode espace etat;especificacion modelo;particle filter;smoothing;analisis variancia;regresion estadistica;monte carlo experiment;methode lissage;monte carlo method;state space;62j10;statistical computation;linear model;calculo estadistico;stochastic volatility;processus stochastique;alisamiento;ley marginal;methode stochastique;realized variance;analyse donnee;calcul statistique;60g35;variance estimation;estimacion no parametrica;stochastic model;analisis covariancia;estimation statistique;proceso estocastico;theorie prediction;volatilite stochastique;espacio estado;state space model;regression statistique;estimacion estadistica;60e05;statistical estimation;modelo estocastico;algorithm design;statistical distribution;lissage;modele stochastique;filtering theory;loi marginale;analyse variance;metodo espacio estado;variance;variancia;algoritmo;metodo estocastico	Filtering and smoothing algorithms that estimate the integrated variance in Lévy-driven stochastic volatility models are analyzed. Particle filters are algorithms designed for nonlinear, nonGaussian models while the Kalman filter remains the best linear predictor if the model is linear but non-Gaussian. Monte Carlo experiments are performed to compare these algorithms across different specifications of the model including different marginal distributions and degrees of persistance for the instantaneous variance. The use of realized variance as an observed variable in the state space model is also evaluated. Finally, the particle filter’s ability to identify the timing and size of jumps is assessed relative to popular nonparametric estimators.	algorithm;experiment;kalman filter;kerrison predictor;marginal model;monte carlo method;nonlinear system;particle filter;smoothing;state-space representation;volatility	Drew D. Creal	2008	Computational Statistics & Data Analysis	10.1016/j.csda.2007.07.009	filter;kalman filter;probability distribution;marginal distribution;stochastic process;algorithm design;econometrics;realized variance;analysis of variance;particle filter;analysis of covariance;numerical analysis;state space;stochastic modelling;state-space representation;distribution function;calculus;linear model;lévy process;mathematics;variance;stochastic volatility;data analysis;design of experiments;specification;regression analysis;statistics;smoothing;monte carlo method;approximation theory	ML	33.427952781956826	-22.32104426522637	117116
10fa2db158a7bbd3a763efe9cc2d5a7188d852a6	a data-driven approach for cooperative wind farm control	wind turbines;bayes methods;wind turbines bayes methods gaussian processes optimal control optimisation power generation control wind power plants;ba algorithm data driven cooperative control strategy wind farm power production maximization greedy control strategy upstream wind turbine downstream wind turbine optimum coordinated control actions bayesian ascent probabilistic optimization method gaussian process regression trust region;wind turbines wind farms optimization production bayes methods servomotors modeling;production;servomotors;optimization;modeling;wind farms	This paper discusses a data-driven, cooperative control strategy to maximize wind farm power production. Conventionally, every wind turbine in a wind farm is operated to maximize its own power production without taking into account the interactions among the wind turbines in a wind farm. Such greedy control strategy, when an upstream wind turbine attempts to maximize its power production, can significantly lower the power productions of the downstream wind turbines and, thus, reduces the overall wind farm power production. As an alternative, we propose a cooperative wind farm control strategy that determines and executes the optimum coordinated control actions that maximize the total wind farm power production. To determine the optimum coordinated control actions of the wind turbines, we employ Bayesian Ascent (BA), a probabilistic optimization method constructed based on Gaussian Process regression and the trust region concept. Wind tunnel experiments using 6 scaled wind turbine models are conducted to assess (1) the effectiveness of the cooperative control strategy in improving the power production, and (2) the efficiency of the BA algorithm in determining the optimum control actions of the wind turbines using only the input control actions and the output power measurement data.	backward chaining;business architecture;consensus dynamics;control theory;downstream (software development);exact algorithm;experiment;greedy algorithm;interaction;kriging;mathematical optimization;resultant;trust region	Jinkyoo Park;Soon-Duck Kwon;Kincho H. Law	2016	2016 American Control Conference (ACC)	10.1109/ACC.2016.7524967	wind power;control engineering;systems modeling;engineering;control theory;servomotor	Robotics	31.768646824647988	-10.85540685542274	117303
11bb5a312939fc54f5932007350686063803c41a	on the estimators of model-based and maximal reliability	engineering;approximation asymptotique;metodo estadistico;approximation normale;analyse multivariable;bad specification;cambio variable;analisis componente principal;62f12;fiabilidad;reliability;theoretical model;analyse survie;analisis factorial;theorie approximation;multivariate analysis;intervalo confianza;maximum likelihood;fonction repartition;62p30;modele mal specifie;industrie;60e99;62e20 secondary;62f25;misspecification model;primary;62e17;simulation;estimation non parametrique;maximum vraisemblance;analisis correspondencia;industria;62g20;simulacion;statistical method;asymptotic expansion;distribucion estadistica;asymptotic behavior;comportement asymptotique;prueba duracion;developpement asymptotique;ingenierie;modele theorique;life test;approximation theory;secondary 62h25;non parametric estimation;comportamiento asintotico;funcion distribucion;confidence interval;distribution function;industry;analyse factorielle;factor model;distribution statistique;desarrollo asintotico;62g15;factor analysis;methode statistique;principal component analysis;fiabilite;modele factoriel;secondary;correspondence analysis;intervalle confiance;changement variable;survival analysis;62h25;62e10;estimacion parametro;analyse composante principale;analyse correspondance;62h25 maximal reliability factor analysis asymptotic expansion model misspecification nonnormal data;analisis multivariable;ingenieria;mauvaise specification;nonnormal data;asymptotic distribution;asymptotic approximation;maximal reliability;estimacion no parametrica;parameter estimation;estimation parametre;estimation statistique;cumulant;primary 62e20;60k10;62n05;estimacion estadistica;60e05;statistical estimation;statistical distribution;normal approximation;maxima verosimilitud;60k20;variable transformation;essai endurance;modelo teorico;aproximacion asintotica;expansion;model misspecification	Four estimators of the reliability for a composite score based on the factor analysis model and five estimators of the maximal reliability for the composite are presented. When the Wishart maximum likelihood is used for the estimation of the model parameters, it is shown that the five estimators of maximal reliability are the same. Asymptotic cumulants of the estimators and their logarithmic transformations are derived under arbitrary distributions with possible model misspecification. The theoretical results considering model misspecification when a model does not hold are shown to be closer to their simulated values than those neglecting model misspecification. Simulations of the confidence intervals using the normal approximation based on the asymptotically distribution-free theory and the asymptotic expansion by Hall's method with variable transformation are performed.	maximal set	Haruhiko Ogasawara	2009	J. Multivariate Analysis	10.1016/j.jmva.2008.11.002	econometrics;asymptotic analysis;calculus;mathematics;factor analysis;statistics	HPC	32.85129087634025	-22.322771813922504	117625
a1bf24760fb747891c2b05b07268970cea3e6250	a cramér-von mises test-based distribution-free control chart for joint monitoring of location and scale		Abstract This paper proposes a new distribution-free control chart by integrating the powerful nonparametric two-sample Cramer-von Mises test and the exponentially weighted moving average control scheme to on-line monitoring. The proposed control chart can be used to monitor the location and the scale parameters of a univariate continuous distribution, simultaneously. The control limits based on Monte-Carlo simulation are provided in a table. The sensitivity analysis of effect of the number of reference samples on the control chart is studied in detail. Comparison results based on Monte-Carlo simulation show that the proposed chart is quite robust to non-normally distributed data, and moreover, it shows satisfactory performance in detecting various process shifts in terms of the average run length and standard deviation of run length. The application of our proposed chart is illustrated by a real data example for automobile engine piston rings.		Jiujun Zhang;Erjie Li;Zhonghua Li	2017	Computers & Industrial Engineering	10.1016/j.cie.2017.06.027	operations management;control limits;moving average;standard deviation;x-bar chart;statistics;statistical process control;control chart;ewma chart;chart;mathematics	SE	28.486699069607894	-19.835409101239758	117735
2e8620eb400cbda7e6185c546c83a641bf860a92	efficient estimation and robust inference of linear regression models in the presence of heteroscedastic errors and high leverage points	null rejection rate;62j05;size distortion;adaptive heteroscedasticity consistent interval estimator;estimated weighted least squares;adaptive estimator;heteroscedasticity consistent interval estimator;62j86	Efficient Estimation and Robust Inference of Linear Regression Models in the Presence of Heteroscedastic Errors and High Leverage Points Muhammad Aslam a b , Tahira Riaz b & Saima Altaf c a Insitut de Mathematiques de Bourgogne , Dijon , France b Department of Statistics , Bahauddin Zakariya University , Multan , Pakistan c Department of Statistics , PMAS Arid Agriculture University , Rawalpindi , Pakistan Accepted author version posted online: 15 Mar 2013.Published online: 20 Mar 2013.	twelve leverage points	Aslam Muhammad;Tahira Riaz;Saima Altaf	2013	Communications in Statistics - Simulation and Computation	10.1080/03610918.2012.695847	econometrics;mathematical optimization;estimator;mathematics;heteroscedasticity;park test;invariant estimator;statistics	ML	28.476795351184258	-22.768038405581667	117776
67f56f04f5fe60e7de12940e9b35f15386a1890c	an r function to non-parametric and piecewise analysis of competing risks survival data	survival data;genie biomedical;cumulative incidence;biomedical engineering;survival analysis;piecewise estimation;parametric analysis;ingenieria biomedica;competing risks;r software	Competing risks are frequently encountered in the analysis of survival data. Different methods of analysis can be used in estimated and comparing cumulative incidence functions provided enough information is available as to the times and causes of failure. Algorithms written in R have been developed to handle this type of data. In this paper we propose an R add-on function to the cmprsk package. This function is specifically oriented towards the non-parametric analysis of competing risk data and which provides analyses of three commonly used methods all in one program. We illustrate the use of this function with two examples in oncology and compare the estimates with those provided on the cmprsk and survival package.		Thomas Filleron;Agnès Laplanche;Jean-Marie Boher;Andrew Kramar	2010	Computer methods and programs in biomedicine	10.1016/j.cmpb.2010.02.004	econometrics;computer science;data mining;cumulative incidence;mathematics;survival analysis;parametric statistics;statistics	SE	29.9832750679882	-20.459768257490307	117893
24f05bae8878d7e474eb2266a41b29349f89226c	enabling high-dimensional surrogate-assisted optimization by using sliding windows		A major drawback of surrogate-assisted evolutionary algorithms is their limited ability to perform in high-dimensional scenarios. This paper describes a possible meta-algorithm scheme for the application of surrogate models to high-dimensional optimization problems. The main assumption of the proposed method is that for some of these expensive problems the nonlinear interactions between variables are sparse. If these interactions can be represented as a band matrix, they can be exploited by applying low-dimensional heuristic solvers in a sliding window fashion to the high-dimensional problem. A special type of composite test function is presented and the proposed meta-algorithm is compared against standard evolutionary algorithms.	distribution (mathematics);evolutionary algorithm;heuristic;heuristic (computer science);interaction;large eddy simulation;mathematical optimization;metaheuristic;microsoft windows;nonlinear system;relevance;sparse matrix;surrogate model	Bernhard Werth;Erik Pitzer;Michael Affenzeller	2017		10.1145/3067695.3082536	sliding window protocol;artificial intelligence;mathematical optimization;band matrix;computer science;machine learning;evolutionary algorithm;nonlinear system;heuristic;test functions for optimization;composite number;optimization problem	AI	28.268076805785682	-10.733937832302916	118030
2fbe8d306b52be48b12de66e96713be132310ac7	why cannot we have a strongly consistent family of skew normal (and higher order) distributions		In many practical situations, the only information that we have about the probability distribution is its first few moments. Since many statistical techniques requires us to select a single distribution, it is therefore desirable to select, out of all possible distributions with these moments, a single “most representative” one. When we know the first two moments, a natural idea is to select a normal distribution. This selection is strongly consistent in the sense that if a random variable is a sum of several independent ones, then selecting normal distribution for all of the terms in the sum leads to a similar normal distribution for the sum. In situations when we know three moments, there is also a widely used selection – of the so-called skewnormal distribution. However, this selection is not strongly consistent in the above sense. In this paper, we show that this absence of strong consistency is not a fault of a specific selection but a general feature of the problem: for third and higher order moments, no strongly consistent selection is possible. 1 Formulation of the Problem Need to select a distribution based on the first few moments. In many practical situations, we only have a partial information about the probability distribution. For example, often, all we know is the values of the first few moments. Most probabilistic and statistical techniques assume that we know the exact form of a probability distribution; see, e.g., [5]. In situations when we only have partial information about the probability distribution, there are many probability distribuThongchai Dumrongpokaphan Department of Mathematics, Faculty of Science, Chiang Mai University, Thailand, e-mail: tcd43@hotmail.com Vladik Kreinovich Department of Computer Science, University of Texas at El Paso, 500 W. University, El Paso, Texas 79968, USA, e-mail: vladik@utep.edu	computer science;email;strong consistency;vladik kreinovich	Thongchai Dumrongpokaphan;Vladik Kreinovich	2017		10.1007/978-3-319-50742-2_4	statistics;strong consistency;probability density function;characteristic function (probability theory);probability distribution;fourier transform;skew;normal distribution;random variable;mathematics	ML	32.43708441293135	-17.98613272150572	118072
5440381a54b3bd33d9510b4ea30917fd8140cb13	improvements and extensions to simulation interval procedures	statistical analysis discrete event simulation spreadsheet programs;spreadsheet programs;spreadsheet functions simulation interval procedures confidence interval procedures two stage student t intervals paired t intervals conservative extension finite alternatives approximate procedure elementary cases revised notation common methods discrete event simulations;confidence interval;statistical analysis;technical report;industrial engineering cities and towns discrete event simulation computational modeling statistics;discrete event simulation	The paper presents improvements to two common confidence interval procedures: the two stage Student's t intervals and the paired t intervals. Further, we provide a conservative extension to the paired t intervals to deal with any finite number of alternatives. Also, we include an approximate procedure that performs better with large numbers of alternatives. These procedures have been presented previously (F.J. Matejcik and B.L. Nelson, 1993), but special elementary cases, revised notation, and comparison to common methods should make these procedures accessible to a broader audience. Numerical examples are included.	approximation algorithm;numerical method;simulation	Frank J. Matejcik	1995		10.1145/224401.224639	econometrics;simulation;confidence interval;computer science;technical report;discrete event simulation;algorithm;statistics	AI	29.075360103292915	-15.659986864676805	118246
330697430353e4117d66da14cbf93354418dfb60	a global scheme for controlling the mean and standard deviation of a process based on the optimal design of gauges			optimal design	Jaime Mosquera;Francisco Aparisi;Eugenio K. Epprecht	2018	Quality and Reliability Eng. Int.	10.1002/qre.2282	statistics;optimal design;econometrics;engineering;standard deviation	EDA	28.711316687148063	-20.51681455927234	118251
18376f5036bf1b1ca58459fc2be85d9fd4a6e8ee	on a class of non-regenerative sampling distributions	non-regenerative sampling distributions;coalescent process;star-shaped case;multiple collision;ewens sampling formula;sampling distribution;sampling formula;kingman case;two-parameter family	Combinatorics, Probability and Computing / Volume 16 / Issue 03 / May 2007, pp 435 444 DOI: 10.1017/S0963548306008212, Published online: 03 November 2006 Link to this article: http://journals.cambridge.org/abstract_S0963548306008212 How to cite this article: MARTIN MÖHLE (2007). On a Class of Non-Regenerative Sampling Distributions. Combinatorics, Probability and Computing, 16, pp 435-444 doi:10.1017/S0963548306008212 Request Permissions : Click here	combinatorics, probability and computing;gibbs sampling;here document	Martin Möhle	2007	Combinatorics, Probability & Computing	10.1017/S0963548306008212	sampling;sampling design;bernoulli sampling;slice sampling;statistical parameter;cluster sampling;stratified sampling;poisson sampling	Vision	28.079748571991516	-22.434198052432695	118470
a941d8abc202f0bf4484d2706113eeccc8c72987	a model-based framework for black-box problem comparison using gaussian processes		An important challenge in black-box optimization is to be able to understand the relative performance of different algorithms on problem instances. This has motivated research in exploratory landscape analysis and algorithm selection, leading to a number of frameworks for analysis. However, these procedures often involve significant assumptions, or rely on information not typically available. In this paper we propose a new, model-based framework for the characterization of black-box optimization problems using Gaussian Process regression. The framework allows problem instances to be compared in a relatively simple way. The model-based approach also allows us to assess the goodness of fit and Gaussian Processes lead to an efficient means of model comparison. The implementation of the framework is described and validated on several test sets.	gaussian process	Sobia Saleem;Marcus Gallagher;Ian Wood	2018		10.1007/978-3-319-99259-4_23	black box (phreaking);artificial intelligence;mathematical optimization;machine learning;goodness of fit;computer science;algorithm selection;gaussian process;kriging;optimization problem	NLP	26.490803683703174	-15.045131694657774	118498
d480a8ec8156929c2386aecf069846d01e3b9e7e	reducing computational complexity of estimating multivariate histogram-based probabilistic model	histogram distribution model computational complexity multivariate histogram complex probabilistic graphical model estimation of distribution algorithms gaussian probabilistic model;evolutionary computation;gaussian processes;probabilistic graphical model;probabilistic model;estimation of distribution algorithm;computational complexity;distributed models;computational complexity evolutionary computation;gaussian processes computational complexity evolutionary computation	In continuous domain, how to efficiently learn the complex probabilistic graphical model is a bottleneck problem for estimation of distribution algorithms (EDAs). The predominant researches focus on Gaussian probabilistic model instead of histogram distribution model because of its comparative superiority in the computational complexity. In this paper, however, we find that using the histogram model does not necessarily bring into exponential computational complexity. Based on the fact many bins are zero-height, we propose a novel method that can learn the multivariate- dependency histogram based probabilistic graphical model with acceptable polynomial computational complexity. Several strategies previously used in the HEDA are combined into the new algorithm to improve the convergence and diversity. Experiments showed the superior performance of the new algorithm on several continuous problems compared with UMDAc IDEA-G and sur-shr-HEDA.	computational complexity theory;estimation of distribution algorithm;graphical model;polynomial;statistical model;time complexity	Nan Ding;Ji Xu;Shude Zhou;Zengqi Sun	2007	2007 IEEE Congress on Evolutionary Computation	10.1109/CEC.2007.4424461	computational indistinguishability;statistical model;mathematical optimization;probabilistic analysis of algorithms;average-case complexity;decision tree model;estimation of distribution algorithm;probabilistic relevance model;computer science;artificial intelligence;theoretical computer science;machine learning;computational resource;gaussian process;mathematics;computational complexity theory;asymptotic computational complexity;statistics;evolutionary computation;divergence-from-randomness model	ML	28.420267670052166	-11.322582691162935	119794
7a160de26bd8c83273394cce0dd2a7ac66a80a33	a general theory of bibliometric and other cumulative advantage processes	cumulant	"""A Cumulative Advantage Distribution is proposed which models statistically the situation in which success breeds success. I t differs from the Negative Binomial Die tribution in that lack of success, being a non-event, is not punished by increased chance of failure. I t is shown that such a stochastic law is governed by the Beta Function, containing only one free parameter, and this is approximated by a skew or hyperbolic distribution of the type that is widespread in bibliometrics and diverse social science phenomena. In particular, this is shown to 0 Introduction It is common in bibliometric matters and in many diverse social phenomena, that success seems to breed success. A paper which has been cited many times is more likely to be cited again than one which has been little cited. An author of many papers is more likely to publish again than one who has been less prolific. A journal which has been frequently consulted for some purpose is more likely to be turned to again than one of previously infrequent use. Words become common or remain rare. A millionaire gets extra income faster and easier than a beggar. In statistics, such a process is commonly described by a skew or hyperbolic distribution function of the type that has been characterized by Simon (I) and correctly shown by him to be given by the Beta Function (this is not the """" Beta Density """" which is discussed by Feller (2) 11:49 and includes a dependent variable) rather than the T h i s researckwas funded by NSF grant SOC73-05428. be an appropriate underlying probabilistic theory for the Bradford Law, the Lotka Law, the Pareto and Zipf Distributions , and for all the empirical results of citation frequency analysis. As side results one may derive also the obsolescence factor for literature use. The Beta Function is peculiarly elegant for these manifold purposes because it yields both the actual and the cumulative distributions in simple form, and contains a limiting case of an inverse square law to which many empirical distributions conform. more commonly used """" contagious """" distributions, for example the negative binomial (3. 4 3 or its limiting form, Fisher's logarithmic series distribution. Although the relation between such distributions, the stochastic processes which lead to them and the Urn models from which they may be derived, seem well known and go back more than 50 years to Yule's …"""	approximation algorithm;bibliometrics;bradford's law;fitts's law;frequency analysis;ibm notes;information cascade;lotka's law;pareto efficiency;stochastic process;zipf's law	Derek de Solla Price	1976	JASIS	10.1002/asi.4630270505	empirical distribution function;zipf's law;econometrics;computer science;mathematics;statistics;cumulant	ML	34.927646401727294	-18.533679873938596	120293
f06872761291acf0d9c2213b7a13daccd9b97556	on the validity of the batch quantile method for markov chains	modelizacion;approximation asymptotique;cuantila;chaine markov;cadena markov;intervalo confianza;batch production;simulation output analysis;procede discontinu;quantile estimation;modelisation;confidence interval;produccion por lote;steady state simulation;intervalle confiance;production par lot;batch process;batch means;procedimiento discontinuo;regime permanent;quantile;asymptotic approximation;regimen permanente;modeling;aproximacion asintotica;steady state;markov chain	The estimation of quantiles of the steady-state distribution of a (general state-space) Markov chain (MC) from observations (e.g., simulated) of the corresponding MC is an important problem. For example, this estimation is required when constructing prediction intervals for a response variable W = g(Q) and Markov chain Monte Carlo has been applied to obtain information on the posterior distribution p(Q|x) of Q (where x represents a vector of data). Recent results show that a batch quantile methodology (similar to the batch means method) can be applied to obtain confidence intervals that are asymptotically valid under the assumption that the MC is exponentially ergodic. In this symposium we are interested in theoretical and application results on the validity of the batch quantile method for Markov chains, empirical assessments on the assumptions that are necessary for the validity of the batch quantile method as well as successful applications of the batch quantile method in any areas are especially welcome.	ergodicity;markov chain monte carlo;monte carlo method;state space;steady state	David F. Muñoz	2010	Oper. Res. Lett.	10.1016/j.orl.2010.01.001	econometrics;markov chain;mathematical optimization;quantile;systems modeling;confidence interval;mathematics;steady state;statistics;batch processing	ML	32.1722066204866	-20.23576550446672	120426
4467e1fe36cea28f33cc38ab0e25fdb78f9e6553	a space-time filter for panel data models containing random effects	serial correlation;panel data;analisis numerico;chaine markov;cadena markov;metodo monte carlo;theorie approximation;65c05;analisis datos;correlation serielle;stochastic method;random effects;62e17;methode monte carlo;space time;distribucion estadistica;donnee panel;espacio tiempo;65c40;analyse numerique;approximation theory;data analysis;correlation spatiale;numerical analysis;efecto aleatorio;estimation erreur;dependance du temps;spatial correlation;correlacion espacial;time dependence;distribution statistique;markov chain monte carlo methods;random effect;error estimation;monte carlo experiment;monte carlo method;space dependence;statistical computation;estimacion error;calculo estadistico;mcmc estimation;mcmc estimations;methode stochastique;analyse donnee;calcul statistique;spatial error correlation;60j10;estimation statistique;effet aleatoire;spatial error correlation serial correlation mcmc estimations;estimacion estadistica;statistical estimation;dependencia del tiempo;statistical distribution;espace temps;panel data model;markov chain;dependance espace;metodo estocastico	A space–time filter structure is introduced that can be used to accommodate dependence across space and time in the error components of panel data models that contain random effects. This specification provides insights regarding several space–time structures that have been used recently in the panel data literature. Markov Chain Monte Carlo methods are set forth for estimating the model which allow simple treatment of initial period observations as endogenous or exogenous. The performance of the approach is demonstrated using both Monte Carlo experiments and an applied illustration. © 2010 Elsevier B.V. All rights reserved.	data model;experiment;markov chain monte carlo;monte carlo method;panel data;random effects model	Olivier Parent;James P. LeSage	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2010.05.016	econometrics;markov chain monte carlo;calculus;mathematics;statistics;random effects model	AI	33.43899989351791	-22.18591032360305	121048
9df2f8cfdc3402fc850387e65040948d8d0d1b0d	quasi-negative binomial distribution: properties and applications	lagrange expansion;akaike information criterion;zero inflation;limit distribution;negative binomial distribution;heavy tail;probability distribution;tail behavior;tail property;poisson distribution;lagrange expansion zero inflation tail property limiting distribution;limiting distribution	In this paper, a quasi-negative binomial distribution (QNBD) derived from the class of generalized Lagrangian probability distributions is studied. The negative binomial distribution is a special case of QNBD. Some properties of QNBD, including the upper tail behavior and limiting distributions, are investigated. It is shown that the moments do not exist in some situations and the limiting distribution of QNBD is the generalized Poisson distribution under certain conditions. A zero-inflated QNBD is also defined. Applications of QNBD and zero-inflated QNBD in various fields are presented and compared with some other existing distributions including Poisson, generalized Poisson and negative binomial distributions aswell as their zero-inflated versions. In general, theQNBDor its zero-inflated version performs better than the other models based on the chi-square statistic and the Akaike Information Criterion, especially for the cases where the data are highly skewed, have heavy tails or excessive numbers of zeros. © 2011 Elsevier B.V. All rights reserved.	akaike information criterion;tails	Shubiao Li;Fang Yang;Felix Famoye;Carl Lee;Dennis Black	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2011.02.003	probability distribution;beta-binomial distribution;econometrics;mathematical optimization;poisson binomial distribution;logarithmic distribution;exponential family;akaike information criterion;beta negative binomial distribution;compound poisson distribution;heavy-tailed distribution;univariate distribution;binomial distribution;index of dispersion;zero-inflated model;mathematics;variance-gamma distribution;compound probability distribution;poisson distribution;asymptotic distribution;negative binomial distribution;infinite divisibility;multinomial distribution;statistics;continuity correction;distribution fitting;negative multinomial distribution	AI	32.66020721858208	-19.863725540173125	121180
5307394be6996501725dfd54c074584c2ca0578c	on a fast, robust estimator of the mode: comparisons to other robust estimators with applications	ley pareto;estimacion sesgada;iterative method;central tendency measure;finite sample;estimator robustness;punto ruptura;robust estimator;analisis datos;fonction repartition;interaction;location estimation;estimacion m;lognormal distribution;62e17;echantillon fini;error sistematico;high energy;outlier;loi lognormale;robust measure of location;mode estimation;calculo automatico;curva gauss;mediane;median;robust estimation;computing;point rupture;queue distribution;loi pareto;metodo iterativo;calcul automatique;observacion aberrante;proton proton;funcion distribucion;data analysis;distribution function;ley lognormal;robustez estimador;numerical analysis;cola distribucion;bias;62h30;breakdown point;methode iterative;pareto distribution;statistical computation;calculo estadistico;loi normale;estimation m;observation aberrante;analyse donnee;rejection point;calcul statistique;interaccion;mediana;mesure tendance centrale;robust mode estimator;60e05;m estimation;biased estimation;estimation biaisee;gaussian distribution;distribution tail;erreur systematique;robustesse estimateur	Advances in computing power enable more widespread use of the mode, which is a natural measure of central tendency since it is not influenced by the tails in the distribution. The properties of the half-sample mode, which is a simple and fast estimator of the mode of a continuous distribution, are studied. The half-sample mode is less sensitive to outliers than most other estimators of location, including many other low-bias estimators of the mode. Its breakdown point is one half, equal to that of the median. However, because of its finite rejection point, the half-sample mode is much less sensitive to outliers that are all either greater or less than the other values of the sample. This is confirmed by applying the mode estimator and the median to samples drawn from normal, lognormal, and Pareto distributions contaminated by outliers. It is also shown that the half-sample mode, in combination with a robust scale estimator, is a highly robust starting point for iterative robust location estimators such as Huber's M-estimator. The half-sample mode can easily be generalized to modal intervals containing more or less than half of the sample. An application of such an estimator to the finding of collision points in high-energy proton-proton interactions is presented.		David R. Bickel;Rudolf Frühwirth	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2005.07.011	minimax estimator;robust statistics;econometrics;computing;outlier;interaction;estimator;numerical analysis;trimmed estimator;pareto distribution;efficiency;distribution function;bias;calculus;m-estimator;mathematics;iterative method;data analysis;median;invariant estimator;statistics	ML	33.53354275014777	-22.84749062397531	121258
a2647135d47d03fc93ad18fa2ea62f179ed2aca2	estimating the number of undetected software errors via the correlated capture-recapture model	software;reliability;correlation multiple;modele capture recapture;analisis estadistico;procedimiento aval;modelo captura recaptura;statistical independence;inspections;software systems;downstream processing;sistema complejo;development process;inspection;procede aval;numerical analysis;statistical analysis;systeme complexe;complex system;analyse statistique;controle qualite;multiple correlation;capture recapture model;fiabilite logiciel;inspeccion;fiabilidad logicial;correlation matrix;software design;quality control;capture recapture;software reliability;software inspection;correlacion multiple;control calidad	Sometimes a complex software system fails because of errors undiscovered in the design stage of the development process. Detecting these errors early in the process would eliminate many downstream problems. The so-called ‘‘capture–recapture’’ model, initially used by biologists to estimate the size of wildlife populations, has been widely used to estimate the number of software design errors. However, one simplifying assumption in capture–recapture models is that the inspections performed by various inspectors are statistically independent from each other. In the paper, we propose a novel method that is based on the correlation matrix of multiple inspectors. In a numerical analysis, we show that our method outperforms other traditional models that are based on the independence assumption. 2005 Elsevier B.V. All rights reserved.	downstream (software development);mark and recapture;numerical analysis;population;software design;software system	Young H. Chun	2006	European Journal of Operational Research	10.1016/j.ejor.2005.06.023	econometrics;complex systems;inspection;computer science;mathematics;statistics	AI	29.61170162979394	-18.158183087568922	121358
3d905d37e1d94773699b9644d1342f0ab32d34a3	zero-inflated poisson regression mixture model	zero inflation;finite mixture model;em algorithm;heterogeneity;poisson	Excess zeros and overdispersion are commonly encountered phenomena that limit the use of traditional Poisson regression models for modeling count data. The focus of this paper is on modeling count data in the case that a population has excess zero counts and also consists of several sub-populations in the non-zero counts. The proposed zero-inflated Poisson regression mixture model accounts for both excess zeros and heterogeneity. The performance of parameter estimation for the proposed model is evaluated through simulation studies.	count data;estimation theory;mixture model;offset binary;poisson regression;population;simulation	Hwa Kyung Lim;Wai Keung Li;Philip L. H. Yu	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.06.021	econometrics;mathematical optimization;quasi-likelihood;expectation–maximization algorithm;heterogeneity;generalized linear model;mixture model;mathematics;poisson regression;poisson distribution;overdispersion;statistics	ML	30.920131710093642	-22.872014322523935	121530
e1491ab361e5fb9eaa65f6e50261da6f1679c55c	large-scale experimental evaluation of gpu strategies for evolutionary machine learning	evolutionary machine learning;large scale data mining;eprints newcastle university;graphics processing units;open access;dr jaume bacardit	We exhaustively evaluate two GPU strategies for evolutionary machine learning systems.We use synthetic datasets to thoroughly explore the space of problem characteristics.The findings of the evaluation on synthetic datasets translate to real-world problems.Our findings help avoiding a blind trial-and-error calibration of GPU data mining code. Graphics Processing Units (GPUs) are effective tools for improving the efficiency of many computationally demanding algorithms. GPUs have been particularly effective at speeding up the evaluation stage of evolutionary machine learning systems. The speedups obtained in these tasks, depend on many factors: dataset characteristics, the parallel strategy of the GPU code and the fit of the GPU code within the rest of the learning system. A solid understanding of all these factors is required to choose and adjust the most suitable GPU strategy in different scenarios. In this paper we present a large-scale performance evaluation of two GPU strategies for speeding up the evaluation of evolutionary machine learning systems. We use highly-tuneable synthetic problems to exhaustively explore the space of problem characteristics and determine the type of problems where each strategy performs best. The lessons learnt from this extensive evaluation are further confirmed by running experiments on a broad range of real-world datasets. Through this thorough evaluation we obtain a solid understanding of the capabilities and limitations of the evaluated GPU strategies for boosting the efficiency of evolutionary machine learning systems.	graphics processing unit;machine learning	María A. Franco;Jaume Bacardit	2016	Inf. Sci.	10.1016/j.ins.2015.10.025	simulation;computer science;data science;machine learning	AI	26.501078801556012	-13.164063298394657	121722
0880036c89dedf768b4ee9a9dd99e26f1c498840	monte carlo extension of quasi-monte carlo	randomized quasi monte carlo;simulation;monte carlo method;random processes;quasi monte carlo;quasi monte carlo method;technical report;simulation monte carlo methods random processes sampling methods;sampling methods;error estimate;higher dimensional problems monte carlo extension monte carlo techniques quasi monte carlo techniques randomized quasi monte carlo methods error estimation scrambled nets latin supercube sampling;monte carlo methods sampling methods analysis of variance computational modeling art statistics error analysis measurement standards hypercubes input variables;monte carlo methods;monte carlo technique	This paper surveys recent research on using Monte Carlo techniques to improve quasi-Monte Carlo techniques. Randomized quasi-Monte Carlo methods provide a basis for error estimation. They have, in the special case of scrambled nets, also been observed to improve accuracy. Finally through Latin supercube sampling it is possible to use Monte Carlo methods to extend quasi-Monte Carlo methods to higher dimensional problems.	monte carlo method;quasi-monte carlo method;randomized algorithm;sampling (signal processing)	Art B. Owen	1998		10.1109/WSC.1998.745036	direct simulation monte carlo;monte carlo method in statistical physics;stochastic process;quasi-monte carlo method;econometrics;dynamic monte carlo method;hybrid monte carlo;particle filter;markov chain monte carlo;monte carlo molecular modeling;mathematics;stochastic tunneling;kinetic monte carlo;rejection sampling;parallel tempering;monte carlo integration;statistics;monte carlo method;control variates	Robotics	32.30590307577623	-16.211183912920344	121902
8fb796c36fe98d4036731053a91aff5acbdccf11	"""corrigendum to """"the spatial sign covariance matrix with unknown location"""" [j. multivariate analysis 130 (2014) 107-117]"""				Alexander Dürre;Daniel Vogel;David E. Tyler	2015	J. Multivariate Analysis	10.1016/j.jmva.2014.10.004	econometrics;mathematical optimization;statistics	Vision	31.201561413017675	-23.738394911922036	122002
ff4f9321134f4849d72b861be57c0143fe53373c	application-oriented estimator selection	standards;measurement;training;signalbehandling;maximum likelihood estimation;vectors;signal processing;training end performance metric estimation experiment design;maximum likelihood estimation bayes methods design of experiments identification;measurement training maximum likelihood estimation vectors modeling standards;application oriented estimator selection linear regression bayesian estimator ml standard maximum likelihood estimation system identification literature optimal experiment design;modeling	Designing the optimal experiment for the recovery of an unknown system with respect to the end performance metric of interest is a recently established practice in the system identification literature. This practice leads to superior end performance to designing the experiment with respect to some generic metric quantifying the distance of the estimated model from the true one. This is usually done by choosing and fixing the estimation method to either a standard maximum likelihood (ML) or a Bayesian estimator. In this paper, we pose the intuitive question: Can we design better estimators than the usual ones with respect to an end performance metric of interest? Based on a simple linear regression example we affirmatively answer this question.	optimal design;system identification	Dimitrios Katselis;Cristian R. Rojas	2015	IEEE Signal Processing Letters	10.1109/LSP.2014.2363464	minimax estimator;econometrics;systems modeling;bayes estimator;maximum a posteriori estimation;signal processing;pattern recognition;mathematics;restricted maximum likelihood;maximum likelihood;maximum likelihood sequence estimation;estimation theory;measurement;statistics	ML	27.467688756648258	-23.948546889921992	122049
92b21b35370d18045054c36bc83d11d458577978	analysis of correlation of 2dpalmhash code and orientation range suitable for transposition	2dpalmhash code;journal;transposition operation;orientation range;correlation analysis	Recently two-dimensional (2D) PalmHash Code, whose original features were extracted by 2D Gabor filter, was proposed as a cancelable palmprint coding scheme for secure palmprint verification. However, vertical correlation inherently found in 2DPalmHash Code is vulnerable to statistical analysis attack. It was empirically attested that the vertical correlation of 2DPalmHash Code was suppressed by transposing Gabor feature matrices that were tuned to vertical orientation. Unfortunately, the mechanism behind why it works remains open. In this paper, we first systematically analyze and reveal the horizontal and vertical correlations of 2DPalmHash Code. We then uncover the secret behind the transposition of Gabor feature matrices for vertical correlation suppression. Finally, the orientation range of Gabor feature matrices suitable for transposition is determined from the correlation analysis and verified by a series of experiments. & 2013 Elsevier B.V. All rights reserved.	experiment;fingerprint;gabor filter;zero suppression	Lu Leng;Andrew Beng-Jin Teoh;Ming Li;Muhammad Khurram Khan	2014	Neurocomputing	10.1016/j.neucom.2013.10.005	arithmetic;theoretical computer science;mathematics;algorithm	AI	36.10452133953947	-10.382073531450708	122126
2842d55988974b5a76ae71852ef1b4a4378e95e7	an automatic adaptive importance sampling algorithm for molecular dynamics in reaction coordinates	metastability;molecular dynamics;girsanov;non equilibrium sampling;adaptive importance sampling;metadynamics;variance reduction	In this article we propose an adaptive importance sampling scheme for dynamical quantities of high dimensional complex systems which are metastable. The main idea of this article is to combine a method coming from Molecular Dynamics Simulation, Metadynamics, with a theorem from stochastic analysis, Girsanov’s theorem. The proposed algorithm has two advantages compared to a standard estimator of dynamic quantities: firstly, it is possible to produce estimators with a lower variance and, secondly, we can speed up the sampling. One of the main problems for building importance sampling schemes for metastable systems is to find the metastable region in order to manipulate the potential accordingly. Our method circumvents this problem by using an assimilated version of the Metadynamics algorithm and thus creates a nonequilibrium dynamics which is used to sample the equilibrium quantities. ∗quer@zib.de †ldonati@zedat.fu-berlin.de ‡bettina.keller@fu-berlin.de §weber@zib.de	action potential;approximation algorithm;complex systems;cyclic redundancy check;german research centre for artificial intelligence;gradient descent;image scaling;importance sampling;mathematical optimization;metadynamics;metastability in electronics;microsoft outlook for mac;molecular dynamics;monte carlo method;numerical analysis;optimization problem;sampling (signal processing);simulation;variance reduction	J. Quer;Luca Donati;B. G. Keller;Michael Weber	2018	SIAM J. Scientific Computing	10.1137/17M1124772	mathematical optimization;computational chemistry;mathematics;statistics;metadynamics	ML	33.60087109119407	-15.62165060230397	122198
3346408c1d4dceb4ff95c8c5de550ba501ed4968	inference in gompertz-type nonhomogeneous stochastic systems by means of discrete sampling	stochastic process;transition probability;stochastic system;house prices;diffusion process;discrete sampling	The study of stochastic systems by using Markovian processes has become of great interest to investigators in many disciplines (biology, physics, demography, economics, cybernetics, etc.). Among these processes, diffusions have been widely considered and its study has covered several areas such the inference (especially the estimation of the parameters of the drift and the diffusion coefficient) and first passage times through varying boundaries. Diffusions can be introduced from stochastic differential equations, this being the natural way when the process is observed continuously. An extensive review of this theory can be found in the literature (Prakasa Rao 1999) and related recent work has been done by Kloeden et al. (1996) and Singer (2002), among others. On the other hand, the statistical	coefficient;cybernetics;gompertz function;stochastic process	Ramón Gutiérrez Jáimez;Ramón Gutiérrez-Sánchez;Ahmed Nafidi;Patricia Román-Román;Fabian Torres	2005	Cybernetics and Systems	10.1080/01969720590897233	stochastic process;econometrics;markov chain;mathematical optimization;continuous-time stochastic process;stochastic modelling;diffusion process;first-hitting-time model;local time;stable process;discrete-time stochastic process;mathematics;statistics	AI	32.47342481774821	-18.550393466210952	122402
16fd5e4cbcaf6bad1e849667e57d294a7b3652e9	authentication of mpeg-4-based surveillance video	compatibility authentication mpeg 4 based surveillance video compressed digital video video stream integrity embedded digital signatures;watermarking;authentication surveillance streaming media cameras video compression mpeg 4 standard network servers web server digital signatures video recording;video streaming;data integrity;data compression;surveillance;digital signatures;code standards;data encapsulation;video coding;digital signature;digital video;data encapsulation digital signatures watermarking surveillance video streaming video coding code standards data compression data integrity	The industry is currently starting to use MPEG-4 compressed digital video for surveillance applications. The transition from analog to digital video raises difficulties for using surveillance video in court. Since it is fairly easy to make hard-to-detect modifications to the video stream captured by the cameras, e.g. mask out a specific event or person, a system for proving authenticity and integrity of video streams is needed. This paper presents such a system based on digital signatures embedded in the video stream. Our concept provides a means for proving authenticity and integrity of MPEG-4 digital video streams, while leaving compatibility with standard media players untouched.	analog-to-digital converter;authentication;closed-circuit television;data compression;digital signature;digital video;embedded system;furreal friends;streaming media;type signature	Michael Pramateftakis;Tobias Oelbaum;Klaus Diepold	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1418683	video compression picture types;microsoft video 1;digital signature;h.263;uncompressed video;computer science;video capture;video tracking;multimedia;video processing;smacker video;internet privacy;world wide web;multiview video coding	EDA	37.64514078560986	-12.802069624254	122439
81aa8c7c06645d87cee62aaee14f78cbc4dc31b0	a generalized modified weibull distribution for lifetime modeling	engineering;medida informacion;theorie statistique;statistical moment;metodo estadistico;fonction vraisemblance;fiabilidad;reliability;analyse survie;order statistic;theorie approximation;funcion tasa;estimacion densidad;analisis datos;funcion azar;maximum likelihood;porcentaje falla;fonction repartition;62p30;industrie;estimation densite;mesure information;statistique ordre;moment statistique;62e17;estimation non parametrique;maximum vraisemblance;industria;matrice information;tiempo vida;62g32;taux defaillance;statistical method;ley weibull;distribucion estadistica;prueba duracion;funcion verosimilitud;fonction hasard;queue distribution;ingenierie;life test;approximation theory;non parametric estimation;weibull distribution;density estimation;funcion distribucion;momento estadistico;data analysis;rate function;distribution function;fonction taux;industry;60g70;lifetime;distribution statistique;cola distribucion;extreme value;information measure;methode statistique;fiabilite;hazard rate;statistical computation;valeur extreme;calculo estadistico;survival analysis;estadistica orden;estimacion parametro;fonction generalisee;fonction repartition empirique;failure rate;generalized function;analyse donnee;ingenieria;calcul statistique;62g30;estimacion no parametrica;parameter estimation;teoria estadistica;estimation parametre;theorie information;information matrix;estimation statistique;duree vie;hazard function;60k10;62n05;estimacion estadistica;funcion generalizada;60e05;statistical estimation;loi weibull;statistical distribution;likelihood function;62b10;maxima verosimilitud;60k20;distribution tail;valor extremo;information theory;essai endurance;statistical theory;teoria informacion	A four parameter generalization of the Weibull distribution capable of modeling a bathtub-shaped hazard rate function is defined and studied. The beauty and importance of this distribution lies in its ability to model monotone as well as non-monotone failure rates, which are quite common in lifetime problems and reliability. The new distribution has a number of well-known lifetime special sub-models, such as the Weibull, extreme value, exponentiated Weibull, generalized Rayleigh and modified Weibull distributions, among others. We derive two infinite sum representations for its moments. The density of the order statistics is obtained. The method of maximum likelihood is used for estimating the model parameters. Also, the observed information matrix is obtained. Two applications are presented to illustrate the proposed distribution.	bathtub curve;formation matrix;maxima and minima;observed information;rayleigh–ritz method;monotone	Jalmar M. F. Carrasco;Edwin M. M. Ortega;Gauss M. Cordeiro	2008	Computational Statistics & Data Analysis	10.1016/j.csda.2008.08.023	weibull distribution;econometrics;generalized extreme value distribution;weibull modulus;exponentiated weibull distribution;information theory;weibull fading;calculus;failure rate;mathematics;statistics;generalized beta distribution	Metrics	32.88050494257877	-21.91268665168996	122501
ec4053087e0874b425564b54251c0bbd95ae8952	content-based digital signature for motion pictures authentication and content-fragile watermarking	recognition of manipulations;watermarking;data integrity;image processing;colored noise;motion pictures;content based digital signature;interactive video;authentication;video authentication;digital signatures;multimedia data security and watermarking;copyright;integrity violation;multimedia systems;image integrity and authenticity for multimedia applications with content based digital signature;copyright protection;digital signatures motion pictures authentication watermarking copyright protection colored noise multimedia systems image processing internet public key;image authentication;public key;internet;image processing multimedia systems security of data copyright data integrity interactive video;digital signature;multimedia data;motion picture authentication;multimedia services;security of data;content based signature technique;content fragile watermarking;integrity violation content based digital signature motion picture authentication content fragile watermarking multimedia services image authentication video authentication copyright protection content based signature technique interactive video	The development of new multimedia services and environments requires new concepts both to support the new working process and to protect the multimedia data during the production and distribution. This article addresses imagehideo authentication and copyright protection as major security demands in digital marketplaces. First we present a content-based signature technique for image and video authenticity and integrity. Based on this technique, we introduce a tool for interactive video authentication and propose contentfragile watermarking, a concept which combines watermarking and content-based digital signatures to ensure copyright protection and detection of integrity violation.	antivirus software;authentication;digital signature;digital watermarking	Jana Dittmann;Arnd Steinmetz;Ralf Steinmetz	1999		10.1109/MMCS.1999.778274	digital signature;digital watermarking alliance;image processing;digital watermarking;computer science;multimedia;internet privacy;world wide web	Security	37.691377324408734	-12.5962585173227	122753
61b4d3afae41b6a08c1d05221f60402644b3082d	the robustness of the three-way chart to non normality	engineering;metodo estadistico;estimator robustness;aplicacion;symmetric function;normal distribution;fonction repartition;62p30;variance component;industrie;carte controle;data stream;funcion simetrica;simulacion numerica;queue lourde;industria;fonction symetrique;traitement par lot;estimacion promedio;three way control chart;non normality;statistical method;heavy tail;curva gauss;distribucion estadistica;procesamiento por lote;variance components 62l20;ingenierie;moyenne;funcion distribucion;control chart;distribution function;industry;average run length;robustez estimador;symmetric law;distribution statistique;methode statistique;62l20;promedio;simulation numerique;carta control;batch process;62e10;loi normale;curso agua;average;ingenieria;coefficient of skewness;coefficient beta 1 pearson;mean estimation;cours eau;skewness;variance components;cola pesada;estimation moyenne;batch processing;application;60e05;ley simetrica;statistical distribution;gaussian distribution;non normalite;49j22;stream;numerical simulation;robustesse estimateur;loi symetrique	In batch processing, the Three-Way control chart has been offered for controlling the mean of a process when the batch-to-batch variation is much greater than the within-batch variation. These two sources of variation are typically monitored along with usual batch sample means. Although the Three-Way chart was originally developed for normally distributed process data, its robustness to violations of the normality assumption is the central theme of this study. For data streams with heavy tails or displaying skewness, the in-control average run lengths (ARLs) for the Three-Way chart are seen to be significantly shorter than expected. On the other hand, out-of-control ARLs are much longer than the normal theory benchmarks for symmetric non-normal distributions. The Three-Way chart is not robust to moderate or strong skewness.		Ananda Bandulasiri;Stephen M. Scariano	2009	Communications in Statistics - Simulation and Computation	10.1080/03610910802521183	normal distribution;computer simulation;ewma chart;econometrics;calculus;mathematics;statistics;batch processing	ML	33.33806431836384	-21.739560808003684	122791
8934fdc53c375a7322fdda0d8839371f85666926	dinamica - a landscape dynamics simulation software	spatial dynamic transition probabilities dinamica spatial simulation model landscape dynamics multi scale vicinity based transitional functions spatial pattern evolution landscape evolution scenarios multiple time step stochastic simulation dynamic spatial transition probabilities cartographic neighborhood spatial feedback stochastic multi step simulation engineering logistic regression weights of evidence amazonian frontier colonization region;transition probability;ecology digital simulation geophysics computing;ecology;logistic regression;weight of evidence;simulation software;spatial pattern;geophysics computing;spatial dynamic;landscape dynamic;simulation model;biological system modeling stochastic processes logistics probability predictive models spatial resolution feedback testing design engineering application software;digital simulation	This paper reports the development of a new spatial simulation model of landscape dynamics – DINAMICA, which presents: 1) multi-scale vicinity-based transitional functions, 2) incorporation of spatial feedback approach to a stochastic multi-step simulation engineering, and 3) the application of logistic regression or weights of evidence to calculate the spatial dynamic transition probabilities. Application of DINAMICA includes the prediction of a region's spatial pattern evolution according to pre-defined transition rates.	logistic regression;markov chain;simulation;spatiotemporal pattern	B. S. Soares-Filho;Gustavo C. Cerqueira;W. L. Araújo;Arnaldo de Albuquerque Araújo	2001		10.1109/SIBGRAPI.2001.963114	econometrics;simulation;geography;statistics	ML	25.17379205436227	-22.04251037091125	123109
c762685bdbd8c62fd10cde7f86bf635520b31035	building confidence in the reliability assessment of thermal-hydraulic passive systems	distribucion binomial;modelizacion;evaluation performance;system reliability;systeme passif;condition initiale;nuclear safety;sample size;fiabilite systeme;condiciones limites;metodo monte carlo;stochastic process;analisis estadistico;performance evaluation;bootstrap;intervalo confianza;monte carlo sampling;condition aux limites;thermal hydraulic passive system;confidence intervals;tamano muestra;evaluacion prestacion;echantillonnage;accident;prior information;methode monte carlo;taille echantillon;prior distribution;statistical method;probabilistic approach;ley a priori;loi binomiale;system performance;seguridad nuclear;fiabilidad sistema;sampling;modelisation;reliability assessment;surete nucleaire;systeme incertain;confidence interval;informacion a priori;condicion inicial;uncertainties;statistical analysis;boundary condition;enfoque probabilista;approche probabiliste;monte carlo method;intervalle confiance;rupture;analyse statistique;initial condition;defaillance;processus stochastique;autogeneration mutuelle;nuclear power plant;bootstrapping;centrale nucleaire;passive system;modele donnee;binomial distribution;accidente;failures;proceso estocastico;monte carlo;muestreo;sistema incierto;modeling;hydraulic drive;fallo;uncertain system;information a priori;ruptura;central nuclear;sistema pasivo;transmission hydraulique;loi a priori;transmision hidraulica;data models	Thermal-hydraulic (T-H) passive systems play a cruc ial role in the development of future solutions for nuclear power plant technologies. A fundamental issue still to be resolved is the quantification of the reliability of such systems. The difficulty comes from the uncertainties in the evaluation of their performance, because of the lack of experimental and operational data and of va lidated models of the phenomena involved. The uncertainties concern the deviations of the underly ing physical principles from the expected T-H behaviour, due to the onset of physical phenomena i nfringing the system performance or to changes in the initial/boundary conditions of system operat ion. In this work, some insights resulting from a survey on the technical issues associated with estimating the reliability of T-H passive systems i n the context of nuclear safety are first provided. It is concluded that the most realistic assessment of the passive system response to the uncertain accident conditions can be achieved by Monte Carlo (MC) sampling of the system uncertain parameters followed by the simulation of the accide nt volution by a detailed mechanistic T-H code. This procedure, however, requires considerabl nd often prohibitive computational efforts for achieving acceptable accuracies, so that a limi tation on the MC sample size, i.e. on the number of code runs, is necessarily forced onto the analys is. As a consequence, it becomes mandatory to provide quantitative measures of the uncertainty of the computed estimates. To this aim, two classes of statistical methods are proposed in the paper to quantify, in terms of confidence intervals, the uncertainties associated to the reliability estimates. The first method is based on the probability laws of the binomial distr ibution governing the stochastic process of system success or failure. The second method is fou nded on the concept of bootstrapping, suitable to assess the accuracy of estimators when no prior inf mation on their distributions is available. To the authors knowledge, it is the first time that these methods are applied to quantitatively	bootstrapping (compilers);computation;monte carlo method;onset (audio);sampling (signal processing);simulation;stochastic process	Enrico Zio;Nicola Pedroni	2009	Rel. Eng. & Sys. Safety	10.1016/j.ress.2008.03.006	stochastic process;econometrics;mathematics;statistics;monte carlo method	ML	32.34296256845838	-19.83507400107825	123208
aef412dc6445b3ec27b51ecbfbcaf1b5b9f7a0f8	a bayesian approach to modeling two-phase degradation using change-point regression	failure time distribution;degradation modeling;gibbs sampling;hierarchical bayesian modeling;change point regression	Influenced by defects or contaminants remaining after a series of manufacturing processes, the degradation paths of some products exhibit two-phase patterns over the testing period. This paper proposes a hierarchical Bayesian change-point regression model to fit the two-phase degradation patterns, and derives the failure-time distribution of a unit that is randomly selected from its population. A Gibbs sampling algorithm is developed for the inference of the parameters in the change-point degradation model, as well as for the prediction of the failure-time distribution of the randomly selected unit. The proposed approach is applied to the degradation paths of plasma display panels (PDPs) presenting the two-phase pattern.	elegant degradation;two-phase locking	Suk Joo Bae;Tao Yuan;Shuluo Ning;Way Kuo	2015	Rel. Eng. & Sys. Safety	10.1016/j.ress.2014.10.009	econometrics;gibbs sampling;computer science;machine learning;bayesian linear regression;bayesian hierarchical modeling;statistics	Robotics	29.85221690639789	-19.043408385563087	123582
55865ffc9a70e4017035f55e380dee4dcda9e902	recent advances and applications of the theory of stochastic convexity. application to complex bio-inspired and evolution models		The theory of stochastic convexity is widely recognised as a framework to analyze the stochastic behaviour of parameterized models by different notions in both univariate and multivariate settings. These properties have been applied in areas as diverse as engineering, biotechnology, and actuarial science. Consider a family of parameterized univariate or multivariate random variables {X(θ)|θ ∈ T} over a probability space (Ω,I,Pr), where the parameter θ usually represents some distribution moments. Regular, sample-path, and strong stochastic convexity notions have been defined to intuitively describe how the random objects X(θ) grow convexly (or concavely) concerning their parameters. These notions were extended to the multivariate case by means of directionally convex functions, yielding the concepts of stochastic directional convexity for multivariate random vectors and multivariate parameters. We aim to explain some of the basic concepts of stochastic convexity, to discuss how this theory has been used into the stochastic analysis, both theoretically and in practice, and to provide some of the recent and of the historically relevant literature on the topic. Finally, we describe some applications to computing/communication systems based on bio-inspired models.	british informatics olympiad;consistency model;convex function	Eva-María Ortega;Jose Alonso	2011			econometrics;mathematical optimization;mathematical economics	AI	34.91376698168551	-17.39166208502424	123721
cbc2334ad9e13c49bfc5705c782b12108772166a	effort associated with a class of random optimization methods	optimisation;algorithm performance;optimizacion;optimal method;optimisation aleatoire;algorithme;algorithm;extreme value;resultado algoritmo;performance algorithme;optimization;algoritmo	When differentiability is not assumed random procedures can be successfully used to estimate the extreme values of a given function. For a class of such algorithms we treat the problem of estimating the mean effort.		Chang C. Y. Dorea	1991	Math. Program.	10.1007/BF01594927	econometrics;mathematical optimization;extreme value theory;mathematics;algorithm	Theory	30.610400931216894	-11.596476830469335	123825
68361f5b44339689203f9ea1d556f3c86dee111b	an r package for value at risk and expected shortfall	value at risk;parametric distributions;expected shortfall;62e99	Value at risk and expected shortfall are the two most popular measures of financial risk. But the available R packages for their computation are limited. Here, we introduce an R contributed package written by the authors. It computes the two measures for over 100 parametric distributions, including all commonly known distributions. We expect that the R package could be useful to researchers and to the financial community.	expected shortfall;value at risk	Stephen Chan;Saralees Nadarajah;Emmanuel Afuecheta	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.944658	econometrics;actuarial science;expected shortfall;value at risk	ECom	26.89480550681244	-20.818107940445223	124176
2a2f59378401bff1bb1ea2a9373e13d7e876e161	visually do statistics for business persons: visual materials from regression to black-sholes model	black sholes model;regressive analysis;covariance;visualization;central limit theorem;principal component analysis;statistics;price yield surface	Visualization has the power to make people understand the content meaning at a glance and visualization is required also in mathematics education. We have developed and used visual teaching materials for our university statistics lectures. In the paper, we present visual materials in various fields from regression to Black-Sholes model.		Yukari Shirota;Yutaka Takahashi;Nobuhide Tanaka;Michiya Morita	2015		10.1145/2801040.2802138	econometrics;computer science;data science;statistics	HCI	26.621839694506477	-22.954320464762297	124239
d557844e158f4c232876c26639da79cc5719edb9	common method variance in is research: a comparison of alternative approaches and a reanalysis of past research	correlacion;empirical study;methode empirique;analisis estadistico;metodo empirico;method biases;empirical method;logit analysis;inference mechanisms;survey research;ecuesta estadistica;literature;marqueur;sample survey;systeme incertain;analisis desplazamiento;marcador;statistical analysis;path analysis;analyse statistique;inferencia;marker variable;information system;marker;correlation;literatura;analyse piste causale;sistema incierto;litterature;uncertain system;sondage statistique;systeme information;inference;mecanisme inferentiel;is research;sistema informacion;common method variance	Despite recurring concerns about common method variance (CMV) in survey research, the information systems (IS) community remains largely uncertain of the extent of such potential biases. To address this uncertainty, this paper attempts to systematically examine the impact of CMV on the inferences drawn from survey research in the IS area. First, we describe the available approaches for assessing CMV and conduct an empirical study to compare them. From an actual survey involving 227 respondents, we find that although CMV is present in the research areas examined, such biases are not substantial. The results also suggest that few differences exist between the relatively new marker-variable technique and other well-established conventional tools in terms of their ability to detect CMV. Accordingly, the marker-variable technique was employed to infer the effect of CMV on correlations from previously published studies. Our findings, based on the reanalysis of 216 correlations, suggest that the inflated correlation caused by CMV may be expected to be on the order of 0.10 or less, and most of the originally significant correlations remain significant even after controlling for CMV. Finally, by extending the marker-variable technique, we examined the effect of CMV on structural relationships in past literature. Our reanalysis reveals that contrary to the concerns of some skeptics, CMV-adjusted structural relationships not only remain largely significant but also are not statistically differentiable from uncorrected estimates. In summary, this comprehensive and systematic analysis offers initial evidence that (1) the marker-variable technique can serve as a convenient, yet effective, tool for accounting for CMV, and (2) common method biases in the IS domain are not as serious as those found in other disciplines.	meteorological reanalysis	Naresh K. Malhotra;Sung S. Kim;Ashutosh Patil	2006	Management Science	10.1287/mnsc.1060.0597	econometrics;survey research;common-method variance;mathematics;empirical research;statistics	HPC	33.64882388850448	-21.30861390365537	124273
f86ac2f2ca75c6c851f814ff2089f66fee760b14	estimating the change point of a poisson rate parameter with a linear trend disturbance	change point estimation;newton s method;confidence set estimation;maximum likelihood estimation;cusum control charts;poisson count processes;statistical process control spc;special cause identification;change point	Abstract#R##N##R##N#Knowing when a process changed would simplify the search and identification of the special cause. In this paper, we compare the maximum likelihood estimator (MLE) of the process change point designed for linear trends to the MLE of the process change point designed for step changes when a linear trend disturbance is present. We conclude that the MLE of the process change point designed for linear trends outperforms the MLE designed for step changes when a linear trend disturbance is present. We also present an approach based on the likelihood function for estimating a confidence set for the process change point. We study the performance of this estimator when it is used with a cumulative sum (CUSUM) control chart and make direct performance comparisons with the estimated confidence sets obtained from the MLE for step changes. The results show that better confidence can be obtained using the MLE for linear trends when a linear trend disturbance is present. Copyright © 2005 John Wiley & Sons, Ltd.		Marcus B. Perry;Joseph J. Pignatiello;James R. Simpson	2006	Quality and Reliability Eng. Int.	10.1002/qre.715	econometrics;mathematical optimization;mathematics;maximum likelihood;newton's method;statistics	EDA	29.18783758130131	-22.02092086600378	124400
5c0ee0ce987604c286f552072c8038a5cc25cba4	simulation selection for empirical model comparison		We propose an efficient statistical method for the empirical model comparison, which is typically referred to as a simulation procedure to evaluate multiple statistical learning algorithms. First, we use experimental designs to appropriately construct the training and test sets for estimating the empirical performances of these models using the mean square errors. Second, we apply the idea of Bayesian fully sequential ranking and selection to optimally allocate the simulation budget according to the value of information. To make the procedure computationally tractable, we assume a normal-Wishart prior distribution, and propose a new approximation scheme for the posterior distribution by matching it with a normal-Wishart distribution using the first-order moment. Numerical experiments are conducted to show the superiority of the proposed approach on empirical model comparison problems.	algorithm;approximation;cobham's thesis;design of experiments;experiment;first-order predicate;machine learning;mean squared error;model selection;norm (social);numerical method;performance;simulation	Qiong Zhang;Yongjia Song	2015	2015 Winter Simulation Conference (WSC)		econometrics;simulation;machine learning;mathematical model;mathematics;social simulation;bayesian linear regression;bayesian hierarchical modeling;statistics	ML	29.909801102409055	-17.187708557661235	124461
947da8581f54656663ab4436486d5fe4b221f8d9	smooth semiparametric and nonparametric bayesian estimation of bivariate densities from bivariate histogram data	institutional repositories;bayes estimation;classification automatique statistiques;theorie echantillonnage;metodo estadistico;teoria muestreo;analyse multivariable;sample size;cuantila;statistical simulation;62g05;correlacion rango;fedora;multivariate analysis;coeficiente correlacion;41a15;estimacion densidad;approximation numerique;analisis datos;bayesian p splines;copulas;conditional quantile;tamano muestra;biometrie;estimation densite;marginal models;social sciences;correlation rang;estimation non parametrique;metodo semiparametrico;biometrics;biometria;moment conditionnel;taille echantillon;metodo penalidad;loi conditionnelle;copula;62g07;statistical method;methode semiparametrique;ecuesta estadistica;ley condicional;medical science;grouped data bivariate density estimation bayesian p splines composite link model copula;aproximacion numerica;vital;conditional moment;discriminant analysis;estimation parametrique;analyse discriminante;non parametric estimation;density estimation;sample survey;data analysis;analisis discriminante;estimacion bayes;histogram;aproximacion esplin;sociometrie;penalty method;methode penalite;simulacion estadistica;donnee groupee;histogramme;65d07;bayesian p;ciencia medica;marginal distribution;62h30;methode statistique;simulation statistique;spline approximation;approximation spline;62f15;sociometria;statistical computation;calculo estadistico;semiparametric method;grouped data;momento condicional;sociometry;62d05;bivariate density estimation;bayesian estimator;ley marginal;semiparametric model;62p25;simulation study;dependence structure;analisis multivariable;analyse donnee;sciences sociales;calcul statistique;b spline;quantile;numerical approximation;estimacion no parametrica;tau kendall;estimation statistique;vtls;correlation coefficient;histograma;estimacion estadistica;ciencias sociales;statistical estimation;rank correlation;coefficient correlation;modele semi parametrique;kendall tau;sondage statistique;62p10	Penalized B-splines combined with the composite link model are used to estimate a bivariate density from a histogram with wide bins. The goals are multiple: they include the visualization of the dependence between the two variates, but also the estimation of derived quantities like Kendall's tau, conditional moments and quantiles. Two strategies are proposed: the first one is semiparametric with flexible margins modeled using B-splines and a parametric copula for the dependence structure; the second one is nonparametric and is based on Kronecker products of the marginal B-spline bases. Frequentist and Bayesian estimations are described. A large simulation study quantifies the performances of the two methods under different dependence structures and for varying strengths of dependence, sample sizes and amounts of grouping. It suggests that Schwarz's BIC is a good tool for classifying the competing models. The density estimates are used to evaluate conditional quantiles in two applications in social and in medical sciences.	bivariate data;semiparametric model	Philippe Lambert	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2010.05.011	econometrics;copula;calculus;mathematics;linear discriminant analysis;statistics	ML	32.951370212417906	-23.083213641693227	124487
dfabf8c0c07f29265887a904d97704ba4a970918	lossless watermarking for verifying the integrity of medical images with tamper localization	sensitivity and specificity;watermarking;hierarchical structure;diagnostic imaging;pacs;data integrity;radiology information systems;computer graphics;detection efficiency;telemedicine;radiographic image enhancement;artifacts;tamper localization;computer security;image authentication;integrity;medical image;image interpretation computer assisted;teleradiology;reproducibility of results;pattern recognition automated;humans;china;roi;security	Given the ease of alteration of digital data, integrity verification and tamper detection for medical images are becoming ever more important. In this paper, instead of using the conventional irreversible block-based watermarking approach to achieve tamper localization, we propose to incorporate such functionality into the region-based lossless watermarking scheme. This is achieved by partitioning an image into certain non-overlapping regions and appending the associated local authentication information directly into the watermark payload. A region of authentication, which can be flexibly specified by the user, is partitioned into small regions in a multilevel hierarchical manner. Such hierarchical structure allows the user to easily adjust the localization accuracy, and makes the tamper detection efficient. Experimental results demonstrate the effectiveness of tamper localization.	algorithm;authentication;data integrity;digital data;digital watermarking;embedding;item unique identification;lossless compression;medical imaging;region of interest;specification;verification of theories;verifying specimen	Xiaotao Guo;Tian-ge Zhuang	2008	Journal of Digital Imaging	10.1007/s10278-008-9120-5	medical imaging;computer vision;radiology;medicine;computer science;information security;theoretical computer science;data integrity;computer graphics;watermark;teleradiology;computer security;china	Graphics	38.3227620684616	-11.495861682034123	124584
9545692dbce3bd9df74763762effca0cb90c2b53	genetic team composition and level of selection in the evolution of cooperation	global dynamics;robot sensing systems;agent interaction;genomics;simulated foraging task;multiagent system;convergence;evolutionary computation;fitness allocation;decoding;evolution of cooperation;cooperation;multiagent team;homogeneous control;service robots;intelligence artificielle;robotics;cooperacion;levels of selection;genetics;simulated foraging task genetic team composition cooperative multiagent system global dynamics multiagent team homogeneous team individual level selection team level selection heterogeneous team;software agents;cooperative multiagent system;genetic team composition;multi agent systems;team composition;genetics genomics bioinformatics multiagent systems evolutionary computation service robots robot kinematics decoding vehicle dynamics guidelines;evolutionary robotics;altruismo;guidelines;altruisme;individual level selection;altruism;homogeneous team;evolutionary;commande homogene;robotica;team composition altruism artificial evolution cooperation evolutionary robotics fitness allocation multiagent systems mas;control homogeneo;artificial intelligence;software agents multi agent systems;algorithme evolutionniste;algoritmo evolucionista;robotique;inteligencia artificial;evolutionary algorithm;team level selection;sistema multiagente;multiagent systems mas;artificial evolution;vehicle dynamics;heterogeneous team;systeme multiagent;multiagent systems;robot kinematics;evolutionary computing;bioinformatics	In cooperative multiagent systems, agents interact to solve tasks. Global dynamics of multiagent teams result from local agent interactions, and are complex and difficult to predict. Evolutionary computation has proven a promising approach to the design of such teams. The majority of current studies use teams composed of agents with identical control rules (ldquogenetically homogeneous teamsrdquo) and select behavior at the team level (ldquoteam-level selectionrdquo). Here we extend current approaches to include four combinations of genetic team composition and level of selection. We compare the performance of genetically homogeneous teams evolved with individual-level selection, genetically homogeneous teams evolved with team-level selection, genetically heterogeneous teams evolved with individual-level selection, and genetically heterogeneous teams evolved with team-level selection. We use a simulated foraging task to show that the optimal combination depends on the amount of cooperation required by the task. Accordingly, we distinguish between three types of cooperative tasks and suggest guidelines for the optimal choice of genetic team composition and level of selection.	agent-based model;emergence;evolutionary computation;fail-safe;galaxy morphological classification;genetic algorithm;genetic distance;interaction;multi-agent system;partial template specialization;population;premature convergence;requirement;robot;simulated annealing;software agent;the evolution of cooperation	Markus Waibel;Laurent Keller;Dario Floreano	2009	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2008.2011741	vehicle dynamics;simulation;convergence;computer science;knowledge management;artificial intelligence;altruism;software agent;evolutionary algorithm;team composition;evolutionary robotics;cooperation;robot kinematics;evolutionary computation	Robotics	25.270771866685873	-11.572293065605795	124605
49aee4472fac55e50aa53ba637df190a8e31dfe9	a proof of orthogonal double machine learning with z-estimators		We consider two stage estimation with a non-parametric first stage and a generalized method of moments second stage, in a simpler setting than [CCD16]. We give an alternative proof of the theorem given in Chernozhukov et al. [CCD16] that orthogonal second stage moments, sample splitting and n-consistency of the first stage, imply √ n-consistency and asymptotic normality of second stage estimates. Our proof is for a variant of their estimator, which is based on the empirical version of the moment condition (Z-estimator), rather than a minimization of a norm of the empirical vector of moments (M-estimator). This note is meant primarily for expository purposes, rather than as a new technical contribution. 1 Two-Stage Estimation Suppose we have a model which predicts the following set of moment conditions: E[m(Z, θ0, h0(X))] = 0 (1) where θ0 ∈ R is a finite dimensional parameter of interest, h0 : S → R is a nuisance function we do not know, Z are the observed data which are drawn from some distribution and X ∈ S is a subvector of the observed data. We want to understand the asymptotic properties of the following two-stage estimation process: 1. First stage. Estimate h0(·) from an auxiliary data set (e.g. running some non-parametric regresssion) yielding an estimate ĥ. 2. Second stage. Use the first stage estimate ĥ and compute an estimate θ̂ of θ0 from an empirical version of the moment condition: i.e.	machine learning	Vasilis Syrgkanis	2017	CoRR		mathematical optimization;estimator;principle of orthogonal design;machine learning;mathematics;artificial intelligence;generalized method of moments;asymptotic distribution	ML	30.742059861211555	-23.324141607910725	124627
e31231b146a049ebf1efb84cfb3d7c3b372bb758	information hiding into obj format file using vector steganography techniques		The paper deals with the problematics of steganography, which is not only the science, but also the art of hiding the very existence of communication from third party. Steganography constantly searches new ways of information hiding, which are often considered as extraordinary inventive. The rise of information and communication technologies brought new possibilities to steganography and many new steganographic techniques are so close connected with ICT, that they do not have counterparts in non-ICT world. The paper deals with one of those possibilities and considers the use of object file format (OBJ) for information hiding using steganography. OBJ is the 3D geometry file format, originally developed by Wavefront Technologies and widely adopted by other 3D application vendors. Properties of OBJ file format, suitable for information hiding, are discussed in the first part of the paper, followed by the design of algorithm that uses combination of vector steganography techniques with the aim to minimize change of original information within the process of hidden message encoding. Last part of the paper summarizes and discusses results of tests performed on different models represented in OBJ file format.	algorithm;object file;steganography;wavefront technologies	Branislav Mados;Anton Baláz;Norbert Ádám;Ján Hurtuk	2018	2018 IEEE 12th International Symposium on Applied Computational Intelligence and Informatics (SACI)	10.1109/SACI.2018.8440965	control engineering;information hiding;steganography;theoretical computer science;computer science;file format;information and communications technology	SE	36.64444830934095	-12.462893816704087	125105
26f2133b934c0a53b8710a0a34888ad751096448	simulation of a lévy process by pca sampling to reduce the effective dimension	monte carlo methods;covariance matrices;eigenvalues and eigenfunctions;principal component analysis;sampling methods;stochastic processes;brownian bridge sampling;brownian motion;levy process;pca sampling;covariance matrix;eigen-decomposition;principal component analysis;quasimonte carlo	We consider a Lévy process monitored at s (fixed) observation times. The goal is to estimate the expected value of some function of these s observations by (randomized) quasi-Monte Carlo. For the case where the process is a Brownian motion, clever techniques such as Brownian bridge sampling and PCA sampling have been proposed to reduce the effective dimension of the problem. The PCA method uses an eigen-decomposition of the covariance matrix of the vector of observations so that a larger fraction of the variance depends on the first few (quasi)random numbers that are generated. We show how this method can be applied to other Lévy processes, and we examine its effectiveness in improving the quasi-Monte Carlo efficiency on some examples. The basic idea is to simulate a Brownian motion at s observation points using PCA, transform its increments into independent uniforms over (0, 1), then transform these uniforms again by applying the inverse distribution function of the increments of the Lévy process. This PCA sampling technique is quite effective in improving the quasi-Monte Carlo performance when the sampled increments of the Lévy process have a distribution that is not too far from normal, which typically happens when the process is observed at a large time scale, but may turn out to be ineffective in cases where the increments are far from normal.	brownian motion;effective dimension;eigen (c++ library);monte carlo method;qr decomposition;quasi-monte carlo method;randomized algorithm;sampling (signal processing);simulation	Pierre L'Ecuyer;Jean-Sebastien Parent-Chartier;Maxime Dion	2008	2008 Winter Simulation Conference		stochastic process;sampling;econometrics;mathematical optimization;mathematics;statistics	Vision	32.47180951015242	-17.22483189372329	125131
d1fabb56823718beb7a613dc4f5f00317db74aa4	robot design support system based on interactive evolutionary computation using boltzmann selection	mobile robots estimation theory evolutionary computation fuzzy set theory humanoid robots inference mechanisms;robot sensing systems;estimation theory;interactive design support system;robot design;evolutionary computation;boltzmann selection;estimation method;humans trajectory robot kinematics robot sensing systems collision avoidance navigation;inference mechanisms;mobile robots;user preferences;fuzzy set theory;support system;navigation;trajectory;humanoid robots;iphone simulator;fuzzy inference;robot design support system;humans;collision avoidance;product design;iphone simulator robot design support system interactive evolutionary computation boltzmann selection interactive design support system estimation method fuzzy inference;interaction design;robot kinematics;design support system;evolutionary computing;interactive evolutionary computation	Recently, the need of users is changing to the efficient quality of functions with the sophisticated design and reasonable price. Furthermore, current users prefer to the personal customization of products. Accordingly, a design support system is useful and helpful for non-expert people to design products easily, but such non-expert people might take much time and load in the product design. Therefore, we proposed interactive design support system based on evolutionary computation, and applied the proposed method to the design of robot partners. However, it is very difficult to reflect human evaluation to the generation of the next design candidates. Therefore, we propose an estimation method of human evaluation using fuzzy inference in the interactive design support using the evolutionary computation. Furthermore, we use iPhone simulator to evaluate the human impression based on direct interaction with the designed robot partner. Finally, we discuss the effectiveness of the proposed system through several simulation and experimental results.	fuzzy logic;interactive design;interactive evolutionary computation;robot;simulation	Naoyuki Kubota;Wataru Sato	2010	IEEE Congress on Evolutionary Computation	10.1109/CEC.2010.5586195	mobile robot;navigation;simulation;interactive evolutionary computation;computer science;humanoid robot;computer-automated design;artificial intelligence;trajectory;machine learning;interaction design;fuzzy set;estimation theory;robot kinematics;evolutionary computation	Robotics	24.69212116885909	-13.985605338066293	125336
f382dc00e1fc04d64c6935e283424e5dd7785fac	joint random-fuzzy variables: a tool for propagating uncertainty through nonlinear measurement functions	systematics;uncertainty;measurement uncertainty;yttrium;uncertainty yttrium measurement uncertainty monte carlo methods systematics entropy;uncertainty evaluation nonlinear operations possibility distributions pds random fuzzy variables rfvs systematic effects;entropy;monte carlo methods	A still open issue, in uncertainty evaluation, is asymmetrical distributions of the values that can be attributed to the measurand. This problem generally becomes not negligible when the measurement function is highly nonlinear. In this case, the law of uncertainty propagation suggested by the Guide to the Expression of Uncertainty in Measurement is not correct any longer, and only Monte Carlo simulations can be used to obtain such distributions. This paper shows how this problem can be solved in a quite immediate way when measurement results are expressed in terms of random-fuzzy variables. Under this approach, nonrandom contributions to uncertainty can also be considered. An experimental example is reported and the results compared with those obtained by means of Monte Carlo simulations, showing the effectiveness of the proposed approach.	computational complexity theory;computer simulation;experiment;monte carlo method;nonlinear system;numerical analysis;propagation of uncertainty;software propagation	Alessandro Ferrero;Marco Prioli;Simona Salicone	2016	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2016.2514782	econometrics;entropy;mathematical optimization;uncertainty analysis;uncertainty;propagation of uncertainty;yttrium;mathematics;systematics;sensitivity analysis;statistics;measurement uncertainty;monte carlo method	Vision	27.949622040968716	-17.784459069011703	125338
fc95d02a66af0cfb9c4351e2b18ff6b4cdc13426	effects of sampling and spatio/temporal granularity in traffic monitoring on anomaly detectability	sampling;hurst parameter;anomaly detection	We quantitatively evaluate how sampling and spatio/temporal granularity in traffic monitoring affect the detectability of anomalous traffic. Those parameters also affect the monitoring burden, so network operators face a trade-off between the monitoring burden and detectability and need to know which are the optimal paramter values. We derive equations to calculate the false positive ratio and false negative ratio for given values of the sampling rate, granularity, statistics of normal traffic, and volume of anomalies to be detected. Specifically, assuming that the normal traffic has a Gaussian distribution, which is parameterized by its mean and standard deviation, we analyze how sampling and monitoring granularity change these distribution parameters. This analysis is based on observation of the backbone traffic, which exhibits spatially uncorrelated and temporally long-range dependence. Then we derive the equations for detectability. With those equations, we can answer the practical questions that arise in actual network operations: what sampling rate to set to find the given volume of anomaly, or, if the sampling is too high for actual operation, what granularity is optimal to find the anomaly for a given lower limit of sampling rate.	anomaly detection;gibbs sampling	Keisuke Ishibashi;Ryoichi Kawahara;Tatsuya Mori;Tsuyoshi Kondoh;Shoichiro Asano	2012	IEICE Transactions		sampling;econometrics;anomaly detection;computer science;data mining;mathematics;hurst exponent;statistics	Metrics	32.90417666882406	-14.099996968788476	125367
1ad38e42fb65c52de31f82f5e272cbbb4aab7dcb	a trinomial test for paired data when there are many ties	test statistics;non parametric test;trinomial test;ties;ordinal data;statistics;statistical tests;sign test	This paper develops a new test, the trinomial test, for pairwise ordinal data samples to improve the power of the sign test by modifying its treatment of zero differences between observations, thereby increasing the use of sample information. Simulations demonstrate the power superiority of the proposed trinomial test statistic over the sign test in small samples in the presence of tie observations. We also show that the proposed trinomial test has substantially higher power than the sign test in large samples and also in the presence of tie observations, as the sign test ignores information from observations resulting in ties.	computer simulation;ordinal data;randomized algorithm;trinomial;turing test	Guorui Bian;Michael McAleer;Wing-Keung Wong	2011	Mathematics and Computers in Simulation	10.1016/j.matcom.2010.11.002	econometrics;statistical hypothesis testing;goldfeld–quandt test;sign test;chi-square test;mathematics;exact test;statistics	EDA	29.839101476001563	-22.69431478126906	125761
d2c15371ec44ce4b5cd89acbb599b52cd62d663f	on partial least squares dimension reduction for microarray-based classification: a simulation study	metodo cuadrado menor;linear combination;methode moindre carre;analisis componente principal;sample size;statistical simulation;least squares method;funcion no lineal;analisis datos;fonction poids;metodo reduccion;dimension reduction;partial least square;tamano muestra;simulacion numerica;taille echantillon;non linear function;classification;partial least squares;reduction dimension;data analysis;simulacion estadistica;logistic discrimination;62h30;combinacion lineal;principal components analysis;tumor;simulation statistique;principal component analysis;simulation numerique;statistical computation;calculo estadistico;62h25;62d05;funcion peso;fonction non lineaire;analyse composante principale;tumeur;simulation study;analyse donnee;methode reduction;calcul statistique;weight function;dna microarray;high dimension;reduction method;clasificacion;combinaison lineaire;numerical simulation	In microarray tumor tissue classi'cation studies, the expressions of thousands of genes (variables) are simultaneously measured across a few tissue samples. Standard statistical methodologies in classi'cation do not work well when the dimension, p, is greater than the sample size, N . One approach to classi'cation problems, when pN , is to 'rst apply a dimension reduction method and then perform the classi'cation in the reduced space. In this paper, we study dimension reduction for classi'cation in high dimension based on partial least squares (PLS) and principal components analysis (PCA). In addition, we propose and explore two hybrid-PLS methods for dimension reduction. PLS components are linear combinations of the original predictors, but the weights are nonlinear functions of both the predictors and response variable. This makes it di:cult to study the PLS classi'cation methodologies analytically, so, in this paper, we turn to a numerical study using simulation. c 2003 Elsevier B.V. All rights reserved.	dimensionality reduction;microarray;partial least squares regression;simulation	Danh V. Nguyen;David M. Rocke	2004	Computational Statistics & Data Analysis	10.1016/j.csda.2003.08.001	computer simulation;sufficient dimension reduction;calculus;mathematics;algorithm;statistics;principal component analysis	ML	33.48952336163097	-23.4097739460641	125965
7b45dc15dbaf49e4e9f64b9a36545f45612a5648	real-world transfer of evolved artificial immune system behaviours between small and large scale robotic platforms	artificial immune systems ais;evolutionary robotics;idiotypic networks;cross platform transfer;genetic algorithms	In mobile robotics, a solid test for adaptation is the ability of a control system to function not only in a diverse number of physical environments, but also on a number of different robotic platforms. This paper demonstrates that a set of behaviours evolved in simulation on a miniature robot (epuck) can be transferred to a much larger-scale platform (Pioneer), both in simulation and in the real world. The chosen architecture uses artificial evolution of epuck behaviours to obtain a genetic sequence, which is then employed to seed an idiotypic, artificial immune system (AIS) on the Pioneers. Despite numerous hardware and software differences between the platforms, navigation and target-finding experiments show that the evolved behaviours transfer very well to the larger robot when the idiotypic AIS technique is used. In contrast, transferability is poor when reinforcement learning alone is used, which validates the adaptability of the chosen architecture.	adaptive system;artificial immune system;control system;evolutionary algorithm;experiment;image scaling;microbotics;mobile robot;obstacle avoidance;population;reinforcement learning;stl (file format);simulation	Amanda M. Whitbrook;Uwe Aickelin;Jonathan M. Garibaldi	2010	Evolutionary Intelligence	10.1007/s12065-010-0039-7	simulation;genetic algorithm;computer science;artificial intelligence;machine learning;evolutionary robotics	Robotics	25.095312953773814	-12.563372687969554	125997
7bf2dcfe495788911631a344bf2e6288dca0ebfc	context-based watermarking cum chaotic encryption for medical images in telemedicine applications		In this paper, we propose a security system for secure transmission of medical images in telemedicine applications. The system couples an IWT-LSB watermarking and an encryption based on random permutation and chaos, to ensure confidentiality, integrity, authentication and nonrepudiation of medical images. We use IWT due to the sensitive nature of medical images and the need to retain diagnostic quality after image reconstruction. During the watermarking phase, the medical image is decomposed into wavelet sub-bands. Electronic patient record and extracted context information are then embedded in the least significant bits of the detail sub-band (host) coefficients. During encryption, the reconstructed watermarked medical image is randomly permuted and the permuted pixels diffused with a chaotic key stream to produce the cipher watermarked image. Experimental results and analyzes show that the system provides sufficient security against various forms of attacks. Furthermore, we propose a security architecture for the system.	authentication;chaos theory;cipher;coefficient;computer security;confidentiality;digital watermarking;embedded system;encryption;iterative reconstruction;least significant bit;medical imaging;non-repudiation;pixel;random permutation;randomness;secure transmission;wavelet	Joshua C. Dagadu;Jianping Li	2018	Multimedia Tools and Applications	10.1007/s11042-018-5725-y	enterprise information security architecture;computer vision;computer science;cipher;wavelet;random permutation;secure transmission;digital watermarking;artificial intelligence;encryption;authentication	Security	38.460255410027855	-10.661850603483954	126199
217810ee0e75cc1b49feeee13a93d1b574f96275	homeostasis and evolution together dealing with novelties and managing disruptions	endocrine system;neural nets;robotics;artificial intelligence	Purpose – The purpose of this paper is to present an artificial homeostatic system whose parameters are defined by means of an evolutionary process. The objective is to design a more biologically plausible system inspired by homeostatic regulations observed in nature, which is capable of exploring key issues in the context of robot behaviour adaptation and coordination. Design/methodology/approach – The proposed system consists of an artificial endocrine system that coordinates two spatially unconstrained GasNet artificial neural network models, called non-spatial GasNets. Both systems are dedicated to the definition of control actions in autonomous navigation tasks via the use of an artificial hormone and a hormone receptor. A series of experiments are performed in a real and simulated scenario in order to investigate the performance of the system and its robustness to novel environmental conditions and internal sensory disruptions. Findings – The designed system shows to be robust enough to self-adapt to a wider variety of disruptions and novel environments by making full use of its in-built homeostatic mechanisms. The system is also successfully tested on a real robot, indicating the viability of the proposed method for coping with the reality gap, a well-known issue for the evolutionary robotics community. Originality/value – The proposed framework is inspired by the homeostatic regulations and gaseous neuro-modulation that are intrinsic to the human body. The incorporation of an artificial hormone receptor stands for the novelty of this paper. This hormone receptor proves to be vital to control the network’s response to the signalling promoted by the presence of the artificial hormone. It is envisaged that the proposed framework is a step forward in the design of a generic model for coordinating many and more complex behaviours in simulated and real robots, employing multiple hormones and potentially coping with further severe disruptions.	artificial neural network;autonomous robot;evolutionary robotics;experiment;homeostasis;modulation;neuro-fuzzy;robustness (computer science)	Patrícia Amâncio Vargas;Renan C. Moioli;Fernando José Von Zuben;Phil Husbands	2009	Int. J. Intelligent Computing and Cybernetics	10.1108/17563780910982680	simulation;computer science;artificial intelligence;machine learning;robotics;endocrine system;artificial neural network	Robotics	25.287744523377395	-13.362258644369016	126274
40cb3ab0444e3134a6a435b06410a114f50bd416	quantifying uncertainty in sample average approximation		We consider stochastic optimization problems in which the input probability distribution is not fully known, and can only be observed through data. Common procedures handle such problems by optimizing an empirical counterpart, namely via using an empirical distribution of the input. The optimal solutions obtained through such procedures are hence subject to uncertainty of the data. In this paper, we explore techniques to quantify this uncertainty that have potentially good finite-sample performance. We consider three approaches: the empirical likelihood method, nonparametric Bayesian approach, and the bootstrap approach. They are designed to approximate the confidence intervals or posterior distributions of the optimal values or the optimality gaps. We present computational procedures for each of the approaches and discuss their relative benefits. A numerical example on conditional value-at-risk is used to demonstrate these methods.	approximation algorithm;booting;bootstrapping (statistics);computation;expected shortfall;mathematical optimization;numerical analysis;stochastic optimization;value at risk	Henry Lam;Enlu Zhou	2015	2015 Winter Simulation Conference (WSC)		convex function;stochastic programming;probability distribution;econometrics;mathematical optimization;approximation error;categorical distribution;convergence;uncertainty;mathematics;social simulation;sampling distribution;posterior probability;sensitivity analysis;empirical probability;statistics	AI	29.706625884357642	-17.055794562759495	126391
863659de8f2486463aa5a5a44012ada1d82bc632	merging expert and empirical data for rare event frequency estimation: pool homogenisation for empirical bayes models	bayes estimation;poisson process;homogeneisation;precursor;empirical bayes;homogenization;comparaison par paire;estimation frequence;modele empirique;methode empirique;pairwise comparison;homogeneous process;weighted averaging;bayes methods;methode bayes;evenement rare;error relativo;metodo empirico;weighting;risk management;empirical method;processus homogene;estimacion promedio;frequency estimation;ponderacion;simulation experiment;poisson processes;estimacion bayes;hierarchical classification;relative error;proceso homogeneo;estimation erreur;empirical bayes method;comparacion por pares;rare event;error estimation;erreur estimation;scale factor;precurseur;acontecimiento rara;estimacion error;erreur relative;classification hierarchique;error estimacion;empirical model;paired comparison;modelo empirico;proceso poisson;homogeneizacion;ponderation;estimation error;mean estimation;estimation moyenne;clasificacion jerarquizada;facteur echelle;factor escala;pra;processus poisson;estimation bayes	Empirical Bayes provides one approach to estimating the frequency of rare events as a weighted average of the frequencies of an event and a pool of events. The pool will draw upon, for example, events with similar precursors. The higher the degree of homogeneity of the pool, then the Empirical Bayes estimator will be more accurate. We propose and evaluate a new method using homogenisation factors under the assumption that events are generated from a Homogeneous Poisson Process. The homogenisation factors are scaling constants which can be elicited through structured expert judgement and used to align the frequencies of different events, hence homogenising the pool. The estimation error relative to the homogeneity of the pool is examined theoretically indicating that reduced error is associated with larger pool homogeneity. The effects of misspecified expert assessments of the homogenisation factors are examined theoretically and through simulation experiments. Our results show that the proposed Empirical Bayes method using homogenisation factors is robust under different degrees of misspecification.	align (company);event generator;experiment;extreme value theory;human proteome project;image scaling;mod database;monte carlo method;naive bayes classifier;pool (computer science);rare events;simulation;spatial variability;spectral density estimation	John Quigley;Gavin Hardman;Tim Bedford;Lesley Walls	2011	Rel. Eng. & Sys. Safety	10.1016/j.ress.2010.12.007	pairwise comparison;econometrics;risk management;pattern recognition;mathematics;statistics	ML	32.84603950994042	-21.914034747778405	126430
4b73800a1252c7e8c881c180f2027d041d9c0811	getting rid of correlations among pseudorandom numbers: discarding versus tempering	gambling test;computacion informatica;tempering;empirical tests;discarding;ciencias basicas y experimentales;matematicas;grupo a;pseudorandom number generator;high speed;correlation analysis	We consider the impact of discarding and tempering on modern huge period high speed linear generators, and illustrate how a simple strategy yields unexpected &mdashh; and unwanted — success in a fair coin gambling which is simulated by a recently proposed generator. It becomes clear that discarding is no general rule to get rid of unwanted correlations.	pseudorandomness	Stefan Wegenkittl;Makoto Matsumoto	1999	ACM Trans. Model. Comput. Simul.	10.1145/347823.347835	simulation;computer science;mathematics;pseudorandom number generator;algorithm;statistics;tempering	Theory	27.48628559952566	-18.77136639062187	126486
0a5485a67846a23b2bbcb4103e4d4982934fd5f9	sensitivity analysis and performance extrapolation for computer simulation models	camino mas corto;simulation ordinateur;sensibilite;shortest path;fiabilidad;reliability;project management;organizacion proyecto;simulation computer simulation models;red cola espera;stochastic system;sensitivity;perturbacion;reseau file attente;sensitivity analysis;fiabilite;chemin plus court;gestion projet;queuing network;simulacion computadora;perturbation;sistema estocastico;computer simulation;sensibilidad;systeme stochastique	We present a method for deriving sensitivities of performance measures for computer simulation models. We show that both the sensitivities (derivatives, gradients, Hessians, etc.) and the performance measure can be estimated simultaneously from the same simulation. Our method is based on probability measure transformations derived from the efficient score. We also present a rather general procedure from which perturbation analysis and our method can be viewed as particular cases. Applications to reliability models and stochastic shortest path networks are given.	computer simulation;extrapolation	Reuven Y. Rubinstein	1989	Operations Research	10.1287/opre.37.1.72	computer simulation;project management;simulation;perturbation;sensitivity;computer science;artificial intelligence;reliability;mathematics;shortest path problem;sensitivity analysis;statistics	HPC	27.938748459282575	-15.653652656690312	126509
00ad117375fdd21c16ba70dc022b68dd12edcaab	numerical methods to quantify the model risk of basket default swaps	archimedean copula;computacion informatica;model risk;basket default swap;ciencias basicas y experimentales;matematicas;importance sampling;grupo a;credit derivative sensitives	The valuation of basket default swaps depends crucially on the joint default probability of the underlying assets in the basket. It is known that this probability can be modeled by means of a copula function which links the marginal default probabilities to a joint probability. The valuation bears risk due to the uncertainty of the copula, the relation of the assets to each other and the marginal distributions which we call together the model risk. To value basket default swaps and to compute model risk parameters we present an efficient numerical approach based on importance sampling and applicable to different classes of copula models. Our numerical findings show that the choice of the underlying copula model influences strongly the risk profile of the basket and should be tailored advisedly.	coefficient;concordance (publishing);contingency (philosophy);entity–relationship model;experiment;importance sampling;marginal model;numerical analysis;paging;sampling (signal processing);value (ethics);variance reduction	Alexander Schröter;P. Heider	2013	J. Computational Applied Mathematics	10.1016/j.cam.2013.03.042	econometrics;actuarial science;importance sampling;mathematics;statistics	ML	26.715730769402942	-20.907463726406174	126742
0dfb4879bd999da17ff10f49fb6ce5a41ffebd0b	application of continuous time markov theory in modeling courtship behavior and copulation rate of the calanoid copepod eurytemora affinis	goodness of fit;courtship behavior;markov theory;behavior modeling;sequential analysis;behavioral modeling;behavioral ecology;zooplankton;mating behavior	We present a simple model, derived using a continuous time Markov (CTM) technique of model formulation, for predicting the probability of a courtship resulting in copulation for a laboratory population of Eurytemora affinis. This model represents a novel use, in behavioral ecology, of CTM as a method for sequential analysis. The model results in an acceptable agreement (validated by 2 goodness-of-fit analysis) between the predicted and observed copulation rates of the laboratory population of E. affinis. The copulation rate predicted by the model, with estimated weightings to the transition rates, is 16.7%; the observed rate was 13.0% (16 copulations out of 123 courtship bouts observed), for a discrepancy of 3.7%. Continuous time Markov theory appears to adequately describe mating behavior in this experimental population, implying ‘‘forgetfulness’’ in the courtship sequence.	discrepancy function;ecology;markov chain	Craig W. Steele;A. David Scarfe	2011	Adaptive Behaviour	10.1177/1059712311403838	behavioral modeling;behavioral ecology;communication;social psychology	ML	31.929040898246686	-13.003751461457755	126855
52b9917ee42093663bace95212e0f5d08f160bd9	regression analysis with partially labelled regressors: carbon dating of the shroud of turin	simulation envelope;robust statistics;computer intensive method;forward search;ha statistics	The twelve results from the 1988 radio carbon dating of the Shroud of Turin show surprising heterogeneity. We try to explain this lack of homogeneity by regression on spatial coordinates. However, although the locations of the samples sent to the three laboratories involved are known, the locations of the 12 subsamples within these samples are not. We consider all 387,072 plausible spatial allocations and analyse the resulting distributions of statistics. Plots of robust regression residuals from the forward search indicate that some sets of allocations are implausible. We establish the existence of a trend in the results and suggest how better experimental design would have enabled stronger conclusions to have been drawn from this multi-centre experiment.	design of experiments	Marco Riani;Anthony C. Atkinson;Giulio Fanti;Fabio Crosilla	2013	Statistics and Computing	10.1007/s11222-012-9329-5	robust statistics;econometrics;mathematics;statistics	AI	30.79659340539454	-22.73539092150281	127047
452adf740d296ef33750dd06caff44df5bc4109c	co-evolving model parameters for anytime learning in evolutionary robotics	learning control;evolutionary robotics;evolutionary genetics;genetic algorithm;evolutionary computing	Evolutionary robotics is a research area that makes use of evolutionary computation (EC) to provide a means of learning in robots. In this paper, we discuss a new way of integrating the actual robot and its model during EC. This method, which involves the co-evolution of model parameters, is applied to the problem of learning gaits for hexapod robots. The form of EC used is the cyclic genetic algorithm (CGA). Tests done in simulation and on the robot show that the CGA operating on the co-evolving model of the robot can adapt to changes in the robot’s capabilities to provide a system of anytime learning. © 2000 Elsevier Science B.V. All rights reserved.	adaptive system;anytime algorithm;elegant degradation;evolutionary computation;evolutionary robotics;genetic algorithm;ibm notes;overhead (computing);robot;simulation	Gary B. Parker	2000	Robotics and Autonomous Systems	10.1016/S0921-8890(00)00093-2	evolutionary programming;robot learning;simulation;genetic algorithm;interactive evolutionary computation;human-based evolutionary computation;computer science;artificial intelligence;machine learning;human evolutionary genetics;evolutionary robotics;evolutionary computation	Robotics	24.683684644932153	-12.264208411378092	127062
5dd4e76fce1b49921aff85462819f7515ebf98d7	the influence of random element displacement on doa estimates obtained with (khatri–rao-)root-music	biological patents;biomedical journals;stochastic collocation method;steering vector error;text mining;europe pubmed central;performance;citation search;doa estimation;khatri rao;citation networks;sensitivity;of arrival estimation;uncertainties;polynomial chaos;technology and engineering;research articles;abstracts;open access;music algorithm;life sciences;clinical guidelines;array;full text;rao subspace approach;rest apis;orcids;europe pmc;acoustic vector sensor;biomedical research;model errors;bioinformatics;literature search	Although a wide range of direction of arrival (DOA) estimation algorithms has been described for a diverse range of array configurations, no specific stochastic analysis framework has been established to assess the probability density function of the error on DOA estimates due to random errors in the array geometry. Therefore, we propose a stochastic collocation method that relies on a generalized polynomial chaos expansion to connect the statistical distribution of random position errors to the resulting distribution of the DOA estimates. We apply this technique to the conventional root-MUSIC and the Khatri-Rao-root-MUSIC methods. According to Monte-Carlo simulations, this novel approach yields a speedup by a factor of more than 100 in terms of CPU-time for a one-dimensional case and by a factor of 56 for a two-dimensional case.	cpu (central processing unit of computer system);central processing unit;collocation method;direction of arrival;displacement mapping;estimated;formal system;greater than;monte carlo method;normal statistical distribution;numerical integration;polynomial;psychologic displacement;root-finding algorithm;simulation;spectinomycin;speedup	Veronique Inghelbrecht;Jo Verhaevert;Tanja van Hecke;Hendrik Rogier	2014		10.3390/s141121258	text mining;speech recognition;polynomial chaos;performance;telecommunications;sensitivity;computer science;bioinformatics;electrical engineering;multiple signal classification;data mining;statistics	Metrics	32.22546504328083	-16.442220189872714	127095
b6f1305c21b27d7adfd5423cb3bd7c801f9d5e37	alternative approaches to implementing lagrange multiplier tests for serial correlation in dynamic regression models	test hypothese;modelo dinamico;autocorrelacion;62f40;serial correlation;metodo monte carlo;probability;bootstrap;65c05;analisis datos;loi probabilite;ley probabilidad;correlation serielle;62h20;test hipotesis;62e17;lagrange multiplier test;dynamic model;methode monte carlo;regression model;lagrange multiplier;statistical regression;62jxx;local alternative;data analysis;modelo regresion;regresion estadistica;monte carlo experiment;modele regression;monte carlo method;probability distribution;bootstrap test;modele dynamique;probabilidad;statistical computation;multiplicateur lagrange;calculo estadistico;analyse correlation;probabilite;multiplicador lagrange;analyse donnee;calcul statistique;estimation statistique;regression statistique;estimacion estadistica;statistical estimation;analisis correlacion;lm test;autocorrelation;correlation analysis;hypothesis test	An approximate F -form of the Lagrange multiplier (LM) test for serial correlation in dynamic regression models is compared with three bootstrap tests. In one bootstrap procedure, residuals from restricted estimation under the null hypothesis are resampled. The other two bootstrap tests use residuals from unrestricted estimation under an alternative hypothesis. A fixed autocorrelation alternative is assumed in one of the two unrestricted bootstrap tests and the other is based upon a Pitman-type sequence of local alternatives. Monte Carlo experiments are used to estimate rejection probabilities under the null hypothesis and in the presence of serial correlation. © 2006 Elsevier B.V. All rights reserved.	algorithmic efficiency;approximation algorithm;autocorrelation;autoregressive model;bootstrapping (compilers);bootstrapping (statistics);computation;experiment;fdb (file format);failure;first-order predicate;lagrange multiplier;monte carlo method;ordinary least squares;r.u.r.;rejection sampling;simulation;turing completeness	L. G. Godfrey	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.05.020	econometrics;autocorrelation;calculus;mathematics;regression analysis;statistics	AI	32.835160356142296	-22.616307150167252	127188
453217da48168e7294a952376a695020f00f1e5e	model-based classification via mixtures of multivariate t-distributions	classification automatique statistiques;grado libertad;analyse multivariable;convergence;multivariate analysis;analisis datos;melange loi probabilite;maximization;ley n variables;degree of freedom;62h10;matrice covariance;multivariate t distributions;classification food authenticity mixture models model based classification multivariate t distributions;mixed distribution;conditional expectation;matriz covariancia;distribucion estadistica;classification;esperance conditionnelle;algorithme;discriminant analysis;analyse discriminante;algorithm;data analysis;analisis discriminante;convergencia;esperanza condicional;distribution statistique;mixture model;62h30;covariance matrices;food authenticity;statistical computation;calculo estadistico;multivariate t;model based classification;mezcla ley probabilidad;analisis multivariable;multivariate distribution;analyse donnee;calcul statistique;mixture models;loi n variables;freedom degree;maximizacion;statistical distribution;maximisation;expectation conditional maximization;covariance matrix;degre liberte;algoritmo	A novel model-based classification technique is introduced based on mixtures of multivariate t-distributions. A family of four mixture models is defined by constraining, or not, the covariance matrices and the degrees of freedom to be equal across mixture components. Parameters for each of the resulting four models are estimated using a multicycle expectation-conditional maximization algorithm, where convergence is determined using a criterion based on the Aitken acceleration. A straightforward, but very effective, technique for the initialization of the unknown component memberships is introduced and compared with a popular, more sophisticated, initialization procedure. This novel four-member family is applied to real and simulated data, where it gives good classification performance, even when compared with more established techniques.		Jeffrey L. Andrews;Paul D. McNicholas;Sanjeena Subedi	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2010.05.019	econometrics;calculus;mixture model;mathematics;linear discriminant analysis;statistics	ML	32.84990549982005	-23.795910080047594	127251
826846f4f16c5d8f95283e83e54ca9408bc9b423	analytic solution of the susceptible-infective epidemic model with state-dependent contact rates and different intervention policies	corresponding deterministic model;epidemiological model;kermack-mckendrick model;continuous-time markov chain;epidemic duration;contact rate;susceptible-infective epidemic model;state-dependent contact rate;si model;different intervention policy;analytic solution;markov chain;stochastic model;resultant model	We consider the susceptible-infective (SI) epidemiological model, a variant of the Kermack-McKendrick models, and let the contact rate be a function of the number of infectives, an indicator of disease spread during the course of the epidemic. We represent the resultant model as a continuous-time Markov chain. The result is a pure death (or birth) process with state-dependent rates, for which we find the probability distribution of the associated Markov chain by solving the Kolmogorov forward equations. This model is used to find the analytic solution of the SI model as well as the distribution of the epidemic duration. We use the maximum likelihood method to estimate contact rates based on observations of inter-infection time intervals. We compare the stochastic model to the corresponding deterministic models through a numerical experiment within a typical household. We also incorporate different intervention policies for vaccination, antiviral prophylaxis, isolation, and treatment considering both full and partial adherence to interventions among individuals.		Hamed Yarmand;Julie S. Ivy	2013	Simulation	10.1177/0037549713479052	econometrics;simulation;mathematics;statistics;psychological intervention	AI	32.210917819383454	-13.21388898999826	127308
5bc06103b935dbc49d2b43e143cffa5e0e727542	a least angle regression control chart for multidimensional data	processus gauss;variabilidad;moyenne mobile;analyse multivariable;control fase;control estadistico proceso;multivariate analysis;surveillance;carte controle;weighting;estimacion promedio;statistical process control;maitrise statistique processus;event detection;ponderacion;statistical process control spc process management;variable selection;profile monitoring;control chart;moving average;vigilancia;procedimiento poliescalonado;diagnostic panne;monitoring;fault diagnostic;commande phase;deteccion de eventos;diagnostico pana;carta control;promedio movil;point changement;analisis multivariable;exponentially weighted moving average ewma exponentially weighted moving average control charts ewma multivariate control charts statistical process control spc variable selection;detection d evenements;procede etage;ponderation;monitorage;gaussian process;mean estimation;variability;estimation moyenne;proceso gauss;monitoreo;variabilite;change point detection;punto cambio;change point;phase control;exponentially weighted moving average;multistage process	In multidimensional applications, it is very rare that all variables shift at the same time. A statistical process control procedure would have superior efficiency when limited to the subset of variables likely responsible for the out-of-control conditions. The key idea of this article consists of combining a variable selection method with a multivariate control chart to detect changes in both the mean and variability of a multidimensional process with Gaussian errors. In particular, we develop a control chart for Phase II monitoring which integrates the least angle regression algorithm with a multivariate exponentially weighted moving average. Comparisons with related multivariate control schemes demonstrate the efficiency of the proposed control chart in a wide range of practical applications, including profile and multistage process monitoring. Further, the proposed scheme may also provide valuable diagnostic information for fault isolation. Supplemental materials, including an R package, are available...	least-angle regression	Giovanna Capizzi;Guido Masarotto	2011	Technometrics	10.1198/TECH.2011.10027	ewma chart;econometrics;control chart;gaussian process;phase-fired controllers;weighting;mathematics;shewhart individuals control chart;multivariate analysis;moving average;feature selection;change detection;statistical process control;statistics	Robotics	28.25548048601801	-19.60769022392832	127325
b9f8d5a5bf731ed2529ee09c1f16847ba1d3d383	a maximum entropy modelling of the rain drop size distribution	rain drop size distribution;maximum entropy method	This paper presents a maximum entropy approach to Rain Drop Size Distribution (RDSD) modelling. It is shown that this approach allows (1) to use a physically consistent rationale to select a particular probability density function (pdf) (2) to provide an alternative method for parameter estimation based on expectations of the population instead of sample moments and (3) to develop a progressive method of modelling by updating the pdf as new empirical information becomes available. The method is illustrated with both synthetic and real RDSD data, the latest coming from a laser disdrometer network specifically designed to measure the spatial variability of the RDSD.	design rationale;estimation theory;portable document format;spatial variability;synthetic intelligence	Ramiro Checa;Francisco J. Tapiador	2011	Entropy	10.3390/e13020293	econometrics;mathematics;physics;statistics	Vision	28.320878215537377	-21.496456494675225	127332
deb309a11b75d50afd3a0da0be129e20038933df	an empirical analysis of reinforcement learning using design of experiments		This study uses a design of experiments approach to understand the behavior of a neural network to learn the mountain car domain using the TD(λ) algorithm. A large experiment is first performed to characterize the probability of empirical convergence based on three parameters of the TD(λ) algorithm (λ, γ, ), and a logistic regression model is fitted to this data. A detailed analysis of the parameter subspace finds that, upon convergence, these parameters significant affect convergence speed and mean performance, though performance differences are minimal.	algorithm;artificial neural network;design of experiments;experiment;logistic regression;mountain car;reinforcement learning	Christopher J. Gatti;Mark J. Embrechts;Jonathan D. Linton	2013			econometrics;simulation;computer science;machine learning;statistics	ML	24.953729077302874	-22.362355698665183	127533
4813d0be374853814b394b9928b7582d8adb8f1c	a modified em alr-algorithm for replacing rounded zeros in compositional data sets	computadora;tratamiento datos;computers;rounded zero;02 70 rr additive logistic normal model;geochemistry;composition chimique;chemical composition;ordinateur;transformations;02 70 c;transformacion;data processing;estrategia;traitement donnee;covariance;compositional data;computer programs;algorithme;strategy;02 50 sk;log ratio transformation;data analysis;modelo;statistical analysis;02 70 rr;detection limit;analyse statistique;simplex;algorithms;additive logistic normal model;modele;missing data;geoquimica;transformation;programa computador;strategie;covariance structure;models;limite detection;geochimie;programme ordinateur;algoritmo	The presence of rounded zeros results in an important drawback for the statistical analysis of compositional data. Data analysis methodology based on log-ratios cannot be applied under these conditions. In this paper rounded zeros are considered as a special kind of missing data. Thus, an EM-type computational algorithm for replacing them is provided. The procedure is based on the additive logistic transformation and assumes an additive logistic normal model for the data. First, the alr transformation moves data from the constrained simplex space to the unconstrained real space. Next, missing transformed data are imputed by using modified EM steps. Last, imputed data are transformed back into the simplex space to obtain a compositional data set free of rounded zeros. Additionally, a sequential strategy is proposed for the case of rounded zeros in all the components of a composition. This work focuses on the algorithm’s properties and on computational implementation details. Also, its effectiveness on simulated data sets with a range of detection limits is analyzed. Special attention is paid on the effects on the covariance structure of a compositional data set. Results confirm the good behavior of our proposal. Finally, MATLAB routines implementing the algorithm are made available to the	algorithm;compositional data;computation;interpretation (logic);matlab;missing data;utility functions on indivisible goods	Javier Palarea-Albaladejo;J. A. Martín-Fernández	2008	Computers & Geosciences	10.1016/j.cageo.2007.09.015	transformation;transformation;econometrics;chemical composition;detection limit;data processing;missing data;strategy;covariance;mathematics;data analysis;simplex;algorithm;statistics	ML	33.89405668958023	-23.77153601553718	127710
65c63f43f6cd79c45994ed969abe9e0a3004065c	on reliability analysis of a k-out-of-n system with components having random weights	order statistics;monte carlo simulation;k out of n system;system with weighted components	Consider a system consisting of  n  components each with its own positive integer-valued random weight (capacity). The system is assumed to have a performance level above  c    if there are at least             k        working components, and the total weight of all working components is above  c . We study the reliability properties of such a system. A recursive formula is obtained for computing the system state probabilities. We present a Monte-Carlo simulation algorithm to observe the time spent by the system in state  c  or above. The algorithm is based on the use of ordered lifetimes of components. We illustrate the results with numerical computations.	reliability engineering	Serkan Eryilmaz	2013	Rel. Eng. & Sys. Safety	10.1016/j.ress.2012.07.010	econometrics;order statistic;simulation;computer science;mathematics;statistics;monte carlo method	SE	30.93736346332839	-18.748893445801677	127740
5cc434ddbe854fd40db1d8bf2e36e1c99e1bdf02	the generalized likelihood ratio chart for monitoring a proportion with autocorrelation	maximum likelihood estimates;loss function;binary data;markov chain	When monitoring a proportion p, it is usually assumed that the binary observations are independent. This paper investigates the problem of monitoring p when the binary observations follow a two-state Markov chain model with first-order dependence. A Markov binary generalized likelihood ratio (MBGLR) chart based on a likelihood ratio statistic with an upper bound on the estimate of p is proposed. The MBGLR chart is used to monitor a continuous stream of autocorrelated binary observation. The MBGLR chart with a relatively large upper bound has good overall performance over a wide range of shifts. The extra number of defectives is defined to measure the loss when using control charts for monitoring p. The MBGLR chart is optimized over a range of upper bounds for the MLE of p. The numerical results show that the optimized MBGLR chart has a smaller extra number of defectives than the optimized Markov binary cumulative sum chart that can detect a shift in p much faster than a Shewhart-type chart. Copyright © 2014 John Wiley & Sons, Ltd.	autocorrelation	Ning Wang;Marion R. Reynolds	2015	Quality and Reliability Eng. Int.	10.1002/qre.1660	econometrics;markov chain;pattern recognition;mathematics;u-chart;statistics;loss function	NLP	29.28045547126366	-20.019992746502197	127895
1d71f81ea405c45b7fba2e826ddbb25e07eece27	repeated measures proportional odds logistic regression analysis of ordinal score data in the statistical software package r	62 07;intervalo tiempo;software;association statistique;correlacion;metodo estadistico;ajustamiento modelo;ecuacion de estimacion;analyse multivariable;repeated measurement;statistical data;test statistique;covariance analysis;theorie approximation;multivariate analysis;analisis datos;logiciel;ordinal analysis;modelo autorregresivo;dato que falta;statistical software;implementation;biometrie;62h20;qa mathematics;repeated measures;test estadistico;proportional odds model;62e17;biometrics;variance analysis;biometria;statistical test;processus autoregressif;statistical association;clinical trial;statistical method;mesure repetee;equation estimatrice;medical science;space time;distribucion estadistica;logistic regression;time interval;statistical regression;62jxx;equation generalisee;autoregressive model;algorithme;qa76 electronic computers computer science computer software;donnee manquante;ajustement modele;approximation theory;algorithm;estimating equation;data analysis;ensayo clinico;regresion logistica;asociacion estadistica;rd surgery;first order;analisis regresion;analyse covariance;distribution statistique;autoregressive processes;ciencia medica;methode statistique;analisis variancia;regresion estadistica;model matching;regression logistique;hip joint;62j10;statistical computation;calculo estadistico;analyse correlation;ordinal data;estimacion parametro;software package;donnee statistique;analyse regression;logicial;analisis multivariable;analyse donnee;generalized estimating equation;regression analysis;calcul statistique;essai clinique;missing data;progiciel;parameter estimation;estimation parametre;correlation;model fitting;analisis covariancia;estimation statistique;dato estadistico;implementacion;regression statistique;generalized equation;modele autoregressif;estimacion estadistica;statistical estimation;statistical distribution;paquete programa;medida repetida;analisis ordinal;62p10;analyse ordinale;analisis correlacion;analyse variance;variance	The widely used proportional odds model is developed for correlated repeated ordinal score data, using a modified version of the generalized estimating equation (GEE) method for model fitting for a range of working correlation models. The algorithm developed estimates the correlation parameter, by minimizing the generalized variance of the regression parameters at each step of the fitting algorithm. Methods for parameter estimation are described for the widely used uniform and first-order autoregressive correlation models, for data potentially recorded at irregularly spaced time intervals. A full implementation of the algorithm (repolr) in the R statistical software package, that both tests the assumption of proportional odds and accommodates missing data, is described and applied to a clinical trial of post-operative treatment, after rupture of the Achilles tendon and a study of patient pain response after hip joint resurfacing. © 2008 Elsevier B.V. All rights reserved.		Nick R. Parsons;Matthew L. Costa;Juul Achten;Nigel Stallard	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.08.004	econometrics;calculus;mathematics;logistic regression;regression analysis;statistics	AI	33.11551823563158	-22.375410625972464	127919
0973cb85885def2b24d5a46266df3556bd259b5a	zero finite-order serial correlation test in a partially linear single-index model	partially linear single index model;xiaohui liu guofu wang xuemei hu bo li 指数模型 相关性检验 序列 线性 有限阶 有限样本性质 测试统计 检验统计量 zero finite order serial correlation test in a partially linear single index model;journal;zero finite order serial correlation;asymptotic distribution;empirical likelihood	The purpose of this paper is to test the underlying serial correlation in a partially linear single-index model. Under mild conditions, the proposed test statistics are shown to have standard chisquared distribution asymptotically when there is no serial correlation in the error terms. To illustrate their finite sample properties, simulation experiments, as well as a real data example, are also provided. It is revealed that the finite sample performances of the proposed test statistics are satisfactory in terms of both estimated sizes and powers.	autocorrelation;single-index model	Xiaohui Liu;Guofu Wang;Xue-mei Hu;Bo Li	2012	J. Systems Science & Complexity	10.1007/s11424-012-0033-5	econometrics;calculus;mathematics;asymptotic distribution;statistics	Logic	30.262063022597406	-22.981939351766204	127976
402841a3ae99f7570857404c20e65e0d9f2bcaeb	a convex version of multivariate adaptive regression splines	convexity;regression splines	Multivariate adpative regression splines (MARS) provide a flexible statistical modeling method that employs forward and backward search algorithms to identify the combination of basis functions that best fits the data and simultaneously conduct variable selection. In optimization, MARS has been used successfully to estimate the unknown functions in stochastic dynamic programming (SDP), stochastic progr amming, and a Markov decision process, and MARS could be potentially useful in many real world optimization problems where objective (or other) functions need to be estimated from data, such as in simulation optimization. Many optimization methods depend on convexity, but a nonconvex MARS approximation is inherently possible becau se interaction terms are products of univariate terms. In this paper a convex MARS modeling algorithm is described. In order to ensure MARS convexity, two major modifications are made: (1) coefficients are constrained, su ch that pairs of basis functions are guaranteed to jointly form convex functions; (2) the form of interaction terms is a ltered to eliminate the inherent nonconvexity. Finally, MARS convexity can be achieved by the fact that the sum of conv ex functions is convex. Convex-MARS is applied to inventory forecasting SDP problems with four and nine dimen sio s.	approximation;backward induction;basis function;coefficient;convex function;convex optimization;dynamic programming;fits;feature selection;linear function;markov chain;markov decision process;mathematical optimization;multivariate adaptive regression splines;sio (software);search algorithm;simulation;smoothing spline;spline (mathematics);statistical model;stepwise regression;stochastic programming	Diana L. Martinez;Dachuan T. Shih;Victoria C. P. Chen;Seoung Bum Kim	2015	Computational Statistics & Data Analysis	10.1016/j.csda.2014.07.015	econometrics;mathematical optimization;convex optimization;convexity;multivariate adaptive regression splines;mathematics;statistics	ML	29.40386416485031	-14.490674664026402	128726
c485e402d3dbe8843bd703fe2773bea778c662ef	optimal test planning for the fatigue-limit model when the fatigue-limit distribution is known	minimum variance;limit distribution;fatigue stress life testing life estimation propulsion parameter estimation extrapolation distribution functions probability density function mathematics;planning life test optimal test planning fatigue limit model fatigue limit distribution equivalence theorem fisher information matrix standard plan type i censoring;fatigue testing;matrix inversion;fisher information matrix;matrix inversion life testing fatigue testing statistical distributions;statistical distributions;life testing;titanium alloy;parameter estimation	This article discusses tools for planning life tests under the random fatigue-limit model, when the form of the fatigue-limit distribution is completely specified. Expressions for the Fisher information matrix elements are provided. Test-plan objectives, such as minimum variance of quantile or parameter estimator, can be written in terms of these expressions. Under Type I censoring, all experimental conditions can be described by a standardized slope parameter & a standardized log(censoring time). This article provides equivalence theorems, which can be used to check the optimality of test plans which often have two levels. The best three-level standard test plans are not optimal, but, in general, involve fewer extrapolations to low stresses than the optimal plans. Test plans for a titanium-alloy fatigue test are presented to illustrate the methods.	censoring (statistics);fisher information;formation matrix;test plan;turing completeness	Francis G. Pascual	2004	IEEE Transactions on Reliability	10.1109/TR.2004.829140	probability distribution;reliability engineering;econometrics;minimum-variance unbiased estimator;titanium alloy;fisher information;mathematics;estimation theory;statistics	ML	30.7060721183486	-19.858172742730194	128861
f5edd36dafb08797b4619b0e222573f3040bc8e0	relationship between brier score and area under the binormal roc curve	correlacion;statistique;signal theory;population;area debajo de la curva;probability;aplicacion medical;detection signal;decision aid;article author;normal distribution;receiver operator characteristic;aire sous la courbe;signal detection;medical decision making;hombre;parameterization;ayuda decision;probabilistic judgments;statistical model;interpretacion probabilista;parametrizacion;epidemiology;probabilistic interpretation;area under the curve;deteccion senal;interpretation probabiliste;receiver operating characteristic curves;epidemiologia;probabilidad;human;probabilite;poblacion;modele statistique;statistics;aide decision;roc curve;signal detection theory;modelo estadistico;medical application;correlation;metodo roc;methode roc;teoria senal;theorie signal;parametrisation;receiver operating characteristic roc curve;estadistica;homme;epidemiologie;application medicale;brier score	If we consider the Brier score (B) in the context of the signal detection theory and assume that it makes sense to consider the existence of B as a parameter for the population (let B be this B), and if we assume that the calibration in the observer's probability estimate is perfect, we find that there is a theoretical relationship between B and the area under the binormal receiver operating characteristic (ROC) curve, A(Z). We have derived this theoretical functional relationship between B and A(Z), by using the parameter a and b in the binormal ROC model and the prior probability of signal events (alpha); here, the two underlying normal distributions are N and N; and, a= and b=. We empirically found that, if parameters b and alpha are constant, B values in relation to given A(Z) values monotonically decrease as A(Z) values increase, and these relationship curves have monotonically decreasing slopes.	detection theory;population parameter;receiver operating characteristic;receiver operator characteristics;slope	Mitsuru Ikeda;Takeo Ishigaki;Kazunobu Yamauchi	2002	Computer methods and programs in biomedicine	10.1016/S0169-2607(01)00157-2	econometrics;epidemiology;mathematics;receiver operating characteristic;algorithm;statistics;detection theory	ML	34.89298134922892	-21.54642564207048	129146
e3f99e8bb355cab842d26b475650f6f70522c97f	on sufficient conditions for mean residual life and related orders	generalized order statistics;reversed hazard rate order;mean inactivity time order;mean residual life order;hazard rate order	A well known sufficient condition for the mean residual life order of two random variables is the hazard rate order of the two random variables. The hazard rate order is characterized by the monotonicity of the ratio of the two survival functions. However in many cases this ratio is non monotone and the hazard rate order does not hold. The purpose is to show that, in some situations, this non monotonicity is still a sufficient condition for the mean residual life order, under some additional mild conditions. Applications to compare some parametric models of distributions and generalized order statistics are provided. Similar results are given for the mean inactivity time order. Highlights? We give conditions for the mean residual life order based on the ratio of the survival functions. ? The result can be easily applied to provide results for parametric models and ordered data. ? Similar results are given for the mean inactivity time.		Félix Belzunce;Carolina Martínez-Riquelme;José-María Ruiz	2013	Computational Statistics & Data Analysis	10.1016/j.csda.2012.12.005	econometrics;mathematical optimization;mathematics;statistics	ML	31.581696568239042	-18.88703704346669	129160
09b7fb04d7d204f0259eed129b84e474eacb14dd	decision confidence and uncertainty in diffusion models with partially correlated neuronal integrators	correlacion;metodo estadistico;calcul neuronal;time varying;neural computation;proceso difusion;incertidumbre;uncertainty;tiempo reaccion;62h20;exact solution;processus diffusion;hombre;prise de decision;statistical method;solucion exacta;difusion;temps reaction;marginal distribution;methode statistique;integrator;partial correlation;integrador;analyse correlation;human;ley marginal;diffusion process;incertitude;correlation;solution exacte;reseau neuronal;toma decision;diffusion;red neuronal;computacion neuronal;loi marginale;diffusion model;reaction time;analisis correlacion;human decision making;integrateur;homme;neural network;correlation analysis	Diffusion models have become essential for describing the performance and statistics of reaction times in human decision making. Despite their success, it is not known how to evaluate decision confidence from them. I introduce a broader class of models consisting of two partially correlated neuronal integrators with arbitrarily time-varying decision boundaries that allow a natural description of confidence. The dependence of decision confidence on the state of the losing integrator, decision time, time-varying boundaries, and correlations is analytically described. The marginal confidence is computed for the half-anticorrelated case using the exact solution of the diffusion process with constant boundaries and compared to that of the independent and completely anticorrelated cases.	decision making;marginal model	Rubén Moreno-Bote	2010	Neural Computation	10.1162/neco.2010.12-08-930	artificial intelligence;calculus;mathematics;diffusion;artificial neural network;weighted sum model;statistics	ML	35.7990013855612	-21.248378398517602	129471
b73ff5e3f99edc4b83dc68526eab91e1f8b41b60	on-line estimation of probabilities for distributed bayesian detection	distributed system;estimacion;systeme reparti;probability;learning;on line estimation;on line;optimal decision;en linea;distributed detection;teoria equipo;team theory;bayesian detection;aprendizaje;regle decision;decision optimale;apprentissage;sistema repartido;estimation;probabilidad;probabilite;en ligne;theorie equipe;regla decision;sensor fusion;deteccion bayes;bayes detection;decision rule;detection bayes;decision optimal	Abstract   A learning distributed detection system is investigated. It is comprised of a bank of local detectors and a data fusion center (DFC). Each local detector chooses one of two hypotheses on the basis of collected measurements to minimize a local Bayesian cost. The data fusion center combines the local decisions into a central decision with the objective of minimizing a centralized Bayesian cost. The optimal decision rules require that the local detectors and the DFC know the  a priori  probabilities of the hypotheses. The DFC should also know the probabilities of false alarm and missed detection for each local detector. These, however, are often unknown. In this study recursive estimators are developed that approximate the probabilities on line. The estimators are based on the evaluation of the unconditional and conditional means of the local and central decisions (conditioned on the hypothesis). Bias correction is applied to the estimates to account partially for the fact that all decision-makers in the system are occasionally erroneous. For ‘ignorant’ distributed detection systems in low signal-to-noise ratios, the bias reduction is shown to significantly improve the overall detection performance.		Ari Naim;Moshe Kam	1994	Automatica	10.1016/0005-1098(94)90152-X	econometrics;estimation;optimal decision;machine learning;probability;decision rule;mathematics;sensor fusion;statistics	ML	25.56706963178164	-19.575994388148747	129495
d862f36245ed5493477cdd9ab766ed635875a655	shewhart and ewma t control charts for short production runs	non central student s t distribution;short run production;control chart;initial setup;ewma	Abstract#R##N##R##N#Short-run productions are common in manufacturing environments like job shops, which are characterized by a high degree of flexibility and production variety. Owing to the limited number of possible inspections during a short run, often the Phase I control chart cannot be performed and correct estimates for the population mean and standard deviation are not available. Thus, the hypothesis of known in-control population parameters cannot be assumed and the usual control chart statistics to monitor the sample mean are not applicable. t-charts have been recently proposed in the literature to protect against errors in population standard deviation estimation due to the limitation of available sampling measures. In this paper the t-charts are tested for implementation in short production runs to monitor the process mean and their statistical properties are evaluated. Statistical performance measures properly designed to test the chart sensitivity during short runs have been considered to compare the performance of Shewhart and EWMA t-charts. Two initial setup conditions for the short run fixing the population mean exactly equal to the process target or, alternatively, introducing an initial setup error influencing the statistic distribution have been modelled. The numerical study considers several out-of-control process operating conditions including one-step shifts for the population mean and/or standard deviation. The obtained results show that the t-charts can be successfully implemented to monitor a short run. Finally, an illustrative example is presented to show the use of the investigated t charts. Copyright © 2010 John Wiley & Sons, Ltd.	chart	Giovanni Celano;Philippe Castagliola;Enrico Trovato;Sergio Fichera	2011	Quality and Reliability Eng. Int.	10.1002/qre.1121	ewma chart;econometrics;control chart;engineering;operations management;shewhart individuals control chart;x-bar chart;statistics	OS	28.145191954430416	-19.24030574217384	129757
3b59124f04e94662243ce34f9e3f23c4c86c864b	importance mixing: improving sample reuse in evolutionary policy search methods		Deep neuroevolution, that is evolutionary policy search methods based on deep neural networks, have recently emerged as a competitor to deep reinforcement learning algorithms due to their better parallelization capabilities. However, these methods still suffer from a far worse sample efficiency. In this paper we investigate whether a mechanism known as ”importance mixing” can significantly improve their sample efficiency. We provide a didactic presentation of importance mixing and we explain how it can be extended to reuse more samples. Then, from an empirical comparison based on a simple benchmark, we show that, though it actually provides better sample efficiency, it is still far from the sample efficiency of deep reinforcement learning, though it is more stable.		Aloïs Pourchot;Nicolas Perrin;Olivier Sigaud	2018	CoRR		machine learning;artificial intelligence;reinforcement learning;artificial neural network;reuse;mathematics;neuroevolution	AI	26.323015558769118	-12.933791596517663	130134
030b65b67e32f716f418fb2a3b58bcb2a806bf96	posterior inference on parameters of stochastic differential equations via non-linear gaussian filtering and adaptive mcmc	stochastic differential equation;adaptive markov chain monte carlo;non linear kalman filter;gaussian approximation;parameter estimation	This article is concerned with Bayesian estimation of parameters in non-linear multivariate stochastic differential equation (SDE) models occurring, for example, in physics, engineering, and financial applications. In particular, we study the use of adaptive Markov chain Monte Carlo (AMCMC) based numerical integration methods with nonlinear Kalman-type approximate Gaussian filters for parameter estimation in non-linear SDEs. We study the accuracy and computational efficiency of gradient-free sigma-point approximations (Gaussian quadratures) in the context of parameter estimation, and compare them with Taylor series and particle MCMC approximations. The results indicate that the sigma-point based Gaussian approximations lead to better approximations of the parameter posterior distribution than the Taylor series, and the accuracy of the approximations is comparable to that of the computationally significantly heavier particle MCMC approximations.	approximation algorithm;estimation theory;gaussian blur;gradient;markov chain monte carlo;monte carlo method;nonlinear system;numerical analysis;numerical integration	Simo Särkkä;Jouni Hartikainen;Isambi S. Mbalawata;Heikki Haario	2015	Statistics and Computing	10.1007/s11222-013-9441-1	econometrics;mathematical optimization;stochastic differential equation;mathematics;estimation theory;statistics	ML	32.45813743156891	-16.302190444914746	130201
5df3ef384e432697c20958a81d943a8c5ff0815c	genetic reinforcement learning through symbiotic evolution for fuzzy controller design	genetic reinforcement learning;magnetic levitation;fuzzy controller design;learning symbiosis fuzzy control design methodology fuzzy sets algorithm design and analysis temperature control genetic algorithms differential equations input variables;learning algorithm;fuzzy controller;fuzzy set;symbiosis;learning;fitness value;input variables;fuzzy rules;reinforcement learning;temperature control;fuzzy control;indexing terms;fuzzy sets;genetics;symbiotic evolution;control problem;design method;control system synthesis;genetic reinforcement;simulated control problems;membership function;fuzzy controllers;genetic algorithms;differential equations;linear equations;learning artificial intelligence;simulated control problems reinforcement learning symbiotic evolution fuzzy controller design genetic reinforcement learning fuzzy controllers;control system synthesis learning artificial intelligence magnetic levitation fuzzy control genetic algorithms;article;algorithm design and analysis;ge netic algorithm;fuzzy system;tsk type fuzzy rules;fuzzy partition;design methodology	An efficient genetic reinforcement learning algorithm for designing fuzzy controllers is proposed in this paper. The genetic algorithm (GA) adopted in this paper is based upon symbiotic evolution which, when applied to fuzzy controller design, complements the local mapping property of a fuzzy rule. Using this Symbiotic-Evolution-based Fuzzy Controller (SEFC) design method, the number of control trials, as well as consumed CPU time, are considerably reduced when compared to traditional GA-based fuzzy controller design methods and other types of genetic reinforcement learning schemes. Moreover, unlike traditional fuzzy controllers, which partition the input space into a grid, SEFC partitions the input space in a flexible way, thus creating fewer fuzzy rules. In SEFC, different types of fuzzy rules whose consequent parts are singletons, fuzzy sets, or linear equations (TSK-type fuzzy rules) are allowed. Further, the free parameters (e.g., centers and widths of membership functions) and fuzzy rules are all tuned automatically. For the TSK-type fuzzy rule especially, which put the proposed learning algorithm in use, only the significant input variables are selected to participate in the consequent of a rule. The proposed SEFC design method has been applied to different simulated control problems, including the cart-pole balancing system, a magnetic levitation system, and a water bath temperature control system. The proposed SEFC has been verified to be efficient and superior from these control problems, and from comparisons with some traditional GA-based fuzzy systems.	biological evolution;central processing unit;complement system proteins;controllers;entity name part qualifier - adopted;equilibrium;fuzzy control system;fuzzy rule;fuzzy set;genetic algorithm;linear equation;reinforcement learning;rule (guideline);software release life cycle;water baths	Chia-Feng Juang;Jiann-Yow Lin;Chin-Teng Lin	2000	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.836377	defuzzification;design methods;adaptive neuro fuzzy inference system;fuzzy transportation;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;control theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Robotics	26.78224788502339	-10.864092287180888	130884
a61c1ff90d6222b047133997269598c19f872852	evolutionary swarm robotics approach to a pursuit problem	topology;predator robot controller evolutionary swarm robotics approach pursuit problem distributed artificial intelligence research coordination mechanism continuous torus field prey robot evolving artificial neural network eann computer simulation artificial evolution;swarm intelligence control engineering computing evolutionary computation mobile robots multi robot systems neural nets predator prey systems;robot kinematics artificial neural networks topology sociology statistics;artificial neural networks;statistics;sociology;robot kinematics	The pursuit problem is a conventional benchmark in distributed artificial intelligence research. The focal point of previous work in this domain has been the development of coordination mechanisms for predators that cooperatively hunt prey in a typically discrete grid world. This paper investigates a pursuit problem in a continuous torus field on the basis of swarm robotics. Twenty predator robots and three prey robots, each of which can be hunted by multiple predators, are assumed. Predators have a controller represented by evolving artificial neural networks (EANNs), and prey have a predetermined behavior rule for escaping predators. A series of computer simulations were conducted to compare three types of EANNs to determine the efficient artificial evolution of the predator robot controllers.	ap computer science a;artificial neural network;benchmark (computing);computer simulation;distributed artificial intelligence;evolutionary algorithm;focal (programming language);neat chipset;prey;pursuit-evasion;robot;swarm robotics	Toshiyuki Yasuda;Kazuhiro Ohkura;Tosei Nomura;Yoshiyuki Matsumura	2014	2014 IEEE Symposium on Robotic Intelligence in Informationally Structured Space (RiiSS)	10.1109/RIISS.2014.7009182	swarm robotics;simulation;computer science;artificial intelligence;machine learning;evolutionary robotics	Robotics	25.185979490314953	-11.917564233216023	130932
952560a711a4f87968b1b3ace2e61df20aba4866	a probabilistic estimation and prediction technique for dynamic continuous social science models: the evolution of the attitude of the basque country population towards eta as a case study	probabilistic estimation;social dynamic models;articulo;attitude dynamics;probabilistic prediction	In this paper, a computational technique to deal with uncertainty in dynamic continuous models in Social Sciences is presented. Considering data from surveys, the method consists of determining the probability distribution of the survey output and this allows to sample data and fit the model to the sampled data using a goodness-of-fit criterion based on the χ-test. Taking the fitted parameters that were not rejected by the χ-test, substituting them into the model and computing their outputs, 95% confidence intervals in each time instant capturing the uncertainty of the survey data (probabilistic estimation) is built. Using the same set of obtained model parameters, a prediction over the next few years with 95% confidence intervals (probabilistic prediction) is also provided. This technique is applied to a dynamic social model describing the evolution of the attitude of the Basque Country population towards the revolutionary organization ETA.	social welfare model	Juan Carlos Cortés;Francisco-José Santonja;Ana C. Tarazona;Rafael J. Villanueva;Javier Villanueva-Oller	2015	Applied Mathematics and Computation	10.1016/j.amc.2015.03.128	econometrics;probabilistic relevance model;computer science;data mining;statistics;divergence-from-randomness model	ML	26.926969229656688	-22.491680115838058	131522
d351c502ed583f68cec2c7bd1f1459b3975e3e5d	updating mean and variance estimates: an improved method	standard deviation;updating estimates;mean;variance estimation;variance;removing data	A method of improved efficiency is given for updating the mean and variance of weighted sampled data when an additional data value is included in the set. Evidence is presented that the method is stable and at least as accurate as the best existing updating method.		D. H. D. West	1979	Commun. ACM	10.1145/359146.359153	econometrics;pattern recognition;variance;standard deviation;one-way analysis of variance;statistics;mean	Graphics	29.48435492122425	-23.07786272022068	131674
a15a5156f8a7a52e8573e4493d0cd07035cfdc6c	efficiency criteria for environmental model quality assessment: a review and its application to wastewater treatment	dissimilarity between criteria;model quality assessment;parameter estimation;modeling objective	In various cases in environmental modeling, modelers need to account for multiple variables and multiple objectives in systems with many processes occurring at different time scales. To assist the modeler to choose a relevant pool of efficiency criteria, a method is proposed to identify dissimilar criteria. A total of 30 efficiency criteria used in environmental modeling are critically reviewed and classified into six groups according to different modeling objectives. After accounting for equivalence of functional form 18 criteria remain for further analysis. To quantify the dissimilarity for the remaining criteria a methodology based on the ratio of shared parameter sets in regions of good performance is proposed. Then, for a wastewater treatment plant case-study the dissimilarity of efficiency criteria is analyzed as a function of target variables and operating conditions. © 2015 Elsevier Ltd. All rights reserved.	approximation error;compiler;design of experiments;higher-order function;turing completeness	H. Hauduc;Marc B. Neumann;Dirk Muschalla;V. Gamerith;S. Gillot;Peter A. Vanrolleghem	2015	Environmental Modelling and Software	10.1016/j.envsoft.2015.02.004	reliability engineering;econometrics;engineering;mathematics;estimation theory;statistics	AI	27.020793842987057	-17.806704542053552	131756
225d60224fed940bcba584a935736a1084f00e85	a survey of current methods for the elimination of initialization bias in digital simulation	replicated data;digital simulation;steady state	Initialization bias in digital simulation typically arises in estimating a steady-state statistic from replicated data. While methods have been developed to avoid this bias, such as batch means, the problem remains in some simulation contexts. This report surveys current methods for dealing with this bias and assesses their effectiveness and usefulness.	simulation;steady state;utility	Delbert L. Kimbler;Barry D. Knight	1987		10.1145/41824.41834	computer science;data mining;steady state;statistics	AI	27.75812108684156	-21.43571316744805	131802
07262334a7dd37c3abb788df089cab835ba98fb1	notes on shift effects for t2-type charts on multivariate arma residuals	moving average process;vector autoregression;tecnologia industrial tecnologia mecanica;spc;computacion informatica;vector arma;statistical process control;mean shift;dynamic system;residual charts;ciencias basicas y experimentales;tecnologias;grupo a;shift effects	Statistical process control (SPC) needs to be aided by computers in order to deal with dynamic systems. Hence, more knowledge on the complexity of this issue is needed. This paper discusses in general the shift effects of residuals from vector autoregressive moving average process for Shewhart-type, i.e. Hotelling T2-type charts (we call it H charts). Three types of parameter shift were considered: mean shift, covariance shift, and coefficient shift. The estimation effects were addressed. The discussions begin with the shift effects for residuals then for T2-type chart on residuals. The out-of-control distributions of the chart statistic were provided in this paper.	chart	Xia Pan	2005	Computers & Industrial Engineering	10.1016/j.cie.2005.07.001	econometrics;engineering;operations management;mathematics;statistical process control;statistics	SE	28.27781581343001	-19.65766678964019	131817
9be1306d8f2c31f105f83d8b0047282d7f47e643	learning in quantum control: high-dimensional global optimization for noisy quantum dynamics		Quantum control is valuable for various quantum technologies such as highfidelity gates for universal quantum computing, adaptive quantum-enhanced metrology, and ultra-cold atom manipulation. Although supervised machine learning and reinforcement learning are widely used for optimizing control parameters in classical systems, quantum control for parameter optimization is mainly pursued via gradient-based greedy algorithms. Although the quantum fitness landscape is often compatible for greedy algorithms, sometimes 1 ar X iv :1 60 7. 03 42 8v 2 [ cs .L G ] 1 6 N ov 2 01 6 greedy algorithms yield poor results, especially for large-dimensional quantum systems. We employ differential evolution algorithms to circumvent the stagnation problem of non-convex optimization, and we average over the objective function to improve quantum control fidelity for noisy systems. To reduce computational cost, we introduce heuristics for early termination of runs and for adaptive selection of search subspaces. Our implementation is massively parallel and vectorized to reduce run time even further. We demonstrate our methods with two examples, namely quantum phase estimation and quantum gate design, for which we achieve superior fidelity and scalability than obtained using greedy algorithms.	algorithmic efficiency;coherent control;convex function;convex optimization;differential evolution;error detection and correction;global optimization;gradient;graphics processing unit;greedy algorithm;heuristic (computer science);image noise;information processing;machine learning;mathematical optimization;optimization problem;quantum computing;quantum dynamics;quantum error correction;quantum gate;quantum information science;quantum phase estimation algorithm;quantum state;quantum system;qubit;random number generation;reinforcement learning;run time (program lifecycle phase);sql;samsung sgr-a1;scalability;severo ornstein;simulation;supervised learning;system dynamics;ultracold atom;westgrid	Pantita Palittapongarnpim;Peter Wittek;Ehsan Zahedinejad;Barry C. Sanders	2017	Neurocomputing	10.1016/j.neucom.2016.12.087	greedy randomized adaptive search procedure;mathematical optimization;theoretical computer science;machine learning;mathematics;quantum phase estimation algorithm;quantum sort	Theory	25.09961993468954	-14.716951134325983	131897
0726554933595fc7abc847748196bdaf487d2d08	evaluating the water sector in italy through a two stage method using the conditional robust nonparametric frontier and multivariate adaptive regression splines	conditional oder m efficiency;modelizacion;productive frontier efficiency conditional oder m efficiency two stage methods multivariate adaptive regression splines water sector;metodo adaptativo;production function;funcion produccion;entreprise;functional form;two stage methods;empresa;methode adaptative;modelisation;aproximacion esplin;efficient frontier;spline approximation;approximation spline;adaptive method;firm;water sector;productive frontier efficiency;low density;multivariate adaptive regression splines;point of view;fonction production;modeling	The aim of this paper is to assess the efficiency of the integrated water service in Italy in recent years, through a robust and flexible methodology. This paper, from a methodological point of view, enhances a ’’two stage’’ method, based on ideas suggested by Florens and Simar (2005), which estimates the efficiency frontier through conditional robust models and bypasses, at the same time, the choice of a specific functional form in the second stage; the MARS (Multivariate Adaptive Regression splines) method, in fact, provides for approximate production function using linear splines without any assumption of a func-	approximation algorithm;biological anthropology;higher-order function;multivariate adaptive regression splines;simulation;smoothing spline;spline (mathematics)	Francesco Vidoli	2011	European Journal of Operational Research	10.1016/j.ejor.2011.02.003	efficient frontier;econometrics;systems modeling;economics;multivariate adaptive regression splines;computer science;operations management;production function;higher-order function;statistics	Vision	25.316448824282837	-19.906313111920646	132113
b9644bf41e3acab49368f162f10276bcb3539b6b	modified normal-based approximation to the percentiles of linear combination of independent random variables with applications	relative risk;modified large sample method;62e17;ratio of odds;fiducial approach;coverage probability;doubly noncentral f;mover;60e05	A modified normal-based approximation for calculating the percentiles of a linear combination of independent random variables is proposed. This approximation is applicable in situations where expectations and percentiles of the individual random variables can be readily obtained. The merits of the approximation are evaluated for the chi-square and beta distributions using Monte Carlo simulation. An approximation to the percentiles of the ratio of two independent random variables is also given. Solutions based on the approximations are given for some classical problems such as interval estimation of the normal coefficient of variation, survival probability, the difference between or the ratio of two binomial proportions, and for some other problems. Furthermore, approximation to the percentiles of a doubly noncentral F distribution is also given. For all the problems considered, the approximation provides simple satisfactory solutions. Two examples are given to show applications of the approximation.	approximation	K. Krishnamoorthy	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.904342	relative risk;econometrics;mathematical optimization;mathematics;statistics	Theory	30.26375640844562	-21.470783675970818	132419
9b535e0f2ceddb002a0681af351a153b05740518	review of statistical network analysis: models, algorithms, and software	mixed membership stochastic blockmodel;latent space model;ergm;blockmodels;mathematics and statistics;erdős renyi;block models;journal article;latent position cluster model;p2;social network analysis;p1;economics	The analysis of network data is an area that is rapidly growing, both within and outside of the discipline of statistics. This review provides a concise summary of methods and models used in the statistical analysis of network data, including the Erdős-Renyi model, the exponential family class of network models and recently developed latent variable models. Many of the methods and models are illustrated by application to the well-known Zachary karate dataset. Software routines available for implementing methods are emphasised throughout. The aim of this paper is to provide a review with enough detail about many common classes of network model to whet the appetite and to point the way to further reading. Corresponding Author Clique Strategic Research Cluster, School of Mathematical Sciences & Complex and Adaptive Systems Laboratory, University College Dublin, Dublin 4, Ireland. This material is based upon work supported by the Science Foundation Ireland under Grant No. 08/SRC/I1407: Clique: Graph & Network Analysis Cluster	adaptive system;algorithm;clique graph;complex and adaptive systems laboratory;erdős number;latent variable;network model;social network analysis;time complexity;whole earth 'lectronic link;zachary lemnios	Michael Salter-Townshend;Arthur J. White;Isabella Gollini;Thomas Brendan Murphy	2012	Statistical Analysis and Data Mining	10.1002/sam.11146	econometrics;social network analysis;variable-order bayesian network;computer science;data science;machine learning;data mining;mathematics;statistics	ML	25.38357169816097	-23.85727218870566	132587
c53bf607fd24e89abf24158a7067bfab133ce52c	regularized simultaneous model selection in multiple quantiles regression	62 07;metodo regularizacion;metodo estadistico;selection problem;model selection;analisis numerico;problema seleccion;62p20;problema mal planteado;cuantila;regularisation;methode parametrique;analisis datos;quantile regression;conditional quantile;economic sciences;metodo parametrico;regression quantile;regularization method;parametric method;probleme mal pose;simulation;estimacion promedio;metodo penalidad;methode regularisation;loi conditionnelle;econometria;simulacion;estadistica rango;statistical method;selection modele;ley condicional;statistical regression;62jxx;analyse numerique;regularization;estimation parametrique;62f07;ciencias economicas;data analysis;methode selection;penalty method;methode penalite;numerical analysis;seleccion modelo;methode statistique;regresion multiple;regresion estadistica;ill posed problem;statistical computation;calculo estadistico;algebra lineal numerica;algebre lineaire numerique;covariate;65f22;rank statistic;analyse donnee;statistique rang;calcul statistique;covariable;regularizacion;sciences economiques;quantile;econometrics;numerical linear algebra;mean estimation;estimation statistique;selection method;gene expression data analysis;regression statistique;estimation moyenne;estimacion estadistica;statistical estimation;regression multiple;conditional distribution;econometrie;probleme selection;multiple regression	Simultaneously estimating multiple conditional quantiles is often regarded as a more appropriate regression tool than the usual conditional mean regression for exploring the stochastic relationship between the response and covariates. When multiple quantile regressions are considered, it is of great importance to share strength among them. In this paper, we propose a novel regularization method that explores the similarity among multiple quantile regressions by selecting a common subset of covariates to model multiple conditional quantiles simultaneously. The penalty we employ is a matrix norm that encourages sparsity in a column-wise fashion. We demonstrate the effectiveness of the proposed method using both simulations and an application of gene expression data analysis. © 2008 Elsevier B.V. All rights reserved.	ibm notes;model selection;simulation;sparse matrix;taxicab geometry	Hui Zou;Ming Yuan	2008	Computational Statistics & Data Analysis	10.1016/j.csda.2008.05.013	conditional probability distribution;regularization;econometrics;quantile;quantile regression;covariate;numerical analysis;linear regression;calculus;penalty method;mathematics;numerical linear algebra;data analysis;model selection;regression analysis;statistics	AI	33.195365353417884	-23.259541875676398	132643
6d37e8900a44e575f0ba11fca9dfc0eb9bd788c3	on the noninvertibility of adaptive digital watermarking	watermarking cryptography reverse engineering robustness computer science motion pictures copyright protection degradation resists data security;watermarking;digital watermark;watermarking attack noninvertibility digital watermarking adaptive watermarking reverse engineering scaled watermark cryptography public watermarking copyright protection copyright management systems;copyright;cryptography watermarking copyright reverse engineering security of data inverse problems;cryptography;security of data;inverse problems;reverse engineering	This paper is intended to demonstrate that the widely researched adaptive watermarking is noninvertible in nature if it can induce and exploit the perturbation in an attacker's reverse engineering properly. The intrinsic noninvertibility of some adaptive watermarking is disclosed and evaluated. Under the technologies that exploit it, attackers have great difficulty dividing a released version into their claimed original and scaled watermark, and in the meantime making the latter be the particular adaptive result based on the former. Their reversed solutions are drastically perturbed and perceptually unacceptable. Compared with the existing noninvertible method based on cryptography, the proposed method is more efficient because it disposes of the cryptographic processing. Furthermore, it also shows a promising way of making a public watermarking scheme, which is considered intrinsically invertible, noninvertible.	cryptography;digital watermarking;reverse engineering;well-posed problem	Xianfeng Zhao;Weinong Wang;Kefei Chen	2003		10.1109/DEXA.2003.1232046	watermarking attack;digital watermarking alliance;digital watermarking;computer science;theoretical computer science;internet privacy;computer security	Security	38.11925105192094	-12.05027713812433	132728
1a7a98a708f30e2a3f9efcab838e0c002b3111bc	dual watermarking method for integrity of medical images	digital watermarking;watermarking;robust watermarking;dual watermarking method;digital watermark;medical image protection dual watermarking method medical image integrity region of interest robust watermark;biomedical imaging;interference;medical image integrity;medical image;watermarking biomedical imaging robustness medical diagnostic imaging protection image storage leak detection cryptography interference privacy;image edge detection;cryptography;region of interest;watermarking biomedical imaging;digital watermarking medical image;robustness;medical image protection;robust watermark;medical diagnostic imaging	As medical images are created, displayed, transmitted or stored in a digital form, there has been a growing interest in protecting the medical images against external/internal attackers. In this paper, we propose a dual watermarking method(DWM) to protect medical images in transmission/storage. As the proposed DWM provides both robustness and fragileness with the embedded watermarks, it can guarantee the integrity of the medical image transmitted and/or stored. In DWM, watermarks are carefully embedded avoiding the areas of region of interest(ROI) and the edge of the contents to protect the integrity of the medical image. Based on experimental results, we confirm that our DWM can detect the robust watermark accurately and detect the intentional/unintentional leakage of the transmitted or stored medical image.	digital watermarking;embedded system;interference (communication);medical imaging;region of interest;secure transmission;spectral leakage	Sung Jin Lim;Hae-Min Moon;Seung-Hoon Chae;Sung Bum Pan;Yongwha Chung;Min Hyuk Chang	2008	2008 Second International Conference on Future Generation Communication and Networking	10.1109/FGCN.2008.213	computer vision;computer science;internet privacy;computer security	EDA	38.29725097472345	-11.511186993992178	132850
d8c47d4eb94d882e9e4b7887415bc053cef7974e	metamodeling using extended radial basis functions: a comparative approach	sample size;multidisciplinary design optimization;computation fluid dynamics;response surfaces;finite element;simulation based design;large scale;sampling technique;radial basis function;computational complexity;optimization;metamodeling;response surface;computer simulation;radial basis functions	The process of constructing computationally benign approximations of expensive computer simulation codes, or metamodeling, is a critical component of several large-scale multidisciplinary design optimization (MDO) approaches. Such applications typically involve complex models, such as finite elements, computational fluid dynamics, or chemical processes. The decision regarding the most appropriate metamodeling approach usually depends on the type of application. However, several newly proposed kernel-based metamodeling approaches can provide consistently accurate performance for a wide variety of applications. The authors recently proposed one such novel and effective metamodeling approach—the extended radial basis function (E-RBF) approach—and reported highly promising results. To further understand the advantages and limitations of this new approach, we compare its performance to that of the typical RBF approach, and another closely related method—kriging. Several test functions with varying problem dimensions and degrees of nonlinearity are used to compare the accuracies of the metamodels using these metamodeling approaches. We consider several performance criteria such as metamodel accuracy, effect of sampling technique, effect of sample size, effect of problem dimension, and computational complexity. The results suggest that the E-RBF approach is a potentially powerful metamodeling approach for MDO-based applications, as well as other classes of computationally intensive applications.	approximation;code;computational complexity theory;computational fluid dynamics;computer simulation;distribution (mathematics);finite element method;kriging;mathematical optimization;metamodeling;multidisciplinary design optimization;nonlinear system;radial (radio);radial basis function kernel;sampling (signal processing)	Anoop A. Mullur;Achille Messac	2005	Engineering with Computers	10.1007/s00366-005-0005-7	computer simulation;mathematical optimization;radial basis function;computer science;theoretical computer science;machine learning;mathematics;algorithm	PL	27.208447249800745	-14.658791962470001	132924
69462e9a2b7f938871d46d81d5714210df96dbf5	face template protection via random permutation maxout transform	face recognition;cancellable biometrics;template protection;rpm transform	Face template protection is of great interest nowadays due to the increasing concerns on privacy and security of the face templates stored in the databases. There were many attempts to develop plausible face template protection schemes that can satisfy four design criteria of biometric template protection, namely non-invertibility, cancellability, non-linkability and performance. In this paper, a cancellable face template scheme, namely random permutation maxout (RPM) transform is proposed. The RPM transforms a real-valued face feature vector (template) into a discrete index code as a means of protected form of face template. Such a transform offers two major merits: 1) robustness to noises in numeric values of original face template; and 2) nonlinear embedding based on the implicit order of the data. The former promotes accuracy performance preservation while the latter offers strong non-invertible transformation that leads to hardness in inversion attack. Several experiments based on the AR face database are conducted to observe the RPM transform performance with respect to its various parameters. The analyses justify its resilience to inversion attack as well as satisfy the revocability and non-linkability criteria of cancellable biometrics.	biometrics;database;experiment;feature vector;implicate and explicate order;nonlinear system;privacy;random permutation	Sejung Cho;Andrew Beng-Jin Teoh	2017		10.1145/3077829.3077833	facial recognition system;computer vision;speech recognition;computer science;data mining;computer security	Security	36.52525574195803	-11.462469910083149	133171
8bd5503527482234395c3a9347cd439572524811	a novel quota sampling algorithm for generating representative random samples given small sample size	random sampling;probability proportional to size sampling;stochastic optimization;discrete probability	In this paper, a novel algorithm is proposed for sampling from discrete probability distributions using the probability proportional to size sampling method, which is a special case of Quota sampling method. The motivation for this study is to devise an efficient sampling algorithm that can be used in stochastic optimization problems --when there is a need to minimize the sample size. Several experiments have been conducted to compare the proposed algorithm with two widely used sample generation methods, the Monte Carlo using inverse transform, and quasi-Monte Carlo algorithms. The proposed algorithm gave better accuracy than these methods, and in terms of time complexity it is nearly of the same order.	algorithm	Ahmed M. Fouad;Mohamed Saleh;Amir F. Atiya	2013	IJSDA	10.4018/ijsda.2013010105	systematic sampling;metropolis–hastings algorithm;sampling;econometrics;mathematical optimization;simple random sample;sampling design;bernoulli sampling;importance sampling;slice sampling;inverse transform sampling;mathematics;rejection sampling;cluster sampling;stratified sampling;umbrella sampling;poisson sampling;statistics;monte carlo method	AI	30.867724521758756	-15.458164259494477	133246
397e85891abf71016a876012e88d89f9df8f34b7	bayesian methods for neural data analysis	bayesian inference probabilistic model generalized linear model spiking neurons;dissertation;fehler in den variablen log lineares modell informationstheoretisches modell bayes inferenz statistische schlussweise	1		Sebastian Gerwinn	2010			econometrics;computer science;machine learning;statistics	ML	26.497351395604575	-23.51420893021795	133381
1fcbe5a108c77c0398ba8efb062ff5376daf51bb	multiple point statistical simulation using uncertain (soft) conditional data		9 Geostatistical simulation methods have been used to quantify spatial variability of reservoir models since the 80s. In the last two decades, state of the art simulation methods have changed from being based on covariance-based 2-point statistics to multiple-point statistics (MPS), that allow simulation of more realistic Earth-structures. In addition, increasing amounts of geoinformation (geophysical, geological, etc.) from multiple sources are being collected. This pose the problem of integration of these different sources of information, such that decisions related to reservoir models can be taken on an as informed base as possible. In principle, though difficult in practice, this can be achieved using computationally expensive Monte Carlo methods. Here we investigate the use of sequential simulation based MPS simulation methods conditional to uncertain (soft) data, as a computational efficient alternative. First, it is demonstrated that current implementations of sequential simulation based on MPS (e.g. SNESIM, ENESIM and Direct Sampling) do not account properly for uncertain conditional information, due to a combination of using only co-located information, and a random simulation path. Then, we suggest two approaches that better account for the available uncertain information. The first make use of a preferential simulation path, where more informed model parameters are visited preferentially to less informed ones. The second approach involves using non co-located uncertain information. For different types of available data, these approaches are demonstrated to produce simulation results similar to those obtained by the general Monte Carlo based approach. These methods allow MPS simuPreprint submitted to Computers and Geosciences May 22, 2018 lation to condition properly to uncertain (soft) data, and hence provides a computationally attractive approach for integration of information about a reservoir model.	analysis of algorithms;computation;conditional entropy;geographic information system;monte carlo method;simulation;spatial variability	Thomas Mejer Hansen;Le Thanh Vu;Klaus Mosegaard;Knud Skou Cordua	2018	Computers & Geosciences	10.1016/j.cageo.2018.01.017	statistics;implementation;sampling (statistics);machine learning;monte carlo method;computer science;spatial variability;artificial intelligence;covariance	AI	30.44936987336286	-16.463575247835934	133414
e84b9d084793b41f7bfe54bd6906f6451241934c	the k-factor garma process with infinite variance innovations	gegenbauer polynomial;60g18;65c05;62m10;whittle estimation;long memory;markov chains monte carlo;conditional sum squares;stable distributions	In this article, we develop the theory of k-factor Gegenbauer Autoregressive Moving Average (GARMA) process with infinite variance innovations which is a generalization of the stable seasonal fractional Autoregressive Integrated Moving Average (ARIMA) model introduced by Diongue et al. (2008). Stationarity and invertibility conditions of this new model are derived. Conditional Sum of Squares (CSS) and Markov Chains Monte Carlo (MCMC) Whittle methods are investigated for parameter estimation. Monte Carlo simulations are also used to evaluate the finite sample performance of these estimation techniques. Finally, the usefulness of the model is corroborated with the application to streamflow data for Senegal River at Bakel.	cascading style sheets;emoticon;estimation theory;graph factorization;k-factor (cisco);markov chain monte carlo;monte carlo method;nl (complexity);random effects model;seasonality;simulation;stationary process;time series	Abdou Kâ Diongue;Mor Ndongo	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.824095	econometrics;mathematical optimization;long-range dependency;mathematics;statistics	ML	30.987970823295974	-22.58397137139691	133488
e28aee35d46c503ddbc70a0a553e84e1fbb975fb	point and expected interval availability analysis with stationarity detection	steady state probability;fiabilidad;reliability;stationarity detection;homogeneous process;probabilite stationnaire;proceso markov;availability;disponibilidad;processus homogene;algorithme;algorithm;uniformization;temps calcul;proceso homogeneo;detection defaut;processus markov;fiabilite;markov process;random variable;tiempo computacion;computation time;disponibilite;deteccion imperfeccion;defect detection;algoritmo	Interval availability is a dependability measure defined by the fraction of time during which a system is in operation over a finite observation period. The system is assumed to be modeled by a finite Markov process. Because the computation of the distribution of this random variable is very expensive, it is	stationary process	Haïscam Abdallah;Raymond A. Marie;Bruno Sericola	1999	Computers & OR	10.1016/S0305-0548(98)00057-4	uniformization;random variable;availability;combinatorics;calculus;reliability;mathematics;markov process;statistics	Logic	28.01546196124495	-14.964071136258644	133693
4e6a58c6c99502151a1527e81e2a8ab81dafb6a7	on tests of homogeneity based on minimum phi-divergence estimator with constraints	multinomial distribution;metodo estadistico;statistical simulation;maximum likelihood;best estimation;estadistica test;statistique test;maximum vraisemblance;multinomial populations;statistical method;φ divergence statistic;estadistica matematica;test homogeneidad;statistique phi divergence;simulacion estadistica;maximum likelihood estimate;loi multinomiale;phi divergence statistic;methode statistique;simulation statistique;test homogeneite;minimum φ divergence estimator;homogeneity test;simulation study;estimateur minimum phi divergence;mejor estimacion;minimum phi divergence estimator;ley multinomial;meilleure estimation;maxima verosimilitud;test statistic;homogeneity	A family of tests of homogeneity of independent multinomial populations is introduced in terms of the 1-divergence when the parameters are estimated using the minimum 2-divergence estimator instead of the maximum likelihood estimator. A simulation study is presented to choose the best function 2 for estimation and the best function 1 for testing. A new test statistic is obtained, more powerful in some cases, than the existing tests for testing homogeneity in multinomial populations. c © 2003 Elsevier Science B.V. All rights reserved.	chi-squared target models;emoticon;multinomial logistic regression;population;simulation	María Luisa Menéndez;Julio Angel Pardo;Leandro Pardo;Kostas Zografos	2003	Computational Statistics & Data Analysis	10.1016/S0167-9473(02)00207-4	efficient estimator;minimax estimator;econometrics;minimum-variance unbiased estimator;calculus;mathematics;maximum likelihood;estimation theory;statistics	AI	32.52687931002478	-22.798267648536875	133866
2a882b61e32d0b582fe16b81dba9c9fb5cf985d9	a wavelet-based spectral procedure for steady-state simulation analysis	wavelet analysis;processus gauss;metodo espectral;transformacion discreta;discrete wavelet transform;performance evaluation;gaussian processes;batch production;simulation;simulacion;threshold scheme;spectrum;time series;satisfiability;processus stationnaire;confidence interval;produccion por lote;tamano lote;coverage probability;bias correction;taille lot;production par lot;analyse ondelette;serie temporelle;discrete transformation;spectral method;processus gaussien;serie temporal;lot sizing;regime permanent;methode spectrale;simulation analysis;gaussian process;regimen permanente;proceso estacionario;steady state analysis;proceso gauss;transformation discrete;stationary process;steady state	A and an analysis are given for an experimental performance evaluation of WASSP, an automated wavelet-based spectral method for constructing an approximate confidence interval on the steady-state mean of a simulation output process such that the delivered confidence interval satisfies user-specified requirements on absolute or relative precision as well as coverage probability. The experimentation involved three difficult test problems, each with an output process exhibiting some combination of the following characteristics: a long warm-up period, a persistent autocorrelation structure, or a highly nonnormal marginal distribution. These problems were used to compare the performance of WASSP with that of the Heidelberger-Welch algorithm and ASAP3, two sequential procedures based respectively on the methods of spectral analysis and nonoverlapping batch means. Concerning efficiency (required sample sizes) and robustness against the statistical anomalies commonly encountered in simulation studies, WASSP outperformed the Heidelberger-Welch procedure and compared favorably with ASAP3.	application domain;approximation algorithm;autocorrelation;experiment;marginal model;performance evaluation;recommender system;requirement;routing;simulation;smoothing;spectral density;spectral method;steady state;subnetwork;wavelet;welch's method;workstation	Emily K. Lada;James R. Wilson	2006	European Journal of Operational Research	10.1016/j.ejor.2005.04.025	econometrics;calculus;gaussian process;mathematics;steady state;statistics	Metrics	32.863397384846216	-21.257540487498268	134125
dd43a44c3c1142b5809fa3b7e7447eeb582c8e22	informative priors or noninformative priors? a bayesian re-analysis of binary data from macugen phase iii clinical trials	hypergeometric function;beta binomial;frequentist p values;age related macular degeneration;bayesian posterior distribution;monte carlo simulation	It has became more popular in the recent statistical literature to see Bayesian approaches for clinical trials, such as assurance calculations for study designs and the use of posterior probability for data analyses. When applying Bayesian analysis to clinical trial data, one common question is to use informative priors or non-informative priors. In order to explore this question, we looked for existing clinical trial data with a simple structure and a simple clinical endpoint so that the Bayesian re-analyses can be easily performed. We came across the published Phase III Macugen® data that were suitable for this exploration. In this manuscript, the Macugen Phase III development program was described, the primary data for the two Phase III pivotal studies were re-analyzed using Bayesian applications with informative priors and noninformative priors. These re-analysis results were summarized, compared, and discussed. D ow nl oa de d by [ N an ya ng T ec hn ol og ic al U ni ve rs ity ] at 0 3: 22 2 0 Ju ne 2 01 6	binary data;communication endpoint;information;programmer	Ding-Geng Chen;Naitee Ting;Shuyen Ho	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1122049	beta-binomial distribution;econometrics;hypergeometric function;mathematics;statistics;monte carlo method	ML	30.60243174487922	-21.675342685560153	134460
1cc9f5ba5ac9393320840afa9b5e1b97ae076751	an alternative to model selection in ordinary regression	selection model;model selection;synthetic estimator;two stage procedure;mean square error;hypothesis testing;single model based estimator;mean squared error;hypothesis test;multiple regression	The weaknesses of established model selection procedures based on hypothesis testing and similar criteria are discussed and an alternative based on synthetic (composite) estimation is proposed. It is developed for the problem of prediction in ordinary regression and its properties are explored by simulations for the simple regression. Extensions to a general setting are described and an example with multiple regression is analysed. Arguments are presented against using a selected model for any inferences.	model selection	Nicholas T. Longford	2003	Statistics and Computing	10.1023/A:1021995912647	econometrics;statistical hypothesis testing;proper linear model;machine learning;polynomial regression;regression diagnostic;mathematics;mean squared error;statistics	ML	29.101413737300884	-23.8324366088805	134628
9a1416e6bd697b42d90d9ffbc1b42d1cdf94c06d	on rates of convergence for stochastic optimization problems under non--independent and identically distributed sampling	rate of convergence;65c05;latin hypercube sampling;two stage stochastic programming with recourse;stochastic optimization;independent and identically distributed;variance reduction techniques;quasi monte carlo methods;90c15;sample average approximation;monte carlo simulation	In this paper we discuss the issue of solving stochastic optimization problems by means of sample average approximations. Our focus is on rates of convergence of estimators of optimal solutions and optimal values with respect to the sample size. This is a well-studied problem in case the samples are independent and identically distributed (i.e., when standard Monte Carlo simulation is used); here we study the case where that assumption is dropped. Broadly speaking, our results show that, under appropriate assumptions, the rates of convergence for pointwise estimators under a sampling scheme carry over to the optimization case, in the sense that convergence of approximating optimal solutions and optimal values to their true counterparts has the same rates as in pointwise estimation. We apply our results to two well-established sampling schemes, namely, Latin hypercube sampling and randomized quasi-Monte Carlo (QMC). The novelty of our work arises from the fact that, while there has been some work on the use of variance reduction techniques and QMC methods in stochastic optimization, none of the existing work—to the best of our knowledge— has provided a theoretical study on the effect of these techniques on rates of convergence for the optimization problem. We present numerical results for some two-stage stochastic programs from the literature to illustrate the discussed ideas.	approximation algorithm;authorization;closed-world assumption;converge;estimation theory;experiment;gibbs sampling;mathematical optimization;monte carlo method;numerical analysis;optimization problem;quantum monte carlo;quasi-monte carlo method;randomized algorithm;rate of convergence;sampling (signal processing);simulation;smoothing;star catalogue;stochastic optimization;variance reduction	Tito Homem-de-Mello	2008	SIAM Journal on Optimization	10.1137/060657418	independent and identically distributed random variables;stochastic programming;econometrics;mathematical optimization;latin hypercube sampling;stochastic optimization;mathematics;stochastic tunneling;rate of convergence;statistics;monte carlo method	ML	31.673393004637795	-15.87119605678119	134694
294ec97bc2da86781bd109175cc2f9c52f9230d3	mewma control chart and process capability indices for simple linear profiles with within-profile autocorrelation	process capability analysis;profile monitoring;average run length;autocorrelation	A multivariate exponentially weighted moving average (MEWMA) control chart is proposed for detecting process shifts during the phase II monitoring of simple linear profiles (SLPs) in the presence of within-profile autocorrelation. The proposed control chart is called MEWMA-SLP. Furthermore, two process capability indices are proposed for evaluating the capability of in-control SLP processes, and their utilization is demonstrated through examples. Intensive simulations reveal that the MEWMA-SLP chart is more sensitive than existing control charts in detecting profile shifts. Copyright © 2016 John Wiley & Sons, Ltd.	algorithm;apple a5;autocorrelation;chart;computation;cynbe ru taren;imperative programming;institute for operations research and the management sciences;john d. wiley;linear algebra;management science;mike lesser;open-source software;ritz dakota digital;run-length encoding;sensor;simulation;superword level parallelism;visual intercept	Jyun-You Chiang;Yuhlong Lio;Tzong-Ru Tsai	2017	Quality and Reliability Eng. Int.	10.1002/qre.2101	reliability engineering;econometrics;process capability;autocorrelation;mathematics;statistics	SE	28.25766087324189	-19.6410996053147	134765
e117875be2ec99f911c8e8f23258792894018307	test without trust: optimal locally private distribution testing		We study the problem of distribution testing when the samples can only be accessed using a locally differentially private mechanism and focus on two representative testing questions of identity (goodness-of-fit) and independence testing for discrete distributions. We are concerned with two settings: First, when we insist on using an already deployed, general-purpose locally differentially private mechanism such as the popular Rappor or the recently introduced Hadamard Response for collecting data, and must build our tests based on the data collected via this mechanism; and second, when no such restriction is imposed, and we can design a bespoke mechanism specifically for testing. For the latter purpose, we introduce the Randomized Aggregated Private Testing Optimal Response (Raptor) mechanism which is remarkably simple and requires only one bit of communication per sample. We propose tests based on these mechanisms and analyze their sample complexities. Each proposed test can be implemented efficiently. In each case (barring one), we complement our performance bounds for algorithms with information-theoretic lower bounds and establish sample optimality of our proposed algorithm. A peculiar feature that emerges is that our sample-optimal algorithm based on Raptor uses public-coins, and any test based on Rappor or Hadamard Response, which are both private-coin mechanisms, requires significantly more samples. Cornell University. Email: acharya@cornell.edu. Stanford University. Email: ccanonne@cs.stanford.edu. Supported by a Motwani Postdoctoral Fellowship. Cornell University. Email: cfreitag@cs.cornell.edu. Indian Institute of Science. Email: htyagi@iisc.ac.in. 1	bespoke;email;general-purpose markup language;information theory;randomized algorithm	Jayadev Acharya;Clément L. Canonne;Cody Freitag;Himanshu Tyagi	2018	CoRR		mathematical optimization;randomness;optimal mechanism;bespoke;hadamard transform;computer science	ML	28.017545976780724	-22.066181056744046	135043
275d3dd2180f26dc61bbfb092e18340ffc44a510	how the “baldwin effect” can guide evolution in dynamic environments		Evolution and learning are two different ways in which the organism can adapt their behaviour to cope with problems posed by the environment. The second type of adaptation occurs when individuals exhibit plasticity in response to environmental conditions that may strengthen their survival. Individuals seek a behaviour that increases fitness. Therefore, it is plausible and rational for the individual to have some learning capabilities to prepare for the uncertain future, some sort of prediction or plastic abilities in different environments. Learning has been shown to benefit the evolutionary process through the Baldwin Effect, enhancing the adaptivity of the evolving population. In nature, when the environment changes too quickly that the slower evolutionary process cannot equip enough information for the population to survive, having the ability to learn during the lifetime is necessary to keep pace with the changing environment. This paper investigates the effect of learning on evolution in evolutionary optimisation. An instance of dynamic optimisation problems is proposed to test the theory. Experimental results show that learning has a significant impact on guiding evolutionary search in the dynamic landscapes. Indications for future work on dynamic optimisation are also presented.		Nam Le;Anthony Brabazon;Michael O'Neill	2018		10.1007/978-3-030-04070-3_13	cognitive science;organism;sort;phenotypic plasticity;pace;population;computer science;too quickly;baldwin effect	HPC	24.754652335898886	-10.992538245961034	135276
b9b8940585e207a4f3534df713d4cd78be8f2c1b	design of control chart in presence of hybrid censoring scheme		In this paper, a new reliability testing control chart has been developed using the hybrid censoring scheme for the Weibull distribution. The in-control and out-of-control probabilities are estimated for the desired quality characteristic using the reliability acceptance sampling plan. The performance of the proposed chart has been evaluated by estimating the average run lengths for the shifted process under the simulation. A numerical example has been given for the practical implementation of the proposed chart. It has been observed that the proposed chart is a valuable addition in the process monitoring for the life testing of the products.	censoring (statistics);numerical analysis;reliability engineering;sampling (signal processing);simulation	Muhammad Ahtisham Aslam;Ritwik Bhattacharya;Mansour Sattam Aldosari	2018	IEEE Access	10.1109/ACCESS.2018.2815646	process control;computer science;censoring (statistics);distributed computing;weibull distribution;control chart;reliability engineering;chart;acceptance sampling	Embedded	28.651682294443013	-18.930313747765723	135379
6b9b0d2c18a1f06efc8c0b03bb0c638f94b282b3	theoretical and empirical estimates of mean-variance portfolio sensitivity	bootstrap;investment analysis;estimation error;asset allocation;mean variance portfolio	This paper studies properties of an estimator of mean-variance portfolio weights in a market model with multiple risky assets and a riskless asset. Theoretical formulas for the mean square error are derived in the case when asset excess returns are multivariate normally distributed and serially independent. The sensitivity of the portfolio estimator to errors arising from the estimation of the covariance matrix and the mean vector is quantified. It turns out that the relative contribution of the covariance matrix error depends mainly on the Sharpe ratio of the market portfolio and the sampling frequency of historical data. Theoretical studies are complemented by an investigation of the distribution of portfolio estimator for empirical datasets. An appropriately crafted bootstrapping method is employed to compute the empirical mean square error. Empirical and theoretical estimates are in good agreement, with the empirical values being, in general, higher. (JEL: C13, C52, G11)	bootstrapping (compilers);mean squared error;norm (social);offset binary;sampling (signal processing)	Andrzej Palczewski;Jan Palczewski	2014	European Journal of Operational Research	10.1016/j.ejor.2013.04.018	financial economics;post-modern portfolio theory;econometrics;mathematical optimization;economics;modern portfolio theory;asset allocation;portfolio optimization;statistics	ML	27.465822952467352	-21.877712839048687	135485
6fc9341ebd4db3f69efcf4701aceb873603f2cd0	a monte carlo approach to quantifying model error in bayesian parameter estimation	ϕ;model error;φ divergence;ϕ divergence;hellinger distance;kullback leibler	Quantifying the discrepancy between two distributions is considered, using the concept of ? -divergence. The motivation is a Bayesian inference scenario where one is interested in comparing different posterior distributions. Strongly consistent estimators for the ? -divergence between two posterior distributions are developed. The proposed estimators alleviate known computational difficulties with estimating normalizing constants. This approach can be used to study the impact that using an approximate likelihood has on the resulting posterior distribution and also to compare the effectiveness of different model approximations. The methodology is applied to two first-order emulator models and an oceanographic application where evaluation of the likelihood function involves the solution to a partial differential equation.	estimation theory;monte carlo method	Staci A. White;Radu Herbei	2015	Computational Statistics & Data Analysis	10.1016/j.csda.2014.10.008	econometrics;mathematical optimization;errors-in-variables models;mathematics;kullback–leibler divergence;statistics;hellinger distance	ML	29.921695735156735	-17.544194676563702	136013
d2ff538e3a7dbb15ecdf8ecd48bd4990d9df5a85	simple and effective boundary correction for kernel densities and regression with an application to the world income and engel curve estimation	estimacion sesgada;theorie echantillonnage;metodo estadistico;kernels;nonparametric density estimation;teoria muestreo;kernel regression;sample size;small sample;62p20;statistical simulation;62g05;covariance analysis;estimacion densidad;analisis datos;methode noyau;economic sciences;kernel density estimation;tamano muestra;estimation densite;estimation non parametrique;variance analysis;62g08;error sistematico;bandwith selection;taille echantillon;kernel density;seleccion de la ventana;62g07;econometria;etude methode;statistical method;distribucion del ingreso;regression non parametrique;distribution revenu;ecuesta estadistica;serveur institutionnel;income distribution;estudio metodo;boundary correction kernel density estimation kernel regression local bandwidth;kernel estimation;statistical regression;62jxx;estimation courbe;non parametric estimation;density estimation;ciencias economicas;sample survey;data analysis;simulacion estadistica;boundary effect;archive institutionnelle;analyse covariance;bias;methode statistique;noyau mathematiques;simulation statistique;analisis variancia;regresion estadistica;regression estimator;open access;metodo nucleo;estimation noyau;62j10;statistical computation;calculo estadistico;kernel density estimate;local bandwidth;62d05;choix fenetre;nonparametric regression;simulation study;analyse donnee;kernel method;pequena muestra;calcul statistique;sciences economiques;econometrics;archive ouverte unige;method study;estimacion no parametrica;05c78;boundary correction;analisis covariancia;estimation statistique;curve estimation;cybertheses;regression statistique;estimacion estadistica;statistical estimation;biased estimation;estimation biaisee;sondage statistique;institutional repository;petit echantillon;econometrie;analyse variance;erreur systematique;variance;variancia;sampling theory;engel curve	In both nonparametric density estimation and regression, the so-called boundary effects, i.e. the bias and variance increase due to one sided data information, can be quite serious. For estimation performed on transformed variables this problem can easily get boosted and may distort substantially the final estimates, and consequently the conclusions. After a brief review of some existing methods a new, straightforward and very simple boundary correction is proposed, applying local bandwidth variation at the boundaries. The statistical behavior is discussed and the performance for density and regression estimation is studied for small and moderate sample sizes. In a simulation study this method is shown to perform very well. Furthermore, it appears to be excellent for estimating the world income distribution, and Engel curves in economics.	kernel (operating system)	Jun Dai;Stefan Sperlich	2010	Computational Statistics & Data Analysis	10.1016/j.csda.2010.03.029	kernel density estimation;econometrics;calculus;mathematics;statistics	ML	32.83090282332875	-23.438065078957045	136046
ee7b590a9690253c809d452a9e2b6a0a611b1472	generalized lotka-volterra (glv) models of stock markets	stock market;generalized lotka volterra	The Generalized Lotka-Volterra (GLV) model: Wi(t + 1) = Xwi(t) + aw(t) cw(t)wi{t) , * = 1, ,iV provides a general method to simulate, analyze and understand a wide class of phenomena that are characterized by power-law probability distributions: P(w)dw ~ w~~dw (a > 1) and truncated Levy flights fluctuations La(w). We show how the model applies to economic systems.	courant–friedrichs–lewy condition;grating light valve;lotka–volterra equations;lévy flight;simulation	Sorin Solomon	2000	Advances in Complex Systems	10.1142/S0219525900000224	financial economics;actuarial science;economics;mathematics;mathematical economics	ML	36.95478651057188	-17.49614078644207	136349
aec05be1abc9375b1add2785090c8159e492dea2	consistent tuning parameter selection in high dimensional sparse linear regression	metodo regularizacion;bayes estimation;theorie echantillonnage;metodo estadistico;teoria muestreo;analyse multivariable;selection problem;analisis numerico;sample size;problema seleccion;problema mal planteado;statistical simulation;regularisation;tuning parameter selection;high dimensionality;62h15;methode parametrique;multivariate analysis;independance;62j05;information criterion;tamano muestra;metodo parametrico;regularization method;selection variable;parametric method;probleme mal pose;62g20;shrinkage estimator;linear regression;taille echantillon;metodo penalidad;methode regularisation;estadistica rango;statistical method;information criteria;ecuesta estadistica;adaptive elastic net bayesian information criterion high dimensionality sure independence screening tuning parameter selection variable selection;adaptive elastic net;statistical regression;statistical model;analyse numerique;model complexity;65j20;regularization;critere information;independence;sure independence screening;estimation parametrique;elastic net;variable selection;62f07;sample survey;estimacion bayes;methode selection;penalty method;methode penalite;simulacion estadistica;numerical analysis;parameter selection;estimateur retrecissement;independencia;methode statistique;simulation statistique;regresion estadistica;ill posed problem;algebra lineal numerica;regresion lineal;algebre lineaire numerique;62d05;65f22;rank statistic;simulation study;analisis multivariable;statistique rang;regularizacion;numerical linear algebra;selection method;regression statistique;bayesian information criterion;regression lineaire;sondage statistique;exhaustive search;estimation bayes;sampling theory;probleme selection	An exhaustive search as required for traditional variable selection methods is impractical in high dimensional statistical modeling. Thus, to conduct variable selection, various forms of penalized estimators with good statistical and computational properties, have been proposed during the past two decades. The attractive properties of these shrinkage and selection estimators, however, depend critically on the size of regularization which controls model complexity. In this paper, we consider the problem of consistent tuning parameter selection in high dimensional sparse linear regression where the dimension of the predictor vector is larger than the size of the sample. First, we propose a family of high dimensional Bayesian Information Criteria (HBIC), and then investigate the selection consistency, extending the results of the extended Bayesian Information Criterion (EBIC), in Chen and Chen (2008) to ultra-high dimensional situations. Second, we develop a two-step procedure, the SIS+AENET, to conduct variable selection in p>n situations. The consistency of tuning parameter selection is established under fairly mild technical conditions. Simulation studies are presented to confirm theoretical findings, and an empirical example is given to illustrate the use in the internet advertising data.	sparse matrix	Tao Wang;Lixing Zhu	2011	J. Multivariate Analysis	10.1016/j.jmva.2011.03.007	sample size determination;independence;statistical model;shrinkage estimator;regularization;econometrics;mathematical optimization;numerical analysis;linear regression;survey sampling;penalty method;brute-force search;mathematics;multivariate analysis;numerical linear algebra;feature selection;bayesian information criterion;elastic net regularization;regression analysis;statistics	HPC	32.817192295927256	-23.307341649691743	136436
22c41fa0a14003d4f1c43a303dd8b10e77c7744b	using watermarking techniques to prove rightful ownership of web images	embedding;watermarking;web;robust;invisibility;rightful ownership;spatial	This article introduces intelligent watermarking scheme to protect Web images from attackers who try to counterfeit the copyright to damage the rightful ownership. Using secret signs and logos that are embedded within the digital images, the technique can investigate technically the ownership claim. Also, the nature of each individual image is taken into consideration which gives more reliable results. The colour channel used was chosen depending on the value of its standard deviation to compromise between robustness and invisibility of the watermarks. Several types of test images, logos, attacks and evaluation metrics were used to examine the performance of the techniques used. Subjective and objective tests were used to check visually and mathematically the solidity and weakness of the used scheme. DOI: 10.4018/978-1-4666-2157-2.ch008	digital image;digital watermarking;embedded system;solidity	Abdallah Saleem Nawaf Al-Tahan Al-Nu'aimi	2011	IJITWE	10.4018/jitwe.2011040103	digital watermarking;computer science;artificial intelligence;embedding;internet privacy;world wide web;computer security;invisibility	Vision	38.12107403455493	-11.778071083046326	136486
17dae16b21213a5dc7887391fb1f888d3a3377a8	selection-channel-aware rich model for steganalysis of digital images	selection channel aware rich model srm spatial rich model probabilistic selection channel signal detection theory digital image steganalysis;conferences forensics security;steganography cryptography image coding probability signal detection;security;forensics;conferences	From the perspective of signal detection theory, it seems obvious that knowing the probabilities with which the individual cover elements are modified during message embedding (the so-called probabilistic selection channel) should improve steganalysis. It is, however, not clear how to incorporate this information into steganalysis features when the detector is built as a classifier. In this paper, we propose a variant of the popular spatial rich model (SRM) that makes use of the selection channel. We demonstrate on three state-of-the-art content-adaptive steganographic schemes that even an imprecise knowledge of the embedding probabilities can substantially increase the detection accuracy in comparison with feature sets that do not consider the selection channel. Overly adaptive embedding schemes seem to be more vulnerable than schemes that spread the embedding changes more evenly throughout the cover.	detection theory;digital image;statistical classification;steganalysis;steganography	Tomás Denemark;Vahid Sedighi;Vojtech Holub;Rémi Cogranne;Jessica J. Fridrich	2014	2014 IEEE International Workshop on Information Forensics and Security (WIFS)	10.1109/WIFS.2014.7084302	steganalysis;computer science;data mining;internet privacy;computer security	Vision	37.32523456577695	-14.67843722383286	136502
3b98ec5a9fc651a03b0994f783b829931c5c3a7b	comparison of bayesian priors for highly reliable limit models	probability interval requirement;historical data;proportions p;confidence interval;posteriori maximum probability;bayesian reliability analysis;reliable limit model;bayesian prior;limit standard model;limit standard;non-conforming trial;bayesian method;reliability;maximum likelihood estimation;bayesian methods;reliability analysis;standard model;sample size	Limit standards are probability interval requirements for proportions. Simulation literature has focused on finding the confidence interval of the population proportion, which is inappropriate for limit standards. Further, some Frequentist approaches cannot be utilized for highly reliable models, or models which produce no or few non-conforming trials. Bayesian methods provide approaches that can be utilized for all limit standard models. We consider a methodology developed for Bayesian reliability analysis, where historical data is used to define the a priori distribution of proportions p, and the customer desired a posteriori maximum probability is utilized to determine sample size for a replication.	bayesian network;requirement;simulation	Roy R. Creasey;K. Preston White;Linda B. Wright;Cheryl F. Davis	2008	2008 Winter Simulation Conference		econometrics;confidence interval;bayesian probability;pattern recognition;mathematics;statistics	ML	29.763572003872568	-20.412290563400116	136510
85a247e727aa15946836d51f014ff6b11d390012	estimation of the inverse weibull distribution based on progressively censored data: comparative study	approximate maximum likelihood;inverse weibull distribution;lindley s approximation;pitman nearness probability;least squares methods;progressive type ii censoring	In this article we consider statistical inferences about the unknown parameters of the Inverse Weibull distribution based on progressively type-II censoring using classical and Bayesian procedures. For classical procedures we propose using the maximum likelihood; the least squares methods and the approximate maximum likelihood estimators. The Bayes estimators are obtained based on both the symmetric and asymmetric ( Linex , General Entropy and Precautionary) loss functions. There are no explicit forms for the Bayes estimators, therefore, we propose Lindley׳s approximation method to compute the Bayes estimators. A comparison between these estimators is provided by using extensive simulation and three criteria, namely, Bias, mean squared error and Pitman nearness ( PN ) probability. It is concluded that the approximate Bayes estimators outperform the classical estimators most of the time. Real life data example is provided to illustrate our proposed estimators.	censoring (statistics)	Rola M. Musleh;Amal Helu	2014	Rel. Eng. & Sys. Safety	10.1016/j.ress.2014.07.006	econometrics;pattern recognition;m-estimator;mathematics;statistics	DB	29.29357001832161	-23.074514778036804	136941
0f80470102b84db9b2d0a9dbae68bb692cd66253	global optimization decomposition methods for bounded parameter minimax risk evaluation	bounded parameter;minimax;ibragimov hasminskii constant;decomposition;90c26;62f10;discrete least favorable prior;decomposition method;minimax risk;global optimization;62 04	There has been much recent statistical research in the area of inference under constraints. The problem considered here is that of bounded parameter estimation, in particular that of normal and Poisson means, using minimaxity as the criterion of evaluation. Because of the ease of calculation of linear minimax rules, the ratio of these risks to the nonlinear minimax risks for these problems is also studied. To find the minimax solution, the dual problem of finding the least favorable prior distribution is often considered. On bounded parameter spaces the least favorable prior is often discrete, so the finding of the minimax estimator and its risk is equivalent to a global optimization problem with constraints. Previously published numerical specifications of the priors have used iterative (often heuristic) procedures. Two global optimization procedures are proposed. The first is based on multivariate Lipschitz optimization and makes use of bounds on the first-order derivatives. The second is a decompositio...	global optimization;minimax	Éric Gourdin;Brigitte Jaumard;Brenda MacGibbon	1994	SIAM J. Scientific Computing	10.1137/0915002	minimax estimator;minimax;mathematical optimization;combinatorics;decomposition method;mathematics;decomposition;minimax approximation algorithm;statistics;global optimization	HPC	30.578851518082466	-16.9990938326815	137021
51f0ddd923414e8fe9d6b6f9c4dc18d4a35e3326	gaussian semiparametric estimation in seasonal/cyclical long memory time series	economia y empresa generalidades;economia y empresa	Gaussian semiparametric or local Whittle estimation of the memory parameter in standard long memory processes was proposed by Robinson [18]. This technique shows several advantages over the popular log-periodogram regression introduced by Geweke and Porter-Hudak [7]. In particular under milder assumptions than those needed in the log periodogram regression it is asymptotically more efficient. We analyse the asymptotic behaviour of the Gaussian semiparametric estimate of the memory parameter in seasonal or cyclical long memory processes allowing for asymmetric spectral divergences or zeros. Consistency and asymptotic normality are obtained.	gaussian (software);semiparametric model;time series	Josu Arteche	2000	Kybernetika		financial economics;econometrics;mathematics;statistics;semiparametric regression	ML	30.878470702576365	-22.861948988153934	137026
3b501e47e609435ee70772a14002d9fa119be487	an adaptive monte carlo algorithm for computing mixed logit estimators	monte carlo samplings;monte carlo sampling;trust region algorithms;mixed logit model;trust region;mixed logit models;monte carlo algorithm;maximum simulated likelihood;quasi monte carlo;mixed logit;numerical experiment;maximum simulated likelihood estimation;monte carlo;computational efficiency;management science	Researchers and analysts are increasingly using mixed logit models for estimating responses to forecast demand and to determine the factors that affect individual choices. However the numerical cost associated to their evaluation can be prohibitive, the inherent probability choices being represented by multidimensional integrals. This cost remains high even if Monte Carlo or quasi-Monte Carlo techniques are used to estimate those integrals. This paper describes a new algorithm that uses Monte Carlo approximations in the context of modern trust-region techniques, but also exploits accuracy and bias estimators to considerably increase its computational efficiency. Numerical experiments underline the importance of the choice of an appropriate optimisation technique and indicate that the proposed algorithm allows substantial gains in time while delivering more information to the practitioner.		Fabian Bastin;Cinzia Cirillo;Philippe L. Toint	2006	Comput. Manag. Science	10.1007/s10287-005-0044-y	monte carlo method in statistical physics;quasi-monte carlo method;econometrics;mathematical optimization;mixed logit;dynamic monte carlo method;hybrid monte carlo;particle filter;markov chain monte carlo;computer science;monte carlo molecular modeling;mathematics;kinetic monte carlo;rejection sampling;parallel tempering;monte carlo integration;monte carlo methods for option pricing;monte carlo algorithm;statistics;monte carlo method;control variates	ML	30.752456960348518	-16.48557370255124	137044
cfea5dbf20a46df800f02c9a13f074aa5c43e5f0	mechanical generation of networks with surplus complexity		Abstract: In previous work I examined an information based complexity measure of networks with weighted links. The measure was compared with that obtained from by randomly shuffling the original network, forming an Erdös-Rényi random network preserving the original link weight distribution. It was found that real world networks almost invariably had higher complexity than their shuffled counterparts, whereas networks mechanically generated via preferential attachment did not. The same experiment was performed on foodwebs generated by an artificial life system, Tierra, and a couple of evolutionary ecology systems, EcoLab and WebWorld. These latter systems often exhibited the same complexity excess shown by real world networks, suggesting that the complexity surplus indicates the presence of evolutionary dynamics. In this paper, I report on a mechanical network generation system that does produce this complexity surplus. The heart of the idea is to construct the network of state transitions of a chaotic dynamical system, such as the Lorenz equation. This indicates that complexity surplus is a more fundamental trait than that of being an evolutionary system.	artificial life;attachments;blum axioms;chaos theory;dynamical system;ecology;evolutionary algorithm;information-based complexity;lorenz system;offset binary;random graph;randomness;tierra (computer simulation)	Russell K. Standish	2015		10.1007/978-3-319-14803-8_30	mathematical optimization;discrete mathematics;dynamical systems theory;shuffling;preferential attachment;mathematics;random graph;information-based complexity;graph	AI	27.398348743333795	-10.455543716916923	137051
1328d7a2fa445dfe3555c90f63f942ed48693ae4	tests of correlation among wavelet-based estimates for long memory processes	transformation ondelette;statistical study;association statistique;correlacion;theorie echantillonnage;statistical moment;metodo estadistico;teoria muestreo;analyse multivariable;long range dependence;finite sample;analisis numerico;sample size;62p20;decomposition;theorie approximation;analisis estadistico;aplicacion;multivariate analysis;coeficiente correlacion;long memory process;economic sciences;wavelets 62m10;62m10;tamano muestra;ondelette;metodo descomposicion;62h20;circulant matrix;moment statistique;62e17;simulacion numerica;metodo semiparametrico;echantillon fini;methode decomposition;taille echantillon;statistical association;semiparametric estimation;econometria;statistical method;65t60;methode semiparametrique;ecuesta estadistica;wavelet decomposition;distribucion estadistica;processus stationnaire;analyse numerique;matrice circulante;processus memoire longue;statistical properties;approximation theory;methode matricielle;decomposition method;ciencias economicas;sample survey;momento estadistico;correlation tests;asociacion estadistica;numerical analysis;42c40;statistical analysis;distribution statistique;telecomunicacion;62g10;methode statistique;hurst parameter estimation;simulation numerique;telecommunication;long memory;analyse statistique;semiparametric method;matrix method;analyse correlation;long range dependent;62d05;estudio estadistico;metodo matriz;etude statistique;dependance longue distance;analisis multivariable;sciences economiques;econometrics;transformacion ondita;40c05;correlation;descomposicion;proceso estacionario;long memory processes;correlation coefficient;60g10;hurst parameter;application;coefficient correlation;statistical distribution;wavelets;sondage statistique;stationary process;wavelet transformation;econometrie;analisis correlacion;matriz circulante;dependencia larga distancia;numerical simulation;correlation analysis;sampling theory	Long memory models have received a significant amount of attention in the theoretical literature as they cover a wide range of applications, including economics and telecommunications. In recent years, a semiparametric estimator of the long memory parameter of stationary processes with long-range dependence, based on wavelet decomposition, has been proposed and studied by Veitch and Abry (1999) under the idealized assumption of decorrelation among wavelet coefficients. The asymptotic statistical analysis of the wavelet-based estimator has been recently complemented taking into account the correlations among wavelet coefficients, at fixed scales as well as among different scales (Bardet et al., 2000). The goal of the present article is to study the statistical properties of the wavelet-based estimator for a finite sample size and the correlation among the wavelet-based long memory estimates. The analysis is conducted by simulation, through the use of the circulant matrix method and shows that the correlati...	wavelet	Livia De Giovanni;Maurizio Naldi	2008	Communications in Statistics - Simulation and Computation	10.1080/03610910701790400	computer simulation;probability distribution;sample size determination;wavelet;matrix method;stationary process;association;econometrics;decomposition method;numerical analysis;survey sampling;circulant matrix;calculus;mathematics;hurst exponent;multivariate analysis;decomposition;correlation;statistics;approximation theory	Metrics	33.84945048423262	-22.10700588116442	137358
407f5b4835762c41988dbcc0b2878b4b346bca89	a simple asymptotic bound on the error of the ordinary normal approximation to the student's t-distribution	ordinary normal approximation ona confidence interval detection theory;random variables approximation error accuracy maximum likelihood estimation electrical engineering;approximation error;random variables;maximum likelihood estimation;accuracy;false alarm probability simple asymptotic bound ordinary normal approximation ona student s t distribution degrees of freedom approximation error confidence interval determination detection theory confidence level;statistical distributions approximation theory;electrical engineering	A simple asymptotic bound on the error of the ordinary normal approximation (ONA) to the Student's t-distribution is derived. The proposed bound provides a quick calculation of the minimum number of degrees of freedom required to ensure a given approximation error. Then, the application of the ONA is investigated in confidence interval determination and detection theory. Finally, using the proposed bound, further simple bounds on the approximation error of the confidence level and the false-alarm probability are obtained.	approximation error;confidentiality;detection theory;interval arithmetic	Akbar Shafiei;S. Mohammad Saberali	2015	IEEE Communications Letters	10.1109/LCOMM.2015.2442576	random variable;econometrics;mathematical optimization;approximation error;round-off error;mathematics;accuracy and precision;maximum likelihood;minimax approximation algorithm;statistics	Robotics	30.625107582805605	-19.93936519541446	137362
9f4b8fc84e8556e15dd13b1a1d036c3f75ce225c	a novel algorithm for colour image steganography using a new intelligent technique based on three phases	rich model;image segmentation;neural networks;steganography;genetic algorithm	Steganography architecture with seven security layers. New steganography algorithm.Proposed new intelligent technique.Proposed seven layers of security.Extract byte characteristics.Construct image segmentation. A three-phase intelligent technique has been constructed to improve the data-hiding algorithm in colour images with imperceptibility. The first phase of the learning system (LS) has been applied in advance, whereas the other phases have been applied after the hiding process. The first phase has been constructed to estimate the number of bits to be hidden at each pixel (NBH); this phase is based on adaptive neural networks with an adaptive genetic algorithm using upwind adaptive relaxation (LSANN_AGAUpAR1). The LS of the second phase (LSANN_AGAUpAR2) has been introduced as a detector to check the performance of the proposed steganographic algorithm by creating a rich images model from available cover and stego images. The LS of the last phase (LSCANN_AGAUpAR3) has been implemented through three steps, and it is based on a concurrent approach to improve the stego image and defend against attacks. The adaptive image filtering and adaptive image segmentation algorithms have been introduced to randomly hide a compressed and encrypted secret message into a cover image. The NBH for each pixel has been estimated cautiously using 32 principle situations (PS) with their 6 branch situations (BS). These situations have been worked through seven layers of security to augment protection from attacks. In this paper, hiding algorithms have been produced to fight three types of attacks: visual, structural, and statistical attacks. The simulation results have been discussed and compared with new literature using data hiding algorithms for colour images. The results of the proposed algorithm can efficiently embed a large quantity of data, up to 12bpp (bits per pixel), with better image quality.	algorithm;color image;steganography	Nameer N. El-Emam;Mofleh Al-Diabat	2015	Appl. Soft Comput.	10.1016/j.asoc.2015.08.057	computer vision;genetic algorithm;computer science;artificial intelligence;theoretical computer science;machine learning;image segmentation;steganography;artificial neural network	Robotics	37.59758336267566	-9.959715648986457	137608
4378ced592344a57dcff9efee681397c4929e3d6	multi-process dynamic modeling of tumor-specific evolution	bayes methods;tumours;tumors predictive models cancer uncertainty biological system modeling computational modeling bayes methods;stochastic processes;diseases;tumours bayes methods diseases stochastic processes;stochastic context multiprocess dynamic modeling tumor specific evolution sequential bayesian forecasting method tumor specific growth mixture model multiscale process	We suggest a multi-process dynamic model and a sequential bayesian forecasting method of tumor-specific growth. The mixture model uses prior information obtained from the general population and becomes more individualized as more observations from the tumor are sequentially taken into account. In this study we propose utilizing all available tumor-specific information up to date to approximate the unknown multi-scale process of tumor growth over time, in a stochastic context. The validation of our approach was performed with experimental data from mice and the results show that after few observations from a tumor are obtained and included in the model, the latter becomes more individualized, in the sense that its parameters are adjusted in order to reect the growth of each individual tumor, yielding more precise estimates of its size.	approximation algorithm;bayesian network;mathematical model;mixture model	Achilleas Achilleos;Charalambos Loizides;Triantafyllos Stylianopoulos;Georgios D. Mitsis	2013	13th IEEE International Conference on BioInformatics and BioEngineering	10.1109/BIBE.2013.6701614	stochastic process;econometrics;computer science;machine learning;statistics	Robotics	25.698465764314417	-22.610942488739276	137709
6a580bd0a89dc17ac738050bfa4c7cabaf11973d	a partially linear model with its applications	method of lagrange multipliers;62g05;62g08;62g20;p splines;62g10;penalized least squares;b splines;wald type spline based test	The nonparametric component in a partially linear model is approximated via cubic B-splines with a second-order difference penalty on the adjacent B-spline coefficients to avoid undersmoothing. A Wald-type spline-based test statistic is constructed for the null hypothesis of no effect of a continuous covariate. When the number of knots is fixed, the limiting null distribution of the test statistic is the distribution of a linear combination of independent chi-squared random variables, each with one degree of freedom. A real-life dataset is provided to illustrate the practical use of the test statistic.	linear model	Chin-Shang Li	2013	Communications in Statistics - Simulation and Computation	10.1080/03610918.2012.674597	b-spline;f-test;econometrics;mathematical optimization;chi-square test;likelihood-ratio test;mathematics;lagrange multiplier;wald test;statistics	ECom	31.01106597409024	-23.300591833801118	138516
c82e50682528e008ed1161f476fd8401b5fd4eaf	practical statistical analyses of simulation output data	output analysis problem;simulation study;corresponding true answer;single simulation;definitive output data analysis;practical statistical analysis;large amount;computer time;particular simulation run;simulation output data;simulation estimate	It has been our observation that in many simulation studies a large amount of time and money is spent on model development and programming, but little effort is made to analyze the simulation output data in an appropriate manner. As a matter of fact, a common mode of operation is to make a single simulation run of somewhat arbitrary length and then treat the resulting simulation estimates as being the “true” answers for the model. Since these estimates are observations of random variables which may have large variances, these estimates may, in a particular simulation run, differ greatly from the corresponding true answers. The net effect is, of course, that there may be a significant probability of making erroneous inferences about the system under study.  One reason for the historical lack of definitive output data analyses is that simulation output data are rarely, if ever, independent. Thus, classical statistical analyses based on independent identically distributed observations are not directly applicable. At the present time, there are still several output analysis problems for which there is no completely accepted solution, and the solutions that do exist are often complicated to apply. Another impediment to getting accurate estimates of a model's true parameters or characteristics is the computer cost associated with collecting the necessary amount of simulation output data. Indeed, there are situations where an appropriate statistical procedure is available, but the cost of collecting the amount of data dictated by the procedure is prohibitive. We expect this latter problem to become less important as the cost of computer time continues to drop.	block cipher mode of operation;simulation	Averill M. Law	1984			simulation;econometrics;matter of fact;computer science;statistics;random variable;independent and identically distributed random variables	ML	30.172291484380295	-17.02192023815348	138525
13aeb92af7eb96dbd737acd6c74c49eb94869a4b	on the empty space function of some germ-grain models	processus ponctuel;poisson process;cluster;amas;simulation;point process;spatial interaction;germ grain model;simulacion;hazard rate;proceso puntual;risk rate;failure rate;tasa riesgo;geometrie stochastique;taux risque;proceso poisson;monton;cluster model;distance spherique;poisson model;stochastic geometry;processus poisson;empty space function	We derive and discuss formulas for the density and the hazard rate of the empty space function of a germ-grain model in R d generated by a stationary point process and i.i.d. convex primary grains n , n 2 N, that are independent of. Our formulas are based on the Palm probability of the germ process and the mean generalized curvature measure of the grain. Particular attention is paid to cluster models, where the grains form a Poisson cluster process. Our discussion of speciic Gauss-Poisson models with spherical grains provides some motivation for the use of the failure rate of F to detect clustering eeects. In the general case we propose a family of functions comparing the behaviour in the neighborhood of a typical germ with the neighborhood of an arbitrary point in space. These characteristics can be used to measure eeects of clustering and spatial interactions between the locations of the individual grains.	cluster analysis;failure rate;interaction;point process;stationary process	Günter Last;M. Holtmann	1999	Pattern Recognition	10.1016/S0031-3203(99)00022-9	econometrics;poisson process;calculus;failure rate;point process;mathematics;poisson regression;hazard ratio;statistics;stochastic geometry;cluster	ML	34.87521088619873	-21.96311048467925	138804
f814e5b8e8a12c328162f9c336cffe454b663d98	diagnostics for regression dependence in tables re-ordered by the dominant correspondence analysis solution	analyse multivariable;analisis componente principal;analisis numerico;analisis factorial;analyse exploratoire;multivariate analysis;analisis datos;variable dependante;interaction;tabla contingencia;analisis correspondencia;exploratory analysis;statistical regression;62jxx;regression diagnostics;analyse numerique;permutation;dependent variable;data analysis;analyse factorielle;diagnostic regression;numerical analysis;factor analysis;regresion estadistica;principal component analysis;correspondence analysis;statistical computation;permutacion;numero de condicionamiento;calculo estadistico;algebra lineal numerica;ordinal data;62h25;algebre lineaire numerique;analyse composante principale;condition number;65f35;62h17;analyse correspondance;analisis multivariable;analyse donnee;calcul statistique;interaccion;numerical linear algebra;stochastic model;contingency table;regression statistique;table contingence;data fitting;modelo estocastico;modele stochastique;indice conditionnement;62j20;analisis exploratorio;donnee ordinale	Correspondence analysis is an exploratory technique for analyzing the interaction in a contingency table. Tables with meaningful orders of the rows and columns may be analyzed using a model-based correspondence analysis that incorporates order constraints. However, if there exists a permutation of the rows and columns of the contingency table so that the rows are regression dependent on the columns and, vice versa, the columns are regression dependent on the rows, then both implied orders are reflected in the first dimension of the unconstrained correspondence analysis [Schriever, B.F., 1983. Scaling of order dependent categorical variables with correspondence analysis. International Statistical Review 51, 225-238]. Thus, using unconstrained correspondence analysis, we may still find that the data fit an ordinal stochastic model. Fit measures are formulated that may be used to verify whether the re-ordered contingency table is regression dependent in either the rows or columns. Using several data examples, it is shown that the fit indices may complement the usual geometric interpretation of the unconstrained correspondence analysis solution in low-dimensional space.	correspondence analysis	Matthijs J. Warrens;Willem J. Heiser	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.07.035	variables;econometrics;interaction;numerical analysis;contingency table;stochastic modelling;condition number;calculus;regression diagnostic;mathematics;permutation;multivariate analysis;numerical linear algebra;ordinal data;correspondence analysis;factor analysis;data analysis;multiple correspondence analysis;regression analysis;statistics;curve fitting;principal component analysis	ML	34.06729564726082	-23.30130956887978	139091
c22493eb0952c7bf538d4dc100c061f9541254ec	convex combinations of quadrant dependent copulas	selected works;copula;quadrant dependence in expectation;quadrant dependence;bepress;convex combination	It is well known that quadrant dependent (QD) random variables are also quadrant dependent in expectation (QDE). Recent literature has offered examples rigorously establishing the fact that there are QDE random variables which are not QD. The examples are based on convex combinations of specially chosen QD copulas: one negatively QD and another positively QD. In this paper we establish general results that determine when convex combinations of arbitrary QD copulas give rise to negatively or positively QD/QDE copulas. In addition to being an interesting mathematical exercise, the established results are helpful when modeling insurance and financial portfolios.	quantum dot	Martin Egozcue;Luis Fuentes García;Wing-Keung Wong;Ricardas Zitikis	2013	Appl. Math. Lett.	10.1016/j.aml.2012.08.019	econometrics;mathematical optimization;convex combination;copula;mathematics;statistics	ML	33.5958458322835	-18.880362216392808	139260
27f2cc3b561c47282031aeed95e5e339cd25a0f3	the generalized biparabolic distribution	biparabolic;uncertainty;generating density;beta;risk;tsp distribution	Beta distributions have been applied in a variety of fields in part due to its similarity to the normal distribution while allowing for a larger flexibility of skewness and kurtosis coverage when compared to the normal distribution. In spite of these advantages, the two-sided power (TSP) distribution was presented as an alternative to the beta distribution to address some of its short-comings, such as not possessing a cumulative density function (cdf) in a closed form and a difficulty with the interpretation of its parameters. The introduction of the biparabolic distribution and its generalization in this paper may be thought of in the same vein. Similar to the TSP distribution, the generalized biparabolic (GBP) distribution also possesses a closed form cdf, but contrary to the TSP distribution its density function is smooth at the mode. We shall demonstrate, using a moment ratio diagram comparison, that the GBP distribution provides for a larger flexibility in skewness and kurtosis coverage than the beta distribution when restricted to the unimodal domain. A detailed mean-variance comparison of GBP, beta and TSP distributions is presented in a Project Evaluation and Review Technique (PERT) context. Finally, we shall fit a GBP distribution to an example of financial European stock data and demonstrate a favorable fit of the GBP distribution compared to other distributions that have traditionally been used in that field, including the beta distribution.	diagram;program evaluation and review technique	Catalina Beatriz García García;José García Pérez;Salvador Cruz Rambaud	2009	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488509005930	beta;shape of the distribution;econometrics;beta negative binomial distribution;uncertainty;product distribution;heavy-tailed distribution;log-cauchy distribution;kurtosis;calculus;beta prime distribution;inverse-chi-squared distribution;risk;mathematics;variance-gamma distribution;compound probability distribution;asymptotic distribution;chi-squared distribution;statistics;generalized beta distribution;three-point estimation;distribution fitting	Arch	27.005158711618428	-21.026593072957205	139282
004a03ced59ae5b14abdfb77515ca90b5905de30	establishing a bayesian predictive survival model adjusting for random effects	bayesian prediction;bayesian statistics;random effects;bone marrow;predictive;model performance;acute leukemia;prior distribution;clinical trial;weibull;leukemia;myelodysplastic syndrome;bayesian;weibull distribution;markov chain monte carlo;pre leukemia;shape parameter;blood cells;survival;high risk	Myelodysplastic syndrome (MDS), sometimes referred to as pre-leukemia or smoldering leukemia, is a group of diseases usually characterized by failure of the bone marrow to produce enough normal blood cells. In about one-third of patients, the disease transforms into acute leukemia. In high-risk MDS, the bone marrow contains too many immature blood cells known as blasts. Patients with high-risk MDS survive for an average of 6–12 months. We have taken data from a large clinical trial and re-examined it considering the pre-leukemia as a random event in a Weibull distribution model. The issue becomes that of examining the affect of the variable on the predictive survival. We have taken the intercept, stress effect and shape parameter of the distribution to be random effects as well with realistic prior distributions based on previous shapes of the survival experience of subjects with this disease. We demonstrate how the model performs under relevant clinical conditions. The conditions are all tested using a Bayesian statistical approach allowing for the robust testing of the model parameters under various stress conditions which we introduce into the model. The convergence of the parameters to stable values are seen in trace plots which follow the convergence patterns This allows for precise estimation for determining clinical conditions under which the survival pattern will change. We give a numerical example of our results. The major platform for the theoretical development follows the Bayesian methodology and the multiple parameter Weibull model with random effects having carefully chosen hyper parameters. We have implemented the basic infrastructure for the analysis using the commercially available WinBugs software employing the Markov Chain Monte Carlo (MCMC) methodology. © 2008 IMACS. Published by Elsevier B.V. All rights reserved.	blast;butterfly effect;carpal tunnel syndrome;markov chain monte carlo;monte carlo method;numerical analysis;random effects model;visual intercept;winbugs	Alfred A. Bartolucci;Sejong Bae;Karan P. Singh	2008	Mathematics and Computers in Simulation	10.1016/j.matcom.2008.01.035	weibull distribution;econometrics;mathematics;statistics	ML	30.713516741618857	-20.717585686872766	139358
9e4628b6af7420add3cb6c885ca8a6629c09ea2e	detecting local regions of change in high-dimensional criminal or terrorist point processes	processus ponctuel;random test;metodo estadistico;selection problem;marked spatial point process;change detection;problema seleccion;62p20;rule induction;criminalite;test aleatoire;computacion informatica;metodo monte carlo;high dimensionality;65c05;analisis datos;fonction repartition;estadistica test;implementation;statistique test;62e17;simulation;point process;methode monte carlo;statistique balayage;60g55;site selection;model based approach;simulacion;statistical method;spatial point process;feature space;test significacion;spatial scan statistic;62f07;ciencias economicas;funcion distribucion;data analysis;distribution function;prim;particion;marked point process;methode statistique;ciencias basicas y experimentales;monte carlo method;matematicas;statistical computation;calculo estadistico;partition;proceso puntual;analyse donnee;criminalidad;calcul statistique;test signification;sciences economiques;economics;implementacion;grupo a;scan statistic;62p12;monte carlo simulation;60e05;random labeling;test statistic;criminality;probleme selection;significance test	A method is presented for detecting changes to the distribution of a criminal or terrorist point process between two time periods using a non-model-based approach. By treating the criminal/terrorist point process as an intelligent site selection problem, changes to the process can signify changes in the behavior or activity level of the criminals/terrorists. The locations of past events and an associated vector of geographic, environmental, and socio-economic feature values are employed in the analysis. By modeling the locations of events in each time period as a marked point process, we can then detect differences in the intensity of each component process. A modified PRIM (patient rule induction method) is implemented to partition the high-dimensional feature space, which can include mixed variables, into the most likely change regions. Monte Carlo simulations are easily and quickly generated under random relabeling to test a scan statistic for significance. By detecting local regions of change, not only can it be determined if change has occurred in the study area, but the specific spatial regions where change occurs is also identified. An example is provided of breaking and entering crimes over two-time periods to demonstrate the use of this technique for detecting local regions of change. This methodology also applies to detecting regions of differences between two types of events such as in case–control data. © 2006 Elsevier B.V. All rights reserved.	feature vector;monte carlo method;point process;rule induction;selection algorithm;sensor;simulation	Michael D. Porter;Donald E. Brown	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.07.002	econometrics;mathematics;statistics;monte carlo method	ML	34.79979548137683	-22.7551630251784	139517
ee6034f9ad5b8d1c06ff39b8fb23a36d3bc1d826	nonparametric multivariate control charts based on a linkage ranking algorithm	data depth;multivariate control charts;nonparametric;hotelling s t2;multivariable control;statistical quality control;ranking algorithm;hotelling s t	Control charts have been widely recognized as important and critical tools in system monitoring for detection of abnormal behavior and quality improvement. In particular, multivariate control charts have been effectively used when a process involves a number of correlated process variables. Most existing multivariate control charts were developed using the assumption of normally distributed process variables. However, process data from modern industries often do not follow the normal distribution. Despite the great need for nonparametric control charts that can control the error rate regardless of the underlying distribution, few efforts have been made in this direction. In this paper, we propose a new nonparametric control chart (called the kLINK chart) based on a k-linkage ranking algorithm that calculates the ranking of a new measurement relative to the in-control training data. A simulation study was performed to demonstrate the effectiveness of our kLINK chart and its superiority over the traditional Hotelling's T 2 chart and the ranking depth control chart in nonnormal situations. In addition, to enable increased sensitivity to small shifts, we present an exponentially weighted moving average version of a kLINK chart.	algorithm;chart;linkage (software);optimal control;sensor;simulation;system monitoring	Helen Meyers Bush;Panitarn Chongfuangprinya;Victoria C. P. Chen;Thuntee Sukchotrat;Seoung Bum Kim	2010	Quality and Reliability Eng. Int.	10.1002/qre.1129	ewma chart;nonparametric statistics;econometrics;computer science;data mining;mathematics;shewhart individuals control chart;statistical process control;statistics	AI	28.220945790967153	-19.855460286374306	140140
323980f453db771bc0fbaef1e29b628d4fe0e06b	optimal selection from distributions with unknown parameters: robustness of bayesian models	dynamic programming;analisis sensibilidad;programacion dinamica;bayesian approach;normal distribution;decision bayes;analyse stochastique;prior distribution;dynamic program;bayes decision;monotonie;sensitivity analysis;monotonicity;programmation dynamique;stochastic order;stochastic analysis;analyse sensibilite;monotonia;analisis estocastico;bayesian model	We consider the problem of making one choice from a known number of i.i.d. alternatives. It is assumed that the distribution of the alternatives has some unknown parameter. We follow a Bayesian approach to maximize the discounted expected value of the chosen alternative minus the costs for the observations. For the case of gamma and normal distribution we investigate the sensitivity of the solution with respect to the prior distributions. Our main objective is to derive monotonicity and continuity results for the dependence on parameters of the prior distributions. Thus we prove some sort of Bayesian robustness of the model.		Alfred Müller	1996	Math. Meth. of OR	10.1007/BF01193937	normal distribution;dirichlet distribution;stochastic process;econometrics;mathematical optimization;prior probability;monotonic function;bayesian probability;dynamic programming;mathematics;bayesian linear regression;bayesian hierarchical modeling;bayesian statistics;bayesian inference;sensitivity analysis;statistics	ML	30.82763091038169	-19.324477913542637	140151
18e49efe4394a2f8e4bd0ddb4a36ad5c45095003	statistical uncertainty analysis for stochastic simulation with dependent input models	dependent input models;stochastic processes;norta representation;stochastic simulation;normal to anything representation;parameter estimation;statistical analysis;random input models;correlation matrix;stochastic system;asymptotic analysis;matrix algebra;equation-based stochastic kriging metamodel;parameter estimation error;simulation;marginal distribution parameter estination;statistical uncertainty analysis;bootstrap	When we use simulation to estimate the performance of a stochastic system, lack of fidelity in the random input models can lead to poor system performance estimates. Since the components of many complex systems could be dependent, we want to build input models that faithfully capture such key properties. In this paper, we use the flexible NORmal To Anything (NORTA) representation for dependent inputs. However, to use the NORTA representation we need to estimate the marginal distribution parameters and a correlation matrix from real-world data, introducing input uncertainty. To quantify this uncertainty, we employ the bootstrap to capture the parameter estimation error and an equation-based stochastic kriging metamodel to propagate the input uncertainty to the output mean. Asymptotic analysis provides theoretical support for our approach, while an empirical study demonstrates that it has good finite-sample performance.	bootstrapping (statistics);complex systems;estimation theory;kriging;marginal model;metamodeling;simulation;stochastic process	Wei Xie;Barry L. Nelson;Russell R. Barton	2014	Proceedings of the Winter Simulation Conference 2014		econometrics;mathematical optimization;stochastic modelling;mathematics;statistics	SE	29.853374378176127	-17.521425793204372	140207
c5e21c740492d40ea02bb4f04787f307d922d3ee	a simple identification proof for a mixture of two univariate normal distributions	medida informacion;theorie statistique;analyse multivariable;computacion informatica;multivariate analysis;analisis datos;melange loi probabilite;normal distribution;mesure information;loi 1 variable;matrice information;mixed distribution;positive definite;curva gauss;exponential family;statistical regression;62jxx;famille exponentielle;data analysis;information measure;ciencias basicas y experimentales;regresion estadistica;matematicas;economists;univariate distribution;loi normale;mezcla ley probabilidad;analisis multivariable;analyse donnee;regression analysis;familia exponencial;finite mixtures;teoria estadistica;theorie information;information matrix;grupo a;regression statistique;62b10;gaussian distribution;information theory;statistical theory;finite mixture;mixture regression;teoria informacion	A simple proof of the identification of a mixture of two univariate normal distributions is given. The proof is based on the equivalence of local identification with positive definiteness of the information matrix and the equivalence of the latter to a condition on the score vector that is easily checked for this model. Two extensions using the same line of proof are also given.	formation matrix;turing completeness	Erik Meijer;Jelmer Y. Ypma	2008	J. Classification	10.1007/s00357-008-9008-6	normal distribution;econometrics;information theory;calculus;mathematics;regression analysis;statistics	ML	32.93053698972077	-23.01148815562945	140328
5500517bed4cb17389ae4dedbff60ae0238b5dc8	optimal genetic manipulations in batch bioreactor control	genetique;bioreacteur;modelizacion;scale model;control optimo;escherichia coli;optimisation;bioreactor;control algorithm;non linear programming;programme commande;modele reduit;optimizacion;estrategia optima;genetica;stochastic method;batch production;metabolic engineering;algoritmo adaptativo;gradient method;scaling law;programacion no lineal;procede discontinu;programmation non lineaire;probabilistic approach;genetics;fermentacion anaerobia;optimal control;modelisation;biorreactor;optimal strategy;methode gradient;modelo reducido;point controle;adaptive algorithm;large scale;scheduling algorithm;fermentation anaerobie;programacion lineal;produccion por lote;scheduling algorithms;algorithme adaptatif;metabolismo;metodo gradiente;ley escala;control program;scheduling;enfoque probabilista;approche probabiliste;commande optimale;batch control;production par lot;batch process;control point;linear programming;programmation lineaire;fermentation process;linear program;programa mando;procedimiento discontinuo;fermentation processes;optimization;punto control;biotecnologia;loi echelle;ethanol production;escala grande;nonlinear optimization;end point control;biotechnology;modeling;anaerobic fermentation;strategie optimale;biotechnologie;ordonnancement;metabolism;reglamento;metabolisme;biological process;echelle grande	Advances in metabolic engineering have enabled bioprocess optimization at the genetic level. Large-scale systematic models are now available at a genome level for many biological processes. There is, thus, a motivation to develop advanced control algorithms, using these complex models, to identify optimal performance strategies both at the genetic and bioreactor level. In the present paper, the bilevel optimization framework previously developed by the authors is coupled with control algorithms to determine the genetic manipulation strategies in practical bioprocess applications. The bilevel optimization includes a linear programming problem in the inner level and a nonlinear optimization problem in the outer level. Both gradient-based and stochastic methods are used to solve the nonlinear optimization problem. Ethanol production in an anaerobic batch fermentation of Escherichia coli is considered in case studies that demonstrate optimization of ethanol production, batch time, and multi-batch scheduling.		Kapil G. Gadkar;Radhakrishnan Mahadevan;Francis J. Doyle	2006	Automatica	10.1016/j.automatica.2006.05.004	control engineering;engineering;linear programming;control theory;mathematics;scheduling	NLP	31.93814132362725	-10.746206594977831	140572
1dfc6d92719a6aa5acb88c41e93a4c3d6b25570f	using knowledge-based system with hierarchical architecture to guide the search of evolutionary computation	fitness landscape;pruning knowledge based system hierarchical architecture evolutionary computation fitness landscape patterns basins valleys multi modality constrained optimization problems infeasible regions regional knowledge evolutionary search region based schemata belief cells acquisition function landscape cultural algorithm evolving population;evolutionary computation;knowledge based system;knowledge acquisition;search problems;belief maintenance;belief maintenance evolutionary computation knowledge based systems search problems knowledge representation knowledge acquisition;knowledge representation;constrained optimization problem;knowledge based systems cultural differences constraint optimization genetic programming problem solving evolutionary computation computer science genetic algorithms computer architecture optimization methods;knowledge based systems;evolutionary computing	"""Regional knowledge is determined by function's fitness landscape patterns, such as basins, valleys and multi-modality. Furthermore, for constrained optimization problems, the knowledge of feasible/infeasible regions can also be regards as regional knowledge. Therefore, it would be very helpful if there were a general tool to allow for the representation of regional knowledge, which can be acquired from evolutionary search and then be in reverse applied to guide the search. We define region-based schemata, implemented as belief-cells, which can provide an explicit mechanism to support the acquisition, storage and manipulation of the regional knowledge of a function landscape. In a cultural algorithm framework, the belief space can """"contain"""" a set of these schemata, which can be arranged in a hierarchical architecture, and can be used to guide the search of the evolving population, i.e. region-based schemata can be used to guide the optimization search in a direct way by pruning the infeasible regions and promoting the promising regions. The experiments for an engineering problem with nonlinear constraints indicate the potential behind this approach."""	evolutionary computation;knowledge-based systems	Xidong Jin;Robert G. Reynolds	1999		10.1109/TAI.1999.809762	interactive evolutionary computation;fitness landscape;cultural algorithm;computer science;knowledge management;artificial intelligence;knowledge-based systems;machine learning;evolutionary computation	AI	25.420772107417278	-9.89020197380763	140595
a7edc842db3a1de52d32868dccfd010f488f6183	the compound class of linear failure rate-power series distributions: model, properties, and applications	62f10;maximum likelihood estimation;linear failure rate distribution;moments;power series class of distributions;monte carlo simulation;em algorithm;60e05	In this paper, a new class of distributions is introduced which generalizes the linear failure rate distribution and is obtained by compounding this distribution and power series class of distributions. This new class of distributions is called the linear failure rate-power series distributions and contains some new distributions such as linear failure rate-geometric, linear failure rate-Poisson, linear failure rate-logarithmic, linear failure rate-binomial distributions and Rayleigh-power series class of distributions. Some former works such as exponential-power series class of distributions, exponential-geometric, exponential-Poisson and exponential-logarithmic distributions are special cases of the new proposed model. The ability of the linear failure rate-power series class of distributions is in covering five possible hazard rate function i.e., increasing, decreasing, upside-down bathtub (unimodal), bathtub and increasing-decreasing-increasing shaped. Several properties of the this class of distributions such as moments, maximum likelihood estimation procedure via an EM-algorithm and inference for a large sample, are discussed in this paper. In order to show the flexibility and potentiality, the fitted results of the new class of distributions and some its submodels are compared using two real data sets. Keyword: EM-algorithm; Linear failure rate distribution; Maximum likelihood estimation; Moments; Monte Carlo simulation; Power series class of distributions.	expectation–maximization algorithm;exponential time hypothesis;failure rate;monte carlo method;rayleigh–ritz method;simulation;time complexity	Eisa Mahmoudi;Ali Akbar Jafari	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1005232	inverse distribution;econometrics;mathematical optimization;stability;expectation–maximization algorithm;heavy-tailed distribution;statistical parameter;mathematics;moment;maximum likelihood;k-distribution;statistics;monte carlo method	ML	31.064366417139443	-20.816402787059484	140605
f02f1edf1ccfa849f368658b05059ad4bb443c69	estimation of time-varying long memory parameter using wavelet method	transformation ondelette;metodo cuadrado menor;metodo estadistico;methode moindre carre;long range dependence;time varying;analisis numerico;donnee economique;estimacion;medio ambiente;ordinary least square estimation;60g99;estimator robustness;statistical simulation;stochastic process;least squares method;long memory process;62m10;ondelette;simulacion numerica;datos financieros;time varying parameter;91b70;economic data;statistical method;65t60;donnee financiere;processus stationnaire;analyse numerique;financial data;financial economics;algorithme;processus memoire longue;algorithm;robustez estimador;simulacion estadistica;numerical analysis;dependance du temps;42c40;time dependence;estimation;dato economico;methode statistique;simulation statistique;seasonality;60g05;environment;indexation;simulation numerique;long memory;ordinary least square;processus stochastique;long range dependent;dependance longue distance;simulation study;environnement;gegenbauer process;discrete wavelet packet transform;transformacion ondita;methode affinement;proceso estacionario;proceso estocastico;62p12;60g10;dependencia del tiempo;91g60;wavelets;stationary process;wavelet transformation;non stationarity;dependencia larga distancia;numerical simulation;algoritmo;robustesse estimateur	Stationary long memory processes have been extensively studied over the past decades. When we deal with financial, economic, or environmental data, seasonality and time-varying long-range dependence can often be observed and thus some kind of non-stationarity exists. To take into account this phenomenon, we propose a new class of stochastic processes: locally stationary k-factor Gegenbauer process. We present a procedure to estimate consistently the time-varying parameters by applying discrete wavelet packet transform. The robustness of the algorithm is investigated through a simulation study. And we apply our methods on Nikkei Stock Average 225 (NSA 225) index series.	wavelet	Zhiping Lu;Dominique Guégan	2011	Communications in Statistics - Simulation and Computation	10.1080/03610918.2010.549986	computer simulation;economic data;wavelet;stochastic process;stationary process;econometrics;estimation;ordinary least squares;numerical analysis;calculus;mathematics;natural environment;least squares;seasonality;statistics	Robotics	33.73694367181447	-22.106711228566258	140667
4745d23517a26f86a5c2c86505dc950f7d193d55	a novel optimal replication allocation strategy for particle swarm optimization algorithms applied to simulation optimization problem		Abstract This study proposes a simulation optimization algorithm, called optimal replication allocation strategy (ORAS), for particle swarm optimization (PSO) algorithms in simulation models with high computing cost in large design spaces. In this study, the PSO is employed to explore and exploit near-optimal or optimal solution in the design space. Given the uncertainties in real-word applications, a simulation model is constructed to evaluate the performance of each design alternative. Optimal computing budget allocation (OCBA), a state-of-the-art resampling method, is combined with metaheuristic principles to improve the accuracy of estimating best solutions and enhancing efficiency by intelligently allocating the number of replications. However, the solution space is nonlinear or multimodal; that is, various local or global solutions exist. In OCBA, the probability of correct selection (P(CS)) in the currect best solution serves as an important measurement. P(CS) refers to the probability that the “best” of k populations is selected according to some specified criteria of best. OCBA can halt allocation when P(CS) reaches higher than the desired value, i.e., P(CS)*. The multimodal solution space features a high probability of reaching a low P(CS) as many solutions perform closely to the current best of each generation. This situation indicates that P(CS) cannot achieve P(CS)*; OCBA cannot stop allocation, and additional computational cost may be wasted. In this study, we redefine and modify P(CS). The new version is P(CS E ), which considers calculation of global best, called the super individual, instead of current best solution within a confidence level. The proposed ORAS can provide an asymptotically optimal allocation rule for combining with population metaheuristics based on P(CS E ). We apply ORAS using an original particle swarm optimization (PSO) and two variants of PSO to address stochastic buffer allocation problem and stochastic function optimization problem compared with several state-of-the-art technologies from literature. The resulting ORAS with three PSOs is an effective procedure as it intelligently utilizes limited computing resources. Numerical tests indicate that ORAS increases P(CS E ) in each generation and subsequently enhances the efficiency of PSO algorithms.	algorithm;mathematical optimization;optimization problem;particle swarm optimization;simulation	Chun-Chih Chiu;James T. Lin	2018	Appl. Soft Comput.	10.1016/j.asoc.2018.07.017	mathematics;simulation modeling;nonlinear system;resampling;mathematical optimization;asymptotically optimal algorithm;metaheuristic;algorithm;optimization problem;particle swarm optimization;population	ECom	28.38602246423226	-12.662847061110904	140948
36524823f21739ebb5b279ab1feaa323f1a6ea98	textural features extraction for image integrity verification	data hiding;image manipulation;malicious manipulation;image integrity;texture features;features;image authentication;digital signature;cryptography;feature extraction;soft authentication;statistics;cutting and pasting attacks;image verification	Soft authentication intends to distinguish specific acceptable image manipulations from malicious ones. In this article, we propose a soft authentication scheme based on texture features and digital signature. It can detect and locate malicious manipulations made to individual image blocks, and verify the integrity of the overall image. It makes use of the invariance of second-order statistics on adjacent pixels to geometric transformations which are generally considered as content preserving modifications. A set of characteristics are extracted from each block, then processed to form a hash to be embedded in an other distant block. Furthermore, a cryptographic digital signature is incrementally formed, in a CBC manner to render the scheme resistant to content cutting and pasting attacks. Experimental results show the efficiency of the proposed scheme.	algorithm;authentication;closed-circuit television;digital signature;embedded system;hash function;pixel;public-key cryptography;randomness;rendering (computer graphics)	Samia Boucherkha;Mohamed Benmohamed	2008	IJESDF	10.1504/IJESDF.2008.020944	computer vision;digital signature;feature extraction;computer science;cryptography;internet privacy;computer security	Security	38.78869105589115	-11.407204104169516	141077
e61870751d160dc8b3d6cd9505a96a17ae4dda82	sustainable dynamics of size-structured forest under climate change	size structured population model;forestry;biologia de poblacions models matematics;desenvolupament sostenible models matematics;carbon sequestration;info eu repo semantics article;sustainability;beta function;forests and forestry climatic factors;population biology mathematical models;boscos i silvicultura factors climatics;sustainable development mathematical models	This work investigates the impact of global climate change on the sustainable growth of forest, namely, on its aggregated characteristics such as the number of trees, the basal area, and the amount of carbon sequestrated in the stand. The forest dynamics is described by a nonlinear size-structured population model. The existence of a steady state regime is proven and explicit formulas for the aggregated characteristics are obtained. A numeric simulation on realistic data illustrates and extends the analytic results obtained. © 2012 Elsevier Ltd. All rights reserved.	aggregate data;basal (phylogenetics);nonlinear system;population model;simulation;steady state;the forest	Natali Hritonenko;Yuri Yatsenko;Renan-Ulrich Goetz;Angels Xabadia	2012	Appl. Math. Lett.	10.1016/j.aml.2011.12.020	beta function;carbon sequestration;sustainability	AI	35.47038616184914	-14.530950162953442	141079
2e288175e6a7770e5f7d380f8ef0e67e0ea137a7	an algorithmic construction of four-level response surface designs	genetique;experimental design;metodo estadistico;estimacion;algorithmique;aplicacion;62k20;genetica;05bxx;efficiency;simulacion numerica;rotatability;plan experiencia;statistical method;response surface design;algoritmo genetico;genetics;surface reponse;62k99;estimation;plan experience;algorithmics;algoritmica;methode statistique;simulation numerique;superficie respuesta;algorithme genetique;genetic algorithm;genetic algorithms;optimization;plan surface reponse;four level factors;application;response surface;ha statistics;response surface designs;numerical simulation	Response surface methodology is widely used for developing, improving and optimizing processes in various fields. In this paper we present a method for constructing four-level design matrices in order to explore and optimize response surfaces where the predictor variables are each at four equally spaced levels, by utilizing a genetic algorithm. The produced designs achieve both properties of near-rotatability and estimation efficiency. Note: The following files were submitted by the author for peer review, but cannot be converted to PDF. You must view these files (e.g. movies) online.	genetic algorithm;kerrison predictor;level design;portable document format;response surface methodology	Christos Koukouvinos;Kalliopi Mylona;Dimitris E. Simos;A. Skountzou	2009	Communications in Statistics - Simulation and Computation	10.1080/03610910903259634	computer simulation;econometrics;response surface methodology;genetic algorithm;mathematics;algorithmics;algorithm;statistics	AI	29.079428074292537	-15.306411640158451	141768
82fe0ae786b4d6486a1a0ed4009ea8ed441b8c8f	fragile watermarking scheme with extensive content restoration capability	tampering rate;content restoration;fragile watermarking;data extraction	This paper proposes a novel fragile watermarking scheme capable of recovering the original principal content in extensive areas. The watermark data are made up of two parts: reference-bits derived from the principal content of the host image and containing some necessary redundancy, and hash-bits matching the content of local blocks. While the reference-bits are embedded into the entire host image, the hash-bits are embedded into the local blocks. On the authentication side, after identifying the tampered areas, the watermark data extracted from the reserved regions can provide sufficient information to restore the principal content of host image even when the rate of tampered blocks is large.	circuit restoration	Xinpeng Zhang;Shuozhong Wang;Guorui Feng	2009		10.1007/978-3-642-03688-0_24	internet privacy;world wide web;computer security	Vision	38.86107365939294	-11.662284039176575	141892
d9dc13c12a4556aa8bdf26e61fa2b24d5f9701a5	reconstruction of conditional distribution field based on empirical data	conditional distribution			Alexey Ya. Chervonenkis	2002			statistics;marginal distribution;chain rule (probability);posterior probability;conditional probability distribution;regular conditional probability;conditional variance;sampling distribution;mathematics;normal-gamma distribution	ML	30.69407855798059	-23.667279174681774	141971
d4413ae2fbcff83e8a3f06acabd4b0b4c732724d	compstat: proceedings in computational statistics, 2000			computation;computational statistics	Eric R. Ziegel	2002	Technometrics	10.1198/tech.2002.s680	econometrics;compstat;mathematics;computational statistics	HPC	27.037844238391422	-23.151710167264547	142359
a97f28547907234f2febd3efaec1553d19f2c832	model fitting with sufficient random sample coverage	random sampling;probabilistic algorithm;non linear model;ransac;probabilistic algorithms;robust regression;synthetic data;model fitting	It has been observed previously that the number of iterations required to derive good model parameter values used by RANSAC-like model estimators is too optimistic. We present the derivation of an analytical formula that allows the calculation of the sufficient limit of iterations needed to obtain good parameter values with the prescribed probability for any number of model parameters. It explains the values that had been found experimentally for certain numbers of model parameters by others very well. Furthermore, the improvement that our approach of SUfficient Random SAmple Coverage (SURSAC) offers, in comparison to the original RANSAC algorithm as well as to its adaptive modification by Hartley and Zisserman, is demonstrated with synthetic data for the case of a non-linear model function over a wide range of outlier fractions and different ratios of inlier and outlier densities.	algorithm;curve fitting;experiment;hartley (unit);iteration;linear model;nonlinear system;random sample consensus;synthetic data	Norbert Scherer-Negenborn;Rolf Schaefer	2010	International Journal of Computer Vision	10.1007/s11263-010-0329-7	sampling;econometrics;mathematical optimization;probabilistic analysis of algorithms;ransac;computer science;machine learning;mathematics;randomized algorithm;robust regression;statistics;synthetic data	Vision	28.87175227617395	-22.987609021730968	142661
a4eebb5c2eb6c5a9b0ef84848e867c32b2391cd9	source-item production laws for the case that items have multiple sources with fractional counting of credits	productivity;mathematical formulas;bibliometrics	This article extends two previous articles on the application of martingale theory to the well-known generalized success-breeds-success principle, generalized in order to comprise also other phenomena such as failure breeds failure and other production rhythms. The extension lies in the fact that items are allowed to have multiple sources, in which case fractional assignment of weights is taking place. In this sense this article differs from another one in which total counts are the assignment rule. Martingale properties of Y t (q), the number of sources with a weight q at time t are studied. In addition to that, applying a steady state assumption, we develop formulae for E(P(t, q)), the expected fraction of sources with weight q at time t. We show that the irregular shapes of this function of q, which are encountered in practice, can be explained by this framework to a large extent.		Leo Egghe	1996	JASIS	10.1002/(SICI)1097-4571(199610)47:10%3C730::AID-ASI2%3E3.0.CO;2-3	library science;econometrics;formula;productivity;social science;bibliometrics;computer science;calculus;weighting;mathematics;management;statistics	Robotics	35.111499634513606	-18.87100001177561	142949
1f4b6cb09c1ec7833cd84e7360e0160524dfd6dd	quas-monte carlo strategies for stochastic optimization		In this paper we discuss the issue of solving stochastic optimization problems using sampling methods. Numerical results have shown that using variance reduction techniques from statistics can result in significant improvements over Monte Carlo sampling in terms of the number of samples needed for convergence of the optimal objective value and optimal solution to a stochastic optimization problem. Among these techniques are stratified sampling and Quasi-Monte Carlo sampling. However, for problems in high dimension, it may be computationally inefficient to calculate Quasi-Monte Carlo point sets in the full dimension. Rather, we wish to identify which dimensions are most important to the convergence and implement a Quasi-Monte Carlo sampling scheme with padding, where the important dimensions are sampled via Quasi-Monte Carlo sampling and the remaining dimensions with Monte Carlo sampling. We then incorporate this sampling scheme into an external sampling algorithm (ES-QMCP) to solve stochastic optimization problems.	algorithm;cma-es;mathematical optimization;monte carlo method;numerical linear algebra;optimization problem;quasi-monte carlo method;sampling (signal processing);stochastic optimization;stratified sampling;variance reduction	Shane S. Drew;Tito Homem-de-Mello	2006	Proceedings of the 2006 Winter Simulation Conference			ML	31.025320674278483	-14.971959200929318	143084
c6b49ccd3631a2324343a208221b8297b0526446	generalised long-memory garch models for intra-daily volatility	variation journaliere;long range dependence;time varying;gegenbauer polynomial;garch model;metodo monte carlo;polinomio gegenbauer;gegenbauer processes;65c05;analisis datos;modelo autorregresivo;long memory process;62e17;simulation;methode monte carlo;processus autoregressif;ajustement;loi conditionnelle;simulacion;ley condicional;intra daily volatility;autoregressive model;equation polynomiale;polynomial equation;fitting;processus memoire longue;data analysis;modele garch;ecuacion polinomial;autoregressive processes;polynome gegenbauer;monte carlo method;long memory;statistical computation;calculo estadistico;long range dependent;gegenbauer polynomials;taux change;dependance longue distance;fractional integral;exchange rate;analyse donnee;calcul statistique;62p05;variacion diaria;ajuste;monte carlo simulation;modele autoregressif;autoregressive conditional heteroskedasticity;conditional distribution;tasa cambio;g garch;dependencia larga distancia;daily variation	The class of fractionally integrated generalised autoregressive conditional heteroskedastic (FIGARCH) models is extended for modelling the periodic long-range dependence typically shown by volatility of most intra-daily financial returns. The proposed class of models introduces generalised periodic long-memory filters, based on Gegenbauer polynomials, into the equation describing the time-varying volatility of standard GARCH models. A fitting procedure is illustrated and its performance is evaluated by means of Monte Carlo simulations. The effectiveness of these models in describing periodic long-memory volatility patterns is shown through an empirical application to the Euro-Dollar intra-daily exchange rate.	volatility	Silvano Bordignon;Massimiliano Caporin;Francesco Lisi	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.11.004	autoregressive conditional heteroskedasticity;econometrics;calculus;mathematics;financial models with long-tailed distributions and volatility clustering;stochastic volatility;statistics;monte carlo method	ML	33.75548906463111	-22.076677914218912	143150
34b3bb5c7a4cf69c291cee882992baf00ac30362	testing the dinosaur hypothesis under empirical datasets	q335 artificial intelligence;genetic program;dinosaur hypothesis;genetic programming	In this paper we present the Dinosaur Hypothesis, which states that the behaviour of a market never settles down and that the population of predictors continually co-evolves with this market. To the best of our knowledge, this observation has only been made and tested under artificial datasets, but not with real data. In this work, we attempt to formalize this hypothesis by presenting its main constituents. We also test it with empirical data, under 10 international datasets. Results show that for the majority of the datasets the Dinosaur Hypothesis is not supported.	entity–relationship model;experiment;fitness function	Michael Kampouridis;Shu-Heng Chen;Edward P. K. Tsang	2010		10.1007/978-3-642-15871-1_21	genetic programming;computer science;artificial intelligence;machine learning	ML	24.99365642310491	-9.939388378896602	143350
0a3a94e65c5c74c56bffec28a6a073cddadcb356	multi-core parallel tempering bayeslands for basin and landscape evolution		In recent years, Bayesian inference has become a popular methodology for the estimation and uncertainty quantification of parameters in geological and geophysical forward models. Badlands is a basin and landscape evolution forward model for simulating topography evolution at a large range of spatial and time scales. Previously, Bayesian inference has been used for parameter estimation and uncertainty quantification in Badlands, an extension known as Bayeslands. It was demonstrated that the posterior surface of these parameters could exhibit highly irregular features such as multi-modality and discontinuities making standard Markov Chain Monte Carlo (MCMC) sampling difficult. Parallel tempering (PT) is an advanced MCMC method suited for irregular and multi-modal distributions. Moreover, PT is more suitable for multi-core implementations that can speed up computationally expensive geophysical models. In this paper, we present a multi-core PT algorithm implemented in a high performance computing architecture for enhancing Bayeslands. The results show that PT in Bayeslands not only reduces the computation time over a multicore architecture, but also provides a means to improve the sampling process in a multi-modal landscape. This motivates its usage in larger-scale problems in basin and landscape evolution models.	algorithm;algorithmic efficiency;analysis of algorithms;badlands;computation;computer architecture;estimation theory;gradient;informatics;köppen climate classification;machine learning;markov chain monte carlo;modal logic;modality (human–computer interaction);multi-core processor;parallel tempering;physical vapor deposition;sampling (signal processing);simulation;supercomputer;time complexity;topography;uncertainty quantification	Rohitash Chandra;R. Dietmar Müller;Ratneel Deo;Nathaniel Butterworth;Tristan Salles;Sally Cripps	2018	CoRR		architecture;geophysics;mathematical optimization;uncertainty quantification;geology;sampling (statistics);parallel tempering;estimation theory;markov chain monte carlo;classification of discontinuities;bayesian inference	ML	38.57478484728574	-22.891320250260986	144114
3b4881c186daaf935515099dc252a28bfc8253ff	analytic properties of noncentral distributions	grado libertad;densite probabilite;asymptotics;probability density;fonction repartition;degree of freedom;fonction speciale;monotonie;densidad probabilidad;funcion especial;funcion distribucion;distribution function;special function;monotonicity;unimodalite;monotonia;noncentral distributions;unimodality;freedom degree;mode;degre liberte	"""A survey of analytic properties of the noncentral @g^2""""n(@l), F""""n""""""""""""1"""",""""n""""""""""""2(@l), and student's t""""m(@l) distributions is given. Emphasis is put on unimodality problems and in particular the modes of these distributions are discussed regarding their dependence on the degrees of freedom n, n""""1, n""""2, m, and the noncentrality parameter @l."""		Andrea van Aubel;Wolfgang Gawronski	2003	Applied Mathematics and Computation	10.1016/S0096-3003(02)00316-8	probability density function;combinatorics;asymptotic analysis;monotonic function;unimodality;distribution function;calculus;mathematics;geometry;degrees of freedom;mode;statistics	Theory	34.24346463460593	-21.027011254861055	144164
9466ad7bebec89e706b50d4544b1f0f6e6ec8c3f	phase i monitoring with nonparametric mixed-effect models	local linear kernel smoothing;parametric bootstrap;multiple testing;mixed effect models			Marjan Rajabi;Mohammad Reza Faridrohani;Shoja'eddin Chenouri	2017	Quality and Reliability Eng. Int.	10.1002/qre.2157	econometrics;mathematical optimization;mathematics;multiple comparisons problem;nonparametric regression;statistics;semiparametric regression	SE	30.365926499489213	-23.92110409954438	144299
7feb8eb151b8ba8b803819fe97fc050730354796	a robust and low-cost video fingerprint extraction method for copy detection		Video fingerprinting for content-based video identification is a very useful task for the management and monetization of copyrighted content distribution. The main challenges of monitoring and copy detection systems are: a) the effective identification of highly transformed videos (robustness) and b) computational efficiency which may be relevant for some applications. Typically, most video fingerprinting methods focus on robustness leaving aside computational efficiency. However, for real-time applications are necessary low computational cost detection methods, for instance, in illegal content monitoring in video streaming distributions. Therefore, in this paper, we propose a low-cost and effective video fingerprint extraction method based on the combination of content-based features using both acoustic and visual video components. Our method is capable of detecting video copies by using computationally efficient fingerprints while maintaining robustness against the decrease in quality and content preserved distortions, which are frequent but severe attacks.	acoustic cryptanalysis;algorithmic efficiency;computation;data rate units;database;digital distribution;digital video fingerprinting;distortion;emoticon;fingerprint (computing);key frame;matlab;mathematical optimization;monetization;real-time clock;robustness (computer science);sensor;streaming media;synergy;video copy detection	Zobeida Jezabel Guzman-Zavaleta;Claudia Feregrino Uribe;Miguel Morales-Sandoval;Alejandra Menendez-Ortiz	2016	Multimedia Tools and Applications	10.1007/s11042-016-4168-6	computer vision;computer science;video tracking;multimedia;internet privacy	Metrics	38.210188612856506	-12.805131038100004	144564
d3e778bfec052ad1eeb32c1aa285d3923055487e	gadf — genetic algorithms for distribution fitting	cumulative distribution function;cumulative distribution functions;kolmogorov smirnov test;steel manufacture genetic algorithms statistical distributions;probability density function;random variables;maximum likelihood estimation;mean absolute error;statistical distributions;distribution fitting statistics genetic algorithms;biological cells;chemical elements;steel manufacture;mean absolute error metric;random variable;statistics;mathematical model;steel;industrial application;genetic algorithm;genetic algorithms;kolmogorov smirnov test metric;distribution fitting;probability density functions;maximum likelihood estimation biological cells random variables mathematical model steel data models chemical elements;data models;kolmogorov smirnov test metric genetic algorithms distribution fitting probability density functions cumulative distribution functions mean absolute error metric	Distribution fitting is a widely recurring problem in different fields such as telecommunication, finance and economics, sociology, physics, etc. Standard methods often require solving difficult equations systems or investments in specialized software. The paper presents a new approach to distribution fitting that exploits Genetic Algorithms in order to simultaneously identify the distribution type and tune its parameters by exploiting a dataset sampled from the observed random variable and a set of distribution families. The strength of this approach lies in the easiness of the expansion of this set: in fact distributions are simply described by means of their probability density functions and cumulative distribution functions, which are well-known. This approach employs two different score metrics, the Mean Absolute Error and the Kolmogorov-Smirnov test, that are linearly combined in order to find the best fitting distribution. The results obtained in an industrial application are presented and discussed.	complex systems;f1 score;genetic algorithm;linear equation;mathematical optimization;multi-objective optimization;nonlinear system;pareto efficiency;portable document format;systems design;windows legacy audio components	Valentina Colla;Gianluca Nastasi;Nicola Matarese	2010	2010 10th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2010.5687298	random variable;econometrics;mathematical optimization;probability density function;genetic algorithm;categorical distribution;mathematics;asymptotic distribution;statistics;probability integral transform;three-point estimation	Robotics	27.60414019365316	-17.56950309530348	144719
36831800c7e716f266fc096323fa9a36337520af	testing for unit root against stationarity using the likelihood ratio test	estimacion sesgada;metodo estadistico;probability theory and statistics;finite sample;likelihood ratio;test statistique;statistical simulation;metodo monte carlo;aplicacion;secondary 62m02;65c05;modelo autorregresivo;analyse tendance;test estadistico;62e17;simulacion numerica;echantillon fini;espace etat;error sistematico;statistical test;methode monte carlo;trend analysis;processus autoregressif;statistical method;stationary alternative;valor critico;autoregressive model;raiz unitaria;lr test;simulacion estadistica;first order;state space method;autoregressive processes;bias;bias correction;methode espace etat;methode statistique;simulation statistique;monte carlo method;state space;simulation numerique;likelihood ratio test;simulation study;unit root;primary 62m10;valeur critique;racine unitaire;espacio estado;state space model;rapport vraisemblance;monte carlo simulation;test razon verosimilitud;modele autoregressif;application;test rapport vraisemblance;critical value;biased estimation;estimation biaisee;sannolikhetsteori och statistik;erreur systematique;metodo espacio estado;analisis tendencia;numerical simulation;relacion verosimilitud	In a first order autoregressive model with drift, we derive the likelihood ratio test for a unit root against the stationary alternative. We also derive the test in a state space model with trend. Finite sample and asymptotic critical values are obtained by Monte Carlo simulations. A simulation study investigates the power performance of the likelihood ratio test.	autoregressive model;monte carlo method;simulation;state-space representation;stationary process	Nikolay Angelov;Rolf Larsson	2007	Communications in Statistics - Simulation and Computation	10.1080/03610910601158401	computer simulation;econometrics;likelihood principle;score test;likelihood-ratio test;calculus;mathematics;unit root test;statistics;monte carlo method	ML	33.20248959038132	-22.16108358590419	144898
650acbb7b37fc7363c81713f37cc5b06af9d0607	dealing with the initial observation in the lm unit root test	cdf of normal distribution;initial condition;lm unit root test	Disclaimer: This is a version of an unedited manuscript that has been accepted for publication. As a service to authors and researchers we are providing this version of the accepted manuscript (AM). Copyediting, typesetting, and review of the resulting proof will be undertaken on this manuscript before final publication of the Version of Record (VoR). During production and pre-press, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal relate to this version also.		Hyejin Lee;Dong-Yop Oh	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.950747	econometrics;mathematics;initial value problem;statistics	ML	34.38060131635104	-17.590556515264055	145065
a5884849f613aeec466e1401904067e79d994ead	adaptive profile-empirical-likelihood inferences for generalized single-index models	generalized single index model;link function;journal article;confidence interval;drntu science mathematics statistics;adaptive empirical likelihood;qlmele	We study generalized single-index models and propose an efficient equation for estimating the index parameter and unknown link function, deriving a quasi-likelihood-based maximum empirical likelihood estimator (QLMELE) of the index parameter. We then establish an efficient confidence region for any components of the index parameter using an adaptive empirical likelihood method. A pointwise confidence interval for the unknown link function is also established using the QLMELE. Compared with the normal approximation proposed by Cui et al. [Ann Stat. 39 (2011) 1658], our approach is more attractive not only theoretically but also empirically. Simulation studies demonstrate that the proposed method provides smaller confidence intervals than those based on the normal approximation method subject to the same coverage probabilities. Hence, the proposed empirical likelihood is preferable to the normal approximation method because of the complicated covariance estimation. An application to a real data set is also illustrated.	single-index model	Zhensheng Huang;Zhen Pang;Riquan Zhang	2013	Computational Statistics & Data Analysis	10.1016/j.csda.2012.12.006	econometrics;mathematical optimization;confidence interval;confidence distribution;mathematics;likelihood function;quasi-maximum likelihood;statistics	ML	29.6833302267283	-23.218517310607485	145193
2bfc2c62b0cadc34f5886884d02e278938bb0f1c	assessing fitness for use: the expected value of spatial data sets	decision making under uncertainty;spatial data;accuracy assessment;digital elevation model;remote sensing imagery;loss function;remote sensing;value of information;sampling strategy;systematic sampling	This paper proposes and illustrates a decision analytical approach to compare the value of alternative spatial data sets. In contrast to other work addressing value of information, its focus is on value of control. This is a useful concept when choosing the best data set for decision making under uncertainty due to error in the reported data. Application of the concept requires probabilistic accuracy measures and a loss function representing the cost of incorrect judgement about some target property. This is illustrated by an assessment of the suitability of two digital elevation models (DEMs) for determining the volume of sand required for building a container port. To demonstrate  exibility of the approach, accuracy assessment was based on both a random and a systematic sample of error data, using design-based estimation and model-based prediction, that is geostatistics. Analysis results included the expected loss for each combination of DEM and sampling strategy. These indicated that both DEMs were equally suitable for the intended use. Operational practicability of the method is highly dependent on the willingness of database producers to give access to sample information similar to the quick looks provided to potential users of remote sensing imagery.	decision theory;digital elevation model;fitness function;loss function;sampling (signal processing)	Sytze de Bruin;Arnold Bregt;Marc van de Ven	2001	International Journal of Geographical Information Science	10.1080/13658810110053116	systematic sampling;econometrics;digital elevation model;computer science;value of information;data mining;spatial analysis;statistics;remote sensing;loss function	HCI	26.986699249063513	-18.124384776307462	145481
f7092f88507ccc2fe10ba75471d5cdda780a8e4d	a hybrid watermarking scheme for ccl-applied video contents	watermarking;neodymium;robust watermarking;hybrid watermarking;hybrid video watermarking scheme;digital watermark;fragile watermarks;copyright;user generated video contents;watermarking copyright internet video coding;copyright protection;video coding;watermarking robustness noise internet neodymium correlation radio frequency;creative common license;radio frequency;internet;ccl hybrid watermarking video watermarking;robustness;correlation;video watermarking;digital watermarking scheme;ccl;noise;fragile watermarks ccl applied video contents creative common license user generated video contents internet copyright protection digital watermarking scheme hybrid video watermarking scheme;ccl applied video contents	With widely spread user-generated video contents through the Internet, copyright protection issues have been arising. In this situation, creative commons license(CCL) has been introduced to make convenient sharing environment and reflect intention of content creators. However, the CCL is prone to be ignored by users because it is not protected by law or technical methods. In order for technical help, digital watermarking scheme would be applied. Conventional watermarking system, unfortunately, has many problems to support CCL-applied video contents. This paper proposes a hybrid video watermarking scheme devised for the various purposes including CCL-applied video. Experimental results show that robust watermarks are detected after scaling, cropping, clipping, and frame rate changing while fragile watermarks inform whether manipulation has been occurred on those attacks.	clipping (computer graphics);digital watermarking;image scaling;internet;technical support;user-generated content	Hee-Dong Kim;Tae-Woo Oh;Ji Won Lee;Heung-Kyu Lee	2011	3rd European Workshop on Visual Information Processing	10.1109/EuVIP.2011.6045512	geography;multimedia;internet privacy;computer security	HCI	37.912485937594646	-12.74414135925444	145508
95d79b9abb1fa5fff8a086a67f455f5b88cb6f09	a robust algorithm of encrypted medical volume data retrieval based on 3d dwt and 3d dft		Image retrieval technology allows doctors to query diagnosed images which are similar to the current diagnostic image from large medical image library, and then doctors develop accurate treatment programs and reasonable clinical diagnosis. However, medical image data may be leaked and tampered with during the transmission of the Internet, resulting in data security issues. Therefore, this paper proposes a robust algorithm for encrypted medical volume data retrieval based on DWT (Discrete Wavelet Transform) and DFT (Discrete Fourier Transform). Firstly, we encrypt medical volume data. Secondly, we extract the DWT-DFT coefficients as encrypted medical images' feature vector and establish features vector database. Then, we compute the NC (The Normalized Cross-correlation) value between the feature vector of query medical volume data and each one in the features vector database. Next, automatically return the retrieved medical image. Finally, we decrypt returned medical image that we retrieved. Experimental results demonstrate that the algorithm has strong robustness against common attacks and geometric attacks.	algorithm;coefficient;cross-correlation;cryptography;data retrieval;data security;discrete fourier transform;discrete wavelet transform;encryption;feature vector;image retrieval;internet;jpeg;logistic map;medical imaging	Shuangshuang Wang;Jingbing Li;Chunyan Zhang;Jintao Liang;Zhaohui Wang	2017	2017 IEEE 15th International Conference on Software Engineering Research, Management and Applications (SERA)	10.1109/SERA.2017.7965720	data mining;robustness (computer science);computer science;encryption;feature extraction;discrete wavelet transform;algorithm;discrete fourier transform;image retrieval;feature vector;data retrieval	Visualization	37.48501609165856	-10.8363659133898	145570
bb626162c27752a8e9b5f83d27659ce69402865e	asymptotic distribution of two-sample empirical u-quantiles with applications to robust tests for shifts in location	62g35;location problem;u;60f05;nonparametric test;time series;60f17;u statistics;functionals of absolutely regular processes;heavy tailed distribution;dependent data;asymptotic distribution;62g30;hodges lehmann estimator;two sample location problem;weak dependence;chaotic dynamical system;time series model	We derive the asymptotical distributions of two-sample U-statistics and two-sample empirical U-quantiles in the case of weakly dependent data. Our results apply to observations that can be represented as functionals of absolutely regular processes, including e.g. many classical time series models as well as data from chaotic dynamical systems. Based on these theoretical results we propose a new robust nonparametric test for the two-sample location problem, which is constructed from the median of pairwise differences between the two samples. We inspect the properties of the test in the case of weakly dependent data and compare the performance with classical tests such as the t-test and Wilcoxon's two-sample rank test with corrections for dependencies. Simulations indicate that the new test offers better power than the Wilcoxon test in case of skewed and heavy tailed distributions, when at least one of the two samples is not very large. The test is then applied for detecting shifts of location in some weakly dependent time series, which are contaminated by outliers.		Herold Dehling;Roland Fried	2012	J. Multivariate Analysis	10.1016/j.jmva.2011.08.014	econometrics;mathematical optimization;time series;mathematics;statistics	Metrics	30.444518252497808	-22.46619335027957	145833
684b4cd2b2759b9b06ca15cea1ed5cc593ccc1ca	nonparametric function fitting in the presence of nonstationary noise		The article refers to the problem of regression functions esti- mation in the presence of nonstationary noise. We investigate the model yi = R (xi)+� i ,i =1 , 2 ,...n ,w herexi is assumed to be the d-dimensional vector, set of deterministic inputs, xi ∈ S d , yi is the scalar, set of proba- bilistic outputs, andi is a measurement noise with zero mean and vari- ance depending on n. R (.) is a completely unknown function. One of the possible solutions of finding function R (.) is to apply non-parametric methodology - algorithms based on the Parzen kernel or algorithms de- rived from orthogonal series. The novel result of this article is the analysis of convergence for some class of nonstationarity. We present the conditions when the algorithm of estimation is convergent even when the variance of noise is divergent with number of observations tending to infinity. The re- sults of numerical experiments are presented.	curve fitting	Tomasz Galkowski;Miros&#x0142;aw Pawlak	2014		10.1007/978-3-319-07173-2_45	econometrics;mathematical optimization;machine learning;mathematics;statistics	Vision	30.729886557900162	-23.200121744083827	146019
c880db90b438c33201df7c8b3f929dd90dd8f66f	evolution of multivariate copulas in continuous and discrete processes				Yasukazu Yoshizawa;Naoyuki Ishimura	2018	Int. Syst. in Accounting, Finance and Management	10.1002/isaf.1420	data mining;copula (linguistics);applied mathematics;partial differential equation;multivariate statistics;computer science	DB	34.74707408094828	-19.286778094892625	146194
1b255cda9c92a5fbaba319aeb4b4ec532693c2a4	dynamic topic models	variational approximation;multinomial distribution;kalman filter;prediction model;state space model;time series model	A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.	approximation algorithm;archive;kalman filter;multinomial logistic regression;predictive modelling;state space;time series;variational principle;wavelet	David M. Blei;John D. Lafferty	2006		10.1145/1143844.1143859	kalman filter;econometrics;dynamic topic model;computer science;state-space representation;machine learning;time series;mathematics;predictive modelling;multinomial distribution;statistics	ML	26.600866968412966	-23.19816015736658	146297
d5cc054276868ddb04ff0ed51c8ffc09f134624c	mathematical aspects of geometric modeling (charles a. micchelli)	geometric model	the latter chapters. The next two chapters develop standard models for simple and multiple linear regression. Chapter 4 discusses a range of regularization techniques to be employed in multiple regression, adaptive ridge regression, principal components regression, partial least-squares regression, and continuum regression. The emphasis in the above as well as in the following chapters is mostly on the prediction aspect. Chapter 5 deals with a wide range of multivariate estimators, procedures, and diagnostics. The treatment in this chapter is based on classical approaches as well as Bayesian analysis with both diffuse and informative prior information. Chapter 6 provides a Bayesian treatment of regression on curves, focusing on the contiguity of regressors. The methods discussed here were mostly developed for spectroscopy, and the methodology is used to introduce a Bayesian alternative to Kriging. Nonlinear regression problems and methods for response selection are discussed in Chapter 7. The final chapter (Chapter 8) deals with pattern recognition from an entirely Bayesian point of view. The five appendices relate to multivariate distribution theory, conditional inference, regularization dominance, matrix results, and partial least-squares algorithm. The book ends with a bibliography of 171 items (of which 23 are dated 1990 or later) and a subject index. The book is suitable for use in graduate courses. As mentioned by the author, particular chapters are sufficiently self-contained in order to be recommended in isolation as reference or reading material. The book will also be found very useful as a reference text by practitioners.	algorithm;bayesian network;emoticon;geometric modeling;information;kriging;nonlinear programming;partial least squares regression;pattern recognition;principal component regression;triune continuum paradigm	Ewald Quak	1995	SIAM Review	10.1137/1037151	applied mathematics;geometric modeling;mathematics;geometry	ML	28.014273500704775	-21.80267797785343	146724
6cc6169fcb1fb7888c2c5eddefd8ea973e90d87f	automated aesthetic selection of evolutionary art by distance based classification of genomes and phenomes using the universal similarity metric	genetique;data compression;genetica;metric;probabilistic approach;classification;observador;similitude;genetics;aesthetics;observateur;compression image;image compression;enfoque probabilista;approche probabiliste;genome;similarity;prediction accuracy;algorithme evolutionniste;metrico;algoritmo evolucionista;esthetique;compresion dato;genoma;similitud;evolutionary algorithm;universal similarity metric;observer;clasificacion;metrique;estetica;compression donnee;compresion imagen	In this paper we present a new technique for automatically approximating the aesthetic fitness of evolutionary art. Instead of assigning fitness values to images interactively, we use the Universal Similarity Metric to predict how interesting new images are to the observer based on a library of aesthetic images. In order to approximate the Information Distance, and find the images most similar to the training set, we use a combination of zip-compression, for genomes, and jpeg-compression of the final images. We evaluated the prediction accuracy of our system by letting the user label a new set of images and then compare that to what our system classifies as the most aesthetically pleasing images. Our experiments indicate that the Universal Similarity Metric can successfully be used to classify what images and genomes are aesthetically pleasing, and that it can clearly distinguish between ugly and pretty images with an accuracy better than the random baseline.	evolutionary art	Nils Svangård;Peter Nordin	2004		10.1007/978-3-540-24653-4_46	data compression;similarity;metric;biological classification;image compression;computer science;artificial intelligence;similitude;evolutionary algorithm;mathematics;observer;algorithm;genome	NLP	38.429614819039564	-16.641356477859034	146867
a96b50ae65f946a47c3e4be5e5859c24e933c126	on pseudo maximum likelihood estimation for multivariate time series models with conditional heteroskedasticity	arch model;r arch;estimation method;regression model;multivariate time series;moving average;charma;ccc;asymptotic variance matrix;arch m;pseudo maximum likelihood;constant conditional correlation;autoregressive conditional heteroskedasticity;conditional distribution;asymptotic variance;conditional heteroskedasticity	S Preface On behalf of the organizers we would like to give all of you a warm welcome. We hope you will enjoy the conference and stay in Uppsala. for their financial support. Moreover, all speakers and contributors are gratefully acknowledged.	conditional entropy;time series	Shuangzhe Liu;Heinz Neudecker	2009	Mathematics and Computers in Simulation	10.1016/j.matcom.2008.12.008	autoregressive conditional heteroskedasticity;conditional probability distribution;econometrics;conditional variance;mathematics;discriminative model;statistics	Vision	30.699113970454473	-23.671674868005706	147333
77eefe6c95964c7b70cba9b674f7da9a2d640ad8	a two parameter variance estimator using auxiliary information	efficiency;bias;product and ratio estimators;simple random sampling;mean squared error	We propose a two parameter ratio-product-ratio estimator for estimating a finite population variance based on simple random sampling without replacement. The bias and mean square error of the proposed estimator are obtained to the first degree of approximation. We have derived the conditions for the parameters under which the proposed estimator has smaller mean square error than the sample variance, ratio and product estimators. We carry out an application showing that the proposed estimator outperforms the traditional estimators using different data sets. 2013 Elsevier Inc. All rights reserved.	mean squared error;order of approximation;sampling (signal processing)	Subhash Kumar Yadav;Cem Kadilar	2014	Applied Mathematics and Computation	10.1016/j.amc.2013.10.044	efficient estimator;minimum mean square error;minimax estimator;bootstrapping;shrinkage estimator;econometrics;mathematical optimization;simple random sample;minimum-variance unbiased estimator;estimator;cramér–rao bound;bayes estimator;stein's unbiased risk estimate;bessel's correction;trimmed estimator;efficiency;bias;mathematics;mean squared error;efficiency;variance;bias of an estimator;estimation theory;orthogonality principle;consistent estimator;invariant estimator;statistics	ML	29.783938939694288	-22.710250461770272	147520
d3c368935ea43e5f39f0113612fd7faf9afd6403	evaluation of integrity verification system for video content using digital watermarking	content extraction;distinguishing attack;digital watermark;digital signature;video watermarking	The effectiveness of a previously proposed system for verifying video content integrity was investigated using a prototype system. Unlike conventional verification systems using digital signatures and fragile watermarking, the proposed verification system embeds timecodes in consecutive frames of the content, extracts them some time later, and checks their continuity to distinguish attacks from regular modifications. Testing demonstrated that the system maintained picture quality, that the watermarks were robust against regular modifications, and that attacks could be detected reliably and be distinguished from regular modifications. This integrity verification system is thus well suited for a variety of applications using video content.	digital signature;digital video;digital watermarking;image quality;prototype;scott continuity;type signature;verification and validation	Takaaki Yamada;Yoshiyasu Takahashi;Yasuhiro Fujii;Ryu Ebisawa;Hiroshi Yoshiura;Isao Echizen	2008		10.1007/978-3-540-68127-4_38	digital watermarking;internet privacy;watermark;world wide web;computer security	Security	38.59212699418827	-12.595688443798366	147553
51d408b4b20dff8a04781c1d95ab3e4aaeec61dd	efficient bayesian estimation of a multivariate stochastic volatility model with cross leverage and heavy-tailed errors	multivariate stochastic volatility;empirical study;asymmetry;stochastic volatility model;heavy tail;natural extension;leverage effect;markov chain monte carlo;markov chain monte carlo methods;stock returns;bayesian estimator;tokyo stock exchange;multi move sampler;discussion paper;heavy tailed error	An efficient Bayesian estimation using a Markov chain Monte Carlo method is proposed in the case of a multivariate stochastic volatility model as a natural extension of the univariate stochastic volatility model with leverage and heavy-tailed errors. Note that we further incorporate cross-leverage effects among stock returns. Our method is based on a multi-move sampler that samples a block of latent volatility vectors. The method is presented as a multivariate stochastic volatility model with cross leverage and heavytailed errors. Its high sampling efficiency is shown using numerical examples in comparison with a single-move sampler that samples one latent volatility vector at a time, given other latent vectors and parameters. To illustrate the method, empirical analyses are provided based on five-dimensional S&P500 sector indices returns.	markov chain monte carlo;monte carlo method;numerical analysis;sampling (signal processing);volatility	Tsunehiro Ishihara;Yasuhiro Omori	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2010.07.015	forward volatility;econometrics;volatility;constant elasticity of variance model;markov chain monte carlo;heavy-tailed distribution;mathematics;stochastic volatility;empirical research;asymmetry;statistics	ML	26.514440575634556	-20.97118488926349	147646
77ad1f1e18f6e172d3121123fa934a23d0b840ea	analytical study of performance of linear discriminant analysis in stochastic settings	health research;uk clinical guidelines;biological patents;expected error;gaussian processes;europe pubmed central;citation search;moving average models;uk phd theses thesis;stochastic settings;auto regressive models;life sciences;non i i d data;correlated data;uk research reports;linear discriminant analysis;medical journals;europe pmc;biomedical research;bioinformatics	This paper provides exact analytical expressions for the first and second moments of the true error for linear discriminant analysis (LDA) when the data are univariate and taken from two stochastic Gaussian processes. The key point is that we assume a general setting in which the sample data from each class do not need to be identically distributed or independent within or between classes. We compare the true errors of designed classifiers under the typical i.i.d. model and when the data are correlated, providing exact expressions and demonstrating that, depending on the covariance structure, correlated data can result in classifiers with either greater error or less error than when training with uncorrelated data. The general theory is applied to autoregressive and moving-average models of the first order, and it is demonstrated using real genomic data.		Amin Zollanvari;Jianping Hua;Edward R. Dougherty	2013	Pattern recognition	10.1016/j.patcog.2013.04.002	econometrics;computer science;machine learning;pattern recognition;data mining;gaussian process;mathematics;linear discriminant analysis;algorithm;statistics	ML	30.237151600468987	-22.241758911966418	147663
3112acf382a5c5e8d105b98342ae76cb0e904ffd	statistical reconstruction of random point patterns	metodo estadistico;k th nearest neighbour distance distribution;funcion esferica;analisis datos;fonction repartition;reconstruction;point process;statistical method;intensity function;funcion distribucion;vecino mas cercano;data analysis;distribution function;43a90;methode statistique;kth nearest neighbour distance distribution;statistical computation;calculo estadistico;fonction intensite;11mxx;plus proche voisin;analyse donnee;nearest neighbour;fonction spherique;calcul statistique;spherical function;60e05;l function;spherical contact distribution;conditional simulation	A general reconstruction method is described which simulates point patterns possessing prescribed summary characteristics, which are free of explicit model conditions. The characteristics are for instance the intensity, the L-function, the spherical contact distribution function and the kth nearest neighbour distance distributions. The use of the statistical reconstruction method is demonstrated on both a theoretical and practical example. © 2005 Elsevier B.V. All rights reserved.		A. Tscheschel;Dietrich Stoyan	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2005.09.007	l-function;nearest neighbour distribution;combinatorics;distribution function;calculus;point process;mathematics;data analysis;statistics	Vision	34.40220996806055	-21.76690209230373	147753
833a201a91e65592008eead3d6145b6c4316f64c	energy based robust video hash algorithm	transform coefficients robust video hash algorithm energy relationships;energy relationship;bit error rate;energy relationships;multimedia security robust hash energy relationship video authentication;authentication;resistance;video authentication;robust video hash algorithm;video coding;video coding cryptography;cryptography;robust hash;robustness bit error rate multimedia communication authentication partitioning algorithms resistance;multimedia communication;multimedia security;robustness;transform coefficients;partitioning algorithms	A simple and efficient energy based robust video hash algorithm is presented. The energy relationships of transform coefficients are extracted with secrete keys for hash construction. Experimental results show that the algorithm outperforms state-of-the-art ones in terms of robustness. Hash comparisons also show that the algorithm is collision resistant.	algorithm;coefficient;collision resistance;cryptographic hash function	Yue-Nan Li	2010	2010 International Conference on Computational Intelligence and Security	10.1109/CIS.2010.100	double hashing;hash function;perfect hash function;bit error rate;quadratic probing;collision resistance;computer science;cryptography;theoretical computer science;hash buster;authentication;distributed computing;rolling hash;internet privacy;resistance;computer security;cryptographic hash function;rabin–karp algorithm;statistics;robustness;swifft;hash tree;hash filter	Vision	39.138876958211355	-12.783477449942394	147777
1b2df0167ce7e955033a3446b7ec148fda5c3e50	analysis of lc-ms data using probabilitic-based mixture regression models (analyse von lc-ms-daten mit wahrscheinlichkeitsbasierter mischung von regressionsmodellen)	regression model			Habtom W. Ressom;Getachew K. Befekadu;Mahlet G. Tadesse	2009	Automatisierungstechnik	10.1524/auto.2009.0791	engineering;mathematics;regression analysis	ML	27.269207450126302	-22.861493381831096	148060
eccffd4e5dce5ee5f31b415afe65687e9de4d1a0	comments on scottknottesd in response to “an empirical comparison of model validation techniques for defect prediction models”		In this article, we discuss the ScottKnottESD test, which was proposed in a recent paper “An Empirical Comparison of Model Validation Techniques for Defect Prediction Models” that was published in this journal. We discuss the implications and the empirical impact of the proposed normality correction of ScottKnottESD and come to the conclusion that this correction does not necessarily lead to the fulfillment of the assumptions of the original Scott-Knott test and may cause problems with the statistical analysis.	log-space reduction;software bug	Steffen Herbold	2017	IEEE Transactions on Software Engineering	10.1109/TSE.2017.2748129	predictive modelling;normality;computer science;analysis of variance;statistics	SE	26.55307238193653	-19.998883640360166	148128
20c5d93c640ed157667882508826ba8915453fa2	kullback-leibler life time testing	information divergence;homogeneity and scale testing;intersection percentage function;deconvolution;lambert w;exact distribution	The paper deals with testing the hypotheses for homogeneity and point null value of the scale parameter in the gamma family. Tests suggested here are based upon the  Kullback–Leibler  divergence from an observed vector to the canonical parameter (see Pazman, 1993 [14]), and upon its decomposition. The latter is used to derive the exact distribution of the test statistic by convolution. A geometric integration method is used alternatively to derive the distribution directly. Because it is observed by simulation, that the test’s performance is poor when the shape parameter is estimated from the data, an interval method is developed and its use is demonstrated in an analysis of real data.	kullback–leibler divergence	Milan Stehlík;Polychronis Economou;Jozef Kiselák;Wolf-Dieter Richter	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.04.027	econometrics;mathematical analysis;lambert w function;deconvolution;jensen–shannon divergence;calculus;mathematics;statistics	SE	30.782488000574627	-22.484064529705826	148177
c663f57ca3b594121fe13deb9a9b9134e0ff299b	a bayesian approach to diagnosis and prognosis using built-in test	probability;fault diagnosis bayes methods built in self test probability measurement uncertainty;bayesian approach;bayesian methods built in self test testing circuit faults measurement uncertainty corona reliability theory signal detection decision theory fault diagnosis;bayes methods;bayesian inference;fault prognosis problem bayesian approach built in test test uncertainty measurement science reliability theory signal detection theory bayesian decision theory end to end probabilistic treatment fault diagnosis;measurement uncertainty;bayesian decision theory;false indication;built in self test;prognosis bayesian inference built in test bit diagnosis false indication measurement uncertainty;built in test bit;built in test;signal detection theory;diagnosis;prognosis;fault diagnosis	Accounting for the effects of test uncertainty is a significant problem in test and diagnosis, especially within the context of built-in test. Of interest here, how does one assess the level of uncertainty and then utilize that assessment to improve diagnostics? One approach, based on measurement science, is to treat the probability of a false indication [e.g., built-in-test (BIT) false alarm or missed detection] as the measure of uncertainty. Given the ability to determine such probabilities, a Bayesian approach to diagnosis, and by extension, prognosis suggests itself. In the following, we present a mathematical derivation for false indication and apply it to the specification of Bayesian diagnosis. We draw from measurement science, reliability theory, signal detection theory, and Bayesian decision theory to provide an end-to-end probabilistic treatment of the fault diagnosis and prognosis problem.	assertion (software development);bayesian approaches to brain function;built-in self-test;decision theory;detection theory;dynamic bayesian network;embedded system;end-to-end principle;reliability engineering;turing test	John W. Sheppard;Mark A. Kaufman	2005	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2005.847351	reliability engineering;econometrics;bayes estimator;bayesian probability;probability;mathematics;bayesian inference;statistics;measurement uncertainty;detection theory	Vision	28.583905006819517	-17.71007819016765	148280
ff5ee554dc4c12e2760e6219fe220a221de52d20	markov chain monte carlo versus importance sampling in bayesian inference of the garch model	markov chain monte carlo;garch model;bayesian inference;importance sampling	Usually, the Bayesian inference of the GARCH model is preferably performed by the Markov Chain Monte Carlo (MCMC) method. In this study, we also take an alternative approach to the Bayesian inference by the importance sampling. Using a multivariate Student’s t-distribution that approximates the posterior density of the Bayesian inference, we compare the performance of the MCMC and importance sampling methods. The overall performance can be measured in terms of statistical errors obtained for the same size of Monte Carlo data. The Bayesian inference of the GARCH model is performed by the MCMC method implemented by the Metropolis-Hastings algorithm and the importance sampling method for artificial return data and stock return data. We find that the statistical errors of the GARCH parameters from the importance sampling are smaller than or comparable to those obtained from the MCMC method. Therefore we conclude that the importance sampling method can also be applied effectively for the Bayesian inference of the GARCH model as an alternative method to the MCMC method. c © 2013 The Authors. Published by Elsevier B.V. Selection and peer-review under responsibility of KES International.	gibbs sampling;importance sampling;markov chain monte carlo;metropolis;metropolis–hastings algorithm;modified huffman coding;monte carlo method;sampling (signal processing)	Tetsuya Takaishi	2013		10.1016/j.procs.2013.09.191	econometrics;statistical inference;fiducial inference;gibbs sampling;particle filter;frequentist inference;markov chain monte carlo;importance sampling;pattern recognition;bayesian linear regression;bayesian statistics;bayesian inference;statistics	ML	28.91731482411731	-22.921619760824797	148331
3a64a5b4cfbcfd16abce361a9ff323993752585f	robust synthetic biology design: stochastic game theory approach	stochastic game theory;game theory;gene regulatory networks;synthetic biology;conception;biology;biologia;stochastic processes;theory;teoria;diseno;algorithms;design;article;stochastic games;biologie;theorie	MOTIVATION Synthetic biology is to engineer artificial biological systems to investigate natural biological phenomena and for a variety of applications. However, the development of synthetic gene networks is still difficult and most newly created gene networks are non-functioning due to uncertain initial conditions and disturbances of extra-cellular environments on the host cell. At present, how to design a robust synthetic gene network to work properly under these uncertain factors is the most important topic of synthetic biology.   RESULTS A robust regulation design is proposed for a stochastic synthetic gene network to achieve the prescribed steady states under these uncertain factors from the minimax regulation perspective. This minimax regulation design problem can be transformed to an equivalent stochastic game problem. Since it is not easy to solve the robust regulation design problem of synthetic gene networks by non-linear stochastic game method directly, the Takagi-Sugeno (T-S) fuzzy model is proposed to approximate the non-linear synthetic gene network via the linear matrix inequality (LMI) technique through the Robust Control Toolbox in Matlab. Finally, an in silico example is given to illustrate the design procedure and to confirm the efficiency and efficacy of the proposed robust gene design method.   AVAILABILITY http://www.ee.nthu.edu.tw/bschen/SyntheticBioDesign_supplement.pdf.	approximation algorithm;artificial gene synthesis;biological factors;biological system;game theory;gene regulatory networks;gene regulatory network;initial condition;linear matrix inequality;matlab;minimax;nonlinear system;robust control;social inequality;synthetic biology;synthetic intelligence	Bor-Sen Chen;Chia-Hung Chang;Hsiao-Ching Lee	2009		10.1093/bioinformatics/btp310	biology;game theory;design;simulation;computer science;bioinformatics;artificial intelligence;mathematics;genetics;synthetic biology;theory;statistics	ML	32.81893116181993	-11.449144087065328	148343
ff0c0ff24920b42ecd082cb778a4bc912c36b210	mixtures of autoregressive models for financial risk analysis	analyse risque;garch process;modelo autorregresivo;risk analysis;datos financieros;autoregressive process;non linear model;donnee financiere;modele non lineaire;time series;analisis programa;autoregressive model;financial data;analisis riesgo;modelo no lineal;mixture model;indexation;serie temporelle;financial time series;serie temporal;teoria mezcla;financial risk;program analysis;risk measure;analyse programme;modele autoregressif;mixture theory;extreme event;theorie melange;risque financier	The structure of the time-series of returns for the IBEX35 stock index is analyzed by means of a class of non-linear models that involve probabilistic mixtures of autoregressive processes. In particular, a specification and implementation of probabilistic mixtures of GARCH processes is presented. These mixture models assume that the time series is generated by one of a set of alternative autoregressive models whose probabilities are produced by a gating network. The ultimate goal is to provide an adequate framework for the estimation of conditional risk measures, which can account for non-linearities, heteroskedastic structure and extreme events in financial time series. Mixture models are sufficiently flexible to provide an adequate description of these features and can be used as an effective tool in financial risk analysis.	autoregressive model	Alberto Suárez	2002		10.1007/3-540-46084-5_192	econometrics;star model;mathematics;autoregressive model;statistics	ML	33.49697340819894	-21.861501140393674	148454
7b3426de6198bc76ad0b97feafd4dadd1cb89389	historical sources: how people counted. a method for estimating the rounding of numbers		Any census — or other enumeration exercise — which yields numerical data (counts) may include data collected using a variety of unspecified counting practices, such as ‘rounding’ to the nearest multiples of a ‘base-unit’, e.g. 5 or 10 or a (half-) dozen, as well as ‘exact’ counts. In general, the greater the age of the data-source, the greater the probability that the recorded data represent ‘rounded’ estimates as opposed to ‘exact’ counts. Also, the greater the number and diversity of the persons involved in data-collection, the greater the probability of variation in counting practice. The presence of ‘rounded’ data in many historical sources produces uncertainty when historians — or others — come to analyse such data. This article presents a new technique for analysing numerical data, one which illuminates our understanding of counting behaviour (whether of enumerators or respondents) and its effects on the resulting data. This method can be used to detect the presence of ‘rounded’ data within a data-s...	rounding	Robin Graham Murison Crockett;Alasdair Charles Crockett	1997	History and Computing	10.3366/hac.1997.9.1-3.43	econometrics;mathematics;algorithm;statistics	Vision	34.81113788520393	-18.290198161650295	148756
05d0031bce8752d22a29303cb909eacbe7a4f77d	size-biased random closed sets	point shape;modelo markov;analisis datos;fonction repartition;random closed set;forme ponctuelle;forma puntual;contacto;taille;granulometry;funcion distribucion;data analysis;distribution function;markov model;size distribution;analisis morfologico;talla;morphological analysis;analyse morphologique;estimateur style hanisch;ensemble aleatoire;analyse donnee;geometrie stochastique;modele markov;size;contact;random set;empty space function;exploratory data analysis;conjunto aleatorio	Abstract   We indicate how granulometries may be useful in the analysis of random sets. We define a suitable size distribution function as a tool in exploratory data analysis and give a new Hanisch style estimator for it. New Markov random sets are constructed which favour certain sizes above others. A size-biased random set model is fitted to a data set concerning the incidence of heather (Diggle, Biometrics 37 (1981) 531–539).		M. N. M. van Lieshout	1999	Pattern Recognition	10.1016/S0031-3203(99)00025-4	combinatorics;random compact set;morphological analysis;computer science;distribution function;random function;calculus;mathematics;markov model;size;data analysis;exploratory data analysis;granulometry;statistics	Vision	34.206834147943304	-21.569411794609238	149041
e0de48048fe9bff0a8c6fc152a3eca0b2f2b0bd6	empirical likelihood-based inference in generalized random coefficient autoregressive model with conditional moment restrictions		Abstract This paper concentrates on the parameter estimating strategy for the generalized random coefficient autoregressive (GRCA) model in the presence of the auxiliary information. We propose a weighted least squares estimate for the model parameters and empirical likelihood (EL) based weights are obtained through using these auxiliary information. The asymptotic distribution of our proposed estimator is normal distribution and the asymptotic variance is reduced compared to the least square (LS) estimator. Therefore, our method yields more efficient estimates. We also carry out some simulation experiments to assess the performance of the suggested estimator and illustrate the usefulness of this method through the analysis of a real time series data sets.		Zhi-Wen Zhao;Yang Li;Cui-Xin Peng;Zhi-Hui Fu	2019	J. Computational Applied Mathematics	10.1016/j.cam.2018.08.048	statistics;empirical likelihood;mathematical optimization;time series;estimator;mathematics;autoregressive model;least squares;delta method;normal distribution;asymptotic distribution	ML	29.638559943313528	-23.338001239555286	149192
220324dad4af80a5d4af8b6b637cf1a32aaf664f	spc scheme to monitor linear predictors embedded in nonlinear profiles	nonlinear profiles;response modeling methodology rmm;canonical correlation analysis;modeling and monitoring linear predictors;statistical process control spc	Response Modeling Methodology (RMM) is a general platform to model monotone convex relationships. In this article, RMM is combined with linear regression analysis to model and estimate linear predictors (LPs) embedded in a nonlinear profile. A regression-adjusted statistical process control scheme is then implemented to monitor the LPu0027s residuals. To model and estimate the LP, RMM defines a Taylor series expansion of an unknown response transformation and then use canonical correlation analysis to estimate the LP. A possible hindrance to the implementation of the new scheme is possible occurrence of nonnormal errors (in violation of the linear regression model). Reasons for the occurrence of this phenomenon are explored and remedies offered. The effectiveness of the new scheme is demonstrated for data generated via Monte Carlo simulation. Results from hypothesis testing clearly indicate that the type of the response distribution, its skewness and the sample size, do not affect the effectiveness of the new approach. A detailed implementation routine is expounded, accompanied by a numerical example. When interest is solely focused on the stability of the LP, and the nonlinear profile per se is of little interest, the new general RMM-based statistical process control scheme delivers an effective platform for process monitoring. Copyright © 2015 John Wiley u0026 Sons, Ltd.	embedded system;nonlinear system	Revital Danoch;Haim Shore	2016	Quality and Reliability Eng. Int.	10.1002/qre.1856	econometrics;canonical correlation;real-time computing;computer science;mathematics;statistics	EDA	27.71421043061674	-21.005190253172252	149287
df575f515ae39acbfcb42e11f814b581a9b115db	comparison of angular interpolation approaches in few-view tomography using statistical hypothesis testing	test hypothese;metodo estadistico;interpolation;objet test;estudio comparativo;test hipotesis;root mean square error;echantillonnage;interpolacion;calcul erreur;statistical significance;statistical method;periodic boundary condition;angular distribution;breast;statistical hypothesis testing;valeur efficace;sampling;etude comparative;error analysis;accuracy;precision;methode statistique;linear interpolation;interpolation method;valor eficaz;comparative study;root mean square value;distribucion angular;calculo error;muestreo;zero padding;objeto prueba;test object;tomography;spline interpolation;sampling theorem;hypothesis test;distribution angulaire	In this work we examine the accuracy of four periodic interpolation methods--circular sampling theorem interpolation, zero-padding interpolation, periodic spline interpolation, and linear interpolation with periodic boundary conditions--for the task of interpolating additional projections in a few-view sinogram. We generated 100 different realizations each of two types of numerical phantom--Shepp-Logan and breast--by randomly choosing the parameters that specify their constituent ellipses. Corresponding sinograms of 128 bins X 1024 angles were computed analytically and subsampled to 16, 32, 64, 128, 256, and 512 views. Each subsampled sinogram was interpolated to 1024 views by each of the methods under consideration and the normalized root-mean-square-error (NRMSE) with respect to the true 1024-view sinogram computed. In addition, images were reconstructed from the interpolated sinograms by FBP and the NRMSE with respect to the true phantom computed. The non-parametric signed rank test was then used to assess the statistical significance of the pairwise differences in mean NRMSE among the interpolation methods for the various conditions: phantom family (Shepp-logan or breast), number of measured views (16, 32, 64, 128, 256, or 512), and endpoint (sinogram or image). Periodic spline interpolation was found to be superior to the others in a statistically significant way for virtually every condition.© (1999) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	angularjs;interpolation;tomography	Patrick J. La Rivière;Xiaochuan Pan	1999		10.1117/12.348595	econometrics;bilinear interpolation;stairstep interpolation;calculus;bicubic interpolation;mathematics;nearest-neighbor interpolation;multivariate interpolation;statistics;trilinear interpolation	Robotics	31.951637220778366	-22.387571131824107	149299
c198854436603a5faf15f0617e0197bcb28e42cd	semi-fragile watermarking for image content authentication and self-restoration		With the popularity and affordability of advanced digital image editing software, users can manipulate images relatively easily and professionally. As a result, the task of guaranteeing the authenticity of digital images, particularly as evidence in the court of law, is becoming more important. In the past few years, semi-fragile watermarking has become increasingly important to verify the content of images and to localize the tampered areas, while simultaneously tolerating some non-malicious manipulations. Moreover, some researchers have proposed self-restoration schemes such that the content of tampered areas can be recovered after the authentication process. In this thesis, we introduce three semi-fragile watermarking schemes for image content authentication and self-restoration and one general-purpose digital watermarking benchmarking framework with high reconfigurability.#R##N##R##N#First, we propose a fast watermarking scheme which utilises the relationship between the four 4×4 sub-blocks mean pixel values and the first four DCT coefficients of 8×8 block for image self-restoration. It is evaluated and compared with two other existing watermarking schemes, and achieves an improvement of PSNR approximately 4 dB for the watermarked image and approximately 2 dB for the restoration image. In order to improve the security of the algorithm, we also propose and evaluate an enhanced perceptual image authentication approach by incorporating perceptual hashing into digital watermarking. This approach can provide better robustness to most content-based image processing operations such as JPEG compression and Gaussian noises than most existing semi-fragile watermarking methods. Experimental results achieved also demonstrate a high authentication accuracy rate to non-malicious and malicious image processing operations.#R##N##R##N#This thesis also presents a new image restoration method based on a linear optimization model which restores part of the image from structured side information (SSI). Our experiments show that given complete information of the SSI, the proposed image restoration technique can outperform the state-of-the-art model by approximately 2 dB in terms of average PSNR value and approximately 0.04 in terms of SSIM value.#R##N##R##N#Finally, we present a new OR-Benchmark framework with open interfaces and high reconfigurability that allows the end user to easily extend the functionalities and freely configure the benchmarking process to obtain and analyse the results. This framework is implemented as a MATLAB software package and can be used to benchmark and further improve existing image authentication and self-restoration watermarking algorithms.	authentication;circuit restoration;digital watermarking;semiconductor industry	Hui Wang	2015			computer science;theoretical computer science;digital image processing;multimedia;computer security	Vision	39.13378003228786	-11.947765853620364	149317
c1f51cbbf72110b51b7b027e222b3ddb6b7651d8	a novel cross folding algorithm for multimodal cancelable biometrics	template security;biometrics;pattern recognition;multimodal cancelable;random projection	Multimodal biometric systems have emerged as highly successful new approach to combat problems of unimodal biometric system such as intraclass variability, interclass similarity, data quality, non-universality, and sensitivity to noise. However, one major issue pertinent to unimodal system remains, which has to do with actual biometric characteristics of users being permanent and their number being limited. Thus, if a user's biometric is compromised, it might be impossible or highly difficult to replace it in a particular system. The concept of cancelable biometric or cancelability is to transform a biometric data or feature into a new one so that the stored biometric template can be easily changed in a biometric security system. In this paper, the authors present a novel solution for cancelable biometrics in a multimodal system. They develop a new cancelable biometric template generation algorithm using random projection and transformation-based feature extraction and selection. Performance of the proposed algorithm is validated on a virtual multi-modal face and ear database.	algorithm;biometrics;multimodal interaction	Padma Polash Paul;Marina L. Gavrilova	2012	IJSSCI	10.4018/jssci.2012070102	computer vision;speech recognition;computer science;pattern recognition;biometrics	ML	36.7231511109458	-11.58318130456914	149333
f3149663be3e9231cbe8191ad34bcbf6626ab551	sample-size determination for two independent binomial experiments	zhanping zhao nianshengtang yunxian li 样本大小 实验 现代医学 假设检验 测试功能 损失函数 二项式 贝叶斯 sample size determination for two independent binomial experiments	Sample size determination is commonly encountered in modern medical studies for two independent binomial experiments. A new approach for calculating sample size is developed by combining Bayesian and frequentist idea when a hypothesis test between two binomial proportions is conducted. Sample size is calculated according to Bayesian posterior decision function and power of the most powerful test under 0-1 loss function. Sample sizes are investigated for two cases that two proportions are equal to some fixed value or a random value. A simulation study and a real example are used to illustrate the proposed methodologies.	experiment	Zhanping Zhao;Niansheng Tang;Yun-Xian Li	2011	J. Systems Science & Complexity	10.1007/s11424-011-8250-x	sample size determination;econometrics;combinatorics;binomial distribution;mathematics;negative binomial distribution;binomial proportion confidence interval;binomial test;statistics	Theory	30.294348846498735	-21.132164867799023	149686
ab5493abd794d00d3b943810291ac796ebc22883	autostat: output statistical analysis for automod users	automod user;output file;simulation model experimentation;random number seed;custom user output file;sup tm;statistical analysis;simulation modeler;analysis software;statistical method;input parameter;output statistical analysis;job shop scheduling;confidence interval;simulation model;statistical independence;displays;design of experiment;error correction;production;debugging;automatic control	AutoStatTM is an extension package for AutoModTM and AutoSchedTM models that provides complete support for simulation model experimentation and statistical analysis of outputs. Within a menu-driven, point and click environment, AutoStat provides assistance for setting up runs, automated execution of runs and consolidation of outputs across replications and scenarios. AutoStat automatically sets random number seeds to achieve statistically independent replications; it also sets factor levels (input parameters) to realize desired scenarios, without having to modify the underlying model. AutoStat has a number of advanced features such as datafile factors, responses (or performance measures) read from custom user output files, and user-defined combination responses (linear combinations of other responses). AutoStat offers several statistical methods, including confidence intervals, a ranking and selection procedure, design of experiments, and warm-up determination. Outputs can be displayed and printed in tabular and graphical formats and exported to spreadsheet and other analysis software. AutoStat saves simulation modelers considerable time and automates the effort of setting up and making runs, managing the input and output files, consolidating results and conducting analyses.	design of experiments;experiment;graphical user interface;input/output;point and click;printing;random number generation;random seed;semiconductor consolidation;simulation;spreadsheet;table (information)	John S. Carson	1997		10.1145/268437.268601	independence;job shop scheduling;simulation;error detection and correction;confidence interval;computer science;engineering;technical report;simulation modeling;automatic control;data mining;mathematics;design of experiments;debugging;world wide web;statistics	ML	28.959358014925105	-14.862798261600787	150133
ee5f28487ef2216feef110b8270b7e2f89314b8a	it's all about the subject - options to improve psychometric procedure performance	psychometric procedures;uml;staircase;performance analysis;varpsi	We investigate the effect of procedure-specific parameters on the performance of three common psychophysical procedures. Methods considered include transformed-staircases, the $$\varPsi $$-method and the UML method, while performance is evaluated in terms of accuracy, efficiency, precision and robustness. Simple Yes/No- and three alternative forced choice response paradigms were considered. A Monte Carlo simulation was conducted for three different types of test persons and analyzed by analysis of variances. Results show a large effect of the test person on the performance, especially for staircase procedures. No parameter exhibited a relevant effect on accuracy for each analyzed methods, estimation precision can be increased with an increasing number of trials. Only for staircase procedures, efficiency can be influenced by the choice of the progression rule.		Christian Hatzfeld;Viet Quoc Hoang;Mario Kupnik	2016		10.1007/978-3-319-42321-0_36	econometrics;simulation;computer science;statistics	Robotics	27.989999915681512	-17.63254301734863	150141
d28741473d01848fc39a721a02074aa335a4d601	comparison of the criteria for updating kriging response surface models in multi-objective optimization	optimisation;response surface methodology;design engineering;expected improvement;multi objective optimization;welding;stochastic processes constraint handling design engineering optimisation response surface methodology statistical analysis;expected hypervolume improvement;accuracy;statistical analysis;estimation;stochastic processes;additional sample multi objective optimization kriging response surface model function estimation expected improvement expected hypervolume improvement;kriging response surface model;constraint handling;optimization;search problems;additional sample;welded beam design problem kriging response surface model update criteria multiobjective optimization expected hypervolume improvement ei criterion ehvi est criterion stochastic improvement objective function value nondominated solutions uncertain constraint handling estimation;numerical models;optimization numerical models welding search problems accuracy estimation stochastic processes;function estimation	This paper compares the criteria for updating the Kriging response surface models in multi-objective optimization: expected improvement (EI), expected hypervolume improvement (EHVI), estimation (EST), and those combination (EHVI+EST). EI has been conventionally used as the criterion considering the stochastic improvement of each objective function value individually, while EHVI has been recently proposed as the criterion considering the stochastic improvement of the front of non-dominated solutions in multi-objective optimization. EST is the value of each objective function, which is estimated non-stochastically by the Kriging model without considering its uncertainties. Numerical experiments were implemented in the welded beam design problem, and empirically showed that, in a non-constrained case, EHVI keeps a balance between accurate and wide search for non-dominated solutions on the Kriging models in multi-objective optimization. In addition, the present experiments suggested future investigation into the techniques for handling uncertain constraints to enhance the capability of EHVI in a constrained case.	binary prefix;estimation theory;experiment;kriging;loss function;mathematical optimization;multi-objective optimization;numerical analysis;numerical method;optimization problem;pareto efficiency;response surface methodology	Koji Shimoyama;Koma Sato;Shinkyu Jeong;Shigeru Obayashi	2012	2012 IEEE Congress on Evolutionary Computation	10.1109/CEC.2012.6256492	stochastic process;econometrics;mathematical optimization;estimation;response surface methodology;multi-objective optimization;mathematics;accuracy and precision;statistics;welding	Robotics	29.893765386691854	-9.887580806014281	150443
89be91a5439eca867c9a6e65f3a994dc33f3d21a	efficiency evaluation of mev spatial sampling strategies: a scenario analysis	experimental design;spatial sequential sampling;sample size;erreur moyenne;computacion informatica;spatial dependence;strategie estimation;analisis datos;05bxx;analisis espacial;superpopulation model;modele superpopulation;tamano muestra;62h20;attente;echantillonnage;estimation variance;erreur quadratique moyenne;variance analysis;plan experiencia;taille echantillon;estimacion promedio;mean error;error medio;ecuesta estadistica;sampling;62f07;sample survey;data analysis;correlation spatiale;estimation erreur;62k99;spatial correlation;correlacion espacial;plan experience;scenario analysis;error estimation;ciencias basicas y experimentales;analisis variancia;mean square error;matematicas;62j10;statistical computation;estimacion error;calculo estadistico;analyse correlation;62d05;superpopulation;expectation;estimation strategy;analyse donnee;survey data;calcul statistique;variance estimation;mean estimation;error medio cuadratico;spatial analysis;muestreo;grupo a;sampling strategy;62p12;estimation moyenne;environmental survey;sondage statistique;analyse spatiale;quantitative evaluation;expectacion;analisis correlacion;sequential sampling;analyse variance;variance;variancia;correlation analysis	The minimum estimation variance (MEV) spatial sampling strategy is compared with some further strategies based on systematic designs and the sample mean. Since in environmental surveys data are usually collected repeatedly in time at sites whose selection is based on practical circumstances only, it seems worth measuring the efficiency of spatial sampling strategies through the expectation of design mean square errors under several superpopulation models assumed about the fixed population generating process. Relating the study to specific superpopulations, MEV efficiencies can be calculated and a quantitative evaluation is made possible by the use of scenario analyses for several sample sizes and different models of spatial drift and correlations. The MEV strategy revealed itself to be the relatively more efficient one under realistic conditions of nonstationary spatial drifts and bounded sample sizes.	sampling (signal processing);scenario analysis	Giovanni Lafratta	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2004.10.010	econometrics;mathematics;mean squared error;statistics	Vision	33.00713861684932	-22.01618733802386	150475
4cd10900e54c0d68c1022fc331ffe6f84df78f9e	extreme value statistics	extreme value statistics	The sea dike problem: The Dutch government specifies that sea dikes should be built so high that a flood occurs once in 10,000 years (assuming stationarity), i.e. the probability of a flood in a given year should be 1/10,000.		Mario Nicodemi	2009		10.1007/978-0-387-30440-3_197	generalized extreme value distribution;extreme value theory;sample maximum and minimum	ECom	25.78026903457052	-21.39753497575818	150691
ff0ea61cae20943afcd73c3b25fab6cce979ef6c	invariant post-tabular protection of census frequency counts	additivity;randomized rounding;transition matrix;microdata keys;invariantprobability transition matrix;ha statistics;consistency	Some countries use forms of random rounding as a post-tabular method to protect Census frequency counts disseminated in tables. These methods typically result in inconsistencies between aggregated internal cells to marginal totals and across same cells in different tables. A post-tabular method for perturbing frequency counts is proposed which preserves totals and corrects to a large extent inconsistencies. The perturbation is based on invariant probability transition matrices and the use of microdata keys. This method will be compared to common pre and post-tabular methods for protecting Census frequency counts.	table (information)	Natalie Shlomo;Caroline Young	2008		10.1007/978-3-540-87471-3_7	econometrics;randomized rounding;data mining;stochastic matrix;mathematics;consistency;additive function;statistics	OS	32.921963524682795	-14.12552247547938	150726
e19f642d7b0cb4f4e1365fbc79c9c286b8947ddd	visual secret sharing scheme with (k, n) threshold based on qr codes		In this paper, a novel visual secret sharing (VSS) scheme with using QR codes is investigated. The proposed visual secret sharing scheme based on QR codes(VSSQR) can visually reveal secret image by stacking k or more shares (shadow images) from all the n QR codes as well as scan the QR code by a QR code reader. Our VSSQR exploits the error correction mechanism in the QR code structure, to embed the bits corresponding to shares generated by VSS from a secret bit into the same locations of QR codes in the processing of encoding QR. Each output share is a valid QR code, which may reduce the likelihood of attracting the attention of potential attackers, that can be scanned and decoded utilizing a QR code reader. The secret image can be recovered by stacking for case (k, n) based on the human visual system without any computation. In addition, it can assist alignment for VSS recovery. The experiment results show the effectiveness of our scheme.	computation;error detection and correction;focus stacking;qr code;secret sharing	Song Wan;Yuliang Lu;Xuehu Yan;Lintao Liu	2016	2016 12th International Conference on Mobile Ad-Hoc and Sensor Networks (MSN)	10.1109/MSN.2016.068	error detection and correction;visualization;distributed computing;computer science;cryptography;theoretical computer science;computation;human visual system model;exploit;secret sharing;shadow	Robotics	38.69526790238527	-10.656576223272449	150989
767350a35f2945894b175fac7895e143ec185c19	a bayesian scheme to detect changes in the mean of a short-run process	bayes estimation;control estadistico proceso;bayesian statistical process control;filtro kalman;filtre kalman;statistical process control;grupo de excelencia;kalman filter;maitrise statistique processus;estimacion bayes;normal mixture;desgate instrumento;ciencias basicas y experimentales;matematicas;tool wear;usure outil;estimation bayes	In this article we propose a model suitable for statistical process control in short production runs. We wish to detect on-line whether the mean of the process has exceeded a prespecified upper threshold value. The theoretical basis of the model is a Bayesian formulation, leading to a mixture of normal distributions. Issues of decisions about whether the process is within specification and forecasting are addressed. The Kalman filter model is shown to be related to a special case of our model. The calculations are illustrated with a clinical chemistry example. The tool wear problem is another potential candidate for our approach.		Panagiotis Tsiamyrtzis;Douglas M. Hawkins	2005	Technometrics	10.1198/004017005000000346	kalman filter;econometrics;mathematics;statistical process control;statistics	Metrics	33.94237768021734	-18.378854547197125	151054
d4e2d250088c2f7c21d036f505fea5524d611351	binomial distribution sample confidence interval estimation for positive and negative likelihood ratio medical key parameters		Likelihood Ratio medical key parameters calculated on categorical results from diagnostic tests are usually express accompanied with their confidence intervals, computed using the normal distribution approximation of binomial distribution. The approximation creates known anomalies,especially for limit cases. In order to improve the quality of estimation, four new methods (called here RPAC, RPAC0, RPAC1, and RPAC2) were developed and compared with the classical method (called here RPWald), using an exact probability calculation algorithm.Computer implementations of the methods use the PHP language. We defined and implemented the functions of the four new methods and the five criterions of confidence interval assessment. The experiments run for samples sizes which vary in 14 - 34 range, 90 - 100 range (0 < X < m, 0< Y < n), as well as for random numbers for samples sizes (4m, n </= 1000) and binomial variables (1 </= X, Y < m, n). The experiment run shows that the new proposed RPAC2 method obtains the best overall performance of computing confidence interval for positive and negative likelihood ratios.	approximation;computation (action);confidence intervals;experiment;international normalized ratio;one thousand;php;polr1c gene;polr1d gene;qt interval feature (observable entity);algorithm;likelihood ratio	Sorana D. Bolboaca;Lorentz Jäntschi	2005	AMIA ... Annual Symposium proceedings. AMIA Symposium			Comp.	30.322599822164108	-21.254657297051246	151100
03913c50d9d1a69f3e5e8dc892c5f2e9121dc557	"""replicated computations results (rcr) report for """"mno-pqrs: max nonnegativity ordering - piecewise-quadratic rate smoothing"""""""		The article “MNOPQRS: Max Nonnegativity Ordering—Piecewise-Quadratic Rate Smoothing” by Chen and Schmeiser constructs a smooth piecewise-quadratic rate estimate for a nonhomogeneuous Poisson process based on event counts over k adjacent time intervals. The event times can be generated by generating a Poisson process with unit rate and inverting the cumulative rate function or by the thinning technique. The overall algorithm has O(k2) time complexity and O(k) space requirements in the number of intervals. This replicated computation report focuses on the reproducibility of the experimental results in the aforementioned article.	algorithm;computation;entity–relationship model;nyquist rate;requirement;retro city rampage;smoothing;thinning;time complexity	Christos Alexopoulos	2017	ACM Trans. Model. Comput. Simul.	10.1145/3097350		Graphics	36.3633563710309	-18.619656080767655	151103
6947f385e07536c6df4b447d3efedb5cda499bf3	programmed methods for printer graphical output	graphical output;output typewriter;explanatory material;digital x;analog device;special hardware;graphical form;programmed method;economic consideration;special variety;programmed graphical output;printer graphical output	It is frequently desirable to display the results of computation in a graphical form. This is often done through the use of special hardware such as digital X,Y-plotters. Programmed graphical output for standard printers is preferable in several situations: (1) when economic considerations do not justify the expense of special hardware for the purpose, (2) when a combination of graphical output with some other kind, such as explanatory material, is desired, and (3) when some special variety of graphical output is needed which cannot readily be drawn by an analog device. A number of routines have been prepared (primarily by users rather than manufacturers) to convert numerical data into graphical form for printing by output typewriters or line printers. Virtually nothing on this subject has been published, and this report represents an admittedly incomplete attempt to describe this technique and suggest possibilities for its use.	analog device;computation;graphical user interface;level of measurement;numerical analysis;printer (computing);printing	David Garfinkel	1962	Commun. ACM	10.1145/368834.368868	computer science;algorithm;statistics	Graphics	28.992784859027605	-14.911225834930752	151152
422b4c8a2df647abc2f78c3f34f9a732cddda083	weighted semiparameter model and its application		A weighted semiparameter estimate model is proposed. The parameter components and nonparameter components are weighted. The weights are determined by the characters of different data. Simulation data and real GPS data are both processed by the new model and least square estimate, ridge estimate, and semiparameter estimate. The main research method is to combine qualitative analysis and quantitative analysis.The deviation between estimated values and the true value and the estimated residuals fluctuation of different methods are used for qualitative analysis. The mean square error is used for quantitative analysis. The results of experiment show that the model has the smallest residual error and the minimummean square error.The weighted semiparameter estimate model has effectiveness and high precision.	global positioning system;mean squared error;quantum fluctuation;simulation	Zhengqing Fu;Guolin Liu;Ke Zhao;Hua Guo	2014	J. Applied Mathematics	10.1155/2014/892107	econometrics;mathematical optimization;mean square weighted deviation;mathematics;method of mean weighted residuals;statistics;weighted geometric mean	ML	28.53297711990545	-21.439389268453205	151186
b9989c106ba6e3e0d5aa40508ee03d8eeca39d38	fuzzy reliability analysis using a new alpha level set optimization approach based on particle swarm optimization		In real world application of structural reliability analysis some random variables contains two types of random and epistemic uncertainty, while in classic methods of structural reliability only the random uncertainty is considered completely. Therefore in order to have a reliable estimation of structural safety, random variables should cover both random and epistemic uncertainty. In this paper, modeling of epistemic uncertainty of random variables has been brought in to focus. Hybrid random variables are simulated using fuzzy numbers. A new alpha level set optimization approach applying particle swarm optimization technique was addressed in order to determine the minimum and the maximum members of reliability index interval. Importance sampling technique was used for reliability analysis to decrease the computational effort. Three numerical examples were given to illustrate the accuracy and efficiency of the proposed method. Results showed that the proposed method was more efficient compared to the alternative search approaches thorough low computational burden.	algorithm;fuzzy number;importance sampling;mathematical optimization;multi categories security;nonlinear system;numerical analysis;particle swarm optimization;portable document format;reliability engineering;sampling (signal processing)	Mansour Bagheri;Mahmoud Miri;Naser Shabakhty	2016	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-151749	probabilistic-based design optimization;econometrics;mathematical optimization;stochastic optimization;mathematics;statistics	AI	28.30678497979789	-17.075180700766918	151191
b04839c349552b0a661a5270d013c90fdd8f28b2	a comparison of generalised maximum entropy and ordinary least square		The generalised maximum entropy (GME) estimation method is based on the classic maximum entropy approach of Jaynes (1957). It has the ability to estimate the parameters of a regression model without imposing any constraints on the probability distribution of errors and it is robust even when we have ill-posed problems. In this paper, we simulate two sets of data from regression model with different distribution for disturbance, standard normal and Cauchy distributions respectively. For this dataset, regression coefficients are obtained by GME and OLS methods and these techniques are compared with each other for some sample sizes. Moreover, we have used some prior information on parameters to obtain GME estimators. The estimation results of GME in the case of non-normal distributed are discussed here.	ordinary least squares	Manije Sanei Tabass;Gholamreza Mohtashami Borzadran	2018	IJIDS	10.1504/IJIDS.2018.10016400	regression analysis;probability distribution;cauchy distribution;principle of maximum entropy;estimator;applied mathematics;data mining;sample size determination;computer science;ordinary least squares;linear regression	AI	29.67157896496316	-23.6174449796338	151205
176e4c10d4b476a2b4784a1e565c2aff4bb77b63	measurement uncertainty with nested mixed effect models	secs s 01 statistica;articolo su rivista scientifica specializzata;misclassification rates;repeatability and reproducibility;settore secs s 01 statistica;variance components	Measurement uncertainty in experiments is receiving increasing interest among practitioners both for quantitative and qualitative evaluations. Applications may be found, for example, to compare experiments run in different laboratories, to certify the precision of gauges used in the masurement process or to measure uncertainty in a classification process, i.e. when objects are qualitatively evaluated by appraisers. In this paper, after having briefly presented how to measure uncertainty using gauge repeatability and reproducibility studies, we focus on a problem quite relevant for practitioners: how to deal with the presence of correlation among ‘replications’. This issue is common to many food/agriculture experiments. By a modification of a nested mixed effect design, we describe the impact of dependencies among replications on measurement capability. Copyright © 2011 John Wiley & Sons, Ltd.	autocorrelation;design of experiments;experiment;john d. wiley;repeatability;system of measurement	Laura Deldossi;D. Zappa	2011	Quality and Reliability Eng. Int.	10.1002/qre.1235	econometrics;engineering;operations management;statistics	SE	27.83349926311403	-20.329259567577832	151222
d2b88e1e897956b27c6e34e7d56afaf1d1c2acbd	an artificial neural network based heterogeneous panel unit root test in case of cross sectional independence	oecd countries;heterogeneous panel unit root test statistics;real exchange rate;linearity;us dollar artificial neural network heterogeneous panel unit root test statistics cross sectional independence ann bilateral real exchange rate series;neural nets;cross sectional independence;biological system modeling;exchange rates;ann;testing;time series exchange rates neural nets statistical testing;time series;data mining;us dollar;artificial neural networks;exchange rates artificial neural network panel unit root test;panel unit root test;artificial neural networks testing econometrics statistical analysis power generation economics linearity exchange rates statistical distributions helium data analysis;exchange rate;cross section;bilateral real exchange rate series;unit root;statistical testing;economics;monte carlo methods;artificial neural network	In this paper we propose an artificial neural network (ANN) based panel unit root test, extending [1] neural test to a dynamic heterogeneous panel context, and following the [2] panel methodology. New asymptotic results are obtained both for the individual ANN-t test statistics for unit root, and the panel unit root test statistic. An application to a panel of bilateral real exchange rate series with the US Dollar from the 20 major OECD countries is provided.	artificial neural network;bilateral filter	Christian de Peretti;Carole Siani;Mario Cerrato	2009	2009 International Joint Conference on Neural Networks	10.1109/IJCNN.2009.5178885	econometrics;statistical hypothesis testing;computer science;panel analysis;machine learning;time series;cross section;linearity;software testing;unit root;artificial neural network;statistics;monte carlo method	ML	25.146747066415077	-21.375407261288156	151498
0dd04ac43ae7027cb2ac4c7dca839e43fedc2799	forecasting financial time series through intrinsic dimension estimation and non-linear data projection	institutional repositories;administracion financiera;prediction method;curvilinear component analysis;statistical forecasting;fedora;financial management;modelo autorregresivo;non linear time series;auto regressive;non linear model;modele non lineaire;time series;vital;autoregressive model;approximation fonction;fractal dimension;modelo no lineal;function approximation;prevision statistique;indexation;financial time series;reseau neuronal;gestion financiere;vtls;modele autoregressif;intrinsic dimension;red neuronal;prevision estadistica;ils;neural network	A crucial problem in non-linear time series forecasting is to determine its auto-regressive order, in particular when the prediction method is non-linear. We show in this paper that this problem is related to the fractal dimension of the time series, and suggest using the Curvilinear Component Analysis (CCA) to project the data in a non-linear way on a space of adequately chosen dimension, before the prediction itself. The performances of this method are illustrated on the SBF 250 index.	algorithm;autoregressive model;fractal dimension;intrinsic dimension;kerrison predictor;nonlinear system;performance;time complexity;time series	Michel Verleysen;Eric de Bodt;Amaury Lendasse	1999		10.1007/BFb0100527	financial management;econometrics;computer science;machine learning;calculus;mathematics;autoregressive model;artificial neural network;statistics	ML	34.38192325728511	-22.45862546047453	151612
2c675cb6c4ad0408847a788f01cbc13ecc39f08f	limit distributions of v- and u-statistics in terms of multiple stochastic wiener-type integrals	ecuacion estocastica;test hypothese;linear combination;equation differentielle;metodo estadistico;stochastic equation;polinomio hermite;kernels;analyse multivariable;multiple integral;integral equation;limit distribution;pont brownien;60h05;multivariate analysis;integral multiple;methode noyau;loi limite;test hipotesis;60f05;differential equation;integral wiener;brownian bridge;statistical method;distribucion estadistica;loi asymptotique;equation stochastique;60f17;ecuacion diferencial;integrale stochastique;polynome hermite;integral estocastica;puente browniano;28c20;distribution statistique;statistique u;u statistic;combinacion lineal;methode statistique;noyau mathematiques;metodo nucleo;estadistica u;equation integrale;stochastic integral;analisis multivariable;kernel method;ecuacion integral;integrale wiener;65c30;wiener integral;integrale multiple;hermite polynomial;statistical distribution;ley limite;combinaison lineaire;33c45;multiple stochastic integral;62e20;hypothesis test	"""We describe the limit distribution of V- and U-statistics in a new fashion. In the case of V-statistics the limit variable is a multiple stochastic integral with respect to an abstract Brownian bridge G""""Q. This extends the pioneer work of Filippova (1961) [8]. In the case of U-statistics we obtain a linear combination of G""""Q-integrals with coefficients stemming from Hermite Polynomials. This is an alternative representation of the limit distribution as given by Dynkin and Mandelbaum (1983) [7] or Rubin and Vitale (1980) [13]. It is in total accordance with their results for product kernels."""		Dietmar Ferger;Michael Scholz	2011	J. Multivariate Analysis	10.1016/j.jmva.2010.09.006	mathematical analysis;calculus;mathematics;geometry;statistics;multiple integral	Vision	33.7910573501333	-21.263700409008674	151796
7347de5e64225d9ed6b62b2f04ac791bf0655d85	distribution-free statistical inference for generalized lorenz dominance based on grouped data	approximation asymptotique;method of moments;metodo momento;analyse multivariable;dominance;test statistique;generalized lorenz curve;multivariate analysis;moment method;stochastic dominance;analisis datos;desigualdad;swf;concave function;fonction repartition;inequality;household income;test estadistico;statistical test;inegalite;distribucion del ingreso;distribution revenu;income distribution;asymptotic behavior;comportement asymptotique;comportamiento asintotico;funcion distribucion;data analysis;distribution function;japon;asie;dominancia;testing multivariate inequality restrictions;methode moment;grouped data;lorenz dominance;statistical inference;modele lorenz;lorenz curve;analisis multivariable;analyse donnee;dominance stochastique;method of moment;courbe lorenz generalisee;asymptotic approximation;social welfare function;1994;lorenz model;japan;asymptotic variance;fonction concave;modelo lorenz;asia;covariance matrix;aproximacion asintotica;variance;variancia	One income distribution is preferable to another under any increasing and Schur-concave (S-concave) social welfare function (SWF) if and only if the generalized Lorenz (GL) curve of the first distribution lies above that of the second. Thus, testing for GL dominance of one distribution over another is of interest. The paper focuses on inference based on grouped data and makes two contributions: (i) it gives a new formula for the asymptotic variance-covariance matrix of a vector of sample GL curve ordinates, interpreting it as a method-of-moments (MM) estimator, and (ii) it proposes a new test for multivariate inequality restrictions, of which GL dominance is a special case. For the Japanese household income data grouped into deciles, the test accepts the null hypothesis that income distribution in Japan improved from 1979 to 1994.		Yasutomo Murasawa;Kimio Morimune	2004	Mathematics and Computers in Simulation	10.1016/S0378-4754(03)00127-7	econometrics;covariance matrix;statistical hypothesis testing;statistical inference;delta method;asymptotic analysis;method of moments;income distribution;lorenz curve;stochastic dominance;distribution function;calculus;inequality;mathematics;dominance;variance;grouped data;multivariate analysis;data analysis;statistics;concave function	ML	33.302410691834005	-21.8727529574119	151832
e454ddf784259fa2c2ff932746ef76c6e0add8fe	a digital watermarking scheme used for authentication of surveillance video	digital watermarking;watermarking;video surveillance;frame cut;on screen display;surveillance system;digital watermark;authentication;fragile digital watermarking;foreign frame insertion;frame swapping digital watermarking scheme surveillance video authentication fragile digital watermarking time information camera id secret key generation chromatism component frame cut foreign frame insertion;digital digest digital watermarking surveillance system on screen display;surveillance video authentication;time information;video coding;camera id;chromatism component;streaming media;watermarking authentication cameras encoding video surveillance streaming media;digital digest;image watermarking;encoding;frame swapping;digital watermarking scheme;secret key generation;cameras;video surveillance image watermarking video coding	Fragile digital watermarking is one of the methods used for authentication of surveillance video. A fragile digital watermarking scheme is proposed, in which the watermark is made up of time information and camera ID, the secret key is generated based on the video feature, the watermark is embedded into the chromatism component of B frames. This watermarking scheme can detect and locate the modification of the video, including frame cut, foreign frame insertion and frame swapping etc. In addition, while the video is played back, the watermark information picked up from the carrier can be displayed on the video, which is a replacement of the traditional OSD (On Screen Display).	advanced visualization studio;authentication;closed-circuit television;data compression;digital watermarking;embedded system;frame grabber;h.264/mpeg-4 avc;key (cryptography);object storage;paging;randomness;video coding format	Duanquan Xu;Jiangshan Zhang;Baochuan Pang	2010	2010 International Conference on Computational Intelligence and Security	10.1109/CIS.2010.147	computer vision;digital watermarking;computer science;video capture;video tracking;multimedia;internet privacy;watermark	EDA	38.45933111441056	-12.506422333865297	152227
8aa76949977bd6c99581dfb19705d884729f10b6	a bayesian approach to capability testing based on cpk with multiple samples	process capability indices;bayesian approach;posterior probability;multiple samples;capability testing	Process capability indices provide numerical measures to compare the output of a process to client’s expectations. However, most of the existing researches have used traditional distribution frequency method by using a single sample due to assess process capability. An alternative to this approach is to use the Bayesian method. In this paper, we utilize a Bayesian approach based on subsamples to check process capability via capability index Cpk. As a new suggestion, we used the informative normal prior distribution and the characteristics of sufficient statistic of the parameter to drive the posterior distribution. The capability test is done, and the posterior probability p, for which the process under investigation is capable, is derived both based on the most popular index Cpk. Finally, a numerical example is given to clarify the method. Copyright © 2013 John Wiley & Sons, Ltd.	bayesian network;information;john d. wiley;numerical analysis	M. Kargar;Mashaallah Mashinchi;Abbas Parchami	2014	Quality and Reliability Eng. Int.	10.1002/qre.1512	econometrics;process capability;bayesian probability;engineering;operations management;mathematics;posterior probability;statistics	AI	28.522816841050066	-19.408287869584402	152352
e1b87ebe3d465d7cd0d38c5fe0190f2f1cb135d8	new exponentially weighted moving average control charts for monitoring process mean and process dispersion	statistical process control;control chart;ordered double ranked set sampling;average run length;monte carlo simulation;exponentially weighted moving average	Exponentially weighted moving average (EWMA) control charts have been widely accepted because of their excellent performance in detecting small to moderate shifts in the process parameters. In this paper, we propose new EWMA control charts for monitoring the process mean and the process dispersion. These EWMA control charts are based on the best linear unbiased estimators obtained under ordered double ranked set sampling (ODRSS) and ordered imperfect double ranked set sampling (OIDRSS) schemes, named EWMA-ODRSS and EWMA-OIDRSS charts, respectively. We use Monte Carlo simulations to estimate the average run length, median run length, and standard deviation of run length of the proposed EWMA charts. We compare the performances of the proposed EWMA charts with the existing EWMA charts when detecting shifts in the process mean and in the process variability. It turns out that the EWMA-ODRSS mean chart performs uniformly better than the classical EWMA, fast initial response-based EWMA, Shewhart-EWMA, and hybrid EWMA mean charts. The EWMA-ODRSS mean chart also outperforms the Shewhart-EWMAmean charts based on ranked set sampling (RSS) and median RSS schemes and the EWMAmean chart based on ordered RSS scheme. Moreover, the graphical comparisons of the EWMA dispersion charts reveal that the proposed EWMAODRSS and EWMA-OIDRSS charts are more sensitive than their counterparts. We also provide illuminating examples to illustrate the implementation of the proposed EWMA mean and dispersion charts. Copyright © 2014 John Wiley & Sons, Ltd.	arnold;chart;graphical user interface;john d. wiley;mathematical model;monte carlo method;performance;portable document format;rss;rare events;run-length encoding;sampling (signal processing);sensor;simulation;spatial variability;the blues brothers;the matrix	Abdul Haq;Jennifer Brown;Elena Moltchanova	2015	Quality and Reliability Eng. Int.	10.1002/qre.1646	ewma chart;econometrics;control chart;computer science;operations management;mathematics;statistical process control;statistics;monte carlo method	Security	28.803780543469944	-20.16094970702238	152452
f1c71bb807e6ddda52c6afd720247b400afe7def	optimal run length for discrete-event distributed cluster-based simulations	transient state;steady state	In scientific simulations the results generated usually come from a stochastic process. New solutions with the aim of improving these simulations have been proposed, but the problem is how to compare these solutions since the results are not deterministic. Consequently how to guarantee that the output results are statistically trusted. In this work we apply a statistical approach in order to define the transient and steady state in discrete event distributed simulation. We used linear regression and batch method to find the optimal simulation size. As contributions of our work we can enumerate: we have applied and adapted the simple statistical approach in order to define the optimal simulation length; we propose the approximate approach to normal distribution instead of generate replications sufficiently large; and the method can be used in other kind of non-terminating science simulations where the data either have a normal distribution or can be approximated by a normal distribution.	approximation algorithm;computation;computational resource;computer cluster;computer simulation;deterministic algorithm;divergence (computer science);enumerated type;experiment;newman's lemma;run-length encoding;steady state;stochastic process;time series	Francisco Borges;Albert Gutierrez-Milla;Remo Suppi;Emilio Luque	2014		10.1016/j.procs.2014.05.007	mathematical optimization;computer science;theoretical computer science;machine learning;data mining;mathematics;algorithm;statistics	HPC	30.50177982460964	-14.47654159091724	152470
475a14f13db11ee47a42e66008c6bd021134ef0f	random coefficient model of basal area growth for longitudinal data	goodness of fit;basal area;forestry;barium;biological system modeling;statistical analysis forestry;basal area growth model;trajectory;statistical analysis;soil sea measurements forestry time measurement vegetation synthetic aperture sonar computational modeling educational institutions laboratories error analysis;forest management random coefficient model longitudinal data basal area growth model;soil;longitudinal data;optimization model;growth model;covariance matrix;forest management;data models;random coefficient model	Basal Area Growth model play an important role in forest management. In the permanent forest plots, the measures of BA growth are taken repeatedly over time, each stand has its individual trajectory. It is necessary to model both main response and individual trajectory of forest stands BA. In this paper, we show how a new random coefficient model of stands Basal Area Growth, which is developed based on longitudinal data. Through comparing the goodness of fit Statistics for different error structures, the optimal model is with AR(1) error structure.	basal (phylogenetics);coefficient;multilevel model	Qing Zhang;Junhui Zhao;Xingang Kang	2009		10.1109/ICNC.2009.636	econometrics;geography;hydrology;forestry	NLP	25.20953603004967	-21.472534281339435	152693
eb4285b743f75e81f5536571f055cd86db1e33dd	on efficient estimation in additive hazards regression with current status data	one step estimator;additive hazards model;statistical software;interval censoring;proportional hazard model;hazard models;linear functionals;renal function;semiparametric efficiency;current status data;simulation study;parameter estimation;efficient estimation	The additive hazard regression (AHR) model is known for its convenience in interpretation, as hazard is modeled as a linear function of covariates. One outstanding issue in the application of such a model in the analysis of current status data is that there lacks an efficient and computationally simple approach for parameter estimation. In the current literature, Lin et al.'s (1998) method enjoys the computational ease but it is not semi-parametrically efficient, whereas Martinussen and Scheike's (2002) method is semi-parametrically efficient but difficult to compute. In this paper, we propose a new estimation approach in the context of Lin et al.'s AHR models where the monitor time process follows a proportional hazard model. We show that not only the proposed estimator achieves semi-parametric information bound, but also its implementation can be done easily using existing statistical software. We evaluate this new method via simulation studies. Also, we illustrate the proposed method through an analysis of renal function recovery data.	utility functions on indivisible goods	Xuewen Lu;Peter X.-K. Song	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2011.12.011	econometrics;computer science;data mining;mathematics;renal function;estimation theory;proportional hazards model;statistics	ML	29.41663789615074	-23.13690493020064	152725
df6dd0a320bbd3726b3c0fa8cad652a3b8d02bed	linearization of local probabilistic sensitivity via sample re-weighting	linear estimation;monte carlo sampling;conditional expectation;re weighting;linearization;local probabilistic sensitivity measures	Local probabilistic sensitivity of input variable X with respect to output variable Z is proportional to the derivative of the conditional expectation E(X|z). This paper reports on experience in computing this conditional expectation. Linearized estimates are found to give acceptable performance, but are not generally applicable. A new method of linearization based on re-weighting a Monte Carlo sample is introduced. Results are comparable to the linearized estimates, but this method is more widely applicable. Results generally improve by conditioning on a small window around z.	algorithm;benchmark (computing);binary prefix;computer performance;linear model;monte carlo method;window function	Roger M. Cooke;Dorota Kurowicka;Isaac Meilijson	2003	Rel. Eng. & Sys. Safety	10.1016/S0951-8320(02)00223-5	econometrics;mathematical optimization;conditional expectation;conditional variance;mathematics;linearization;statistics;monte carlo method	AI	30.70608622089642	-17.742953360113532	152790
33d9ed0ea59265906d14552f4ac1da9bdfdae605	analyzing supersaturated designs by means of an information based criterion	62 07;type ii error;62j12;performance evaluation;receiver operator characteristic;screening;physical sciences;data mining;variable selection;simulation experiment;roc;generalized linear models;symmetrical uncertainty;roc curve;error rate;error rates;general linear model;62k15;supersaturated design;information theory	The cost and time consumption of many industrial experimentations can be reduced using the class of supersaturated designs since this can be used for screening out the important factors from a large set of potentially active variables. A supersaturated design is a design for which there are fewer runs than effects to be estimated. Although there exists a wide study of construction methods for supersaturated designs, their analysis methods are yet in an early research stage. In this article, we propose a method for analyzing data using a correlation-based measure, named as symmetrical uncertainty. This method combines measures from the information theory field and is used as the main idea of variable selection algorithms developed in data mining. In this work, the symmetrical uncertainty is used from another viewpoint in order to determine more directly the important factors. The specific method enables us to use supersaturated designs for analyzing data of generalized linear models for a Bernoulli respons...		Christos Koukouvinos;Christina Parpoula	2012	Communications in Statistics - Simulation and Computation	10.1080/03610918.2011.579365	econometrics;mathematical optimization;information theory;mathematics;receiver operating characteristic;statistics	EDA	27.982232205889108	-20.02939678409812	153739
f65f1b643b3550d316101a8064bb06da635b6530	efficiently pricing continuously monitored barrier options under stochastic volatility model with jumps	65c05;barrier options;35c05;60j65;option pricing;stochastic volatility;jump diffusion;monte carlo	Stochastic volatility model with jumps fits almost perfectly the empirical implied volatility surface. Under this model, this paper considers continuously monitored barrier options pricing by Monte Carlo simulation. Based on quadratic exponential scheme, this paper develops an algorithm for pricing barrier options and provides convergence of the algorithm by moment-matching techniques. Variance reduction technique based on control variates further improves the efficiency of the algorithm. The algorithm is also extended to stochastic volatility model with contemporaneous jumps in variance and stock price. Simulations show that the proposed algorithm is efficient and easy to implement. Compared to contemporaneous jumps in variance and stock price, only jumps in stock price produce more profound impact on barrier options prices.	volatility	Sumei Zhang;Junhao Geng	2017	Int. J. Comput. Math.	10.1080/00207160.2016.1210796	forward volatility;implied volatility;actuarial science;volatility smile;valuation of options;stochastic volatility;binomial options pricing model;monte carlo methods for option pricing;monte carlo method	ECom	32.54991699376526	-17.06808566676096	153845
d5f2fc562bb2a49c56b06c491a7666a03a88017b	simulation-based designs for multiperiod control	modelo dinamico;experimental design;predictive control;time varying;adaptive design;analisis datos;05bxx;adaptive designs;simulation;dynamic model;plan experiencia;simulacion;modelo input output;certainty equivalence;h p d regions;data analysis;input output model;62k99;modele entree sortie;plan experience;62l20;modele dynamique;stochastic approximation;statistical computation;approximation stochastique;calculo estadistico;fieller interval;stochastic control;analyse donnee;calcul statistique;aproximacion estocastica;sequential optimization	In multiperiod control one sequentially chooses the value of an input variable to keep an output variable close to a target value. At any stage the input variable has to be such that the output is close to target while at the same time it provides as much information as possible about the model that relates input and output, to help keep the output close to target in the future. Certainty equivalence policies set the input variable at the value under which the output is estimated to be equal to target. Three alternative approaches that improve on certainty equivalence by searching for the best policy among families of policies that embed them are proposed, and their use is exemplified both on static as well as on time-varying dynamic models. Two approaches observe on credible regions for the root, while the third one is an adaptation of stochastic approximation sequences. © 2007 Elsevier B.V. All rights reserved.	input/output;simulation;stochastic approximation;stochastic control;turing completeness	Enrique González-Dávila;Roberto Dorta-Guerra;Josep Ginebra	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2007.04.010	stochastic approximation;econometrics;input–output model;stochastic control;control theory;mathematics;mathematical economics;data analysis;design of experiments;model predictive control;statistics	ML	34.98238460071035	-21.9944116457015	154128
0d2d8279849d7ba401d28d61fe4d2c5098da86aa	estimation of the shape parameter of the weibull distribution using linear regression methods: non-censored samples	robust estimators;linear regression;weibull distribution;shape parameter;monte carlo simulation	The two-parameter Weibull distribution is one of the most widely applied probability distributions, particularly in reliability and lifetime modelings. Correct estimation of the shape parameter of the Weibull distribution plays a central role in these areas of statistical analysis. Many different methods can be used to estimate this parameter, most of which utilize regression methods. In this paper, we presented various regression methods for estimating the Weibull shape parameter and an experimental study using classical regression methods to compare the results of the methods. A complete list of the parameter estimators considered in this study is as follows: ordinary least squares (OLS), weighted least squares (WLS, Bergman, F&T, Lu), non-parametric robust Theil's (Theil) and weighted Theil's (WeTheil), robust Winsorized least squares (WinLS), and M-estimators (Huber, Andrew, Tukey, Cauchy, Welsch, Hampel and Logistic). Estimator performances were compared based on bias and mean square error criteria using Monte-Carlo simulations. The simulation results demonstrated that for small, complete, and non-outlier data sets, the Bergman, F&T, and Lu estimators are more efficient than the others. When the data set contains one or two outliers in the X direction, Theil is the most efficient estimator. Copyright © 2012 John Wiley & Sons, Ltd.		Arzu Altin Yavuz	2013	Quality and Reliability Eng. Int.	10.1002/qre.1472	weibull distribution;econometrics;mathematical optimization;exponentiated weibull distribution;linear regression;mathematics;shape parameter;robust regression;statistics;monte carlo method	Robotics	29.008298186585264	-22.455120621848163	154430
fb6a90e6a9d0e30eb6fc541b4405854dfadb6674	a matlab graphical user interface for nonintrusive polynomial chaos theory	uncertain systems;mathematics computing;chaos;uncertainty;simulation;simulation uncertain systems stochastic systems;random variables;polynomials;graphical user interfaces;polynomials graphical user interfaces mathematics computing;graphical user interfaces polynomials mathematical model chaos uncertainty random variables load modeling;mathematical model;simulationx matlab graphical user interface nonintrusive polynomial chaos theory uncertainty propagation black boxes mathematical foundations buck converter;stochastic systems;load modeling	Polynomial chaos theory is a sophisticated method for the analysis of uncertainty propagation in dynamic systems, which is considerably faster than Monte Carlo methods in systems with few random parameters. Classically, all system laws are re-formulated into a deterministic set of equations, but in recent years, a nonintrusive variation has been developed to apply polynomial chaos to black boxes. However, a major constraint for the application of this concept is the depth of its mathematical foundations, followed by many adjustments. After a brief introduction of this concept, we would therefore like to present a graphical user interface which facilitates its use. Through this tool, the numerous difficulties in applying polynomial chaos become irrelevant for the user. Additionally, we show possible applications using the example of a buck converter, with a custom made interface tool between the commercial simulation environments MATLAB and SimulationX.	black box;buck converter;chaos theory;dynamical system;graphical user interface;matlab;monte carlo method;polynomial;portable document format;propagation of uncertainty;relevance;simulation;simulationx;software propagation	Kanali Togawa;Andrea Benigni;Antonello Monti	2012	2012 Complexity in Engineering (COMPENG). Proceedings	10.1109/CompEng.2012.6242955	discrete mathematics;simulation;polynomial chaos;computer science;theoretical computer science	AI	32.278362296560324	-16.12379865205598	154673
c9b52433868b269eb98f700c014f646eef3e49f4	extension of standardized time series for continuous-time statistics	continuous time;jump process;stochastic process;expected utility;monte carlo study;time series;simulation experiment;confidence interval;technical report;computer simulation;discrete time observation;steady state	In steady-state computer simulation experiments continuous-time statistics play an important role. They are used to estimate a variety of different system's parameters. For example, in queueing simulations continuous-time statistics are used to estimate expected number of customers in the system, expected utilization of the server, and the expected departure rate of customers, among others. In steady-state simulation experiments we typically observe two types of continuous-time statistics:<list><item>A stationary stochastic process y(t), 0≤t≤T, such that E[y(t)] = μ, 0≤t≤T. </item><item>A stationary increment jump process y(t), 0≤t≤T such that E[y(t)/t] = μ, 0<t≤T, and y(0) = 0. </item></list> The objective of the experiment is typically to estimate an interval, I, which with a prespecified probability &eegr; contains μ, i.e., Pr{μεI} = &eegr;. The interval I is referred to as a confidence interval and is denoted by ± μ H, where μ is the center and H is the half-width of the interval. For the above cases, μ is respectively computed by:<list><item>μ = T<supscrpt>-1</supscrpt> μ<supscrpt>T</supscrpt><subscrpt>0</subscrpt> y(t)dt. </item><item>μ = T<supscrpt>-1</supscrpt> y(T). </item></list> In this talk we present formulas for estimating H for each of the above cases. These formulas are developed and evaluated in Nozari [1]. The formulas are extensions of the method of “Standardized Time Series” developed by Schruben [2] for discrete-time observations. We give formulas for two different estimators: Area, which is the continuous-time version of the Sum estimator of Schruben, and Maximum. Also, we give formulas for combining these estimators with the classical estimator. To examine the behavior of the estimators, we present results of a Monte Carlo study. Our estimators work quite well and our results are consistent with those of Schruben for discrete-time observations.	computer simulation;experiment;monte carlo method;server (computing);stationary process;steady state;stochastic process;time series	Ardavan Nozari	1985		10.1145/21850.253093	computer simulation;stochastic process;econometrics;mathematical optimization;simulation;confidence interval;expected utility hypothesis;computer science;technical report;time series;mathematics;steady state;statistics	ML	31.28426194536319	-18.815175525088943	154701
88af8d9f565fd0b7e56e3d39f945d72950e64202	analysis of the conditional stock-return distribution under incomplete specification	distribution;asset prices;goodness of fit;bootstrap method;high frequency data;theoretical model;stock market;functional form;maximum likelihood;schwarz information criterion;confidence interval;portfolio theory;stock returns;likelihood ratio test;cramer von mises test statistic;conditional distribution;production efficiency	The analysis of the distribution of the stock-returns plays an important role in financial theory since a distributional assumption is required for mean–variance portfolio theory, theoretical models of capital asset prices, determining the price of derivative products, efficient estimation by maximum likelihood procedure and establishing forecasting confidence intervals. Up to now, most of the studies analyze the unconditional distribution of the stock-returns using the likelihood ratio test, the Schwarz Information Criterion or by graphical analysis. However, the analysis of the conditional distribution is very important since most of the empirical financial studies use high-frequency data and these data present dynamic structure. In this paper, we focus on the conditional distribution. We design different bootstrap mechanisms for comparing the goodness-of-fit of several functional forms postulated under the null hypothesis for individual Spanish stocks and for the General Index of the Madrid Stock Market. We use the Cr amer–von Mises test statistic, based on the standardized residuals and assuming that the null distribution can depend on some unknown parameters. 2003 Elsevier B.V. All rights reserved.	bayesian information criterion;distribution (mathematics);modern portfolio theory;time value of money	J. Samuel Baixauli-Soler;Susana Álvarez-Díez	2004	European Journal of Operational Research	10.1016/S0377-2217(03)00086-9	financial economics;distribution;conditional probability distribution;econometrics;confidence interval;economics;likelihood-ratio test;productive efficiency;modern portfolio theory;mathematics;maximum likelihood;goodness of fit;higher-order function;statistics	ML	31.045610535816234	-22.499580158503026	155164
f7b42ba807acb980fff5681ea02bd4f34915bd9d	sensitivity analysis using the monte carlo acceptance-rejection method	acceptance rejection sampling;analisis sensibilidad;estimacion;fiabilidad;reliability;metodo monte carlo;coeficiente variacion;62;intervalo confianza;plan echantillonnage;monte carlo method sensitivity analysis;methode monte carlo;sampling design;cas pire;coefficient variation;90;importance function;sampling;variations;sensitivity;confidence interval;plan muestreo;intervals;estimation;variation coefficient;fonction importance;sensitivity analysis;coefficients;fiabilite;methode rejet acceptation;monte carlo method;intervalle confiance;analyse sensibilite;planning;monte carlo;confidence limits;varianza;variance	This paper describes a Monte Carlo sampling plan for estimating how a function varies in response to changes in its arguments. Most notably, the plan effects this sensitivity analysis by applying the acceptance-rejection technique to data sampled at only one specified setting for the arguments, thus saving considerable computing time when compared to alternative methods. The plan which applies for a 0-1 response on each replication has immediate application when estimating variation in system performance measures in reliability analysis.The paper derives the variances of the proposed estimators and shows how to use worst-case bounds on these or on corresponding coefficients of variation to choose the arguments, at which to sample, that minimize the worst-case bounds. Individual and simultaneous confidence intervals are derived and an example based on s-t reliability illustrates the method. The paper also compares the proposed method and an alternative Monte Carlo approach that uses an importance function.	monte carlo method;rejection sampling	George S. Fishman	1990	SIAM J. Scientific Computing	10.1137/0911066	econometrics;confidence interval;calculus;mathematics;monte carlo integration;statistics;monte carlo method	HPC	30.22488181366201	-17.7792228191019	155423
fe2252fe932ac79936c355fde2fdecb53b77ba97	baselining network traffic and online faults detection	traffic pattern;fluctuations;maximum likelihood;online fault detection;traffic control;false alarm rate;random variables;traffic model;maximum likelihood estimation;baseline random variable;telecommunication traffic;monitoring;stochastic processes;automatic detection;network traffic;network anomalies;fault detection;stochastic approximation;local area networks telecommunication traffic computer network reliability stochastic processes fault diagnosis maximum likelihood estimation;random variable;maximum likelihood detection;normal operator;asymptotic distribution;parameter estimation;network operation;finite mixture model;maximum likelihood function;telecommunication traffic fault detection traffic control random variables stochastic processes maximum likelihood detection maximum likelihood estimation parameter estimation monitoring fluctuations;traffic pattern network operation network traffic online fault detection network anomalies finite mixture model stochastic approximation maximum likelihood function asymptotic distribution baseline random variable;change point;local area networks;fault diagnosis;computer network reliability	This paper addresses the problem of normal operation baselining for automatic detection of network anomalies. A model of network traffic is presented in which studied variables are viewed as sampled from finite mixture model. Based on the stochastic approximation of the maximum likelihood function, we propose baselining network normal operation, using the asymptotic distribution of the difference between successive estimates of model parameters. The baseline random variable is shown to be stationary, with mean zero under normal operation. Anomalous events are shown to induce an abrupt jump in the mean. Detection is formulated as an online change point problem, where the task is to process the baseline random variable realizations, sequentially, and raise alarms as soon as anomalies occur. An analytical expression of false alarm rate allows us to choose the design threshold, automatically. Extensive experimental results on a real network showed that our monitoring agent is able to detect unusual changes in the characteristics of network traffic, adapt to diurnal traffic patterns, while maintaining a low alarm rate. Despite large fluctuations in network traffic, this work proves that tailoring traffic modeling to specific goals can be efficiently achieved.	baseline (configuration management);network traffic control	Hassan Hajji	2003		10.1109/ICC.2003.1204189	traffic generation model;stochastic approximation;econometrics;machine learning;mathematics;maximum likelihood;statistics	ML	30.7618585993304	-13.424950225520364	155564
f16cb662a5b5e5eae92c13ca88461fcb1a83206f	retrospective bayesian outlier detection in ingarch series	time series of counts;generalized linear models;additive outliers	INGARCH models for time series of counts arising, e.g., in epidemiology or finance assume the observations to be Poisson distributed conditionally on the past, with the conditional mean being an affine-linear function of the previous observations and the previous conditional means. We model outliers within such processes, assuming that we observe a contaminated process with additive Poisson distributed contamination, affecting each observation with a small probability. Our particular concern are additive outliers, which do not enter the dynamics of the process and can represent measurement artifacts and other singular events influencing a single observation. Retrospective analysis of such outliers is difficult within a non-Bayesian framework since the uncontaminated values entering the dynamics of the process at contaminated time points are unobserved. We propose a Bayesian approach to outlier modeling in INGARCH processes, approximating the posterior distribution of the model parameters by application of a componentwise Metropolis-Hastings algorithm. Analyzing real and simulated data sets, we find Bayesian outlier detection with non-informative priors to work well in practice when there are some outliers in the data.	anomaly detection;information;linear function;metropolis;metropolis–hastings algorithm;time series;utility functions on indivisible goods	Roland Fried;Inoncent Agueusop;Björn Bornkamp;Konstantinos Fokianos;Jana Fruth;Katja Ickstadt	2015	Statistics and Computing	10.1007/s11222-013-9437-x	econometrics;generalized linear model;data mining;mathematics;statistics	ML	30.378363247297507	-22.743044433333054	155832
b670caf6424751910cb6a02eb913894ddb1241c8	useful periods for lifetime distributions with bathtub shaped hazard rate functions	reliability;hazards;weibull distribution;reliability problem lifetime distribution bathtub shaped hazard rate function formal mathematical definition additive weibull distribution pseudoturning points;hazard rate;pseudoturning points additive weibull distribution curvature;weibull distribution reliability hazards;hazards turning shape aging integrated circuit modeling distributed computing weibull distribution switches reliability engineering councils	We propose computationally tractable formal mathematical definitions for the 'useful period' of lifetime distributions with bathtub shaped hazard rate functions. Detailed analysis of the reduced additive Weibull hazard rate function illustrates its utility for identifying such useful periods. Examples of several other bathtub shaped hazard rate functions are also presented with applications to lifetime data. The suggestion is made of defining and considering analogous 'stable periods' in the case of the corresponding upside-down bathtub shaped mean residual life functions.	bathtub curve;cobham's thesis;curve fitting;second life;utility functions on indivisible goods	Mark Bebbington;Chin-Diew Lai;Ricardas Zitikis	2006	IEEE Transactions on Reliability	10.1109/TR.2001.874943	reliability engineering;weibull distribution;econometrics;exponentiated weibull distribution;hazard;engineering;weibull fading;reliability;mathematics;bathtub curve;hazard ratio;statistics	Metrics	31.37570596582503	-18.54363238005117	156243
7362aa1fa6139ac98cce253ba01ec145398f0644	scalable model based evolutionary multi-objective optimization	inversed modeling scalable model based evolutionary multiobjective optimization regularity based estimation distribution algorithm probabilistic model principal curve local gaussian model large dimensional optimization problems;probability evolutionary computation gaussian processes	This talk presents some recent advances in model-based evolutionary multi-objective optimization. We first present a regularity based estimation of distribution algorithm that uses a probabilistic model containing a principal curve and a local Gaussian model. We show that the proposed algorithm is able to work efficiently for large dimensional optimization problems with a small population size. Then, a new evolutionary multi-objective optimization algorithm by using inversed modeling is discussed. By means of constructing inverse models, we are able to sample preferred solutions in the objective space, which is proved to be helpful in maintaining the diversity and generating additional solutions at a low computational cost.	multi-objective optimization;program optimization	Yaochu Jin	2015			probabilistic-based design optimization;mathematical optimization;combinatorics;estimation of distribution algorithm;machine learning;mathematics	EDA	28.97667358428174	-10.338995676943137	156936
5ebb7386045bcc986c3c929718f2cea5af62fabf	hierarchical clustering of subpopulations with a dissimilarity based on the likelihood ratio statistic: application to clustering massive data sets	algorithme rapide;multinomial distribution;hierarchical clustering;analyse amas;likelihood ratio;distribution donnee;analisis estadistico;generic algorithm;analisis datos;algorithme k moyenne;classification non supervisee;inversion;base donnee tres grande;multivariate normal;k means;theoreme central limite;large data sets;binned data;probabilistic approach;curva gauss;data mining;data distribution;data analysis;hierarchical classification;cluster analysis;likelihood ratio statistic;statistical analysis;central limit theorem;loi multinomiale;fouille donnee;enfoque probabilista;approche probabiliste;fast algorithm;clasificacion no supervisada;analyse statistique;dendrogram;classification hierarchique;loi normale;unsupervised classification;teorema central limite;algoritmo k media;dissimilarity;analyse donnee;k means algorithm;analisis cluster;very large databases;rapport vraisemblance;ley multinomial;clasificacion jerarquizada;distribucion dato;busca dato;algoritmo rapido;gaussian distribution;massive data sets;relacion verosimilitud	The problem of clustering subpopulations on the basis of samples is considered within a statistical framework: a distribution for the variables is assumed for each subpopulation and the dissimilarity between any two populations is defined as the likelihood ratio statistic which compares the hypothesis that the two subpopulations differ in the parameter of their distributions to the hypothesis that they do not. A general algorithm for the construction of a hierarchical classification is described which has the important property of not having inversions in the dendrogram. The essential elements of the algorithm are specified for the case of well-known distributions (normal, multinomial and Poisson) and an outline of the general parametric case is also discussed. Several applications are discussed, the main one being a novel approach to dealing with massive data in the context of a two-step approach. After clustering the data in a reasonable number of ‘bins’ by a fast algorithm such as k-Means, we apply a version of our algorithm to the resulting bins. Multivariate normality for the means calculated on each bin is assumed: this is justified by the central limit theorem and the assumption that each bin contains a large number of units, an assumption generally justified when dealing with truly massive data such as currently found in modern data analysis. However, no assumption is made about the data generating distribution.	algorithm;automated theorem proving;cluster analysis;dendrogram;hierarchical clustering;k-means clustering;multinomial logistic regression;population	Antonio Ciampi;Yves Lechevallier;Manuel Castejón Limas;Ana González-Marcos	2007	Pattern Analysis and Applications	10.1007/s10044-007-0088-4	econometrics;combinatorics;computer science;mathematics;cluster analysis;single-linkage clustering;statistics;k-means clustering	ML	34.3615867149172	-23.222435761334072	157170
389eabecd158e59afe6ee56e1a7f9a6b4c521a1d	a central limit theorem and hypotheses testing for risk-averse stochastic programs		We study statistical properties of the optimal value of the Sample Average Approximation of risk averse stochastic problems. Central Limit Theorem type results are derived for the optimal value when the stochastic program is expressed in terms of a law invariant coherent risk measure having a discrete Kusuoka representation. The obtained results are applied to hypotheses testing problems aiming at comparing the optimal values of several risk averse convex stochastic programs on the basis of samples of the underlying random vectors. We also consider non-asymptotic tests based on confidence intervals on the optimal values of the stochastic programs obtained using the Robust Stochastic Approximation algorithm. Numerical simulations show how to use our developments to choose among different distributions and show the superiority of the asymptotic tests on a class of risk averse stochastic programs.	approximation algorithm;coherence (physics);coherent risk measure;numerical linear algebra;optimization problem;risk aversion;simulation;stochastic approximation;stochastic programming	Vincent Guigues;Volker Krätschmer;Alexander Shapiro	2018	SIAM Journal on Optimization	10.1137/16M1104639	stochastic optimization;mathematical optimization;statistical hypothesis testing;mathematics;statistical inference;risk aversion;central limit theorem;invariant (mathematics);coherent risk measure;confidence interval	ML	30.529255278516903	-17.42229926127353	157172
a5d9f46dc3d0f0b646b421882146d9f20be9b2d8	a new phenotypically constrained control policy in boolean networks based on basins of attraction	basins of attraction;basins of attraction genetic regulatory network boolean network probabilistic boolean network network intervention;genetic regulatory network phenotypically constrained control policy boolean networks basins of attraction stationary control policy structural properties greedy control policy steady state mass ambiguous network states potential collateral damage long run network behavior randomly generated networks;genetic regulatory network;random functions biology computing boolean functions genetics genomics;probabilistic boolean network;boolean network;network intervention	ABSTRACT In this paper, a new stationary control policy in Boolean networks (BNs) is derived based on the structural properties of basins of attraction (BOA). The new greedy control policy guarantees that no new attractor cycles will be created by intervention to the original BNs. Hence, it will not introduce significant steady-state mass to ambiguous network states, which may represent unknown phenotypes corresponding to potential collateral damage. Comparing the control performance of this new phenotypically constrained BOA control policy with previous control policies based on long-run network behavior, the experiments based on randomly generated networks have shown that the new control policy obtains competitive performance with reduced complexity.	boolean network;experiment;greedy algorithm;procedural generation;stationary process;steady state	Xiaoning Qian	2012	Proceedings 2012 IEEE International Workshop on Genomic Signal Processing and Statistics (GENSIPS)	10.1109/GENSIPS.2012.6507725	boolean network;computer science;theoretical computer science;machine learning;data mining;mathematics	Robotics	33.39985951059115	-10.941032726563362	157353
9f8a7f3dfb251941b20f89a0f551fe85233169df	a stopping criterion based on kalman estimation techniques with several progress indicators	kalman filtering;stopping criteria;estimation theory;pareto front;kalman filter;data fusion;moeas;moop;stopping criterion;fusion architectures;quality measures;cumulant	The need for a stopping criterion in MOEA's is a repeatedly mentioned matter in the domain of MOOP's, even though it is usually left aside as secondary, while stopping criteria are still usually based on an a-priori chosen number of maximum iterations. In this paper we want to present a stopping criterion for MOEA's based on three different indicators already present in the community. These indicators, some of which were originally designed for solution quality measuring (as a function of the distance to the optimal Pareto front), will be processed so they can be applied as part of a global criterion, based on estimation theory to achieve a cumulative evidence measure to be used in the stopping decision (by means of a Kalman filter). The implications of this cumulative evidence are analyzed, to get a problem and algorithm independent stopping criterion (for each individual indicator). Finally, the stopping criterion is presented from a data fusion perspective, using the different individual indicators' stopping criteria together, in order to get a final global stopping criterion.	algorithm;estimation theory;iteration;kalman filter;moea framework;pareto efficiency;progress indicator	José Luis Guerrero;Jesús Caja García;Luis Martí;José M. Molina López;Antonio Berlanga	2009		10.1145/1569901.1569983	kalman filter;econometrics;mathematical optimization;optimal stopping;computer science;mathematics;statistics	Vision	28.760331383203617	-13.420466186723486	157401
355fb33b2af3deaf7e8184d640e1b37e521c66a8	a robust video watermarking technique for the tamper detection of surveillance systems	tamper detection;video watermarking;surveillance systems	A digital watermark embeds an imperceptible signal into data such as audio, video and images, for different purposes including authentication and tamper detection. A real-time video surveillance application requires a large quantity of sequences to be processed, which makes computational efficiency an additional constraint on video watermarking for surveillance systems. As a result, spatial domain schemes are a more efficient than frequency domain schemes. This paper focuses on video watermarking, particularly with respect to the Audio Video Interleaved (AVI) form of video file format. It proposes two new watermarking schemes which seem to offer a high degree of imperceptibility and efficient tamper detection. Both schemes were subjected to nine different types of common attack, which revealed one scheme, VW8F, to be superior, particularly in terms of imperceptibility. VW8F was then compared with a range of similar schemes by other authors. The results show that VW8F offers both improved imperceptibility (average PSNR of 47.87 dB) and proven efficiency at detecting a wider range of tampering compared to the other similar schemes.	audio signal processing;authentication;closed-circuit television;digital watermarking;peak signal-to-noise ratio;real-time clock;sensor;video file format	Farnaz Arab;Shahidan M. Abdullah;Siti Zaiton Mohd Hashim;Azizah Abdul Manaf;Mazdak Zamani	2015	Multimedia Tools and Applications	10.1007/s11042-015-2800-5	computer vision;computer science;video tracking;internet privacy;watermark;computer security	EDA	38.60656957994801	-12.459195071672365	157446
e9e2a7334c321b5f32bc25f9d0b6e1472a05ecfe	distribution of time between failures of machining center based on type i censored data	mean time between failure;reliability;machining center;weibull distribution;censored data;goodness of fit test;likelihood function;type i censored data	This paper applies a type I censor likelihood function to make the fitting of Weibull distribution of time between failures of machining center (MC). The paper also gives Goodness-of-fit tests by Hollander’s method and proves that the time between MC failures follows the Weibull distribution. The conclusion not only deeply analyzes the MC failure law, but also establishes the basis of calculation for the mean time between failures of MC with censored lifetime data. q 2002 Elsevier Science Ltd. All rights reserved.	bsd;censoring (statistics);mean time between failures	Yi Dai;Yun-fei Zhou;Yazhou Jia	2003	Rel. Eng. & Sys. Safety	10.1016/S0951-8320(02)00243-0	reliability engineering;weibull distribution;econometrics;mean time between failures;engineering;reliability;mathematics;likelihood function;goodness of fit;censoring;statistics	Theory	29.7429597059394	-19.615584245265804	157484
b266fd5853f22cca03c082038278579d0a0719e4	an efficient estimation for switching regression models: a monte carlo study	estimator efficiency;iterative method;metodo estadistico;finite sample;62f12;analisis numerico;regression function;gaussian mixture;metodo monte carlo;covariancia;theorie approximation;65c05;fonction regression;funcion exponencial;distance measure;melange loi probabilite;stochastic method;estimation method;gaussian mixtures;funcion regresion;fonction exponentielle;implementation;62f10;62e17;exponential function;simulacion numerica;estimation non parametrique;echantillon fini;62g20;methode monte carlo;matrice covariance;monte carlo study;mixed distribution;regression model;covariance;statistical method;matriz covariancia;distribucion estadistica;asymptotic behavior;comportement asymptotique;statistical regression;62jxx;analyse numerique;integral square error;metodo iterativo;fonction caracteristique;approximation theory;non parametric estimation;comportamiento asintotico;modelo regresion;numerical analysis;estimation erreur;distribution statistique;error estimation;methode statistique;methode iterative;regresion estadistica;monte carlo experiment;modele regression;monte carlo method;simulation numerique;estimacion error;33b10;characteristic function;efficacite estimateur;methode stochastique;finite sample properties;mezcla ley probabilidad;switching regression;60e10;estimacion no parametrica;weight function;switching regression model;estimation statistique;efficient estimation;implementacion;regression statistique;estimacion estadistica;statistical estimation;funcion caracteristica;statistical distribution;eficacia estimador;covariance matrix;numerical simulation;integrated squared error;metodo estocastico	This paper investigates an efficient estimation method for a class of switching regressions based on the characteristic function (CF). We show that with the exponential weighting function, the CF based estimator can be achieved from minimizing a closed form distance measure. Due to the availability of the analytical structure of the asymptotic covariance, an iterative estimation procedure is developed involving the minimization of a precision measure of the asymptotic covariance matrix. Numerical examples are illustrated via a set of Monte Carlo experiments examining the implentability, finite sample property and efficiency of the proposed estimator.	characteristic function (convex analysis);experiment;iterative method;monte carlo method;time complexity;weight function	Dinghai Xu	2010	Communications in Statistics - Simulation and Computation	10.1080/03610918.2010.497242	computer simulation;estimation of covariance matrices;econometrics;asymptotic analysis;calculus;mathematics;regression analysis;statistics	ML	32.710478226728355	-22.711632092378125	157652
633086fccc78d94ab8a3cae119db0bb07b0e6b70	interval-valued linear model	best binary linear unbiased estimation;interval valued linear model;least square estimation;dp metric;best binsary linear unbiased estimation;期刊论文	This paper introduces a new type of statistical model: the intervalvalued linear model, which describes the linear relationship between an interval-valued output random variable and real-valued input variables. Firstly, notions of variance and covariance of set-valued and interval-valued random variables are introduced. Then, we give the definition of the interval-valued linear model and its least square estimation, as well as some properties of the least square estimator (LSE). Thirdly, we show that, whereas the best linear unbiased estimation does not exist, the best binary linear unbiased estimator exists and it is the LSE. Finally, we present simulation experiments and an application example regarding temperatures of cities affected by their latitude, which illustrates the application of the proposed model.	convex set;experiment;linear model;performance;simulation;statistical model	Xun Wang;Shoumei Li;Thierry Denoeux	2015	Int. J. Comput. Intell. Syst.	10.1080/18756891.2014.967010	u-statistic;econometrics;generalized linear mixed model;mathematical optimization;minimum-variance unbiased estimator;proper linear model;best linear unbiased prediction;computer science;lehmann–scheffé theorem;linear predictor function;machine learning;linear model;mathematics;linear probability model;least squares;statistics;log-linear model	AI	29.552418805736025	-23.65629902956999	157674
bd8ee84198b35a3dcb2405204e62814d1eca9711	on the use of information criteria in analytic hierarchy process	model selection;analytic hierarchy process;normal distribution;information criteria;statistical model;multiple comparisons;decision process;entropy;pairwise comparisons	In this article, we discuss how the model-selection procedures such as Akaike’s information criteria (AIC) can be used for selecting the most appropriate one out of several existing statistical models in the literature for the judgment data used in analytic hierarchy process (AHP). Furthermore, once the appropriate model is selected, a procedure is proposed on the basis of AIC for statistical ranking of the alternatives. This ranking procedure does not suffer from the problem of intransitivity and can be based on non-normal distribution. It enables one to obtain the detailed pattern for the ordered priorities of the alternatives in the decision process involving AHP. 2002 Elsevier Science B.V. All rights reserved.	akaike information criterion;analytical hierarchy;information theory;statistical model	Indrani Basak	2002	European Journal of Operational Research	10.1016/S0377-2217(01)00238-7	normal distribution;pairwise comparison;statistical model;econometrics;entropy;analytic hierarchy process;data mining;mathematics;multiple comparisons problem;model selection;analytic network process;statistics	AI	26.488130293309688	-19.14101815890026	157940
a5ef9e47fd203ccccd17d42963d45e8b79d377cc	fusion of dependent detection systems using copula theory		The US Air Force has multiple detection systems for specific applications that could be combined to work together to yield better accuracy than the individual systems. The amount of time and money used to design, build, simulate, test, validate and verify such combining can be long and expense. Also, there can be several ways to combine these multiple systems, thus, generating more time and cost to determine an optimal (or approximately optimal) combination rule. This paper considers a simple version of this greater problem posed as follows. Suppose we have two legacy detections system families that are designed to detect the same “target” and we conjecture that combining them would yield a new detection system with improved accuracy. Suppose we know the ROC functions of both detection system families, but do not know (or have access to) the data that produced them. Can we construct the ROC function of the combined systems from the individual ROC functions? Copula theory has been in existence since 1959. This theory produces the means to address the dependence between random variables. This paper takes copula and applies it to the fusion of detection systems. Examples will be given that demonstrate how the formulas are used.	money;sensor;simulation	Mark E. Oxley;Christine M. Schubert-Kabban	2018	2018 21st International Conference on Information Fusion (FUSION)	10.23919/ICIF.2018.8455398	computer science;machine learning;copula (linguistics);conjecture;artificial intelligence;random variable	Robotics	33.443394104873505	-14.442582176610332	158083
a62107520d712a8a2beda0c5b91cef1b7368a04e	hiding clinical information in medical images: a new high capacity and reversible data hiding technique	embedding;checksum;tamper localization;reversibility;chaotic encryption	A new high capacity and reversible data hiding scheme for e-healthcare applications has been presented in this paper. Pixel to Block (PTB) conversion technique has been used as an effective and computationally efficient alternative to interpolation for the cover image generation to ensure reversibility of medical images. A fragile watermark and Block Checksum (computed for each 4×4 block) have been embedded in the cover image for facilitating tamper detection and tamper localization, and hence content authentication at receiver. The EPR, watermark data and checksum data has been embedded using Intermediate Significant Bit Substitution (ISBS) to avoid commonly used LSB removal/replacement attack. Non-linear dynamics of chaos have been put to use for encrypting the Electronic Patient Record (EPR)/clinical data and watermark data for improving the security of data embedded. The scheme has been evaluated for perceptual imperceptibility and tamper detection capability by subjecting it to various image processing and geometric attacks. Experimental results reveal that the proposed system besides being completely reversible is capable of providing high quality watermarked images for fairly high payload. Further, it has been observed that the proposed technique is able to detect and localise the tamper. A comparison of the observed results with that of some state-of-art schemes show that our scheme performs better.	algorithmic efficiency;authentication;chaos;checksum;computation;display resolution;epr paradox;embedded system;embedding;encryption;glossary of computer graphics;image processing;interpolation imputation technique;least significant bit;low-noise block downconverter;pixel;watermark (data file);non-t, non-b childhood acute lymphoblastic leukemia	Shabir A. Parah;Farhana Ahad;Javaid A. Sheikh;Ghulam Mohiuddin Bhat	2017	Journal of biomedical informatics	10.1016/j.jbi.2017.01.006	checksum;computer science;theoretical computer science;embedding;internet privacy;watermark;computer security;statistics	Security	38.84977634845364	-11.320562810849271	158179
94184dd73576f23d0c92885c39792269e8ed5bb2	classified mixed logistic model prediction		We develop a classified mixed logistic model prediction (CMLMP) method for clustered binary data by extending a method proposed by Jiang et al. (2018) for continuous outcome data. By identifying a class, or cluster, that the new observations belong to, we are able to improve the prediction accuracy of a probabilistic mixed effect associated with a future observation over the traditional method of logistic regression and mixed model prediction without matching the class. Furthermore, we develop a new strategy for identifying the class for the new observations by utilizing covariates information, which improves accuracy of the class identification. In addition, we develop a method of obtaining second-order unbiased estimators of the mean squared prediction errors (MSPEs) for CMLMP, which are used to provide measures of uncertainty. We prove consistency of CMLMP, and demonstrate finite-sample performance of CMLMP via simulation studies. Our results show that the proposed CMLMP method outperforms the traditional methods in terms of predictive performance. An application to medical data is discussed.	logistic regression	Hanmei Sun;Thuan Nguyen;Yihui Luan;Jiming Jiang	2018	J. Multivariate Analysis	10.1016/j.jmva.2018.06.004	statistics;estimator;mathematics;econometrics;square (algebra);mixed model;logistic regression;probabilistic logic;covariate;binary data	NLP	29.299797878381536	-23.787072228354372	158204
6a160669054a30632d6036eb2023bd660937b1a0	examples of using the research queueing package modeling environment (resqme)	common random numbers;model specification;performance evaluation;bonferroni inequality;selection;confidence interval;performance model;technical report;variance reduction	The RESearch Queueing Package Modeling Environment (RESQME) is a system which provides an integrated, graphics-oriented, performance evaluation workstation environment for constructing, maintaining, revising and evaluating performance models of resource contention systems. In this paper we discuss examples illustrating its use and emphasize the iterative nature of the modeling process. Scenarios of model specification, selection and use of confidence interval methods, and model output analysis are introduced.	graphics;interval arithmetic;iterative method;performance evaluation;resource contention;workstation	Robert F. Gordon;Edward A. MacNair;Peter D. Welch;Kurtiss J. Gordon;James F. Kurose	1986		10.1145/318242.318482	selection;econometrics;simulation;confidence interval;computer science;technical report;data mining;mathematics;specification;statistics;variance reduction	Metrics	28.893974086307715	-15.788466705811198	158268
aa6a2f47ee6e925dff2b2c59ad95e62a98e1c029	data hiding in homomorphically encrypted medical images for verifying their reliability in both encrypted and spatial domains	watermarking;reliability;encryption;biomedical imaging;distortion	In this paper, we propose a new scheme of data hiding of encrypted images for the purpose of verifying the reliability of an image into both encrypted and spatial domains. This scheme couples the Quantization Index Modulation (QIM) and the Paillier cryptosystem. It relies on the insertion into the image, before its encryption, of a predefined watermark, a “pre-watermark”. Message insertion (resp. extraction) is conducted into (resp. from) the encrypted image using a modified version of QIM. It is the impact of this insertion process onto the “pre-watermark” that gives access to the message in the spatial domain, i.e. after the image has been decrypted. With our scheme, encryption/decryption processes are completely independent from message embedding/extraction. One does not need to know the encryption/decryption key for hiding a message into the encrypted image. Experiments conducted on ultrasound medical images show that the image distortion is very low while offering a high capacity that can support different watermarking based security objectives.	authentication;body dysmorphic disorders;clinical act of insertion;cryptosystem;digital watermarking;distortion;embedding;encryption;image compression;insertion mutation;jpeg;key (cryptography);lossy compression;modulation;need to know;verification and validation;verifying specimen;watermark (data file)	Dalel Bouslimi;Reda Bellafqira;Gouenou Coatrieux	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7591237	medical imaging;radiology;distortion;telecommunications;digital watermarking;computer science;theoretical computer science;reliability;mathematics;filesystem-level encryption;on-the-fly encryption;internet privacy;computer security;encryption;statistics	Vision	38.53047811392647	-10.603357035253364	158269
7baf8c074e8a4e3cedacb5b61f2f47c530f4ef7d	uncertainty analysis based on sensitivities generated using automatic differentiation	confidence limit;uncertainty analysis;automatic differentiation;mathematical model;radiation transport	The objective is to determine confidence limits for the outputs of a mathematical model of a physical system that consists of many interacting computer codes. Each code has many modules that receive inputs, write outputs, and depend on parameters. Several of the outputs of the system of codes can be compared to sensor measurements. The outputs of the system are uncertain because the inputs and parameters of the system are uncertain. The method uses sensitivities to propagate uncertainties from inputs to outputs through the complex chain of modules. Furthermore, the method consistently combines sensor measurements with model outputs to simultaneously obtain best estimates for model parameters and reduce uncertainties in model outputs. The method was applied to a test case where ADIFOR2 was used to calculate sensitivities for the radiation transport code MODTRAN.	automatic differentiation	Jacob Barhen;David B. Reister	2003		10.1007/3-540-44843-8_8	automatic differentiation;econometrics;confidence interval;uncertainty analysis;computer science;mathematical model;control theory;statistics	NLP	27.848207316262638	-16.753956058529344	158427
a71fbb044d09f61e0a98856a9e42e05caa04a21a	numerical analysis of the vertex models for simulating grain boundary networks	93e03;grain growth;vertex model;topological transitions;37m05;35q80;polycrystalline materials	Abstract. Polycrystalline materials undergoing coarsening can be represented as evolving networks of grain boundaries, whose statistical characteristics determine macroscopic materials properties. The process of formation of various statistical distributions is extremely complex and is strongly influenced by topological changes in the network. This work is an attempt to elucidate the role of these changes by conducting a thorough numerical investigation of one of the simplest types of grain growth simulation models, called vertex models. While having obvious limitations in terms of its ability to represent realistic systems, the vertex model enables full control over topological transitions and retains essential geometric features of the network. We formulate a self-consistent vertex model and investigate the role of microscopic parameters on the mesoscale network behavior. This study sheds light onto several important questions, such as how statistics are affected by the choice of temporal and spatial resolution and rules governing topological changes. Statistical analysis of the data produced by the simulation is performed for both isotropic and anisotropic grain boundary energy.	algorithm;evolving networks;ibm notes;mesoscopic physics;numerical analysis;robustness (computer science);semiconductor industry;simulation;the times;vertex model	C. E. Torres;M. Emelianenko;Dmitry Golovaty;David Kinderlehrer;Shlomo Ta'asan	2015	SIAM Journal of Applied Mathematics	10.1137/140999232	grain growth;combinatorics;discrete mathematics;vertex model;mathematics;geometry;algebra	Graphics	38.80026589451833	-22.530984580059403	158440
30f53b369af72414334066f88a8cfab851a3c0f5	direct comparison of bibliometric models	bibliometrie;circulacion documento;ciencia informacion;document circulation;scientometrics;comparative analysis;modele mathematique;loi probabilite;ley probabilidad;information science;library circulation;estudio comparativo;biblioteconomia;bibliometria;variance analysis;bibliotheconomie;regression statistics;modelo matematico;recherche developpement;analisis varianza;circulation document;etude comparative;probability law;chi square;statistical distributions;research and development;scientometria;investigacion desarrollo;productivite auteur;theory;teoria;comparative study;scientometrie;estimacion parametro;mathematical model;analysis of variance;librarianship;bibliometrics;statistical inference;ejemplo;evaluation;evaluacion;productividad autor;parameter estimation;estimation parametre;science information;example;models;author productivity;analyse variance;theorie;exemple	Abstract   This study describes a technique for statistically comparing bibliometric models, and illustrates its use with three different examples. The technique is based on the idea of comparing full and restricted models as developed in analysis of variance, regression, and log-linear models. In bibliometrics, any two models where one is a special case of the other can be thought of as a full model and a restricted model. One can use the likelihood-ratio chi-square statistic, which has gained acceptance with log-linear models, as a test statistic to directly compare the full model and the restricted model. The first two examples involved Lotka's law. In the first example we investigated the feasibility of applying a single set of global parameter values to eight different author productivity distributions drawn from two different disciplines. In the second example we looked at whether or not a finite maximum productivity level was necessary as an additional parameter in Lotka-type models of author productivity. The final example compared three different forms of a model of library circulation frequencies.		Mark T. Kinnucan;Dietmar Wolfram	1990	Inf. Process. Manage.	10.1016/0306-4573(90)90051-3	econometrics;analysis of variance;information science;statistics	DB	34.966188766638474	-20.668413469533263	158524
3337a487d3d684435cf215133219af77ceac96be	flexible visual cryptography scheme without distortion	non distortion;secret sharing;flexible;visual cryptography	For visual cryptography scheme (VCS), normally, the size of the recovered secret image will be expanded by m(≥1) times of the original secret image. In most cases, m is not a square number, hence the recovered secret image will be distorted. Sometimes, m is too large that will bring much inconvenience to the participants to carry the share images. In this paper, we propose a visual cryptography scheme which simulated the principle of fountains. The proposed scheme has two advantages: non-distortion and flexible (with respect to the pixel expansion). Furthermore, the proposed scheme can be applied to any VCS that is under the pixel by pixel encryption model, such as VCS for general access structure, color VCS and extended VCS, and our VCS does not restrict to any specific underlying operation. Compared with other non-distortion schemes, the proposed scheme is more general and simpler, real flexible and has competitive visual quality for the recovered secret image.	distortion;visual cryptography	Feng Liu;Teng Guo;Chuan Kun Wu;Ching-Nung Yang	2011		10.1007/978-3-642-32205-1_18	arithmetic;computer science;theoretical computer science;mathematics;secret sharing;computer security;statistics	ML	38.822144114420865	-10.408677430782445	158907
42ca1b38ce2047d6e6627c2ee1b1fe3f912f2743	computation of the arl for cusum-s	distributed data;ji cuadrado;fredholm integral equation of the second kind;grado libertad;integral equation;62p20;analisis datos;normal distribution;variable aleatoire;fonction repartition;methode collocation;degree of freedom;variable aleatoria;estimacion promedio;exact results;metodo colocacion;curva gauss;khi deux;integral;moyenne;methode cusum;chi square;funcion distribucion;data analysis;cusum method;control chart;distribution function;45xx;average run length;control charts;metodo cusum;promedio;62j10;statistical computation;integrale;calculo estadistico;random variable;equation integrale;monitoring variance;loi normale;collocation;analyse donnee;average;ecuacion integral;calcul statistique;mean estimation;collocation method;estimation moyenne;60e05;freedom degree;erlang distribution;gaussian distribution;variance;variancia;degre liberte;fredholm integral equation	Contrary to CUSUM schemes for monitoring the mean of normally distributed random variables, there is a lack of accurate computation of the average run length (ARL) for CUSUM schemes based on the sample variance S^2, which are of importance for variance monitoring. Some very accurate methods will be suggested. Evaluating CUSUM charts based on S^2 for normal data leads to, naturally, the chi-squared distribution. Then, in the case of even degrees of freedom exact results for Erlang distributed data are employed. For odd degrees piecewise collocation methods are applied for solving the ARL integral equation. Thus, with these methods the ARL for CUSUM-S^2 schemes can be determined with high precision.	computation	Sven Knoth	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2005.09.015	normal distribution;econometrics;control chart;calculus;mathematics;statistics	ML	33.480850086281265	-21.611806372693337	158915
eac945d0250630041f3a6b052a0548046ef7c6d7	palmhash code vs. palmphasor code	palmhash code;multi orientation score level fusion;2d cancelable palmprint code;perpendicular orientation transposition;palmphasor code	The existing palmprint protection schemes without verification degrading do not strictly meet noninvertible requirement; while non-invertible palmprint protection schemes do not meet verification performance requirement in stolen-token scenario. PalmHash Code and PalmPhasor Code, as two cancelable palmprint coding schemes, are proposed in this paper to balance the conflict between cancelable palmprint coding frameworks are extended from one dimension to two dimensions. Besides, two measures, perpendicular orientation transposition and multi-orientation score level fusion, are employed to further improve the performance of two dimensional (2D) cancelable palmprint codes. PalmHash Code and PalmPhasor Code are compared extensively in this paper. The experimental results and analysis confirm that 2D PalmHash Code and 2D PalmPhasor Code enhanced by the two measures are effective and secure even in stolen-token scenario. & 2012 Elsevier B.V. All rights reserved.	code;fingerprint	Lu Leng;Jiashu Zhang	2013	Neurocomputing	10.1016/j.neucom.2012.08.028	theoretical computer science;computer security	SE	36.195178417147524	-10.59710791669634	158962
c924308a2279697c1c026d40f62e5c1d630326c6	remaining useful life estimation based on stochastic deterioration models: a comparative study	wiener process;modelizacion;proceso wiener;analisis componente principal;fiabilidad;reliability;durabilite;processus wiener;stochastic process;analisis datos;securite;maintenance;availability;disponibilidad;wiener process with drift;durabilidad;tiempo vida;deterioration modelling;probabilistic approach;prognostic;modelisation;data analysis;lifetime;remaining useful life time estimation;durability;enfoque probabilista;approche probabiliste;principal component analysis;fiabilite;safety;processus stochastique;analyse composante principale;mantenimiento;analyse donnee;stochastic model;duree vie;proceso estocastico;seguridad;modeling;disponibilite;modelo estocastico;modele stochastique	Prognostic of system lifetime is a basic requirement for condition-based maintenance in many application domains where safety, reliability, and availability are considered of first importance. This paper presents a probabilistic method for prognostic applied to the 2008 PHM Conference Challenge data. A stochastic process (Wiener process) combined with a data analysis method (Principal Component Analysis) is proposed to model the deterioration of the components and to estimate the RUL on a case study. The advantages of our probabilistic approach are pointed out and a comparison with existing results on the same data is made.		Khanh Le Son;Mitra Fouladirad;Anne Barros;Eric Levrat;Benoît Iung	2013	Rel. Eng. & Sys. Safety	10.1016/j.ress.2012.11.022	stochastic process;availability;econometrics;systems modeling;wiener process;engineering;stochastic modelling;durability;reliability;mathematics;data analysis;statistics;principal component analysis	EDA	32.27423356773966	-20.000530384254393	159017
e3398bd602160e1dddf5dabab48ed21b16d7ce4b	nonparametric linear tests with multiple events	test hypothese;survival function;theorie echantillonnage;metodo estadistico;teoria muestreo;analyse multivariable;selection problem;vector distribution;test multiple;test statistique;problema seleccion;statistical simulation;comparaison par paire;covariancia;theorie approximation;62h15;methode parametrique;multivariate analysis;analisis datos;test lineaire;martingale;fonction repartition;non parametric test;metodo parametrico;test hipotesis;loi conjointe;linear test;test estadistico;62e17;parametric method;estimation non parametrique;sobrevivencia;statistical test;donnee censuree;multiple test;matrice covariance;covariance;estadistica rango;statistical method;proportional hazard model;matriz covariancia;ecuesta estadistica;distribucion estadistica;loi asymptotique;statistical regression;funcion sobrevivencia;62j15;discriminant analysis;analyse discriminante;approximation theory;non parametric estimation;62f07;funcion distribucion;sample survey;data analysis;analisis discriminante;distribution function;seno patologia;62n02;simulacion estadistica;fonction survie;censored data;pathologie du sein;consistent estimator;distribution statistique;comparacion por pares;62g10;marginal distribution;62h30;methode statistique;simulation statistique;regresion estadistica;prueba no parametrica;comparacion multiple;statistical computation;62f03;calculo estadistico;comparaison multiple;62d05;62n99;test non parametrique;ley conjunta;paired comparison;ley marginal;survie;rank statistic;simulation study;analisis multivariable;analyse donnee;statistique rang;loi vectorielle;calcul statistique;62nxx;multiple hypotheses testing;logrank test;estimacion no parametrica;estimation statistique;survival;test log rang;multiple comparison;regression statistique;estimacion estadistica;60e05;statistical estimation;breast disease;breast cancer;statistical distribution;sondage statistique;loi marginale;joint distribution;covariance matrix;sampling theory;probleme selection;hypothesis test	A generalization of the nonparametric linear rank statistics is presented to handle the twogroup comparison with multiple events. For a sample divided into two groups, in which each subject may experience at least two distinct failures, the logrank tests are extended to test the null hypothesis that the vector of the marginal survival distributions of the first group equals that of the second group. Two cases are distinguished depending on whether the null hypothesis does or does not imply the equality of the joint survival functions. In both cases, under the null hypothesis, the asymptotic joint distribution of the vector of the marginal statistics is shown to be Gaussian with covariance matrix consistently estimated using martingale properties. These theoretical results are illustrated by a simulation study and an application on the German Breast Cancer data. An extension tomultiple hypotheses testing in multivariate proportional hazards models is also developed. © 2009 Elsevier B.V. All rights reserved.	marginal model;proportional hazards model;simulation	Claire Pinçon;Odile Pons	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2009.05.022	null distribution;econometrics;statistical hypothesis testing;p-value;calculus;mathematics;size;linear discriminant analysis;null hypothesis;statistics	ML	32.66890796161417	-22.67869833576857	159138
c7c0e6f8e139226cef8eff3a0e13a856a3c6c93d	reproducibility probability estimation and rp-testing for some nonparametric tests	kendall test;binomial test;asymptotic power approximation;reproducibility of tests outcomes;stability of test outcomes;wilcoxon signed rank test;sign test	Proof. To prove point 1, consider a realization (x1, ..., xn) of Xn such that ∑l=1 xl = s. The number of all the possible re-samples (xi 1, ..., x i n) that can be drawn with replacement from (x1, ..., xn) such that ∑l=1 x i l = k is given by ( n k)s k(n− s)n−k. Consequently, the number of possible re-samples with ∑l=1 x i l ≥ (ncα + 1) (i.e. the number of re-samples leading to a rejection of H0 using the exact test Ψα) is given by ∑k=ncα+1 ( n k)s k(n − s)n−k. Then, the value of π̂e associated to a realization of (x1, ..., xn) with ∑l=1 xl = s is given by π̂ PI e (s) = ∑ n k=ncα+1 ( n k) ( s n )k (1− s n )n−k = 1− B (ncα; n, s n ) = 1− B (ncα; n, p̂) and P(π̂PI e = π̂PI e (s)) = (s)p(1− p)n−s. This proves points 1 and 2. Concerning point 3, note that the value of the estimators π̂PI e corresponding to the sample realizations leading to the critical value p̂ = cα is π̂PI e (ncα) = ∑ n k=ncα+1 ( n k) ( ncα n )k (1− ncα n )n−k = 1− B(ncα; n, cα). Since ncα is an integer, the median of the binomial distribution with parameters n and cα coincides with ncα. Consequently, π̂PI e (ncα) ≤ 1/2. Analogously, the value of π̂PI e corresponding to ncα + 1 is given by π̂PI e (ncα + 1) = ∑ n k=ncα+1 ( n k) ( ncα+1 n )k ( 1− ncα+1 n )n−k = 1− B(ncα; n, (ncα + 1)/n). Again, since ncα + 1 is an integer, it coincides with the median of the binomial distribution with parameters n and (ncα + 1)/n. Then 1− B(ncα; n, (ncα + 1)/n) > 1/2, which coincides with π̂PI e (ncα + 1) > 1/2. This demonstrates that, if the null hypotheses is rejected (accepted) by the classical exact test (or by the test based on the parametric exact RP-estimator), also the RP-testing rule defined on the basis of π̂PI e reject (accept) H0. The converse implication is straightforward since a binomial random variable is strictly increasing in pt with respect to the usual stochastic ordering. Then, the RP-testing rule based on π̂PI e is equivalent to the exact one. The proof for Lemma 3.2 is analogous.	rp (complexity);rejection sampling	Lucio De Capitani;Daniele De Martini	2016	Entropy	10.3390/e18040142	econometrics;discrete mathematics;sign test;wilcoxon signed-rank test;mathematics;binomial test;statistics	Theory	29.99237939244313	-21.200176088648135	159141
7403bdbe3f0fc4bd7d614b8fa3de28b4a959659b	fitness modeling with markov networks	evolutionary computation;markov random fields estimation of distribution algorithms graphical models;markov random fields;markov networks complexity global fitness models fitness modeling capability fitness prediction correlation walsh functions modeling quality discrete functions expensive fitness function hybrid guided evolutionary operators evolutionary algorithm efficiency evolutionary computation community;journal article;walsh functions evolutionary computation markov processes;walsh functions;graphical models;markov random fields computational modeling sociology statistics mathematical model probabilistic logic probability distribution;markov processes;estimation of distribution algorithms	Fitness modeling has received growing interest from the evolutionary computation community in recent years. With a fitness model, one can improve evolutionary algorithm efficiency by directly sampling new solutions, developing hybrid guided evolutionary operators or using the model as a surrogate for an expensive fitness function. This paper addresses several issues on fitness modeling of discrete functions, particularly how modeling quality and efficiency can be improved. We define the Markov network fitness model in terms of Walsh functions. We explore the relationship between the Markov network fitness model and fitness in a number of discrete problems, showing how the parameters of the fitness model can identify qualitative features of the fitness function. We define the fitness prediction correlation, a metric to measure fitness modeling capability of local and global fitness models. We use this metric to investigate the effects of population size and selection on the tradeoff between model quality and complexity for the Markov network fitness model.	algorithmic efficiency;evolutionary algorithm;evolutionary computation;fitness function;fitness model (network theory);hadamard transform;markov chain;markov random field;sampling (signal processing)	Alexander E. I. Brownlee;John A. W. McCall;Qingfu Zhang	2013	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2013.2281538	mathematical optimization;interactive evolutionary computation;estimation of distribution algorithm;fitness proportionate selection;computer science;machine learning;evolutionary algorithm;mathematics;graphical model;markov process;markov model;fitness approximation;evolutionary robotics;fitness function;walsh function;statistics;evolutionary computation;variable-order markov model	Metrics	28.606091683697286	-10.735106645907933	159241
2ebc4fa85bb1c4922ee6faaadae9dff00bb39b26	a cumulative evidential stopping criterion for multiobjective optimization evolutionary algorithms	high dimensionality;numerical method;efficient algorithm;soft computing;rule based;kalman filter;moeas;stopping criterion;multiobjective optimization evolutionary algorithms;bayesian estimator;multiobjective optimization;evolutionary algorithm;cumulant;optimal algorithm	In this work we present a novel and efficient algorithm independent stopping criterion, called the MGBM criterion, suitable for Multiobjective Optimization Evolutionary Algorithms (MOEAs). The criterion, after each iteration of the optimization algorithm, gathers evidence of the improvement of the solutions obtained so far. A global (execution wise) evidence accumulation process inspired by recursive Bayesian estimation decides when the optimization should be stopped. Evidenceis collected using a novel relative improvement measure constructed on top of the Pareto dominance relations. The evidence gathered after each iteration is accumulated and updated following a rule based on a simplified version of a discrete Kalman filter. Our criterion is particularly useful in complex and/or high-dimensional problems where the traditional procedure of stopping after a predefined amount of iterations cannot be used and the waste of computational resources can induceto a detriment of the quality of the results. Although the criterion discussed here is meant for MOEAs,it can be easily adapted to other soft computing or numerical methods by substituting the local improvement metric with a suitable one.	early stopping;evolutionary algorithm;mathematical optimization;multi-objective optimization	Luis Martí;Jesús Caja García;Antonio Berlanga;José M. Molina López	2007		10.1145/1274000.1274053	kalman filter;mathematical optimization;numerical analysis;computer science;artificial intelligence;multi-objective optimization;machine learning;evolutionary algorithm;mathematics;soft computing;statistics;cumulant	AI	28.7070645011136	-13.080348537016645	159720
a9bb9f603515f012a0c9aa94292d508861cd13f2	testing fuzzy hypotheses with crisp data	test hypothese;probabilidad error;sample size;test statistique;fuzzy set;tamano muestra;test hipotesis;test estadistico;statistical test;taille echantillon;conjunto difuso;ensemble flou;fuzzy hypotheses;curva gauss;valor critico;gaus tests;loi normale;error probability;valeur critique;critical value;gaussian distribution;probabilite erreur;hypothesis test	Abstract   In this paper an approach is presented how statistical tests originally constructed to examine crisp hypotheses can also be applied to fuzzily formulated hypotheses. In particular, criterions α and β are proposed generalizing the probabilities of the errors of type I and type II, respectively. The general approach is applied to one- and two-sided Gaus tests. Here, diagrams are given to determine the critical values in the most popular cases of  α  = 0.01 and  α  = 0.05. If, in addition, the value of β is fixed in advance the sample size of a one- or two-sided Gaus test can be obtained using supplementary graphs.		Bernhard F. Arnold	1998	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00258-8	econometrics;statistical hypothesis testing;calculus;mathematics;statistics	Robotics	31.518419949229976	-21.269378304647027	159945
a6e8cc50c21b1e0bacb6faf595a7f7a0ba5d20ed	evolution of recurrent neural controllers using an extended parallel genetic algorithm	parallel genetic algorithm;mobile robot;multi population genetic algorithm;intelligent agent;evolutionary strategy;recurrent neural network;evolutionary algorithm;sequential task;population genetics	Autonomous intelligent agents often must complete non-Markovian sequential tasks, which require complex recurrent neural controllers. In order to improve the convergence of evolution and reduce the computation time, this paper proposes application of an extended evolutionary algorithm. We implemented an extended multi-population genetic algorithm (EMPGA), where subpopulations apply different evolutionary strategies. In addition, subpopulations compete and cooperate among each other. Results show that EMPGA outperformed single population genetic algorithm (SPGA) by efficiently distributing the number of individuals among subpopulations as different strategies became successful during the course of evolution. In addition, the comparison with other multi-population GA shows that competition between subpopulations improved the quality of solution. The evolved neural controllers were also tested in the real hardware of Cyber Rodent robot. ©	computation;evolution;evolutionary algorithm;genetic algorithm;intelligent agent;pin grid array;recurrent neural network;simulation;software release life cycle;time complexity	Genci Capi;Kenji Doya	2005	Robotics and Autonomous Systems	10.1016/j.robot.2005.04.003	mobile robot;simulation;cultural algorithm;computer science;artificial intelligence;recurrent neural network;machine learning;evolutionary algorithm;genetic representation;evolution strategy;evolutionary robotics;population genetics;intelligent agent;population-based incremental learning	AI	25.05940131074971	-11.778594157353533	160037
b93c53117f5c98da6fc7e0559c2f22d73dab03ee	generalized confidence intervals for process capability indices	process capability indices;lower specification limit;generalized pivotal quantity;process capability index;confidence interval;generalized confidence interval;target value;upper specification limit	Abstract#R##N##R##N#The concept of generalized confidence intervals is used to derive lower confidence limits for some of the commonly used process capability indices. For the cases where approximate lower confidence limits are already available, numerical comparisons are made among the available approximations and the generalized lower confidence limit. The numerical results indicate that the generalized confidence interval does provide coverage probabilities very close to the nominal confidence level. Two examples are given to illustrate the results. Copyright © 2006 John Wiley & Sons, Ltd.		Thomas Mathew;G. Sebastian;K. M. Kurian	2007	Quality and Reliability Eng. Int.	10.1002/qre.828	reliability engineering;econometrics;confidence interval;process capability index;confidence distribution;operations management;confidence region;mathematics;tolerance interval;confidence and prediction bands;cdf-based nonparametric confidence interval;robust confidence intervals;statistics	SE	29.497089783180808	-19.581832954147703	160040
8b2ea1043e5f419db7d3d034c0c47d4b80b21d39	an empirical assessment of ranking accuracy in ranked set sampling	auxiliary variable;estimation proportion;sample selection;analisis datos;echantillonnage;selection variable;estadistica rango;ranked set sampling;logistic regression;sampling;variable selection;62f07;proportion estimation;data analysis;statistical computation;calculo estadistico;concomitant ranking;estimacion proporcion;rank statistic;simulation study;analyse donnee;statistique rang;calcul statistique;muestreo;simple random sampling	Ranked set sampling (RSS) involves ranking of potential sampling units on the variable of interest using judgment or an auxiliary variable to aid in sample selection. Its effectiveness depends on the success in this ranking. We provide an empirical assessment of RSS ranking accuracy in estimation of a population proportion. © 2006 Elsevier B.V. All rights reserved.	rss;sampling (signal processing)	Haiying Chen;Elizabeth A. Stasny;Douglas A. Wolfe	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2006.07.018	sampling;econometrics;simple random sample;pattern recognition;mathematics;logistic regression;data analysis;feature selection;statistics	AI	32.27162630840299	-22.404095917552343	160057
8286887dbb7dc06459a2465f7fee8aa1c0baf71e	inflation rate dynamics convergence within the euro	inflation rate synchronization;wavelet dissimilarity matrix;multidimensional scaling;continuous wavelet transform;european union integration	In this paper we use wavelet analysis to address the problem of inflation dynamics convergence in the Euro area. The study is conducted for the 11 countries that first joined the Euro. We use a dissimilarity measure derived in the time-frequency space to estimate the degree of inflation synchronization among these countries. With this measure, we fill a dissimilarity matrix which, with multidimensional scaling, is then used to plot the different countries in a plane. Our results indicate that the degree of inflation cycles synchronization is much stronger before than after the Euro. For example, while before the Euro, Portuguese inflation is synchronized with the inflation in six other countries, after the Euro adoption any statistical evidence of synchronization disappears.	distance matrix;image scaling;multidimensional scaling;synchronization (computer science);wavelet	Maria Joana Soares;Luís Aguiar-Conraria	2014		10.1007/978-3-319-09144-0_10	econometrics;multidimensional scaling;continuous wavelet transform;computer science	Metrics	34.7896590935517	-19.688608227930263	160212
a4f1ab4207f71239ed8700cd62aeec26095bd4d0	bayesian estimation of the parameters in two non-independent component series system with dependent time failure rate	sistema lineal;bayes estimation;ley uniforme;system reliability;shock model;analisis numerico;reliability;fiabilite systeme;exponential distribution;matematicas aplicadas;mathematiques appliquees;bayesian approach;porcentaje falla;reliability function;prior distribution;mean time to failure;taux defaillance;linear system;analyse numerique;fiabilidad sistema;bayes estimator;estimacion bayes;numerical analysis;uniform prior distribution;estimacion parametro;bayesian estimator;failure rate;parameter estimation;estimation parametre;bayes analysis;systeme lineaire;applied mathematics;loi uniforme;independent component;estimation bayes;uniform distribution	In this paper, we have presented a Bayesian approach for estimating the parameters included in a two non-independent and non-identical component series system with linear failure rate subjected to three sources of fatal shocks. Also the Bayes estimators for the values of reliability functions of the system components are deduced. Estimations of the system s mean time to failure and the system s failure rate are derived. Uniform prior distributions are assumed for the unknown parameters to be estimated. Special cases of the present results are obtained to show how the current results generalize the results in other literatures. 2003 Elsevier Inc. All rights reserved.	failure rate;mean time between failures	Awad I. El-Gohary	2004	Applied Mathematics and Computation	10.1016/S0096-3003(03)00688-X	exponential distribution;econometrics;mean time between failures;prior probability;bayes estimator;numerical analysis;bayesian probability;calculus;failure rate;reliability;mathematics;uniform distribution;linear system;estimation theory;statistics	ML	31.319929950443143	-19.899590785239553	160292
5c719abf5eecb9fd0f7b3f8e686ec7a2973cb189	analysis of attractor distances in random boolean networks	experimental analysis;distance measure;random boolean networks;chaotic dynamics;genetic regulatory network;quantitative method;evolutionary computing	We study the properties of the distance between attractors in Random Boolean Networks, a prominent model of genetic regulatory networks. We define three distance measures, upon which attractor distance matrices are constructed and their main statistic parameters are computed. The experimental analysis shows that ordered networks have a very clustered set of attractors, while chaotic networks’ attractors are scattered; critical networks show, instead, a pattern with characteristics of both ordered and chaotic networks.	boolean network	Andrea Roli;Stefano Benedettini;Roberto Serra;Marco Villani	2010		10.3233/978-1-60750-692-8-201	boolean network;quantitative research;computer science;machine learning;experimental analysis of behavior;evolutionary computation	ML	27.184776549838897	-10.027171182993756	160513
8a0a30d319f77cf84da4633527ea1ecb8334560f	efficient multiple importance sampling estimators	standards;computational complexity;partial deterministic mixture mis estimator multiple importance sampling estimator variance reduction computational complexity classical mixture approach weight calculation computer simulation;proposals monte carlo methods standards computational complexity sociology computational efficiency;computational efficiency;proposals;signal sampling computational complexity estimation theory;sociology;monte carlo methods;multiple importance sampling adaptive importance sampling deterministic mixture monte carlo methods	Multiple importance sampling (MIS) methods use a set of proposal distributions from which samples are drawn. Each sample is then assigned an importance weight that can be obtained according to different strategies. This work is motivated by the trade-off between variance reduction and computational complexity of the different approaches (classical vs. deterministic mixture) available for the weight calculation. A new method that achieves an efficient compromise between both factors is introduced in this letter. It is based on forming a partition of the set of proposal distributions and computing the weights accordingly. Computer simulations show the excellent performance of the associated partial deterministic mixture MIS estimator.	computer simulation;gibbs sampling;importance sampling;sampling (signal processing);variance reduction	Victor Elvira;Luca Martino;David Luengo;Mónica F. Bugallo	2015	IEEE Signal Processing Letters	10.1109/LSP.2015.2432078	econometrics;mathematical optimization;importance sampling;computational resource;mathematics;umbrella sampling;computational complexity theory;monte carlo integration;asymptotic computational complexity;statistics;monte carlo method	ML	30.84551259693888	-15.468746377020553	160924
b5f36aa7d888073744cd70849aa395440a7e36c6	an adaptive shiryaev-roberts procedure for monitoring dispersion	spc;likelihood ratio;journal;zero state;quality control;steady state;exponentially weighted moving average	In this paper, the Shiryaev-Roberts (SR) procedure is examined and compared with the change point CUSUM (CPC) procedure for monitoring the dispersion of a normal process. It will be shown that the SR chart performs better than the CPC chart for the pre-specified dispersion shift. In practice, when the magnitude of a future dispersion shift is unknown, it is always desired to design a control chart to perform reasonably well over a range of shifts rather than to optimize the performance at detecting a particular level of shifts. Compared with SR based on a pre-specified dispersion shift, an adaptive SR (ASR) chart that continually adjusts its form to be efficient for signaling a smoothing exponentially weighted moving average (EWMA) statistic in deviation from its target value is proposed in this paper. It can be easily implemented and numerical results show that it balances protection against a broad range of shift sizes.		Jiujun Zhang;Changliang Zou;Zhaojun Wang	2011	Computers & Industrial Engineering	10.1016/j.cie.2011.07.006	quality control;zero state response;likelihood-ratio test;engineering;operations management;control theory;steady state;statistical process control;statistics	SE	28.111178503035372	-19.21103576720683	160946
4060df4f8cb5891a3abfc9e6aebb54cf55135217	the jeffreys–lindley paradox and discovery criteria in high energy physics	composite;resolution;bayesian;statistical analysis;statistics;stat me;frequentist;physics data an	The Jeffreys–Lindley paradox displays how the use of a $$p$$ p value (or number of standard deviations $$z$$ z ) in a frequentist hypothesis test can lead to an inference that is radically different from that of a Bayesian hypothesis test in the form advocated by Harold Jeffreys in the 1930s and common today. The setting is the test of a well-specified null hypothesis (such as the Standard Model of elementary particle physics, possibly with “nuisance parameters”) versus a composite alternative (such as the Standard Model plus a new force of nature of unknown strength). The $$p$$ p value, as well as the ratio of the likelihood under the null hypothesis to the maximized likelihood under the alternative, can strongly disfavor the null hypothesis, while the Bayesian posterior probability for the null hypothesis can be arbitrarily large. The academic statistics literature contains many impassioned comments on this paradox, yet there is no consensus either on its relevance to scientific communication or on its correct resolution. The paradox is quite relevant to frontier research in high energy physics. This paper is an attempt to explain the situation to both physicists and statisticians, in the hope that further progress can be made.	bayes factor;brainfuck;epr paradox;elementary particle;experiment;heterogeneous element processor;image scaling;interval arithmetic;lucas sequence;moravec's paradox;naive bayes classifier;numerical analysis;relevance;resolution (logic);scientific communication;tom duff	Robert D. Cousins	2014	Synthese	10.1007/s11229-014-0525-z	econometrics;statistical hypothesis testing;p-value;bayes factor;resolution;portmanteau test;frequentist inference;bayesian probability;statistical power;mathematics;statistical significance;composite number;size;null hypothesis;statistics	ML	30.638482320438058	-21.87290015158677	161073
0390bd9b7ed6498fb5aaf68bee2b37aa50968ca4	nonparametric density estimation	curse of dimensionality;kernel density estimation	Data: X1, . . . , Xn iid ∼ P where P is a distribution with density f(x). Aim: Estimation of density f(x) Parametric density estimation: ◦ Fit parametric model {f(x|θ)|θ ∈ Θ} to data Ã parameter estimate θ̂ ◦ Estimate f(x) by f(x|θ̂) ◦ Problem: Choice of suitable model Ã danger of misfits ◦ Complex models (eg mixtures) are difficult to fit Nonparametric density estimation: ◦ Few assumptions (eg density is smooth) ◦ Exploratory tool Example: Velocities of galaxies ◦ Velocities in km/sec of 82 galaxies from 6 well-separated conic sections of an unfilled survey of the Corona Borealis region. ◦ Multimodality is evidence for voids and superclusters in the far universe.	galaxy;kernel density estimation;parametric model	Ricardo Cao	2011		10.1007/978-3-642-04898-2_412	density estimation;multivariate kernel density estimation;nonparametric regression	ML	27.71734244028719	-22.488560591782708	161252
677650b3df5a1f00f8b5d3ece0cc9134a4f9172b	exact distribution under independence of the diagonal section of the empirical copula	60c05;archimedean copula;analisis estadistico;systeme multivariable;distribution mathematics;copulae;matrice diagonale;independence;62h05;distribution mathematiques;statistical analysis;independence test;matriz diagonal;sistema multivariable;analyse statistique;multivariable system;test independance;test independencia;diagonal section;distribucion matematicas;62e15;diagonal matrix	In this paper we analyze some properties of the empirical diagonal and we obtain its exact distribution under independence for the two and three-dimensional cases, but the ideas proposed in this paper can be carried out to higher dimensions. The results obtained are useful in designing a nonparametric test for independence, and therefore giving solution to an open problem proposed by Alsina, Frank and Schweizer [2].		Arturo Erdely;José M. González-Barrios	2008	Kybernetika		distribution;independence;econometrics;calculus;mathematics;diagonal matrix;statistics	ML	34.17367295553444	-21.076935517448153	161292
578fabdaaa407d7a3ba08d19f52a094997878978	sequential parameter optimization for symbolic regression	experimental design;system configuration;sequential parameter optimization;complex system;symbolic regression;parameter optimization;empirical research;design of experiment	Modern Symbolic Regression (SR) engines are complex systems of many components, most of which require some form of parameterization. In this talk, we show how to apply Sequential Parameter Optimization (SPO) as a rigorous method for finding near-optimal parameter settings for SR systems. As modern SR systems often offer alternative operator sets for population initialization, variation, and selection, we also demonstrate how to use modern Design of Experiments (DoE) methods to find problem-specific near-optimal SR system configurations, in addition to near-optimal parameterizations for each selected system component. The experimental design for SR can somehow be tricky, because of interactions in the parameter settings. Methods for handling configurations of parameters which depend on higher-level parameters will be presented. Our exposition is based on a simple framework for statistical sound, reproducible empirical research in SR.	complex systems;design of experiments;experiment;interaction;mathematical optimization;pareto efficiency;symbolic regression	Thomas Bartz-Beielstein;Oliver Flasch;Martin Zaefferer	2012		10.1145/2330784.2330861	econometrics;mathematical optimization;computer science;statistics	ML	26.73521798099181	-15.233935056408685	161519
a1a34665a4daf5a36ce3986624a3283367e1b206	an evaluation of statistical software in the social sciences	bivariate tables;statistical software;regression;social science;pearson correlation;factor analysis;statistical computing;one way analysis of variance;statistical computation;analysis of variance;descriptive statistics	Several hundred college and university computer installations now offer various types of statistical packages for general use. Among those most widely available are OSIRIS, SPSS, BMD, DATA-TEXT, and TSAR. In order to provide users with a basis for selection and use, tests were made for each of these systems, and the results are summarized as to cost and performance.	computer performance;list of statistical packages;spss	William D. Slysz	1974	Commun. ACM	10.1145/355616.364014	pearson product-moment correlation coefficient;econometrics;descriptive statistics;statistical theory;regression;analysis of variance;computer science;data science;factor analysis;one-way analysis of variance;computational statistics;statistics	Graphics	34.64686357657319	-20.50370629613961	161612
978bf13c6786635d9ba25388d8325168dd05402b	entropy-based modeling and simulation of evolution in biological systems	transversality condition;shortest path;modeling and simulation;variational problem;biological systems;thermodynamics;euclidean space;geometric constraints;variational principle;maximum entropy	We report computer-aided modeling and simulation of evolution in biological systems with living organisms as effect of extremum properties of classical statistical entropy of Gibbs-Boltzmann type or its associates, e.g. Tsallis q-entropy. Evolution for animals with multiple organs is considered. A variational problem searches for the maximum entropy subject to the geometric constraint of constant thermodynamic distance in a non-Euclidean space of independent probabilities pi, plus possibly other constraints. Tensor dynamics is found. Some developmental processes progress in a relatively undisturbed way, whereas others may terminate rapidly due to inherent instabilities. For processes with variable number of states the extremum principle provides quantitative eveluation of biological development. The results show that a discrete gradient dynamics (governed by the entropy) can be predicted from variational principles for shortest paths and suitable transversality conditions.	biological system;simulation	Stanislaw Sieniutycz	2007		10.1007/978-3-540-75867-9_5	mathematical optimization;combinatorics;mathematical analysis;binary entropy function;variational principle;maximum entropy probability distribution;principle of maximum entropy;euclidean space;modeling and simulation;mathematics;maximum entropy thermodynamics;configuration entropy;shortest path problem;entropy	Logic	34.311279833867836	-12.30319728577241	161788
01da6276188ca12227210f37c717d6477ed0078c	a use of algorithms for numerical modeling of order statistics	order statistic;numerical model	In this paper a modification of the standard algorithm for the order statistics modeling, tied with the usage of confidence intervals is proposed. A study of applications of the standard algorithm for the order statistics modeling leads us to a conclusion that one of these applications (namely, the modeling of beta-distribution with integer parameters) gives the most effective algorithm for the order statistics modeling. A possibility to use the constructed algorithms in numerical modeling of random variables with polynomial distribution, as well as the beta-distribution with non-integer parameters, is shown.	algorithm;effective method;numerical analysis;polynomial	Anton Voytishek;Alexandr Myasnikov;Leonid Saneev	2008	Monte Carlo Meth. and Appl.	10.1515/mcma.2007.024	econometrics;mathematical optimization;order statistic;mathematics;statistics	ML	30.72536076918094	-20.121851846978632	161903
7f468851d974676cd2acf83bb906c74973b9c598	global sensitivity analysis of environmental models: convergence and validation	convergence;screening;ranking;sensitivity analysis;validation;swat	We address two critical choices in Global Sensitivity Analysis (GSA): the choice of the sample size and of the threshold for the identification of insensitive input factors. Guidance to assist users with those two choices is still insufficient. We aim at filling this gap. Firstly, we define criteria to quantify the convergence of sensitivity indices, of ranking and of screening, based on a bootstrap approach. Secondly, we investigate the screening threshold with a quantitative validation procedure for screening results. We apply the proposed methodologies to three hydrological models with varying complexity utilizing three widely-used GSA methods (RSA, Morris, Sobol’). We demonstrate that convergence of screening and ranking can be reached before sensitivity estimates stabilize. Convergence dynamics appear to be casedependent, which suggests that “fit-for-all” rules for sample sizes should not be used. Other modellers can easily adopt our criteria and procedures for a wide range of GSA methods and cases. © 2016 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).	booting;data validation;global storage architecture;norsk data	Fanny Sarrazin;Francesca Pianosi;Thorsten Wagener	2016	Environmental Modelling and Software	10.1016/j.envsoft.2016.02.005	econometrics;convergence;ranking;computer science;data mining;mathematics;sensitivity analysis;statistics	Vision	27.788333735361636	-20.992353505508845	161930
7216b1e082263a3598724ff951c6e3e76c2949f7	stationary density of stochastic search processes		The optimization of a cost function which has a number of local minima is a relevant subject in many important fields. For instance, the determination of the weights of learning machines depends in general on the solution of global optimization tasks (Haykin, 1999). A feature shared by almost all of the most common deterministic and stochastic algorithms for continuous non – linear optimization is that their performance is strongly affected by their starting conditions. Depending on the algorithm, the correct selection of an initial point or set of points have direct consequences on the efficiency, or even on the possibility to find the global minima. Of course, adequate selection of seeds implies prior knowledge on the structure of the optimization task. In the absence of prior information, a natural choice is to draw seeds from a uniform density defined over the search space. Knowledge on the problem can be gained through the exploration of this space. In this contribution is presented a method to estimate probability densities that describe the asymptotic behavior of general stochastic search processes over continuously differentiable cost functions. The relevance of such densities is that they give a description of the residence times over the different regions of the search space, after an infinitely long exploration. The preferred regions are those which minimize the cost globally, which is reflected in the asymptotic densities. In first instance, the resulting densities can be used to draw populations of points that are consistent with the global properties of the associated optimization tasks. BACKGROUND	algorithm;global optimization;linear programming;loss function;mathematical optimization;maxima and minima;population;relevance;seeds (cellular automaton);simon haykin;stationary process;stochastic optimization	Arturo Berrones;Dexmont Peña;Ricardo Sánchez	2009			stochastic optimization;global optimization;mathematical optimization;gauss–markov process;maxima and minima;local time;mathematics;geometric brownian motion;stationary sequence;continuous-time stochastic process	ML	30.38829915497247	-11.878712618235182	161940
3a53e1a11e808e0d1bc1a8be45e49e1ae2d85c4e	inference for three-parameter 푴-wright distributions with applications		We propose point estimators for the three-parameter (location, scale, and the fractional parameter) variant distributions generated by a Wright function. We also provide uncertainty quantification procedures for the proposed point estimators under certain conditions. The class of densities includes the threeparameter one-sided and the three-parameter symmetric bimodal M -Wright family of distributions. The one-sided family naturally generalizes the Airy and half-normal models. The symmetric class includes the symmetric Airy and normal or Gaussian densities. The proposed interval estimator for the scale parameter outperformed the estimator derived in Cahoy (2012) when the location parameter is zero. We obtain the asymptotic covariance structure for the scale and fractional parameter estimators, which allows estimation of the correlation. The coverage probabilities of the interval estimators slightly depend on the proposed location parameter estimators. For the symmetric case, the sample mean (or median) is favored than the median (or mean) when the fractional parameter is greater (or lesser) than 0.39106 in terms of their asymptotic relative efficiency. The estimation algorithms were tested using synthetic data and were compared with their bootstrap counterparts. The proposed inference procedures were demonstrated on age and height data. 1 ar X iv :1 70 5. 01 21 6v 2 [ st at .M E ] 9 N ov 2 01 7	algorithm;mike lesser;synthetic data;uncertainty quantification	Dexter O. Cahoy;Sharifa Minkabo	2017	MASA	10.3233/MAS-170388		ML	30.49463791127003	-22.424846642328557	161991
8f74e6fcc4b5d8588966ebdd0b49e5659dbcf3e0	scaling up the greedy equivalence search algorithm by constraining the search space of equivalence classes	greedy equivalence search;constrained search;bayesian networks	Greedy Equivalence Search (GES) is nowadays the state of the art algorithm for learning Bayesian networks (BNs) from complete data. However, from a practical point of view, this algorithm may not be fast enough to work in high dimensionality domains. This paper proposes some variants of GES aimed to increase its efficiency. Under faithfulness assumption, the modified algorithms preserve the same theoretical properties as the original one, that is, they recover a perfect map of the target distribution in the large sample limit. Moreover, experimental results confirm that, although they carry out much less computations, BNs learnt by those algorithms have the same quality as those learnt by GES.	asymptotically optimal algorithm;brute-force search;computation;experiment;greedy algorithm;iteration;iterative method;linear algebra;metaheuristic;requirement;search algorithm;shin megami tensei: persona 3;spatial variability;turing completeness	Juan Ignacio Alonso-Barba;Luis de la Ossa;José A. Gámez;Jose Miguel Puerta	2013	Int. J. Approx. Reasoning	10.1016/j.ijar.2012.09.004	greedy randomized adaptive search procedure;mathematical optimization;combinatorics;computer science;machine learning;bayesian network;mathematics;best-first search;statistics	AI	28.176418027840974	-10.8610253899718	162234
fa03cb1640ef974f49354282a47f1418b9b8ab0e	optimal experimental designs for fmri when the model matrix is uncertain	design efficiency;genetic algorithms;information matrix;probabilistic behavior;stimulus frequency	Abstract This study concerns optimal designs for functional magnetic resonance imaging (fMRI) experiments when the model matrix of the statistical model depends on both the selected stimulus sequence (fMRI design), and the subjectu0027s uncertain feedback (e.g. answer) to each mental stimulus (e.g. question) presented to her/him. While practically important, this design issue is challenging. This mainly is because that the information matrix cannot be fully determined at the design stage, making it difficult to evaluate the quality of the selected designs. To tackle this challenging issue, we propose an easy-to-use optimality criterion for evaluating the quality of designs, and an efficient approach for obtaining designs optimizing this criterion. Compared with a previously proposed method, our approach requires a much less computing time to achieve designs with high statistical efficiencies.	cognition;computation (action);design of experiments;efficiency;experiment;magnetic resonance imaging;neuritis, autoimmune, experimental;optimal design;optimality criterion;statistical model;fmri	Ming-Hung Kao;Lin Zhou	2017	NeuroImage	10.1016/j.neuroimage.2017.05.008	optimal design;design of experiments;genetic algorithm;fisher information;matrix (mathematics);optimality criterion;machine learning;mathematical optimization;statistical model;computer science;artificial intelligence	EDA	27.523363013472107	-15.28292036982779	162564
384d17529a4b541b2a9ece982197a24b758bf9bb	small sample properties of ml, cols and dea estimators of frontier models in the presence of heteroscedasticity: a reply to banker, chang, and cooper	small samples;col			Antonio N. Bojanic;Steven B. Caudill;Jon M. Ford	2002	European Journal of Operational Research	10.1016/S0377-2217(01)00025-X	econometrics;mathematics;mathematical economics;mountain pass;statistics	Theory	33.28417208280904	-20.219700475787175	163100
b68af21f9badc22723e2cf29374c6af4d612c087	a sieve bootstrap method for correlation analysis	modelizacion;pulse response;metodo correlacion;bootstrap method;variabilidad;predictive control;wiener hopf equation;estimation theory;metodo monte carlo;bootstrap;nonparametric statistics;methode non parametrique;correlation method;excitation aleatoire;methode monte carlo;analysis of variance equations steady state white noise uncertainty finite impulse response filter predictive models system identification covariance matrix sampling methods;excitacion aleatoria;ecuacion lineal;predictive control bootstrapping correlation methods estimation theory fir filters nonparametric statistics;respuesta impulsion;correlation methods;small samples;identificacion sistema;modelisation;ecuacion wiener hopf;systeme incertain;metodo no parametrico;system identification;impulse response coefficient variance estimation;model uncertainty;reponse impulsion;monte carlo method;fir models;fir models correlation analysis nonparametric sieve bootstrap method impulse response coefficient variance estimation process steady state gain estimation wiener hopf equations;analyse correlation;reduccion variancia;impulse response;non parametric method;bootstrapping;regime permanent;reduction variance;system identification bootstrap correlation analysis impulse response model uncertainty;fir filters;regimen permanente;linear equations;variability;linear equation;monte carlo simulation;sistema incierto;nonparametric sieve bootstrap method;variabilite;modeling;process steady state gain estimation;variance reduction;uncertain system;equation wiener hopf;wiener hopf equations;random excitation;white noise;identification systeme;equation lineaire;analisis correlacion;methode correlation;steady state;correlation analysis	This note presents a nonparametric sieve bootstrap method for estimating the variance of impulse response coefficients and the process steady-state gain determined via correlation analysis. The bootstrap estimates are demonstrated to be better for small samples than the analytical finite sample variance expression for the simplified form (assuming white noise input) of the Wiener-Hopf equations. Monte Carlo simulations demonstrate that solving the linear equations resulting from the Wiener-Hopf equations can result in a variance reduction.	bootstrapping (statistics);coefficient;linear equation;marginal model;monte carlo method;parametric design;quadratic sieve;resampling (statistics);simulation;steady state;variance reduction;white noise	Jonathan R. Webber;Yash P. Gupta	2007	IEEE Transactions on Automatic Control	10.1109/TAC.2007.899079	econometrics;mathematical optimization;mathematics;linear equation;statistics;monte carlo method	Visualization	33.42552814418525	-22.263950502540307	163359
ff2594798c92b8f2fe2d24f8eee9e8e3315e3e72	generalized f-tests for the multivariate normal mean	test hypothese;exact test;proyeccion;test f;theorie echantillonnage;metodo estadistico;teoria muestreo;analyse multivariable;analisis numerico;test multiple;sample size;test statistique;small sample;comparaison par paire;metodo monte carlo;theorie approximation;multivariate analysis;65c05;spherical distribution;small sample size;analisis datos;stochastic method;normal distribution;fonction repartition;multivariate normal;test comparacion;tamano muestra;biometrie;test hipotesis;test hotelling;test estadistico;62e17;biometrics;biometria;statistical test;methode monte carlo;taille echantillon;estimacion promedio;multiple test;monte carlo study;statistical method;ecuesta estadistica;medical science;distribucion estadistica;statistical regression;analyse numerique;62j15;approximation theory;funcion distribucion;sample survey;data analysis;distribution function;numerical analysis;distribution statistique;comparacion por pares;ciencia medica;multiple comparisons;methode statistique;regresion estadistica;projection;monte carlo method;comparacion multiple;test exact;statistical computation;calculo estadistico;comparaison multiple;62d05;paired comparison;methode stochastique;analisis multivariable;analyse donnee;robustness;pequena muestra;principal components;calcul statistique;hotelling test;f test;mean estimation;comparison test;multiple comparison;regression statistique;estimation moyenne;high dimension;60e05;statistical distribution;sondage statistique;62p10;petit echantillon;test comparaison;sampling theory;science medicale;hypothesis test;metodo estocastico	Based on Lauter's [Lauter, J., 1996. Exact t and F tests for analyzing studies with multiple endpoints. Biometrics 52, 964-970] exact t test for biometrical studies related to the multivariate normal mean, we develop a generalized F-test for the multivariate normal mean and extend it to multiple comparison. The proposed generalized F-tests have simple approximate null distributions. A Monte Carlo study and two real examples show that the generalized F-test is at least as good as the optional individual Lauter's test and can improve its performance in some situations where the projection directions for the Lauter's test may not be suitably chosen. The generalized F-test could be superior to individual Lauter's tests and the classical Hotelling T^2-test for the general purpose of testing the multivariate normal mean. It is shown by Monte Carlo studies that the extended generalized F-test outperforms the commonly-used classical test for multiple comparison of normal means in the case of high dimension with small sample sizes.		Jiajuan Liang;Man-Lai Tang	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.10.023	econometrics;statistical hypothesis testing;calculus;mathematics;multiple comparisons problem;statistics	ML	32.62341159205308	-22.253620653319402	163533
043dae906b2519c816c3f493c6df810d837b4f29	a high capacity distortion-free data hiding algorithm for palette image	digital watermarking;watermarking;high capacity data hiding algorithm;data hiding;image coding;medical images;information security;embedded information;color;data encapsulation biomedical imaging pixel watermarking color magnetic resonance imaging laboratories information security internet steganography;stego images;biomedical imaging;host image color redundancy;digital watermarking stego images high capacity data hiding algorithm distortion free data hiding algorithm medical images military images palette image host image color redundancy embedded information;data encapsulation;steganography;internet;redundancy;redundancy data encapsulation watermarking image coding;magnetic resonance imaging;pixel;palette image;distortion free data hiding algorithm;military images	Data hiding techniques have extensive applications and have attracted much attention. However, one common drawback of virtually all current data hiding methods is the fact that the host image is inevitably and permanently distorted by some small amount of noise due to data hiding itself. Although the distortion is often quite small, it may not be acceptable for medical or military images. For these applications, the stego-image cannot have any distortion, a high capacity distortion-free data hiding algorithm for the palette image is proposed in this paper. By utilizing the color redundancy in the host image, the algorithm can embed large amount of information, which usually can reach more than 10% of size of the host image. Meanwhile, it can be guaranteed that no distortion is introduced to the host image during data hiding process.	algorithm;distortion;palette (computing);steganography	Hongmei Liu;Zhang Zhefeng;Jiwu Huang;Huang Xialing;Yun Q. Shi	2003		10.1109/ISCAS.2003.1206124	computer vision;digital watermarking;computer science;electrical engineering;information security;theoretical computer science;magnetic resonance imaging;internet privacy	Graphics	38.907820200392635	-12.85990780089949	163536
70084f2c436d0ade907695629e3c5055440ef1dd	a reevaluation of the adaptive exponentially weighted moving average control chart when parameters are estimated	aewma control chart;bootstrap;shewhart control chart;statistical process control;ewma control chart;estimation effect;standard deviation of average run length	The performance of control charts can be adversely affected when based on parameter estimates instead of known in-control parameters. Several studies have shown that a large number of phase I observations may be needed to achieve the desired in-control statistical performance. However, practitioners use different phase I samples and thus different parameter estimates to construct their control limits. As a consequence, there would be in-control average run length (ARL) variation between different practitioners. This kind of variation is important to consider when studying the performance of control charts with estimated parameters. Most of the previous literature has relied primarily on the expected value of the ARL (AARL) metric in studying the performance of control charts with estimated parameters. Some recent studies, however, considered the standard deviation of the ARL metric to study the performance of control charts. In this paper, the standard deviation of the ARL metric is used to study the in-control and out-of-control performance of the adaptive exponentially weighted moving average (AEWMA) control chart. The performance of the AEWMA chart is then compared with that of the Shewhart X and EWMA control charts. The simulation results show that the AEWMA chart might represent a good solution for practitioners to achieve a reasonable amount of ARL variation from the desired in-control ARL performance. In addition, we apply a bootstrapbased design approach that provides protection against frequent false alarms without deteriorating too much the out-ofcontrol performance. Copyright © 2014 John Wiley & Sons, Ltd.	gantt chart;john d. wiley;run-length encoding;simulation	Aya A. Aly;Nesma A. Saleh;Mahmoud A. Mahmoud;William H. Woodall	2015	Quality and Reliability Eng. Int.	10.1002/qre.1695	ewma chart;reliability engineering;econometrics;control chart;control limits;computer science;operations management;shewhart individuals control chart;u-chart;x-bar chart;statistical process control;statistics	DB	28.179521783930618	-19.213858157198835	163642
eeb08b20e2ae748cfabf715f62828cd669c60a79	comparison of bootstrap methods for missing survey data: a simulation study		Bootstrap technique is used in the estimation of variance of non-linear statistics in case of complex surveys. This technique is gaining popularity for survey data with missing observations. In this paper, bootstrap techniques with missing observations have been compared through a simulation study under different imputation techniques. The technique namely “Proportional Bootstrap Without Replacement (PBWO)” for missing observations has also been compared with the Rescaling Bootstrap Without Replacement (RSBWO) method for complete data set. Further, the efficiency of Proportional Bootstrap With Replacement (PBWR) technique for missing observations has been compared with the standard bootstrap technique for complete data set. An optimum number of bootstrap samples required for the reliable estimation of variance in the case of missing observations has also been obtained.	resampling (statistics);simulation	Tauqueer Ahmad;Randhir Singh;Anil Rai	2006	MASA		imputation (statistics);statistics;survey data collection;bootstrapping (electronics);mathematics	ML	29.19316913968531	-22.584834152522948	163894
1189a1c8ca047be5133cc5d5f9f7994fae6c3842	effects of non-normality on economic and economic statistical designs of -control charts with multiple assignable causes and weibull in-control times		ABSTRACTThe common assumption for designing a control chart is that the quality measurements are normally distributed, although this may not be tenable in some industrial systems. This study investigates the effects of non-normal quality data on economic and economic statistical designs of -control charts with multiple assignable causes and Weibull process failure mechanism. Numerical examples assess the performance of the multiplicity-cause model in three cases of Normal, Burr, and Johnson distributions along with the single-cause model under the same quantities of time and cost. The results reveal that the choice of quality characteristic distribution significantly affects optimal design parameters.	chart;design of experiments	M. Bameni Moghadam;Y. Khadem;Shabnam Fani;Muhammad Adeel Pasha	2018	Communications in Statistics - Simulation and Computation	10.1080/03610918.2017.1335406	econometrics;statistics	ECom	28.322411305523175	-19.050543373206583	164152
0f1afeeb4163802c9a84b0e9d03ee992b8f08826	a statistical reanalysis of the classical rutherford's experiment	62 07;distribucion binomial;ji cuadrado;theorie statistique;statistical moment;metodo estadistico;ley poisson;test statistique;theorie approximation;aplicacion;analisis datos;fonction repartition;rutherford s experiment;test cramer von mises;test estadistico;moment statistique;62e17;simulacion numerica;statistical test;moment type estimators;statistical method;distribucion estadistica;loi binomiale;khi deux;approximation theory;chi square;funcion distribucion;momento estadistico;data analysis;distribution function;distribution statistique;methode statistique;62p35;simulation numerique;62f03;cramer von mises test;chi square test;analyse donnee;cramer von mises tests;binomial distribution;teoria estadistica;point of view;exponential decay;application;loi poisson;60e05;statistical distribution;feller s distribution;decomposition exponentielle;statistical theory;poisson distribution;modified chi squared tests;numerical simulation	Modified chi-squared and some newly developed tests for the Poisson, binomial, and an approximated Feller's distribution are discussed. A reanalysis of the classical Rutherford's experimental data on alpha decay is done. Previous analyses of the data were not correct from the point of view of the theory of statistical testing. Tests used show that the data contradict to both Poisson and binomial distribution and do not contradict to a precise “binomial” approximation of Feller's distribution that takes into account a counter's dead time. This gives a plausible statistically correct confirmation of the well-established exponential law of radioactive decay.	experiment;meteorological reanalysis;statistical model	Vassilly Voinov;E. Voinov	2010	Communications in Statistics - Simulation and Computation	10.1080/03610910903355499	computer simulation;econometrics;chi-square test;binomial distribution;calculus;count data;mathematics;negative binomial distribution;binomial test;statistics;continuity correction	AI	33.605635078095524	-21.890980217591427	164606
e7bb1a1b6362d5dad798bdc7e0cf7564b9aa5128	cooperative updating in the hopfield model	parallel computing;constraint optimization neural networks annealing clustering algorithms equations convergence traveling salesman problems physics energy states boltzmann distribution;n queen problem;graph theory;hopfield model;cooperative updating algorithm;graph bipartitioning;mathematics computing hopfield neural nets boltzmann machines constraint theory parallel processing travelling salesman problems graph theory simulated annealing convergence of numerical methods;convergence;mathematics computing;constraint optimization;neural networks;annealing;travelling salesman problem;convergence of numerical methods;combinatorial optimization problem;hopfield neural nets;indexing terms;simulated annealing;constraint satisfaction;physics;four color problem;annealing algorithm;travelling salesman problems;energy states;boltzmann distribution;parallel computer;constraint theory;clustering algorithms;model updating;constraint satisfaction problem;traveling salesman problems;combinatorial optimization;local minima;boltzmann machines;parallel processing;annealing algorithm hopfield model cooperative updating algorithm convergence boltzmann distribution combinatorial optimization constraint satisfaction problem travelling salesman problem four color problem n queen problem graph bipartitioning neural networks parallel computing;detailed balance;neural network	We propose a new method for updating units in the Hopfield model. With this method two or more units change at the same time, so as to become the lowest energy state among all possible states. Since this updating algorithm is based on the detailed balance equation, convergence to the Boltzmann distribution is guaranteed. If our algorithm is applied to finding the minimum energy in constraint satisfaction and combinatorial optimization problems, then there is a faster convergence than those with the usual algorithm in the neural network. This is shown by experiments with the travelling salesman problem, the four-color problem, the N-queen problem, and the graph bi-partitioning problem. In constraint satisfaction problems, for which earlier neural networks are effective in some cases, our updating scheme works fine. Even though we still encounter the problem of ending up in local minima, our updating scheme has a great advantage compared with the usual updating scheme used in combinatorial optimization problems. Also, we discuss parallel computing using our updating algorithm.	algorithm;artificial neural network;combinatorial optimization;computation (action);constraint satisfaction problem;convergence (action);detailed balance;energy level;experiment;four color theorem;hopfield network;mathematical optimization;maxima and minima;neural network simulation;parallel computing;partition problem;travelling salesman problem;unit	Tomo Munehisa;Masaki Kobayashi;Haruaki Yamazaki	2001	IEEE transactions on neural networks	10.1109/72.950153	mathematical optimization;annealing;index term;convergence;simulated annealing;constraint satisfaction;boltzmann distribution;computer science;energy level;theoretical computer science;four color theorem;machine learning;maxima and minima;detailed balance;mathematics;cluster analysis;travelling salesman problem;constraint satisfaction problem;artificial neural network	ML	27.49749117570159	-12.365286118949221	164874
2dfd3400254a4023bfb7c5ef0ee27d4756a41930	noisy encrypted image correction based on shannon entropy measurement in pixel blocks of very small size		Many techniques have been presented to protect image content confidentiality. The owner of an image encrypts it using a key and transmits the encrypted image across a network. If the recipient is authorized to access the original content of the image, he can reconstruct it losslessly. However, if during the transmission the encrypted image is noised, some parts of the image can not be deciphered. In order to localize and correct these errors, we propose an approach based on the local Shannon entropy measurement. We first analyze this measure as a function of the block-size. We provide then a full description of our blind error localization and removal process. Experimental results show that the proposed approach, based on local entropy, can be used in practice to correct noisy encrypted images, even with blocks of very small size.		Pauline Puteaux;William Puech	2018	2018 26th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2018.8553101		Vision	38.963641107853036	-11.247513670800764	165130
811e9e9ec61046c11b3da606bd8dd9fac1c7fc8c	multivariate dependence concepts through copulas	positively quadrant dependent;copula;affiliation;linear interpolation;affiliated signals;multivariate skew normal distribution	In this paper, multivariate dependence concepts such as affiliation, association and positive lower orthant dependent are studied in terms of copulas. Relationships among these dependent concepts are obtained. An affiliation is a notion of dependence among the elements of a random vector. It has been shown that the affiliation property is preserved using linear interpolation of subcopula. Also our results are applied to the multivariate skew-normal copula. As an application, the dependence concepts used in auction with affiliated signals are discussed. Several examples are given for illustration of the main results.		Zheng Wei;Tonghui Wang;Phuong Anh Nguyen	2015	Int. J. Approx. Reasoning	10.1016/j.ijar.2015.04.004	econometrics;computer science;copula;calculus;mathematics;linear interpolation;copula;statistics	AI	34.17070284105094	-19.738798908429473	165348
26f28661ce4efe5574204950935cf955ff64ecb4	estimation of means of multivariate normal populations with order restriction	metodo estadistico;analyse multivariable;parametre ordre;multivariate analysis;multivariate normal mean order restrict graybill deal estimator isotonic regression;maximum likelihood;multivariate normal;order restrict;restriction ordre;primary;maximum vraisemblance;estimacion promedio;matrice covariance;statistical method;regression multivariable;isotonic regression;matriz covariancia;curva gauss;graybill deal estimator;secondary 62h12;parametro orden;order restriction;maximum likelihood estimate;regression isotonique;methode statistique;covariance matrices;secondary;06axx;loi normale;multivariate normal mean;statistical inference;analisis multivariable;order parameter;mean estimation;estimation statistique;estimation moyenne;estimacion estadistica;statistical estimation;multivariate regression;gaussian distribution;maxima verosimilitud;62h12;covariance matrix;partial order;primary 62f30	Multivariate isotonic regression theory plays a key role in the field of statistical inference under order restriction for vector valued parameters. Two cases of estimating multivariate normal means under order restricted set are considered. One case is that covariance matrices are known, the other one is that covariance matrices are unknown but are restricted by partial order. This paper shows that when covariance matrices are known, the estimator given by this paper always dominates unrestricted maximum likelihood estimator uniformly, and when covariance matrices are unknown, the plug-in estimator dominates unrestricted maximum likelihood estimator under the order restricted set of covariance matrices. The isotonic regression estimators in this paper are the generalizations of plug-in estimators in unitary case.	population	Tie-Feng Ma;Song-Gui Wang	2010	J. Multivariate Analysis	10.1016/j.jmva.2009.10.002	estimation of covariance matrices;econometrics;mathematical optimization;law of total covariance;covariance;mathematics;maximum likelihood;statistics;covariance function	Logic	32.62977691046934	-23.002954110726385	165384
4f4657735c61d065ccd919c6a6b85ef2e7fdbf55	outlier detection and accommodation in general spatial models	general spatial models;outlier accommodation;variance weight model;outliers;score test;mean shift outlier model	This paper studies outlier detection and accommodation in general spatial models including spatial autoregressive models and spatial error model as special cases. Using mean-shift and variance-weight models respectively, test statistics for multiple outliers are derived and the detecting procedures are proposed. In addition, several key diagnostic measures such as standardized residuals and leverage measure are defined in general spatial models. Outlier modified models are proposed to accommodate outliers in the data set. The performance of test statistics, including size and power, are examined via simulation studies. Three real examples are analyzed and the results show that the proposed methodology is useful for identifying and accommodating outliers in general spatial models.	anomaly detection;autoregressive model;mean shift;sensor;simulation;software studies;spatial analysis	Xiaowen Dai;Libin Jin;Anqi Shi;Lei Shi	2016	Statistical Methods and Applications	10.1007/s10260-015-0348-1	econometrics;outlier;score test;pattern recognition;mathematics;statistics	ML	29.5205469495016	-23.911163320844153	165476
38b844dd7c254fa9aa86871de0621ffc6ea2c659	blockwise empirical likelihood for time series of counts	modelo lineal generalizado;autocorrelacion;count data;association statistique;variation journaliere;metodo estadistico;analyse multivariable;finite sample;statistical simulation;62g05;covariance analysis;stochastic process;multivariate analysis;latent process;north america;america del norte;maximum likelihood;amerique du nord;modele lineaire generalise;amerique;62m10;autocorrelation generalized linear model latent process nonstationarity overdispersion regression analysis;62h20;secondary 62e20;primary;nonstationarity;echantillon fini;maximum vraisemblance;variance analysis;convergence asymptotique;estimacion promedio;statistical association;etude methode;statistical method;time series;estudio metodo;surdispersion;statistical regression;62jxx;vraisemblance empirique;convergencia asintotica;canada;asociacion estadistica;simulacion estadistica;nonstationary time series;analisis regresion;analyse serie temporelle;analyse covariance;time series analysis;methode statistique;simulation statistique;analisis variancia;regresion estadistica;secondary;62j10;serie temporelle;processus stochastique;serie temporal;analyse regression;simulation study;general linear model;analisis multivariable;regression analysis;primary 62m10;generalized linear model;asymptotic convergence;method study;variacion diaria;mean estimation;analisis covariancia;estimation statistique;proceso estocastico;america;regression statistique;estimation moyenne;estimacion estadistica;empirical likelihood;asymptotic normal;statistical estimation;overdispersion;maxima verosimilitud;analyse variance;autocorrelation;daily variation;variance;variancia	Time series of counts has a wide variety of applications in real life. Analyzing time series of counts requires accommodations for serial dependence, discreteness, and overdispersion of data. In this paper, we extend blockwise empirical likelihood (Kitamura, 1997) to the analysis of time series of counts under a regression setting. In particular, our contribution is the extension of Kitamura’s (1997) method to the analysis of nonstationary time series. Serial dependence among observations is treated nonparametrically using blocking technique; and overdispersion in count data is accommodated by the specification of variance-mean relationship. We establish consistency and asymptotic normality of the maximum blockwise empirical likelihood estimator. Simulation studies show that our method has a good finite sample performance. The method is also illustrated by analyzing two real data sets: monthly counts of poliomyelitis cases in the U.S.A. and daily counts of non-accidental deaths in Toronto, Canada.	autocorrelation;blocking (computing);count data;error-tolerant design;real life;simulation;time series	Rongning Wu;Jiguo Cao	2011	J. Multivariate Analysis	10.1016/j.jmva.2010.11.008	stochastic process;econometrics;calculus;time series;mathematics;regression analysis;statistics	ML	33.42996333988722	-22.263976947618197	165769
81edbf85f1ae0869f3b7d05217aa79c15b171029	modeling and inversion of net ecological exchange data using an ito stochastic differential equation approach	65dxx;metodo cuadrado menor;ecuacion estocastica;equation differentielle;stochastic equation;methode moindre carre;condition initiale;analisis numerico;integral equation;metodo monte carlo;stochastic process;matematicas aplicadas;probability density;65c05;least squares method;mathematiques appliquees;approximation numerique;loi probabilite;ley probabilidad;stochastic method;methode monte carlo;differential equation;stochastic differential equation;time series;equation stochastique;probabilistic inversion;aproximacion numerica;stochastic differential equations;analyse numerique;ecuacion diferencial;stochastic system;65d99;fonction densite;numerical analysis;condicion inicial;estimation erreur;58b10;density function;error estimation;funcion densidad;monte carlo method;probability distribution;joint probability density function;estimacion error;serie temporelle;initial condition;processus stochastique;equation integrale;quasi monte carlo;least squares estimate;serie temporal;methode stochastique;ecuacion integral;numerical approximation;stochastic model;proceso estocastico;sistema estocastico;65c30;applied mathematics;modelo estocastico;modele stochastique;systeme stochastique;output least squares estimation;metodo estocastico	A system of stochastic differential equations is studied describing a compartmental carbon transfer model that includes uncertainties arising in the model from environmental and photosynthetic effects as well as initial conditions. Justification is given for the modeling of observed times series as Weiner processes. The solution of the resulting system is obtained as a stochastic process, a formulation is given appropriate for obtaining continuity and differentiability with respect to model transfer coefficients, and numerical approximation results are given. Estimation results of coefficients from NEE data are obtained using quasi-Monte Carlo techniques. The error resulting in NEE stochastic models is observed to be approximately Gaussian. This result is used to construct a joint probability density defined on a sample space of transfer coefficients. Finally, the joint probability density function is used with quasi-Monte Carlo to obtain information on transfer coefficients and predicted carbon pools. This information is compared with a priori results obtained without the benefit of NEE data. 2007 Published by Elsevier Inc.	approximation;coefficient;initial condition;monte carlo method;numerical analysis;quasi-monte carlo method;scott continuity;stochastic process	Luther W. White;Yiqi Luo	2008	Applied Mathematics and Computation	10.1016/j.amc.2007.07.004	stochastic process;econometrics;probability density function;stochastic differential equation;calculus;mathematics;statistics	ML	33.578352594311994	-19.05921120022827	165847
a79ffebda6b2d72ba721695f5a2b3d3c9f14839e	ideal bootstrap estimation of expected prediction error for k-nearest neighbor classifiers: applications for classification and error assessment	bootstrap method;prediction error;posterior probability;euclidean distance;discriminant analysis;remote sensing data;classification rules;nearest neighbor;simulation study;k nearest neighbor;monte carlo;nonparametrics	Euclidean distance -nearest neighbor ( -NN) classifiers are simple nonparametric classification rules. 5 5 Bootstrap methods, widely used for estimating the expected prediction error of classification rules, are motivated by the objective of calculating the ideal bootstrap estimate of expected prediction error. In practice, bootstrap methods use Monte Carlo resampling to estimate the ideal bootstrap estimate because exact calculation is generally intractable. In this article, we present analytic formulae for exact calculation of the ideal bootstrap estimate of expected prediction error for -NN classifiers and 5 propose a new weighted -NN classifier based on resampling ideas. The resampling-weighted -NN 5 5 classifier replaces the -NN posterior probability estimates by their expectations under resampling and 5 predicts an unclassified covariate to belong to the group with the largest resampling expectation. A simulation study and an application involving remotely sensed data show that the resampling-weighted 5 5 -NN classifier compares favorably to unweighted and distance-weighted -NN classifiers.	black–scholes model;euclidean distance;k-nearest neighbors algorithm;monte carlo method;resampling (statistics);simulation	Brian M. Steele;David A. Patterson	2000	Statistics and Computing	10.1023/A:1008933626919	machine learning;pattern recognition;mathematics;linear discriminant analysis;k-nearest neighbors algorithm;statistics	ML	29.810882131364256	-21.731958457125923	166328
30bb308a109eec074bb413c6b305a654089cad10	standard errors for the multiple roots in quadratic response surface models	racine multiple;response surface methodology;methodologie surface reponse;simulation;simulacion;matriz simetrica;standard error;response surface model;symmetric matrix;eigenvalue;surface reponse;region confiance;quadratic fit;valor propio;superficie respuesta;surface reponse quadratique;matrice symetrique;valeur propre;asymptotic distribution;response surface	In this article we develop the asymptotic distribution for the eigenvalues of a quadratic response surface by using the result of Eaton and Tyler to extend the results of Carter, Chinchilli, and Campbell and Bisgaard and Ankenman to the case including nondistinct eigenvalues. In particular, we suggest a method for estimating the confidence regions for multiple roots.	response surface methodology;roots	Xiangrong Yin;Lynne Seymour	2005	Technometrics	10.1198/004017005000000049	econometrics;response surface methodology;calculus;mathematics;statistics	Vision	33.337455604716155	-23.38240373033716	166404
e98e6fc0070950ef21f8d4d5884e8df13d4ff574	accurate uncertainty quantification using inaccurate computational models	equation derivee partielle;calcul scientifique;methode discretisation;equation differentielle;metodo estadistico;partial differential equation;ecuacion derivada parcial;analisis numerico;uncertainty quantification;metodo monte carlo;35xx;65c05;stochastic method;computer model;echantillonnage;aproximacion;62g08;methode monte carlo;differential equation;statistical method;34xx;integration;gran sistema;analyse numerique;approximation;ecuacion diferencial;sampling;bayesian;metodo discretizacion;computacion cientifica;65lxx;iteraccion;numerical analysis;large system;methode statistique;integracion;62f15;monte carlo method;nonparametric regression;iteration;methode stochastique;discretization method;monte carlo;scientific computation;muestreo;28xx;grand systeme;metodo estocastico	This paper proposes a novel uncertainty quantification framework for computationally demanding systems characterized by a large vector of non-Gaussian uncertainties. It combines state-of-the-art techniques in advanced Monte Carlo sampling with Bayesian formulations. The key departure from existing works is the use of inexpensive, approximate computational models in a rigorous manner. Such models can readily be derived by coarsening the discretization size in the solution of the governing PDEs, increasing the time step when integration of ODEs is performed, using fewer iterations if a non-linear solver is employed or making use of lower order models. It is shown that even in cases where the inaccurate models provide very poor approximations of the highfidelity response, statistics of the latter can be quantified accurately with significant reductions in the computational effort. Multiple approximate models can be used and rigorous confidence bounds of the estimates produced are provided at all stages.	algorithmic efficiency;approximation algorithm;basis function;bayesian network;computation;computational complexity theory;computational model;computer simulation;consistency model;discretization;experiment;fitness approximation;gauss–seidel method;heart rate variability;image scaling;inference engine;information;iteration;iterative method;jacobian matrix and determinant;krylov subspace;linear model;linear system;modal logic;model checking;monte carlo method;newton's method;nonlinear system;normal mode;numerical linear algebra;refinement (computing);requirement;runge–kutta methods;sampling (signal processing);solver;spatial variability;uncertainty quantification	Phaedon-Stelios Koutsourelakis	2009	SIAM J. Scientific Computing	10.1137/080733565	computer simulation;econometrics;calculus;mathematics;statistics;monte carlo method	AI	31.459219338908802	-16.37677648027424	166411
874086d31305cdd83fe5d9261536602555a4826b	order statistics and inference: estimation methods (n. balakrishnan and a. clifford cohen)	order statistic;estimation method			H. Leon Harter	1993	SIAM Review	10.1137/1035023	econometrics;order statistic;mathematics;statistics	Theory	30.887956217938715	-23.846031022197153	166914
799441951f660a17f8450c31ce1d2a1b75e6f8a9	accuracy vs. robustness: bi-criteria optimized ensemble of metamodels	uncertainty;design	Simulation has been widely used in modeling engineering systems. A metamodel is a surrogate model used to approximate a computationally expensive simulation model. Extensive research has investigated the performance of different metamodeling techniques in terms of accuracy and/or robustness and concluded no model outperforms others across diverse problem structures. Motivated by this finding, this research proposes a bi-criteria (accuracy and robustness) optimized ensemble framework to optimally identify the contributions from each metamodel (Kriging, Support Vector Regression and Radial Basis Function), where uncertainties are modeled for evaluating robustness. Twenty-eight functions from the literature are tested. It is observed for most problems, a Pareto Frontier is obtained, while for some problems only a single point is obtained. Seven geometrical and statistical metrics are introduced to explore the relationships between the function properties and the ensemble models. It is concluded that the bi-criteria optimized ensembles render not only accurate but also robust metamodels.	analysis of algorithms;approximation algorithm;ensemble forecasting;kriging;mit engineering systems division;metamodeling;pareto efficiency;radial basis function;simulation;support vector machine;surrogate model	Can Cui;Teresa Wu;Mengqi Hu;Jeffery D. Weir;Xianghua Chu	2014	Proceedings of the Winter Simulation Conference 2014		random variable;mathematical optimization;statistical inference;uncertainty;performance engineering;vector calculus;computer science;machine learning;mathematical model;mathematics;accuracy and precision;information system;regression analysis;statistics	AI	27.224760576238328	-14.996465200782602	167017
d8d2c0fd2a52eb7e4bc23bc0a81c00c56a1dc740	a bibliography of accelerated test plans	minimum variance;regression model statistical plans accelerated testing degradation extrapolation;regression analysis reliability theory bibliographies life testing extrapolation;accelerated testing;extrapolation;bibliographies;regression model;reliability theory;life testing;test plans accelerated testing degradation extrapolation life minimum variance overstress regression models;regression analysis;bibliographies life estimation life testing stress degradation history extrapolation distribution functions maximum likelihood estimation statistical analysis	This article provides a current bibliography of 159 references on statistical plans for accelerated tests. It will aid practitioners in selecting plans, and will stimulate researchers to develop needed plans & software.	test plan	W. B. Nelson	2005	IEEE Transactions on Reliability	10.1109/TR.2005.847247	reliability engineering;econometrics;computer science;mathematics;regression analysis;statistics	Visualization	29.826525749708647	-19.401620438865354	167071
0713cfc4536be6edffcaf79d2cae090020faa8f1	randomly generating portfolio-selection covariance matrices with specified distributional characteristics	covariance matrix factorization;matrice aleatoire;optimisation;matrix factorization;desviacion tipica;covariancia;matriz correlacion;optimizacion;random generation;standard deviation;portfolio selection;seleccion cartera;matrice diagonale;matrice covariance;covariance;random matrix;matriz covariancia;factorisation matricielle;selection portefeuille;portfolio optimization;descomposicion matricial;expected value;positive semidefinite matrices;matriz diagonal;decomposition matricielle;matrix decomposition;covariance matrices;motivacion;random correlation matrices;ecart type;portfolio management;motivation;matrice correlation;optimization;gestion cartera;gestion portefeuille;correlation matrix;optimal algorithm;matriz aleatoria;factorizacion matricial;random covariance matrices;covariance matrix;diagonal matrix	In portfolio selection, there is often the need for procedures to generate “realistic” covariance matrices for security returns, for example to test and benchmark optimization algorithms. For application in portfolio optimization, such a procedure should allow the entries in the matrices to have distributional characteristics which we would consider “realistic” for security returns. Deriving motivation from the fact that a covariance matrix can be viewed as stemming from a matrix of factor loadings, a procedure is developed for the random generation of covariance matrices (a) whose off-diagonal (covariance) entries possess a pre-specified expected value and standard deviation and (b) whose main diagonal (variance) entries possess a likely different pre-specified expected value and standard deviation. The paper concludes with a discussion about the futility one would likely encounter if one simply tried to invent a valid covariance matrix in the absence of a procedure such as in this paper. ∗Research conducted while a Visiting Scholar at the Department of Banking and Finance, Terry College of Business, University of Georgia, October 2003–March 2004. †Corresponding author rsteuer@uga.edu	algorithm;benchmark (computing);emoticon;factor analysis;mathematical optimization;procedural generation;pseudorandom number generator;randomness;stemming	Markus Hirschberger;Yue Qi;Ralph E. Steuer	2007	European Journal of Operational Research	10.1016/j.ejor.2005.10.014	estimation of covariance matrices;covariance matrix;mathematical optimization;law of total covariance;combinatorics;cma-es;covariance;portfolio optimization;mathematics;matrix decomposition;statistics;covariance function;project portfolio management	ML	32.77315788158555	-17.761683748272507	167321
8d40bd70043350042e96d3b3bdd4cf22726eee51	maximum-likelihood estimation for the multivariate sarmanov distribution: simulation study	method of moments;62g05;data simulation;65c10;parameters estimation;sarmanov distribution;maximum likelihood method;62h12	Used to model dependency in a multivariate setting with given marginals, Sarmanov's family of distributions creates difficulties when it comes to statistical inference. In this paper, we study maximum-likelihood procedures for estimating Sarmanov's distribution parameters for two different models: Under model I, we make use of a random data sample of volume m observed from an n-dimensional random vector, while model II consists of the first n dependent univariate random variables from a discrete-time stochastic process to which we try to fit Sarmanov's distribution starting from the corresponding n-tuple of observed values. To estimate some specific parameters, the use of the method of moments based on the covariance/correlation coefficient is also suggested. We illustrate these methods on simulated data and discuss the results.	package testing;simulation	Elena Pelican;Raluca Vernic	2013	Int. J. Comput. Math.	10.1080/00207160.2013.770148	econometrics;mathematical optimization;method of moments;mathematics;maximum likelihood;statistics	ECom	30.705510316613626	-19.939757563473286	167344
b8b417978d90bedc56c7026ca99853865eb811fe	bootstrap and fast double bootstrap tests of cointegration rank with financial time series	methode jackknife;algorithme rapide;autocorrelacion;62f40;metodo estadistico;finite sample;analisis numerico;likelihood ratio;test statistique;limit distribution;metodo monte carlo;stochastic process;theorie approximation;finance;corresponding author;taux interet;bootstrap;65c05;analisis datos;stochastic method;fonction repartition;loi limite;62m10;metodo jackknife;test estadistico;62f05;62e17;simulation;echantillon fini;statistical test;methode monte carlo;simulacion;cointegracion;test rango;statistical method;rank test;time series;distribucion estadistica;cointegration;asymptotic behavior;prix;comportement asymptotique;analyse numerique;interest rate;estimation parametrique;approximation theory;comportamiento asintotico;funcion distribucion;data analysis;distribution function;numerical analysis;distribution statistique;sciences actuarielles;stock price;methode statistique;monte carlo method;fast algorithm;bootstrap test;statistical computation;serie temporelle;likelihood ratio test;calculo estadistico;financial time series;processus stochastique;serie temporal;methode stochastique;analyse donnee;calcul statistique;methode reechantillonnage;62p05;tasa interes;resampling method;estimation statistique;proceso estocastico;jackknife method;rapport vraisemblance;monte carlo simulation;test razon verosimilitud;estimacion estadistica;test rapport vraisemblance;60e05;price;statistical estimation;60fxx;test rang;statistical distribution;algoritmo rapido;cours action;ley limite;precio;finanzas;autocorrelation;relacion verosimilitud;metodo estocastico	The likelihood ratio test of cointegration rank is the most widely used test for cointegration. Many studies have shown that its finite sample distribution is not well approximated by the limiting distribution. Bootstrap and fast double bootstrap (FDB) algorithms for the likelihood ratio test are introduced and evaluated by Monte Carlo simulation experiments. It is found that the performance of the ordinary (single) bootstrap test is in most cases good in terms of the size of the test. The FDB produces a further improvement in cases where the performance of the asymptotic test is unsatisfactory and the single bootstrap test overrejects noticeably. The FDB is shown to be a useful supplement to the single bootstrap as a tool for determining the cointegration rank. The tests are applied to US interest rates and international stock prices series. By simulating the data assuming that the cointegration rank is known, it is found that the asymptotic test tends to overestimate the cointegration rank, while the bootstrap and FDB tests choose the correct cointegration rank.	time series	N. Ahlgren;J. Antell	2008	Computational Statistics & Data Analysis	10.1016/j.csda.2008.03.025	stochastic process;econometrics;likelihood-ratio test;calculus;mathematics;statistics;monte carlo method	ML	32.89104322966422	-21.91263833350977	167416
f014311edd0ea54fce56c59afb5080ff97cfbfa3	robust approaches to 3d object secret sharing	secret sharing;3d model;lossless data compression;secret sharing scheme;3d graphics	Inspired by the successful application of image secret sharing schemes in multimedia protection, we present in this paper two secret sharing approaches for 3D models using Blakely and Thien & Lin schemes. We show that encoding 3D models using lossless data compression algorithms prior to secret sharing helps reduce share sizes and remove redundancies and patterns that possibly ease cryptanalysis. The proposed approaches provide a higher tolerance against data corruption/loss than existing 3D protection mechanisms, such as encryption. Experimental results are provided to demonstrate the secrecy and safety of the proposed schemes. The feasibility of the proposed algorithms is demonstrated on various 3D models.	secret sharing	Esam Elsheh;A. Ben Hamza	2010		10.1007/978-3-642-13772-3_33	computer science;theoretical computer science;lossless compression;homomorphic secret sharing;secure multi-party computation;internet privacy;proactive secret sharing;secret sharing;computer security;verifiable secret sharing;statistics;3d computer graphics	HCI	38.67844544844614	-11.192954829435545	167447
22d07857dbad2547317db2a5c21172778954a37f	on the origin of environments by means of natural selection	natural selection;genetic algorithm	The field of adaptive robotics involves simulations and real-world implementations of robots which adapt to their environments. In this paper we introduce adaptive environmentics—the flip side of adaptive robotics—in which the environment adapts to the robot. To illustrate the approach we offer three simple experiments in which a genetic algorithm is used to shape an environment for a simulated Khepera robot. We then discuss at length the potential of adaptive environmentics, also delineating several possible avenues of future research. Keyword: Adaptive environmentics, adaptive robotics, genetic algorithms. The reasonable man adapts himself to the world; the unreasonable man persists to adapt the world to himself. Therefore, all progress depends on the unreasonable.	adaptive filter;adaptive grammar;experiment;genetic algorithm;khepera mobile robot;robotics;simulation	Moshe Sipper	2001	AI Magazine		natural selection;simulation;genetic algorithm;computer science;engineering;artificial intelligence;machine learning	Robotics	24.83961778888014	-11.995714415332415	167745
1de0645dfd91ddf8630e240d06e291ddf5d54ac6	new ewma control charts for monitoring process dispersion	engineering;49k40;moyenne mobile;metodo estadistico;in control;statistical simulation;process dispersion;analisis datos;62p30;industrie;carte controle;industria;estimacion promedio;statistical method;ingenierie;estimacion insesgada;data analysis;control chart;control proceso;moving average;industry;average run length;simulacion estadistica;methode statistique;simulation statistique;statistical computation;calculo estadistico;carta control;promedio movil;process dispersion exponentially weighted moving average average run length arl unbiased in control out of control;process control;simulation study;analyse donnee;ingenieria;calcul statistique;arl unbiased;mean estimation;unbiased estimation;estimation moyenne;commande processus;estimation sans biais;out of control;exponentially weighted moving average	There exist two EWMA-type dispersion charts for monitoring dispersion increases in the literature. One resets the EWMA statistic to zero whenever it is below zero. The other one truncates negative normalized observations to zero in the EWMA statistic. This paper proposes two one-sided EWMA charts for detecting dispersion increases and decreases, respectively, and one two-sided EWMA chart for monitoring dispersion increases or decreases simultaneously. Simulation studies show that the proposed upper-sided EWMA chart performs better than the two existing counterparts for detecting increases in dispersion, and that the proposed lower-sided EWMA chart significantly outperforms the two lower-sided EWMA charts developed similar to their two existing upper-sided EWMA charts for detecting decreases in dispersion. Moreover, the proposed two-sided EWMA chart provides much better sensitivity than the two two-sided EWMA charts generalized from the two existing upper-sided EWMA charts for detecting overall changes in dispersion. © 2010 Elsevier B.V. All rights reserved.	gantt chart;sensor;simulation	Longcheen Huwang;Chun-Jung Huang;Yi-Hua Tina Wang	2010	Computational Statistics & Data Analysis	10.1016/j.csda.2010.03.011	ewma chart;econometrics;control chart;process control;mathematics;moving average;data analysis;statistics	AI	33.29339124084675	-22.98129183236746	167806
bdadafef4defa4e1d148e5f59e001589ad3f9b9f	a variable fidelity information fusion method based on radial basis function	variable fidelity;simulation based design;metamodel;radial basis function;information fusion	Radial basis function (RBF) model has been widely used in complex engineering design process to replace the computational-intensive simulation models. This paper proposes a variable-fidelity metamodeling (VFM) approach based on RBF, in which different levels fidelity information can be integrated and fully exploited. In the proposed VFM approach, a RBF metamodel is constructed for the low-fidelity (LF) model as a start. Then by taking the constructed LF metamodel as a prior-knowledge and mapping the output space of the LF metamodel to that of the studied high-fidelity (HF) model, a variable fidelity (VF) metamodel is created to approximate the relationships between the design variables and corresponding output responses. A numerical illustrative example is adopted to make a detailed comparison between the VFM approach developed in this research and three existing scaling function based VFM approaches, considering different sample sizes and sample noises. Results illustrate that the proposed VFM approach outperforms the scaling function based VFM approaches both in global and local accuracy. Then the proposed VFM approach is applied to two engineering problems, modeling aerodynamic data for a three-dimensional aircraft and the prediction of weld bead profile in laser welding, to illustrate its ability in support of complex engineering design.	radial (radio);radial basis function	Qi Zhou;Ping Jiang;Xinyu Shao;Jiexiang Hu;Longchao Cao;Li Wan	2017	Advanced Engineering Informatics	10.1016/j.aei.2016.12.005	metamodeling;radial basis function;simulation;computer science;engineering;artificial intelligence;machine learning;engineering drawing	SE	35.61342668110213	-16.389244516792267	167866
032e1185d465118b06a5f3dc59aa2e5255e2bce3	lares: an artificial chemical process approach for optimization	stochastic algorithm;test bed;spectrum;global optimization;value function;stochastic algorithms;stochastic algorithms global optimization	This article introduces a new global optimization procedure called LARES. LARES is based on the concept of an artificial chemical process (ACP), a new paradigm which is described in this article. The algorithm's performance was studied using a test bed with a wide spectrum of problems including random multi-modal random problem generators, random LSAT problem generators with various degrees of epistasis, and a test bed of real-valued functions with different degrees of multi-modality, discontinuity and flatness. In all cases studied, LARES performed very well in terms of robustness and efficiency.	benchmark (computing);class;convergence (action);cryptococcus sp. tgpaug08dx1-s-y6;discrete mathematics;emoticon;erwinia phage vb_eamm-y2;evolutionary computation;general-purpose modeling;genetic algorithm;global optimization;hyperactive behavior;mathematical optimization;maxima and minima;modal logic;modality (human–computer interaction);npy2r gene;numerous;one thousand;paratylenchus sp. yj;program optimization;programming paradigm;reflections of signals on conducting lines;rhizobium sp. yj-ar-5-1;solutions;tap2 wt allele;tablespoon dosing unit;testbed;weatherstar;zj0273;neuropeptide y-y1 receptor;tyrosyl radical y(d);yard (length)	Roberto Irizarry	2004	Evolutionary Computation	10.1162/1063656043138897	spectrum;mathematical optimization;computer science;stochastic optimization;machine learning;mathematics;bellman equation;mathematical economics;algorithm;statistics;global optimization;testbed	Theory	34.295619384395806	-13.619039503971642	168025
a75a7abc23255790b076f48a895593efa0c5a3fe	the least trimmed quantile regression	quantile regression;linear regression;least trimmed quantile regression;outlier detection;breakdown point;simulation study	The linear quantile regression estimator is very popular and widely used. It is also well known that this estimator can be very sensitive to outliers in the explanatory variables. In order to overcome this disadvantage, the usage of the least trimmed quantile regression estimator is proposed to estimate the unknown parameters in a robust way. As a prominent measure of robustness, the breakdown point of this estimator is characterized and its consistency is proved. The performance of this approach in comparison with the classical one is illustrated by an example and simulation studies.	simulation	N. M. Neykov;Pavel Cízek;Peter Filzmoser;P. N. Neytchev	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2011.10.023	econometrics;binomial regression;anomaly detection;quantile;quantile regression;least trimmed squares;trimmed estimator;linear regression;machine learning;bayesian multivariate linear regression;pattern recognition;mathematics;robust regression;regression analysis;statistics	ML	29.16897861326033	-23.91301385216967	168032
ea3ada87be469270efdefaac6add3f89a6020e53	adjusting the tests for skewness and kurtosis for distributional misspecifications	62p20;kurtosis;asymmetry;60e99;00a72;anil k bera;monte carlo test;gamini premaratne;62f03;adjusting the tests for skewness and kurtosis for distributional misspecifications;distributional misspecification;skewness;rao score test;ssrn	The standard root−b1 test is widely used for testing skewness. However, several studies have demonstrated that this test is not reliable for discriminating between symmetric and asymmetric distributions in the presence of excess kurtosis. The main reason for the failure of the standard root−b1 test is that its variance formula is derived under the assumption of no excess kurtosis. In this paper we theoretically derive adjustment to the root−b1 test under the framework of Roa’s Score (or the Lagrange multiplier) test principle. Our adjusted test automatically corrects the variance formula and does not lead to over− or under−rejection of the correct null hypothesis. In a similar way, we also suggest an adjusted test for kurtosis in the presence of asymmetry. These tests are then applied to both simulated and real data. The finite sample performances of the adjusted tests are far superior compared to those of their unadjusted counterparts Published: 2001 URL: http://www.business.uiuc.edu/Working_Papers/papers/01−0116.pdf Adjusting the Tests for Skewness and Kurtosis for Distributional Misspeci cations Anil K. Bera and Gamini Premaratne Department of Economics,University of Illinois, 1206 S. 6th Street,Champaign, IL 61820,USA.	distributional semantics;lagrange multiplier;naruto shippuden: clash of ninja revolution 3;offset binary;performance;simulation	Gamini Premaratne;Anil K. Bera	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.988254	econometrics;skewness;jarque–bera test;kurtosis;mathematics;normality test;d'agostino's k-squared test;asymmetry;statistics	ML	29.28439147048907	-22.832118852212158	168035
5cf4cebc5095679b5f024a79906d30aafba0adb2	improved estimation of fixed effects panel data partially linear models with heteroscedastic errors	fixed effects;primary;heteroscedastic errors;consistent estimator;secondary;partially linear;incidental parameter	Fixed effects panel data regression models are useful tools in econometric and microarray analysis. In this paper, we consider statistical inferences under the setting of fixed effects panel data partially linear regression models with heteroscedastic errors. We find that the usual local polynomial estimator of the error variance function based on residuals is inconsistent, and develop a consistent estimator. Applying this consistent estimator of error variance and spline series approximation of the nonparametric component, we further construct a weighted semiparametric least squares dummy variables estimator for the parametric and nonparametric components. Asymptotic normality of the proposed estimator is derived and its asymptotic covariance matrix estimator is provided. The proposed estimator is shown to be asymptotically more efficient than those ignoring heteroscedasticity. Simulation studies are conducted to demonstrate the finite sample performances of the proposed procedure. As an application, a set of economic data is analyzed by the proposed method. © 2016 Elsevier Inc. All rights reserved.	approximation;dummy variable (statistics);fixed effects model;least squares;linear model;microarray;panel data;performance;polynomial;semiparametric model;simulation;spline (mathematics)	Jianhua Hu;Jinhong You;Xian Zhou	2017	J. Multivariate Analysis	10.1016/j.jmva.2016.10.010	efficient estimator;minimax estimator;shrinkage estimator;econometrics;mathematical optimization;minimum-variance unbiased estimator;estimator;delta method;bayes estimator;stein's unbiased risk estimate;ordinary least squares;trimmed estimator;fixed effects model;efficiency;newey–west estimator;mathematics;mean squared error;first-difference estimator;bias of an estimator;consistent estimator;invariant estimator;statistics	ML	29.650921366060967	-23.754784859034984	168551
3d91c97baa4076885f86075d89d69fb8bfd67f2d	boxplot-based outlier detection for the location-scale family	6209;order statistics;semi interquartile range siqr;fences;outlier identification;interquartile range iqr;boxplot;6207	Boxplots are among the most widely used exploratory data analysis (EDA) tools in statistical practice. Typical applications of boxplots include eliciting information about the underlying distribution (shape, location, etc.) as well as identifying possible outliers. This article focuses on a modification using a type of lower and upper fences similar in concept to those used in a traditional boxplot; however, instead of constructing the upper and lower fences using the upper and lower quartiles, respectively, and a multiple of the interquartile range (IQR), multiples of the upper and the lower semi-interquartile ranges (SIQR), respectively, measured from the sample median, are used. Any observation beyond the proposed fences is labeled a potential outlier. An exact expression for the probability that at least one sample observation is wrongly classified as an outlier, the so-called “some-outside rate per sample” (Hoaglin et al. (1986)), is derived for the family of location-scale distributions and is used ...	anomaly detection	Y. H. Dovoedo;Subha Chakraborti	2015	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.813037	econometrics;outlier;order statistic;box plot;data mining;mathematics;statistics	ML	29.930404545089683	-20.906800255822073	168589
068e13da918819a3850ff8fbfa61dfa5c269a67d	deriving space-time variograms from space-time autoregressive (star) model specifications	model specification;space time	Many geospatial science subdisciplines analyze variables that vary over both space and time. The space-time autoregressive (STAR) model is one specification formulated to describe such data. This paper summarizes STAR specifications that parallel geostatistical model specifications commonly used to describe space-time variation, with the goal of establishing synergies between these two modeling approaches. Resulting expressions for space-time correlograms derived from 1-order STAR models are solved numerically, and then linked to appropriate space-time semivariogram models.	autoregressive model;geomatics;geospatial analysis;numerical analysis;synergy	Daniel A. Griffith;Gerard B. M. Heuvelink	2010		10.1007/978-3-642-25926-5_1	econometrics;discrete mathematics;star model;mathematics;statistics	SE	36.36024029477962	-21.690406177671637	168638
86fcfab4851bbaa36470eada03f4124a01bc9156	computational issues with fitting joint location/dispersion models in unreplicated 2k factorials	experimental design;ajustamiento modelo;analyse multivariable;factorial design;analisis componente principal;ridge regression;convergence;analisis factorial;convergence problems;multivariate analysis;analisis datos;maximum likelihood;regresion ridge;maximum vraisemblance;analisis correspondencia;plan experiencia;statistical regression;ajustement modele;data analysis;convergencia;regression pseudo orthogonale;analyse factorielle;maximum likelihood estimate;plan experience;infinite likelihood;estimateur retrecissement;factor analysis;regresion estadistica;principal component analysis;correspondence analysis;model matching;statistical computation;calculo estadistico;62h25;analyse composante principale;analyse correspondance;analisis multivariable;analyse donnee;calcul statistique;62k15;maximum likelihood likelihood ridges infinite likelihood convergence problems hessian;regression statistique;62j07;hessian;dispersion measure;maxima verosimilitud;medida dispersion;plan factorial;likelihood ridges;plan factoriel;mesure dispersion	Maximum likelihood estimation for a joint location/dispersion model has been found occasionally to experience convergence problems when applied to experiments of the 2k factorial series. We explore these problems and identify models for which the likelihood diverges or is multimodal. We derive the conditions under which this occurs and provide simple ways to check for problems both before and during computation. © 2010 Elsevier B.V. All rights reserved.	computation;experiment;multimodal interaction	Thomas M. Loughin;Jorge E. Rodríguez	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2010.05.017	econometrics;calculus;mathematics;maximum likelihood;statistics	AI	33.39265942046932	-23.132452123808992	168677
08978f1d980313730d04f3ebd8a8558706bf3cc7	on cost function transformations for the reduction of uncertain model parameters' impact towards the optimal solutions	computacion informatica;uncertainty;mathematics and statistics;finite element method;robust optimization;accuracy;ciencias basicas y experimentales;matematicas;optimization;grupo a;consistency	Uncertainties affect the accuracy of nonlinear static or dynamic optimization and inverse problems. The propagation of uncertain model parameters towards the optimal problem solutions can be assessed in a deterministic or stochastic way using Monte Carlo based techniques and efficient spectral collocation and Galerkin projection methods. This paper presents cost function transformations for reducing the impact of uncertain model parameters towards the optimal solutions. We assess the consistency of the methodology by determining sufficient conditions on the cost function transformations and apply the methodology on several test functions. © 2014 Elsevier B.V. All rights reserved.	algorithm;collocation;distribution (mathematics);dynamic programming;galerkin method;loss function;mathematical optimization;monte carlo method;nonlinear system;propagation of uncertainty;software bug;software propagation;stochastic gradient descent	Guillaume Crevecoeur;Rob H. De Staelen	2015	J. Computational Applied Mathematics	10.1016/j.cam.2014.11.030	econometrics;mathematical optimization;robust optimization;uncertainty;finite element method;mathematics;accuracy and precision;consistency;statistics	AI	31.391862531567472	-14.856083778778327	169534
b4e2dc54e796b281f570f031bd7cac0ef6586b51	a bayesian semiparametric accelerated failure time model for arbitrarily censored data with covariates subject to measurement error	measurement error;arbitrarily censored;semiparametric aft model;dirichlet process mixture;62n86 fuzziness and survival analysis and censored data;62 statistics	AbstractA flexible Bayesian semiparametric accelerated failure time (AFT) model is proposed for analyzing arbitrarily censored survival data with covariates subject to measurement error. Specifically, the baseline error distribution in the AFT model is nonparametrically modeled as a Dirichlet process mixture of normals. Classical measurement error models are imposed for covariates subject to measurement error. An efficient and easy-to-implement Gibbs sampler, based on the stick-breaking formulation of the Dirichlet process combined with the techniques of retrospective and slice sampling, is developed for the posterior calculation. An extensive simulation study is conducted to illustrate the advantages of our approach.	censoring (statistics);semiparametric model	Xiaoyan Lin	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.977919	econometrics;mathematics;statistics;observational error	Metrics	30.102990547970546	-22.993326875994697	169537
c17da6542bf12c8fc3c2ac3fa09ab98b34ee93ad	multivariate two-sided tests for normal mean vectors with unknown covariance matrix	62h15;62f30;β distribution;orthogonal projection;power of the test	In this study, we discuss two kinds of multivariate two-sided tests for normal mean vectors with unknown covariance matrix. First, assuming that all components of a normal mean vector are simultaneously nonnegative or non positive, we consider a multivariate two-sided test for testing whether the normal mean vector is equal to zero or not. Next, assuming that all differences of components between two normal mean vectors are simultaneously non negative or non positive, we consider a multivariate two-sided test for testing whether the two normal mean vectors are equal or not. We construct methods for testing by referring to Glimm et al. (2002), Tamhane and Logan (2002) and Sasabuchi (2007). Finally, we give some simulation results regarding critical values and power of the test intended to compare the three methods.		T. Imada	2013	Communications in Statistics - Simulation and Computation	10.1080/03610918.2011.633727	scatter matrix;matrix t-distribution;econometrics;multivariate normal distribution;combinatorics;normal-wishart distribution;mathematics;orthographic projection;matrix normal distribution;statistics	AI	31.12739127063545	-23.061973004510662	169575
252af1a0459a1b2445498f74c763f687e7a4140d	finding approximate solutions to combinatorial problems with very large data sets using birch	metodo cuadrado menor;algorithme rapide;classification automatique statistiques;62g35;methode moindre carre;analyse multivariable;analisis numerico;estimator robustness;statistical simulation;fonction valeur;approximate algorithm;covariancia;multivariate analysis;least squares method;analisis datos;large dataset;ley gran numero;62f35;estimation non parametrique;loi grand nombre;combinatorial problems;large data sets;outlier;optimization method;covariance;calculo automatico;funcion valor;metodo optimizacion;fonction objectif;analyse numerique;computing;calcul automatique;resolucion problema;objective function;discriminant analysis;estimation parametrique;analyse discriminante;optimization problem;law of large numbers;observacion aberrante;non parametric estimation;combinatorial problem;data analysis;analisis discriminante;probleme combinatoire;65f40;robustez estimador;problema combinatorio;simulacion estadistica;numerical analysis;62h30;simulation statistique;approximate solution;fast algorithm;determinante;statistical computation;calculo estadistico;algebra lineal numerica;algebre lineaire numerique;covariate;methode optimisation;determinant;observation aberrante;simulation study;funcion objetivo;analisis multivariable;analyse donnee;least trimmed squares;calcul statistique;covariable;value function;numerical linear algebra;estimacion no parametrica;estimation statistique;58a25;estimacion adaptativa;estimacion estadistica;statistical estimation;algoritmo rapido;adaptive estimation;minimum covariance determinant;problem solving;resolution probleme;estimation adaptative;robustesse estimateur	Computing estimators with good robustness properties generally requires solving highly complex optimization problems. The current state-of-the-art algorithms to find approximate solutions to these problems need to access the data set a large number to times and become unfeasible when the data do not fit in memory. In this paper the BIRCH algorithm is adapted to calculate approximate solutions to problems in this class. For data sets that fit in memory, this approach is able to find approximate Least Trimmed Squares (LTS) and Minimum Covariance Determinant (MCD) estimators that compare very well with those returned by the fast-LTS and fast-MCD algorithms, and in some cases is able to find a better solution (in terms of value of the objective function) than those returned by the fast- algorithms. This methodology can also be applied to the Linear Grouping Algorithm and its robust variant for very large datasets. Finally, results from a simulation study indicate that this algorithm performs comparably well to fast-LTS in simple situations (large data sets with a small number of covariates and small proportion of outliers) and does much better than fast-LTS in more challenging situations without requiring extra computational time. These findings seem to confirm that this approach provides the first computationally feasible and reliable approximating algorithm in the literature to compute the LTS and MCD estimators for data sets that do not fit in memory.	approximation algorithm;birch	Justin Harrington;Matias Salibian-Barrera	2010	Computational Statistics & Data Analysis	10.1016/j.csda.2008.08.001	optimization problem;econometrics;computing;outlier;determinant;law of large numbers;covariate;numerical analysis;least trimmed squares;covariance;mathematics;multivariate analysis;numerical linear algebra;linear discriminant analysis;data analysis;algorithm;statistics	ML	33.64515211526553	-23.645073306472376	169630
9603dccd9405e9ae088bf541abad0bef0038d490	destructive weighted poisson cure rate models with bivariate random effects: classical and bayesian approaches	dirichlet process;reml;competing risks	In this paper, random effects are included in the destructive weighted Poisson cure rate model. For parameter estimation we implemented a classical approach based on the restricted maximum likelihood (REML) methodology and a Bayesian approach based on Dirichlet process priors. A small scale simulation study is conducted to discuss parameter recovery and the performance of the proposed methodology is illustrated with a real data example.	bivariate data;random effects model	Diego I. Gallardo;Heleno Bolfarine;Antonio Carlos Pedroso de Lima	2016	Computational Statistics & Data Analysis	10.1016/j.csda.2015.12.006	econometrics;mathematical optimization;mathematics;restricted maximum likelihood;statistics	ML	30.317574791911632	-23.41778763177471	169650
538390779a454722f70854463ade0e242b888cf8	reak: reliability analysis through error rate-based adaptive kriging		As models in various fields are becoming more complex, associated computational demands have been increasing significantly. Reliability analysis for these systems when failure probabilities are small is very challenging, requiring a large number of costly simulations. To address this challenge, this paper introduces Reliability analysis through Error rate-based Adaptive Kriging (REAK). An extension of the Central Limit Theorem based on Lindeberg condition is adopted here to derive the distribution of the number of design samples with wrong sign estimate and subsequently determine the maximum error rate for failure probability estimates. This error rate enables optimal establishment of effective sampling regions at each stage of an adaptive scheme for strategic generation of design samples. Moreover, it facilitates setting a target accuracy for failure probability estimation, which is used as the stopping criterion for reliability analysis. These capabilities together can significantly reduce the number of calls to sophisticated, computationally demanding models. The application of REAK for four examples with varying extent of nonlinearity and dimension is presented. Results indicate that REAK is able to reduce the computational demand by as high as 50% compared to the state-of-the-art methods of Adaptive Kriging with Monte Carlo Simulation (AK-MCS) and Improved Sequential Kriging Reliability Analysis (ISKRA).		Zeyu Wang;Abdollah Shafieezadeh	2019	Rel. Eng. & Sys. Safety	10.1016/j.ress.2018.10.004	engineering;reliability engineering;sampling (statistics);mathematical optimization;lindeberg's condition;monte carlo method;kriging;central limit theorem;nonlinear system;word error rate	Logic	28.533314880203186	-17.635264987766934	169722
0d26569aee94489479ed3ad4e6e50f5e9e1c2fb4	phase-type distributions and optimal stopping for autoregressive processes		Autoregressive processes are intensively studied in statistics and other fields of applied stochastics. For many applications, the overshoot and the threshold time are of special interest. When the upward innovations are in the class of phase-type distributions, we determine the joint distribution of these two quantities and apply this result to problems of optimal stopping. Using a principle of continuous fit, this leads to explicit solutions. © 2012 Applied Probability Trust.	autoregressive model;optimal stopping	Sören Christensen	2012	J. Applied Probability	10.1017/S0021900200008846	econometrics;mathematical optimization;optimal stopping;stopping time;star model;mathematics;phase-type distribution;autoregressive model;statistics	ML	32.28144129688232	-18.703109366881264	169767
f339f010d40e0b7cb5e7575fed2b412bdc4d8331	use of the lognormal distribution for the coefficients of friction and wear	frotamiento;modelizacion;tribologia;system reliability;fiabilite systeme;limit distribution;methode empirique;wear;coeficiente roce;parametro variable;loi limite;coefficient of friction;lognormal distribution;metodo empirico;theoreme central limite;empirical method;tribology;swinburne;loi lognormale;fiabilidad sistema;parametre variable;modelisation;empirical evidence;friction coefficient;ley lognormal;coefficient frottement;tribologie;fonction densite;usure;central limit theorem;variable parameter;density function;frottement;funcion densidad;desgaste;teorema central limite;algorithme evolutionniste;algoritmo evolucionista;evolutionary algorithm;friction;modeling;density functional;ley limite;distribution selection	To predict the reliability of a system, an engineer might allocate a distribution to each input. This raises a question: how to select the correct distribution? Siddall put forward an evolutionary approach that was intended to utilise both the understanding of the engineer and available data. However, this method requires a subjective initial distribution based on the engineer's understanding of the variable or parameter. If the engineer's understanding is limited, the initial distribution will be misrepresentative of the actual distribution, and application of the method will likely fail. To provide some assistance, the coefficients of friction and wear are considered here. Basic tribology theory, dimensional issues and the central limit theorem are used to argue that the distribution for each of the coefficients will typically be like a lognormal distribution. Empirical evidence from other sources is cited to lend support to this argument. It is concluded that the distributions for the coefficients of friction and wear would typically be lognormal in nature. It is therefore recommended that the engineer, without data or evidence to suggest differently, should allocate a lognormal distribution to the coefficients of friction and wear.	coefficient	Clint Steele	2008	Rel. Eng. & Sys. Safety	10.1016/j.ress.2007.09.005	econometrics;probability density function;systems modeling;empirical evidence;engineering;central limit theorem;evolutionary algorithm;calculus;friction;tribology;log-normal distribution;mathematics;wear;empirical research;statistics	Robotics	28.612479628738456	-18.74287511129008	169806
905e8a1c8f40ab71f08c9b6525d5df628daec1cd	a reading guide for last passage times with financial applications in view	running maximum;last passage times;journal article;class σ;62p05;60g17	In this survey on last passage times, we propose a new viewpoint which provides a unified approach to many different results which appear in the mathematical finance literature and in the theory of stochastic processes. In particular, we are able to improve the assumptions under which some well-known results are usually stated. Moreover we give some new and detailed calculations for the computation of the distribution of some large classes of last passage times. We have kept in this survey only the aspects of the theory which we expect potentially to be relevant for financial applications.	computation;expect;stochastic process;the hitchhiker's guide to the galaxy	Ashkan Nikeghbali;Eckhard Platen	2013	Finance and Stochastics	10.1007/s00780-013-0207-6	operations research;statistics	ML	35.01975620794058	-18.213352904901583	169876
9325ccf65d4ea2c07006a65a121bf9c01f95ca2e	comparison of methods of estimation for parameters of generalized poisson distribution through simulation study	generalized poisson distribution gpd;goodness of fit;65cxx;62f10;62j02;methods of estimation;97k80	Comparison of Methods of Estimation for Parameters of Generalized Poisson Distribution through Simulation Study Y.S. Wagh & K.K. Kamalja To cite this article: Y.S. Wagh & K.K. Kamalja (2015): Comparison of Methods of Estimation for Parameters of Generalized Poisson Distribution through Simulation Study, Communications in Statistics Simulation and Computation, DOI: 10.1080/03610918.2015.1105971 To link to this article: http://dx.doi.org/10.1080/03610918.2015.1105971	communications in statistics – simulation and computation;iso/iec 11404;matlab;nl (complexity);windows me	Yogita S. Wagh;Kirtee K. Kamalja	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1105971	econometrics;data mining;mathematics;goodness of fit;statistics	ML	28.478605211077713	-22.392690758887102	170174
a38d6766b210f2a5c47b152963ea5c37526f9655	multidimensional penalized signal regression	producto tensorial;analyse multivariable;signal regression;multivariate calibration;problema mal planteado;spline p;fluorescence;multivariate analysis;p spline;probleme mal pose;produit tensoriel;etalonnage multivariable;grupo de excelencia;metodo penalidad;spectrum;signal n dimensionnel;spectra;fluorescencia;tensor product;penalty method;methode penalite;ciencias basicas y experimentales;matematicas;ill posed problem;spectre;analisis multivariable;regression signal;espectro	We propose a general approach to regression on digitized multidimensional signals that can pose severe challenges to standard statistical methods. The main contribution of this work is to build a two-dimensional coefficient surface that allows for interaction across the indexing plane of the regressor array. We aim to use the estimated coefficient surface for reliable (scalar) prediction. We assume that the coefficients are smooth along both indices. We present a rather straight-forward and rich extension of penalized signal regression using penalized B-spline tensor products, where appropriate difference penalties are placed on the rows and columns of the tensor product coefficients. Our methods are grounded in standard penalized regression, thus crossvalidation, effective dimension and other diagnostics are accessible. Further the model is easily transplanted into the generalized linear model framework. An illustrative example motivates our proposed methodology and performance comparisons are made to other popular methods.	b-spline;coefficient;column (database);effective dimension;generalized linear model	Brian D. Marx;Paul H. C. Eilers	2005	Technometrics	10.1198/004017004000000626	tensor product;spectrum;econometrics;fluorescence;calculus;penalty method;mathematics;multivariate analysis;statistics	ML	34.540649682556285	-23.09973627699813	170407
3218ea685d44d1c3303e7f6c934548ed99bc9276	an improved exponential estimator of finite population mean in simple random sampling using an auxiliary attribute	serial correlation;auxiliary information;study variable;point bi serial correlation;finite population;first order;bias;mean square error;percent relative efficiency;comparative study;relative efficiency;simple random sampling;auxiliary attribute	In this paper, we propose an exponential ratio type estimator of the finite population mean when auxiliary information is qualitative in nature. Under simple random sampling without replacement scheme, the expressions for the bias and the mean square error of the proposed estimator have been obtained, up to first order of approximation. To show that our proposed estimator is more efficient as compared to the existing estimators, we have made a comparative study with respect to their mean square errors. Theoretically and numerically, we have found that our proposed estimator is always more efficient as compared to its competitor estimators including all the estimators of Abd-Elfattah et al. [1] [A.M. Abd-Elfattah, E.A. El-Sherpieny, S.M. Mohamed, and O.F. Abdou. Improvement in estimating the population mean in simple random sampling using information on auxiliary attribute. Applied Mathematics and Computation, 215 (2010), 4198–4202]. 2011 Elsevier Inc. All rights reserved.	computation;mean squared error;numerical analysis;order of approximation;sampling (signal processing);time complexity	Lovleen Kumar Grover;Parmdeep Kaur	2011	Applied Mathematics and Computation	10.1016/j.amc.2011.08.035	efficient estimator;minimum mean square error;econometrics;mathematical optimization;simple random sample;minimum-variance unbiased estimator;estimator;autocorrelation;efficiency;comparative research;bias;first-order logic;mathematics;mean squared error;bias of an estimator;orthogonality principle;invariant estimator;statistics	ML	29.793362084494966	-22.659357190175225	170433
206189293977f92f7ba352e83fd8834ae535cdcd	misspecifications in interaction model distance decay relations: a spatial structure effect	statistical approach;maps;distribucion espacial;model specification;population;local distance decay;systeme information geographique;mapa;north america;america del norte;migration;amerique du nord;time variations;spatial structure;variation temporelle;etats unis;estados unidos;carte;spatial variations;modelo;poisson regression;statistical analysis;spatial distribution;heterogeneidad;geographic information systems;variacion espacial;box cox transformation;analyse statistique;poblacion;variation spatiale;modele;interaction model;geografia;distribution spatiale;communities;geographie;spatial heterogeneity;variacion temporal;models;multicollinearity;migracion;heterogeneity;heterogeneite;geography	An exclusively statistical approach is proposed to address the spatial structure effects of general interaction models. It is shown that the spatial heterogeneity in the estimated region-specific distance decay parameters may in part be due to the combination of two factors: (a) a functional mis-specification of the global distance decay relationship; and (b) the heterogeneity in the region-specific conditional distance distributions. A properly specified global distance decay function allows controlling for these spatially induced biases in the local distance decay parameters. However, inherent multicollinearities between the set of region specific distance decay parameters and other estimated model parameters prevent an unambiguous interpretation. A key conclusion is that a proper model specification, in particular, the specification of the global distance decay relationship, is of paramount importance in interaction modeling and for accessibility studies.		Michael Tiefelsdorf	2003	Journal of Geographical Systems	10.1007/s101090300102	power transform;multicollinearity;geography;human migration;heterogeneity;mathematics;poisson regression;geographic information system;specification;cartography;statistics;spatial heterogeneity;population	DB	35.658910162260064	-22.360301780752533	170634
8f850781abda1c71c134902cd787d3b26ae8c757	gauge capability for pass - fail inspection	profile likelihood;inspection errors;binary data;mixture models;em algorithm;go no go testing	Quantitative measurement is an accepted ideal, but pass–fail inspection remains a fact of life, even in high-technology industries. For pass–fail data, variance components do not separate gauge and material variation. This article focuses on maximum likelihood estimation of conditional misclassification rates, with and without reference evaluations to anchor the analysis. Likelihood-based confidence intervals and testing for reproducibility effects are also discussed.		Russell A. Boyles	2001	Technometrics	10.1198/004017001750386332	econometrics;expectation–maximization algorithm;mixture model;data mining;mathematics;statistics	Vision	29.641113867972987	-20.551507603492226	170645
205a6b2555874fa4021cd4b5bdb0ee6ed4ea52c5	effect of imprecise knowledge of the selection channel on steganalysis	steganalysis;steganography;adaptive;selection channel	It has recently been shown that steganalysis of content-adaptive steganography can be improved when the Warden incorporates in her detector the knowledge of the selection channel -- the probabilities with which the individual cover elements were modified during embedding. Such attacks implicitly assume that the Warden knows at least approximately the payload size. In this paper, we study the loss of detection accuracy when the Warden uses a selection channel that was imprecisely determined either due to lack of information or the stego changes themselves. The loss is investigated for two types of qualitatively different detectors -- binary classifiers equipped with selection-channel-aware rich models and optimal detectors derived using the theory of hypothesis testing from a cover model. Two different embedding paradigms are addressed -- steganography based on minimizing distortion and embedding that minimizes the detectability of an optimal detector within a chosen cover model. Remarkably, the experimental and theoretical evidence are qualitatively in agreement across different embedding methods, and both point out that inaccuracies in the selection channel do not have a strong effect on steganalysis detection errors. It pays off to use imprecise selection channel rather than none. Our findings validate the use of selection-channel-aware detectors in practice.	distortion;sensor;steganalysis;steganography	Vahid Sedighi;Jessica J. Fridrich	2015		10.1145/2756601.2756621	steganalysis;adaptive behavior;pattern recognition;data mining;mathematics;steganography;statistics	ML	37.16192545325695	-14.8908398382683	171235
49def2a672b0dc30bb51078125fa35e3e2102043	using a markov network model in a univariate eda: an empirical cost-benefit analysis	distributed estimation;evolutionary computation;optimization problem;probabilistic modelling;estimation of distribution algorithm;probability distribution;markov network;estimation of distribution algorithms;cost benefit analysis;evolutionary computing	This paper presents an empirical cost-benefit analysis of an algorithm called Distribution Estimation Using MRF with direct sampling (DEUMd). DEUMd belongs to the family of Estimation of Distribution Algorithm (EDA). Particularly it is a univariate EDA. DEUMd uses a computationally more expensive model to estimate the probability distribution than other univariate EDAs. We investigate the performance of DEUMd in a range of optimization problem. Our experiments shows a better performance (in terms of the number of fitness evaluation needed by the algorithm to find a solution and the quality of the solution) of DEUMd on most of the problems analysed in this paper in comparison to that of other univariate EDAs. We conclude that use of a Markov Network in a univariate EDA can be of net benefit in defined set of circumstances.	estimation of distribution algorithm;experiment;markov chain;markov random field;mathematical optimization;network model;optimization problem;sampling (signal processing)	Siddhartha Shakya;John A. W. McCall;Deryck Forsyth Brown	2005		10.1145/1068009.1068130	econometrics;mathematical optimization;estimation of distribution algorithm;computer science;artificial intelligence;machine learning;mathematics;statistics;evolutionary computation	Metrics	28.650498060378922	-11.229266102536743	171245
ad44f8d5281b2d951ec4ac5f6c4ea04c926967ea	reject inference, augmentation, and sample selection	modelizacion;analyse risque;estimacion sesgada;selection problem;model selection;sample selection;problema seleccion;credit scoring;scoring credit;prestamo;dato que falta;risk analysis;echantillonnage;error sistematico;inference mechanisms;augmentation;selection modele;sampling;donnee manquante;pret;modelisation;analisis riesgo;loans;seleccion modelo;bias;sample selection model;inferencia;valuacion credito;reject inference;missing data;muestreo;modeling;biased estimation;estimation biaisee;inference;mecanisme inferentiel;erreur systematique;methode score;probleme selection	Many researchers see the need for reject inference in credit scoring models to come from a sample selection problem whereby a missing variable results in omitted variable bias. Alternatively, practitioners often see the problem as one of missing data where the relationship in the new model is biased because the behaviour of the omitted cases differs from that of those who make up the sample for a new model. To attempt to correct for this, differential weights are applied to the new cases. The aim of this paper is to see if the use of both a Heckman style sample selection model and the use of sampling weights, together, will improve predictive performance compared with either technique used alone. This paper will use a sample of applicants in which virtually every applicant was accepted. This allows us to compare the actual performance of each model with the performance of models which are based only on accepted cases. 2006 Elsevier B.V. All rights reserved.	ar (unix);autoregressive model;contingency (philosophy);experiment;feature selection;gigabyte;missing data;modeling language;observable;sampling (signal processing);selection algorithm	John Banasik;Jonathan Crook	2007	European Journal of Operational Research	10.1016/j.ejor.2006.06.072	sampling;econometrics;systems modeling;risk analysis;missing data;bias;data mining;mathematics;model selection;statistics	AI	32.07246315583984	-23.766334716873413	171343
9546b4e9a3bcf8596079c56bc4f6134aa9122638	multilevel monte carlo for reliability theory	multilevel monte carlo;reliability theory;cut sets;system lifetime estimation	As the size of engineered systems grows, problems in reliability theory can become computationally challenging, often due to the combinatorial growth in the cut sets. In this paper we demonstrate how Multilevel Monte Carlo (MLMC) — a simulation approach which is typically used for stochastic differential equation models — can be applied in reliability problems by carefully controlling the bias-variance tradeoff in approximating large system behaviour. In this first exposition of MLMC methods in reliability problems we address the canonical problem of estimating the expectation of a functional of system lifetime and show the computational advantages compared to classical Monte Carlo methods. The difference in computational complexity can be orders of magnitude for very large or complicated system structures.		Louis J. M. Aslett;Tigran Nagapetyan;Sebastian J. Vollmer	2017	Rel. Eng. & Sys. Safety	10.1016/j.ress.2017.03.003	quantum monte carlo;monte carlo method in statistical physics;reliability engineering;econometrics;mathematical optimization;dynamic monte carlo method;hybrid monte carlo;reliability theory;engineering;monte carlo molecular modeling;mathematics;kinetic monte carlo;statistics;monte carlo method	ML	31.581592224935044	-15.434565906453692	171470
3ab5dddd707be0bf918f6ca1821151fe5bd10314	hierarchic entropy: an information theoretic measure of evolutionary robotic behavioral diversity	behavioral diversity;evolutionary robotics;information theory;graph entropy	This paper investigates the design of information theoretic-based ̄tness function for embedded evolutionary robotics (ERs). Such ̄tness relies on the assumption that interesting behaviors result in a high sensorimotor (individual) diversity. The current simple entropy as a diversity metric only considers individuals' di®erence but ignores their spatial relationship. The sensorimotor stream can be analyzed to construct a simple directed graph that has unique entry and exit nodes. This paper proposes a hierarchic entropy as a diversity metric by incorporating the simple entropy and the spatial relationship based graph entropy. Maximizing the hierarchic entropy, achieved by on-board evolutionary algorithm, thus de ̄nes a self-driven ̄tness function enforcing the controller visiting diverse sensorimotor states. The proposed algorithm achieves better performance than the published results of other entropy-based methods only relying on simple entropy, without requiring additional computational resources.	computational resource;directed graph;embedded system;entropy (information theory);evolutionary algorithm;evolutionary robotics;information theory;on-board data handling	Guohua Zhang;Weijia Wang	2017	IJPRAI	10.1142/S0218001417510028	mathematical optimization;information diagram;information theory;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;evolutionary robotics;conditional entropy;statistics	ML	26.528855701844318	-12.057541428617741	171498
03a6280478a2f66da23fda826d62080b18a0e9fc	reliability modeling for humidity sensors subject to multiple dependent competing failure processes with self-recovery	dependent competing failure;humidity sensor;random shocks;reliability model;self-recovery	Recent developments in humidity sensors have heightened the need for reliability. Seeing as many products such as humidity sensors experience multiple dependent competing failure processes (MDCFPs) with self-recovery, this paper proposes a new general reliability model. Previous research into MDCFPs has primarily focused on the processes of degradation and random shocks, which are appropriate for most products. However, the existing reliability models for MDCFPs cannot fully characterize the failure processes of products such as humidity sensors with significant self-recovery, leading to an underestimation of reliability. In this paper, the effect of self-recovery on degradation was analyzed using a conditional probability. A reliability model for soft failure with self-recovery was obtained. Then, combined with the model of hard failure due to random shocks, a general reliability model with self-recovery was established. Finally, reliability tests of the humidity sensors were presented to verify the proposed reliability model. Reliability modeling for products subject to MDCFPs with considering self-recovery can provide a better understanding of the mechanism of failure and offer an alternative method to predict the reliability of products.	compiler;elegant degradation;experiment;exposure to humidity;heart failure;manuscripts;reliability engineering;shock;time of arrival;sensor (device)	Jia Qi;Zhen Zhou;Chenchen Niu;Chunyu Wang;Juan Wu	2018		10.3390/s18082714	humidity;reliability engineering;electronic engineering;engineering	Web+IR	29.379758569161083	-18.613284460519168	171995
2c0cb4d08cedd3e1656de0549e1808f8d12269f0	gamma lifetimes and one-shot device testing analysis	asymptotic confidence intervals;asymptotic estimate;articles;gamma distribution;accelerated life test;binary data;em algorithm;transformation approach;one shot device testing	Gamma distribution is widely used to model lifetime data in reliability and survival analysis. In the context of one-shot device testing, encountered commonly in testing devices such as munitions, rockets, and automobile air-bags, either left- or right-censored data are collected instead of actual lifetimes of the devices under test. The destructive nature of one-shot devices makes it difficult to collect sufficient lifetime information on the devices. For this reason, accelerated life-tests are commonly used in which the test devices are subjected to conditions in excess of its normal use-condition in order to induce more failures, so as to obtain more lifetime information within a relatively short period of time. In this paper, we discuss the analysis of one-shot device testing data under accelerated life-tests based on gamma distribution. Both scale and shape parameters of the gamma distribution are related to stress factors through log–linear link functions. Since lifetimes of devices under this test are censored, the EM algorithm is developed here for the estimation of the model parameters. The inference on the reliability at a specific mission time as well as on the mean lifetime of the devices is also developed. Moreover, by using missing information principle, the asymptotic variance–covariance matrix of the maximum likelihood estimates under the EM framework is determined, and is then used to construct asymptotic confidence intervals for the parameters of interest. For the reliability at a specific mission time and also for the mean lifetime of the devices, transformation approaches are proposed for the construction of confidence intervals. These confidence intervals are then compared through a simulation study in terms of coverage probabilities and average widths. Recommendations are then made for an appropriate approach for the construction of confidence intervals for different sample sizes and different levels of reliability. A distance-based statistic is suggested for testing the validity of the model to an observed data. Finally, since current status data with covariates in survival analysis and one-shot device testing data with stress factors in reliability analysis share the same data structure, a real data from a toxicological study is used to illustrate the developed methods.		Narayanaswamy Balakrishnan;Man Ho Ling	2014	Rel. Eng. & Sys. Safety	10.1016/j.ress.2014.01.009	reliability engineering;gamma distribution;econometrics;expectation–maximization algorithm;engineering;mathematics;statistics	SE	29.844340980192047	-19.647357078967975	172050
1bfeb90ec81836c95e04f8582ccd9ac72ead01ab	gaussian process surrogates for failure detection: a bayesian experimental design approach	experimental design;uncertainty quantification;gaussian processes;failure detection;bayesian inference;response surfaces;monte carlo	An important task of uncertainty quantification is to identify the probability of undesired events, in particular, system failures, caused by various sources of uncertainties. In this work we consider the construction of Gaussian process surrogates for failure detection and failure probability estimation. In particular, we consider the situation that the underlying computer models are extremely expensive, and in this setting, determining the sampling points in the state space is of essential importance. We formulate the problem as an optimal experimental design for Bayesian inferences of the limit state (i.e., the failure boundary) and propose an efficient numerical scheme to solve the resulting optimization problem. In particular, the proposed limit-state inference method is capable of determining multiple sampling points at a time, and thus it is well suited for problems where multiple computer simulations can be performed in parallel. The accuracy and performance of the proposed method is demonstrated by both academic and practical examples.		Hongqiao Wang;Guang Lin;Jinglai Li	2016	J. Comput. Physics	10.1016/j.jcp.2016.02.053	econometrics;mathematical optimization;uncertainty quantification;gaussian process;mathematics;design of experiments;bayesian inference;statistics;monte carlo method	ML	31.18457832169832	-16.432571613042285	172054
4407c0d4a6bd256fbb056b5841e767a5099aa0a5	non-parametric contextual stochastic search	stochastic processes;covariance matrices;robots;linear programming;h671 robotics;g760 machine learning;search problems;entropy;context	Stochastic search algorithms are black-box optimizer of an objective function. They have recently gained a lot of attention in operations research, machine learning and policy search of robot motor skills due to their ease of use and their generality. Yet, many stochastic search algorithms require relearning if the task or objective function changes slightly to adapt the solution to the new situation or the new context. In this paper, we consider the contextual stochastic search setup. Here, we want to find multiple good parameter vectors for multiple related tasks, where each task is described by a continuous context vector. Hence, the objective function might change slightly for each parameter vector evaluation of a task or context. Contextual algorithms have been investigated in the field of policy search, however, the search distribution typically uses a parametric model that is linear in the some hand-defined context features. Finding good context features is a challenging task, and hence, non-parametric methods are often preferred over their parametric counter-parts. In this paper, we propose a non-parametric contextual stochastic search algorithm that can learn a non-parametric search distribution for multiple tasks simultaneously. In difference to existing methods, our method can also learn a context dependent covariance matrix that guides the exploration of the search process. We illustrate its performance on several non-linear contextual tasks.	bandwidth (signal processing);black box;loss function;machine learning;mathematical optimization;nonlinear system;operations research;optimization problem;parametric model;parametric search;robot learning;search algorithm;stochastic optimization;usability	Abbas Abdolmaleki;Nuno Lau;Luís Paulo Reis;Gerhard Neumann	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759411	robot;entropy;mathematical optimization;simulation;computer science;linear programming;artificial intelligence;machine learning;incremental heuristic search	Robotics	25.92925595410242	-14.315534696881484	172198
e5aaced3046ae59fcf202b0e1c494ec17595f5b2	the moments of the mixel distribution and its application to statistical image classification	metodo estadistico;mixed;estimacion densidad;estimation densite;mixto;image classification;image;statistical method;classification;photographie par satellite;statistical properties;density estimation;imagen;methode statistique;calcul numerique;numerical computation;pixel;calculo numerico;algorithme em;satellite imagery;algoritmo em;fotografia por satelite;em algorithm;satellite photography;clasificacion;mixte	"""The mixel is a heterogeneous pixel that contains multiple constituents within a single pixel, and the statistical properties of a population of mixels can be characterized by the mixel distribution. Practically this model has a drawback that it cannot be represented in closed form, and prohibitive numerical computation is required for mixture density estimation problem. Our discovery however shows that the """"moments"""" of the mixel distribution can be derived in closed form, and this solution brings about significant reduction of computation cost for mixture density estimation after slightly modifying a typical algorithm. We then show the experimental result on satellite imagery, and find out that the modified algorithm runs more than 20 times faster than our previous method, but suffers little deterioration in classification performance."""		Asanobu Kitamoto	2000		10.1007/3-540-44522-6_54	econometrics;contextual image classification;density estimation;expectation–maximization algorithm;biological classification;computer science;machine learning;image;mathematics;pixel;statistics	Vision	34.53578021843059	-23.77785948147955	172224
778464cc89dfd916e0bf3f4c36bc4beea7ec03d5	an evaluation method of the number of monte carlo sta trials for statistical path delay analysis	sta;tecnologia electronica telecomunicaciones;evaluation method;ranking;ssta;timing analysis;tecnologias;monte carlo;grupo a;monte carlo sta;lower bound	We present an evaluation method for estimating the lower bound number of Monte Carlo STA trials required to obtain at least one sample which falls within top-k % of its parent population. The sample can be used to ensure that target designs are timing-error free with a predefined probability using the minimum computational cost. The lower bound number is represented as a closed-form formula which is general enough to be applied to other verifications. For validation, Monte Carlo STA was carried out on various benchmark data including ISCAS circuits. The minimum number of Monte Carlo runs determined using the proposed method successfully extracted one or more top-k % delay instances.	monte carlo method	Masanori Imai;Takashi Sato;Noriaki Nakayama;Kazuya Masu	2008	IEICE Transactions	10.1093/ietfec/e91-a.4.957	econometrics;hybrid monte carlo;markov chain monte carlo;ranking;computer science;mathematics;upper and lower bounds;monte carlo integration;static timing analysis;algorithm;statistics;monte carlo method	EDA	30.312320751796438	-17.790025280281	172435
ba25792acc4ccdc101b536847d50b78371e50b07	unbiased bootstrap error estimation for linear discriminant analysis	signal image and speech processing;systems biology;computational biology bioinformatics;biomedical engineering	Convex bootstrap error estimation is a popular tool for classifier error estimation in gene expression studies. A basic question is how to determine the weight for the convex combination between the basic bootstrap estimator and the resubstitution estimator such that the resulting estimator is unbiased at finite sample sizes. The well-known 0.632 bootstrap error estimator uses asymptotic arguments to propose a fixed 0.632 weight, whereas the more recent 0.632+ bootstrap error estimator attempts to set the weight adaptively. In this paper, we study the finite sample problem in the case of linear discriminant analysis under Gaussian populations. We derive exact expressions for the weight that guarantee unbiasedness of the convex bootstrap error estimator in the univariate and multivariate cases, without making asymptotic simplifications. Using exact computation in the univariate case and an accurate approximation in the multivariate case, we obtain the required weight and show that it can deviate significantly from the constant 0.632 weight, depending on the sample size and Bayes error for the problem. The methodology is illustrated by application on data from a well-known cancer classification study.	approximation;bootstrapping (statistics);computation;gene expression;linear discriminant analysis;neoplasms;normal statistical distribution;population;sample size;whole earth 'lectronic link	Thang T. Vu;Chao Sima;Ulisses M. Braga-Neto;Edward R. Dougherty	2014		10.1186/s13637-014-0015-0	efficient estimator;biology;minimax estimator;mathematical optimization;minimum-variance unbiased estimator;stein's unbiased risk estimate;computer science;bioinformatics;mean squared error;biological engineering;bias of an estimator;consistent estimator;systems biology	ML	29.727831722154516	-22.381687101475112	172778
a300881d364ac361c99c24341a93107974535f82	on the conjecture of kochar and korwar	theorie echantillonnage;metodo estadistico;teoria muestreo;analyse multivariable;parametre dispersion;sample size;exponential distribution;normalized;scale parameter;order statistic;multivariate analysis;variable independante;ley exponencial;variable aleatoire;fonction repartition;loi exponentielle;tamano muestra;relacion orden;primary;statistique ordre;estimation non parametrique;variable aleatoria;taille echantillon;ordering;statistical method;ecuesta estadistica;hazard rate ordering;non parametric estimation;primary 60e15;funcion distribucion;sample survey;relation ordre;distribution function;heterogeneous exponential distribution hazard rate order normalized spacing;secondary 60k10;methode statistique;secondary;estadistica orden;random variable;62d05;fonction repartition empirique;analisis multivariable;variable independiente;normalized spacing;62g30;estimacion no parametrica;estimation statistique;estimacion estadistica;statistical estimation;heterogeneous exponential distribution;sondage statistique;hazard rate order;independent variable;parametro dispersion;sampling theory	In this article, we partially solve a conjecture by Kochar and Korwar (1996) [9] in relation to the normalized spacings of the order statistics of a sample of independent exponential random variables with different scale parameters. In the case of a sample of size n = 3, they proved the ordering of the normalized spacings and conjectured that result holds for all n. We prove this conjecture for n = 4 for both spacings and normalized spacings and generalize some results to n > 4. © 2009 Elsevier Inc. All rights reserved.	time complexity	Nuria Torrado;Rosa E. Lillo;Michael P. Wiper	2010	J. Multivariate Analysis	10.1016/j.jmva.2009.11.006	variables;sample size determination;exponential distribution;random variable;econometrics;order statistic;scale parameter;order theory;survey sampling;distribution function;normalization;calculus;mathematics;multivariate analysis;statistics	Theory	33.22273166081522	-20.596243844083133	173024
1932ee6b432ce93df923aaad4ee53378bd70bbe8	a comparison of semiparametric estimators for the ordered response model	metodo cuadrado menor;job satisfaction;methode moindre carre;metodo monte carlo;least squares method;analisis datos;variable dependante;semi parametric estimation;metodo semiparametrico;methode monte carlo;semiparametric estimation;methode semiparametrique;satisfaccion profesional;word order;environmental protection;protection environnement;dependent variable;data analysis;ordered response models;monte carlo method;satisfaction professionnelle;statistical computation;calculo estadistico;semiparametric method;least squares estimate;analyse donnee;calcul statistique;modele reponse;proteccion medio ambiente;willingness to pay;monte carlo;response model	Three semi-parametric estimators for ordered response models are compared: a “semi-nonparametric” series estimator, an estimator based on a polynomial transformation to normality, and a least squares estimator for a transformed dependent variable. The empirical estimates are found to differ markedly across estimators in a job satisfaction illustration presented, but much less so in the second illustration, a model of willingness to pay for environmental protection. Monte Carlo evidence comparing the three estimators is also presented.	least squares;monte carlo method;ordered probit;polynomial;polynomial-time reduction;semiconductor industry;semiparametric model;single-index model	Mark B. Stewart	2005	Computational Statistics & Data Analysis	10.1016/j.csda.2004.05.027	efficient estimator;econometrics;estimator;extremum estimator;calculus;mathematics;invariant estimator;statistics;monte carlo method	ML	33.01952710303834	-23.009645790995496	173043
1d0985b0bbbfa973c6f897a71bcd5ea5907b6f2f	using sectioning to construct confidence intervals for quantiles when applying importance sampling	estimation theory;financial management;importance sampling;monte carlo method;vrt;confidence interval;finance;importance sampling;quantile estimator;risk measurement;sectioning technique;values-at-risk;variance-reduction technique	Quantiles, which are known as values-at-risk in finance, are often used to measure risk. Confidence intervals provide a way of assessing the error of quantile estimators. When estimating extreme quantiles using crude Monte Carlo, the confidence intervals may have large half-widths, thus motivating the use of variance-reduction techniques (VRTs). This paper develops methods for constructing confidence intervals for quantiles when applying the VRT importance sampling. The confidence intervals, which are asymptotically valid as the number of samples grows large, are based on a technique known as sectioning. Empirical results seem to indicate that sectioning can lead to confidence intervals having better coverage than other existing methods.	importance sampling;monte carlo method;sampling (signal processing);variance reduction	Marvin K. Nakayama	2012	Proceedings Title: Proceedings of the 2012 Winter Simulation Conference (WSC)		financial management;econometrics;mathematical optimization;importance sampling;mathematics;estimation theory;cdf-based nonparametric confidence interval;robust confidence intervals;statistics;variance reduction	ML	29.862877589064272	-21.41102052304725	173246
310712d0626267c97934b66b190fb8eca09a068b	admissible estimator of the eigenvalues of the variance-covariance matrix for multivariate normal distributions	equation non lineaire;secondary 62f10;primary 62c15;ecuacion no lineal;metodo estadistico;admissible estimator;analyse multivariable;analisis numerico;karlin s method;ecuacion trascendente;covariance analysis;multivariate analysis;ley n variables;primary;62h10;ecuacion algebraica;variance analysis;matrice covariance;covariance matrix wishart distribution squared error loss karlin s method;equation transcendante;estimador admisible;statistical method;matriz covariancia;curva gauss;distribucion estadistica;statistical regression;eigenvector;analyse numerique;eigenvalue;loi wishart;vector propio;numerical analysis;estimation erreur;squared error loss;analyse covariance;distribution statistique;error estimation;methode statistique;transcendental equation;analisis variancia;multivariate normal distribution;estimateur admissible;regresion estadistica;secondary;valor propio;62j10;estimacion error;algebra lineal numerica;algebre lineaire numerique;loi normale;invariante;analisis multivariable;multivariate distribution;equation algebrique;valeur propre;numerical linear algebra;wishart distribution;analisis covariancia;non linear equation;regression statistique;loi n variables;algebraic equation;65f15;statistical distribution;65h17;vecteur propre;gaussian distribution;invariant;scale invariance;analyse variance;covariance matrix;variance;variancia	An admissible estimator of the eigenvalues of the variance-covariance matrix is given for multivariate normal distributions with respect to the scale-invariant squared error loss. AMS(2000) Subject Classification: Primary 62C15; Secondary 62F10	mean squared error	Yo Sheena;Akimichi Takemura	2011	J. Multivariate Analysis	10.1016/j.jmva.2010.12.007	scatter matrix;efficient estimator;matrix t-distribution;econometrics;multivariate normal distribution;minimum-variance unbiased estimator;normal-wishart distribution;spectrum of a matrix;eigenvalues and eigenvectors;trimmed estimator;calculus;mathematics;mean squared error;statistics	Theory	32.780328749561015	-22.90004870479072	173351
2a0848396f54264f7ed92a6ba77b8e1c59cba05a	the information-cost-reward framework for understanding robot swarm foraging	swarm robotics;foraging;modelling;information flow	Demand for autonomous swarms, where robots can cooperate with each other without human intervention, is set to grow rapidly in the near future. Currently, one of the main challenges in swarm robotics is understanding how the behaviour of individual robots leads to an observed emergent collective performance. In this paper, a novel approach to understanding robot swarms that perform foraging is proposed in the form of the Information-Cost-Reward (ICR) framework. The framework relates the way in which robots obtain and share information (about where work needs to be done) to the swarm’s ability to exploit that information in order to obtain reward efficiently in the context of a particular task and environment. The ICR framework can be applied to analyse underlying mechanisms that lead to observed swarm performance, as well as to inform hypotheses about the suitability of a particular robot control strategy for new swarm missions. Additionally, the information-centred understanding that the framework offers paves a way towards a new swarm design methodology where general principles of collective robot behaviour guide algorithm design.	algorithm selection;algorithm design;autonomous robot;cognition;collective intelligence;control theory;distributed computing;emergence;expectation propagation;exploit (computer security);high- and low-level;information flow (information theory);intelligent character recognition;mathematical optimization;nonlinear system;particle swarm optimization;robot control;swarm intelligence;swarm robotics	Lenka Pitonakova;Richard M. Crowder;Seth Bullock	2017	Swarm Intelligence	10.1007/s11721-017-0148-3	swarm behaviour;machine learning;artificial intelligence;computer science;robot control;robot;algorithm design;design methods;exploit;foraging;swarm robotics	Robotics	25.816441763983633	-12.419113812736706	173436
1a24472bc9aff973f1b827bc23b8dc39bfd185e1	an entropy search portfolio for bayesian optimization		Bayesian optimization is a sample-efficient method for black-box global optimization. However, the performance of a Bayesian optimization method very much depends on its exploration strategy, i.e. the choice of acquisition function, and it is not clear a priori which choice will result in superior performance. While portfolio methods provide an effective, principled way of combining a collection of acquisition functions, they are often based on measures of past performance which can be misleading. To address this issue, we introduce the Entropy Search Portfolio (ESP): a novel approach to portfolio construction which is motivated by information theoretic considerations. We show that ESP outperforms existing portfolio methods on several real and synthetic problems, including geostatistical datasets and simulated control tasks. We not only show that ESP is able to offer performance as good as the best, but unknown, acquisition function, but surprisingly it often gives better performance. Finally, over a wide range of conditions we find that ESP is robust to the inclusion of poor acquisition functions.	bayesian optimization;esp game;experiment;information theory;mathematical optimization;synthetic intelligence	Bobak Shahriari;Ziyu Wang;Matthew W. Hoffman;Alexandre Bouchard-Côté;Nando de Freitas	2014	CoRR		computer science;artificial intelligence;machine learning;data mining;statistics	ML	25.596063148636897	-14.582927368732072	173768
1cd0e5ddfc2f023ae52563719fdc3584eeddd2f3	estimation of parameter sensitivities for stochastic reaction networks using tau-leap simulations		We consider the important problem of estimating parameter sensitivities for stochastic models of reaction networks that describe the dynamics as a continuous time Markov process over a discrete lattice. These sensitivity values are useful for understanding network properties, validating their design, and identifying the pivotal model parameters. Many methods for sensitivity estimation have been developed, but their computational feasibility suffers from the critical bottleneck of requiring time-consuming Monte Carlo simulations of the exact reaction dynamics. To circumvent this problem, one needs to devise methods that speed up the computations while suffering acceptable and quantifiable loss of accuracy. We develop such a method by first deriving a novel integral representation of parameter sensitivity and then demonstrating that this integral may be approximated by any convergent tau-leap method. Our method is easy to implement and works with any tau-leap simulation scheme, and its accuracy is proved to...	computer simulation	Ankit Gupta;Muruhan Rathinam;Mustafa Khammash	2018	SIAM J. Numerical Analysis	10.1137/17M1119445	mathematical optimization;stochastic modelling;computation;monte carlo method;mathematics;speedup;markov process;bottleneck;reaction dynamics	Theory	33.60412659813153	-12.80800663009306	173806
0e5643dea06ae9e622616d537c3c00f5ad129a22	selective scalable secret image sharing with adaptive pixel-embedding technique		Different from secret image sharing technique, the secret of a scalable secret image sharing is displayed in the way that it could be progressively recovered by a set of shares. In other word, incomplete gathering of shadows cannot be used to reconstruct the whole image S immediately. To improve the security of SSIS, Lee and Chen have designed a selective scalable secret image sharing mechanism (SSSIS) to reduce the awareness of malicious attackers. Nevertheless, the quality of Lee and Chen’s scheme is not good due to the image distortion and storage overhead of static embedding. Thus, we introduce the concept of adaptive pixel-embedding into SSSIS, in which the embedded bits could be uniformly distributed in the stego image. Aside from the human vision perception, experimental results have demonstrated the superiority of new method over related works in terms of two objective indexes, including peak signal to noise ratio (PSNR) and structural similarity (SSIM).	distortion;embedded system;entity–relationship model;overhead (computing);peak signal-to-noise ratio;pixel;sql server integration services;scalability;secret sharing;selectivity (electronic);steganography;structural similarity	Ying-Chin Chen;Jung-San Lee;Hong-Chi Su	2018	Multimedia Tools and Applications	10.1007/s11042-018-5908-6	computer vision;pixel;steganography;computer science;scalability;peak signal-to-noise ratio;structural similarity;artificial intelligence;distortion;image sharing;secret sharing	Graphics	39.10555240858457	-11.138740745841135	173858
fcce0a44412c4fee93d4862e269ed5377d1bd522	the expected cost of a dual-hypothesis test (corresp.)	bayes procedures;hypothesis test			N. Thomas Gaarder	1966	IEEE Trans. Information Theory	10.1109/TIT.1966.1053844	econometrics;statistical hypothesis testing;bayes factor;test statistic;score test;mathematics;exact test;statistics	Theory	31.01399052884622	-21.385679923063357	174170
e7be67be9c3db63697d2bed689c863ae3d4344ee	exact representation of the second-order moments for resubstitution and leave-one-out error estimation for linear discriminant analysis in the univariate heteroskedastic gaussian model	genomics;resubstitution;rms;error estimation;heteroskedasticity;linear discriminant analysis;leave one out	This paper provides exact analytical expressions for the bias, variance, and RMS for the resubstitution and leave-one-out error estimators in the case of linear discriminant analysis (LDA) in the univariate heteroskedastic Gaussian model. Neither the variances nor the sample sizes for the two classes need be the same. The generality of heteroskedasticity (unequal variances) is a fundamental feature of the work presented in this paper, which distinguishes it from past work. The expected resubstitution and leaveone-out errors are represented by probabilities involving bivariate Gaussian distributions. Their second moments and cross-moments with the actual error are represented by 3and 4-variate Gaussian distributions. From these, the bias, deviation variance, and RMS for resubstitution and leave-one-out as estimators of the actual error can be computed. The RMS expressions are applied to the determination of sample size and illustrated in biomarker classification. & 2011 Elsevier Ltd. All rights reserved.	bivariate data;leave-one-out error;linear discriminant analysis;stationary process	Amin Zollanvari;Ulisses M. Braga-Neto;Edward R. Dougherty	2012	Pattern Recognition	10.1016/j.patcog.2011.08.006	econometrics;genomics;root mean square;computer science;machine learning;pattern recognition;mathematics;heteroscedasticity;linear discriminant analysis;statistics	ML	30.48848144996704	-22.03652306336181	174233
b8afd1b1e149541be6b4a463382f94ce3862cec3	a flexible spatial scan test for case event data	processus ponctuel;62m30;metodo estadistico;spatial process;stochastic process;analisis datos;processus spatial;fonction repartition;relacion orden;biometrie;point process;biometrics;biometria;statistique balayage;60g55;ordering;statistical method;medical science;spatial point process;epidemiology;funcion distribucion;relation ordre;data analysis;distribution function;ciencia medica;methode statistique;epidemiologia;statistical computation;calculo estadistico;processus stochastique;proceso puntual;analyse donnee;calcul statistique;estimation statistique;proceso estocastico;scan statistic;spatial clustering;estimacion estadistica;60e05;statistical estimation;62p10;epidemiologic studies;epidemiologie;science medicale	A new method is proposed for identifying clusters in spatial point processes. It relies on a specific ordering of events and the definition of area spacings which have the same distribution as one-dimensional spacings. Then the spatial clusters are detected using a scan statistic adapted to the analysis of one-dimensional point processes. This flexible spatial scan test seems to be very powerful against any arbitrarily-shaped cluster alternative. These results have applications in epidemiological studies of rare diseases. © 2008 Elsevier B.V. All rights reserved.		Lionel Cucala	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.10.008	stochastic process;econometrics;epidemiology;order theory;distribution function;point process;mathematics;data analysis;biometrics;statistics	AI	34.72691000421691	-22.711722804829783	174803
91e8f004e10f60faf411d8f24cd5db385c23857e	detection of commercial offset printing using an adaptive software architecture for the dft	printing;image segmentation;authentication;image color analysis;feature extraction;software algorithms;discrete fourier transforms	The way how we interact with banknotes is changing. This raises questions on how we interact with electronic payment systems. The general idea is to design low-cost electronics for cash handling systems. We establish a prototypical demonstrator which allows a consistent image capture quality and is able to handle complex algorithms for banknote authentication on cost-effective hardware. Therefore, tasks regarding reducing the evaluation time, without diminishing the reliability of the algorithms have to be considered. In this contribution we focus on the re-design of an authentication module for detection of commercial offset printing. This module analyses images in view to periodic printing patterns by means of the Discrete Fourier Transform (DFT). We propose to implement two concepts: an adaptive software architecture for DFT and parallel image processing. The re-design reduces evaluation time, without compromising the reliability of the authentication algorithm.	algorithm;discrete fourier transform;feature extraction;image processing;knowledge-based authentication;microsoft outlook for mac;printing;software architecture;switzerland	Anton Pfeifer;Volker Lohweg	2016	2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)	10.1109/ETFA.2016.7733660	embedded system;computer vision;telecommunications;feature extraction;computer science;operating system;machine learning;authentication;image segmentation;computer graphics (images)	Robotics	36.930195842760384	-13.016810092959723	175593
c781ff6e5d9ee803e580926cd145d7b09199b3f6	manipulating the energy landscape in stochastic relaxation : novel techniques and simulations			linear programming relaxation;simulation	Peter Burge	1998				Robotics	34.57924651748354	-12.945935456411853	175634
0fd9b432706466ab38c003fcf17c72d834345aae	on statistics of log-ratio of arithmetic mean to geometric mean for nakagami-m fading power	moment generating function;maximum likelihood;gamma distribution	To assess the performance of maximum-likelihood (ML) based Nakagami m parameter estimators, current methods rely on Monte Carlo simulation. In order to enable the analytical performance evaluation of ML-based m parameter estimators, we study the statistical properties of a parameter Δ, which is defined as the log-ratio of the arithmetic mean to the geometric mean for Nakagami-m fading power. Closed-form expressions are derived for the probability density function (PDF) of Δ. It is found that for large sample size, the PDF of Δ can be well approximated by a two-parameter Gamma PDF. key words: Gamma distribution, maximum likelihood, moment generating function, Nakagami m parameter estimation	approximation algorithm;estimation theory;gamma correction;monte carlo method;performance evaluation;portable document format;simulation	Ning Wang;Julian Cheng;Chintha Tellambura	2012	IEICE Transactions		gamma distribution;econometrics;mathematical optimization;mathematics;maximum likelihood;moment-generating function;statistics	Vision	30.949162992538707	-20.45514337820842	175649
2b30f117e1df4ce74e74f7847aeccdcda1e4628b	statistical approach to measure stylistic centrality			centrality	Peter Zörnig;Ioan-Iovitz Popescu;Gabriel Altmann	2015	Glottometrics		centrality;econometrics;mathematics	NLP	31.21359751411269	-23.753305843267988	175842
c96a583b683ec23812dc217aa1d076bf68f0e7f0	test for randomness of the technology parameter in a stochastic frontier regression model	test for randomness;stochastic frontier models;regression model;stochastic frontier;technical efficiency;random coefficient models;electric utilities;asymptotic distribution;stochastic frontier model;random coefficient model	This paper considers the problem of testing for randomness of the technology parameter in a stochastic frontier regression model. A test statistic is proposed and its asymptotic distribution theory is discussed. Simulation results show that the proposed test maintains its level and also quite powerful against various alternatives. An empirical investigation has been carried out by applying the suggested test procedure to the data set on electric utility companies. The results are consistent with the general finding that the technology parameter of the stochastic frontier model used for modeling these data is random.	randomness tests;simulation	T. V. Ramanathan;Chanchala Ghadge	2010	Statistical Methods and Applications	10.1007/s10260-009-0129-9	econometrics;stochastic modelling;stochastic optimization;randomness tests;data envelopment analysis;mathematics;mathematical economics;asymptotic distribution;regression analysis;statistics	SE	26.951285537934417	-20.945455094943117	176237
a62997cb218c4fa7611fbccbbac0479d5f8e0e86	a flexible extreme value mixture model	bayes estimation;bandwidth estimation;selection problem;problema seleccion;cuantila;statistical simulation;methode parametrique;estimacion densidad;analisis datos;melange loi probabilite;metodo parametrico;estimation densite;evenement rare;bayesian inference;parametric method;estimation non parametrique;queue lourde;bandwith selection;prior information;kernel density;seleccion de la ventana;mixed distribution;62g07;62g32;estimateur noyau;estadistica rango;heavy tail;kernel estimation;extreme values;quantile estimation;queue distribution;estimation parametrique;non parametric estimation;extreme value theory;density estimation;62f07;data analysis;estimacion bayes;60g70;simulacion estadistica;informacion a priori;cola distribucion;extreme value;mixture model;rare event;threshold selection;simulation statistique;62f15;estimation noyau;statistical computation;valeur extreme;acontecimiento rara;calculo estadistico;kernel density estimate;choix fenetre;heavy tailed distribution;rank statistic;mezcla ley probabilidad;simulation study;analyse donnee;statistique rang;extreme values mixture model kernel density threshold selection;calcul statistique;quantile;estimacion no parametrica;05c78;estimation statistique;cola pesada;estimacion estadistica;statistical estimation;information a priori;kernel estimator;distribution tail;valor extremo;estimation bayes;probleme selection	Extreme value theory is used to derive asymptotically motivated models for unusual or rare events, e.g. the upper or lower tails of a distribution. A new flexible extreme value mixture model is proposed combining a non-parametric kernel density estimator for the bulk of the distribution with an appropriate tail model. The complex uncertainties associated with threshold choice are accounted for and new insights into the impact of threshold choice on density and quantile estimates are obtained. Bayesian inference is used to account for all uncertainties and enables inclusion of expert prior information, potentially overcoming the inherent sparsity of extremal data. A simulation study and empirical application for determining normal ranges for physiological measurements for pre-term infants is used to demonstrate the performance of the proposed mixture model. The potential of the proposed model for overcoming the lack of consistency of likelihood based kernel bandwidth estimators when faced with heavy tailed distributions is also demonstrated.	maxima and minima;mixture model	A. MacDonald;Carl John Scarrott;D. Lee;B. Darlow;Marco Reale;G. Russell	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2011.01.005	kernel density estimation;econometrics;pattern recognition;extreme value theory;mathematics;statistics	ML	32.82100156709755	-23.051844716958854	176379
8571a537db086e31a00bb1c3f73682987d4fc563	quantifying reliability uncertainty from catastrophic and margin defects: a proof of concept	catastrophic and margins failure modes;bootstrap method;method of moments;system reliability;uncertainty quantification;bootstrap;failure mode;proof of concept;method of moment;bayesian analysis	We aim to analyze the effects of component level reliability data, including both catastrophic failures and margin failures, on system level reliability. While much work has been done to analyze margins and uncertainties at the component level, a gap exists in relating this component level analysis to the system level. We apply methodologies for aggregating uncertainty from component level data to quantify overall system uncertainty. We explore three approaches towards this goal, the classical Method of Moments (MOM), Bayesian, and Bootstrap methods. These three approaches are used to quantify the uncertainty in reliability for a system of mixed series and parallel components for which both pass/fail and continuous margin data are available. This paper provides proof of concept that uncertainty quantification methods can be constructed and applied to system reliability problems. In addition, application of these methods demonstrates that the results from the three fundamentally different approaches can be quite comparable. & 2011 Elsevier Ltd. All rights reserved.	complex system;computation;embedded system;failure cause;margin (machine learning);margin classifier;norm (social);population;quantification of margins and uncertainties;resampling (statistics);uncertainty quantification	Christine M. Anderson-Cook;Stephen Crowder;Aparna V. Huzurbazar;John Lorio;James Ringland;Alyson G. Wilson	2011	Rel. Eng. & Sys. Safety	10.1016/j.ress.2010.10.006	reliability engineering;econometrics;uncertainty quantification;uncertainty analysis;method of moments;bayesian probability;engineering;mathematics;proof of concept;sensitivity analysis;failure mode and effects analysis;statistics	SE	28.513441592626556	-17.764831595538364	176762
8fad2a289edfa8d7a06b13f7e8b229be971e41e6	"""erratum to """"psn-toolkit - a collection of computer intensive statistical methods for non-linear mixed effect modeling using nonmem"""" [comput. methods prog. biomedicine 79 (2005) 241-257]"""	mixed effects model;statistical method;computational method;psn		computer programming;nonlinear system	Lars Lindbom;Pontus Pihlgren;E. Niclas Jonsson	2005	Computer Methods and Programs in Biomedicine	10.1016/j.cmpb.2005.10.001	mixed model;computational science;econometrics;computer science;mathematics;statistics	Theory	34.41658050773291	-14.613531969937071	176999
fd0252988abbd229380fbc5290a2f27ad3943996	sequential allocation of sampling budgets in unknown environments	foraging strategy sequential sampling budgets allocation ecological models foraging scientific observations;sampling methods mobile robots;random variables robots uncertainty biological system modeling algorithm design and analysis standards soil	This paper presents an algorithm based on ecological models of foraging and that uses uncertainty in scientific observations made by the robot to value future actions. It is the hypothesis of this work that the foraging strategy will be an improvement over strategies based on principles from the design of experiments literature for small budget sizes. The experiment in this paper shows that for small budget sizes the new algorithm performs at least as well as other methods. However the new algorithm does not exhaust its sampling budget by default and thus needs to be modified to outperform other approaches for large sample budgets, which provides opportunity for future work in this area.	algorithm;baseline (configuration management);design of experiments;experiment;multi-armed bandit;robot;sampling (signal processing);simulation	P. Michael Furlong;David Wettergreen	2014	2014 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2014.6906603	simulation	Robotics	24.77428897529761	-17.120706128838492	177038
5bec57e9f3ed919facbc897d053251996036e510	nomark: a novel method for copyright protection of digital videos without embedding data	illegal distribution;digital watermarking;histograms;watermarking;social networking;watermark attack;authorisation;digital watermark;copyright infringement;data embedding;copyright;digital media content;averaged scene image;unauthorized scene insertion;averaged scene image video content protection visual cryptography scene change;copyright protection;visual cryptography;social network;visualization;digital media;data privacy;video content protection;image color analysis;cryptography;nomark method;pixel;media sharing site;social networking online;content protection;social networking online authorisation copyright cryptography data privacy image watermarking;digital video;image watermarking;video watermarking;data embedding copyright protection digital video digital media content social networking media sharing site copyright infringement illegal distribution digital watermarking watermark attack nomark method visual cryptography unauthorized scene insertion video watermarking image watermarking;videos;scene change;videos watermarking histograms image color analysis cryptography visualization pixel	Copyright protection of digital media content is becoming increasingly important with increasing potential for content monetization which is attributable to digital convergence. High volumes of uploaded content in social networking and media sharing sites makes it increasingly difficult to monitor all the uploaded content for copyright infringement, piracy and illegal distribution. Currently, digital watermarking is predominantly used for copyright protection of images and videos. Watermarking involves embedding data into an image or a video in an invisible or visible manner. This traditional method is prone to many known watermark attacks aimed at detecting and distorting or destroying the embedded data. In this paper, we present the No Mark Method, a robust, reliable, and efficient scheme for copyright protection of digital video content without embedding any data in to it. The method is based on visual cryptography and can be used to watermark individual scenes in a video and hence protects not only the copyright of complete video but also of each individual scene in the video. The method is robust against video fragmentation attack, unauthorized insertion of scenes from a protected video into another video and various other video watermarking attacks. There is no involvement of any notary agency to establish copyright ownership of a digital video.	authorization;computation;confidentiality;cuecat;data compression;dictionary attack;digital media;digital signature;digital video;digital watermarking;distortion;embedded system;encryption;ip fragmentation attack;interpolation;key (cryptography);lossy compression;paging;sensor;visual cryptography;website monetization	Aditya Vashistha;Rajarathnam Nallusamy;Sanjoy Paul	2010	2010 IEEE International Symposium on Multimedia	10.1109/ISM.2010.32	information privacy;digital watermarking;computer science;electrical engineering;video tracking;internet privacy;world wide web;computer security;statistics	Graphics	38.06711502198681	-12.610509294569471	177354
1ccaab5fe3de34a4197195812459a9830c28e65f	contact networks and the spread of mrsa in stockholm hospitals	social networks analysis epidemiological modelling healthcare associated infections;epidemiological modelling;probability;hospitals;social networks contact network analysis mrsa spread stockholm hospitals meticillin resistant staphylococcus aureus network structure disease transition matrixes estimator matrixes of probability statistical property mcqmc method markov chain quasimonte carlo method artificial networks;statistics;mathematical model;diseases;healthcare associated infections;social networking online diseases hospitals markov processes medical computing monte carlo methods probability;hospitals diseases mathematical model sociology probability statistics;sociology;social networks analysis	We have been studying the spread of meticillin (multi) resistant Staphylococcus Aureus (MRSA). In the course of these studies, we investigated information about all patient visits within Stockholm County during the outbreak period, with registry over all diagnosed MRSA cases. As part of this project, we developed methods for analyzing the contact network of persons who had visited the same care facility, as well as methods for analyzing the affects of network structure on the transmission of MRSA. We also studied matrixes of disease transition in hospital populations (infected patients vs. Merely contagious patients). In the stationary case: (a) we have estimator matrixes of probabilities and other statistical properties of the contact networks. In the time evolution case: (b) we divided the outbreak into smaller, periodic intervals and looked at how MRSA spread over time. The MCqMC (Markov chain quasi Monte Carlo) method and artificial networks (whose main parameter is the number of contacts during a specific time interval) help us to understand real and simulated-paths of disease transition. Probability matrices (b) were used to find the mechanism of change states (vectors of all population, 0-susceptible or 1-infected).	emoticon;markov chain;monte carlo method;population;stationary process	Andrzej Jarynowski;Fredrik Liljeros	2015	2015 Second European Network Intelligence Conference	10.1109/ENIC.2015.30	simulation;medicine;artificial intelligence;operations research	ML	32.19545637648041	-13.21199038930954	177869
45b0e5966c91896e664f87da1bedf5ea921144e0	reliability analysis for electronic devices using beta-weibull distribution		Today, in reliability analysis, the most used distribution to describe the behavior of electronic products under voltage profiles is the Weibull distribution. Nevertheless, the Weibull distribution does not provide a good fit to lifetime datasets that exhibit bathtub-shaped or upside-down bathtub–shaped (unimodal) failure rates, which are often encountered in the reliability analysis of electronic devices. In this paper, a reliability model based on the beta-Weibull distribution and the inverse power law is proposed. This new model provides a better approach to model the performance and fit of the lifetimes of electronic devices. To estimate the parameters of the proposed model, a Bayesian analysis is used. A case study based on the lifetime of a surface mounted electrolytic capacitor is presented, the results showed that the estimation of the proposed model differs from the inverse power law–Weibull and that it affects directly the mean time to failure, the failure rate, the behavior, and the performance of the capacitor under analysis.	reliability engineering	Luis Carlos Méndez González;Luis Alberto Rodríguez-Picón;Delia J. Valles-Rosales;Roberto Romero-López;Abel Eduardo Quezada-Carreón	2017	Quality and Reliability Eng. Int.	10.1002/qre.2214	weibull fading;mean time between failures;econometrics;statistics;failure rate;power law;engineering;distribution fitting;weibull modulus;weibull distribution;bayes estimator;reliability engineering	OS	29.43011616402653	-18.8485890629342	177972
3f53ff7dbb4f6cc802a3b2bf542d43ee97850cce	an approach of digital image copyright protection by using watermarking technology		Digital watermarking system is a paramount for safeguarding valuable resources and information. Digital watermarks are generally imperceptible to the human eye and ear. Digital watermark can be used in video, audio and digital images for a wide variety of applications such as copy prevention right management, authentication and filtering of internet content. The proposed system is able to protect copyright or owner identification of digital media, such as audio, image, video, or text. The system permutated the watermark and embed the permutated watermark into the wavelet coefficients of the original image by using a key. The key is randomly generated and used to select the locations in the wavelet domain in which to embed the permutated watermark. Finally, the system combines the concept of cryptography and digital watermarking techniques to implement a more secure digital watermarking system.	algorithm;authentication;coefficient;copy protection;cryptography;digital image;digital media;digital video;digital watermarking;discrete wavelet transform;distortion;filter (signal processing);image compression;procedural generation;requirement;secure digital;speaker recognition	Md. Selim Reza;Mohammed Shafiul Alam Khan;Md. Golam Rabiul Alam;Serajul Islam	2012	CoRR		digital watermarking alliance;digital watermarking;computer science;multimedia;internet privacy;watermark;computer security	Graphics	38.35467701891204	-12.139433665499393	178149
a84c79ac559d1a4dd392cdb4bf348bcbbf8363f3	examples in which misspecification of a random effects distribution reduces efficiency, and possible remedies	distribucion binomial;survival function;reponse binaire;modele survie;analisis datos;melange loi probabilite;selected works;random effects;variance component;binary response;loi gamma;estimation non parametrique;nonparametric;survival model;ajustement;mixed distribution;curva gauss;componente variancia;loi binomiale;funcion sobrevivencia;fitting;frailty model;non parametric estimation;ley gama;data analysis;62n02;efecto aleatorio;fonction survie;composante variance;random effect;binomial;62j10;statistical computation;calculo estadistico;62e10;62n99;loi normale;gamma distribution;mezcla ley probabilidad;bepress;analyse donnee;calcul statistique;odd ratio;62nxx;binomial distribution;estimacion no parametrica;ajuste;efficiency loss;modele logit;logit model;effet aleatoire;modele fragilite;60e05;perte efficacite;gaussian distribution;modelo logit;odds ratio	This note shows three cases in which a considerable loss of efficiency can result from assuming a parametric distribution for a random effect that is substantially different from the true distribution. For two simple models for binary response data, we studied the effects of assuming normality or of using a nonparametric fitting procedure for random effects, when the true distribution is potentially far from normal. Although usually the choice of random effects distribution has little effect on efficiency of predicting outcome probabilities, the normal approach suffered when the true distribution was a two-point mixture with a large variance component. Likewise, for a simple survival model, assuming a gamma distribution for the frailty distribution when the true one was a two-point mixture resulted in considerable loss of efficiency in predicting the frailties. This also occurred, but to a lesser extent, in using nonparametric fitting when the true frailty distribution was a gamma distribution.	mike lesser;random effects model	Alan Agresti;Brian Caffo;Pamela Ohman-Strickland	2004	Computational Statistics & Data Analysis	10.1016/j.csda.2003.12.009	econometrics;mixture distribution;nonparametric skew;calculus;inverse-chi-squared distribution;mathematics;compound probability distribution;statistics;probability integral transform;odds ratio;random effects model	ML	32.84464972064608	-22.493531795092444	178206
34b44802f3ba24df88362acc2a483a88856c853f	a high-low-based omnibus test for symmetry, the lévy property, and other hypotheses on intraday returns	continuous time;time change;permutation tests;conditional inference;intraday up and downside volatility;62g10;time changed levy processes;intraday returns;60g51;91b28;permutation test	We consider different models for intraday log-returns: Lévy models, symmetric models, and Lévy processes subjected to independent continuous time-changes. For these models, we show bivariate interchangeability of intraday upand downside volatility ratios which are built using daily high-low-prices. Using conditional inference permutation tests on bivariate interchangeability, we develop an omnibus test for the abovementioned models. Empirically, we find strong evidence against intraday returns belonging to these model classes, as we reject bivariate interchangeability of the volatility ratios for half of the components of the DJIA, two thirds of the S&P 500 shares and almost all stocks of the German DAX.	bivariate data;donald becker;resampling (statistics);volatility	Stefan Klößner	2010	Finance and Stochastics	10.1007/s00780-009-0088-x	financial economics;econometrics;economics;daylight saving time;resampling;mathematics;statistics	ML	31.975072788484983	-20.778247768021966	178247
9fdbab5a48e44f22df8aaca09364581938d2a736	double sampling designs in multivariate linear models with missing variables	experimental design;metodo estadistico;analyse multivariable;62k05;multivariate linear model;aplicacion;multivariate analysis;62j05;05bxx;plan echantillonnage;simulacion numerica;modele lineaire;plan experiencia;linear regression;estimacion promedio;missing variables;regression model;statistical method;regression multivariable;valor medio;sampling design;modelo lineal;pollution air;statistical regression;62jxx;echantillonnage double;plan muestreo;modelo regresion;62k99;plan experience;methode statistique;double sampling;regresion estadistica;air pollution;optimal design 62h99;modele regression;simulation numerique;linear model;regresion lineal;valeur moyenne;optimal design;62h99;muestreo optimo;contaminacion aire;mean value;analisis multivariable;mean estimation;optimal sampling;estimation statistique;regression statistique;estimation moyenne;application;estimacion estadistica;statistical estimation;multivariate regression;echantillonnage optimal;regression lineaire;numerical simulation;multivariate linear regression	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	emoticon;francis;general linear model;kerrison predictor;nl-complete;nonlinear system;optimal design;primary source;sampling (signal processing);stratified sampling	Noam Cohen;Ori Davidov;Yoel Haitovsky	2008	Communications in Statistics - Simulation and Computation	10.1080/03610910801885993	computer simulation;systematic sampling;sampling;econometrics;sampling design;calculus;mathematics;cluster sampling;stratified sampling;poisson sampling;regression analysis;statistics	Robotics	32.653643592466196	-21.558545236674625	178285
ec8a68a798548fa8a7725a68701d46a4a24b2191	further analysis of spurious causality	integrated time series;granger causality;spurious rejection;time series;granger causality test	The properties of Granger-causality tests are examined when applied to integrated time series. Recently presented results suggesting spurious causality in such circumstances are shown to be highly dependent upon the absence of deterministic terms from the causality testing equations. The analysis is completed by the examination of an alternative non-parametric causality test.	causality;time series	Steven Cook	2008	Mathematics and Computers in Simulation	10.1016/j.matcom.2008.04.011	granger causality;econometrics;mathematics;mathematical economics;statistics	Metrics	32.57695116066461	-20.738911349168074	178504
55f6413a5b56f985c472da54b8fe815565159548	correlated simulation experiments in first-order response surface design	statistical simulation;simulation;simulacion;conception surface reponse;response surface design;simulation experiment;763 simulation designs for response surfaces;simulacion estadistica;first order;simulation statistique;767 estimation of first order response surfaces	The collection of mathematical models, experimental strategies, and statistical inference referred to as response surface methodology RSM has been used in the empirical exploration of a wide variety of systems, particularly industrial situations in which a large number of variables influence the system response of interest. This paper examines experimental strategies for implementing RSM procedures in a simulation environment. Of particular interest is the question of how to best assign the pseudorandom number streams that drive the simulation to the experimental points when the objective is to estimate a first-order response surface model. We present general results for factorial and fractional-factorial plans where each factor is present at two levels. For this class of response surface designs, we consider three strategies for the assignment of pseudorandom number streams to experimental points: i the use of a unique set of streams at each design point; ii the assignment of a common set of streams to all experiments; and iii the simultaneous use of common and antithetic stream sets by the use of design blocking. We base our analysis of these correlation induction strategies on variance criteria commonly employed in response surface design, including: generalized variance, prediction variance, integrated variance, and variance of slopes. Our findings show that the simultaneous use of common and antithetic stream sets is the preferred correlation induction strategy, but that no one assignment procedure is uniformly superior for all four criteria. Our results provide a basis for selecting among the three correlation induction strategies.	first-order reduction;response surface methodology;simulation	James R. Hussey;Raymond H. Myers;Ernest C. Houck	1987	Operations Research	10.1287/opre.35.5.744	econometrics;mathematical optimization;response surface methodology;simulation;computer science;first-order logic;algorithm;statistics	ECom	31.7231901761152	-20.710111364389522	178886
b695fcd4553beddcca9ddc10247c1e3007a7e8a6	probabilistic reachability analysis for large scale stochastic hybrid systems	air traffic control;risk analysis;air transportation;stochastic processes;safety;stochastic processes air traffic control reachability analysis;sequential estimation;stochastic systems;advanced air traffic control probabilistic reachability analysis large scale stochastic hybrid systems rare event estimation stochastic analysis framework;collision processes;stochastic systems air transportation collision processes monte carlo methods risk analysis safety sequential estimation;reachability analysis large scale systems stochastic systems estimation theory stochastic processes diffusion processes analytical models discrete event simulation monte carlo methods air safety;reachability analysis;monte carlo methods	This paper studies probabilistic reachability analysis for large scale stochastic hybrid systems (SHS) as a problem of rare event estimation. In literature, advanced rare event estimation theory has recently been embedded within a stochastic analysis framework, and this has led to significant novel results in rare event estimation for a diffusion process using sequential MC simulation. This paper presents this rare event estimation theory directly in terms of probabilistic reachability analysis of an SHS, and develops novel theory which allows to extend the novel results for application to a large scale SHS where a very huge number of rare discrete modes may contribute significantly to the reach probability. Essentially, the approach taken is to introduce an aggregation of the discrete modes, and to develop importance sampling relative to the rare switching between the aggregation modes. The practical working of this approach is demonstrated for the safety verification of an advanced air traffic control example.	embedded system;estimation theory;extreme value theory;hybrid system;importance sampling;reachability;sampling (signal processing);secure hash standard;simulation	Henk A. P. Blom;G. J. Bakker;Jaroslav Krystul	2007	2007 46th IEEE Conference on Decision and Control	10.1109/CDC.2007.4434095	sequential estimation;stochastic process;econometrics;mathematical optimization;simulation;risk analysis;continuous-time stochastic process;computer science;air traffic control;mathematics;statistics;aviation;monte carlo method	Robotics	33.615611971401265	-15.833333755486395	179092
b7e8bb4a0563dc6e73d1fff2ee8b90473fbd5940	on-line bayesian context change detection in web service systems	change detection;web service;bic	In real-life situations characteristics of Web service systems evolve in time. Therefore, change detection techniques become substantial elements of adaptive procedures for Web service systems management, such as resource allocation and anomaly detection methods. In this paper, we propose an on-line change detector which uses the Bayesian inference. We define two models which describe situations with one change and no change within data. Next we apply Bayesian model comparison for change detection. In order to obtain analytical expressions of model evidences used in the model comparison we provide a coherent framework of change detection which focuses on an approximation of the Bayes factor. The proposed solution, contrary to state-of-the-art methods, works in an on-line fashion and the algorithm's computational complexity is proportional to the constant size of the shifting window. Low computational complexity of the change detector enables its application in complex computer networks. At the end of the research paper, the quality of the proposed algorithm is examined using simulated Web service system.	algorithm;anomaly detection;approximation;bayes factor;bayesian approaches to brain function;bayesian network;coherence (physics);computational complexity theory;online and offline;real life;systems management;web service	Jakub M. Tomczak;Maciej Zieba	2013		10.1145/2462307.2462311	simulation;computer science;data science;data mining	AI	30.738627850978727	-13.535051206277368	179229
20100ec99d4e0db3a32cd35428845bfc09b16066	new greedy myopic and existing asymptotic sequential selection procedures: preliminary empirical results	bayes methods;approximation theory;greedy algorithms;operations research;bayesian approach;adaptive stopping rules;asymptotic allocations;asymptotic sequential selection procedures;expected value of information;greedy myopic procedures;probabilistic approximations	"""Statistical selection procedures can identify the best of a finite set of alternatives, where """"best"""" is defined in terms of the unknown expected value of each alternative's simulation output. One effective Bayesian approach allocates samples sequentially to maximize an approximation to the expected value of information (EVI) from those samples. That existing approach uses both asymptotic and probabilistic approximations. This paper presents new EVI sampling allocations that avoid most of those approximations, but that entail sequential myopic sampling from a single alternative per stage of sampling. We compare the new and old approaches empirically. In some scenarios (a small, fixed total number of samples, few systems to be compared), the new greedy myopic procedures are better than the original asymptotic variants. In other scenarios (with adaptive stopping rules, medium or large number of systems, high required probability of correct selection), the original asymptotic allocations perform better."""	approximation;asymptote;greedy algorithm;sampling (signal processing);simulation	Stephen E. Chick;Juergen Branke;Christian Schmidt	2007	2007 Winter Simulation Conference		econometrics;mathematical optimization;greedy algorithm;bayesian probability;mathematics;statistics;approximation theory	ML	25.63592735692897	-16.97558058248491	179343
2a2ddf49aa5df5e16a9c2c774925106a54ba8545	an objective visual security assessment for cipher-images based on local entropy	security evaluation;human vision;security analysis;encryption;local entropy;evaluation method;journal;visual security;objective assessment;security assessment;information entropy;subjective assessment;selective encryption	In recent years, many practical algorithms have been put forward for images and videos encryption. Security analysis on these encryption algorithms focuses research on cryptographic security, and few work relate to visual security. Visual security means that the encrypted video content is unintelligible to human vision. The higher visual security the encryption algorithm can provide, the less information an attacker from the cipher-images to obtain, the greater the difficulty of attack is. Therefore, visual security assessment for cipher-images is a very important indicator in security evaluation of visual media. So far, systematic research on visual security assessment for cipher-images is far from enough. Moreover, there are no practical objective indicators or evaluation methods on visual security have been proposed at present. According to the changes on image information entropy between cipher-images and original images, we present a visual security assessment algorithm based on local entropy. The experiments result shows that the scheme can provide an efficient objective assessment which is match up to subjective assessment, and is also suitable for security assessment of other selective encryption algorithms.	algorithm;authorization;cipher;computational complexity theory;digital video;encryption;entropy (information theory);experiment;online and offline;pixel;plaintext;randomness;time complexity	Jing Sun;Zhengquan Xu;Jin Liu;Ye Yao	2010	Multimedia Tools and Applications	10.1007/s11042-010-0491-5	computer security model;computer vision;computer science;internet privacy;security analysis;computer security;encryption;statistics;entropy	Security	38.00142571792491	-10.146301212181177	179392
be029803855dcf78e691c95ff04f05f73cc62f4b	a new method for adaptive sequential sampling for learning and parameter estimation	sample size;chernoff bound;hoeffding bound;adaptive sampling;sequential sampling	Sampling is an important technique for parameter estimation and hypothesis testing widely used in statistical analysis, machine learning and knowledge discovery. In contrast to batch sampling methods in which the number of samples is known in advance, adaptive sequential sampling gets samples one by one in an on-line fashion without a pre-defined sample size. The stopping condition in such adaptive sampling scheme is dynamically determined by the random samples seen so far. In this paper, we present a new adaptive sequential sampling method for estimating the mean of a Bernoulli random variable. We define the termination conditions for controlling the absolute and relative errors. We also briefly present a preliminary theoretical analysis of the proposed sampling method. Empirical simulation results show that our method often uses significantly lower sample size (i.e., the number of sampled instances) while maintaining competitive accuracy and confidence when compared with most existing methods such as that in [14]. Although the theoretical validity of the sampling method is only partially established. we strongly believe that our method should be sound in providing a rigorous guarantee that the estimation results under our scheme have desired accuracy and confidence.	estimation theory	Jianhua Chen;Xinjia Chen	2011		10.1007/978-3-642-21916-0_25	sequential estimation;systematic sampling;sample size determination;sampling error;mathematical optimization;simple random sample;sampling design;importance sampling;slice sampling;sequential analysis;hoeffding's inequality;chernoff bound;statistics	ML	28.196459707670794	-23.105266472238146	179479
e419cdf0635c1d0b14e725171b73ef0ac549d500	from differential to difference importance measures for markov reliability models	analisis sensibilidad;optimisation;fiabilidad;reliability;derivada direccional;modelo markov;optimizacion;systeme aide decision;proceso markov;gestion risque;reliability modeling;risk management;derivee directionnelle;prise de decision;dynamic system;sistema ayuda decision;system performance;directional derivative;dynamical system;systeme dynamique;decision support system;markov model;sensitivity analysis;processus markov;fiabilite;markov process;analyse sensibilite;differential importance measures;total variation;regime permanent;optimization;gestion riesgo;regimen permanente;modele markov;sistema dinamico;toma decision;reliability sensitivity analysis differential importance measures markov process;steady state	This paper presents the development of the differential importance measures (DIM), proposed recently for the use in risk-informed decision-making, in the context of Markov reliability models. The proposed DIM measures are essentially based on directional derivatives. They can be used to quantify the relative contribution of a component (or a group of components, a state or a group of states) of the system on the total variation of system performance provoked by the changes in system parameters values. The estimation of DIM measures at steady state using only a single sample path of a Markov process is also investigated. A numerical example of a dynamic system is finally introduced to illustrate the use of DIM measures, as well as the advantages of proposed evaluation approaches.	directional derivative;dynamical system;ergodic process;ergodicity;expanded memory;functional dependency;markov chain;markov model;mathematical optimization;numerical analysis;reliability engineering;steady state;transient state	Phuc Do Van;Anne Barros;Christophe Bérenguer	2010	European Journal of Operational Research	10.1016/j.ejor.2009.11.025	econometrics;decision support system;risk management;artificial intelligence;dynamical system;mathematics;statistics	Robotics	27.92262247670779	-16.044045736392693	179656
f7a37517f2e22fadddd56ccea3029a8a5fcc8113	cross-entropy minimization estimation for two-phase sampling and non-response	wu changchun tang linjun zhang shangli 两相抽样 估计量 交叉熵 极小 大样本性质 辅助变量 有效距离 经验频率 cross entropy minimization estimation for two phase sampling and non response	This paper considers the problem of estimating the finite population total in two-phase sampling when some information on auxiliary variable is available. The authors employ an information-theoretic approach which makes use of effective distance between the estimated probabilities and the empirical frequencies. It is shown that the proposed cross-entropy minimization estimator is more efficient than the usual estimator and has some desirable large sample properties. With some necessary modifications, the method can be applied to two-phase sampling for stratification and non-response. A simulation study is presented to assess the finite sample performance of the proposed estimator.	cross entropy;sampling (signal processing);two-phase locking	Changchun Wu;Linjun Tang;Shangli Zhang	2015	J. Systems Science & Complexity	10.1007/s11424-015-2089-5	econometrics;mathematical optimization;mathematics;statistics	Logic	29.6091001428826	-22.64381679074875	179789
755ad595ec904f92757f933fe3f8f7da7af202fc	computer simulation of two continuous spin models using wang-landau-transition-matrix monte carlo algorithm	exact solution;61 30 v;05 10 ln;transition matrix;wang landau;monte carlo algorithm;monte carlo;monte carlo simulation;computer simulation;64 60 de	Monte Carlo simulation using a combination of Wang Landau (WL) and Transition Matrix (TM) Monte Carlo algorithms to simulate two lattice spin models with continuous energy is described. One of the models, the one-dimensional LebwohlLasher model has an exact solution and we have used this to test the performance of the mixed algorithm (WLTM). The other system we have worked on is the two dimensional XY-model. The purpose of the present work is to test the performance of the WLTM algorithm in continuous models and to suggest methods for obtaining best results in such systems using this algorithm. PACS: 64.60.De; 61.30.-v; 05.10.Ln	classical xy model;computer simulation;monte carlo algorithm;monte carlo method;picture archiving and communication system;stochastic matrix;wang and landau algorithm	Shyamal Bhar;Soumen Kumar Roy	2009	Computer Physics Communications	10.1016/j.cpc.2008.11.010	direct simulation monte carlo;computer simulation;statistical physics;quantum monte carlo;monte carlo method in statistical physics;quasi-monte carlo method;mathematical optimization;diffusion monte carlo;dynamic monte carlo method;simulation;hybrid monte carlo;particle filter;markov chain monte carlo;computer science;monte carlo molecular modeling;mathematics;kinetic monte carlo;rejection sampling;parallel tempering;monte carlo integration;monte carlo algorithm;statistics;monte carlo method;monte carlo method for photon transport	HPC	32.91659676080301	-15.836823058701855	179958
eff0e5517745739c6384f3d471a2caee73b16e3f	iterative estimation maximization for stochastic linear programs with conditional value-at-risk constraints	portfolio optimization;portfolio selection;column generation;master problem;coherent risk measure	We present a new algorithm, Iterative Estimation Maximization (IEM), for stochastic linear programs with Conditional Value-at-Risk constraints. IEM iteratively constructs a sequence of compact-sized linear optimization problems, and solves them sequentially to find the optimal solution. The problem size IEM solves in each iteration is unaffected by the size of random samples, which makes it extreme efficient for real-world, large-scale problems. We prove that IEM converges to the true optimal solution, and give a lower bound on the number of samples required to probabilistically bound the solution error. We present computational performance on large problem instances and also conduct a case study on a financial portfolio optimization problem using an S&P 500 data set.	analysis of algorithms;approximation algorithm;best, worst and average case;cvar;coherence (physics);expectation–maximization algorithm;expected shortfall;integrated enterprise modeling;iteration;linear programming;mathematical optimization;optimization problem;risk measure;value at risk	Pu Huang;Dharmashankar Subramanian	2012	Comput. Manag. Science	10.1007/s10287-011-0135-x	mathematical optimization;machine learning;mathematics;statistics	ML	27.695120986486952	-13.697020445384085	180129
0546fcfd61e98be24bcf23ddfb26f4f5fe405e2b	evaluation of the standard deviation of the random component of the measured signal from its autocorrelated observations		"""Described is the proposal of evaluation the standard deviation of the stationary random component of measured signal from its regularly sampled observations when they are auto-correlated. As, the first step is the identifica- tion and removing the regularly variable components from the raw sample data. Then formulas for standard deviation of the sample and of the mean value are expressed with use the correction coefficients or the so-called """"effective num- ber"""" of observations. These quantities depend on number of observations and on the autocorrelation function of the sample cleaned from regular components. How to estimate the autocorrelation function for the sample data is also described. Few numerical examples to illustrate problems are included."""	autocorrelation	Zygmunt L. Warsza	2014		10.1007/978-3-319-05353-0_69	studentized range;econometrics;calculus;mathematics;standard error;statistics	Theory	29.412389050299463	-20.428434804923572	180146
d91c29fe66c8d1c4e9e7a0d641263b05ffd6cd5b	the distance correlation t-test of independence in high dimension	primary;dcor;secondary;distance covariance;distance correlation;dcov;multivariate independence;high dimension	AMS subject classifications: primary 62G10 secondary 62H20 Keywords: dCor dCov Multivariate independence Distance covariance Distance correlation High dimension a b s t r a c t Distance correlation is extended to the problem of testing the independence of random vectors in high dimension. Distance correlation characterizes independence and determines a test of multivariate independence for random vectors in arbitrary dimension. In this work, a modified distance correlation statistic is proposed, such that under independence the distribution of a transformation of the statistic converges to Student t, as dimension tends to infinity. Thus we obtain a distance correlation t-test for independence of random vectors in arbitrarily high dimension, applicable under standard conditions on the coordinates that ensure the validity of certain limit theorems. This new test is based on an unbiased es-timator of distance covariance, and the resulting t-test is unbiased for every sample size greater than three and all significance levels. The transformed statistic is approximately normal under independence for sample size greater than nine, providing an informative sample coefficient that is easily interpretable for high dimensional data. 1. Introduction Many applications in genomics, medicine, engineering, etc. require analysis of high dimensional data. Time series data can also be viewed as high dimensional data. Objects can be represented by their characteristics or features as vectors p. In this work, we consider the extension of distance correlation to the problem of testing independence of random vectors in arbitrarily high, not necessarily equal dimensions, so the dimension p of the feature space of a random vector is typically large. measure all types of dependence between random vectors in arbitrary, not necessarily equal dimensions. (See Section 2 for definitions.) Distance correlation takes values in [0, 1] and is equal to zero if and only if independence holds. It is more general than the classical Pearson product moment correlation, providing a scalar measure of multivariate independence that characterizes independence of random vectors. The distance covariance test of independence is consistent against all dependent alternatives with finite second moments. In practice, however, researchers are often interested in interpreting the numerical value of distance correlation, without a formal test. For example, given an array of distance correlation statistics, what can one learn about the strength of dependence relations from the dCor statistics without a formal test? This is in fact, a difficult question, but a solution is finally available for a large class of problems. The …	coefficient;feature vector;information;numerical analysis;time series;vhdl-ams;vector graphics	Gábor J. Székely;Maria L. Rizzo	2013	J. Multivariate Analysis	10.1016/j.jmva.2013.02.012	econometrics;correlation dimension;combinatorics;mathematics;statistics;distance correlation	ML	31.423104767465716	-21.797950421542193	180264
fa4609a75fc95e1af8cc50fd6aff9b560b692f3c	estimation of the frank copula model for dependent competing risks in accelerated life testing		A competing risks situation where a potential critical unit failure at random time \(X_2\) in a life test may be avoided by observing a degraded failure at some random time \(X_1\) is considered. It is thus logical to expect a dependence between the event times \(X_1\) and \(X_2\). We model the joint distribution of \(X_1\) and \(X_2\) by the Frank copula because it captures the full range of dependence and it is symmetric in its dependence structure. This paper shows how expert opinion is used to estimate the assumed Frank copula when only incomplete competing risks data are observed. Estimation of the copula allows the marginal distributions to be identified from competing risks data. Our result is thus apparent in reliability where primary interest is in the estimation of marginal failure distributions and can be extended to other applications.	accelerated life testing	Herbert Hove;Frank Beichelt;Parmod K. Kapur	2017	Int. J. Systems Assurance Engineering and Management	10.1007/s13198-016-0548-6	financial economics;econometrics;mathematics;statistics	SE	30.49570330412978	-19.395085010222335	180311
37892189fa4aade8da9bc2d771c3d912dc755b44	authenticating video streams	video stream encoding;video streaming;data compression;packet loss;udp;streaming media authentication video compression redundancy videoconference protocols digital signatures cities and towns encoding telephony;stream authentication scheme;transport protocols;video coding;video streaming data compression message authentication transport protocols video coding;temporal compression;spatial compression;spatial compression stream authentication scheme video stream encoding save user datagram protocol udp temporal compression;save;message authentication;user datagram protocol	In this paper, we propose SAVe, a real-time stream authentication scheme for video streams. Each packet in the stream is authenticated to correspond to packet loss seen in UDP-based streaming. Since temporal and spatial compression techniques are adopted for video stream encoding, there are differences in the importance and dependencies between frames. Therefore, to take the characteristics special to video into consideration, in SAVe the amount of redundancy distributed to each frame is adjusted according to the importance of each frame. Simulation results show that SAVe improves the playing rate 50% compared to previously proposed schemes	authentication;data compression;network packet;real-time clock;real-time transcription;redundancy (engineering);simulation;streaming media	Shintaro Ueda;Shin-ichiro Kaneko;Nobutaka Kawaguchi;Hiroshi Shigeno;Ken-ichi Okada	2006	20th International Conference on Advanced Information Networking and Applications - Volume 1 (AINA'06)	10.1109/AINA.2006.107	user datagram protocol;computer science;video tracking;internet privacy;computer security;computer network	Mobile	39.042283237095425	-13.975212369378259	180416
c9f86e221fe7164ed88cec63bd101f4e3394265e	competing causes of failure and reliability tests for weibull lifetimes under type i progressive censoring	reliability;extreme value distribution;indexing terms;failure analysis;weibull distribution;accelerated life testing;life testing;extreme value distribution reliability test weibull lifetime distribution type i progressive censoring accelerated life test life testing variable sampling plan degradation related information product quality acceptance sampling;product quality;sampling methods;production testing;sampling methods reliability weibull distribution failure analysis production testing life testing;life testing degradation life estimation time factors test facilities sampling methods weibull distribution maximum likelihood estimation lifetime estimation parameter estimation;time constraint	For many high reliability products where very few items are expected to fail during the test period, testing under normal conditions is not feasible. Further, the requirement for high reliability increases the need for test procedures which yield valuable degradation and other useful information for improving product reliability. Thus in some manufacturing and other experiments, various types of failure censored and accelerated life tests are commonly employed for life testing. In this paper we discuss Type I progressively censored variable-sampling plans for Weibull lifetime distributions under competing causes of failure. The proposed procedure is attractive as it yields useful degradation-related information for improving product quality. In addition, the procedure is useful when a test is conducted under severe time constraint and/or when the experimenter wishes to save costly specimens or scarce test facilities for other use.	censoring (statistics);elegant degradation;experiment;progressive meshes;sampling (signal processing)	Uditha Balasooriya;Chan-Kee Low	2004	IEEE Transactions on Reliability	10.1109/TR.2003.821947	reliability engineering;weibull distribution;sampling;econometrics;failure analysis;generalized extreme value distribution;index term;accelerated life testing;engineering;reliability;mathematics;statistics	SE	29.28108218190297	-18.67985018338526	180446
50bede3f05fb047fb1fa7161c2065db27b86d31f	hypothesis tests for bernoulli experiments: ordering the sample space by bayes factors and using adaptive significance levels for decisions		The main objective of this paper is to find the relation between the adaptive significance level presented here and the sample size. We statisticians know of the inconsistency, or paradox, in the current classical tests of significance that are based on p-value statistics that are compared to the canonical significance levels (10%, 5%, and 1%): “Raise the sample to reject the null hypothesis” is the recommendation of some ill-advised scientists! This paper will show that it is possible to eliminate this problem of significance tests. We present here the beginning of a larger research project. The intention is to extend its use to more complex applications such as survival analysis, reliability tests, and other areas. The main tools used here are the Bayes factor and the extended Neyman–Pearson Lemma.	bayes factor;bernoulli polynomials;significance arithmetic;software reliability testing	Carlos Alberto De Bragança Pereira;Eduardo Y. Nakano;Victor Fossaluza;Luís Gustavo Esteves;Mark A. Gannon;Adriano Polpo	2017	Entropy	10.3390/e19120696	sample space;statistics;mathematics;statistical hypothesis testing;bayes factor;bernoulli's principle	AI	29.75408672441718	-21.18572280477462	180566
bd2496e4fed9e33cbbbe9cc39d7f44c56bbab22d	correction to: enhancing security and privacy of images on cloud by histogram shifting and secret sharing		In the original publication, figure labels a and b of Fig. 11 were missing. The original article has been corrected.	adaptive histogram equalization;privacy;secret sharing;xfig	Min-Ying Wu;Min-Chieh Yu;Jenq-Shiou Leu;Sheng-Kai Chen	2017	Multimedia Tools and Applications	10.1007/s11042-017-5420-4	computer science;internet privacy;cloud computing;computer security;secret sharing;histogram	Security	37.85764384107766	-10.718218690009392	180632
855c78e12603a2b74f47553e2bf63328427647b2	cell cycle control in eukaryotes: a biospi model	stochastic π calculus;stochastic simulation;stochastic calculus;stochastic process algebra;cell cycle control;cell cycle;stochastic model;mobile systems;cyclin dependent kinase;qh301 biology	This paper presents a stochastic model of the cell cycle control in eukaryotes. The framework used is based on stochastic process algebras for mobile systems. The automatic tool used in the simulation is the BioSpi. We compare our approach with classical ODE specifications.		Paola Lecca;Corrado Priami	2007	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.08.072	stochastic calculus;combinatorics;discrete mathematics;stochastic modelling;cell cycle;stochastic simulation;mathematics;cyclin-dependent kinase	ECom	34.14563012277507	-10.698406371834857	180647
6d5d48543aa1eb1febd26cd71ca0539c9d1aab78	a belief propagation algorithm based on domain decomposition		This note provides a detailed description and derivation of the domain decomposition algorithm that appears in previous works by the author. Given a large re-estimation problem, domain decomposition provides an iterative method for assembling Boltzmann distributions associated to small subproblems into an approximation of the Bayesian posterior of the whole problem. The algorithm is amenable to using Boltzmann sampling to approximate these Boltzmann distributions. In previous work, we have shown the capability of heuristic versions of this algorithm to solve LDPC decoding and circuit fault diagnosis problems too large to fit on quantum annealing hardware used for sampling. Here, we rigorously prove soundness of the method.	approximation algorithm;belief propagation;boson sampling;domain decomposition methods;heuristic;iterative method;low-density parity-check code;quantum annealing;sampling (signal processing);simulated annealing;software propagation;soundness (interactive proof)	Brad Lackey	2018	CoRR		low-density parity-check code;discrete mathematics;boltzmann constant;iterative method;belief propagation;domain decomposition methods;sampling (statistics);quantum annealing;decoding methods;algorithm;mathematics	ML	32.204873129001	-15.10162376611782	180784
71c97d737b1126e17887a3e91609eff5cf162af3	simulation of the sum-product algorithm using stratified sampling	sample size;confidence level;low density parity check code;simulation;stratified sampling;sum product algorithm	Using stratified sampling a desired confidence level and a specified margin of error can be achieved with smaller sample size than under standard sampling. We apply stratified sampling to the simulation of the sum-product algorithm on a binary low-density parity-check code.	algorithm;belief propagation;gibbs sampling;simulation;stratified sampling	John Brevik;Michael E. O'Sullivan;Anya Umlauf;Richard Wolski	2009		10.1007/978-3-642-02181-7_7	systematic sampling;sample size determination;sampling;sampling error;econometrics;simple random sample;combinatorics;sampling design;margin of error;confidence interval;bernoulli sampling;balanced repeated replication;slice sampling;sampling fraction;mathematics;cluster sampling;stratified sampling;poisson sampling;statistics;variance reduction	EDA	30.514671555790486	-21.3001520292675	180966
965c9ef5abfbfe5bb7456f09d8b825914df8975d	a note on testing hypotheses for stationary processes in the frequency domain	62m10;weak convergence under the alternative;stationary process goodness of fit tests kernel estimate smoothed periodogram weak convergence under the alternative;62m15;spectral density;local alternative;smoothed periodogram;62g10;goodness of fit tests;goodness of fit test;asymptotic properties;simulation study;weak convergence;frequency domain;asymptotic normal;stationary process;asymptotic theory;kernel estimate	In a recent paper Eichler (2008) considered a class of nonand semiparametric hypotheses in multivariate stationary processes, which are characterized by a functional of the spectral density matrix. The corresponding statistics are obtained using kernel estimates for the spectral distribution and are asymptotically normal distributed under the null hypothesis and local alternatives. In this paper we derive the asymptotic properties of these test statistics under fixed alternatives. In particular we show also weak convergence but with a different rate compared to the null hypothesis.		Holger Dette;Thimo Hildebrandt	2012	J. Multivariate Analysis	10.1016/j.jmva.2011.07.002	econometrics;mathematical analysis;mathematics;goodness of fit;statistics	ML	30.895189761118107	-22.43008628750291	181367
9c18a3f47df88efdeff00083e57625430fbd2d5b	storage requirements for information handling centers	information systems;information retrieval;integral equations;standard deviation;data processing;stochastic processes;mathematical models;probability theory;expectation;poisson density functions;computer design;storage		requirement	Herbert M. Gurk;Jack Minker	1970	J. ACM	10.1145/321556.321563	probability theory;data processing;computer science;theoretical computer science;mathematical model;data mining;mathematics;standard deviation;integral equation;information system;expected value;statistics	Theory	32.501376866605334	-18.615087438103696	181607
43c79d2919e721ab8cd7e0bf7e9e195290012714	generalized prediction intervals for blups in mixed models	generalized pivotal quantity gpq;reml;primary;coverage probability;best linear unbiased predictor blup;secondary;unbalanced data	A prediction interval is derived for the BLUP (Best Linear Unbiased Predictor) in mixed models involving a single random effect of interest, using the generalized inference approach. The resulting prediction interval is referred to as a generalized prediction interval. The solution in the case of the simplest balanced random effects model is first derived to provide better insight into the approach. Extensions to the unbalanced case, as well as to a general model are then provided. A simulation study is carried out to show the advantage of the proposed interval compared to the ML and REML based intervals available from widely used software packages such as SAS and R/S+. The estimated coverage probabilities show that the generalized prediction interval exhibits substantially better performance compared to ML and REML based intervals; the latter intervals were found to be highly conservative.	mixed model;random effects model;sas;simulation;unbalanced circuit	Jinadasa Gamage;Thomas Mathew;Samaradasa Weerahandi	2013	J. Multivariate Analysis	10.1016/j.jmva.2013.05.011	econometrics;best linear unbiased prediction;mathematics;restricted maximum likelihood;statistics	Comp.	29.564241500957277	-22.965253626969734	181698
280dafb26fe98ef33ba1364574457f2505adf32f	on auxiliary variables and many-core architectures in computational statistics	bayesian inference;computationally intensive statistics;statistics see also social sciences;markov chain monte carlo;sequential monte carlo		computational statistics;manycore processor	Anthony Lee	2011			monte carlo method in statistical physics;quasi-monte carlo method;econometrics;dynamic monte carlo method;hybrid monte carlo;particle filter;markov chain monte carlo;marginal likelihood;computer science;machine learning;monte carlo molecular modeling;kinetic monte carlo;rejection sampling;bayesian statistics;parallel tempering;monte carlo integration;computational statistics;statistics;monte carlo method	ML	27.100467841917297	-23.252726838273933	181808
e47897fa93921ed297aaf1bd8fe7e8b59c173122	generalized sichel distribution and associated inference				Yeh Ching Low;Seng-Huat Ong;Ramesh C. Gupta	2017	JSTA	10.2991/jsta.2017.16.3.4	statistics;overdispersion;inference;poisson distribution;mathematics	Crypto	30.798015021500433	-23.60152886937145	181813
875e12f664aed192dec65a4a39cf7851f796c8b3	application of mcmc-gsa model calibration method to urban runoff quality modeling	bayes estimation;contraste;metropolis algorithm;analisis sensibilidad;optimisation;escorrentia suelo;confidence level;chaine markov;cadena markov;metodo monte carlo;conformismo;optimizacion;intervalo confianza;conformity;model calibration;bayesian inference;global sensitivity analysis;methode monte carlo;conceptual model;inference mechanisms;analyse globale;latin hypercube sampling;lumped parameter system;systeme incertain;confidence interval;estimacion bayes;conformisme;posterior distribution;uncertainty analysis;systeme parametre localise;algorithme metropolis;sensitivity analysis;model uncertainty;muestreo hipercubo latino;monte carlo method;intervalle confiance;inferencia;ley a posteriori;analyse sensibilite;ruissellement eau sol;parameter uncertainty;echantillonnage hypercube latin;etalonnage;optimization;quality model;sistema incierto;sistema parametro localizado;loi a posteriori;uncertain system;calibration;inference;urban runoff;quality modeling;mecanisme inferentiel;estimation bayes;algoritmo metropolis;global analysis;markov chain;runoff	In stormwater quality modeling, estimating the confidence level in conceptual model parameters is necessary but difficult. The applicability and the effectiveness of a method for model calibration and model uncertainty analysis in the case of a four parameters lumped urban runoff quality model are illustrated in this paper. This method consists of a combination of the Metropolis algorithm for parameters' uncertainties and correlation assessment and a variance-based method for global sensitivity analysis. The use of the Metropolis algorithm to estimate the posterior distribution of parameters through a likelihood measure allows the replicated Latin hypercube sampling method to compute the parameters' importance measures. Calibration results illustrate the usefulness of the Metropolis algorithm in the assessment of parameters' uncertainties and their interaction structure. The sensitivity analysis demonstrates the insignificance of some parameters in terms of driving the model to have a good conformity with the data. This method provides a realistic evaluation of the conceptual description of the processes used in models and a progress in our capability to assess parameters' uncertainties.	global storage architecture;markov chain monte carlo;typset and runoff	Assem Kanso;Ghassan Chebbo;Bruno Tassin	2006	Rel. Eng. & Sys. Safety	10.1016/j.ress.2005.11.051	econometrics;confidence interval;artificial intelligence;mathematics;statistics	SE	27.90044114507213	-15.76378577326075	181838
94feb9fd2b5dfc53be17545d798d37888be2f58c	squint pixel steganography: a novel approach to detect digital crimes and recovery of medical images		Technology is playing a major role in the rapid growth of Techno media in relation to information security. Tampers are a major handicap while transferring medical images. In order to circumvent these issues used Steganography to hide the information inside a cover medium with different carrier formats. In this paper, the author proposes a novel squint pixel based medical image steganography technique to avoid distortion by an attacker. In this method, Original medical image itself acts as a carrier image. A Medical image segmented into two sets of pixels, Region of interest (ROI) and squint pixels of region of non-interest (RONI). The authentic data and information of ROI of a medical image embedded in penultimate and least significant bits (PLSB) of squint pixels of RONI. Results of experiments on various medical images show that the proposed method produces high quality stego medical images with high accuracy and recovery of ROI data without loss. KeywoRDS PLSB, ROI, RONI, Squint Pixel, Steganography	display resolution;distortion;embedded system;experiment;information security;least significant bit;medical imaging;pixel;region of interest;steganography	Ch. Rupa	2016	IJDCF	10.4018/IJDCF.2016100104	computer vision;computer graphics (images)	Vision	38.7036746083443	-12.202769044646342	181845
1185403ad505b76bc3b80c85ad7f8a16748f87e1	improved measures of the spread of data for some unknown complex distributions using saddlepoint approximations	random sum;62;saddlepoint approximation;poisson erlang distribution;poisson negative binomial distribution	Measures of the spread of data for random sums arise frequently in many problems and have a wide range of applications in real life, such as in the insurance field (e.g., the total claim size in a portfolio). The exact distribution of random sums is extremely difficult to determine, and normal approximation usually performs very badly for this complex distributions. A better method of approximating a random-sum distribution involves the use of saddlepoint approximations.Saddlepoint approximations are powerful tools for providing accurate expressions for distribution functions that are not known in closed form. This method not only yields an accurate approximation near the center of the distribution but also controls the relative error in the far tail of the distribution.In this article, we discuss approximations to the unknown complex random-sum Poisson–Erlang random variable, which has a continuous distribution, and the random-sum Poisson-negative binomial random variable, which has a discrete distributi...	approximation;digraphs and trigraphs	Alya O. Al Mutairi;Heng Chin Low	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.849739	inverse distribution;beta-binomial distribution;mathematical optimization;combinatorics;beta negative binomial distribution;stability;product distribution;compound poisson distribution;heavy-tailed distribution;mixture distribution;univariate distribution;degenerate distribution;stochastic simulation;mathematics;compound probability distribution;negative binomial distribution;infinite divisibility;statistics;probability integral transform;continuity correction	ML	32.99265703187912	-18.674769020302953	181907
f5df1f593f2e31367e384a13282a00cd47a355de	learning bayesian networks from incomplete data with stochastic search algorithms	bayesian network;stochastic algorithm;incomplete data;expectation maximization algorithm;missing data;stochastic search	This paper describes stochastic search approaches, including a new stochastic algorithm and an adaptive mutation operator, for learning Bayesian networks from incomplete data. This problem is characterized by a huge solution space with a highly multimodal landscape. State-of-the-art approaches all involve using deterministic approaches such as the elrpectation-maximization algorithm. These approaches are guaranteed to find local maxima, but do not explore the landscape for other modes. Our approach evolves structure and the missing data. We compare our stochastic algorithms and show they all produce accurate results.	algorithm;bayesian network;stochastic optimization	James W. Myers;Kathryn B. Laskey;Tod S. Levitt	2013	CoRR		stochastic neural network;mathematical optimization;expectation–maximization algorithm;missing data;computer science;stochastic optimization;machine learning;bayesian network;mathematics;statistics	AI	28.094383678574857	-10.7446618011509	182249
7696ef547758049cc9377c03b25d43bd7aa158de	decision making of robot partners based on fuzzy control and boltzmann selection	sound field;boltzmann selection;fuzzy control;robot partners;multi objective behavior coordination;behavioral learning	This paper discusses the social learning of robot partners through interaction with a person. We use a robot music player; Miuro, and we focus on the music selection for providing the comfortable sound field for the person. First, we propose the control architecture of Miuro based on autonomous behavior mode, interactive behavior mode, and human control mode. Next, we propose a learning method of the relationship between human interaction and its corresponding reaction based on Boltzmann selection, adaptive reward function, and temperature control. The experimental results show that the proposed method can learn the relationship between human interaction and its corresponding behavior, even if the human intention is changed in the learning. Furthermore, the experimental results show that the proposed method can provide the person the preferable song as the comfortable sound field.	autonomous robot;fuzzy control system;interaction;reinforcement learning	Naoyuki Kubota;Aiko Yaguchi	2011	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488511007118	robot learning;simulation;computer science;artificial intelligence;machine learning;fuzzy control system	Robotics	24.611598400868925	-13.86753228116297	182327
48415149f63ab88d653dcb11c59e7e8d00304bc2	strong convergence rate of estimators of change point and its application	strong convergence;iterative method;65b99;convergence forte;statistical moment;analisis numerico;statistical simulation;algoritmo busqueda;theorie approximation;convergencia fuerte;analisis datos;fonction repartition;algorithme recherche;estimacion m;relacion convergencia;moment statistique;62e17;search algorithm;estimacion promedio;mean shift;taux convergence;convergence rate;distribucion estadistica;satisfiability;iterative algorithm;negative association;analyse numerique;60e07;metodo iterativo;acceleration convergence;approximation theory;methode cusum;funcion distribucion;momento estadistico;data analysis;cusum method;distribution function;simulacion estadistica;numerical analysis;distribution statistique;methode iterative;simulation statistique;metodo cusum;statistical computation;calculo estadistico;aceleracion convergencia;point changement;estimation m;simulation study;analyse donnee;calcul statistique;mean estimation;cumulant;estimation moyenne;stable distribution;60e05;m estimation;punto cambio;statistical distribution;change point;convergence acceleration	"""Let {X""""n,n>=1} be an independent sequence with a mean shift. We consider the cumulative sum (CUSUM) estimator of a change point. It is shown that, when the rth moment of X""""n is finite, for n>=1 and r>1, strong convergence rate of the change point estimator is o(M(n)/n), for any M(n) satisfying that M(n)@6~, which has improved the results in the literature. Furthermore, it is also shown that the preceding rate is still valid for some dependent or negative associate cases. We also propose an iterative algorithm to search for the location of a change point. A simulation study on a mean shift model with a stable distribution is provided, which demonstrates that the algorithm is efficient. In addition, a real data example is given for illustration."""	rate of convergence	Xiaoping Shi;Yuehua Wu;Baiqi Miao	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.11.015	econometrics;calculus;mathematics;iterative method;statistics	ML	33.4883589385427	-23.22283002072267	182372
77422f3a98c1be9673197d44a97a1872ce40f32e	tests for consistent measurement of external subjective software quality attributes	statistical approach;multinomial distribution;quality attributes;data imputation;unbiased estimator;bayesian inference;statistical test;software systems;distribution principle;markov chain monte carlo;subjective software quality attributes;error rate;error rates;consistent measurement;minimum rejection principle;missing data;conditional probability;software quality	One reason that researchers may wish to demonstrate that an external software quality attribute can be measured consistently is so that they can validate a prediction system for the attribute. However, attempts at validating prediction systems for external subjective quality attributes have tended to rely on experts indicating that the values provided by the prediction systems informally agree with the experts’ intuition about the attribute. These attempts are undertaken without a pre-defined scale on which it is known that the attribute can be measured consistently. Consequently, a valid unbiased estimate of the predictive capability of the prediction system cannot be given because the experts’ measurement process is not independent of the prediction system’s values. Usually, no justification is given for not checking to see if the experts can measure the attribute consistently. It seems to be assumed that: subjective measurement isn’t proper measurement or subjective measurement cannot be quantified or no one knows the true values of the attributes anyway and they cannot be estimated. However, even though the classification of software systems’ or software artefacts’ quality attributes is subjective, it is possible to quantify experts’ measurements in terms of conditional probabilities. It is then possible, using a statistical approach, to assess formally whether the experts’ measurements can be considered consistent. If the measurements are consistent, it is also possible to identify estimates of the true values, which are independent of the prediction system. These values can then be used to assess the predictive capability of the prediction system. In this paper we use Bayesian inference, Markov chain Monte Carlo simulation and missing data imputation to develop statistical tests for consistent measurement of subjective ordinal scale attributes.	bayesian approaches to brain function;circuit complexity;coherence (physics);entity;geo-imputation;level of measurement;list of system quality attributes;markov chain monte carlo;missing data;monte carlo method;non-functional requirement;observed information;rejection sampling;simulation;social inequality;software development;software quality;software system;visual artifact	John Moses;Malcolm Farrow	2007	Empirical Software Engineering	10.1007/s10664-007-9058-0	econometrics;statistical hypothesis testing;variable and attribute;conditional probability;markov chain monte carlo;missing data;word error rate;data mining;mathematics;bias of an estimator;bayesian inference;software quality;multinomial distribution;statistics;software system	SE	25.057288751727928	-20.758230407316606	182477
1f4454b36e20e6c09768b6427dc469fd7a28ddf1	how the expertfit® distribution-fitting software can make your simulation models more valid	data structures;digital simulation;statistical distributions;expertfit distribution-fitting software;data representation;probability distributions;real-world data;simulation input modeling;simulation models;simulation software	"""In this paper, we discuss the critical role of simulation input modeling in a successful simulation study. Two pitfalls in simulation input modeling are then presented and we explain how any analyst, regardless of their knowledge of statistics, can easily avoid these pitfalls through the use of the ExpertFit distribution-fitting software. We use a set of real-world data to demonstrate how the software automatically specifies and ranks probability distributions, and then tells the analyst whether the """"best"""" candidate distribution is actually a good representation of the data. If no distribution provides a good fit, then ExpertFit can define an empirical distribution. In either case, the selected distribution is put into the proper format for direct input to the analyst's simulation software."""	simulation software	Averill M. Law	2002	Proceedings of the 2011 Winter Simulation Conference (WSC)	10.1145/1030818.1030842		HPC	29.967183297022547	-17.413554657873476	182554
a5f603b14721e23a83d73286cbe2051df3827ce6	subspace estimation of distribution algorithms: to perturb part of all variables in estimation of distribution algorithms	bayesian network;subspace estimation of distribution algorithms subedas;estimation of distribution algorithm;estimation of distribution algorithms edas;univariate marginal distribution algorithm umda;univariate marginal distribution algorithm;estimation of bayesian network algorithm ebna	Abstract: In the traditional estimation of distribution algorithms (EDAs), all the variables of candidate individuals are perturbed through sampling from a probability distribution of promising individuals. However, it may be unnecessary for the EDAs to perturb all variables of candidate individuals at each generation. This is because one variable may be dependent on another variable and all variables may have different saliences even if they are independent. Therefore, only a subset of all variables in EDAs really function at each generation. This paper proposes a novel class of EDAs, termed as subspace estimation of distribution algorithms (subEDAs), from a new perspective to reduce the space of variables for use in model building and model sampling based on EDAs' performance. In subEDAs, only part of all variables of candidate individuals are perturbed at each generation. Three schemes are described in details to determine which variables should be perturbed at each generation: the random picking method (RP), the majority voting based on the similarity between high quality individuals (MVSH) and the majority voting based on the difference between high quality and low quality individuals (MVDHL). Then, subEDAs+RP, subEDAs+MVSH and subEDAs+MVDHL are tested on several benchmark functions and their algorithmic results are compared with those obtained by EDAs. Our experimental results indicate that subEDAs are able to obtain a comparative result using only a subset of problem variables in the model when compared with traditional EDAs.	estimation of distribution algorithm;perturbation theory	Helong Li;Yi Hong;Sam Kwong	2011	Appl. Soft Comput.	10.1016/j.asoc.2010.11.022	marginal distribution;econometrics;mathematical optimization;estimation of distribution algorithm;computer science;machine learning;bayesian network;mathematics;statistics	Theory	28.706495131077837	-11.341585686778524	182566
fa93e767fb11d28292d3464c046146548d3e9509	global sensitivity analysis in wastewater applications: a comprehensive comparison of different methods	global sensitivity analysis;mbr modelling;calibration;wastewater treatment	Three global sensitivity analysis (GSA) methods are applied and compared to assess the most relevant processes occurring in wastewater treatment systems. In particular, the Standardised Regression Coefficients, Morris Screening and Extended-FAST methods are applied to a complex integrated membrane bioreactor (MBR) model considering 21 model outputs and 79 model factors. The three methods are applied with numerical settings as suggested in literature. The main objective considered is to classify important factors (factors prioritisation) as well as non-influential factors (factors fixing). The performance is assessed by comparing the most reliable method (Extended-FAST), by means of proposed criteria, with the two other methods. In particular, similarity to results obtained from Extended-FAST is assessed for sensitivity indices, for the ranking of sensitivity indices, for the classification into important/ non-influential factors and for the method’s ability to detect interaction among factors and to provide results in a reasonable time. It was found that the computationally less expensive SRC method was applied outside its range of applicability (R2) 1⁄4 (0.3e0.6) < 0.7. Still, the SRC produced a ranking of important factors similar to Extended-FAST. For some variables significant interactions among the factors were revealed by computing the total effect indices STi using Extended-FAST. This means that to obtain reliable variance decomposition and to detect and quantify interactions among the factors, the use of the Extended-FAST is recommended. Regarding the comparison between Morris screening and Extended-FAST a poor agreement was found. In particular, the Morris screening overestimated the number of both important and non-influential factors compared to Extended-FAST for the analysed case study. 2013 Elsevier Ltd. All rights reserved.	coefficient;discrepancy function;global storage architecture;interaction;international webmasters association;modeller;membrane computing;morris–lecar model;nonlinear system;numerical analysis;sample rate conversion;simulation;virtual screening	Alida Cosenza;Giorgio Mannina;Peter A. Vanrolleghem;Marc B. Neumann	2013	Environmental Modelling and Software	10.1016/j.envsoft.2013.07.009	econometrics;calibration;environmental engineering;engineering;data mining;sewage treatment	SE	26.804162456316977	-17.736632222515443	182730
04e7bed41a2711bbca8be569baeece48e9f83b00	estimation of a change point in the hazard rate of lindley model under right censoring	secondary 62n02;lindley distribution;62n03;bone marrow transplant data;lambert function;primary 62f03;acute lymphoblastic leukemia;kaplan meier survival curve;change point;62n01;right censoring	ABSTRACTIn this article, we consider a single change point model for a sudden change in the hazard rate of Lindley distribution to model right-censored survival data. We derive the quantile function to generate random numbers from the proposed distribution by using the Lambert function. The maximum likelihood estimation method is used to estimate parameters of the change point model. A simulation study is also carried out to analyze the performance of the estimators. To validate our findings, a dataset on bone marrow transplant for patients of acute lymphoblastic leukemia is analyzed using the proposed model and is compared with the existing exponential single change point model.	censoring (statistics)	Savitri Joshi;K. K. Jose;Deepesh Bhati	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2015.1096381	econometrics;demography;lambert w function;mathematics;censoring;statistics	ECom	30.875572089963057	-21.067237064849788	182895
9944c92db7290bd319eee0f97d89242fcbcce4ed	a novel jfd scheme for drm systems based on dwt and collusion resistance fingerprint encoding		With the proliferation of the internet and rapid development of multimedia, media distribution and traitor tracing issues have become imperative and critical. In this paper, the Digital Rights Management (DRM) system based on novel Joint Fingerprinting and Decryption (JFD) scheme is proposed, which transmit the encrypted image to different customers and makes each customer decrypt the image into a different copy that contains the customer’s unique information. Till now, some JFD schemes have been reported, that solves encryption and fingerprinting simultaneously and has high efficiency, but several problems still remain to be tackled in JFD, including poor encryption security, severe fingerprinted image distortion, etc. An improved JFD scheme, which based on Discrete Wavelet Transform (DWT) of media and collusion resistant fingerprint encoding, is presented in the paper. The experimental results show that the proposed scheme is more secure compared with the existing scheme, it obtains good imperceptibility and robustness. These properties make it a suitable choice for secure image distribution in real time applications.	content format;digital recording;digital rights management;digital watermarking;discrete wavelet transform;distortion;elegant degradation;encryption;fingerprint (computing);fingerprint recognition;image quality;imperative programming;peak signal-to-noise ratio;set partitioning in hierarchical trees;streaming media;traitor tracing	Alaa Salah El-Din Mohamed;Reham Mostafa;Hamdy K. Elminir	2016	Int. Arab J. Inf. Technol.		artificial intelligence;collusion;machine learning;fingerprint;encoding (memory);computer science;theoretical computer science;computer security	Security	38.8228916866358	-12.208581607906796	182989
b0eb91c5746cc7fedf6d5940ef894ab58c41d5e0	notes on the calculation of posterior probabilities related to outcomes from 2 × 2 matched pair tables		Computationally convenient expressions, and R code for performing the calculations, are provided for Bayesian posterior probabilities regarding the equivalence or noninferiority of the event rates for bivariate binary observations from a matched-pair design. These expressions provide an alternative to, and extend, an expression given by Wellek ∗ (eqn. 5.24). In addition, the expression obtained using the alternative derivation for the posterior probability that the difference between the event rates under the two conditions is positive (or negative) depends only on the counts in the off-diagonal cells of the outcome table, which is not immediately evident from the expression given by Wellek.		A. Lawrence Gould	2012	MASA	10.3233/MAS-2012-0229	econometrics;discrete mathematics;mathematics;statistics	NLP	29.972972548615445	-21.10651810422607	183110
1ba90340485a54de350ee426e00efb4de2df8eef	computing densities for markov chains via simulation	simulation;look ahead;density estimation;density estimator;markov chain	Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.	archive;download;markov chain;simulation	Shane G. Henderson;Peter W. Glynn	2001	Math. Oper. Res.	10.1287/moor.26.2.375.10562	econometrics;markov chain;mathematical optimization;estimator;density estimation;extremum estimator;multivariate kernel density estimation;m-estimator;mathematics;statistics	Metrics	28.21122454017336	-22.591043284495292	183472
04012ca7039ba9ad4d0df26f878f141e285d2aef	statistical mechanics and disordered systems	statistical mechanics;model system;optimization problem;complex system;disordered system	Since computers are able to simulate the equilibrium properties of model systems, they may also prove useful for solving the hard optimization problems that arise in the engineering of complex systems.	complex systems;computer;mathematical optimization;simulation	Scott Kirkpatrick;Robert H. Swendsen	1985	Commun. ACM	10.1145/3341.3344	optimization problem;statistical mechanics	Logic	34.10170300650926	-12.391491071870309	183479
2622097b659f4ec3906c79e59cd0cb438ccb5e64	rank statistics for two-sample location and scale problem for rounded-off data				Dana Vorlícková	1991	Kybernetika		econometrics;mathematics;statistics;quadratic form;ranking;l-estimator;mean reciprocal rank	ML	31.317109737377642	-23.328734645986902	183657
0d15b7236ca2f0508f0c11c446d7a46adc36925a	automated abstraction methodology for genetic regulatory networks	quasi steady state approximation;genetic regulatory network;large scale system;chemical master equation;state space;markov chain	In order to efficiently analyze the complicated regulatory systems often encountered in biological settings, abstraction is essential. This paper presents an automated abstraction methodology that systematically reduces the small-scale complexity found in genetic regulatory network models, while broadly preserving the large-scale system behavior. Our method first reduces the number of reactions by using rapid equilibrium and quasi-steady-state approximations as well as a number of other stoichiometry-simplifying techniques, which together result in substantially shortened simulation time. To further reduce analysis time, our method can represent the molecular state of the system by a set of scaled Boolean (or n-ary) discrete levels. This results in a chemical master equation that is approximated by a Markov chain with a much smaller state space providing significant analysis time acceleration and computability gains. The genetic regulatory network for the phage λ lysis/lysogeny decision switch is used as an example throughout the paper to help illustrate the practical applications of our methodology.	abstraction layer;approximation algorithm;asynchronous circuit;biological system;british informatics olympiad;computability;computation;gene regulatory network;high- and low-level;markov chain;observable;principle of abstraction;refinement (computing);relativistic electron beam;scalability;simulation;state space;steady state;time complexity	Hiroyuki Kuwahara;Chris J. Myers;Michael S. Samoilov;Nathan A. Barker;Adam Paul Arkin	2006		10.1007/11880646_7	markov chain;mathematical optimization;computer science;bioinformatics;state space;theoretical computer science;machine learning;mathematics;algorithm;statistics	Metrics	33.57573656837523	-12.612958313534621	183869
cf16809d97dbe3a4e40d053b668e5a324bcb1fe1	unbiased and efficient greeks of financial options	likelihood ratio;65c05;65l12;path dependent options;finite difference;option pricing;65d25;greeks;rainbow options;lipschitz continuity;path dependence;numerical experiment;importance sampling;monte carlo simulation;direct method	The price of a derivative security equals the discounted expected payoff of the security under a suitable measure, and Greeks are price sensitivities with respect to parameters of interest. When closed-form formulas do not exist, Monte Carlo simulation has proved very useful for computing the prices and Greeks of derivative securities. Although finite difference with resimulation is the standard method for estimating Greeks, it is in general biased and suffers from erratic behavior when the payoff function is discontinuous. Direct methods, such as the pathwise method and the likelihood ratio method, are proposed to differentiate the price formulas directly and hence produce unbiased Greeks (Broadie and Glasserman, Manag. Sci. 42:269–285, 1996). The pathwise method differentiates the payoff function, whereas the likelihood ratio method differentiates the densities. When both methods apply, the pathwise method generally enjoys lower variances, but it requires the payoff function to be Lipschitz-continuous. Similarly to the pathwise method, our method differentiates the payoff function but lifts the Lipschitz-continuity requirements on the payoff function. We build a new but simple mathematical formulation so that formulas of Greeks for a broad class of derivative securities can be derived systematically. We then present an importance sampling method to estimate the Greeks. These formulas Earlier versions of the paper were presented at the Midwest Finance Association 57th Annual Meeting, San Antonio, Texas, February 29, 2008, and at the Eighth International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing, Montreal, Canada, July 10, 2008. The first author was supported in part by the National Science Council of Taiwan under Grant 97-2221-E-002-096-MY3 and Excellent Research Projects of National Taiwan University under Grant 98R0062-05. Y.-D. Lyuu Dep artment of Finance and Department of Computer Science and Information Engineering, National Taiwan University, No. 1, Sec. 4, Roosevelt Road, Taipei 106, Taiwan H.-W. Teng ( ) Graduate Institute of Statistics, National Central University, No. 300, Jhongda Rd., Jhongli City, Taoyuan County 32049, Taiwan e-mail: wenteng@ncu.edu.tw 142 Y.-D. Lyuu, H.W. Teng are the first in the literature. Numerical experiments show that our method gives unbiased Greeks for several popular multi-asset options (also called rainbow options) and a path-dependent option.	computer science;email;experiment;finite difference;importance sampling;information engineering;monte carlo method;numerical method;path dependence;requirement;sampling (signal processing);scott continuity;silk road;simulation	Yuh-Dauh Lyuu;Huei-Wen Teng	2011	Finance and Stochastics	10.1007/s00780-010-0137-5	direct method;financial economics;econometrics;finite difference;likelihood-ratio test;importance sampling;valuation of options;greeks;mathematics;lipschitz continuity;mathematical economics;statistics;monte carlo method	DB	33.82937939743583	-17.16556237165431	183878
bc2bb83da187dbbfe537e8c079f2bd8f1803fb9b	fragile image watermarking scheme based on vq index sharing and self-embedding		In this paper, we propose a self-embedding fragile watermarking scheme using vector quantization (VQ) and index sharing. First, the principle contents of original image are compactly represented by a series of VQ indices. Then, after permutation, the binary bits of VQ indices are extended to generate reference-bits by a random binary matrix, which can make all reference-bits share the information of VQ index bits from different regions of the whole image. The image is embedded with watermark-bits including hash-bits for tampering localization and reference-bits for content recovery, and is transmitted to receiver side. Tampered regions in the received, suspicious image can be accurately located and then be recovered by VQ index reconstruction. Experimental results demonstrate that the proposed scheme can achieve successful content recovery for larger tampering rate and obtain better visual quality of recovered results than the reported schemes.	algorithm;authentication;codebook;digital watermarking;embedded system;local interconnect network;schedule (computer science);vector quantization;yang	Chuan Qin;Ping Ji;Jinwei Wang;Chin-Chen Chang	2015	Multimedia Tools and Applications	10.1007/s11042-015-3218-9	computer vision;theoretical computer science;computer security	Vision	39.077633181027785	-11.02950293972839	183975
5f0965e6b382c7149d81b114446b2955ae79d84f	confidence intervals for quantiles in finite populations with randomized nomination sampling	nomination sampling;finite population;confidence interval;quantile	Given a finite population consisting of N elements, it is desired to obtain confidence intervals for (t/N)th quantile x(t) of the population based on the randomized nomination sampling (RNS) design. Three without replacement sampling protocols are described and procedures for constructing nonparametric confidence intervals for population quantiles are developed. Formulas for computing coverage probabilities for these confidence intervals are presented. Simulation studies are conducted and the performance of the RNS based confidence intervals is compared with those based on the simple random sample without replacement design. © 2013 Elsevier B.V. All rights reserved.	population;randomized algorithm;residue number system;sampling (signal processing);simulation;software studies	Mohammad Nourmohammadi;Mohammad Jafari Jozani;Brad C. Johnson	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.11.020	econometrics;quantile;confidence interval;pattern recognition;mathematics;cdf-based nonparametric confidence interval;robust confidence intervals;statistics	AI	30.356231793424598	-21.376560886078007	184241
5346bd30dc30865e17111cf72563883a6074cdf4	fast robust estimation of prediction error based on resampling	methode jackknife;estimator efficiency;science general;62f40;classification automatique statistiques;theorie filtrage;analyse multivariable;estimacion robusta;estimator robustness;prediction error;least angle regression;stochastic process;validacion cruzada;multivariate analysis;robust estimator;bootstrap;analisis datos;62j05;estimation robuste;62m20;metodo jackknife;62g09;aproximacion;estimation non parametrique;modele lineaire;selection;linear regression;regression model;modelo lineal;statistical regression;62jxx;robust estimation;approximation;discriminant analysis;estimation parametrique;analyse discriminante;non parametric estimation;data analysis;analisis discriminante;modelo regresion;robustez estimador;estimation erreur;prediction theory;62h30;error estimation;erreur estimation;regresion estadistica;modele regression;statistical computation;linear model;estimacion error;calculo estadistico;validation croisee;prediccion lineal;regresion lineal;processus stochastique;estimacion parametro;efficacite estimateur;error estimacion;robust regression;subset regression models;60g25;algorithms;analisis multivariable;analyse donnee;robustness;calcul statistique;methode reechantillonnage;linear prediction;cross validation;estimation error;estimacion no parametrica;parameter estimation;bootstrap cross validation prediction error robustness;estimation parametre;resampling method;estimation statistique;proceso estocastico;theorie prediction;jackknife method;regression statistique;estimacion estadistica;statistical estimation;multivariate regression;regression lineaire;s estimators;eficacia estimador;filtering theory;prediction lineaire;robustesse estimateur	Robust estimators of the prediction error of a linear model are proposed. The estimators are based on the resampling techniques cross-validation and bootstrap. The robustness of the prediction error estimators is obtained by robustly estimating the regression parameters of the linear model and by trimming the largest prediction errors. To avoid the recalculation of timeconsuming robust regression estimates, fast approximations for the robust estimates of the resampled data are used. This leads to time efficient and robust estimators of prediction error.	algorithm;algorithmic efficiency;approximation;booting;cross-validation (statistics);eddy willems;emoticon;linear model;loss function;mean squared error;mean squared prediction error;microtransaction;national fund for scientific research;nonlinear system;resampling (statistics);robustness (computer science);simulation	Jafar A. Khan;Stefan Van Aelst;Ruben H. Zamar	2010	Computational Statistics & Data Analysis	10.1016/j.csda.2010.01.031	stochastic process;bootstrapping;econometrics;pattern recognition;m-estimator;mathematics;linear discriminant analysis;regression analysis;statistics	ML	32.6297036460398	-23.87037649187192	184243
597e75a5e77ecccb09ddcca85e645e45601b04d3	secure remote fingerprint verification using dual watermarks	dual watermarking;robust watermarking;biometrics;user authentication;fingerprint verification	As user authentication by using biometric information such as fingerprint has been widely accepted, there has been a growing interest in protecting the biometric information itself against external attackers. In this paper, we propose a dual watermarking technique to protect fingerprint images in transmission/storage. As the proposed dual watermarking technique provides both robustness and fragileness with the embedded watermarks, it can guarantee the integrity of the fingerprint image transmitted and/or stored. In particular, when the embedding locations for fragile watermarks are selected, we consider the ridge information of the fingerprint images to avoid possible interference between the robust watermark detection and fingerprint verification systems. Based on experimental results, we confirm that our dual watermarking technique can detect the robust watermark accurately and avoid any significant degradation in the accuracy of fingerprint verification.	authentication;biometrics;digital watermarking;elegant degradation;embedded system;fingerprint;interference (communication);secure transmission	Taehae Kim;Yongwha Chung;Seunghwan Jung;Daesung Moon	2005		10.1007/11787952_17	engineering;internet privacy;world wide web;computer security;fingerprint recognition	Security	38.20770133213867	-11.52297563846562	184291
5379ff70efe0350ae1dfb7a3d06ef7e70df816a6	statistical analysis of stochastic processes in time	stochastic process;statistical analysis	Preface Part I. Basic Principles: 1. What is a stochastic process? 2. Normal theory models and extensions Part II. Categorical State Space: 3. Survival processes 4. Recurrent events 5. Discrete-time Markov chains 6. Event histories 7. Dynamics models 8. More complex dependencies Part III. Continuous State Space: 9. Time series 10. Growth curves 11. Dynamic models 12. Repeated measurements Bibliography Author index Subject index.	stochastic process	Bruce D. McCullough	2005	Technometrics	10.1198/tech.2005.s290	continuous-time stochastic process;superstatistics;stochastic optimization;discrete-time stochastic process;gauss–markov process	Logic	26.088057054433182	-22.92871687562783	184388
e6cc76b49f42ea6bfaa331dba13c2794c1092106	individual cognitive parameter setting based on black stork foraging process	cognitive learning factor;pediatrics;particle swarm optimisation cognition learning artificial intelligence;probability density function;cognitive selection strategy;layout;data mining;particle swarm optimization stochastic processes acceleration automation hybrid intelligent systems computational intelligence laboratories process design competitive intelligence animals;particle swarm optimizer;particle swarm optimization;cognition;black stork foraging process particle swarm optimization cognitive learning factor;sun;cognitive selection strategy cognitive parameter setting black stork foraging process cognitive learning factor particle swarm optimization selection strategies;learning artificial intelligence;particle swarm optimization algorithm;particle swarm optimisation;black stork foraging process;conferences;selection strategies;cognitive parameter setting	Cognitive learning factor is an important parameter in particle swarm optimization algorithm(PSO). Although many selection strategies have been proposed, there is still much work need to do. Inspired by the black stork foraging process, this paper designs a new cognitive selection strategy, in which the whole swarm is divided into adult and infant particle, and each kind particle has its special choice. Simulation results show this new strategy is superior to other two previous modifications.	algorithm;cognition;cognitive science;mathematical optimization;particle swarm optimization;simulation	Zhihua Cui	2009	2009 Ninth International Conference on Hybrid Intelligent Systems	10.1109/HIS.2009.80	simulation;engineering;artificial intelligence;machine learning	Robotics	25.823691754608564	-10.937429180944324	184429
ec6841a837e1206665232f87c07b036855180220	topographic implementation of particle filters on cellular processor arrays	non linear filtering;sequential monte carlo method;hidden markov process;processor array;particle filter;cellular neural networks	Particle filters are a state-of-the-art method for the state estimation of non-linear stochastic systems. Recent many-core architectures and cellular processor arrays offer a new paradigm for algorithm development, which provides not only high performance, but also theoretical advances for parallel implementations. We have developed a new variant of the particle filter algorithm, which suits ideally implementation on a cellular processor array. The new algorithm often performs better than the classical one and a significant gain in running time can be achieved, especially when there is a large number of particles to be simulated.	particle filter;topography	András Horváth;Miklós Rásonyi	2013	Signal Processing	10.1016/j.sigpro.2012.11.025	electronic engineering;cellular neural network;real-time computing;particle filter;computer science;theoretical computer science;machine learning;statistics	HPC	38.719903366719116	-23.013594391016138	184470
8e0ec37d534178bc07175a64a845220ad6b646c7	a bayesian model for local smoothing in kernel density estimation	sample size;adaptive kernel density estimation;density estimation;variable bandwidth;bandwidth selection;markov chain monte carlo;kernel density estimate;graphical model;cross validation;bayesian model	A new procedure is proposed for deriving variable bandwidths in univariate kernel density estimation, based upon likelihood cross-validation and an analysis of a Bayesian graphical model. The procedure admits bandwidth selection which is flexible in terms of the amount of smoothing required. In addition, the basic model can be extended to incorporate local smoothing of the density estimate. The method is shown to perform well in both theoretical and practical situations, and we compare our method with those of Abramson (The Annals of Statistics 10: 1217–1223) and Sain and Scott (Journal of the American Statistical Association 91: 1525–1534). In particular, we note that in certain cases, the Sain and Scott method performs poorly even with relatively large sample sizes.#R##N##R##N#We compare various bandwidth selection methods using standard mean integrated square error criteria to assess the quality of the density estimates. We study situations where the underlying density is assumed both known and unknown, and note that in practice, our method performs well when sample sizes are small. In addition, we also apply the methods to real data, and again we believe our methods perform at least as well as existing methods.	kernel density estimation;smoothing	Mark J. Brewer	2000	Statistics and Computing	10.1023/A:1008925425102	kernel;sample size determination;kernel density estimation;econometrics;mathematical optimization;density estimation;markov chain monte carlo;multivariate kernel density estimation;mathematics;graphical model;bayesian inference;variable kernel density estimation;cross-validation;statistics	ML	29.448022404899476	-23.48179816962304	184650
4bbe549b678d9debdd2c2f56ec6f1c566493f462	a note on the asymptotic behaviour of empirical likelihood statistics	garch model;profile likelihood;autoregressive model;estimating function;dependent data;pseudo likelihood;asymptotic behaviour;empirical likelihood;stationary process;whittle s estimator;time series model	This paper develops some theoretical results about the asymptotic behaviour of the empirical likelihood and the empirical profile likelihood statistics, which originate from fairly general estimating functions. The results accommodate, within a unified framework, various situations potentially occurring in a wide range of applications. For this reason, they are potentially useful in several contexts, such as, for example, in inference for dependent data. We provide examples showing that known findings in literature about the asymptotic behaviour of some empirical likelihood statistics in time series models can be derived as particular cases of our results.		Gianfranco Adimari;Annamaria Guolo	2010	Statistical Methods and Applications	10.1007/s10260-010-0137-9	autoregressive conditional heteroskedasticity;stationary process;econometrics;mathematical optimization;likelihood principle;likelihood-ratio test;marginal likelihood;time series;m-estimator;mathematics;likelihood function;quasi-maximum likelihood;autoregressive model;statistics;estimating equations	ML	30.55239811502794	-23.21630448555912	184936
468f9b8a379f26c51bd3be417ded989cb85f88ee	singular value decomposition of large random matrices (for two-way classification of microarrays)	classification automatique statistiques;metodo estadistico;matrice aleatoire;analyse multivariable;analisis componente principal;ruido aleatorio;valor singular;decomposition valeur singuliere;almost sure convergence;noise matrix;analisis factorial;random perturbation;multivariate analysis;blown up matrix;bruit aleatoire;convergencia casi segura;ley gran numero;singular value;singular value decomposition;tabla contingencia;loi grand nombre;analisis correspondencia;15a52;blown up matrix noise matrix random perturbation two way classification microarray correspondence matrix;statistical method;two way classification;random matrix;asymptotic behavior;comportement asymptotique;algorithme;discriminant analysis;analyse discriminante;law of large numbers;algorithm;comportamiento asintotico;random noise;analisis discriminante;perturbacion;analyse factorielle;62h30;factor analysis;methode statistique;valeur singuliere;random matrices;principal component analysis;correspondence analysis;62h25;analyse composante principale;62h17;analyse correspondance;15a42;analisis multivariable;correspondence matrix;decomposicion valor singular;microarray;contingency table;perturbation;puce a adn;table contingence;matriz aleatoria;large deviation;convergence presque sure;algoritmo	Asymptotic behavior of the singular value decomposition (SVD) of blown up matrices and normalized blown up contingency tables exposed to Wigner-noise is investigated. It is proved that such an m×n matrix almost surely has a constant number of large singular values (of order √ mn), while the rest of the singular values are of order √ m + n as m,n → ∞. Concentration results of Alon at al. for the eigenvalues of large symmetric random matrices are adapted to the rectangular case, and on this basis, almost sure results for the singular values as well as for the corresponding isotropic subspaces are proved. An algorithm, applicable to two-way classification of microarrays, is also given that finds the underlying block structure.	algorithm;contingency table;microarray;singular value decomposition	Marianna Bolla;Katalin Friedl;András Krámli	2010	J. Multivariate Analysis	10.1016/j.jmva.2009.09.006	combinatorics;asymptotic analysis;random matrix;calculus;singular solution;mathematics;linear discriminant analysis;statistics	Theory	34.09405684756769	-21.441252098510905	184955
34c3c833a5c3bb10225e907a3e94883552abf491	a generalized maximum entropy stochastic frontier measuring productivity accounting for spatial dependency	total factor productivity;agricultural production;generalized maximum entropy;spatial dependence;technological development;stochastic frontier;spatial dependency;stochastic frontier model	In this paper, a stochastic frontier model accounting for spatial dependency is developed using generalized maximum entropy estimation. An application is made for measuring total factor productivity in European agriculture. The empirical results show that agricultural productivity growth in Europe is driven by upward movements of technology over time through technological developments. Results are then compared for a situation in which spatial dependency in the technical inefficiency effects is not accounted.	entropy estimation;gme of deutscher wetterdienst;panel data;time-invariant system;total functional programming	Axel Tonini;Valerien Pede	2011	Entropy	10.3390/e13111916	econometrics;spatial dependence;total factor productivity;agricultural productivity;statistics	AI	26.0867184795965	-21.21026548217788	185053
585edb94cfab360066d52aec558039b3da374643	a spreadsheet template compatible with microsoft excel and iwork numbers that returns the simultaneous confidence intervals for all pairwise differences between multiple sample means	tukey;genie biomedical;degree of freedom;bonferroni;dunn sidak;scheffe;confidence interval;fisher lsd;biomedical engineering;multiple comparisons;analysis of variance;ingenieria biomedica;f test;simultaneous confidence interval;post hoc test	The objective of the method described in this paper is to develop a spreadsheet template for the purpose of comparing multiple sample means. An initial analysis of variance (ANOVA) test on the data returns F--the test statistic. If F is larger than the critical F value drawn from the F distribution at the appropriate degrees of freedom, convention dictates rejection of the null hypothesis and allows subsequent multiple comparison testing to determine where the inequalities between the sample means lie. A variety of multiple comparison methods are described that return the 95% confidence intervals for differences between means using an inclusive pairwise comparison of the sample means.		Angus M. Brown	2010	Computer methods and programs in biomedicine	10.1016/j.cmpb.2009.10.001	f-test;econometrics;post-hoc analysis;confidence interval;scheffé's method;analysis of variance;computer science;bonferroni correction;data mining;mathematics;degrees of freedom;multiple comparisons problem;statistics	ML	31.45170987284384	-22.168403849941054	185064
3369dc5f45b2bd70d98b281af3a1a22024589d25	comparison, utility, and partition of dependence under absolutely continuous and singular distributions	marshall olkin;utility;elliptical distribution;convolution;62h20;copula;94a15;sarmanov families;predictability;shock models;94a17;mutual information;entropy;62b10	This paper first illustrates that a mutual information index detects and ranks dependence of a wide variety of absolutely continuous families, but the popular association and variance reduction indices fail to serve as such “common metrics”. We then elaborate on some theoretical merits of the mutual information and give several results. The mutual information provides a notion of the utility of dependence for predicting random variables and quantifies how much the joint distribution is more informative about the variables than the independent model. We present insightful partitions of dependence among the components of a random vector, for a class of models recently proposed for dependence of uncorrelated variables, and for the elliptical families. We also recall that the mutual information is not applicable to singular distributions and give some results for a generalized information index for these models. The generalized index is derived for the Marshall–Olkin copula and for a new singular copula that represents the dependence of the consecutive terms of the exponential autoregressive and related processes.		Nader Ebrahimi;Nima Y. Jalali;Ehsan S. Soofi	2014	J. Multivariate Analysis	10.1016/j.jmva.2014.06.014	econometrics;entropy;mathematical optimization;elliptical distribution;predictability;multivariate mutual information;copula;mathematics;convolution;mutual information;interaction information;utility;statistics;pointwise mutual information	HPC	31.86952146230909	-19.898575961141983	185276
27934f95aaa4e5f253423aad17a75867b3450e1d	subtilin production by bacillus subtilis: stochastic hybrid models and parameter identification	genetic engineering;commande multimodele;ajustamiento modelo;echantillonnage;stochastic hybrid models;modelo hibrido;biochimie;systeme echantillonne;bioinformatique;subtilin production;control multimodelo;multimodel control;reseau;gene expression data;probabilistic approach;sparse sampling;dinamica poblacion;algoritmo genetico;bacillus subtilis;modele hybride;parameter identification;red;hybrid model;sampling;identificacion sistema;ajustement modele;gene expression;expression genique;population dynamic;stochastic processes;system identification;sampled observations;enfoque probabilista;approche probabiliste;switching dynamics;genetic network;model matching;ingenieria genetica;genie genetique;algorithme genetique;population dynamics;biochemical systems;antibiotic;sistema muestreado;nonlinear estimation;genetic algorithm;bioinformatica;parameter estimation;dynamique population;muestreo;antibiotico;bioquimica;biotechnology;expresion genetica;biochemistry;identification systeme;sampled system;antibiotique;network;bioinformatics	This paper presents methods for the parameter identification of a model of subtilin production by Bacillus subtilis. Based on a stochastic hybrid model, identification is split in two subproblems: estimation of the genetic network regulating subtilin production from gene expression data, and estimation of population dynamics based on nutrient and population level data. Techniques for identification of switching dynamics from sparse and irregularly sampled observations are developed and applied to simulated data. Numerical results are provided to show the effectiveness of our methods.	gene regulatory network;numerical linear algebra;population dynamics;sparse matrix	Eugenio Cinquemani;Riccardo Porreca;Giancarlo Ferrari-Trecate;John Lygeros	2008	IEEE Transactions on Automatic Control	10.1109/TAC.2007.911327	genetic engineering;sampling;gene expression;genetic algorithm;system identification;bioinformatics;artificial intelligence;mathematics;population dynamics;estimation theory;statistics	Visualization	31.43014093432329	-11.047427660478427	185385
d1c324e39465f732520f561a1dc35616eda342dc	an efficient monte carlo solution for problems with random matrices		Random matrices, that is, matrices whose entries are measurable functions of a random vector Z, are encountered in finite element/difference formulations of a broad range of stochastic mechanics problems. Monte Carlo simulation, the only general method for solving this class of problems, is usual impractical when dealing with realistic problems. A new method is proposed for solving this class of problems. The method can be viewed as a smart Monte Carlo simulation. Like Monte Carlo, it calculates statistics for quantities of interest from deterministic matrices corresponding to samples of Z. In contract to Monte Carlo that uses a large number of samples of Z selected at random, the proposed method uses a small number of samples of this vector selected in an optimal manner. The method is based on stochastic reduced models (SROMs) for Z, i.e., random vectors with finite numbers of samples, and surrogate models expressing quantities of interest as known functions of Z. Theoretical arguments are followed by numerical examples providing statistics for inverses of random matrices, solutions of stochastic algebraic equations, and eigenvalues/eigenvectors of random matrices.	monte carlo method	Mircea Grigoriu	2014	Monte Carlo Meth. and Appl.	10.1515/mcma-2013-0021	quantum monte carlo;monte carlo method in statistical physics;quasi-monte carlo method;mathematical optimization;combinatorics;random field;dynamic monte carlo method;hybrid monte carlo;markov chain monte carlo;stochastic optimization;random function;monte carlo molecular modeling;stochastic simulation;mathematics;monte carlo integration;statistics;monte carlo method	Theory	31.729077302697675	-16.099028775907197	185509
f16313335266b90899405227febbedaa6657426f	analysis on wang's kwta with stochastic output nodes				John Sum;Andrew Chi-Sing Leung;Kevin I.-J. Ho	2011		10.1007/978-3-642-24965-5_29	econometrics;artificial intelligence;mathematical economics	Robotics	32.86291260066867	-19.551115247796243	185699
adf7d8ab51dead46c36e3f487cc2b934536cc415	evaluation of modified non-normal process capability index and its bootstrap confidence intervals		Process capability index (PCI) is used to quantify the process performance and is becoming an attracted area of research. A variability measure plays an important role in PCI. The interquartile range (IQR) or the median absolute deviation (MAD) is commonly used for a variability measure in estimating PCI when a process follows a non-normal distribution In this paper, the efficacy of the IQR and MAD-based PCIs was evaluated under low, moderate, and high asymmetric behavior of the Weibull distribution using different sample sizes through three different bootstrap confidence intervals. The result reveals that MAD performs better than IQR, because the former produced less bias and mean square error. Also, the percentile bootstrap confidence interval is recommended for use, because it has less average width and high coverage probability.	bsd;mad;mean squared error;spatial variability	Muhammad Kashif;Muhammad Ahtisham Aslam;Ali Hussein Al-Marshadi;Chi-Hyuck Jun;Muhammad Imran Khan	2017	IEEE Access	10.1109/ACCESS.2017.2713884	econometrics;statistics;coverage probability;sample size determination;percentile;interquartile range;process capability index;confidence interval;median absolute deviation;mean squared error;mathematics	Metrics	28.773172144056343	-20.8089380486109	185913
5e4625118de5850fdeeb596725b5ddd8e70b35ef	evolutionary computation using interaction among genetic evolution, individual learning and social learning	fitness landscape;evolutionary computation;genetic evolution;social learning;genetics;nk fitness landscape;dynamic environment;individual learning;evolutionary computing	This paper studies the characteristics of interaction among genetic evolution, individual learning and social learning using an evolutionary computation system with NK fitness landscape, both under static and dynamic environments. We show conditions for effective social learning: at least 1.5 times lighter cost of social learning than that of individual learning, beneficial teaching action, low epistasis and dynamic environment.	evolutionary computation	Takashi Hashimoto;Katsuhide Warashina	2008		10.1007/978-3-540-89197-0_17	genetic programming;social learning;interactive evolutionary computation;human-based evolutionary computation;fitness landscape;computer science;bioinformatics;social evolution;artificial intelligence;machine learning;evolutionary algorithm;learnable evolution model;evolutionary computation	AI	24.76889448073007	-10.58208740216091	186295
08c9e786f00dc7e4bac0053de3b23de07f92618e	effect of model complexity for estimation of distribution algorithm in nk landscapes	town and country planning;evolutionary computation;bivariate marginal distribution algorithm;bayes methods;random variables bayes methods sociology statistics computational modeling estimation;nk landscapes;random variables;complex model;model complexity;north korea;statistical distributions;town and country planning computational complexity evolutionary computation statistical distributions;estimation of distribution algorithm;computational modeling;nk landscape;estimation;computational complexity;north korea model complexity nk landscape evolutionary algorithm estimation of distribution algorithm probability model complex model univariate marginal distribution algorithm umda bivariate marginal distribution algorithm bmda estimation of bayesian network ebna;probability model estimation of distribution algorithm model complexity nk landscapes;statistics;ebna;evolutionary algorithm;probability model;bmda;umda;univariate marginal distribution algorithm;sociology;estimation of bayesian network	Evolutionary algorithms (EAs) have been widely proved to be effective in solving complex problems. Estimation of distribution algorithm (EDA) is an emerging EA, which manipulates probability models instead of genes for evolution EDA creates probability models based on the promising solution in the population and generates offspring by sampling from these models. The model complexity is a key factor in the performance of EDA. Complex models can express the relations among variables more accurately than simple models. However, for some problems with strong interaction among variables, building a model for all the relations becomes unrealistic and impractical due to its high computational cost and requirement for a large population size. This study aims to understand the behaviors of EDAs with different model complexities in NK landscapes. Specifically, this study compares the solution quality and convergence speed of univariate marginal distribution algorithm (UMDA), bivariate marginal distribution algorithm (BMDA), and estimation of Bayesian network (EBNA) in the NK landscapes with different parameter settings. The comparative results reveal that high complexity does not imply high performance: Simple model such as UMDA and BMDA can outperform complex mode like EBNA on the tested NK landscape problems. The results also show that BMDA achieves a stable high probability of generating the best solution and satisfactory solution quality; by contrast, the probability for EBNA drastically declines after some generations.	algorithmic efficiency;bayesian network;bivariate data;computation;estimation of distribution algorithm;evolutionary algorithm;marginal model;sampling (signal processing)	Rung-Tzuo Liaw;Chuan-Kang Ting	2013	2013 IEEE Symposium on Foundations of Computational Intelligence (FOCI)	10.1109/FOCI.2013.6602458	probability distribution;random variable;econometrics;mathematical optimization;estimation;estimation of distribution algorithm;computer science;machine learning;evolutionary algorithm;mathematics;computational complexity theory;computational model;statistics;evolutionary computation	AI	28.634311481938813	-11.08027131086524	186359
8a73ad7c2ab02187a84c8964f9c20338d0b14d13	a copula-based correlation measure and its application in chinese stock market	empirical study;copula;tail interdependence;chinese stock market;journal;theoretical analysis;copula function;correlation coefficient;gini correlation coefficient	In this paper, a copula-based correlation measure is proposed to test the interdependence among stochastic variables in terms of copula function. Based on a geometric analysis of copula function, a new derivation method is introduced to derive the Gini correlation coefficient. Meantime theoretical analysis finds that the Gini correlation coefficient tends to overestimate the tail interdependence in the case of stochastic variables clustering at the tails. For this overestimation issue, a fully new correlation coefficient called Co is developed and extended to measure the tail interdependence. Empirical study shows that the new correlation coefficient Co can effectively solve the overestimation issue, which implies that the proposed new correlation coefficient is more suitable to describe the interdependence among stochastic variables than the Gini correlation coefficient.		Fenghua Wen;Zhifeng Liu	2009	International Journal of Information Technology and Decision Making	10.1142/S0219622009003612	financial economics;association;econometrics;economics;copula;empirical research;copula;statistics	Vision	32.58652450836589	-20.174332372590026	186501
2bc739878330c82182124122019db8229861fd61	information force clustering using directed trees	statistical moment;densite probabilite;analyse amas;cluster;entropia;probability density;partition donnee;amas;moment statistique;data partition;metric;classification;quantite information;densidad probabilidad;devis estimatif;non parametric estimation;momento estadistico;cluster analysis;presupuesto estimativo;entropie;metrico;analisis cluster;entropy;monton;estimate;theorie information;potential energy;second order statistics;cantidad informacion;information theoretic;information quantity;clasificacion;metrique;information theory;variance;variancia;teoria informacion	We regard a data pattern as a physical particle experiencing a force acting on it imposed by an overall “potential energy” of the data set, obtained via a non-parametric estimate of Renyi’s entropy. The “potential energy” is called the information potential, and the forces are called information forces, due to their information-theoretic origin. We create directed trees by selecting the predecessor of a node (pattern) according to the direction of the information force acting on the pattern. Each directed tree correspond to a cluster, hence enabling us to partition the data set. The clustering metric underlying our method is thus based on entropy, which is a quantity that conveys information about the shape of a probability density, and not only it’s variance, as many traditional algorithms based on mere second order statistics rely on. We demonstrate the performance of our clustering technique when applied to both artificially created data and real data, and also discuss some limitations of the proposed method.	algorithm;cluster analysis;entropy (information theory);information theory	Robert Jenssen;Deniz Erdogmus;Kenneth E. Hild;José Carlos Príncipe;Torbjørn Eltoft	2003		10.1007/978-3-540-45063-4_5	econometrics;entropy;combinatorics;information theory;mathematics;statistics	ML	35.40633253329613	-22.118775699687728	186823
e30bc0d91f6f1e843d128824343e41b70450e17d	analysis of housing price by means of star models with neighbourhood effects: a bayesian approach	goodness of fit;bayesian approach;real estate market;bayesian inference;estimation algorithm;autoregressive model;neighbourhood effect;hedonic model;house prices;star	In this paper, we extend the Bayesian methodology introduced by Beamonte et al. (Stat Modelling 8:285–311, 2008) for the estimation and comparison of spatiotemporal autoregressive models (STAR) with neighbourhood effects, providing a more general treatment that uses larger and denser nets for the number of spatial and temporal influential neighbours and continuous distributions for their smoothing weights. This new treatment also reduces the computational time and the RAM necessities of the estimation algorithm in Beamonte et al. (Stat Modelling 8:285–311, 2008). The procedure is illustrated by an application to the Zaragoza (Spain) real estate market, improving the goodness of fit and the outsampling behaviour of the model thanks to a more flexible estimation of the neighbourhood parameters.	algorithm;autoregressive model;bayesian programming;computation;neighbourhood (graph theory);random-access memory;star model;smoothing;time complexity	Asuncion Beamonte;Pilar Gargallo;Manuel Salvador	2010	Journal of Geographical Systems	10.1007/s10109-010-0115-7	econometrics;geography;bayesian probability;mathematics;star;autoregressive model;goodness of fit;bayesian inference;statistics	ML	26.2153397990639	-21.78683216805807	187049
1f8861f1cc89d21cdac1d90b379784b852898c33	discretization-based direct random sample generation	monte carlo sampling;discretization;r package;visualization;direct sampling;multivariate random variate generation	An efficientMonte Carlomethod for random sample generation fromhigh dimensional distributions of complex structures is developed. The method is based on random discretization of the sample space and direct inversion of the discretized cumulative distribution function. It requires only the knowledge of the target density function up to a multiplicative constant and applies to standard distributions as well as high-dimensional distributions arising from real data applications. Numerical examples and real data applications are used for illustration. The algorithms are implemented in statistical software R and a package dsample has been developed and is available online. © 2013 Elsevier B.V. All rights reserved.	algorithm;algorithmic efficiency;computation;discretization;execution unit;high- and low-level;hybrid algorithm;list of statistical packages;monte carlo method;network switch;numerical method	Liqun Wang;Chel Hee Lee	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.06.011	discretization error;econometrics;mathematical optimization;dynamic monte carlo method;visualization;hybrid monte carlo;computer science;slice sampling;discretization;mathematics;statistics;monte carlo method	AI	32.21577989879513	-16.409076134098257	187470
ba1f41f389f66bc3fd106fbdb853f0de762e58ab	a bayesian network incorporating observation error to predict phosphorus and chlorophyll a in saginaw bay	bayesian network;bayesian hierarchical modeling;saginaw bay;dreissenid invasion;phosphorus targets;observation error;water quality criteria	Empirical relationships between lake chlorophyll a and total phosphorus concentrations are widely used to develop predictive models. These models are often estimated using sample averages as implicit surrogates for unknown lake-wide means, a practice than can result in biased parameter estimation and inaccurate predictive uncertainty. We develop a Bayesian network model based on empirical chlorophyll-phosphorus relationships for Saginaw Bay, an embayment on Lake Huron. The model treats the means as unknown parameters, and includes structure to accommodate the observation error associated with estimating those means. Compared with results from an analogous simple model using sample averages, the observation error model has a lower predictive uncertainty and predicts lower chlorophyll and phosphorus concentrations under contemporary lake conditions. These models will be useful to guide pending decision-making pursuant to the 2012 Great Lakes Water Quality Agreement. Published by Elsevier Ltd.	bayesian network;estimation theory;granular computing;network model;predictive modelling;surrogates	YoonKyung Cha;Craig A. Stow	2014	Environmental Modelling and Software	10.1016/j.envsoft.2014.02.010	environmental engineering;hydrology;computer science;machine learning;bayesian network;bayesian hierarchical modeling;ecology	ML	25.94283335127319	-18.280581733549212	187614
da68f37a1a5229b8e08bf80c28edfb681f7a7783	testing for generalized lorenz dominance	distribution free;income distribution;lorenz dominance;lorenz curve;asymptotic distribution;generalized asymptotic distribution;power;income inequality	The Generalized Lorenz dominance can be used to take account of differences in mean income as well as income inequality in case of two income distributions possessing unequal means. Asymptotically distribution-free and consistent tests have been proposed for comparing two generalized Lorenz curves in the whole interval [p1, p2] where 0 < p1 < p2 < 1. Size and power of the test has been derived.		Sangeeta Arora;Kanchan Jain	2006	Statistical Methods and Applications	10.1007/s10260-006-0003-y	econometrics;income distribution;lorenz curve;power;mathematical economics;asymptotic distribution;statistics	Theory	32.78679217203877	-20.12497999347106	187957
bc7a33254b3566c29a78cec24315b756ac349184	wishart distributions: advances in theory with bayesian application	bayesian estimation;hypergeometric wishart;matrix variate convergence;matrix variate normal;postprint article;62h05;62f15;wishart distribution;frobenius norm	In this paper, we generalize the Wishart distribution utilizing a fresh approach that leads to the hypergeometric Wishart generator distribution with the Wishart generator and the Wishart as special cases. Important statistical characteristics are derived. The significance of this generator distribution is further demonstrated by assuming a special case as a prior for the underlying matrix variate normal model.		Andriëtte Bekker;Janet van Niekerk;Mohammad Arashi	2017	J. Multivariate Analysis	10.1016/j.jmva.2016.12.002	matrix t-distribution;econometrics;combinatorics;normal-wishart distribution;multivariate gamma function;bayes estimator;inverse-wishart distribution;matrix norm;mathematics;wishart distribution;matrix gamma distribution;matrix normal distribution;statistics	NLP	30.81213677324547	-23.70583385579166	188248
be8764d7d661477003e5d09c3c89bef0cd6bbd39	a birth-process approach to moranda's geometric software-reliability model	software testing;intensity functions geometric software reliability model birth process approach jelinski moranda model software failures geometric de eutrophication model probability generating function reliability functions mean functions;reliability functions;electric shock;stochastic process;cost function;geometric de eutrophication model;statistical independence;reliability function;intensity functions;random variables;indexing terms;counting process;probability generating function;failure analysis;program testing;stochastic processes;timing optimization;solid modeling;fault detection;birth process approach;random variable;statistics;software failures;data systems;failure rate;jelinski moranda model;solid modeling software reliability random variables fault detection cost function statistics electric shock software testing stochastic processes data systems;software reliability;geometric software reliability model;mean functions;program testing software reliability failure analysis	To alleviate some of the objections to the basic Jelinski Moranda (JM) model for software failures, Moranda [14] proposed a geometric de-eutrophication model. This model assumes that the times between failures are statistically-independent exponential random variables with given failure rates. In this model the failure rates decrease geometrically with the detection of a fault. Using an intuitive approach, Musa, Iannino, Okumoto [15], see also Farr [5], derived expressions for the mean and the intensity functions of the process ( ) which counts the number of faults detected in the time interval [0 ] for the Moranda geometric de-eutrophication model. ( ) is studied as a pure birth stochastic process; its probability generating function is derived, as well as its mean, intensity and reliability functions. The expressions for the mean and intensity functions derived by MIO are only approximations and can be quite different from the true functions for certain choices of the failure rates. The exact expressions for the mean function and the intensity function of ( ) are used to find the optimum release time of software based on a cost structure for Moranda’s geometric de-eutrophication model.	approximation;point process;stochastic process;the times;time complexity	Philip J. Boland;Harshinder Singh	2003	IEEE Trans. Reliability	10.1109/TR.2003.813166	reliability engineering;random variable;stochastic process;econometrics;mathematics;statistics	Theory	30.777457973024735	-18.69891887477695	188434
9710c5e22b7a98ad181dd01f863dd8e8c219994e	adjusted confidence bands for complex survey data	local linear estimator;62g05;confidence bands;secondary 62d05;nonparametric regression;complex surveys;primary 62	Confidence bands in nonparametric regression have been studied for a long time with data assumed to be generated from independent and identically distributed (iid) random variables. The methods and theoretical results for iid data, however, do not directly apply to data from stratified multistage samples. In this paper, we extend the confidence bands introduced by Zhang and Lu (2008) for iid case to complex surveys based on an entirely data-driven procedure; the proposed confidence bands incorporate both the sampling weights and the kernel weights. Simulation studies show that the proposed method works well.		Guoyi Zhang;Maozhen Gong;Yang Cheng	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.882946	econometrics;mathematics;confidence and prediction bands;nonparametric regression;statistics	ML	29.835612824947816	-23.54410063573027	188469
55f42d964fd53e6e57caaf37d8136e94de3d530e	application of run time control to a multi-objective, user oriented simulation system	order statistics;confidence level;system dynamics;moments;stability condition;system analysis;technical report;systems of probability distributions;monte carlo;data fitting;steady state;discrete event simulation	The application of simulation as a system analysis tool is growing. One reason for this growth is that user-oriented simulation systems are being developed so that end users can interface directly with simulation systems without support from simulation “experts.” Although this is a healthy trend, end users of simulations tend to accept the results of their simulations without regard to confidence levels, stability conditions, or steady state attainment. It is the authors' experience that users tend to terminate simulations before the system has reached steady state in order to conserve computer time. This paper describes a system for automatic run time control of a discrete event simulation. A variety of run time control techniques were identified. Autocorrelation was selected on the bases of simplicity and efficiency and applied to a “user-oriented” simulation system, “Data System Dynamic Simulator (DSDS).” A brief description of DSDS is provided along with results of a sample simulation employing automatic run time control of a DSDS implemented simulation.	autocorrelation;data system;race condition;run time (program lifecycle phase);simulation;steady state;system analysis;terminate (software)	David Roggendorff;Dale Rowe	1979			econometrics;real-time computing;order statistic;simulation;confidence interval;computer science;technical report;discrete event simulation;mathematics;moment;system analysis;system dynamics;steady state;statistics;curve fitting;monte carlo method;simulation language	Robotics	28.72222040233426	-16.425884106849434	188637
527dae547ad4e315fd8a0a3f3eeed1e03df4ed0c	technological modelling for graphical models: an approach based on genetic algorithms	genetique;search problem;graphic method;selection problem;model selection;scientific modelling;problema seleccion;analisis datos;genetica;model search;optimization method;log linear graphical model;selection modele;problema investigacion;algoritmo genetico;metodo optimizacion;genetics;62f07;data analysis;methode selection;methode graphique;seleccion modelo;statistical computation;calculo estadistico;technological modelling;62 09;methode optimisation;algorithme genetique;graphical model;metodo grafico;analyse donnee;genetic algorithm;context dependent;calcul statistique;selection method;probleme recherche;modele graphique;fitness function;probleme selection	Automatic model search procedures aim at identifying the model that maximises a given +tness function, thereby treating model selection as an optimisation problem. However, it is unrealistic to believe that the +ttest model represents the best solution to the search problem. In fact, even if it is possible to score all of the candidate models, it hardly happens that there exists an unequivocal answer to the question of which model best explains data. An automatic model search procedure for the identi+cation of an optimal set of good models is proposed. In a technological approach to model selection the identi+ed models can co-exist, whereas in a scienti+c modelling approach such models represent a starting point for further context-dependent analysis. An example of the application of the proposed procedure to real data is given. c © 2003 Elsevier B.V. All rights reserved.	context-sensitive language;genetic algorithm;graphical model;mathematical optimization;model selection;search problem	Alberto Roverato;Sandra Paterlini	2004	Computational Statistics & Data Analysis	10.1016/j.csda.2003.11.006	econometrics;scientific modelling;genetic algorithm;search problem;machine learning;context-dependent memory;mathematics;graphical model;data analysis;fitness function;algorithm;model selection;statistics	AI	29.752752775783193	-11.779127372804112	188675
2509784009f8f362233e742562c88d99c7502ea9	testing for positive evidence of equally likely outcomes	testing uniformity;multinomial distribution;62h15;62f25;62f03;roulette;goodness of fit test;intersection union testing;digits of pi equivalence testing intersection union testing multinomial distribution roulette testing uniformity;simultaneous confidence interval;digits of π;equivalence testing	Goodness-of-fit tests allow one to conclude that k possible outcomes are not equally likely. In this paper, we develop an exact equivalence test that allows one to conclude that k possible outcomes are approximately equally likely. We show that the power properties of the test compare favorably to those of possible alternative tests, and we develop an associated simultaneous confidence interval procedure. We apply the test to data sets on the digits of @p, winning roulette numbers, and winning numbers from the Pennsylvania Lottery.		Jesse Frey	2012	J. Multivariate Analysis	10.1016/j.jmva.2011.06.006	econometrics;discrete mathematics;mathematics;goodness of fit;roulette;multinomial distribution;statistics	ML	29.95959282376102	-21.243211948735816	188678
0e3cb069c26b877cb55953e244fd4502b3adb25f	on the first failure time of dependent multicomponent reliability systems	coherent structures;repairable and nonrepairable components;system reliability;fiabilidad;reliability;fiabilite systeme;ifr;stochastic monotonicity;failure;multivariate analysis;dependent maintained reliability system;random variables;mathematical analysis;fiabilidad sistema;performance process;nbu processes;stochastic processes;fiabilite;systeme n composantes;uniformization by a poisson process;nbu;stochastic ordering;repair	In this paper multicomponent reliability systems are considered, where component failure and repair completion rates depend on the state, ages and current repair durations of the other components. This is a generalization of a model of Ross (Ross, S. M. 1984. A model in which component failure rates depend on working set. Naval Res. Logist. Quart. 31 297–300.). Sufficient conditions on the sets of rates which imply stochastic ordering between first failure times of two such systems are found. Sufficient conditions on the rates which imply that the first failure time of such a system is new better than used (NBU) are given. Some results of Barlow and Proschan (Barlow, R. E., Proschan, F. 1976. Theory of maintained systems: Distribution of time to first system failure. Math. Oper. Res. 1 32–42.), Chiang and Niu (Chiang, D. T., Niu, S. C. 1980. On the distribution of time to first system failure. J. Appl. Probab. 17 481–489.), and Ross (Ross, S. M. 1976. On the time to first failure in multicomponent exponen...		Moshe Shaked;J. George Shanthikumar	1988	Math. Oper. Res.	10.1287/moor.13.1.50	random variable;stochastic process;lagrangian coherent structures;stochastic ordering;reliability;mathematics;multivariate analysis;statistics	Theory	31.70558397507963	-18.472332846680043	188861
d77e397f973d23f58a923f8393412e77c497f156	a universal generating function-based multi-state system performance model subject to correlated failures	correlated failure;multi state system;correlated identical components;universal generating function;reliability measures	Multi-state system (MSS) reliability modeling is a paradigm that allows both systems and components to exhibit more than two performance levels. While several researchers have introduced correlation or dependence into MSS models to assess its negative influence on performance and associated measures, these methods exhibit complexity that is exponential or worse in the worst case. To overcome this limitation, this paper proposes an extension to the discrete universal generating function approach for MSS to allow correlation between the elements comprising a multi-state component. We subsequently generalize to the continuous case and allow failures to follow any life distribution. The approach possesses an analytical form and therefore enables efficient performance and reliability assessment as well as sensitivity analysis on the impact of correlation. This sensitivity analysis can be applied to a wide range of measures including performance, reliability, the density function, hazard rate, mean time to failure, availability, and mean residual life. The approach is illustrated through a series of examples, demonstrating the efficiency of the approach to assess performance and reliability as well as to conduct sensitivity analysis. The results indicate that the approach can identify the impact of correlation on performance, reliability, and the many measures of interest.		Bentolhoda Jafary;Lance Fiondella	2016	Rel. Eng. & Sys. Safety	10.1016/j.ress.2016.02.004	reliability engineering;engineering;statistics	SE	28.533821421753064	-17.941518658583803	189081
021a8e7a95af00755bd1432dec8a481770ed5135	statistical inference for the power lindley model based on record values and inter-record times		Abstract A new generalization of the Lindley distribution, called the power Lindley distribution was proposed by Ghitany et al., (2013), which offers a more flexible distribution for modeling lifetime data, such as in reliability. They studied classical inferences for the model based on complete data sets. However, we may deal with record breaking data sets in which only values smaller (or larger) than the current extreme value are reported. In this paper, by using record values and inter-record times, we develop inference procedures for the estimation of the parameters and prediction of future record values for the power Lindley distribution. First, the maximum likelihood estimate of the parameters and their asymptotic confidence intervals are obtained. Next, we consider Bayes estimation under the symmetric (squared error) and asymmetric (linear-exponential (LINEX)) loss functions by using the joint bivariate density function. Since the closed forms of the estimates are not available, we encounter some computational difficulties to evaluate the Bayes estimates of the parameters involved in the model. For this reason, we use Tierney and Kadane’s method as well as Markov Chain Monte Carlo (MCMC) procedure to compute approximate Bayes estimates. We further consider the non-Bayesian and Bayesian prediction for future lower record arising from the power Lindley distribution based on record data. The comparison of the derived predictors is carried out by using Monte Carlo simulations. A real data set is analyzed for illustration purposes.		Abbas Pak;Subhrakanti Dey	2019	J. Computational Applied Mathematics	10.1016/j.cam.2018.08.012	probability density function;applied mathematics;mathematical optimization;extreme value theory;statistical inference;monte carlo method;mathematics;markov chain monte carlo;bayes' theorem;inference;mean squared error	NLP	29.96976766652695	-21.03340333892093	189257
33560180c7e13806b5e0df4eb6addbd5db6770d1	utilizing identity-by-descent probabilities for genetic fine-mapping in population based samples, via spatial smoothing of haplotype effects	genes;modelo lineal generalizado;bayes estimation;similarity metric;62j12;trait linkage analysis;probability;genetic association analysis;covariancia;cladistic analysis;analisis datos;loi probabilite;60j70;ley probabilidad;modele lineaire generalise;generalized linear mixed model;disease;modele lineaire;modele mixte;covariance;genetica poblacion;modelo lineal;statistical regression;genetics;data analysis;estimacion bayes;smoothing methods;mixed model;associations;smoothing;regresion estadistica;ibd;methode lissage;probability distribution;probabilidad;statistical computation;linear model;calculo estadistico;probabilite;alisamiento;mesure similarite;analyse donnee;identity by descent;calcul statistique;generalized linear model;genetique population;spatial smoothing;modelo mixto;regression statistique;similarity measure;covariance structure;sannolikhetsteori och statistik;lissage;structural similarity;ibs;estimation bayes;general pedigrees;snp;population genetics	Genetic fine mapping can be performed by exploiting the notion that haplotypes that are structurally similar in the neighbourhood of a disease predisposing locus aremore likely to harbour the same susceptibility allele. Within the framework of Generalized Linear Mixed Models this can be formalized using spatial smoothing models, i.e. inducing a covariance structure for the haplotype risk parameters, such that risks associated with structurally similar haplotypes are dependent. In a Bayesian procedure a local similarity measure is calculated for each update of the presumed disease locus. Thus, the disease locus is searched as the placewhere the similarity structure produces risk parameters that can best discriminate between cases and controls. From a population genetic perspective the use of an identity-by-descent based similarity metric is theoretically motivated. This approach is then compared to other more intuitively motivated models and other similarity measures based on identity-by-state, suggested in the literature. © 2008 Elsevier B.V. All rights reserved.	locus;similarity measure;smoothing	Linda Hartman;Ola Hössjer;Keith Humphreys	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2008.05.029	probability distribution;mixed model;econometrics;generalized linear mixed model;combinatorics;cladistics;identity by descent;snp;covariance;structural similarity;linear model;gene;probability;generalized linear model;mathematics;data analysis;population genetics;regression analysis;statistics;smoothing	ML	33.27885028752571	-23.906225342050366	189284
bf1a2b8953e826edc76067f0d0935699b90adaba	non-linear modelling and forecasting of s&p 500 volatility	modelo dinamico;forecasting;correlacion;parametric model;conditional volatility;prevision;volatility;modelo markov;dynamic model;estimation non parametrique;modele lineaire;autoregression;non linear model;risque;modele non lineaire;modelo lineal;long terme;dynamic behaviour;analyse canonique;long term;algorithme;algorithm;non parametric estimation;riesgo;volatility forecasting;modelo no lineal;markov model;largo plazo;risk;non parametric model;indexation;modele dynamique;linear model;non linear markov modelling;autoregresion;analisis canonico;volatilite;estimacion no parametrica;correlation;modele markov;volatibilidad;cva;canonical analysis;forecasting method;variance;variancia;canonical variate analysis;algoritmo	This paper investigates the use of a flexible forecasting method based on non-linear Markov modelling and canonical variate analysis, and the use of a prediction algorithm to forecast conditional volatility. We assess the dynamic behaviour of the model by forecasting volatility of a stock index. It is found that the non-linear non-parametric model based on canonical variate analysis forecasts stock index volatility significantly better than the GJR-GARCH(1,1)t model due to the flexibility in accommodating multiple dynamic patterns in volatility which are not captured by its parametric counterpart. © 2002 IMACS. Published by Elsevier Science B.V. All rights reserved.	additive white gaussian noise;algorithm;markov chain;markov model;nonlinear system;parametric model;persistence (computer science);utility functions on indivisible goods;volatility	Peter Verhoeven;Berndt Pilgram;Michael McAleer;Alistair Mees	2002	Mathematics and Computers in Simulation	10.1016/S0378-4754(01)00411-6	forward volatility;econometrics;implied volatility;parametric model;canonical analysis;volatility;forecasting;linear model;risk;mathematics;variance;autoregressive model;markov model;stochastic volatility;sabr volatility model;correlation;statistics	AI	33.404502086019605	-21.997670247534682	189298
aa173a8984b21e26ef1c95bcdf7710fed6019f75	approximate small-sample tests of fixed effects in nonlinear mixed models	first order conditional approximation foce;nonlinear random coefficients model;62h15;62f05;62h05;first order approximation firo to likelihood;approximate f test	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	approximation algorithm;francis;fixed effects model;mixed model;primary source	Julia Volaufova	2014	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.835407	econometrics;mathematical optimization;mathematics;statistics	Robotics	32.606829957286905	-21.113644467295785	189405
a0c93b3e7a3827ca4581ca4bf77e8fe4b11f6252	quasi-continuous histograms	granulation;engineering;49k40;kernels;fuzzy set;procesamiento informacion;analisis estadistico;loi probabilite;ley probabilidad;implementation;probability density function;conjunto difuso;ensemble flou;ingenierie;density estimation;histogram;fonction densite;particion;statistical analysis;density function;funcion densidad;probability distribution;analyse statistique;information processing;kernel density estimate;partition;possibility theory;ingenieria;sistema difuso;systeme flou;theorie information;implementacion;traitement information;high sensitivity;fuzzy system;information theory;fuzzy partition;teoria informacion	Histograms are very useful for summarizing statistical information associated with a set of observed data. They are one of the most frequently used density estimators due to their ease of implementation and interpretation. However, histograms suffer from a high sensitivity to the choice of both reference interval and bin width. This paper addresses this difficulty by means of a fuzzy partition. We propose a new density estimator based on transferring the counts associated with each cell of the fuzzy partition to any subset of the reference interval. We introduce three different methods of achieving this transfer. The properties of each method are illustrated with a classic real observation set. The density estimator obtained relates to the Parzen–Rosenblatt kernel density estimation technique. In this paper, we only consider the monovariate case with precise and imprecise observations. © 2009 Elsevier B.V. All rights reserved.	frank rosenblatt;interpretation (logic);kernel density estimation	Olivier Strauss	2009	Fuzzy Sets and Systems	10.1016/j.fss.2009.01.013	econometrics;probability density function;information processing;information theory;computer science;mathematics;algorithm;fuzzy control system;statistics	Vision	35.488488827557994	-23.381686858146228	189501
9befe179b001a93a261abc06eeb00095200f623f	fast watermark detection scheme from camera-captured images on mobile phones	digital watermark;mobile phone;homography;quadrangle detection;camera;java	An improved scheme for integrity protection of binary images representing text documents based on the topologies of these images is proposed. The image skeleton and the inverse skeleton are found through thinning and the skeleton signature is combined with watermark information. The result is encrypted asymmetrically and hidden in embeddable locations of the image. A series of attack experiments are conducted to demonstrate that the approach is capable of detecting tampering. Even single malicious pixel modifications are detected. The approach has a lower computation cost than previous methods.	algorithm;binary image;block size (cryptography);computation;computational complexity theory;data integrity;digital watermarking;encryption;experiment;hash function;pixel;sensor;thinning	Takao Nakamura;Atsushi Katayama;Masashi Yamamuro;Noboru Sonehara	2006	IJPRAI	10.1142/S0218001406004818	computer vision;homography;digital watermarking;computer science;internet privacy;watermark;java;computer graphics (images)	Vision	38.49363434163172	-11.077821914177976	189526
7c17dd9ec4b5fa095687c0aa4c6a9879b94c826d	audio integrity protection and falsification estimation by embedding multiple watermarks	watermarking;protection watermarking information retrieval streaming media frequency estimation signal processing algorithms data engineering magneto electrical resistivity imaging technique production information analysis;information retrieval;frequency estimation;data engineering;satisfiability;protection;streaming media;magneto electrical resistivity imaging technique;production;signal processing algorithms;information analysis	The recognition of digital audio data manipulation is a challenge addressed by various fragile and contentfragile watermarking algorithms. But so far none of the approaches provides satisfying results with respect to manipulation detection. Especially distinguishing malicious attacks from allowed post production operations is still an open issue. We introduce a novel watermarkingbased approach for audio data falsification recognition. We embed multiple watermarks with various characteristics into the audio data to be protected. By analyzing the quality of the retrieved watermarks we obtain information about the nature of the manipulation of the audio data. In comparison to known feature-embedding algorithms, this is rather a forensic approach using watermarking as a hint. Our method is verified using several audio data sets.	algorithm;data integrity;watermark (data file)	Michael Gulbis;Erika Müller;Martin Steinebach	2006	2006 International Conference on Intelligent Information Hiding and Multimedia	10.1109/IIH-MSP.2006.60	computer vision;information engineering;digital watermarking;computer science;theoretical computer science;internet privacy;data analysis;computer security;satisfiability	DB	37.573178062730015	-12.73668323319347	189592
34a61151128983e58e2c98bb4dc572cc9119555b	validating markov switching var through spectral representations	settore secs p 05 econometria	We develop a method to validate the use of Markov switching models in modelling time series subject to structural changes. Particularly, we consider multivariate autoregressive models subject to Markov switching and derive close-form formulae for the spectral density of such models, based on their autocovariance functions and stable representations. Within this framework, we check the capability of the model to capture the relative importance of highand low-frequency variability of the series. Numerical examples and applications to U.S. macroeconomic and financial data illustrate the behaviour at different frequencies.	autoregressive model;heart rate variability;markov chain;nonlinear system;spectral density;spectrum analyzer;time series	Monica Billio;Maddalena Cavicchioli	2016		10.1007/978-3-319-27284-9_1	mathematical optimization;combinatorics;discrete mathematics;mathematics	ML	26.437315501932105	-20.760993615862148	189783
551d0b63433ffc43829583f2e98eb54cd89a585e	glimmix: software for estimating mixtures and mixtures of generalized linear models	philosophy;humanities;general linear model	GLIMMIX is a commercial WINDOWS-based computer program that implements the EM algorithm (Dempster, Laird and Rubin 1977) for the estimation of finite mixtures and mixtures of generalized linear models. The program allows for the specification of a number of distributions in the exponential family, including the normal, gamma, binomial, Poisson, and multinomial distributions. For each of those distributions, a variety of link functions can be specified to relate the expectation of the dependent variable to a linear predictor. Several statistics, including AIC, CAlC and BIC are computed to aid in model selection (cf. Akaike 1974; Bozdogan 1987), missing values are accommodated, and posterior membership probabilities are computed for cases, included or not included in the analysis. Simple discriminant type models dealing with concomitant variables to describe the classes are supported, and a random responder class can be added to the model. Various graphs are provided. A demonstration version ofthe program can be obtained from http://www/ganuna.rug.nl. Before providing some details on the GLIMMIX software, a brief review of a few relevant issues in Mixture modelling are provided.	generalized linear model	Michel Wedel	2001	J. Classification	10.1007/s00357-001-0008-z	econometrics;computer science;machine learning;mathematics;statistics;general linear model	Logic	30.255646771232122	-22.31387950461895	189893
5eda2d22952bdf1c555658a08a0e6595ea9934a2	statistical verification of conformance to geometric tolerance	verification;modelizacion;metodo estadistico;forma cilindrica;defecto;confidence region;simulation;simulacion;statistical method;prise decision;coordinate measuring machine;modelisation;cylindrical shape;methode statistique;defect;decision theoretic;defaut;tolerancia dimensional;verificacion;forme cylindrique;toma decision;modeling;tolerance dimensionnelle;dimensional tolerance;hypothesis test	Modern coordinate measurement machines have provided industry with new tools for inspecting complex parts. The paper develops statistical procedures that complement these inspection methods, and it ties the conformance problem into the hypothesis testing formalism of conventional statistics. Also, the paper suggests strategies for efficient sampling of the part surface, and an implementation of the usual decision-theoretic formulation of the tradeoff between false acceptance and false rejection of part geometries. These ideas are illustrated through the analysis of real and simulated data for perfect and imperfect cylinders.	conformance testing;rejection sampling;sampling (signal processing);semantics (computer science);theory	Thomas R. Kurfess;David L. Banks	1995	Computer-Aided Design	10.1016/0010-4485(95)96799-R	statistical hypothesis testing;verification;systems modeling;computer science;engineering;artificial intelligence;confidence region;mathematics;geometry;engineering drawing;algorithm;statistics	Robotics	27.569131939245004	-15.62301236648093	190253
261da05db7852ee013bb4faee79b5cd45bac3ebb	analyzing binary outcome data with small clusters: a simulation study	62f40;secondary 62p10;primary 62f10;binary outcome data;random effects logistic regression;62p25;generalized estimating equation;within cluster resampling method;small clusters;62p12;standard logistic regression	Binary outcome data with small clusters often arise in medical studies and the size of clusters might be informative of the outcome. The authors conducted a simulation study to examine the performance of a range of statistical methods. The simulation results showed that all methods performed mostly comparable in the estimation of covariate effects. However, the standard logistic regression approach that ignores the clustering encountered an undercoverage problem when the degree of clustering was nontrivial. The performance of random-effects logistic regression approach tended to be affected by low disease prevalence, relatively small cluster size, or informative cluster size.	simulation	Ying Xu;Chun-Fan Lee;Yin Bun Cheung	2014	Communications in Statistics - Simulation and Computation	10.1080/03610918.2012.744044	econometrics;data mining;mathematics;logistic regression;statistics;generalized estimating equation	ECom	29.127940738170103	-21.754052531880763	190327
3edf5d179337f22badac7f1409760bd0aac41917	generalized spatial dynamic factor models	modelo dinamico;experimental design;processus gauss;conditional independence;mcmc algorithm;analyse multivariable;analisis componente principal;analisis numerico;exponential distribution;chaine markov;cadena markov;analisis factorial;metodo monte carlo;common factor;theorie approximation;filtro kalman;multivariate analysis;65c05;estimacion densidad;analisis datos;saut reversible;ley exponencial;gaussian processes;stochastic method;05bxx;loi exponentielle;algorithme metropolis hastings;metropolis hastings;estimation densite;60g15;echantillonnage;62e17;filtre kalman;dynamic model;analisis correspondencia;plan experiencia;methode monte carlo;spatial variation;estimacion promedio;temporal variation;kalman filter;dynamic system;spatial structure;distribucion estadistica;exponential family;65c40;analyse canonique;analyse numerique;famille exponentielle;sampling;approximation theory;density estimation;data analysis;analyse factorielle;numerical analysis;62k99;posterior distribution;factor model;reversible jump;distribution statistique;plan experience;markov chain monte carlo;spatial dynamic;factor analysis;algoritmo metropolis hastings;principal component analysis;modele factoriel;monte carlo method;correspondence analysis;modele dynamique;variacion espacial;statistical computation;calculo estadistico;canonical transformation;62h25;analyse composante principale;processus gaussien;ley a posteriori;analyse correspondance;methode stochastique;variation spatiale;analisis multivariable;analyse donnee;exponential family factor model gaussian process markov chain monte carlo reversible jump sampling schemes;analisis canonico;reversible jump markov chain monte carlo;calcul statistique;familia exponencial;spatio temporal models;60j10;gaussian process;mean estimation;estimation statistique;muestreo;extended kalman filter;estimation moyenne;proceso gauss;estimacion estadistica;loi a posteriori;statistical estimation;canonical analysis;algorithm design;metropolis hastings algorithm	This paper introduces a new class of spatio-temporal models for measurements belonging to the exponential family of distributions. In this new class, the spatial and temporal components are conditionally independently modeled via a latent factor analysis structure for the (canonical) transformation of themeasurementsmean function. The factor loadings matrix is responsible for modeling spatial variation, while the common factors are responsible for modeling the temporal variation. One of the main advantages of our model with spatially structured loadings is the possibility of detecting similar regions associated to distinct dynamic factors. We also show that the new class outperforms a large class of spatial-temporal models that are commonly used in the literature. Posterior inference for fixed parameters and dynamic latent factors is performed via a custom tailored Markov chain Monte Carlo scheme for multivariate dynamic systems that combines extended Kalman filter-basedMetropolis–Hastings proposal densitieswith block-sampling schemes. Factor model uncertainty is also fully addressed by a reversible jump Markov chain Monte Carlo algorithmdesigned to learn about the number of common factors. Three applications, two based on synthetic Gamma and Bernoulli data and one based on real Bernoulli data, are presented in order to illustrate the flexibility and generality of the new class of models, as well as to discuss features of the proposed MCMC algorithm. © 2010 Elsevier B.V. All rights reserved.	bayesian network;dynamical system;euler–bernoulli beam theory;extended kalman filter;factor analysis;kriging;latent variable;metropolis;metropolis–hastings algorithm;monte carlo algorithm;monte carlo method;region of interest;reversible-jump markov chain monte carlo;sampling (signal processing);sensor;synthetic intelligence;time complexity;word lists by frequency	Hedibert Freitas Lopes;Dani Gamerman;Esther Salazar	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2010.09.020	metropolis–hastings algorithm;econometrics;calculus;gaussian process;mathematics;factor analysis;statistics	AI	34.683653466730995	-23.148449506754805	190485
63268f0c400592fadf13600acf065d66cd2fbcff	efficient computation of hedging parameters for discretely exercisable options	alternative method;modelizacion;approximation asymptotique;metodo monte carlo;arret optimal;matrice intervalle;proceso difusion;finance;intervalo confianza;payoff function;bolsa valores;garantie contre risque;american options;derivative hedging;heuristic method;processus diffusion;methode monte carlo;grupo de excelencia;metodo heuristico;option pricing;opcion financiera;greeks;random number;bourse valeurs;malliavin calculus;stock exchange;identificacion sistema;modelisation;interrupcion optima;confidence interval;fonction gain;tariffication;matriz intervalo;system identification;methode alternative;ciencias basicas y experimentales;tarification;monte carlo method;metodo alternativo;intervalle confiance;matematicas;warranty;interval matrix;estimacion parametro;presupuesto;nombre aleatoire;optimal stopping;garantia contra riesgo;aritmetica intervalo;budget;diffusion process;option financiere;methode heuristique;asymptotic approximation;parameter estimation;estimation parametre;interval arithmetic;grupo a;arithmetique intervalle;monte carlo simulation;numero aleatorio;modeling;complete market;financial option;identification systeme;finanzas;funcion ganancia;tarificacion;aproximacion asintotica	We propose an algorithm to calculate confidence intervals for the values of hedging parameters of discretely exercisable options using Monte-Carlo simulation. The algorithm is based on a combination of the duality formulation of the optimal stopping problem for pricing discretely exercisable options and Monte-Carlo estimation of hedging parameters for European options. We show that the width of the confidence interval for a hedging parameter decreases, with an increase in the computer budget, asymptotically at the same rate as the width of the confidence interval for the price of the option. The method can handle arbitrary payoff functions, general diffusion processes, and a large number of random factors. We also present a fast, heuristic, alternative method and use our method to evaluate its accuracy. Efficient Computation of Hedging Parameters for Discretely Exercisable Options The idea behind no-arbitrage option pricing is that, in a complete market, one can almost surely replicate the payoff of an option using a suitably chosen portfolio of instruments. The construction of this replicating portfolio is based on the computation of the hedging parameters, or sensitivities, of option prices with respect to parameters of the underlying process.1 Indeed, the first derivative of the option price with respect to the initial asset price, ∆, corresponds to the amount of the underlying asset held in the replicating portfolio, while the second derivative, Γ, corresponds to the characteristic time interval between rebalancings. Reliable estimation of option prices and hedging parameters, or option price sensitivities, has become very important with the ever expanding range of applications of options from, for example, problems in supply chain management, to problems in energy finance and real estate. In this paper we develop an algorithm that uses Monte-Carlo simulation to estimate option price sensitivities for options with multiple exercise dates and a potentially large number of underlying assets. The advantage of Monte-Carlo simulation and the reason it is the method of choice for problems with many assets is that, by its nature, Monte-Carlo simulation does not suffer from an exponential increase in effort for a linear increase in the number of underlying assets, a common problem with finite difference discretizations of partial differential equations and high dimensional lattice algorithms. In addition, Monte-Carlo simulation offers an estimate of its own accuracy and is relatively easy to perform in parallel, leading to significant increases in computational speed. In the literature, Monte-Carlo simulation has been used to calculate sensitivities of the option price for European options; i.e., options with a single exercise date, see Glynn (1989), Broadie and Glasserman (1996), Fournié, Lasry, Lebuchoux, Lions, and Touzi (1999), and Glasserman (2003) for an overview. We show how the simulation-based algorithms for calculating sensitivities of European options, and in particular the likelihood ratio algorithm proposed by Broadie and Glasserman (1996), can be used to compute confidence intervals for the values of sensitivities of discretely exercisable options with multiple exercise dates. To this end, we combine the likelihood ratio algorithm for estimating sensitivities of European options with an algorithm based on a dual representation of discretely exercisable option prices, originally proposed by Davis and Karatzas (1994) and further developed by Rogers (2002), Haugh and Kogan (2004), and Andersen and Broadie (2004). The duality based algorithm provides confidence intervals for the price of a discretely exercisable option. Our algorithm uses these intervals in a multi-stage algorithm, where Monte-Carlo simulation is employed at every stage. 1These sensitivities are frequently referred to as ”Greeks”.	algorithm;computation;finite difference;heuristic;monte carlo method;optimal stopping;self-replication;simulation;time complexity	Ron Kaniel;Stathis Tompaidis;Alexander Zemlianov	2008	Operations Research	10.1287/opre.1080.0557	financial economics;econometrics;mathematics;mathematical economics;statistics;monte carlo method	ML	30.749672728306358	-16.991651067531418	190615
85f34934501e23490f46e85a0bb4bf4ff919de6d	a discrete point estimate method for probabilistic load flow based on the measured data of wind power	probability;random variables wind power generation probability density function approximation methods load flow;wind power plants;probability density function;wind power;random variables;accuracy;stochastic processes;power system discrete point estimate method probabilistic load flow plf calculation integrated wind power probability density function pdf monte carlo method gram charlier expansion method output random variable cumulative distribution input stochastic variables ieee 16 generator system;load flow;gram charlier expansion;wind power cumulative distribution gram charlier expansion point estimate probabilistic load flow probability density function;approximation methods;point estimate;correlation;wind power generation;cumulative distribution;probabilistic load flow;wind power plants load flow monte carlo methods probability stochastic processes;monte carlo methods	Probabilistic load flow (PLF) calculation is the first step to evaluate the impact of the integrated wind power to the power system. The wind power is featured with stochastic and variable properties and it's hard to fit its distribution characteristics to any common probability density function (PDF). However, the traditional methods including Monte Carlo for PLF are based on the input variable's PDF. In the paper, the point estimate method and Gram-Charlier expansion method are combined. Based only on the sample data of the wind power, the expectation, variance and cumulative distribution of the output random variables can be estimated with the method by 2n+1 times of load flow calculation where n is the number of input stochastic variables, exempting the need for distribution of the input variables. The simulation results in the IEEE 16-generator system show that the method provides high precision with less computation burden. The method can also be applied to other problems with uncertainty factors whose distribution is unknown in the power system.	computation;monte carlo method;newton's method;portable document format;simulation	Xiaomeng Ai;Jinyu Wen;Tong Wu;Wei-Jen Lee	2012	IEEE Transactions on Industry Applications	10.1109/IAS.2012.6373999	econometrics;mathematical optimization;mathematics;statistics	Visualization	31.44081635884785	-16.84552090829541	190812
d7ba23e935eb62cac89d2c476925e5796463c7a5	a comparison of a new family of goodness-of-fit statistics	test hypothese;multinomial distribution;goodness of fit;ajustamiento modelo;likelihood ratio;variable aleatoire;test hipotesis;variable aleatoria;ajustement modele;loi multinomiale;model matching;random variable;rapport vraisemblance;ley multinomial;relacion verosimilitud;hypothesis test	Wc carry out a study of the family of Q(Ps) statistics introduced by Lorenzen [1] for testing goodness-of-fit. The exact powers based on exact critical regions are calculated for each statistic against different alternatives. ©Elsevier Science Inc. 1996 1. I N T R O D U C T I O N One way to resolve the goodness-of-fi t p roblem consists of classifying the underlying r andom variable on to M mutually exclusive classes with probabilities 7r 1 . . . . . ~'M where EM 1 ~ i = 1 and observing n independent realizations of the variable. Then the goodness-of-fi t problem can be reduced to testing a hypothesis about the pa ramete r ~r= (7rl, . . . , zr M) f rom a mult inomial distribution: n~ P ( X = x ) 7rl!...TrM ! ~ ' ...rr~ M, i.e., Io test the null hypothesis	pa-risc	María del Carmen Pardo;Julio Angel Pardo	1996	Inf. Sci.	10.1016/S0020-0255(96)00138-7	random variable;econometrics;statistical hypothesis testing;combinatorics;likelihood-ratio test;exact statistics;mathematics;goodness of fit;multinomial distribution;statistics	Theory	31.823184877558813	-21.27778417447557	190859
34fc20a2538336777cadc6be424b2623ebe7ac48	analysis and forecasting of time series by averaged scalar products of flow vectors	article	"""The relat ionship between the quality of state space reconst ruction and the accuracy in time series forecast ing is analyzed. The averaged scalar product of the dynamical system flowvectors has been used to give a degree of determinism to the selected state space reconst ruct ion. This value helps dist inguish between those regions of the state space where predictions will be accurate and those where they are not . A time series measured in an industri al environment where noise is present is used as an example. It is shown that prediction methods used to estimate futu re values play a less important role than a good reconstruction of the state space itself. 1. Int roduct ion Much work has recently been done in nonlinear time series pr edictio n [1-4] . However , most of this effort has been focused on t ime series originat ing from dyn amical systems where there is a well-defined, alt hough complex and ofte n analytic, underlying model. For such systems, a great number of different techniques have been developed to tackle the pr edict ion problem. These techn iques include state space reconst ru ction of chaotic sys tems [57] as well as var ious methods to ap proximate future values from eit her local [2, 8] or global models [1,9]. State space reconstruction is aimed at obtaining a trajectory in t he state space leading to a determinist ic reconstru cti on of the t ime series. This is usually accomplished using Takens' theorem [10], which states that if the underlying dynam ical syst em has dimension d, t he reconstru ct ion , usually called 26 C. Santa Cruz, R. Huerta, J. R. Dorronsoro, and Vicente Lopez an embedding, can be carr ied out using delay vecto rs in an m-dimensional space (m 2': 2d + 1). Unlike the behav ior of irregular time series, the state space usua lly demonst rates simplicity and regulari ty. Vector fields of the state space are approximated to est ima te future values. The quality of the predictions st rongly depend s on the quality of the state space reconstruction. Furthermore, any lack of accuracy or defect in t he state space reconstruction has consequences for t he prediction of t ime series. Therefore, a measure of the quality of a state space reconst ruction gives an est ima te of the goodness of the forecasting model. The average d scalar product P of the dynami cal system flow vecto rs [5] has recently been used to determine optimal state space reconstruction by maximizing the P -value as a function of both the state space dimension and the time delay. The P-value can be used to qualify th e """"degree of determinism"""" of state space reconstruction ; tha t is, it provides a measure of how para llel the local flow vectors are throughout th e state space. It has been shown [5] th at for tim e series originating from ordinary differential equa tions the P-value can be raised close to unity. This value indic ates that the neighboring flow vectors in th e reconstructed state space are para llel to each other. On t he oth er hand, neighboring flows for time series contamina ted wit h noise or originati ng from ill-defined syste ms will be far from par allel. This leads to lower P-values. This means that th e syste ms are less determini stic than the former, since the flow of point s along the state space are not so well-defined. Hence, the predict ion will not be as accurate . This work highlight s the exist ing relati onship between the characteristics of state space reconstruction of t ime series and t he quality of its predictions in light of results derived from the P-value. Some of th e conclusions drawn from state space analysis apply to both , substant iat ing t he results obtained in the pr ediction and devising new prediction algorithms. As an example, we will use the state space reconstruction and time series prediction of a variable measured in an industrial environment . This t ime series corresponds to an imp ortant temp erature measur ed in a petrochemical plant. Forecasting is one of the techniques that has been int egrat ed into th e HINT proj ect (ESP RIT) for int elligent control. The series is sampled every five minutes and the time histor y over three month s (25,000 dat a points) is used. Figure 1 illustrates 20 hour s of the act ua l time series and the filtered signal obtained after removing th e high frequences using a low-pass filter . The filtered signal will be used throughou t this paper . Since the time series is measur ed in an industri al setting where a grea t numb er of uncont rolled factor s are no doub t present , the und erlying mod el is not well-defined. This lack of accuracy in the definit ion of the system leads to various problems in both state space reconstructi on and the quality of the prediction that would ot herwise not appear in sit uations where a well-defined model existed . The article is st ructured as follows. Section 2 describ es problems th at arise when performing state space reconstructions of real tim e series. Section 3 ana lyses t he forecasting of t ime series using result s obtained in the previous section, and finally, section 4 summarizes the conclusions of this pap er . Analysis and Forecasting of Tim e Series 10 ,------.----,--------,----------,"""	ana (programming language);approximation algorithm;broadcast delay;carr–benkler wager;dynamical system;hough transform;low-pass filter;nonlinear system;scalar processor;software bug;state space;test and evaluation master plan;time series;value (computer science);ical	Carlos Santa Cruz;Ramón Huerta;José R. Dorronsoro;Vicente López	1994	Complex Systems		econometrics;computer science;data mining;mathematics;statistics	ML	37.39536713105917	-18.220297344867827	191027
a88b7f08a1a7a065266443912fbc6f290b2c92b9	watermark template attack	digital watermarking	This paper presents a new attack, called the watermark template attack, for watermarked images. In contrast to the Stirmark benchmark [1-2], this attack does not severely reduce the quality ofthe image. This atack maintains, therefore, the commercial value of the watermarked image. In contrast to previous approaches [3-4], it is not the aim of the attack to change the statistics of embedded watermarks fooling the &tection process but to utilize specific concepts that have been recently developed for more mbust watermarking schemes. The attack estimates the corresponding template points in the FFT domain and then removes them using local interpolation. We demonstrate the effetiveness of the attack showing different test cases that have been watermarked with commercial available watermark products. The approach presented is not limited to the FFT domain. Other transformation domains may be also exploited by very similar variants of the described attack.	benchmark (computing);embedded system;fast fourier transform;interpolation;test case;watermark (data file)	Alexander Herrigel;Sviatoslav Voloshynovskiy;Yuri B. Rytsar	2001		10.1117/12.435423	watermarking attack;digital watermarking;internet privacy;world wide web;computer security;physics	Security	39.14308498483495	-11.58140729634118	191196
63441af8a49666c2787df13211a1aa926a41b7b9	exact likelihood ratio testing for homogeneity of the exponential distribution	test hypothese;homogeneity testing;optimal test;exact test;62j12;metodo estadistico;likelihood ratio;test statistique;exponential distribution;bahadur efficiency;asymptotic efficiency;62h15;asymptotic optimality;melange loi probabilite;ley exponencial;loi exponentielle;test hipotesis;echantillonnage;test estadistico;62f05;loi exacte;simulacion numerica;statistical test;mixed distribution;statistical method;physical sciences;distribucion estadistica;sampling;test homogeneidad;estimation parametrique;optimalite asymptotique;test optimal;efficacite bahadur;distribution statistique;mixture model;eficacia asintotica;methode statistique;exact slopes;test homogeneite;simulation numerique;test exact;62f03;likelihood ratio test;efficacite asymptotique;algorithme em;homogeneity test;mezcla ley probabilidad;algoritmo em;muestreo;mixture models;rapport vraisemblance;exact distribution;test razon verosimilitud;em algorithm;test rapport vraisemblance;subpopulation model;statistical distribution;62e15;numerical simulation;relacion verosimilitud;hypothesis test	The aim of this paper is to discuss homogeneity testing of the exponential distribution. We introduce the exact likelihood ratio test of homogeneity in the subpopulation model, ELR, and the exact likelihood ratio test of homogeneity against the two-components subpopulation alternative, ELR2. The ELR test is asymptotically optimal in the Bahadur sense when the alternative consists of sampling from a fixed number of components. Thus in some setups the ELR is superior to frequently used tests for exponential homogeneity which are based on the EM algorithm. We demonstrate this fact by both theoretical comparisons and simulations. Note: The following files were submitted by the author for peer review, but cannot be converted to PDF. You must view these files (e.g. movies) online.	asymptotically optimal algorithm;expectation–maximization algorithm;exponential time hypothesis;portable document format;sampling (signal processing);simulation;time complexity	Milan Stehlík;Helga Wagner	2011	Communications in Statistics - Simulation and Computation	10.1080/03610918.2010.551011	computer simulation;econometrics;statistical hypothesis testing;likelihood-ratio test;calculus;mixture model;mathematics;statistics	ML	32.46195315325408	-22.272352153962764	191266
ed768b38c16b3b57b2d76e72e3a56c1bd9471941	optimal importance sampling for simulation of lévy processes		This paper provides an efficient algorithm using Newton's method under sample average approximation (SAA) to solve the parametric optimization problem associated with the optimal importance sampling change of measure in simulating Lévy processes. Numerical experiments on variance gamma (VG), geometric Brownian motion (GBM), and normal inverse Gaussian (NIG) examples illustrate the computational advantages of the SAA-Newton algorithm over stochastic approximation (SA) based algorithms.	approximation algorithm;brownian motion;computation;experiment;gamma correction;ibm notes;importance sampling;lvm;mathematical optimization;mesa;newton;newton's method;numerical method;optimization problem;sampling (signal processing);simulation;stochastic approximation;stochastic optimization	Guangxin Jiang;Michael C. Fu;Chenglong Xu	2015	2015 Winter Simulation Conference (WSC)		pricing;random variable;econometrics;mathematical optimization;mathematics;social simulation;newton's method;statistics;monte carlo method	Robotics	32.47113089847205	-16.132151212976392	191279
ca8a7b379c92dd6387fd52e591256f760b12fe46	constraining the optimization of a fuzzy logic controller using an enhanced genetic algorithm	automatic control;constrained optimization;logic arrays;optimisation;constraint optimization fuzzy logic genetic algorithms fuzzy sets humans process control automatic control logic arrays algorithm design and analysis optimization methods;genetic operator;fuzzy set;constraint optimization;fuzzy control;rule based;search techniques;fuzzy logic controller;indexing terms;fuzzy sets;fuzzy logic;search problems genetic algorithms fuzzy control controllers optimisation;genetic operators;controllers;genetic operators optimization fuzzy logic controller enhanced genetic algorithm search techniques fuzzy sets;process control;genetic algorithm;genetic algorithms;optimization;common sense;humans;search problems;enhanced genetic algorithm;algorithm design and analysis;optimization methods	Fuzzy logic controllers (FLCs) are gaining in popularity across a broad array of disciplines because they allow a more human approach to control. Recently, the design of the fuzzy sets and the rule base has been automated by the use of genetic algorithms (GAs) which are powerful search techniques. Though the use of GAs can produce near optimal FLCs, it raises problems such as messy overlapping of fuzzy sets and rules not in agreement with common sense. This paper describes an enhanced genetic algorithm which constrains the optimization of FLCs to produce well-formed fuzzy sets and rules which can be better understood by human beings. To achieve the above, we devised several new genetic operators and used a parallel GA with three populations for optimizing FLCs with 3x3, 5x5, and 7x7 rule bases, and we also used a novel method for creating migrants between the three populations of the parallel GA to increase the chances of optimization. In this paper, we also present the results of applying our GA to designing FLCs for controlling three different plants and compare the performance of these FLC's with their unconstrained counterparts.	base;controllers;fibrolamellar hepatocellular carcinoma;fuzzy control system;fuzzy logic;fuzzy set;genetic algorithm;genetic operator;mathematical optimization;population;rule (guideline);rule-based system;well-formed formula	France Cheong;Richard Lai	2000	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.826945	mathematical optimization;constrained optimization;genetic algorithm;computer science;artificial intelligence;machine learning;mathematics;fuzzy set	Visualization	26.506441154319813	-10.399487908779765	191289
e3df4049890eab4b039d7c4c0ee4ee86bc2674be	the use of order statistics in estimation	order statistic	A survey is given of recent advances, including a number of original contributions by the author, in the use of order statistics to obtain point and interval estimates of the parameters of various statistical populations from complete and from censored samples. In a few cases estimators based on order statistics are the efficient estimators, but more often they are substitute estimators that sacrifice some efficiency in the interest of computational simplicity and/or robustness in the presence of outliers. In life testing, they are often used to obtain estimates of the parameters of the life distribution before all of the items placed on test have failed. Point estimators based on order statistics may be either best linear unbiased estimators, based on all available observations from complete or censored samples or on one or more observations chosen in some optimal manner, or maximum-likelihood estimators, based on censored samples, of a single parameter or of two or more parameters jointly. Interval estimators may be based on percentage points of order statistics or of functions of order statistics, or they may be based on asymptotic variances and covariances of maximum-likelihood estimators. A summary is given of available results, together with a list of references and examples of applications to such problems as estimating the scatter of bullets aimed at a target and the reliability of an electronic device. E VERY statistical population is characterized by one or more parameters whose values specify the population completely, once the form of the population is known. The familiar normal population has two parameters, the mean and the standard deviation, while the one-parameter negative exponential population has, as the name implies, just one parameter, which is both the mean and the standard deviation. A mathematical expression that estimates a population parameter as a function of a sample statistic is called an estimator, while a numerical value obtained from an estimator by substituting a numerical value for the sample statistic is called an estimate. Distinction should be made at the outset between point estimation and interval estimation. In the former the result is a single value, while in the latter it is an interval with which is associated a specified level of confidence that the population parameter lies in this interval. It is desirable that a point estimator should be unbiased; that is, that its expected value should be equal to the true value of the parameter. It is also desirable that		H. Leon Harter	1968	Operations Research	10.1287/opre.16.4.783	robust statistics;bootstrapping;econometrics;mathematical optimization;estimator;order statistic;interval estimation;confidence interval;trimmed estimator;unbiased estimation of standard deviation;point estimation;robust measures of scale;m-estimator;mathematics;invariant estimator;statistics;l-estimator	DB	29.935412815975916	-21.524265017179655	191507
5317265a82998caa8e3172c4c14aa964cb539cb2	generalised procrustes analysis with optimal scaling: exploring data from a power supplier	analisis numerico;analisis datos;database;base dato;ajustement;large data sets;analyse numerique;fitting;optimal scaling;data analysis;numerical analysis;multivariate data analysis;statistical computation;numero de condicionamiento;calculo estadistico;algebra lineal numerica;algebre lineaire numerique;base de donnees;condition number;65f35;analyse donnee;calcul statistique;numerical linear algebra;ajuste;linear extension;indice conditionnement	(2009). Generalised procrustes analysis with optimal scaling: exploring data from a power supplier. Copyright and Moral Rights for the articles on this site are retained by the individual authors and/or other copyright owners. For more information on Open Research Online's data policy on reuse of materials please consult the policies page. Abstract Generalised Procrustes Analysis (GPA) is a method for matching several, possibly large, data sets by fitting to each other using transformations, typically rotations. The linear version of GPA has been applied in a wide range of contexts. A non-linear extension of GPA is developed which uses Optimal Scaling (OS). The approach is suited to match data sets that contain nominal variables. A data base of a Dutch power supplier that contains many categorical variables unfit for the usual linear GPA methodology is used to illustrate the approach.	database;image scaling;level of measurement;nonlinear system;open research;operating system;procrustes analysis;scalability	Jaap Wieringa;Garmt Dijksterhuis;John Gower;Frederieke van Perlo	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2009.03.017	combinatorics;numerical analysis;condition number;mathematics;multivariate analysis;numerical linear algebra;data analysis;algorithm;statistics;linear extension	DB	34.08909399193672	-23.530988106585784	191581
d7d5551265de5d35efc70d5aa065e5e5e71eb94b	video authentication using spatio temporal relationship for tampering detection		Abstract This paper discusses a novel approach to detect inter frame and intra frame video forgery using content based signature. A novel technique called the “Spatio Temporal Triad Feature Relationship” (STTFR) is employed to generate a unique content based signature – value for any given video sequence. The proposed STTFR algorithm aims to verify video integrity through the creation of a 128 bit message digest from the input video of variable length that will be unique to that video and acts as a fingerprint. Change in the video sequence, either at the spatial or at the temporal level will result in a different fingerprint than the one obtained originally. The knowledge of the signature will not enable any person/entity to recreate the original video as the signature is generated by combining spatial and temporal fingerprints in an orderly and systematic approach. We have verified our technique with standard datasets and found accurate results.	authentication	N. SowmyaK.;H. R. Chennamma;Lalitha Rangarajan	2018	J. Inf. Sec. Appl.	10.1016/j.jisa.2018.07.002	theoretical computer science;cryptographic hash function;intra-frame;inter frame;authentication;computer science	NLP	38.52920241667202	-12.897866550522203	191870
e03b23f72bcd6fa545afeed035169c3664dfa425	a dct domain visible watermarking technique for images	internet;copy protection;digital libraries;discrete cosine transforms;image coding;multimedia communication;dct domain;dct domain visible watermarking technique;internet;computer networks;conventional libraries;digital libraries;digital watermarking;mathematical model;multimedia object;publicly available images;scholarly research;unauthorized use	The growth of computer networks has boosted the growth of the information technology sector to a greater extent. There is a trend to move from conventional libraries to digital libraries. In the digital libraries images and texts are made available through the internet for scholarly research. At the same time care is taken to prevent the unauthorized use of the images commercially. In some cases the observer is encouraged to patronize the institution that owns the material. To satisfy both these needs simultaneously the owner needs to use visible watermarking. Visible watermarking is a type of digital watermarking used for protection of publicly available images. In this paper, we describe a visible watermarking scheme that is applied into the host image in the DCT domain. A mathematical model has been developed for that purpose. We have also proposed a modification of the algorithm to make the watermark more robust.	algorithm;authorization;digital library;digital watermarking;discrete cosine transform;internet;library (computing);mathematical model	Saraju P. Mohanty;K. R. Ramakrishnan;Mohan S. Kankanhalli	2000			digital library;digital watermarking alliance;digital watermarking;computer science;theoretical computer science;multimedia;world wide web	Graphics	37.74101046469457	-12.31540599569347	192011
0d351f1b50d08080260eab642d3642415a594ae0	confidence intervals of effect size for randomized comparative parallel-group studies with unequal variances	effect size;small sample;secondary 62e20;signed log likelihood ratio;r formula;confidence interval;primary 62f03;asymptotic theory	Recent years have seen a heightened interest in estimating effect size—a common measure of effect magnitude in biomedical research—because of its direct clinical relevance. In this article, three interval estimates of effect size for randomized comparative parallel-group studies with unequal variances are discussed. Two real-life examples illustrate that confidence intervals obtained by three methods are quite different, especially when the sample sizes are small. Simulation results show that confidence intervals generated by the modified signed log-likelihood ratio method yield essentially the exact coverage probabilities, whereas the other two methods, even though they are more popular methods, yield less satisfactory results.	randomized algorithm	Jianrong Wu;Augustine C. M. Wong;Guoyong Jiang	2014	Communications in Statistics - Simulation and Computation	10.1080/03610918.2012.752833	econometrics;mathematical optimization;confidence interval;mathematics;asymptotic theory;cdf-based nonparametric confidence interval;effect size;robust confidence intervals;statistics	Metrics	29.896407587249403	-21.61547008433307	192689
5d9761b6f06b3cc9f8b1b1bdda795d6ccac3c93d	constructing optimal designs for fitting pharmacokinetic models	general optimisation;design criteria;pharmacokinetic model;model checking;mixed model;numerical integration;optimal design;d optimality;nonlinear mixed model;parameter estimation;model fitting;bayesian optimal design	We consider some computational issues that arise when searching for optimal designs for pharmacokinetic (PK) studies. Special factors that distinguish these are (i) repeated observations are taken from each subject and the observations are usually described by a nonlinear mixed model (NLMM), (ii) design criteria depend on the model ®tting procedure, (iii) in addition to providing ecient parameter estimates, the design must also permit model checking, (iv) in practice there are several design constraints, (v) the design criteria are computationally expensive to evaluate and often numerical integration is needed and ®nally (vi) local optimisation procedures may fail to converge or get trapped at local optima. We review current optimal design algorithms and explore the possibility of using global optimisation procedures. We use these latter procedures to ®nd some optimal designs. For multi-purpose designs we suggest two surrogate design criteria for model checking and illustrate their use.	algorithm;analysis of algorithms;converge;emoticon;global optimization;local optimum;mathematical optimization;mixed model;model checking;multi-purpose viewer;nonlinear system;numerical analysis;numerical integration;optimal design;public-key cryptography	B. Jones;Jun Wang	1999	Statistics and Computing	10.1023/A:1008922030873	mixed model;model checking;econometrics;mathematical optimization;numerical integration;optimal design;mathematics;estimation theory;statistics	EDA	32.0668638339531	-14.642088612403255	192715
d2dac3f09b3464f68331762a954c4af90f7dd451	gaussian process model - an exploratory study in the response surface methodology	ols;multiresponse;sur;optimization;gaussian process	Quality and Reliability Engineering International#R##N#Early View (Online Version of Record published before inclusion in an issue)	exploratory testing;gaussian process;response surface methodology	Nuno Ricardo Costa;João Lourenço	2016	Quality and Reliability Eng. Int.	10.1002/qre.1940	econometrics;engineering;data mining;gaussian process;mathematics;statistics	HCI	28.370104771254727	-20.875657087248218	192832
43facdcfaae3c378cd432d0f451f67394ae4761b	tree-structured smooth transition regression models	test hypothese;classification automatique statistiques;parametric model;smooth transition regression;analyse multivariable;selection problem;analisis numerico;test statistique;problema seleccion;metodo monte carlo;cart;recursive partitioning;regression tree;theorie approximation;62h15;validacion cruzada;methode parametrique;multivariate analysis;functional form;65c05;analisis datos;stochastic method;05c05;predictive value;metodo parametrico;test hipotesis;test estadistico;aproximacion;62e17;non linear regression;tree architecture;parametric method;estimation non parametrique;statistical test;methode monte carlo;regression model;estadistica rango;statistical significance;non linear model;modele non lineaire;lagrange multiplier;time series;distribucion estadistica;modele parametrique;statistical regression;62jxx;analyse numerique;model complexity;approximation;discriminant analysis;estimation parametrique;analyse discriminante;approximation theory;non parametric estimation;62f07;data analysis;analisis discriminante;confidence interval;modelo regresion;modelo no lineal;numerical analysis;prediction theory;distribution statistique;modeling cycle;62g10;62h30;regresion multiple;regresion estadistica;modele regression;monte carlo method;tree structure;statistical computation;62f03;multiplicateur lagrange;calculo estadistico;validation croisee;smooth transition;semi parametric model;nonparametric regression;statistical inference;multiplicador lagrange;methode stochastique;rank statistic;analisis multivariable;analyse donnee;statistique rang;calcul statistique;cross validation;estimacion no parametrica;parameter estimation;estimation statistique;theorie prediction;regression statistique;estimacion estadistica;statistical estimation;regression multiple;statistical distribution;regression trees;lm test;artificial neural network;nonlinear regression;nonlinear model;probleme selection;hypothesis test;multiple regression;metodo estocastico	This paper introduces a tree-based model that combines aspects of classification and regression trees (CART) and smooth transition regression (STR). The model is called the STR-tree. The main idea relies on specifying a parametric nonlinear model through a tree-growing procedure. The resulting model can be analyzed as a smooth transition regression with multiple regimes. Decisions about splits are entirely based on a sequence of Lagrange multiplier (LM) tests of hypotheses. An alternative specification strategy based on a 10-fold cross-validation is also discussed and a Monte Carlo experiment is carried out to evaluate the performance of the proposed methodology in comparison with standard techniques. The STR-tree model outperforms CART when the correct selection of the architecture of simulated trees is discussed. Furthermore, the LM test seems to be a promising alternative to 10-fold cross-validation. Function approximation is also analyzed. When put into proof with real and simulated data sets, the STR-tree model has a superior predictive ability than CART. © 2007 Elsevier B.V. All rights reserved.	approximation;cross-validation (statistics);decision tree learning;lagrange multiplier;monte carlo method;nonlinear system	Joel Corrêa da Rosa;Alvaro Veiga;Marcelo C. Medeiros	2008	Computational Statistics & Data Analysis	10.1016/j.csda.2007.08.018	econometrics;statistical hypothesis testing;calculus;mathematics;linear discriminant analysis;artificial neural network;regression analysis;nonlinear regression;statistics	AI	32.82328118094146	-22.934293776801635	193319
b79590bc51bc12d2253f6fc57f828bb4f5de3253	estimation of pharmacokinetic parameters from non-compartmental variables using microsoft excel®	pharmacokinetic parameters;non compartmental variables;random effects;solver r;microsoft excel r;compartment model parameters;solver;visual basic;microsoft excel;compartment model;parameter estimation	This study was conducted to develop a method, termed 'back analysis (BA)', for converting non-compartmental variables to compartment model dependent pharmacokinetic parameters for both one- and two-compartment models. A Microsoft Excel spreadsheet was implemented with the use of Solver and visual basic functions. The performance of the BA method in estimating pharmacokinetic parameter values was evaluated by comparing the parameter values obtained to a standard modelling software program, NONMEM, using simulated data. The results show that the BA method was reasonably precise and provided low bias in estimating fixed and random effect parameters for both one- and two-compartment models. The pharmacokinetic parameters estimated from the BA method were similar to those of NONMEM estimation.	anatomical compartments;bayesian network;business architecture;computer program;estimated;futures studies;kernel density estimation;multi-compartment model;non-repudiation;paper;population parameter;random effects model;solver;spatial variability;spreadsheet;visual basic	Chantaratsamon Dansirikul;Malcolm Choi;Stephen B. Duffull	2005	Computers in biology and medicine	10.1016/j.compbiomed.2004.02.008	econometrics;simulation;computer science;mathematics;estimation theory;multi-compartment model;statistics;random effects model	ML	30.28246929467833	-23.26609704382046	193562
dd87a627624fdc479dec052b42f247aa8b533dfc	secret sharing in images based on error-diffused block truncation coding and error diffusion		This paper presents a novel ((n,n))–threshold secret sharing in images by using error-diffused block truncation coding (EDBTC) and error diffusion. The proposed scheme is designed to share a secret binary image, such as text image or natural image, into (n) EDBTC-compressed images including meaningful contents. The compressed shadows generated by our proposed scheme have good visual quality and no pixel expansion, which are beneficial to reduce suspicions from the invaders. Each value in the secret image is reconstructed by using boolean XOR operations which are considered as light-weight devices, thus the recovering process is simple and fast. In addition, EDBTC-compressed images instead of the original format images are selected as shadows, which can improve the efficiency for data transmission and storing. The experimental results demonstrate that the proposed scheme offers a high secure and effective mechanism for secret image sharing.	block truncation coding;error diffusion;secret sharing	Duanhao Ou;Xiaotian Wu;Lu Dai;Wei Sun	2013		10.1007/978-3-662-43886-2_9	block truncation coding	Vision	38.87450395853972	-10.645531710486303	193770
93a2caf464c25a091cdd42b9f10486519fef47e8	multiple comparisons with more than one control for exponential location parameters under heteroscedasticity	62 07;two parameter exponential distribution;experimental design;metodo estadistico;test multiple;exponential distribution;comparaison par paire;intervalo confianza;bonferroni inequality;analisis datos;ley exponencial;05bxx;loi exponentielle;location parameter;test comparacion;62f25;two stage procedure;simulacion numerica;estimation non parametrique;plan experiencia;multiple test;parametre position;statistical method;valor critico;statistical regression;inegalite bonferroni;62j15;estimation parametrique;non parametric estimation;data analysis;confidence interval;62k99;comparacion por pares;plan experience;heteroscedasticidad;one stage procedure;62g15;multiple comparisons;methode statistique;regresion estadistica;disgualdad bonferroni;intervalle confiance;heteroscedasticite;simulation numerique;comparacion multiple;62f03;comparaison multiple;paired comparison;parametro posicion;analyse donnee;best control;estimacion no parametrica;valeur critique;heteroscedasticity;estimation statistique;comparison test;multiple comparison;regression statistique;estimacion estadistica;critical value;statistical estimation;test comparaison;numerical simulation	In this article, the design-oriented two-stage and data analysis one-stage multiple comparison procedures for comparing several two-parameter exponential populations with more than one control population under heteroscedasticity are proposed. One-sided and two-sided simultaneous confidence intervals are also given. Upper limits of critical values are obtained using the recent techniques given in Lam (1987, 1988). These approximate critical values are shown to have better results than the approximate critical values using the Bonferroni inequality developed in this article. The multiple comparisons of test treatments with the best control are also developed. Finally, the application of the proposed procedures is illustrated with an example.		Vishal Maurya;Anju Goyal;Amar Nath Gill	2011	Communications in Statistics - Simulation and Computation	10.1080/03610918.2010.549988	computer simulation;econometrics;calculus;mathematics;multiple comparisons problem;statistics	ML	32.61939566797901	-22.002230861435518	194096
431307667425030ef5c8cdcfc70911521548a186	gamma degradation process and accelerated model combined reliability analysis method for rubber o-rings		The sealing performance of rubber O-rings plays an irreplaceable role in ensuring the high reliability and safety of mechanical systems. However, traditional statistical analysis may not consider the aging process of rubber O-rings at different temperatures and the time-varying characteristics of rubber material parameters. In view of these problems, an improved reliability analysis method based on the accelerated aging test of rubber O-rings is proposed in this paper. This method is featured by a combination with the traditional accelerated model and the Gamma stochastic process. This paper first adopts the stationary Gamma process to describe the degradation of compression set which is the performance degradation indicator of rubber O-rings. Then, the reliability model is derived from the Gamma degradation model. Next, the Arrhenius model is used to represent the shape parameter of Gamma degradation model and the approach of maximum likelihood estimation is adopted to solve the parameters. Finally, the rubber O-rings of gas steering engine were selected and a series of accelerated aging tests were conducted to obtain the reliability and the storage lifetime of rubber O-rings at different temperature stresses through our improved method. Besides, the comparison with the traditional statistical method is also conducted to prove the advantage of this method.	accelerated aging;elegant degradation;gamma correction;record sealing;reliability engineering;stationary process;stochastic process	Bo Sun;Meichen Yan;Qiang Feng;Yu Li;Yi Ming Ren;Kun Zhou;Weifang Zhang	2018	IEEE Access	10.1109/ACCESS.2018.2799853	compression set;natural rubber;maximum likelihood;accelerated aging;distributed computing;computer science;control engineering;shape parameter;arrhenius equation;stochastic process	EDA	30.692857410591	-18.460535020372113	194506
111d15b1a52907e96b802e56cb98dd62d3842857	real-time recognition of percussive sounds by a model-based method	signal image and speech processing;a1 alkuperaisartikkeli tieteellisessa aikakauslehdessa;real time;quantum information technology spintronics	Powered by TCPDF (www.tcpdf.org) This material is protected by copyright and other intellectual property rights, and duplication or sale of all or part of any of the repository collections is not permitted, except that material may be duplicated by you for your research use or educational purposes in electronic or print form. You must obtain permission for any other use. Electronic or print copies may not be offered, whether for sale or otherwise to anyone who is not an authorised user. Simsekli, Umut; Jylha, Antti; Erkut, Cumhur; Cemgil, Taylan	algorithm;authorization;definition;filter bank;generic programming;online and offline;palette (computing);precision and recall;real-time clock;real-time computing;real-time locating system;real-time transcription;requirement;sensor;statistical classification;supervised learning	Umut Simsekli;Antti Jylhä;Cumhur Erkut;Ali Taylan Cemgil	2011	EURASIP J. Adv. Sig. Proc.	10.1155/2011/291860	speech recognition;computer science;machine learning	HCI	37.96364019497388	-12.880811491300223	194626
b84f2937b01f9f7a983ddc733a2c119918d7aad3	sumsrm: a new statistic for the structural break detection in time series	sliding window;empirical evidence;structural change;time series;cumulant;structural break;decay rate;sum of squares	Abstract. Structural break is one of the important concerns in non-stationary time series prediction. The cumulative sum of square (CUSUMS) statistic proposed by Brown et al (1975) has been developed as a general method for detecting a structural break. To better understand CUSUMS, this paper analyses the relationship among the bias of the break location estimation, pre-break data size and the decay rate of square residual. Our analysis reveals that small pre-break data size or low decay rate will greatly increase the bias of the break location estimation when there is a change of the mean. Based on the analysis, the paper proposes a new statistic SUMSRM to improve the performance of structural break detection and to reduce the bias of break location estimation. Our empirical evidence confirms that our intended design of the new statistic performs better than the CUSUMS statistic when there is a change of mean in the time series.	sensor;stationary process;time series	Kwok Pan Pang	2005		10.1137/1.9781611972757.35	econometrics;time series;residual;explained sum of squares;statistics;statistic;structural break;structural change;order of integration;cumulant;mathematics	SE	29.07048893657365	-22.169329067421394	194685
8c8e092648299574f8fe7a6f9bab96ff3f54c24b	pls regression on a stochastic process	metodo cuadrado menor;methode moindre carre;60g99;computacion informatica;stochastic process;least squares method;approximation numerique;analisis datos;bolsa valores;partial least square;pls regression;statistical regression;62jxx;eigenvector;aproximacion numerica;bourse valeurs;stock exchange;vector propio;data analysis;ciencias basicas y experimentales;60g05;regresion estadistica;principal component analysis;matematicas;statistical computation;calculo estadistico;processus stochastique;analyse donnee;calcul statistique;numerical approximation;proceso estocastico;grupo a;regression statistique;vecteur propre;eigenvectors;escoufier s operator	Partial least squares (PLS) regression on an L2-continuous stochastic process is an extension of the 2nite set case of predictor variables. The PLS components existence as eigenvectors of some operator and convergence properties of the PLS approximation are proved. The results of an application to stock-exchange data will be compared with those obtained by other methods. c © 2003 Elsevier B.V. All rights reserved.	approximation;kerrison predictor;partial least squares regression;stochastic process	C. Preda;Gilbert Saporta	2005	Computational Statistics & Data Analysis	10.1016/j.csda.2003.10.003	stochastic process;econometrics;eigenvalues and eigenvectors;calculus;mathematics;partial least squares regression;statistics	AI	33.32639985917243	-23.22818362182703	194957
375e32c9058914ca183ac282135754df8b09b569	downturn loss given default: mixture distribution estimation	hg finance;mixed random variable;mixture model;downturn lgd;em algorithm	The internal estimates of Loss Given Default (LGD) must reflect economic downturn conditions, thus estimating the “downturn LGD”, as the new Basel Capital Accord Basel II establishes. We suggest a methodology to estimate the downturn LGD distribution to overcome the arbitrariness of the methods suggested by Basel II. We assume that LGD is a mixture of an expansion and recession distribution. In this work, we propose an accurate parametric model for LGD and we estimate its parameters by the EM algorithm. Finally, we apply the proposed model to empirical data on Italian bank loans.	expectation–maximization algorithm;parametric model	Raffaella Calabrese	2014	European Journal of Operational Research	10.1016/j.ejor.2014.01.043	financial economics;loss given default;econometrics;economics;expectation–maximization algorithm;computer science;mixture model;mathematics;economy	ML	26.383322101889554	-21.12662229526315	195120
0a0873c18ae0dd64542bcdc4d73287268b1a9f5e	conventional analysis with categorical data from a statistically designed experiment	six sigma;quality engineering;design of experiments;mathematical modeling;categorical data	Abstract#R##N#Insights are offered on the interpretation of results of analysis of designed experiments with response expressed on a nominal or ordinal scale, in terms of formulation of a cause-and-effect mathematical model as well as the subsequent choice of factor settings for a future desired response. As generic design of experiments software packages are based on procedures of parametric statistics, the inherent limitations peculiar to the analysis of categorical data by such software packages are illustrated by a numerical example for the benefit of non-statisticians among quality practitioners. Copyright © 2016 John Wiley & Sons, Ltd.	categorical variable;design of experiments	Thong Ngee Goh	2017	Quality and Reliability Eng. Int.	10.1002/qre.2069	statistics;econometrics;quality assurance;design of experiments;software;ordinal scale;categorical variable;parametric statistics;six sigma;data mining;multifactor design of experiments software;computer science	SE	27.84075036060038	-21.16197269711899	195226
0d6791182a19f71bcdd4c52d2ae300301e80f0ed	approximating the solution of the chemical master equation by combining finite state projection and stochastic simulation	stochastic processes biochemistry master equation numerical analysis;master equation;gillespie stochastic simulation algorithm chemical master equation single cell technologies biochemical reaction networks mathematical models time evolution computation probability distribution finite state projection algorithm state space truncation;approximation methods integrated circuits sociology statistics equations mathematical model kalman filters;numerical analysis;stochastic processes;biochemistry	The advancement of single-cell technologies has shown that stochasticity plays an important role in many biochemical reaction networks. However, our ability to investigate this stochasticity using mathematical models remains rather limited. The reason for this is that computing the time evolution of the probability distribution of such systems requires one to solve the chemical master equation (CME), which is generally impossible. Therefore, many approximate methods for solving the CME have been proposed. Among these one of the most prominent is the finite state projection algorithm (FSP) where a solvable system of equations is obtained by truncating the state space. The main limitation of FSP is that the size of the truncation which is required to obtain accurate approximations is often prohibitively large. Here, we propose a method for approximating the solution of the CME which is based on a combination of FSP and Gillespie's stochastic simulation algorithm. The important advantage of our approach is that the additional stochastic simulations allow us to choose state truncations of arbitrary size without sacrificing accuracy, alleviating some of the limitations of FSP.	approximation algorithm;decision problem;gillespie algorithm;mathematical model;simulation;state space;stochastic process;truncation	Aron Hjartarson;Jakob Ruess;John Lygeros	2013	52nd IEEE Conference on Decision and Control	10.1109/CDC.2013.6759972	stochastic process;mathematical optimization;combinatorics;numerical analysis;computer science;calculus;control theory;mathematics;master equation;statistics	Robotics	33.5847053871096	-12.873044089239546	195383
54aa3dc5aee1b7191725dde2299cbb2e2a57ad88	modelling logistic growth by a new diffusion process: application to biological systems	logistic growth;dynamic systems;diffusion process;stochastic modelling;first passage times	The present paper introduces a new diffusion process for the purpose of modelling logistic-type behaviour patterns. Unlike other processes in the same context, this one verifies that its mean function is a logistic curve. In addition, its transition density can be found explicitly, which allows to analyse inference from the discrete sampling of trajectories. The main features of the process will be analysed and the maximum likelihood estimation of parameters will be carried out through discrete sampling. Regarding the numerical problems found to solve the likelihood equations, several strategies are developed for obtaining initial solutions for the usual numerical procedures. Such strategies are compared by means of a simulation example. Also, another simulation study is carried out in order to compare the estimation in this process to that developed by means of continuous sampling in the logistic diffusion model considered by Giovanis and Skiadas (1999). Finally an example is given for the growth of a microorganism culture. This example illustrates the predictive possibilities of the new process, as well as its ability to study time variables formulated as first-passage-times.		Patricia Román-Román;Francisco Torres-Ruiz	2012	Bio Systems	10.1016/j.biosystems.2012.06.004	logistic function;econometrics;mathematical optimization;stochastic modelling;diffusion process;dynamical system;mathematics;statistics	ML	34.062425186982104	-15.58991485466118	195548
1fe4fc9cd18348bfee88a9bf0103cd4c1c84bb39	shrinkage estimators for large covariance matrices in multivariate real and complex normal distributions under an invariant quadratic loss	bayes estimation;secondary 62f10;distribucion invariante;singular wishart distributions;metodo estadistico;analyse multivariable;sample size;ridge regression;quadratic function;funcion cuadratica;covariancia;fonction quadratique;empirical bayes estimation;multivariate analysis;shrinkage estimation;normal distribution;regresion ridge;fonction repartition;ley n variables;perte quadratique;integration by parts formula;estimation bayes empirique;decision bayes;unbiased estimator;primary;unbiased estimate of risk integration by parts formula singular wishart distributions stein haff identity calculus on eigenstructures;62h10;matrice singuliere;62c12;calculus on eigenstructures;shrinkage estimator;matrice covariance;covariance;statistical method;singular matrix;matriz covariancia;curva gauss;distribution invariante;distribucion estadistica;bayes decision;fonction perte;funcion perdida;statistical regression;teoria decision;estimacion insesgada;decision estadistica;estimation parametrique;funcion distribucion;estimacion bayes;distribution function;matriz wishart;regression pseudo orthogonale;distribution statistique;estimateur retrecissement;risk estimation;methode statistique;covariance matrices;loss function;theorie decision;real function;regresion estadistica;62f15;secondary;decision theory;matrice wishart;unbiased estimate of risk;stein haff identity;integration by parts;fonction reelle;loi normale;quadratic loss;wishart matrix;invariante;analisis multivariable;multivariate distribution;wishart distribution;statistical decision;unbiased estimation;matriz singular;estimation statistique;regression statistique;26xx;62j07;estimacion estadistica;loi n variables;60e05;statistical estimation;statistical distribution;decision statistique;funcion real;gaussian distribution;invariant;estimation sans biais;62h12;invariant distribution;covariance matrix;estimation bayes;primary 62h 12	The problem of estimating large covariance matrices of multivariate real normal and complex normal distributions is considered when the dimension of the variables is larger than the number of sample size. The Stein-Haff identities and calculus on eigenstructures for singular Wishart matrices are developed for real and complex cases, respectively. By using these techniques, the unbiased risk estimates for certain class of estimators for the population covariance matrices under an invariant quadratic loss functions are obtained for real and complex cases, respectively. Based on the unbiased risk estimates, shrinkage estimators which are counterparts of the estimators due to Haff [1980, Ann. Statist. 8 ∗ 2000 Mathematics Subject Classification. Primary: 62H12, Secondary: 62F10.	loss function;mathematics subject classification;singular value decomposition	Yoshihiko Konno	2009	J. Multivariate Analysis	10.1016/j.jmva.2009.05.002	normal distribution;covariance mapping;scatter matrix;estimation of covariance matrices;matrix t-distribution;integration by parts;econometrics;complex normal distribution;covariance;calculus;mathematics;wishart distribution;rational quadratic covariance function;statistics;covariance function	ML	32.54888910588342	-23.451071441253163	195624
4e0385611d68455b8d093cc427e66966f4c16d82	confidence interval construction for disease prevalence based on partial validation series	disease prevalence;bootstrap method;wald test;data collection;approximate unconditional method;gold standard;confidence interval;nuisance parameter;double sampling;likelihood ratio test;score test;resampling method;validation series;bootstrap resampling method	It is desirable to estimate disease prevalence based on data collected by a gold standard test, but such a test is often limited due to cost and ethical considerations. Data with partial validation series thus become an alternative. The construction of confidence intervals for disease prevalence with such data is considered. A total of 12 methods, which are based on two Wald-type test statistics, score test statistic, and likelihood ratio test statistic, are developed. Both asymptotic and approximate unconditional confidence intervals are constructed. Two methods are employed to construct the unconditional confidence intervals: one involves inverting two one-sided tests and the other involves inverting one two-sided test. Moreover, the bootstrapping method is used. Two real data sets are used to illustrate the proposed methods. Empirical results suggest that the 12 methods largely produce satisfactory results, and the confidence intervals derived from the score test statistic and the Wald test statistic with nuisance parameters appropriately evaluated generally outperform the others in terms of coverage. If the interval location or the non-coverage at the two ends of the interval is also of concern, then the aforementioned interval based on the Wald test becomes the best choice.		Man-Lai Tang;Shi-Fang Qiu;Wai-Yin Poon	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2011.02.010	econometrics;computerized classification test;test statistic;confidence interval;score test;chi-square test;prevalence;likelihood-ratio test;gold standard;nuisance parameter;pattern recognition;mathematics;exact test;wald test;cdf-based nonparametric confidence interval;robust confidence intervals;statistics;data collection	Theory	29.835623386879924	-21.930974025179193	195937
f0bc6ffac39aa7d6f8a011f81695c525d47b036c	sobol′ sensitivity analysis of napl-contaminated aquifer remediation process based on multiple surrogates	groundwater contamination;surrogate;remediation;sobol sensitivity analysis	Sobol' sensitivity analyses based on different surrogates were performed on a trichloroethylene (TCE)-contaminated aquifer to assess the sensitivity of the design variables of remediation duration, surfactant concentration and injection rates at four wells to remediation efficiency First, the surrogate models of a multi-phase flow simulation model were constructed by applying radial basis function artificial neural network (RBFANN) and Kriging methods, and the two models were then compared. Based on the developed surrogate models, the Sobol' method was used to calculate the sensitivity indices of the design variables which affect the remediation efficiency. The coefficient of determination (R2) and the mean square error (MSE) of these two surrogate models demonstrated that both models had acceptable approximation accuracy, furthermore, the approximation accuracy of the Kriging model was slightly better than that of the RBFANN model. Sobol' sensitivity analysis results demonstrated that the remediation duration was the most important variable influencing remediation efficiency, followed by rates of injection at wells 1 and 3, while rates of injection at wells 2 and 4 and the surfactant concentration had negligible influence on remediation efficiency. In addition, high-order sensitivity indices were all smaller than 0.01, which indicates that interaction effects of these six factors were practically insignificant. The proposed Sobol' sensitivity analysis based on surrogate is an effective tool for calculating sensitivity indices, because it shows the relative contribution of the design variables (individuals and interactions) to the output performance variability with a limited number of runs of a computationally expensive simulation model. The sensitivity analysis results lay a foundation for the optimal groundwater remediation process optimization. Applying sensitivity analysis with multiple surrogates at a TCE contaminated aquifer.Using the Sobol' method to assess the variables contributions in remediation efficiency.Kriging model's approximation accuracy is a little higher than the RBFANN model.Surrogate model reduces computational burden of global sensitivity analysis process.Sensitivity analysis reduces the variables dimensions of optimization process.	surrogates	Jiannan Luo;Wenxi Lu	2014	Computers & Geosciences	10.1016/j.cageo.2014.03.012	econometrics;environmental remediation;hydrology	Vision	26.75436204539997	-17.485535295876268	196018
78d5e264202afdada7941f8c167b9b8e9f2c5dcd	fractional genetic programming for a more gradual evolution		We propose a softening of a genetic program by the so–called fractional instructions. Thanks to their adjustable strengths, a new instruction can be gradually introduced to a program, and the other instructions may gradually adapt to the new member. In this way, a transformation of one candidate into another can be continuous. Such an approach makes it possible to take advantage of properties of real–coded genetic algorithms, but in the realm of genetic programming. We show, that the approach can be successfully applied to a precise generalisation of functions, including those exhibiting periodicity.	genetic algorithm;genetic programming;quasiperiodicity;softening	Artur Rataj	2013			softening;genetic program;genetic algorithm;machine learning;genetic programming;computer science;generalization;artificial intelligence	PL	24.827613538785467	-9.941497262239826	196205
8c69d7d6b892701b9744b80ff1b523be7fe1cdd6	limiting distributions of maxima under triangular schemes	extreme value distribution functions spectral density limiting distribution functions triangular schemes residual dependence;metodo estadistico;analyse multivariable;limit distribution;fonction valeur;multivariate analysis;independance;developpement mathematique;fonction repartition;loi limite;estimation non parametrique;densite spectrale;62g32;extreme value distribution;statistical method;funcion valor;spectral density;mathematical expansion;queue distribution;random vector;independence;loi spectrale;non parametric estimation;extreme value theory;funcion distribucion;distribution function;62h05;60g70;densidad espectral;cola distribucion;extreme value;independencia;limiting distribution functions;methode statistique;extreme value distribution functions;triangular schemes;valeur extreme;residual dependence;dependence structure;analisis multivariable;value function;vector aleatorio;estimacion no parametrica;estimation statistique;estimacion estadistica;vecteur aleatoire;spectral distribution;statistical estimation;60fxx;distribution tail;valor extremo;ley limite;expansion	0. Introduction Intuitively speaking, one may say that there is upper tail dependence in a pair of data (x, y) if x and y are simultaneously large. If this is not the case, one speaks of upper tail independence.We discuss these ideas by regarding bivariate data drawn from thenormal distributionwith different correlation coefficientsρ; see Fig. 1. Ifρ = 0, onemay agree to tail independence, i.e., stochastic independence implies tail independence. If ρ = 0.9, a strong tail dependence becomes visual. More precisely, one may argue with frequencies of data. We say that there is tail dependence if the frequency of both values xi and yi being large, relative to the frequency of xi being large, is strictly bounded away from zero. Within the stochastical model, this formulation entails that the conditional probability P(Y > u|X > u) is bounded away from zero, where X and Y are assumed to be identically distributed random variables with standard normal distribution function (df) Φ . However, within the stochastical model, we have tail independence for every ρ < 1, i.e. P(Y > u|X > u)→ 0, as u ↑ ∞, contrary to any intuition. This reveals that there may be a strong residual dependence in the data if tail independence holds. Such a tail dependence is captured by the coefficient of tail dependence χ̄ , see [12,1], with χ̄ = ρ in the normal case. In Section 1, we will introduce a residual tail dependence parameter β with β = (1− χ̄)/(1+ χ̄). According to the literature (see [7, page 301], [14, Prop. 5.27] and [13, Theorem 7.2.5]), pairwise tail independence of random variables X1, . . . , Xd is equivalent to the asymptotic stochastic independence of componentwise taken sample maxima max i≤n Xi := ( max i≤n Xi1, . . . ,max i≤n Xid ) of a sequence Xi = (Xi1, . . . , Xid), i ≤ n, of independent, identically distributed (iid) random vectors. Thus, this result concerns d-variate sample maxima for sample sizes n as n→∞. ∗ Corresponding author. E-mail addresses: frick.melanie@googlemail.com (M. Frick), reiss@stat.math.unisiegen.de (R.-D. Reiss). 0047-259X/$ – see front matter© 2010 Elsevier Inc. All rights reserved. doi:10.1016/j.jmva.2010.06.006 M. Frick, R.-D. Reiss / Journal of Multivariate Analysis 101 (2010) 2346–2357 2347 Fig. 1. Plots of bivariate normal samples with ρ = 0 (left) and ρ = 0.9 (right). A different formulation of residual tail dependence can be found in [10]: instead of a sequence of random vectors these authors consider a triangular scheme of normally distributed random vectors. In such a triangular scheme the nth line contains n random vectors Xi, i ≤ n. These are independent in each line and the pairwise correlation coefficients ρ(n) converge to 1 with growing sample sizes n. The joint limiting distribution of the sample maxima in each line is an extreme value distribution (EVD) or, equivalently, a max-stable distribution with a certain dependence structure. The family of EVDswith reversely exponentialmargins can be characterized as follows; see [3, Theorem4.3.1]. A d-variate function G is an EVD with reversely exponential margins exp(x), x ≤ 0, if, and only if, the representation G(x) = exp (∫ S min i≤d (uixi) dμ(u) ) , x < 0, (1) is valid, where μ is a finite measure on the d-variate unit simplex	bivariate data;coefficient;converge;exptime;independent set (graph theory);maxima and minima;tail call;time complexity	Melanie Frick;Rolf-Dieter Reiss	2010	J. Multivariate Analysis	10.1016/j.jmva.2010.06.006	calculus;extreme value theory;mathematics;geometry;statistics	ML	33.39159445091574	-20.356692077830324	196273
cb616e0185f8e1b4a5e2d65db34f8bb9dd2d3b91	finite mixtures of maxent distributions	metodo cuadrado menor;methode moindre carre;analisis numerico;entropia;matematicas aplicadas;least squares method;mathematiques appliquees;maximum likelihood;fonction repartition;maximum vraisemblance;ajustement;analyse numerique;fitting;maximum equality procedure;funcion distribucion;distribution function;numerical analysis;entropie;least square;estimacion parametro;normal component;entropy;parameter estimation;estimation parametre;ajuste;mixture distribution;applied mathematics;likelihood function;maxima verosimilitud;finite mixture	Mixture distribution analysis has been the subject of a large remarkable diverse body of literature. So, in order to obtain a mixture density, the problem of parameter estimation has arisen and has taken an important role in this analysis. Parameter estimation is required not only for the parameters of the mixture component but also for the mixture proportion. Widely used method for this problem has been maximum likelihood whereas there are a number of specialized procedures such as least-squares criterion, graphical procedure, etc. In this study, in order to model mixture density, mixture of MaxEnt distributions is proposed instead of mixture of familiar distributions. Since MaxEnt distributions are non-parametric distributions, the problem is reduced to parameter estimation only for mixture proportion. Then, maximum equality estimator which also is based on Shannon’s entropy measure is proposed to be used. It is proved that the mixture of MaxEnt distributions is identifiable and gives more accurate fitting values rather than the mixture of familiar distributions without constructing the likelihood function.	principle of maximum entropy	Aladdin Shamilov;Senay Asma	2008	Applied Mathematics and Computation	10.1016/j.amc.2008.05.058	econometrics;entropy;statistical parameter;calculus;mixture model;mathematics;least squares;statistics	Theory	32.10428314517843	-23.36175584317014	196311
3279021c58575db22b2104674df3e00795bc41a6	issues on simulation and optimization i: simulation-based retrospective optimization of stochastic systems: a family of algorithms	performance measure;sample size;point estimation;system performance;stochastic system;objective function;expected value;random numbers;computational efficiency;simulation model	We consider optimizing a stochastic system, given only a simulation model that is parameterized by continuous decision variables. The model is assumed to produce unbiased point estimates of the system performance measure(s), which must be expected values. The performance measures may appear in the objective function and/or in the constraints. We develop a family of retrospective-optimization (RO) algorithms based on a sequence of sample-path approximations to the original problem with increasing sample sizes. Each approximation problem is obtained by substituting point estimators for each performance measure and using common random numbers over all values of the decision variables. We assume that these approximation problems can be deterministically solved to within a specified error in the decision variables, and that this error is decreasing to zero. The computational efficiency of RO arises from being able to solve the next approximation problem efficiently based on knowledge gained from the earlier, easier approximation problems.	approximation algorithm;computation;deterministic algorithm;loss function;mathematical optimization;optimization problem;simulation;stochastic process	Jihong Jin;Bruce W. Schmeiser	2003		10.1145/1030818.1030892	sample size determination;econometrics;mathematical optimization;stochastic optimization;point estimation;simulation modeling;mathematics;computer performance;expected value;statistics	ML	29.852851963351704	-16.730730401002482	196337
c499cc79115829158f884f44d376b19b384230f3	a quantitative analysis of evolvability for an evolutionary fuzzy logic controller	fuzzy controller;mobile robot;fuzzy rules;fuzzy logic controller;quantitative analysis;genetic algorithm;fuzzy system;simulation environment	This paper presents a quantitative analysis of evolvability with evolutionary activity statistics in an evolutionary fuzzy system. In general, one can estimate the performance of an evolved fuzzy controller by its fitness. However, it is difficult to explain how its fitness or adaptability has been obtained. Evolutionary activity is used to measure the evolvability of fuzzy rules and explain why salient rules have higher evolvability. A genetic algorithm is used to construct a fuzzy logic controller for a mobile robot in simulation environments. The quantitative analysis shows that sufficient evolvability is maintained during the evolution and that it contributes to the construction of the optimal controller.	fuzzy logic;fuzzy rule;genetic algorithm;mobile robot;optimal control;simulation	Seung-Ik Lee;Sung-Bae Cho	2003	Integrated Computer-Aided Engineering		mobile robot;genetic algorithm;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;quantitative analysis;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	24.62046158938707	-12.487344293470114	196435
1906778eda8b1d6ea86be06a298fa78096c1310e	construction and assessment of classification rules	covariance matrices;classification rules;nonparametric smoothing methods;recursive partitioning methods	Only for you today! Discover your favourite construction and assessment of classification rules book right here by downloading and getting the soft file of the book. This is not your time to traditionally go to the book stores to buy a book. Here, varieties of book collections are available to download. One of them is this construction and assessment of classification rules as your preferred book. Getting this book b on-line in this site can be realized now by visiting the link page to download. It will be easy. Why should be here?	download;goto;line level;link page;online and offline	Mark R. Wade	1999	Technometrics	10.1080/00401706.1999.10485683	econometrics;mathematics;statistics	Web+IR	26.1300395179897	-19.71048475169851	196612
40933fe819462bcda0178b2ed49290f38ada9c5c	a quantitative semi-fragile jpeg2000 image authentication system	public key cryptography;integrated approach;watermarking;lowest authenticable bit rate;content based feature extraction;lowest allowable bit rate;re encoding rates;error correction codes;image coding;codecs;content based feature extraction quantitative semi fragile authentication jpeg2000 image authentication system ecc pki infrastructure acceptable manipulations lowest authenticable bit rate repetitive re encoding re encoding rates lowest allowable bit rate content based features ebcot encoding error correction coding pki cryptographic signing robust embedding feature codes watermarks image coding public key;data compression;content based features;pki infrastructure;repetitive re encoding;ecc;error correction coding;authentication;jpeg2000 image authentication system;public key cryptography data compression image coding watermarking code standards telecommunication standards message authentication error correction codes feature extraction;quantitative semi fragile authentication;pki cryptographic signing;code standards;transform coding;acceptable manipulations;bit rate;ebcot encoding;feature codes;watermarks;image authentication;public key;authentication image coding bit rate watermarking transform coding robustness feature extraction error correction codes cryptography codecs;error correction code;digital signature;cryptography;feature extraction;telecommunication standards;robustness;message authentication;robust embedding	In this article, we propose a novel integrated approach to quantitative semi-fragile authentication of JPEG2000 images under a generic framework which combines ECC and PKI infrastructures. Firstly acceptable manipulations (e.g., re-encoding) which should pass authentication are defined based on considerations of some target applications. We propose a unique concept of lowest authenticable bit rate – images undergoing repetitive reencoding are guarantee to pass authentication provided the re-encoding rates are above the lowest allowable bit rate. Our solutions include computation of content-based features derived from the EBCOT encoding procedure of JPEG2000, error correction coding of the derived features, PKI cryptographic signing, and finally robust embedding of the feature codes into image as watermarks.	authentication;code;computation;cryptography;ecc memory;error detection and correction;forward error correction;jpeg 2000;public key infrastructure;semiconductor industry;synapomorphy	Qibin Sun;Shih-Fu Chang;Kurato Maeno;Masayuki Suto	2002		10.1109/ICIP.2002.1040102	data authentication algorithm;computer science;theoretical computer science;internet privacy;public-key cryptography;computer security;statistics	Security	38.74993253648132	-13.194175890355432	196776
ac12065c92864c4407907bb96e981baf89eb9d73	importance measure of correlated normal variables and its sensitivity analysis	correlated variables;importance measure;journal;uncertainty analysis;sensitivity analysis;期刊论文;variance decomposition	In order to explore the contributions by correlated input variables to the variance of the polynomial output in general engineering problems, the correlated and uncorrelated contributions by correlated inputs to the variance of model output are derived analytically by taking the quadratic polynomial output without cross term as an illustration. The analytical sensitivities of the variance contributions with respect to the distribution parameters of input variables are derived, which can explicitly expose the basic factors affecting the variance contributions. Numeric examples are employed and their results demonstrate that the derived analytical expressions are correct, and then they are applied to two engineering examples. The derived analytical expressions can be used directly in recognition of the contributions by input variables and their influencing factors in quadratic or linear polynomial output without cross term. Additionally, the analytical method can be extended to the case of higher order polynomial output, and the results obtained by the proposed method can provide the reference for other new methods. & 2011 Elsevier Ltd. All rights reserved.	higher-order function;polynomial;quadratic function	Wenrui Hao;Zhenzhou Lu;Longfei Tian	2012	Rel. Eng. & Sys. Safety	10.1016/j.ress.2011.10.010	econometrics;mathematical optimization;uncertainty analysis;engineering;variance-based sensitivity analysis;mathematics;variance decomposition of forecast errors;sensitivity analysis;statistics	AI	27.14133601844263	-17.852329811803273	196868
14e2f987d98d72d7f2475fb24e8ca6801ff8c7d7	closed-form expansion, conditional expectation, and option valuation	iterated stochastic integral;option pricing;conditional expectation;asymptotic expansion;diffusion	Enlightened by the theory of Watanabe [Watanabe S (1987) Analysis of Wiener functionals (Malliavin calculus) and its applications to heat kernels. Ann. Probab. 15:1–39] for analyzing generalized random variables and its further development in Yoshida [Yoshida N (1992a) Asymptotic expansions for statistics related to small diffusions. J. Japan Statist. Soc. 22:139–159], Takahashi [Takahashi A (1995) Essays on the valuation problems of contingent claims. Ph.D. thesis, Haas School of Business, University of California, Berkeley, Takahashi A (1999) An asymptotic expansion approach to pricing contingent claims. Asia-Pacific Financial Markets 6:115–151] as well as Kunitomo and Takahashi [Kunitomo N, Takahashi A (2001) The asymptotic expansion approach to the valuation of interest rate contingent claims. Math. Finance 11(1):117–151, Kunitomo N, Takahashi A (2003) On validity of the asymptotic expansion approach in contingent claim analysis. Ann. Appl. Probab. 13(3):914–952] etc., we focus on a wide range of mult...	value (ethics)	Chenxu Li	2014	Math. Oper. Res.	10.1287/moor.2013.0613	mathematical optimization;conditional expectation;valuation of options;calculus;mathematics;diffusion;mathematical economics;statistics;asymptotic expansion	NLP	33.95911426824611	-17.09577990340525	196934
6aecb2b1df9dce40d28539ed2b5cb3d6a42ea500	optimal threshold from roc and cap curves	test kolmogorov smirnov;classification automatique statistiques;metodo estadistico;analyse multivariable;true rate;score;discriminatory power;funcion tasa;aplicacion;multivariate analysis;cost function;statistique score;fonction repartition;estadistica test;kolmogorov smirnov test;statistique test;recuento poblacion;score statistics;simulacion numerica;statistical method;default;funcion coste;cut off point;effectif population;discriminant analysis;analyse discriminante;funcion distribucion;accuracy;analisis discriminante;rate function;distribution function;fonction taux;receiver operating characteristic curves;62h30;methode statistique;90b50;simulation numerique;roc curve;population number;fonction cout;analisis multivariable;credit rating;metodo roc;methode roc;application;60e05;92b06;test statistic;courbe roc;numerical simulation	Receiver Operating Characteristic (ROC) and Cumulative Accuracy Profile (CAP) curves are used to assess the discriminatory power of different credit-rating approaches. The thresholds of optimal classification accuracy on an ROC curve and of maximal profit on a CAP curve can be found by using iso-performance tangent lines, which are based on the standard notion of accuracy. In this article, we propose another accuracy measure called the true rate. Using this rate, one can obtain alternative optimal thresholds on both ROC and CAP curves. For most real populations of borrowers, the number of the defaults is much less than that of the non defaults, and in such cases using the true rate may be more efficient than using the accuracy rate in terms of cost functions. Moreover, it is shown that both alternative optimal thresholds by using the true rate are the identical, and this single threshold coincides with the score corresponding to Kolmogorov–Smirnov statistic used to test the homogeneous distribution functi...	the legend of zelda: the minish cap	Choong Seong Hong	2009	Communications in Statistics - Simulation and Computation	10.1080/03610910903243703	computer simulation;econometrics;calculus;mathematics;score;linear discriminant analysis;statistics	ML	32.395327750380375	-22.871411525415983	197139
b8e4448fb1e4c4456de37b9da83a4477cfe24338	comparing systemic properties of ensembles of biological networks by graphical and statistical methods	via metabolica;modelizacion;graphic method;analisis estadistico;development strategy;propriete biologique;standard deviation;reference model;voie metabolique;statistical method;probabilistic approach;modelisation;methode graphique;statistical analysis;biological properties;enfoque probabilista;approche probabiliste;analyse statistique;metodo grafico;propiedad biologica;metabolic pathway;correlation coefficient;modeling;biological network	MOTIVATION When dealing with questions that concern a general class of models for biological networks, large numbers of distinct models within the class can be grouped into an ensemble that gives a statistical view of the properties for the general class. Comparing properties of different ensembles through the use of point measures (e.g. medians, standard deviations, correlation coefficients) can mask inhomogeneities in the correlations between properties. We are therefore motivated to develop strategies that allow these inhomogeneities to be more easily detected.   RESULTS Methods are described for constructing ensembles of models within the context of a Mathematically Controlled Comparison. A Density of Ratios Plot for a given systemic property is then defined as follows: the y axis represents the value of the systemic property in a reference model divided by the value in the alternative model, and the x axis represents the value of the systemic property in the reference model. Techniques involving moving quantiles are introduced to generate secondary plots in which correlations and inhomogeneities in correlations are more easily detected. Several examples that illustrate the advantages of these techniques are presented and discussed.	apache axis;axis vertebra;biological network;coefficient;graphical user interface;reference model	Rui Alves;Michael A. Savageau	2000	Bioinformatics	10.1093/bioinformatics/16.6.527	biology;econometrics;metabolic pathway;biological network;reference model;systems modeling;mathematics;standard deviation;statistics	Comp.	35.27796634861676	-21.152035242501018	197407
fa0dda8815f108cf9013a08ba6e5e640df9eeb19	monte carlo methods in fuzzy linear regression	fuzzy linear regression · monte carlo · random fuzzy vectors;error measure;quasi-random number generator;fuzzy vector;best solution;example problem;triangular fuzzy number;monte carlo method;new fuzzy monte carlo;fuzzy coefficient;certain fuzzy linear regression	We apply our new fuzzy Monte Carlo method to a certain fuzzy linear regression problem to estimate the best solution. The best solution is a vector of triangular fuzzy numbers, for the fuzzy coefficients in the model, which minimizes one of two error measures. We use a quasi-random number generator to produce random sequences of these fuzzy vectors which uniformly fill the search space. We consider an example problem and show this Monte Carlo method obtains the best solution for one error measure and is approximately best for the other error measure.	coefficient;fuzzy number;low-discrepancy sequence;monte carlo method;random number generation	Areeg Abdalla;James J. Buckley	2007	Soft Comput.	10.1007/s00500-006-0148-5		ML	26.588965996043356	-16.345286131742395	197538
3f041cf66cc42349df5978ce0f57428b5608dea4	online optimization with costly and noisy measurements using random fourier expansions	optimization radio frequency kernel bayes methods approximation algorithms linear programming learning systems;surrogate model adaptive optics bayesian optimization derivative free optimization dfo learning systems	This paper analyzes data-based online nonlinear extremum-seeker (DONE), an online optimization algorithm that iteratively minimizes an unknown function based on costly and noisy measurements. The algorithm maintains a surrogate of the unknown function in the form of a random Fourier expansion. The surrogate is updated whenever a new measurement is available, and then used to determine the next measurement point. The algorithm is comparable with Bayesian optimization algorithms, but its computational complexity per iteration does not depend on the number of measurements. We derive several theoretical results that provide insight on how the hyperparameters of the algorithm should be chosen. The algorithm is compared with a Bayesian optimization algorithm for an analytic benchmark problem and three applications, namely, optical coherence tomography, optical beam-forming network tuning, and robot arm control. It is found that the DONE algorithm is significantly faster than Bayesian optimization in the discussed problems while achieving a similar or better performance.	algorithm;bayesian optimization;benchmark (computing);computational complexity theory;iteration;mathematical optimization;maxima and minima;nonlinear system;online optimization;robotic arm;tomography	Laurens Bliek;Hans R. G. W. Verstraete;Michel Verhaegen;Sander Wahls	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2016.2615134	mathematical optimization;multi-swarm optimization;meta-optimization;computer science;derivative-free optimization;machine learning;mathematics;continuous optimization;random optimization;statistics;population-based incremental learning	ML	25.009491417832727	-14.947694940601222	197581
f6daf9e8f0334dda87083a5eec46be785da6de67	maximum entropy principle and the logistic model	maximum entropy principle;principle of maximum entropy;logistic model;principle of maximium entropy;survival analysis;entropy	This paper proposes the use of the maximum entropy principle to construct a probability model under constraints for the analysis of dichotomous data using the odds ratio adjusted for covariates. It gives a new understanding of the now famous logistic model. We show that we can do away with the hypothesis of linearity of the log odds and still effectively use the model properly. Prom a practical point of view, the result implies that we do not have to discuss the plausability of the linearity hypothesis relative to the data or the phenomenon under study. Hence, when using the logistic model, we do not have to discuss the multiplicative effect of the covariates on the odds ratio. This is a major gain in the use of the model if one does not have to establish or justify the multiplicative effect, for instance, of alcohool consumption while considering low birth weight babies.	doppler effect;ising model;logistic regression;principle of maximum entropy;programmable read-only memory	Raymond Leblanc;Stanley Shapiro	1999	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488599000040	econometrics;joint entropy;binary entropy function;transfer entropy;maximum entropy probability distribution;principle of maximum entropy;calculus;mathematics;maximum entropy spectral estimation;min entropy;statistics	NLP	33.24994220552738	-19.64160194638431	197718
85b8a0b7dc3d7719bf1300b5c9913bf2d6a98d3b	testing stochastic order for reliability analysis of complex systems	expected survivability;testing stochastic order;reliability analysis;complex system;reliable system;available information;survival function;point measure;stochastic order;complex stochastic system;conditional survivability function;system reliability;best system selection	System reliability plays a critical role in the comparison of complex stochastic systems. The reliability of a system can be articulated by its survivability or conditional survivability function. Systems' survivability may be compared based on a point measure such as the expected survivability. However, a point based comparison does not take advantage of all the available information. Here the interest is in the comparison of survival functions based on the stochastic order. The survival functions are assumed to be estimated via simulation. A statistical sequential procedure is presented for selecting the most reliable system with a guarantee of the best system selection.	complex systems;reliability engineering;simulation;stochastic process	Demet Batur;F. Fred Choobineh	2012			reliability engineering;computer science;statistics	EDA	30.33132253724169	-18.813110255943254	197867
3c173c59792bed2bad553ed2abddff9fb6ec256c	towards the semantic extraction of digital signatures for librarian image-identification purposes	identification;digital signature;image analysis	In study we attempt to solve the problem of determining how authentic published images on the Internet are, and to what degree they may be identified by comparison to the original image. The technique proposed aims to serve the new requirements of libraries. One of these is the development of computational tools for the control and preservation of intellectual property such as digital objects, and especially of digital images. For this purpose, this article proposes the use of a serial number extracted using a previously tested semantic-properties method. This method, based on the multilayers of a set of arithmetic points, assures the following two properties: the uniqueness of the final extracted number, and the semantic dependence of this number on the image used as the method’s input. The major advantage of this method is that it can serve as the authentication for a published image or detect partial modifications to a reliable degree. Also, it requires the better of the known hash functions that the digital-signature schemes use, and produces alphanumeric strings for checking authenticity and the degree of similarity between an unknown image and an original image. As an example of a possible application, this article suggests that this method could be incorporated into the well-known DOI system in order to provide a reliable tool for identification and comparison of digital images.	authentication;digital image;digital signature;electronic signature;hash function;internet;librarian;library (computing);requirement;whole earth 'lectronic link	Marios Poulos;George Bokos;Fotios Vaioulis	2008	JASIST	10.1002/asi.20753	identification;digital signature;image analysis;computer science;artificial intelligence;comparative research;data mining;law;world wide web;algorithm;intellectual property	SE	36.658510744202495	-11.571874997643924	197904
2023dd6a437b872e81b8ae05848bc54e3554a016	coevolutionary design of a watermark embedding scheme and an extraction algorithm for detecting replicated two-dimensional barcodes	multi objective optimization;replication detection;two dimensional barcode;coevolutionary optimization;semi fragile watermark;image processing filter design	Two-dimensional barcodes (2D codes) are currently used to authenticate airplane boarding passes and online payments. However, little attention has been paid to the malicious replication or fabrication of 2D codes. In a previous study, the authors have proposed a semi-fragile watermark to distinguish an authentic 2D code from a replicated one and have attempted to design the watermarking scheme automatically using optimization. The optimization of a watermark extraction algorithm and watermarking scheme for stable watermark extractions from 2D codes displayed on various display types is the actual desire. However, it is difficult to formulate the two tasks, as a unified optimization problem and solve it with an optimization algorithm because the two problems have different structures and properties. Consequently, this study proposes a sparsely synchronized heterogeneous coevolutionary method for the simultaneous optimization of a watermark embedding scheme and extracting algorithm to detect the replication of 2D codes displayed on mobile phone screens. Experimental results have shown that the proposed method can design a watermark with desirable semi-fragileness that works well on different types of mobile phone displays.	algorithm;barcode;sensor	Satoshi Ono;Takeru Maehara;Kazunari Minami	2016	Appl. Soft Comput.	10.1016/j.asoc.2015.11.001	computer vision;mathematical optimization;simulation;computer science;theoretical computer science;multi-objective optimization;watermark	EDA	37.75576593253362	-11.771131971821351	198309
ad1db8ee7615bb271ce8f234d1c2160f13606529	online robot strategy adaptation by learning and evolution		Robots operating in everyday life environments have to adapt their strategy as the environment conditions change. The goal of this paper is to develop robots that are able to adapt their strategy based on environmental conditions. In our method, we apply evolutionary computation to find the optimal relation between reinforcement learning parameters and robot performance. The proposed algorithm is evaluated in the simulated environment of the Cyber Rodent (CR) robot, where the robot has to increase its energy level by capturing the active battery packs. The CR robot lives in two environments with different settings that replace each other four times. The results show that evolution can generate an optimal relation between the robot performance and exploration-exploitation of reinforcement learning, enabling the robot to adapt online its strategy as the environmental conditions change.	algorithm;energy level;evolutionary computation;reinforcement learning;robot;virtual reality	Genci Capi	2010	J. Intelligent Systems	10.1515/JISYS.2010.19.1.1	robot learning	Robotics	24.81368296380798	-12.12394810772901	198511
be19a6fe5701ee0237909b09aaddaa50ad90959f	comparative analysis of interpolative and non-interpolative fuzzy rule based machine learning systems applying various numerical optimization methods	learning artificial intelligence evolutionary computation fuzzy set theory inference mechanisms interpolation;interpolation;interpolative fuzzy rule;comparative analysis;evolutionary computation;optimization microorganisms particle swarm optimization genetics machine learning memetics;inference techniques interpolative fuzzy rule noninterpolative fuzzy rule machine learning systems numerical optimization methods;inference mechanisms;numerical optimization;memetics;fuzzy set theory;numerical optimization methods;genetics;inference techniques;fuzzy rule base;noninterpolative fuzzy rule;machine learning;particle swarm optimization;optimization;learning artificial intelligence;machine learning systems;microorganisms	In this paper interpolative and non-interpolative fuzzy rule based machine learning systems are investigated by using simulation results. The investigation focuses mainly on two objectives: to compare the efficiency of the inference techniques combined with different numerical optimization methods for solving machine learning problems and to discover the difference between the properties of systems applying interpolative and non-interpolative inference techniques.	computation;continuation;fuzzy rule;machine learning;mathematical optimization;memetic algorithm;numerical analysis;simulation	Krisztián Balázs;János Botzheim;László T. Kóczy	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584156	qualitative comparative analysis;memetics;interpolation;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;fuzzy set;microorganism;particle swarm optimization;evolutionary computation	EDA	26.380421345207942	-10.153938393268993	198589
03fbf20f60e911058c860c91202148c014f7f5b3	archimedean copula estimation using bayesian splines smoothing techniques	institutional repositories;bayes estimation;parametric model;analyse multivariable;chaine markov;statistical simulation;archimedean copula;cadena markov;metodo monte carlo;correlacion rango;fedora;methode parametrique;multivariate analysis;coeficiente correlacion;65c05;41a15;analisis datos;loi probabilite;bayesian p splines;ley probabilidad;copulas;fonction repartition;ley n variables;metodo parametrico;correlation rang;62h10;62e17;parametric method;methode monte carlo;posterior probability;modele parametrique;statistical regression;62jxx;65c40;vital;funcion distribucion;data analysis;estimacion bayes;aproximacion esplin;distribution function;smoothing methods;simulacion estadistica;posterior distribution;65d07;marginal distribution;markov chain monte carlo;markov chain monte carlo methods;convex function;simulation statistique;spline approximation;smoothing;approximation spline;regresion estadistica;62f15;methode lissage;monte carlo method;probability distribution;probabilite a posteriori;statistical computation;calculo estadistico;systolic blood pressure;estimacion parametro;probabilidad a posteriori;alisamiento;ley a posteriori;ley marginal;simulation study;dependence structure;analisis multivariable;multivariate distribution;analyse donnee;calcul statistique;b spline;60j10;parameter estimation;estimation parametre;tau kendall;markov chains monte carlo;vtls;monotonicity and convexity constraints;regression statistique;correlation coefficient;kendall s tau;loi n variables;60e05;fonction convexe;loi a posteriori;rank correlation;coefficient correlation;kendall tau;lissage;loi marginale;ils;b splin;funcion convexa;estimation bayes;markov chain	Copulas enable to specify multivariate distributions with given marginals. Various parametric proposals were made in the literature for these quantities, mainly in the bivariate case. They can be systematically derived from multivariate distributions with known marginals, yielding e.g. the normal and the Student copulas. Alternatively, one can restrict his/her interest to a sub-family of copulas named Archimedean. They are characterized by a strictly decreasing convex function on (0,1) which tends to +∞ at 0 (when strict) and which is 0 at 1. A ratio approximation of the generator and of its first derivative using B-splines is proposed and the associated parameters estimated using Markov chains Monte Carlo methods. The estimation is reasonably quick. The fitted generator is smooth and parametric. The generated chain(s) can be used to build “credible envelopes” for the above ratio function and derived quantities such as Kendall’s tau, posterior predictive probabilities, etc. Parameters associated to parametric models for the marginals can be estimated jointly with the copula parameters. This is an interesting alternative to the popular two-step procedure which assumes that the regression parameters are fixed known quantities when it comes to copula parameter(s) estimation. A simulation study is performed to evaluate the approach. The practical utility of the method is illustrated by a basic analysis of the dependence structure underlying the diastolic and the systolic blood pressures in male subjects. © 2007 Elsevier B.V. All rights reserved.	approximation;b-spline;bayesian network;bivariate data;convex function;kendall tau distance;markov chain monte carlo;monte carlo method;simulation;smoothing;spline (mathematics)	Philippe Lambert	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2007.01.018	econometrics;kendall tau rank correlation coefficient;calculus;mathematics;posterior probability;copula;statistics	ML	32.24142653959255	-22.5281341853066	198666
f9b2ef2c74aedf542bdbd041f6a84bc4cf2c57a5	optimal design of accelerated life tests for an extension of the exponential distribution	step stress test;fisher information matrix;maximum likelihood estimation;accelerated life tests;cumulative exposure model;asymptotic variance;an extension of the exponential distribution	Accelerated life tests provide information quickly on the lifetime distribution of the products by testing them at higher than usual levels of stress. In this paper, the lifetime of a product at any level of stress is assumed to have an extension of the exponential distribution. This new family has been recently introduced by Nadarajah and Haghighi (2011   [1]  ); it can be used as an alternative to the gamma, Weibull and exponentiated exponential distributions. The scale parameter of lifetime distribution at constant stress levels is assumed to be a log-linear function of the stress levels and a cumulative exposure model holds. For this model, the maximum likelihood estimates (MLEs) of the parameters, as well as the Fisher information matrix, are derived. The asymptotic variance of the scale parameter at a design stress is adopted as an optimization objective and its expression formula is provided using the maximum likelihood method. A Monte Carlo simulation study is carried out to examine the performance of these methods. The asymptotic confidence intervals for the parameters and hypothesis test for the parameter of interest are constructed.	optimal design;time complexity	Firoozeh Haghighi	2014	Rel. Eng. & Sys. Safety	10.1016/j.ress.2014.04.017	gamma distribution;econometrics;mathematical optimization;delta method;fisher information;mathematics;maximum likelihood;statistics	EDA	30.732208033581905	-20.31320876092352	198809
0f3c3555286252b1f734a58cb6b2185fe7a94edb	application of statistical estimation theory, adaptive sensory systems and time series processing to reinforcement learning		In this analytical study we derive the optimal unbiased value estimator (Minimum Variance Unbiased estimator, MVU) and compare its statistical risk to three well known value estimators: Temporal Difference learning (TD), Monte Carlo estimation (MC) and Least-Squares Temporal Difference Learning (LSTD). In particular, we demonstrate that the LSTD estimator is equivalent to the MVU for acyclic Markov Reward Processes (MRPs) and show that both differ for cyclic MRPs as LSTD is then typically biased. In general, estimators that fulfill the Bellman equation, such as LSTD, cannot be unbiased. Two main factors that separate unbiased estimators and estimators that fulfill the Bellman equation, are identified: 1) the discount and 2) a normalization. Furthermore, we show that in the undiscounted case the MC estimator is equivalent to the MVU and to the LSTD estimator if the same amount of information is available to MC. In the discounted case this equivalence does not hold anymore. For the case of TD we show that TD gets unbiased with a minor modification for acyclic MRPs. In general, the TD update rule “moves” the estimate towards the LSTD solution. Consequently, TD is biased for cyclic MRPs. Finally, counterexamples are presented to show that no general ordering exists between the MVU and the LSTD estimator, between MC and LSTD and between TD and MC. Theoretical results are supported by examples and an empirical evaluation.		Steffen Grünewälder	2009			sensory system;artificial intelligence;estimation theory;machine learning;reinforcement learning;computer science	ML	27.95617941029793	-22.84371482388339	198822
9159c4948b23f57e7d2b6ac2e52d4d610bd77b16	on detecting space-time clusters	clusters;search space;space time;spatial scan statistic;heuristic search;search;monte carlo method;randomized algorithm;multiple hypothesis testing;monte carlo;scan statistic;public health;space time region	Detection of space-time clusters is an important function in various domains (e.g., epidemiology and public health). The pioneering work on the spatial scan statistic is often used as the basis to detect and evaluate such clusters. State-of-the-art systems based on this approach detect clusters with restrictive shapes that cannot model growth and shifts in location over time. We extend these methods significantly by using the flexible square pyramid shape to model such effects. A heuristic search method is developed to detect the most likely clusters using a randomized algorithm in combination with geometric shapes processing. The use of Monte Carlo methods in the original scan statistic formulation is continued in our work to address the multiple hypothesis testing issues. Our method is applied to a real data set on brain cancer occurrences over a 19 year period. The cluster detected by our method shows both growth and movement which could not have been modeled with the simpler cylindrical shapes used earlier. Our general framework can be extended quite easily to handle other flexible shapes for the space-time clusters.	heuristic;monte carlo method;randomized algorithm;sensor	Vijay S. Iyengar	2004		10.1145/1014052.1014124	econometrics;mathematical optimization;public health;mathematics;statistics;monte carlo method	Graphics	37.542376810569806	-23.44691096481906	198921
9747e270fae5e608edd74df46a55bde17b95d42f	a comparison of the optimality of statistical significance tests for information retrieval evaluation	student s t test;wilcoxon test;bootstrap;statistical significance;permutation;evaluation;randomization;sign test	Previous research has suggested the permutation test as the theoretically optimal statistical significance test for IR evaluation, and advocated for the discontinuation of the Wilcoxon and sign tests. We present a large-scale study comprising nearly 60 million system comparisons showing that in practice the bootstrap, t-test and Wilcoxon test outperform the permutation test under different optimality criteria. We also show that actual error rates seem to be lower than the theoretically expected 5%, further confirming that we may actually be underestimating significance.	ampersand;bootstrapping (statistics);information retrieval;loss of significance;resampling (statistics);significance arithmetic	Julián Urbano;Mónica Marrero;Diego Martín	2013		10.1145/2484028.2484163	randomization;sign test;student's t-test;wilcoxon signed-rank test;evaluation;permutation;statistical significance;exact test;algorithm;statistics	ML	29.143601662499453	-21.52111324208278	199437
b8ab5161b6fc13729637ecf5d47e0e02d0c721c7	a computer tool for a minimax criterion in binary response and heteroscedastic simple linear regression models	c optimality;equal variance optimality;fisher information matrix;equivalence theorem;optimal design;applet	BACKGROUND AND OBJECTIVE Binary response models are used in many real applications. For these models the Fisher information matrix (FIM) is proportional to the FIM of a weighted simple linear regression model. The same is also true when the weight function has a finite integral. Thus, optimal designs for one binary model are also optimal for the corresponding weighted linear regression model. The main objective of this paper is to provide a tool for the construction of MV-optimal designs, minimizing the maximum of the variances of the estimates, for a general design space.   METHODS MV-optimality is a potentially difficult criterion because of its nondifferentiability at equal variance designs. A methodology for obtaining MV-optimal designs where the design space is a compact interval [a, b] will be given for several standard weight functions.   RESULTS The methodology will allow us to build a user-friendly computer tool based on Mathematica to compute MV-optimal designs. Some illustrative examples will show a representation of MV-optimal designs in the Euclidean plane, taking a and b as the axes. The applet will be explained using two relevant models. In the first one the case of a weighted linear regression model is considered, where the weight function is directly chosen from a typical family. In the second example a binary response model is assumed, where the probability of the outcome is given by a typical probability distribution.   CONCLUSIONS Practitioners can use the provided applet to identify the solution and to know the exact support points and design weights.	applet;assumed;data general eclipse mv/8000;estimated;fisher information;linear iga bullous dermatosis;mv-algebra;minimax;nevus sebaceous;optimal design;sample variance;usability;weight function;wolfram mathematica	V. Casero-Alonso;J. López-Fidalgo;B. Torsney	2017	Computer methods and programs in biomedicine	10.1016/j.cmpb.2016.10.009	econometrics;mathematical optimization;proper linear model;optimal design;fisher information;machine learning;mathematics;statistics;java applet	AI	29.83926854941487	-20.968975081313147	199486
37ced903e95bb582827a00fc10ef3c7bfe5733be	bounds on generalized linear predictors with incomplete outcome data	metodo cuadrado menor;methode moindre carre;least squares method;dato que falta;informacion incompleta;donnee manquante;incomplete information;ordinary least squares regression;information incomplete;prediccion lineal;linear prediction;missing data;prediction lineaire;instrumental variable	This paper develops easily computed, tight bounds on Generalized Linear Predictors and instrumental variable estimators when outcome data are partially identied. A salient example is given by Best Linear Predictors under square loss, or Ordinary Least Squares regressions, with missing outcome data, in which case the setup specializes the more general but intractable problem examined by Horowitz et al. [9]. The result is illustrated by re-analyzing the data used in that paper. I am indebted to Chuck Manski for his guidance and feedback and to two anonymous referees for extremely helpful suggestions. Of course, all errors are mine. Financial support through the Robert Eisner Memorial Fellowship, Department of Economics, Northwestern University, as well as a Dissertation Year Fellowship, The Graduate School, Northwestern University, is gratefully acknowledged.	algorithm;best, worst and average case;chuck;computation;computational complexity theory;observable;ordinary least squares;plausibility structure;randomness;sampling (signal processing);worst-case complexity	Jörg Stoye	2007	Reliable Computing	10.1007/s11155-006-9030-5	generalized least squares;econometrics;mathematical optimization;instrumental variable;ordinary least squares;missing data;linear prediction;mathematics;least squares;complete information;statistics	ML	25.851805011967187	-23.822930977436926	199515
ef4dd6107ee41ebd17bdb2273b3babca1bd77585	a turning bands program for conditional co-simulation of cross-correlated gaussian random fields	computadora;tratamiento datos;computers;maps;europa;distribucion espacial;metal pesado;geostatistique;geoestadistica;three dimensional models;mapa;modelo 3 dimensiones;krigeage;cross correlation;ordinateur;modele 3 dimensions;gaussian random field;suiza;stochastic simulation;soil contamination;simulacion numerica;modele lineaire;suelo;data processing;geostatistics;suisse;europa central;traitement donnee;covariance;heavy metals;sol;carte;co kriging;three dimensional;metal lourd;algorithme;soils;jura suisse;spatial distribution;cokrigeage;multivariate normal distribution;contaminacion;simulation numerique;linear model;europe centrale;jura switzerland;algorithms;central europe;cokriging;switzerland;contamination;covariance function;distribution spatiale;europe;multivariate geostatistics;polucion;jura suizo;linear models;kriging;modele stochastique;parallel simulation;digital simulation;algoritmo;stochastic models;random field;linear model of coregionalization;pollution	A Matlab program (TBCOSIM) is provided for co-simulating a set of stationary or intrinsic Gaussian random fields in R, whose simple and cross-covariance functions are fitted by a linear model of coregionalization. It relies on the turning bands method, which performs three-dimensional simulation via a series of one-dimensional simulations along lines that span R. There is no restriction on the number of random fields to simulate, on the number of basic structures used in the coregionalization model, and on the number and configuration of the locations where simulation has to be performed. Additionally, the realizations can be made conditional to data, back-transformed and averaged over a block support. TBCOSIM uses parallel simulation algorithms: at each location, the random fields are simulated simultaneously and a single co-kriging is needed for conditioning all the realizations. The capabilities of the program are illustrated with the analysis of a set of non-conditional realizations and with an application to a soil contamination dataset. r 2008 Elsevier Ltd. All rights reserved.	algorithm;co-simulation;cross-covariance;kriging;linear model;matlab;simulation;stationary process	Xavier Emery	2008	Computers & Geosciences	10.1016/j.cageo.2007.10.007	econometrics;data processing;linear model;mathematics;statistics;geostatistics	AI	34.96548870494009	-22.712277900120927	199621
ec8f631ef5e5e510ea33c8b5fd01dd45ef6b0fe5	analysis of incomplete, censored data in competing risks models with generalized exponential distributions	goodness of fit;relative risk;exponential distribution;generalized exponential distributions;maximum likelihood;risk analysis;asymptotic confidence interval estimations;probability density function;generalized exponential distribution;testing;maximum likelihood estimation;reliability theory;failure analysis;incomplete data;weibull distribution;confidence interval;competing risks model;maximum likelihood estimate;censored data;risk analysis exponential distribution maximum likelihood estimation distribution functions probability density function weibull distribution failure analysis parameter estimation testing covariance matrix;maximum likelihood estimator;survival analysis;variance covariance matrix exponential distribution goodness of fit maximum likelihood estimator;survival analysis incomplete data censored data competing risks model generalized exponential distributions maximum likelihood procedure asymptotic confidence interval estimations unknown parameter estimation;distribution functions;risk analysis exponential distribution failure analysis maximum likelihood estimation reliability theory;parameter estimation;unknown parameter estimation;variance covariance matrix;covariance matrix;competing risks;maximum likelihood procedure	This paper presents estimates of the parameters included in a competing risks model in the presence of incomplete & censored data. We consider the case when the competing risks have generalized exponential distributions. The maximum likelihood procedure is used to derive point, and asymptotic confidence interval estimations of the unknown parameters. The relative risks due to each cause of failure are investigated. A set of real data is used to test the hypothesis that the causes of failure follow exponential distributions against that they follow generalized exponential distributions	censoring (statistics);time complexity	Ammar M. Sarhan	2007	IEEE Transactions on Reliability	10.1109/TR.2006.890899	econometrics;natural exponential family;pattern recognition;mathematics;maximum likelihood;statistics	Robotics	30.492119574915527	-19.807488520080245	199761
