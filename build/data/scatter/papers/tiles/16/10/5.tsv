id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
e14e33dfbf8d7367593b67686be9a7ee1a5ecf68	adaptive proximal average approximation for composite convex minimization		We propose a fast first-order method to solve multi-term nonsmooth composite convex minimization problems by employing a recent proximal average approximation technique and a novel adaptive parameter tuning technique. Thanks to this powerful parameter tuning technique, the proximal gradient step can be performed with a much larger stepsize in the algorithm implementation compared with the prior PAAPG method (Yu 2013), which is the core to enable significant improvements in practical performance. Moreover, by choosing the approximation parameter adaptively, the proposed method is shown to enjoy the O( 1 k ) iteration complexity theoretically without needing any extra computational cost, while the PA-APG method incurs much more iterations for convergence. The preliminary experimental results on overlapping group Lasso and graph-guided fused Lasso problems confirm our theoretic claim well, and indicate that the proposed method is almost five times faster than the stateof-the-art PA-APG method and therefore suitable for higherprecision required optimization. Introduction Let X be a finite-dimensional linear space endowed with the inner product 〈·, ·〉 and its induced norm ‖ · ‖. Here, we are interested in solving the following multi-term nonsmooth composite convex minimization problem F ∗ := min x∈X F (x) = f(x) + g(x) (1) with g(x) = ∑N i=1 αigi(x), where αi ≥ 0 satisfying ∑N i=1 αi = 1, gi : X → [−∞,+∞] is a proper, closed convex function, and f : X → (−∞,+∞) is a continuously differentiable and gradient Lipschitz convex function with modulus Lf , i.e., ‖∇f(x)−∇f(y)‖ ≤ Lf‖x− y‖, ∀x, y ∈ X. Moreover, we assume that Qi = dom g∗ i is a bounded convex set for all i = 1, · · · , N , in which g∗ i denotes the Fenchel conjugate of gi with the following definition g∗ i (x) = sup ui∈Qi {〈ui, x〉 − gi(ui)} . (2) Copyright c © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Notice that the boundedness assumption about Qi is actually equivalent to the global Lipschitz continuousness of gi used in (Yu 2013, Assumption 1) according to (Borwein and Vanderwerff 2010, Proposition 4.4.6). This multi-term nonsmooth composite convex minimization problem (1) covers a large number of important applications in machine learning, such as overlapping group Lasso (Zhao, Rocha, and Yu 2009; Mairal et al. 2010), graphguided fused Lasso (Chen et al. 2012; Kim and Xing 2009), graph-guided logistic regression (Ouyang et al. 2013), and other types of regularized risk minimization problems (Teo et al. 2010). The regularization term g(x) = ∑N i=1 αigi(x) often carries some important structure information about the structure of the problem itself or data, such as the structured sparsity (Bach et al. 2011; 2012) and nonnegativity. However, the involved vital multi-term nonsmooth components make the optimization problem (1) too complicated to be solved even if N is small. For the special case N = 0, 1, the most popular first-order methods are the accelerated gradient-type methods enjoying the O( 1 K2 ) optimal iteration complexity (Nesterov 2013b), which were first proposed by Nesterov (Nesterov 1983) for N = 0 and then popularized for N = 1 by Beck and Teboulle (Beck and Teboulle 2009a) and Nesterov (Nesterov 2013a). Beck and Teboulle’s method is called “FISTA” while Nesterov’s method in (Nesterov 2013a) is called “APG”. When N is larger, one feasible method is the subgradient-type method (Nemirovsky, Yudin, and Dawson 1982; Polyak 1977) with an extremely slow iteration complexity O( 1 √ K ). To opt out of this dilemma, Nesterov proposed the smoothed accelerated proximal gradient (S-APG) method (Nesterov 2005a; 2005b) for dealing with nonsmooth minimization involving multi-term nonsmooth functions. To make the smoothed accelerated proximal gradient method to achieve the iteration complexity O( 1 K ), the smoothing parameter must be taken as small as O( 1 K ). However, the small smoothing parameter leads to a small iteration stepsize, which has a negative effect on practical optimization performance. To make the smoothed accelerated proximal gradient method much more appealing, some adaptive smoothing algorithms (Boţ and Hendrich 2015; Tran-Dinh 2015) were proposed based on Nesterov’s smoothing technique. However, another drawProceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)	algorithmic efficiency;approximation algorithm;artificial intelligence;closed convex function;computation;convex conjugate;convex optimization;convex set;emoticon;entity–relationship model;experiment;fenchel's duality theorem;first-order reduction;iteration;lasso;logistic regression;machine learning;mathematical optimization;maxima and minima;modulus of continuity;optimization problem;pa-risc;proximal gradient method;proximal gradient methods for learning;smoothing;sparse matrix;subderivative;subgradient method;theory;verification and validation	Li Shen;Wei Liu;Junzhou Huang;Yu-Gang Jiang;Necdet Serhat Aybat	2017			mathematical optimization;proper convex function;computer science;subderivative;proximal gradient methods;convex analysis;proximal gradient methods for learning;convex optimization;composite number;convex combination	AI	25.60490862371343	-35.06798084782739	48232
956a100eef7abaa50cb62cf513b18e8cb2d0e917	information bottleneck for non co-occurrence data	cluster algorithm;generic model;gene expression data;model complexity;collaborative filtering;missing values;information bottleneck	We present a general model-independent approach to the anal ysis of data in cases when these data do not appear in the form of co-occurrence of t wo variablesX,Y , but rather as a sample of values of an unknown (stochastic) fu n tionZ(X,Y ). For example, in gene expression data, the expression level Z is a function of geneX and conditionY ; or in movie ratings data the rating Z is a function of viewerX and movieY . The approach represents a consistent extension of the Info rmati n Bottleneck method that has previously relied on the availab ility of co-occurrence statistics. By altering the relevance variable we eliminat e the need in the sample of joint distribution of all input variables. This new formula tion also enables simple MDL-like model complexity control and prediction of missin g values ofZ. The approach is analyzed and shown to be on a par with the best know n clustering algorithms for a wide range of domains. For the prediction of missing values (collaborative filtering) it improves the currently best kn own results.	algorithm;cluster analysis;collaborative filtering;comstock–needham system;execution unit;mdl (programming language);missing data;non-functional requirement;relevance	Yevgeny Seldin;Noam Slonim;Naftali Tishby	2006			information bottleneck method;computer science;collaborative filtering;machine learning;data mining;statistics	ML	29.55421039559328	-30.154944742969352	49022
02cc84596d5ea90e0cefaccad53f7959decf9226	feature selection with ensembles, artificial variables, and redundancy elimination	redundancy elimination;trees;residuals;masking;feature selection;resampling;importance	Predictive models benefit from a compact, non-redundant sub set of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effec ts that require complex models. This is a challenge for filters, wrappers, and embedded feature selec tion methods. We describe details of an algorithm using tree-based ensembles to generate a compa t subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mi xed method that can uncover masking and detect features of secondary effect. Simulated and actu al examples illustrate the effectiveness of the approach.	algorithm;embedded system;experiment;feature selection;iterative method;markov blanket;markov chain;numerical analysis;relevance	Eugene Tuv;Alexander Borisov;George C. Runger;Kari Torkkola	2009	Journal of Machine Learning Research	10.1145/1577069.1755828	resampling;computer science;machine learning;masking;pattern recognition;data mining;mathematics;feature selection;statistics	ML	30.644543161601174	-36.29562671567283	49182
d105ee7520b60fb11b11b7b917ca1ef2da493d91	anomaly detection based on probability density function with kullback-leibler divergence	uwb radar;kullback leibler divergence;anomaly detection;through wall human detection;principal component analysis	Anomaly detection is a popular problem in many fields. We investigate an anomaly detection method based on probability density function (PDF) of different status. The constructed PDF only require few training data based on Kullback–Leibler Divergence method and small signal assumption. The measurement matrix was deduced according to principal component analysis (PCA). And the statistical detection indicator was set up under iid Gaussian Noise background. The performance of the proposed anomaly detection method was tested with through wall human detection experiments. The results showed that the proposed method could detection human being for brick wall and gypsum wall, but had unremarkable results for concrete wall. & 2016 Elsevier B.V. All rights reserved.	anomaly detection;experiment;kullback–leibler divergence;portable document format;principal component analysis;small-signal model;statistical model	Wei Wang;Baoju Zhang;Dan Wang;Yu Jiang;Shan Qin;Lei Xue	2016	Signal Processing	10.1016/j.sigpro.2016.01.008	anomaly detection;speech recognition;computer science;engineering;machine learning;pattern recognition;mathematics;kullback–leibler divergence;statistics;principal component analysis	AI	33.61490404765043	-35.192589295489704	49372
b8111ccd1a945887783115628c8862baa241608a	distributed stochastic variance reduced gradient methods by sampling extra data with replacement		We study the round complexity of minimizing the average of convex functions under a new setting of distributed optimization where each machine can receive two subsets of functions. The first subset is from a random partition and the second subset is randomly sampled with replacement. Under this setting, we define a broad class of distributed algorithms whose local computation can utilize both subsets and design a distributed stochastic variance reduced gradient method belonging to in this class. When the condition number of the problem is small, our method achieves the optimal parallel runtime, amount of communication and rounds of communication among all distributed first-order methods up to constant factors. When the condition number is relatively large, a lower bound is provided for the number of rounds of communication needed by any algorithm in this class. Then, we present an accelerated version of our method whose the rounds of communication matches the lower bound up to logarithmic terms, which establishes that this accelerated algorithm has the lowest round complexity among all algorithms in our class under this new setting.	computation;condition number;convex function;distributed algorithm;first-order predicate;frank–wolfe algorithm;gradient method;iterative method;mathematical optimization;randomness;sampling (signal processing)	Jason D. Lee;Qihang Lin;Tengyu Ma;Tianbao Yang	2017	Journal of Machine Learning Research		artificial intelligence;mathematics;pattern recognition;sampling (statistics);statistics	ML	25.082813024475172	-32.71724983731017	49492
580fbcebf7026d3cb09aed61c86648aec8e22721	probability-possibility transformation:		Probability-possibility transformation is a purely mechanical transformation of probabilistic support to possibilistic support and vice versa. In this paper, we apply the most common transformations to graphical models, i.e., Bayesian into possibilistic networks. We show that existing transformations are not appropriate to transform Bayesian networks to possibilistic ones since they cannot preserve the information incorporated in joint distributions. Therefore, we propose new consitency properties, exclusively useful for graphical models transformations.	bayesian network;graphical model	Yosra Ben Slimen;Raouia Ayachi;Nahla Ben Amor	2013		10.1007/978-3-319-03200-9_13	pattern recognition;computer science;machine learning;artificial intelligence;probabilistic logic;bayesian network;versa;joint probability distribution;graphical model;bayesian probability	AI	25.70854500308285	-27.537768838544	49499
ef2911c4a5b458ac57c2a30c2e2446ed0af7e762	profile predictive inference		Predictive inference uses a model to analyze a dataset and make predictions about new observations. When a model does not match the data, predictive accuracy suffers. To mitigate this effect, we develop the profile predictive, a predictive density that incorporates the population distribution of data into Bayesian inference. This leads to a practical method for reducing the effect of model mismatch. We extend this method into variational inference and propose a stochastic optimization algorithm, called bumping variational inference (bump-vi). We demonstrate improved predictive accuracy over classical variational inference in two models: a Bayesian mixture model of image histograms and a latent Dirichlet allocation topic model of a text corpus.	algorithm;bump mapping;calculus of variations;experiment;inference engine;latent dirichlet allocation;mathematical optimization;mixture model;stochastic optimization;text corpus;thermal copper pillar bump;topic model;variational inequality;variational principle	Alp Kucukelbir;David M. Blei	2014	CoRR		predictive inference	ML	26.190960831831536	-31.275204438550215	49573
001dbff0e0acd7731762b95cafa524bd262cadbc	quantifying uncertainty in brain network measures using bayesian connectomics	connectomics;graph theory;biological patents;biomedical journals;text mining;europe pubmed central;bayesian inference;citation search;credible interval;citation networks;diffusion weighted imaging;article letter to editor;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	The wiring diagram of the human brain can be described in terms of graph measures that characterize structural regularities. These measures require an estimate of whole-brain structural connectivity for which one may resort to deterministic or thresholded probabilistic streamlining procedures. While these procedures have provided important insights about the characteristics of human brain networks, they ultimately rely on unwarranted assumptions such as those of noise-free data or the use of an arbitrary threshold. Therefore, resulting structural connectivity estimates as well as derived graph measures fail to fully take into account the inherent uncertainty in the structural estimate. In this paper, we illustrate an easy way of obtaining posterior distributions over graph metrics using Bayesian inference. It is shown that this posterior distribution can be used to quantify uncertainty about graph-theoretical measures at the single subject level, thereby providing a more nuanced view of the graph-theoretical properties of human brain connectivity. We refer to this model-based approach to connectivity analysis as Bayesian connectomics.	bayesian network;connectomics;deterministic algorithm;estimated;graph - visual representation;line graph;wiring diagram	Ronald J. Janssen;Max Hinne;Tom Heskes;Marcel van Gerven	2014		10.3389/fncom.2014.00126	diffusion mri;text mining;computer science;bioinformatics;artificial intelligence;graph theory;data science;machine learning;data mining;bayesian inference;credible interval;statistics	ML	26.06738308825643	-26.71399347966554	49693
0d0a697f8ca17924dc94002372e8053b53d8afe8	tracking multiple audio sources with the von mises distribution and variational em.		In this paper, we address the problem of simultaneously tracking several audio sources, namely the problem of estimating source trajectories from a sequence of the observed features. We propose to use the von Mises distribution to model audio-source directions of arrival (DOAs) with circular random variables. This leads to a multi-target Kalman filter formulation which is intractable because of the combinatorial explosion of associating observations to state variables over time. We propose a variational approximation of the filter's posterior distribution and we infer a variational expectation maximization (VEM) algorithm which is computationally efficient. We also propose an audio-source birth method that favors smooth source trajectories and which is used both to initialize the number of active sources and to detect new sources. We perform experiments with a recently released dataset comprising several moving sources as well as a moving microphone array.		Yutong Ban;Xavier Alameda-Pineda;Christine Evers;Radu Horaud	2018	CoRR			ML	38.31661759164143	-27.40561502038682	49830
a062e2b71c28168645aac15118cb1ea1eadbf04d	online arima algorithms for time series prediction	time series forecasting;on line estimation;online optimization;full informations;time series;statistical properties;estimation;online learning algorithms;artificial intelligence;optimization;auto regressive integrated moving average;computational efficiency;time series prediction	Autoregressive integrated moving average (ARIMA) is one of the most popular linear models for time series forecasting due to its nice statistical properties and great flexibility. However, its parameters are estimated in a batch manner and its noise terms are often assumed to be strictly bounded, which restricts its applications and makes it inefficient for handling large-scale real data. In this paper, we propose online learning algorithms for estimating ARIMA models under relaxed assumptions on the noise terms, which is suitable to a wider range of applications and enjoys high computational efficiency. The idea of our ARIMA method is to reformulate the ARIMA model into a task of full information online optimization (without random noise terms). As a consequence, we can online estimation of the parameters in an efficient and scalable way. Furthermore, we analyze regret bounds of the proposed algorithms, which guarantee that our online ARIMA model is provably as good as the best ARIMA model in hindsight. Finally, our encouraging experimental results further validate the effectiveness and robustness of our method.	algorithm;autoregressive integrated moving average;autoregressive model;linear model;machine learning;mathematical optimization;noise (electronics);online optimization;regret (decision theory);robustness (computer science);scalability;synthetic data;time series	Chenghao Liu;Steven C. H. Hoi;Peilin Zhao;Jianling Sun	2016			autoregressive integrated moving average;computer science;artificial intelligence;machine learning;time series;statistics	AI	26.50551128191113	-31.96296405770979	49982
892b6198c6afaa321d2de5a0ec897e13c7af19cc	nonlinear fault detection based on an improved kernel approach		Quality-related issue is a recently raised subject that attracts a lot of attention in process monitoring community. Since most industrial processes present more or less nonlinear characteristics, the study of nonlinear quality-related methods is thus very necessary. Most of the existing methods are based on a kernel partial least square (KPLS) model; however, they usually have a very large amount of computation due to the iterative computation of KPLS. To make matters worse, the logic of these methods is complex, since they use four subspaces to detect a fault. In this paper, we will propose a new kernel-based method whose computation only involves eigenvalue solution and singular value decomposition. Besides, it has a simple logic using only two subspaces. What is more, it has a stable performance with high computational efficiency. All these advantages of the new method are demonstrated by simulation results.	computation;fault detection and isolation;iterative method;kernel (operating system);nonlinear system;simulation;singular value decomposition	Guang Wang;Jianfang Jiao	2018	IEEE Access	10.1109/ACCESS.2018.2802939	kernel (linear algebra);linear subspace;mathematical optimization;computer science;computation;distributed computing;principal component analysis;eigenvalues and eigenvectors;singular value decomposition;nonlinear system;fault detection and isolation	Robotics	25.16820141149048	-36.56747782837928	50046
254047805368118de53b6e434d534dd984be13ba	multiple predictor smoothing methods for sensitivity analysis	response surface regression;following nonparametric regression technique;multiple predictor;rank regression;weighted regression;recursive partitioning regression;nonparametric regression technique;sampling-based sensitivity analysis;informative sensitivity analysis result;linear regression;projection pursuit regression;recursive partitioning;mathematical model;nonparametric regression;regression analysis;complex system;radioactive waste;response surface;radioactive waste disposal;additive model;sensitivity analysis	The use of multiple predictor smoothing methods in sampling-based sensitivity analyses of complex models is investigated. Specifically, sensitivity analysis procedures based on smoothing methods employing the stepwise application of the following nonparametric regression techniques are described: (i) locally weighted regression (LOESS), (ii) additive models (GAMs), (iii) projection pursuit regression (PP_REG), and (iv) recursive partitioning regression (RP_REG). The indicated procedures are illustrated with both simple test problems and results from a performance assessment for a radioactive waste disposal facility (i.e., the Waste Isolation Pilot Plant). As shown by the example illustrations, the use of smoothing procedures based on nonparametric regression techniques can yield more informative sensitivity analysis results than can be obtained with more traditional sensitivity analysis procedures based on linear regression, rank regression or response surface regression when nonlinear relationships between model inputs and model predictions are present.	additive model;information;kerrison predictor;learning to rank;nonlinear system;recursion (computer science);response surface methodology;sampling (signal processing);smoothing;stepwise regression;turing test;waste	Curtis B. Storlie;Jon C. Helton	2005	Proceedings of the Winter Simulation Conference, 2005.		segmented regression;econometrics;mathematical optimization;complex systems;regression dilution;proper linear model;local regression;computer science;engineering;polynomial regression;regression diagnostic;mathematics;radioactive waste;factor regression model;nonparametric regression;regression analysis;statistics	HPC	28.35483475583203	-24.623470117810665	50189
a42eb5013c73e7550b9b9f4702e061f9fda94827	a statistical fault detection strategy using pca based ewma control schemes	chemical reactors;cstr pca model ewma control scheme fault detection;cstr data statistical fault detection strategy pca based ewma control data based method principal component analysis exponentially weighted moving average control scheme ewma control scheme pca model fault detection performance improvement fault detection algorithm continuously stirred tank reactor;principal component analysis;principal component analysis chemical reactors fault diagnosis moving average processes;moving average processes;principal component analysis fault detection inductors chemical reactors process control data models vectors;fault diagnosis	In data-based method for fault detection, principal component analysis (PCA) has been used successfully for fault detection in system with highly correlated variables. The aim of this paper is to combine the exponentially weighted moving average (EWMA) control scheme with PCA model in order to improve fault detection performance. In fact, PCA is used to provide a modeling framework for the develop fault detection algorithm. Because of the ability of EWMA control scheme for detecting small changes, this technique is appropriate to improve the detection of a small fault in PCA model. The performance of the PCA-based EWMA fault detection algorithm is illustrated and compared to conventional fault detection methods using simulated continuously stirred tank reactor (CSTR) data. The results show the effectiveness of the developed algorithm.	algorithm;approximation algorithm;fault detection and isolation;principal component analysis;reactor (software);sl-1;sensor	Fouzi Harrou;Mohamed N. Nounou;Hazem N. Nounou	2013	2013 9th Asian Control Conference (ASCC)	10.1109/ASCC.2013.6606311	control engineering;real-time computing;engineering;control theory	Robotics	36.240960192257496	-28.985420677708728	50305
321ae41973558eb6f7142705ef7500d7cfb28110	rgb-w: when vision meets wireless	cameras noise measurement antennas wireless communication hidden markov models bluetooth fuses;high accuracy location based services rgb w rgb d cameras quasifree modality wireless signal cell phones received signal strength wireless data image driven representation sparsity driven framework data association problem;rssi image colour analysis image fusion image representation mobile computing	"""Inspired by the recent success of RGB-D cameras, we propose the enrichment of RGB data with an additional """"quasi-free"""" modality, namely, the wireless signal (e.g., wifi or Bluetooth) emitted by individuals' cell phones, referred to as RGB-W. The received signal strength acts as a rough proxy for depth and a reliable cue on their identity. Although the measured signals are highly noisy (more than 2m average localization error), we demonstrate that the combination of visual and wireless data significantly improves the localization accuracy. We introduce a novel image-driven representation of wireless data which embeds all received signals onto a single image. We then indicate the ability of this additional data to (i) locate persons within a sparsity-driven framework and to (ii) track individuals with a new confidence measure on the data association problem. Our solution outperforms existing localization methods by a significant margin. It can be applied to the millions of currently installed RGB cameras to better analyze human behavior and offer the next generation of high-accuracy location-based services."""	alexandre oliva;autostereogram;bluetooth;correspondence problem;gene ontology term enrichment;internationalization and localization;location-based service;mobile phone;modal logic;modality (human–computer interaction);software deployment;sparse matrix;unified framework	Alexandre Alahi;Albert Haque;Li Fei-Fei	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.376	embedded system;computer vision;computer science	Vision	37.3624737557898	-37.99994238201937	50481
f070febcb74713d81b0b6b7f583a14b165f8eb2f	a parallel algorithm for modeling of dynamical processes on large stochastic kronecker graphs		A stochastic Kronecker graph (SKG) is one of the most widely used approaches to model complex networks due to the simplicity of generative procedure and the ability to reproduce the properties of real graphs. In this paper, we present a novel parallel algorithm for modeling dynamical processes on large Poisson SKGs (PSKGs). Proposed algorithm combines the modeling of spreading of a process with the generation of vertices and edges which are involved in it. An experimental study of an algorithm was carried out for different initiator matrices and graphs with the widely varying (from several million to one billion) number of vertices using the supercomputer from Lobachevsky State University of Nizhni Novgorod, Russia. The results confirmed the applicability of an algorithm for mid-size HPC clusters with the efficiency varying on average from 0.2 to 0.7.	dynamical system;parallel algorithm	Klavdiya Bochenina;Sergey Kesarev	2016		10.1016/j.procs.2016.05.539	mathematical optimization;combinatorics;discrete mathematics;computer science;theoretical computer science;hopcroft–karp algorithm;machine learning;mathematics;algorithm;statistics	ML	25.521447417700607	-28.249063208623255	50636
3849f5c0f2539f10606c3b85d5880471807d6b29	an effective model selection criterion for mixtures of gaussian processes	model selection;parameter learning;mixture of gaussian processes;em algorithm;likelihood	The Mixture of Gaussian Processes MGP is a powerful statistical learning framework in machine learning. For the learning of MGP on a given dataset, it is necessary to solve the model selection problem, i.e., to determine the number C of actual GP components in the mixture. However, the current learning algorithms for MGPs cannot solve this problem effectively. In this paper, we propose an effective model selection criterion, called the Synchronously Balancing or SB criterion for MGPs. It is demonstrated by the experimental results that this SB criterion is feasible and even outperforms two classical criterions: AIC and BIC, for model selection on MGPs. Moreover, it is found that there exists a feasible interval of the penalty coefficient for correct model selection.	gaussian process;mixture model;model selection	Longbo Zhao;Ziyi Chen;Jinwen Ma	2015		10.1007/978-3-319-25393-0_38	expectation–maximization algorithm;machine learning;pattern recognition;mathematics;likelihood function;bayesian information criterion;model selection;statistics	ML	28.907503717360594	-31.22326945368955	51012
c085b5c33ecdd7e68125ec6f7ff96a0b2d52a6b5	stable estimation of a covariance matrix guided by nuclear norm penalties	em clustering;regularization;discriminant analysis;condition number;covariance estimation	Estimation of a covariance matrix or its inverse plays a central role in many statistical methods. For these methods to work reliably, estimated matrices must not only be invertible but also well-conditioned. The current paper introduces a novel prior to ensure a well-conditioned maximum a posteriori (MAP) covariance estimate. The prior shrinks the sample covariance estimator towards a stable target and leads to a MAP estimator that is consistent and asymptotically efficient. Thus, the MAP estimator gracefully transitions towards the sample covariance matrix as the number of samples grows relative to the number of covariates. The utility of the MAP estimator is demonstrated in two standard applications - discriminant analysis and EM clustering - in this sampling regime.	cluster analysis;condition number;expectation–maximization algorithm;linear discriminant analysis;phenylephrine hydrochloride 10 mg oral tablet;sampling (signal processing);statistical cluster	Eric C. Chi;Kenneth Lange	2014	Computational statistics & data analysis	10.1016/j.csda.2014.06.018	covariance mapping;matérn covariance function;estimation of covariance matrices;regularization;econometrics;covariance intersection;covariance matrix;mathematical optimization;law of total covariance;cma-es;expectation–maximization algorithm;covariance;machine learning;condition number;mathematics;linear discriminant analysis;rational quadratic covariance function;statistics;covariance function	ML	30.89472586337352	-27.311235557974864	51037
08b59850ca6f08b48378107fe14173fb8016aa8f	nonparametric density estimation for multivariate bounded data using two non-negative multiplicative bias correction methods	asymmetric kernels;bias correction;multivariate density estimation	In this article we propose two new Multiplicative Bias Correction (MBC) techniques for nonparametric multivariate density estimation. We deal with positively supported data but our results can easily be extended to the case of mixtures of bounded and unbounded supports. Both methods improve the optimal rate of convergence of the mean squared error up to O(n−8/(8+d)), where d is the dimension of the underlying data set. Moreover, they overcome the boundary effect near the origin and their values are always non-negative. We investigate asymptotic properties like bias and variance as well as the performance of our estimators in Monte Carlo Simulations and in a real data example.	computer simulation;mbc-55x;mean squared error;monte carlo method;rate of convergence	Benedikt Funke;Rafael Kawka	2015	Computational Statistics & Data Analysis	10.1016/j.csda.2015.07.006	econometrics;mathematical optimization;multivariate kernel density estimation;mathematics;statistics	ML	29.533224617971666	-24.939648540869385	51112
ce3b17119d6f81515148904eb91da953da4b301b	gaussian processes for time-marked time-series data		In many settings, data is collected as multiple time series, where each recorded time series is an observation of some underlying dynamical process of interest. These observations are often time-marked with known event times, and one desires to do a range of standard analyses. When there is only one time marker, one simply aligns the observations temporally on that marker. When multiple time-markers are present and are at different times on different time series observations, these analyses are more difficult. We describe a Gaussian Process model for analyzing multiple time series with multiple time markings, and we test it on a variety of data.	dynamical system;gaussian process;temporal logic;time series	John P. Cunningham;Zoubin Ghahramani;Carl E. Rasmussen	2012			econometrics;data mining;mathematics;statistics	ML	34.720204562152794	-27.06137653531677	51165
1a95a55766f9dd8c22d9fd5be978abf64ae61c3a	nonlinear prediction based on independent component analysis mixture modelling	forecasting;wiener structure;neural networks;articulo;nonlinear prediction;mixtures;independent component analysis;time series;icamm;algorithms;kriging;ica	This paper presents a new algorithm for nonlinear prediction based on independent component analysis mixture modelling (ICAMM). The data are considered from several mutually-exclusive classes which are generated by different ICA models. This strategy allows linear local projections that can be adapted to partial segments of a data set while maintaining generalization (capability for nonlinear modelling) given the mixture of several ICAs. The resulting algorithm is a general purpose technique that could be applied to time series prediction, to recover missing data in images, etc. The performance of the proposed method is demonstrated by simulations in comparison with several classical linear and nonlinear methods.	independent component analysis	Gonzalo Safont;Addisson Salazar;Luis Vergara	2011		10.1007/978-3-642-21498-1_64	independent component analysis;econometrics;forecasting;computer science;machine learning;time series;pattern recognition;kriging;mixture;artificial neural network;statistics	ML	29.648034033380384	-28.490861821478735	51281
70db74a93ed93a9bbceb2cea2dc85d9fb74791b5	an efficient decomposition framework for discriminative segmentation with supermodular losses		Several supermodular losses have been shown to improve the perceptual quality of image segmentation in a discriminative framework such as a structured output support vector machine (SVM). These loss functions do not necessarily have the same structure as the one used by the segmentation inference algorithm, and in general, we may have to resort to generic submodular minimization algorithms for loss augmented inference. Although these come with polynomial time guarantees [18, 13, 14], they are not practical to apply to image scale data. Many supermodular losses come with strong optimization guarantees, but are not readily incorporated in a loss augmented graph cuts procedure. This motivates our strategy of employing the alternating direction method of multipliers (ADMM) decomposition for loss augmented inference. In doing so, we create a new API for the structured SVM that separates the maximum a posteriori (MAP) inference of the model from the loss augmentation during training. In this way, we gain computational efficiency, making new choices of loss functions practical for the first time, while simultaneously making the inference algorithm employed during training closer to the test time procedure. We show improvement both in accuracy and computational performance on the Microsoft Research Grabcut database and a brain structure segmentation task, empirically validating the use of several supermodular loss functions during training, and the improved computational properties of the proposed ADMM approach over the Fujishige-Wolfe minimum norm point algorithm.	algorithm;application programming interface;augmented lagrangian method;cut (graph theory);grabcut;image segmentation;loss function;mathematical optimization;microsoft research;structured support vector machine;submodular set function;supermodular function;time complexity	Jiaqian Yu;Matthew B. Blaschko	2017	CoRR		structured support vector machine;artificial intelligence;submodular set function;support vector machine;machine learning;grabcut;time complexity;computer science;maximum a posteriori estimation;inference;image segmentation;algorithm	ML	24.675553487719142	-32.55925526491172	51522
39c681c70a923703d7c404f998dc7df414ebbe09	angle domain average and cwt for fault detection of gear crack	vibration analysis;shafts;vibrations;gear crack;continuous wavelet transform cwt gear crack condition monitoring gearbox system faults speed machinery faults detection nonstationary machine vibration angle domain average technique;fixed time;gear;continuous wavelet transforms fault detection gears wavelet analysis wavelet transforms signal analysis condition monitoring machinery signal sampling frequency;speed machinery faults detection;continuous wavelet transform vibration faul diagnosis angle domain average gear;wavelet transforms;system faults;condition monitoring;gears;vibration;fault detection;transforms;angle domain average technique;faul diagnosis;cwt;time domain;gearbox;continuous wavelet transform;wavelet transforms condition monitoring fault diagnosis gears vibrations;machinery;nonstationary machine vibration;angle domain average;fault diagnosis;continuous wavelet transforms	Run-up and run-down are of particular interest in condition monitoring of gearbox as they highlight many unobservable system faults. However, varying speed machinery faults detection is fraught with difficulties due to non-stationary machine vibration. Fixed time sampling cannot cope with the varying rotational frequency of the machine, resulting in increasing leakage error and spectral smearing. In order to process the non-stationary vibration signals during run-up and run-down of gears drive effectively, the angle domain average (ADA) technique is combined with the continuous wavelet transform (CWT), which is applied to vibration analysis of gear for the detection of failure. Firstly, the vibration signal is sampled at constant time increments and then is re-sampled at constant angle increments. Therefore, the time domain non-stationary signal is converted into angle domain stationary one. In the end, the re-sampled signals are analyzed using continuous wavelet transform. The experimental results show that the presented method can effectively diagnose the faults of the gear crack.	complex wavelet transform;continuous wavelet;sampling (signal processing);spectral leakage;stationary process;stellar classification;time complexity	Hui Li;Yuping Zhang;Haiqi Zheng	2008	2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2008.62	vibration	EDA	37.73879498878621	-30.811910539533685	51671
771992fcb5ff07222707826cf3b6fa38694d54f9	adaptive variational sparse bayesian estimation	automated sparsity imposing mechanism;variational bayes;sparse bayesian learning;statistical analysis bayes methods learning artificial intelligence signal processing;bayes methods;statistical analysis;vectors;estimation;fully automatic learning method;sparse bayesian learning algorithm;signal processing;online sbl algorithm;bayes methods signal processing algorithms adaptation models vectors signal processing adaptive estimation estimation;variational bayes adaptive estimation sparse bayesian learning;automated sparsity imposing mechanism adaptive variational sparse bayesian estimation sparse bayesian learning algorithm online sbl algorithm fully automatic learning method adaptive sparse time varying signal estimation second order statistics;learning artificial intelligence;signal processing algorithms;second order statistics;adaptation models;adaptive estimation;adaptive variational sparse bayesian estimation;adaptive sparse time varying signal estimation	This paper presents an online version of the widely used sparse Bayesian learning (SBL) algorithm. Exploiting the variational Bayes framework, an efficient online SBL algorithm is constructed, that acts as a fully automatic learning method for the adaptive estimation of sparse time-varying signals. The new method is based on second order statistics and comprises a simple, automated sparsity-imposing mechanism, different from that of other known schemes. The effectiveness of the proposed online Bayesian algorithm is illustrated using experimental results conducted on synthetic data. These results show that the proposed scheme achieves faster initial convergence and superior estimation performance compared to other related state-of-the-art schemes.	algorithm;sparse matrix;synthetic data;variational principle	Konstantinos Themelis;Athanasios A. Rontogiannis;Konstantinos Koutroumbas	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6855094	estimation;computer science;machine learning;signal processing;pattern recognition;sparse approximation;mathematics;statistics	Robotics	29.96255856250208	-32.63576981637114	51851
3150bbcf52445b6dc3c769c027755f5ab719a06c	multiple target tracking using the extended kalman particle probability hypothesis density filter	clutter;atmospheric measurements;particle measurements;surveillance;nonlinear non gaussian system multiple target tracking extended kalman particle probability hypothesis density filter multitarget posterior first order moment state transition ek pfphd filter;tracking filters kalman filters nonlinear filters particle filtering numerical methods probability target tracking;target tracking;target tracking proposals monte carlo methods clutter surveillance atmospheric measurements particle measurements;proposals;monte carlo methods	The Particle Probability Hypothesis Density Filter (PFPHD) provides a numeric solution for the probability hypothesis density (PHD) filter, which propagates the first-order moment of the multi-target posterior instead of the posterior distribution itself because evaluating the multiple-target posterior distribution is currently computationally intractable. The PFPHD considers the target states as a single global target state and then avoids data association steps. Various implementations using particle filter had shown the efficiency of this method in real time applications. However, most of them use the state transition prior as the proposal distribution to draw particles from. Because the state transition does not take into account the most recent observation, we present, in this paper, a new approach that mixes the PFPHD filter with the Extended Kalman filter (EKF) named EK-PFPHD filter. The first part provides the general probabilistic framework to handle non linear non gaussian systems when the second part generates better proposal distributions by considering the updated observation. Simulation shows that the proposed filter outperforms the PFPHD filter.	computational complexity theory;correspondence problem;extended kalman filter;first-order predicate;particle filter;simulation;state transition table	M. Melzi;Abdelaziz Ouldali;Zahir Messaoudi	2010	2010 18th European Signal Processing Conference		adaptive filter;nonlinear filter;monte carlo localization;econometrics;invariant extended kalman filter;ensemble kalman filter;kernel adaptive filter;particle filter;auxiliary particle filter;filtering problem;control theory;mathematics;extended kalman filter;moving horizon estimation;filter design;statistics;alpha beta filter;simultaneous localization and mapping	Robotics	38.83650993177271	-26.679111519278155	51929
120540254b282ad3d7bdbef63a4e345c7b054b78	a robust and low-complexity gas recognition technique for on-chip tin-oxide gas sensor array	low complexity;tin oxide;gas sensor;chip	Gas recognition is a new emerging research area with many civil, military, and industrial applications. The success of any gas recognition system depends on its computational complexity and its robustness. In this work, we propose a new low-complexity recognition method which is tested and successfully validated for tin-oxide gas sensor array chip. The recognition system is based on a vector angle similarity measure between the query gas and the representatives of the different gas classes. The latter are obtained using a clustering algorithm based on the same measure within the training data set. Experimented results on our in-house gas sensors array show more than 98% of correct recognition. The robustness of the proposed method is tested by recognizing gas measurements with simulated drift. Less than 1% of performance degradation is noted at the worst case scenario which represents a significant improvement when compared to the current state-of-the-art.	algorithm;cluster analysis;computation;computational complexity theory;data structure;dimensionality reduction;elegant degradation;embedded system;k-means clustering;preprocessor;robustness (computer science);sensor;similarity measure;test set;triangulated irregular network;vector graphics;worst-case scenario	Farid Flitti;Aïcha Beya Far;Bin Guo;Amine Bermak	2008	J. Sensors	10.1155/2008/465209	chip;electronic engineering;simulation;telecommunications;computer science;engineering;data mining	Robotics	36.37839964454342	-34.18415791551849	52092
1abaad325bcb0b9d7c812ec80b85595b37e48683	an efficient method for probabilistic knowledge integration	belief networks;statistical distributions bayes methods belief networks constraint theory inference mechanisms iterative methods set theory;bayesian network;convergence;iterative proportional fitting;bayes methods;probabilistic knowledge integration;probability distribution bayesian methods artificial intelligence astronomy physics space technology computer science random variables iterative algorithms q measurement;bayesian methods;knowledge integration smooth ipfp inconsistent;inference mechanisms;joints;set theory;satisfiability;fitting;iterative methods;distance measurement;statistical distributions;smooth;inconsistent constraint set;bayes reasoning;ipfp;inconsistent;constraint theory;knowledge integration;iterative proportional fitting procedure;smooth method probabilistic knowledge integration joint probability distribution inconsistent constraint set iterative proportional fitting procedure bayesian network bayes reasoning;probabilistic logic;smooth method;conferences;joint probability distribution	"""This paper presents an efficient method, SMOOTH, for modifying a joint probability distribution to satisfy a set of inconsistent constraints. It extends the well-known """"iterative proportional fitting procedure"""" (IPFP), which only works with consistent constraints. Comparing with existing methods, SMOOTH is computationally more efficient and insensitive to data. Moreover, SMOOTH can be easily integrated with Bayesian networks for Bayes reasoning with inconsistent constraints."""	bayesian network;iterative method;iterative proportional fitting;knowledge integration;whole earth 'lectronic link	Shenyong Zhang;Yun Peng;Xiaopu Wang	2008	2008 20th IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2008.57	mathematical optimization;knowledge integration;machine learning;mathematics;iterative proportional fitting;statistics	Robotics	32.038337026597794	-31.852853889315863	52224
c9c44f5a65753ee0906026f693285aaeeed7c643	latent-space variational bayes	bayes estimation;unsupervised learning;analytical models;modelizacion;calculo de variaciones;processus gauss;mixture of gaussians variational bayesian inference machine learning unsupervised learning latent variable model conjugate exponential family variational method;probabilistic models;approximate inference method;variational bayes;melange loi probabilite;complexite calcul;latent variable;approximation algorithms;variational bayesian expectation maximization;collapsed variational methods latent space variational bayes variational bayesian expectation maximization approximate inference method probabilistic models bayesian inference computational complexity;bayes methods;approximation algorithm;bayesian inference;collapsed variational methods;inference mechanisms approximation theory bayes methods expectation maximisation algorithm;bayesian methods;mixed distribution;variational bayesian;inference mechanisms;exponential family;famille exponentielle;latent variable model;modelisation;approximation theory;probabilistic model;modele variable latente;bayesian methods gaussian processes convergence computational complexity machine learning statistical analysis maximum likelihood estimation predictive models encoding monte carlo methods;modelo variable latente;calcul variationnel;estimacion bayes;complejidad computacion;computational modeling;first order;hidden markov models;conjugate exponential family;machine learning;expectation maximization;algorithms artificial intelligence bayes theorem computer simulation models theoretical pattern recognition automated;computational complexity;inferencia;algoritmo aproximacion;variational bayesian inference;algorithme em;mezcla ley probabilidad;familia exponencial;mixture of gaussians;algoritmo em;approximation methods;approximate inference;latent space variational bayes;gaussian process;variational method;algorithme approximation;proceso gauss;em algorithm;modeling;variational calculus;inference;mixture of bernoullis	Variational Bayesian expectation-maximization (VBEM), an approximate inference method for probabilistic models based on factorizing over latent variables and model parameters, has been a standard technique for practical Bayesian inference. In this paper, we introduce a more general approximate inference framework for conjugate-exponential family models, which we call latent-space variational Bayes (LSVB). In this approach, we integrate out model parameters in an exact way, leaving only the latent variables. It can be shown that the LSVB approach gives better estimates of the model evidence as well as the distribution over latent variables than the VBEM approach, but in practice, the distribution over latent variables has to be approximated. As a practical implementation, we present a first-order LSVB (FoLSVB) algorithm to approximate this distribution over latent variables. From this approximate distribution, one can estimate the model evidence and the posterior over model parameters. The FoLSVB algorithm is directly comparable to the VBEM algorithm and has the same computational complexity. We discuss how LSVB generalizes the recently proposed collapsed variational methods [20] to general conjugate-exponential families. Examples based on mixtures of Gaussians and mixtures of Bernoullis with synthetic and real-world data sets are used to illustrate some advantages of our method over VBEM.	algorithmic efficiency;appendix;approximation algorithm;arabic numeral 0;blast e-value;bayesian analysis;behavior;calculus of variations;computational complexity theory;convex function;departure - action;exptime;emoticon;estimated;expectation–maximization algorithm;first-order predicate;functional derivative;immunostimulating conjugate (antigen);inference;initial condition;iteration;latent variable;mixture model;numerical analysis;overfitting;public-key cryptography;qxorm;social inequality;surface acoustic wave device component;synthetic intelligence;syphilis, latent;vesa bios extensions;variational principle;vergence;yk-fh312;exponential	JaeMo Sung;Zoubin Ghahramani;Sung Yang Bang	2008	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2008.157	latent class model;econometrics;mathematical optimization;expectation–maximization algorithm;computer science;variational message passing;machine learning;mathematics;approximation algorithm;statistics	ML	28.023318406647977	-30.200193386130074	52338
9ea4a1358744053e6b36f79d6775a38ce33ace77	statistical inference in a redesigned radial basis function neural network	hybrid learning process;anova;radial basis function;residual analysis;statistical inference	A Hybrid Learning Process method was fitted into a RBF. The resulting redesigned RBF intends to show how to test if the statistical assumptions are fulfilled and to apply statistical inference to the redesigned RBFNN bearing in mind that it allows to determine the relationship between a response (to a process) and one or more independent variables, testing how much each factor contributes to the total variation of the response is also feasible. The results show that statistical methods such as inference, Residual Analysis, and statistical metrics are all good alternatives and excellent methods for validation of the effectiveness of the Neural Network models. The foremost conclusion is that the resulting redesigned Radial Basis Function improved the accuracy of the model after using a Hybrid Learning Process; moreover, the new model also validates the statistical assumptions for using statistical inference and statistical analysis, satisfying the assumptions required for ANOVA to determine the statistical significance and the relationship between variables. & 2013 Elsevier Ltd. All rights reserved.	artificial neural network;coefficient of determination;evaluation function;foremost;genetic algorithm;mathematical model;mathematical optimization;multi-objective optimization;neural network software;radial (radio);radial basis function;robot welding;software release life cycle;the matrix	Rolando J. Praga-Alejo;David S. González-González;Mario Cantú-Sifuentes;Pedro Perez-Villanueva;Luis M. Torres-Treviño;Bernardo D. Flores-Hermosillo	2013	Eng. Appl. of AI	10.1016/j.engappai.2013.06.001	statistical model;radial basis function;statistical inference;fiducial inference;statistical theory;analysis of variance;computer science;machine learning;residual;statistics	AI	27.185343876921443	-24.463015606091425	52358
0f6bd30fe1aa5372a542bcd462247a871298a4e6	dirichlet aggregation: unsupervised learning towards an optimal metric for proportional data	unsupervised learning;information re trieval;distance metric;earth mover s distance	"""Proportional data (normalized histograms) have been frequently occurring in various areas, and they could be mathematically abstracted as points residing in a geometric simplex. A proper distance metric on this simplex is of importance in many applications including classification and information retrieval. In this paper, we develop a novel framework to learn an optimal metric on the simplex. Major features of our approach include: 1) its flexibility to handle correlations among bins/dimensions; 2) widespread applicability without being limited to ad hoc backgrounds; and 3) a """"real"""" global solution in contrast to existing traditional local approaches. The technical essence of our approach is to fit a parametric distribution to the observed empirical data in the simplex. The distribution is parameterized by affinities between simplex vertices, which is learned via maximizing likelihood of observed data. Then, these affinities induce a metric on the simplex, defined as the earth mover's distance equipped with ground distances derived from simplex vertex affinities."""	dirichlet kernel;hoc (programming language);information retrieval;statistical classification;unsupervised learning	Hua-Yan Wang;Hongbin Zha;Hong Qin	2007		10.1145/1273496.1273617	unsupervised learning;earth mover's distance;mathematical optimization;combinatorics;metric;computer science;machine learning;mathematics;statistics	ML	24.74889845778373	-37.76159881876444	52455
269cfd0a68638322229c67efec96c84ac12953dd	inference in multilayer networks via large deviation bounds	directed acyclic graph;rate of convergence;lower and upper bound;probabilistic inference;large deviation	We study probabilistic inference in large, layered Bayesian networks represented as directed acyclic graphs. We show that the intractability of exact inference in such networks does not preclude their e ective use. We give algorithms for approximate probabilistic inference that exploit averaging phenomena occurring at nodes with large numbers of parents. We show that these algorithms compute rigorous lower and upper bounds on marginal probabilities of interest, prove that these bounds become exact in the limit of large networks, and provide rates of convergence.	approximation algorithm;bayesian network;directed acyclic graph;marginal model	Michael Kearns;Lawrence K. Saul	1998			variable elimination;mathematical optimization;combinatorics;computer science;mathematics;rate of convergence;directed acyclic graph;statistics	ML	25.845670484515633	-28.81448009859889	52456
3b0ee92b710ad5ca3b05e997a209ca0ab2cf659f	a method for detecting hidden additivity in two-factor unreplicated experiments	interaction effects;latent variables;non additivity	Assessment of interaction in unreplicated two-factor experiments is a challenging problem that has received considerable attention in the literature. A model is proposed in which the levels of one factor belong in two or more groups. Within each group the effects of the two factors are additive but the groups may interact with the ungrouped factor. This structure is called ''hidden additivity'' if group membership is latent. To identify plausible groupings a search is performed over the space of all possible configurations, or placement of units into two or more groups. A multiplicity-adjusted all-configurations maximum interaction F (ACMIF) test to detect hidden additivity is developed. The method is illustrated using two data sets taken from the literature and a third taken from a recent study of copy number variation due to lymphoma. A simulation study demonstrates the power of the test for hidden additivity and compares it with other well-known tests from the literature.	experiment;multi-factor authentication;sensor	Christopher T. Franck;Dahlia M. Nielsen;Jason A. Osborne	2013	Computational Statistics & Data Analysis	10.1016/j.csda.2013.05.002	latent variable;econometrics;combinatorics;interaction;mathematics;statistics	ML	32.64106988553678	-25.835655929543684	52816
03a7f7d262f43c92bc7bee82a97c32f73c77c0e4	quantifying the information of the prior and likelihood in parametric bayesian modeling		We suggest using a pair of metrics which quantify the extent to which the prior and likelihood functions influence inferences of parameters within a parametric Bayesian model, one of which is closely related to the reference prior of Berger and Bernardo. Our hope is that the utilization of these metrics will allow for the precise quantification of prior and likelihood information and mitigate the use of potentially nebulous terminology such as “informative”, “objectivity”, and “subjectivity”. We develop a Monte Carlo algorithm to estimate these metrics and demonstrate that they possess desirable properties via a combination of theoretical results, simulations, and applications on public medical data sets. While we do not suggest a default prior or likelihood, we suggest a way to quantify the information of the prior and likelihood functions utilized in a parametric Bayesian model; hence these metrics may be useful diagnostic tools when performing a Bayesian analysis.	bayesian network;causal filter;causal inference;causality;computability;inferential theory of learning;information;monte carlo algorithm;monte carlo method;objectivity/db;simulation	Giri Gopalan	2015	CoRR		econometrics;computer science;machine learning;statistics	ML	26.024682584266625	-26.351100335628878	53753
51a7d334607facb677dc1bb7e6fab9997e2031cb	bayesian out-trees	semi supervised learning;matrix tree theorem	A Bayesian treatment of latent directed graph structure for non-iid data is provided where each child datum is sampled with a directed conditional dependence on a single unknown parent datum. The latent graph structure is assumed to lie in the family of directed out-tree graphs which leads to efficient Bayesian inference. The latent likelihood of the data and its gradients are computable in closed form via Tutte’s directed matrix tree theorem using determinants and inverses of the out-Laplacian. This novel likelihood subsumes iid likelihood, is exchangeable and yields efficient unsupervised and semi-supervised learning algorithms. In addition to handling taxonomy and phylogenetic datasets the out-tree assumption performs surprisingly well as a semi-parametric density estimator on standard iid datasets. Experiments with unsupervised and semisupervised learning are shown on various UCI and taxonomy datasets.	algorithm;bayesian network;computable function;directed graph;geodetic datum;gradient;kirchhoff's theorem;machine learning;phylogenetics;semi-supervised learning;semiconductor industry;supervised learning;unsupervised learning	Tony Jebara	2008			computer science;machine learning;pattern recognition;mathematics;statistics	ML	26.195219955664793	-29.46648903531802	53928
02517cbac0881348400a03269975624198413e05	variance estimation in heteroscedastic models by undecimated haar transform	piecewise constant functions;analysis prior;primary 62g08;secondary 65t60;heteroscedasticity;undecimated haar transform	We propose a method in order to maximize the accuracy in the estimation of piecewise constant and piecewise smooth variance functions in a nonparametric heteroscedastic fixed design regression model. The difference-based initial estimates are obtained from the given observations. Then an estimator is constructed by using iterative regularization method with the analysis-prior undecimated three-level Haar transform as regularizer term. We notice that this method shows better results in the mean square sense over an existing adaptive estimation procedure considering all the standard test functions used in addition to the functions that we target. Some simulations and comparisons with other methods are conducted to assess the performance of the proposed method.	haar wavelet	Thangavel Palanisamy;Joghee Ravichandran	2015	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.822713	econometrics;mathematical optimization;mathematics;heteroscedasticity;statistics	Vision	29.905676854026638	-25.109225974188934	54053
ff062f174984380fcbc6757ada6a799b9c5a1ce6	disclosure risk assessment in perturbative microdata protection	analyse risque;confidencialidad;mixture;information loss;melange;risk analysis;rank swapping;additive noise;ruido aditivo;statistical databasis;bruit additif;mixtures;confidentiality;analisis riesgo;base donnee statistique;confidentialite;protection;algorithme em;disclosure risk;proteccion;algoritmo em;em algorithm;mezcla;record linkage	This paper describes methods for data perturbation that include rank swapping and additive noise. It also describes enhanced methods of re-identification using probabilistic record linkage. The empirical comparisons use variants of the framework for measuring information loss and re-identification risk that were introduced by Domingo-Ferrer and Mateo-Sanz.	additive white gaussian noise;linkage (software);microdata (html);paging;perturbation theory (quantum mechanics);risk assessment;utility functions on indivisible goods	William E. Yancey;William E. Winkler;Robert H. Creecy	2002		10.1007/3-540-47804-3_11	econometrics;computer science;mathematics;mixture;statistics	AI	33.80152564705044	-24.627931577098792	54097
2bec23dd29e78d211cc5dbc4606ecab480f097e2	bayesian network classification with continuous attributes: getting the best of both discretization and parametric fitting	bayesian network	In a recent paper, Friedman, Geiger, and Goldszmidt [8] introduced a classifier based on Bayesian networks, called Tree Augmented Naive Bayes (TAN), that outperforms naive Bayes and performs competitively with C4.5 and other state-of-the-art methods. This classifier has several advantages including robustness and polynomial computational complexity. One limitation of the TAN classifier is that it applies only to discrete attributes, and thus, continuous attributes must be prediscretized. In this paper, we extend TAN to deal with continuous attributes directly via parametric (e.g., Gaussians) and semiparametric (e.g., mixture of Gaussians) conditional probabilities. The result is a classifier that can represent and combine both discrete and continuous attributes. In addition, we propose a new method that takes advantage of the modeling language of Bayesian networks in order to represent attributes both in discrete and continuous form simultaneously, and use both versions in the classification. This automates the process of deciding which form of the attribute is most relevant to the classification task. It also avoids the commitment to either a discretized or a (semi)parametric form, since different attributes may correlate better with one version or the other. Our empirical results show that this latter method usually achieves classification performance that is as good as or better than either the purely discrete or the purely continuous TAN models.	bayesian network;c4.5 algorithm;computational complexity theory;curve fitting;discretization;embedded system;experiment;mixture model;modeling language;naive bayes classifier;polynomial;robustness (computer science);select (sql);semiparametric model;statistical classification;time complexity	Nir Friedman;Moisés Goldszmidt;Thomas J. Lee	1998			bayesian average;variable-order bayesian network;computer science;machine learning;pattern recognition;bayesian network;bayesian statistics;discretization of continuous features;dynamic bayesian network;statistics	ML	27.86278560748513	-29.55965573782331	54279
297c6b45234077330eb6e976567673bdba5916b6	factorial hidden markov models	distribution;mean field theory;learning algorithms;factorial design;learning algorithm;learning;factorial hidden markov model;ai;gibbs sampling;hidden markov model;posterior probability;mean;time series;maximum likelihood estimation;baum welch;sampling;probabilistic model;sneural networks;sfactorial;graphical models;maximum likelihood estimate;hidden markov models;machine learning;expectation maximization;mathematical models;exact algorithm;graphical model;forward backward algorithm;time series data;artificial intelligence;algorithms;approximate inference;markov processes;variational method;mit;linear equations;em algorithm;bayesian networks	Hidden Markov models (HMMs) have proven to be one of the most widely used tools for learning probabilistic models of time series data. In an HMM, information about the past is conveyed through a single discrete variable—the hidden state. We discuss a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. We describe an exact algorithm for inferring the posterior probabilities of the hidden state variables given the observations, and relate it to the forward–backward algorithm for HMMs and to algorithms for more general graphical models. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or variational methods. Within the variational framework, we present a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model. Empirical comparisons suggest that these approximations are efficient and provide accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach's chorales and show that factorial HMMs can capture statistical structure in this data set which an unconstrained HMM cannot.	approximation algorithm;cobham's thesis;exact algorithm;forward–backward algorithm;gibbs sampling;graphical model;hidden markov model;markov chain;sampling (signal processing);time series;variational principle	Zoubin Ghahramani;Michael I. Jordan	1995	Machine Learning	10.1023/A:1007425814087	expectation–maximization algorithm;computer science;machine learning;hidden semi-markov model;time series;forward–backward algorithm;pattern recognition;mathematics;maximum likelihood;graphical model;hidden markov model;statistics	ML	26.656144034962207	-30.18763841222086	54643
66b18fbef4f7563865606923fd337aa94b60aba9	self-consistent estimation of conditional multivariate extreme value distributions	quantile regression;coefficient of tail dependence;62g32;conditional analysis;self consistency;the heffernan tawn model;multivariate extreme value theory	Analysing the extremes of multi-dimensional data is a difficult task for many reasons, e.g. the wide range of extremal dependence structures and the scarcity of the data. Some popular approaches that account for various extremal dependence types are based on asymptotically motivated models so that there is a probabilistic underpinning basis for extrapolating beyond observed levels. Among these efforts, Heffernan and Tawn developed a methodology for modelling the distribution of a d-dimensional variable when at least one of its components is extreme. Their approach is based on a series (i=1,…,d) of conditional distributions, in which the distribution of the rest of the vector is modelled given that the ith component is large. This model captures a wide range of dependence structures and is applicable to cases of large d. However their model suffers from a lack of self-consistency between these conditional distributions and so does not uniquely determine probabilities when more than one component is large. This paper looks at these unsolved issues and makes proposals which aim to improve the efficiency of the Heffernan–Tawn model in practice. Tests based on simulated and financial data suggest that the proposed estimation method increases the self-consistency and reduces the RMSE of the estimated coefficient of tail dependence.	conditional entropy;maxima and minima	Y. Liu;Jonathan A. Tawn	2014	J. Multivariate Analysis	10.1016/j.jmva.2014.02.003	novikov self-consistency principle;econometrics;quantile regression;data mining;mathematics;statistics	ML	27.52521012188221	-26.136603015592122	54850
bd8409496af620532e324f580069dcdc043f7ca6	study of punch die condition discrimination based on wavelet packet and genetic neural network	conditional discrimination;wavelet packet;punch die;genetics;pattern recognition;time factor;genetic algorithm;acoustic emission;eigenvectors;neural network	According to the characteristics of the acoustic emission signal which was induced by punch die when It fails, the characteristic parameters of failure signal is determined. The energy eigenvector of signal failure die is extracted by wavelet packet analysis technology, and the comparison between the energy in different frequency bands and total energy is taken as the characteristic parameters. Then a BP neural network is established in which the time factor is considered based on genetic algorithm. The characteristic parameters are used as input specimen, learning and training the network to complete the pattern recognition of model working state. Experiments show that the method can quickly and reliably discriminate the conditions of the punch die and has strong practicability.	artificial neural network;wavelet	Zhigao Luo;Xiuli Wang;Ju Li;Binbin Fan;Xiaodong Guo	2008		10.1007/978-3-540-87734-9_55	speech recognition;genetic algorithm;eigenvalues and eigenvectors;computer science;artificial intelligence;acoustic emission;machine learning;artificial neural network	ML	37.13179549905853	-31.75148479882651	54851
419cbd37d13aef3bb124187cd6503a1f9badbfe7	novel statistical learning and data mining methods for service systems improvement	dissertation;mixture semi markov model clustering expectation maximization variogram mri sequences data mining		data mining;machine learning	Chitta Ranjan	2017			computer science;machine learning;pattern recognition;data mining	ML	30.76114175239343	-27.690748401620162	55012
1248ec7fae6c2b34a40cc0b99100227af6d2e980	integrating low-rank and group-sparse structures for robust multi-task learning	optimal solution;low rank patterns;optimization problem;robust;real world application;multi task learning;group sparsity;unconstrained optimization;analytic solution	Multi-task learning (MTL) aims at improving the generalization performance by utilizing the intrinsic relationships among multiple related tasks. A key assumption in most MTL algorithms is that all tasks are related, which, however, may not be the case in many real-world applications. In this paper, we propose a robust multi-task learning (RMTL) algorithm which learns multiple tasks simultaneously as well as identifies the irrelevant (outlier) tasks. Specifically, the proposed RMTL algorithm captures the task relationships using a low-rank structure, and simultaneously identifies the outlier tasks using a group-sparse structure. The proposed RMTL algorithm is formulated as a non-smooth convex (unconstrained) optimization problem. We propose to adopt the accelerated proximal method (APM) for solving such an optimization problem. The key component in APM is the computation of the proximal operator, which can be shown to admit an analytic solution. We also theoretically analyze the effectiveness of the RMTL algorithm. In particular, we derive a key property of the optimal solution to RMTL; moreover, based on this key property, we establish a theoretical bound for characterizing the learning performance of RMTL. Our experimental results on benchmark data sets demonstrate the effectiveness and efficiency of the proposed algorithm.	advanced power management;algorithm;benchmark (computing);computation;computer multitasking;mathematical optimization;monte carlo method;multi-task learning;optimization problem;proximal operator;relevance;sparse matrix	Jianhui Chen;Jiayu Zhou;Jieping Ye	2011		10.1145/2020408.2020423	optimization problem;multi-task learning;closed-form expression;mathematical optimization;computer science;theoretical computer science;machine learning;data mining;mathematics;statistics	ML	26.036049239191435	-37.804163421624196	55022
9a47ae757d9505fe1613ba9bdc8e399fe8899cdc	fastex: hash clustering with exponential families		Clustering is a key component in any data analysis toolbox. Despite its importance, scalable algorithms often eschew rich statistical models in favor of simpler descriptions such as k-means clustering. In this paper we present a sampler, capable of estimating mixtures of exponential families. At its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals, which is crucial for clustering models with large numbers of clusters.	algorithm;cluster analysis;k-means clustering;random projection;sampling (signal processing);scalability;statistical model;throughput;time complexity	Amr Ahmed;Sujith Ravi;Shravan M. Narayanamurthy;Alexander J. Smola	2012			correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;theoretical computer science;canopy clustering algorithm;cure data clustering algorithm;data mining;mathematics;cluster analysis;statistics;clustering high-dimensional data	ML	29.38048385402865	-29.870156512214795	55025
f9cca5a7c3327f6075c58361d7f98188dc4f9d7c	a two-layer ica-like model estimated by score matching	independent component analysis;statistical model;higher order;machine learning;signal processing;high dimensional data;linear model	Capturing regularities in high-dimensional data is an important problem in machine learning and signal processing. Here we present a statistical model that learns a nonlinear representation from the data that reflects abstract, invariant properties of the signal without making requirements about the kind of signal that can be processed. The model has a hierarchy of two layers, with the first layer broadly corresponding to Independent Component Analysis (ICA) and a second layer to represent higher order structure. We estimate the model using the mathematical framework of Score Matching (SM), a novel method for the estimation of non-normalized statistical models. The model incorporates a squaring nonlinearity, which we propose to be suitable for forming a higher-order code of invariances. Additionally the squaring can be viewed as modelling subspaces to capture residual dependencies, which linear models cannot capture.	emergence;independent computing architecture;independent component analysis;linear model;machine learning;monte carlo method;nonlinear system;numerical integration;requirement;signal processing;sparse matrix;statistical model;unsupervised learning	Urs Köster;Aapo Hyvärinen	2007		10.1007/978-3-540-74695-9_82	independent component analysis;statistical model;higher-order logic;computer science;artificial intelligence;machine learning;signal processing;linear model;pattern recognition;mathematics;statistics;clustering high-dimensional data	ML	29.55408977275311	-34.29844723497982	55301
0829963cb07328f5044ae63d41bae9252a4f0d35	variable selection and estimation for partially linear single-index models with longitudinal data	partially linear single index model;gee;variable selection;secondary 62j07;bias correction;期刊论文;qif;longitudinal data;primary 62j05	In this paper, we consider the partially linear single-index models with longitudinal data. To deal with the variable selection problem in this context, we propose a penalized procedure combined with two bias correction methods, resulting in the bias-corrected generalized estimating equation and the bias-corrected quadratic inference function, which can take into account the correlations. Asymptotic properties of these methods are demonstrated. We also evaluate the finite sample performance of the proposed methods via Monte Carlo simulation studies and a real data analysis.	feature selection;single-index model	Gaorong Li;Peng Lai;Heng Lian	2015	Statistics and Computing	10.1007/s11222-013-9447-8	econometrics;mathematical optimization;gee;machine learning;mathematics;feature selection;statistics	ML	29.49218795239352	-24.288766912135753	55398
12fa079bf3f070408a08609b108d9e2e9f164e8b	quick traffic matrix estimation based on link count covariances	second order;routing;gravity;projection method;traffic control;minimization methods;constrained minimization;maximum likelihood estimation;telecommunication traffic;computational modeling;traffic matrix estimation;ip networks;covariance matrix;covariance matrix telecommunication traffic traffic control equations maximum likelihood estimation minimization methods ip networks routing gravity computational modeling	In this paper we consider the problem of traffic matrix estimation. As the problem is underconstrained, some additional information has to be brought in to obtain a solution. If we have a sequence of link count measurements available, a natural candidate is to use the link count sample covariance matrix under the assumption of a functional relationship between the mean and the variance of the traffic. We propose two computationally light-weight methods for traffic matrix estimation based on the covariance matrix, the projection method and constrained minimization method. The accuracy of these methods is compared with that of other methods using second order moment estimates by simulation under synthetic traffic scenarios.	reference counting;simulation;synthetic intelligence	Ilmari Juva;Sandrine Vaton;Jorma T. Virtamo	2006	2006 IEEE International Conference on Communications	10.1109/ICC.2006.254861	estimation of covariance matrices;econometrics;covariance matrix;mathematical optimization;routing;gravity;mathematics;maximum likelihood;projection method;computational model;second-order logic;statistics	Robotics	37.00433681377129	-26.318771077622507	55572
e636ee77a1323009d769916048e5708446d3440c	parameter estimation in gaussian mixture models with malicious noise, without balanced mixing coefficients		We consider the problem of estimating the means of two Gaussians in a 2-Gaussian mixture, which is not balanced and is corrupted by noise of an arbitrary distribution. We present a robust algorithm to estimate the parameters, together with upper bounds on the numbers of samples required for the estimate to be correct, where the bounds are parametrised by the dimension, ratio of the mixing coefficients, a measure of the separation of the two Gaussians, related to Mahalanobis distance, and a condition number of the covariance matrix. In theory, this is the first sample-complexity result for imbalanced mixtures corrupted by adversarial noise. In practice, our algorithm outperforms the vanilla Expectation-Maximisation (EM) algorithm in terms of estimation error.	coefficient;condition number;estimation theory;expectation–maximization algorithm;mixture model	Jing Xu;Jakub Marecek	2017	CoRR		mahalanobis distance;mixture model;mathematical optimization;econometrics;statistics;noise measurement;covariance matrix;mathematics;approximation algorithm;estimation theory;gaussian;condition number	ML	32.34035909169653	-29.293320122329128	55753
0948c5884fcb35698f101434bcee10d7dd86840c	geostatistical modelling of spatial uncertainty using p-field simulation with conditional probability fields	conditional probability	This paper presents a variant of p-field simulation that allows generation of spatial realizations through sampling of a set of conditional probability distribution functions (ccdf) by sets of probability values, called p-fields. Whereas in the common implementation of the algorithm the p-fields are nonconditional realizations of random functions with uniform marginal distributions, they are here conditional to 0.5 probability values at data locations, which entails a preferential sampling of the central part of the ccdf around these locations. The approach is illustrated using a randomly sampled (200 observations of the NIR channel) SPOT scene of a semi-deciduous tropical forest. Results indicate that the use of conditional probability fields improves the reproduction of statistics such as histogram and semivariogram, while yielding more accurate predictions of reflectance values than the common p-field implementation or the more CPU-intensive sequential indicator simulation. Pixel values are then classi...	simulation	Pierre Goovaerts	2002	International Journal of Geographical Information Science	10.1080/13658810110099125	probability distribution;marginal distribution;conditional probability distribution;econometrics;conditional probability;conditional expectation;computer science;conditional variance;regular conditional probability;machine learning;chain rule;mathematics;law of total probability;posterior probability;joint probability distribution;generative model;statistics;conditional probability table	Robotics	30.519289266773246	-24.13945711068968	56067
be43ac79bd2770a58cc50bc1ccb86ac24b1aa53f	mixtures of common t-factor analyzers for modeling high-dimensional data with missing values	dimension reduction;visualization;ecme algorithm;clustering;common factor loadings;missing data	Mixtures of common t-factor analyzers (MCtFA) have emerged as a sound parsimonious model-based tool for robust modeling of high-dimensional data in the presence of fattailed noises and atypical observations. This paper presents a generalization of MCtFA to accommodatemissing values as they frequently occur inmany scientific researches. Under amissing at randommechanism, a computationally efficient Expectation ConditionalMaximization Either (ECME) algorithm is developed for parameter estimation. The techniques for visualization of the data, classification of new individuals, and imputation of missing values under an incomplete-data structure of MCtFA are also investigated. Illustrative examples concerning the analysis of real and simulated data sets are presented to describe the usefulness of the proposed methodology and compare the finite sample performance with its normal counterparts. © 2014 Elsevier B.V. All rights reserved.	algorithm;algorithmic efficiency;data structure;estimation theory;geo-imputation;missing data;occam's razor	Wan-Lun Wang	2015	Computational Statistics & Data Analysis	10.1016/j.csda.2014.10.007	econometrics;visualization;missing data;computer science;machine learning;data mining;cluster analysis;imputation;statistics;dimensionality reduction	AI	29.855004446533087	-28.668277080433406	56155
810674e4e5171d36040266284cc7aad1acce2ac9	demystifying fixed k-nearest neighbor information estimators		Estimating mutual information from independent identically distributed samples drawn from an unknown joint density function is a basic statistical problem of broad interest with multitudinous applications. The most popular estimator is the one proposed by Kraskov, Stogbauer, and Grassberger (KSG) in 2004 and is nonparametric and based on the distances of each sample to its $k^{mathrm{ th}}$ nearest neighboring sample, where $k$ is a fixed small integer. Despite of its widespread use (part of scientific software packages), theoretical properties of this estimator have been largely unexplored. In this paper, we demonstrate that the estimator is consistent and also identify an upper bound on the rate of convergence of the $ell _{2}$ error as a function of a number of samples. We argue that the performance benefits of the KSG estimator stems from a curious “correlation boosting” effect and build on this intuition to modify the KSG estimator in novel ways to construct a superior estimator. As a by-product of our investigations, we obtain nearly tight rates of convergence of the $ell _{2}$ error of the well-known fixed $k$ -nearest neighbor estimator of differential entropy by Kozachenko and Leonenko.		Weihao Gao;Sewoong Oh;Pramod Viswanath	2018	IEEE Trans. Information Theory	10.1109/TIT.2018.2807481	rate of convergence;combinatorics;entropy (information theory);estimator;statistics;mutual information;computer science;differential entropy;k-nearest neighbors algorithm;random variable;independent and identically distributed random variables	Theory	28.987476410191473	-27.519260312425008	56745
b70f8b02b7b140a7911c9fdde9064ce8ae2dc149	a likelihood ratio test for functional mri data analysis to account for colored noise	temporal correlation;modelo lineal generalizado;nuclear magnetic resonance imaging;ar model;medical imagery;imagineria rmn;colored noise;analisis datos;modele lineaire generalise;modelo autorregresivo;bruit colore;generalized likelihood ratio test;functional mri;false alarm rate;resonancia magnetica;time series;autoregressive model;decision estadistica;data analysis;ruido coloreado;magnetic resonance;color noise;likelihood ratio test;detection rate;imagineria medica;imagerie medicale;statistical inference;analyse donnee;imagerie rmn;generalized linear model;statistical decision;brain activation;test razon verosimilitud;taux fausse alarme;modele autoregressif;test rapport vraisemblance;resonance magnetique;porcentaje falsa alarma;decision statistique	Functional magnetic resonance (fMRI) data are often corrupted with colored noise. To account for this type of noise, many prewhitening and pre-coloring strategies have been proposed to process the fMRI time series prior to statistical inference. In this paper, a generalized likelihood ratio test for brain activation detection is proposed in which the temporal correlation structure of the noise is modelled as an autoregressive (AR) model. The order of the AR model is determined from experimental null data sets. Simulation tests reveal that, for a fixed false alarm rate, the proposed test is slightly (2-3%) better than current tests incorporating colored noise in terms of detection rate.	autoregressive model;colors of noise;electroencephalography;general linear model;generalized linear model;graph coloring;national fund for scientific research;resonance;simulation;time series	Jan Sijbers;Arnold Jan den Dekker;Robert Bos	2005		10.1007/11558484_68	econometrics;colors of noise;likelihood-ratio test;magnetic resonance imaging;mathematics;autoregressive model;statistics	ML	35.02022716079295	-24.422119183089116	56789
f6ef82d7d1cc9a7b089207c95fdd60d74da6a975	aggregative quantification for regression	distribution;articulo;segmentation;quantification;aggregation;regression quantification;probability estimation	The problem of estimating the class distribution (or prevalence) for a new unlabelled dataset (from a possibly different distribution) is a very common problem which has been addressed in one way or another in the past decades. This problem has been recently reconsidered as a new task in data mining, renamed quantification when the estimation is performed as an aggregation (and possible adjustment) of a single-instance supervised model (e.g., a classifier). However, the study of quantification has been limited to classification, while it is clear that this problem also appears, perhaps even more frequently, with other predictive problems, such as regression. In this case, the goal is to determine a distribution or an aggregated indicator of the output variable for a new unlabelled dataset. In this paper, we introduce a comprehensive new taxonomy of quantification tasks, distinguishing between the estimation of the whole distribution and the estimation of some indicators (summary statistics), for both classification and regression. This distinction is especially useful for regression, since predictions are numerical values that can be aggregated in many different ways, as in multi-dimensional hierarchical data warehouses. We focus on aggregative quantification for regression and see that the approaches borrowed from classification do not work. We present several techniques based on segmentation which are able to produce accurate estimations of the expected value and the distribution of the output variable. We show experimentally that these methods especially excel for the relevant scenarios where training and test distributions dramatically differ.	aggregate data;approximation error;batch processing;cluster analysis;data mining;elegant degradation;estimation theory;experiment;hierarchical database model;hoc (programming language);iterative method;loss function;microsoft windows;numerical analysis;ordinal data;ordinal regression;parameter (computer programming);predictive modelling;qr decomposition;quantifier (logic);single-instance storage;smoothing;social network aggregation;software deployment;statistical classification;step detection;stepwise regression;taxonomy (general);test set	Antonio Bella;César Ferri;José Hernández-Orallo;M. José Ramírez-Quintana	2013	Data Mining and Knowledge Discovery	10.1007/s10618-013-0308-z	distribution;econometrics;data mining;mathematics;segmentation;statistics	ML	28.76913761135994	-24.761533764835352	56950
e1963cf05e8b2c63555aa6f847a1a861e8ff90b1	online gradient descent for least squares regression: non-asymptotic bounds and application to bandits			gradient descent;least squares	Nathaniel Korda;A. L. PrashanthL.;Rémi Munos	2013	CoRR		generalized least squares;total least squares;mathematical optimization;machine learning;mathematics;stochastic gradient descent;non-linear least squares;least squares;statistics	ML	25.339098850958962	-31.805637693385062	57015
51c64133fd985e98f76ac2923ffd81efe328f7a4	a multi-modal approach to continuous material identification through tactile sensing	robot sensing systems;vibrations;skin;heating;electrodes;temperature measurement	Tactile sensing has been used in robotics for object identification, grasping, and material recognition. Most material recognition approaches use vibration signals from a tactile exploration, typically above one second long, to identify the material. This work proposes a tactile multi-modal (vibration and thermal) material identification approach based on recursive Bayesian estimation. Through the frequency response of the vibration induced by the material and thermal features, like an estimate of the thermal power loss of the finger, we show that it is possible to identify materials in less than half a second. Moreover, a comparison between vibration only and multi-modal identification shows that both recognition time and classification errors are reduced by adding thermal information.	frequency response;modal logic;recursion;robotics;sensor	Augusto Gomez Eguiluz;I. Rano;Sonya A. Coleman;T. Martin McGinnity	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759721	control engineering;computer vision;acoustics;temperature measurement;engineering;electrode;vibration;skin;tactile sensor	Robotics	37.86868818182013	-33.298703857212786	57262
31de27cfb6578f39d88bd8811f2eaae5abdfeca8	sparsity-driven laplacian-regularized outlier identification for dictionary learning	manifolds;geometry;laplace equations;dictionaries;robustness;signal processing algorithms;data models	Anomalies in data have traditionally been considered as nuisances whose presence, if ignored, can bring detrimental effects on the output of many data processing tasks. Nevertheless, in many situations anomalies correspond to events of interest and as such should be promptly identified before their presence is masked by the data preprocessing schemes being used to reduce the complexity of the main data processing task. This work develops a robust dictionary learning algorithm that exploits the notions of sparsity and local geometry of the data to identify anomalies while constructing sparse representations for the data. Sparsity is used to model the presence of anomalies in a dataset, and local geometry is exploited to better qualify a datum as an anomaly. The robust dictionary learning problem is cast as a regularized least-squares problem where sparsity-inducing and Laplacian regularization terms are used. Efficient iterative solvers based on block-coordinate descent and proximal gradient are developed to tackle the resulting joint dictionary learning and anomaly detection problems. The proposed framework is extended to address variations of classical dictionary learning and matrix factorization problems. Numerical tests on real datasets with artificial and real anomalies are used to illustrate the performance of the proposed algorithms.	algorithm;anomaly detection;coordinate descent;data pre-processing;dictionary;geodetic datum;iterative method;least squares;machine learning;matrix regularization;numerical method;preprocessor;presence information;proximal gradient methods for learning;sparse matrix	Pedro A. Forero;Scott Shafer;Josh Harguess	2017	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2701310	data modeling;manifold;k-svd;computer science;theoretical computer science;machine learning;pattern recognition;mathematics;statistics;robustness	ML	26.573954763511736	-37.65620363068814	57355
1065b0b6f5dd4f71c7c355c0dcaa791d8e10daa2	a non-linear tensor tracking algorithm for analysis of incomplete multi-channel eeg data		Tensor decomposition is a popular tool to analyse and process data which can be represented by a higher-order tensor structure. In this paper, we consider tensor tracking in challenging situations where the observed data are streaming and incomplete. Specifically, we proposed a non-linear formulation of the PETRELS cost function and based on which we proposed NL-PETRELS subspace and tensor tracking algorithms. The non-linear function allows us to improve the convergence rate. We also illustrated the use of our proposed tensor tracking for incomplete multi-channel electroencephalogram (EEG) data in a real-life experiment in which the data can be represented by a third-order tensor.		Nguyen Linh-Trung;Truong Minh-Chinh;Viet-Dung Nguyen;Karim Abed-Meraim	2018	2018 12th International Symposium on Medical Information and Communication Technology (ISMICT)	10.1109/ISMICT.2018.8573711		Vision	29.29381816721129	-34.635410583906946	57687
0211d886b99f772380077a441ae9a4fe9d6cd0d1	adaptive seeding for gaussian mixture models		This paper is a pre-print of a paper that has been accepted for publication in the Proceedings of the 20th Pacific Asia Conference on Knowledge Discovery and Data Mining (PAKDD) 2016. The final publication is available at link.springer.com (http://link.springer.com/chapter/10.1007/978-3-319-31750-2 24). Abstract. We present new initialization methods for the expectationmaximization algorithm for multivariate Gaussian mixture models. Our methods are adaptions of the well-known K-means++ initialization and the Gonzalez algorithm. Thereby we aim to close the gap between simple random, e.g. uniform, and complex methods, that crucially depend on the right choice of hyperparameters. Our extensive experiments indicate the usefulness of our methods compared to common techniques and methods, which e.g. apply the original K-means++ and Gonzalez directly, with respect to artificial as well as real-world data sets.	data mining;dijkstra's algorithm;experiment;hyperlink;k-means clustering;k-means++;mixture model;provable prime;sigkdd;suicidegirls;whole earth 'lectronic link	Johannes Blömer;Kathrin Bujna	2016		10.1007/978-3-319-31750-2_24	econometrics;computer science;machine learning;statistics	ML	27.53164961425698	-29.384695737828224	57878
50f92e9a5e4526c1e84b0ede288f55caf5a79e71	sparse structure inference for group and network tracking	animals;target tracking inference mechanisms learning artificial intelligence markov processes monte carlo methods;surveillance;target tracking mathematical model covariance matrices surveillance animals markov processes;markov chain monte carlo sparse structure inference group tracking network tracking interaction strength inference multiple target tracking mtt application mcmc inference method tracked target relationship learning leader follower relationship group relationship animal flocking data;covariance matrices;mathematical model;markov processes;target tracking	This paper presents a method for inferring interaction strength and structure amongst targets in multiple target tracking (MTT) applications. By making simple assumptions, it is shown how an efficient and well-mixing MCMC inference method can be developed to learn about the relationships between tracked targets, including leader-follower relationships, group relationships and the influence of targets on others. This network structure of influence between targets is inferred in a sparse way, setting many interaction terms to zero and allowing for more efficient inference and clearer structural conclusions to be drawn. The effectiveness of the method is demonstrated on both synthetic and real animal flocking data.	markov chain monte carlo;meaning–text theory;sparse matrix;synthetic intelligence	James K. Murphy;Emre Özkan;Pete Bunch;Simon J. Godsill	2016	2016 19th International Conference on Information Fusion (FUSION)		econometrics;computer science;machine learning;statistics	Robotics	28.07771827762975	-32.439085567154045	57899
e59e983364fa715dd47a4eb499fa1e825773222c	robust nonparametric nearest neighbor random process clustering		"""We consider the problem of clustering noisy finite-length observations of stationary ergodic random processes according to their generative models without prior knowledge of the model statistics and the number of generative models. Two algorithms, both using the <inline-formula><tex-math notation=""""LaTeX"""">$L^1$</tex-math></inline-formula> -distance between estimated power spectral densities (PSDs) as a measure of dissimilarity, are analyzed. The first one, termed nearest neighbor process clustering (NNPC), relies on partitioning the nearest neighbor graph of the observations via spectral clustering. The second algorithm, simply referred to as <inline-formula> <tex-math notation=""""LaTeX"""">$k$</tex-math></inline-formula>-means, consists of a single <inline-formula> <tex-math notation=""""LaTeX"""">$k$</tex-math></inline-formula>-means iteration with farthest point initialization and was considered before in the literature, albeit with a different dissimilarity measure. We prove that both algorithms succeed with high probability in the presence of noise and missing entries, and even when the generative process PSDs overlap significantly, all provided that the observation length is sufficiently large. Our results quantify the tradeoff between the overlap of the generative process PSDs, the observation length, the fraction of missing entries, and the noise variance. Finally, we provide extensive numerical results for synthetic and real data and find that NNPC outperforms state-of-the-art algorithms in human motion sequence clustering."""	algorithm;cluster analysis;ergodicity;generative model;iteration;kinesiology;numerical analysis;sequence clustering;spectral clustering;stationary process;stochastic process;synthetic intelligence;with high probability	Michael Tschannen;Helmut Bölcskei	2017	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2736513	correlation clustering;fuzzy clustering;machine learning;pattern recognition;mathematics;cluster analysis;statistics	ML	28.300824148395016	-37.02426303739735	57941
fb550dd8f04862724fd996e73d875c1f49e85098	distribution-aware block-sparse recovery via convex optimization		We study the problem of reconstructing a blocksparse signal from compressively sampled measurements. In certain applications, in addition to the inherent block-sparse structure of the signal, some prior information about the block support, i.e. blocks containing non-zero elements, might be available. Although many block-sparse recovery algorithms have been investigated in Bayesian framework, it is still unclear how to incorporate the information about the probability of occurrence into regularization-based block-sparse recovery in an optimal sense. In this work, we bridge between these fields by the aid of a new concept in conic integral geometry. Specifically, we solve a weighted optimization problem when the prior distribution about the block support is available. Moreover, we obtain the unique weights that minimize the expected required number of measurements. Our simulations on both synthetic and real data confirm that these weights considerably decrease the required sample complexity.	algorithm;compressed sensing;convex optimization;mathematical optimization;optimization problem;sample complexity;simulation;sparse matrix;synthetic data	Sajad Daei;Farzan Haddadi;Arash Amini	2018	CoRR		mathematics;mathematical optimization;prior probability;conic section;regularization (mathematics);convex optimization;sample complexity;integral geometry;optimization problem;bayesian probability	ML	27.852806074303437	-36.470093470289186	57999
3da31138c38903e688d7e73a05db235c002a216e	non-bayesian additive regularization for multimodal topic modeling of large collections	bigartm;latent dirichlet allocation;additive regularization for topic modeling;probabilistic latent sematic analysis;em algorithm;probabilistic topic modeling	Probabilistic topic modeling of text collections is a powerful tool for statistical text analysis based on the preferential use of graphical models and Bayesian learning. Additive regularization for topic modeling (ARTM) is a recent semiprobabilistic approach, which provides a simpler inference for many models previously studied only in the Bayesian settings. ARTM reduces barriers to entry into topic modeling research field and facilitates combination of topic models. In this paper we develop the multimodal extension of ARTM approach and implement it in BigARTM open source project for online parallelized topic modeling. We demonstrate the ability of non-Bayesian regularization to combine modalities, languages and multiple criteria to find sparse, diverse, and interpretable topics.	additive model;graphical model;multimodal interaction;open-source software;parallel computing;sparse matrix;topic model	Konstantin Vorontsov;Oleksandr Frei;Murat Apishev;Peter Romov;Marina Suvorova;Anastasia Yanina	2015		10.1145/2809936.2809943	computer science;machine learning;pattern recognition;data mining	ML	27.13390410940702	-32.0485702515127	58014
607ed5238589b5b7c113d50ca26ede32563cfdb9	histogram based blind identification and source separation from linear instantaneous mixtures	observed data cloud;efficient clustering method;multi-level input;noiseless case;source separation;new geometric method;blind identification;cluster center;core algorithm;linear instantaneous mixtures;linear instantaneous mimo system;noisy case	The paper presents a new geometric method for the blind identification of linear instantaneous MIMO systems driven by multi-level inputs. The number of outputs may be greater than, equal to, or even less than the number of sources. The sources are then extracted using the identified system parameters. Our approach is based on the fact that the distribution of the distances between the cluster centers of the observed data cloud reveals the mixing vectors in a simple way. In the noiseless case the method is deterministic, non-iterative and fast: it suffices to calculate the histogram of these distances. In the noisy case, the core algorithm must be combined with efficient clustering methods in order to yield satisfactory results for various SNR levels.	source separation	Konstantinos I. Diamantaras;Theophilos Papadimitriou	2009		10.1007/978-3-642-00599-2_29	econometrics;mathematical optimization;mathematics;statistics	Crypto	31.982826670773488	-30.21036070502395	58217
4ac33afb30fb5a5c4b4995269278efa32fdb72fb	design of fault diagnosis system of fpso production process based on mspca	wavelet analysis;floating production storage;fault monitor;wavelet transforms decorrelation fault diagnosis principal component analysis process design;decorrelate autocorrelated measurement;data mining;fault diagnosis production systems principal component analysis wavelet analysis data mining wavelet transforms vectors information analysis decorrelation monitoring;production process;off loading system;process design;linear relationship extraction;variable decorrelation;wavelet transforms;fpso production process;fpso mspca pca fault diagnose;monitoring;matrix decomposition;feature extraction;principal component analysis;fault diagnosis system;deterministic feature extraction;fault diagnosis system design;decorrelation;multiscale pca;fpso;fault diagnose;off loading system fault diagnosis system design fpso production process wavelet analysis principal component analysis multiscale pca linear relationship extraction variable decorrelation deterministic feature extraction decorrelate autocorrelated measurement fault monitor floating production storage;pca;mspca;fault diagnosis	Based on the theory of wavelet analysis and principal component analysis, Multi-scale PCA is introduced which combines the ability of PCA to decorrelate the variables by extracting a linear relationship, with that of wavelet analysis to extract deterministic features and approximately decorrelate autocorrelated measurements to improve the performance of PCA whose modeling is limited to a single scale. It is applied to the fault monitor and diagnose of Floating Production Storage and Off-loading System. The result show: the fault diagnose method based on multi-scale principal components analysis can realized FPSO earlier period fault monitor and diagnose accurately, and the capability of multi-scale principal components analysis fault diagnosis is better than the principal components analysis for the small disturbance.	autocorrelation;interference (communication);principal component analysis;process modeling;real-time clock;wavelet	Qiang Gao;Miao Han;Shu-liang Hu;Hai-jie Dong	2009	2009 Fifth International Conference on Information Assurance and Security	10.1109/IAS.2009.221	real-time computing;engineering;pattern recognition;data mining	Robotics	38.20464827694731	-30.51377502727186	58253
7a0162fbeac8b6fb5ee57bc4971db1e5e1bd50cb	simultaneous sampling and multi-structure fitting with adaptive reversible jump mcmc		Multi-structure model fitting has traditionally taken a two-stage approach: First, sample a (large) number of model hypotheses, then select the subset of hypotheses that optimise a joint fitting and model selection criterion. This disjoint two-stage approach is arguably suboptimal and inefficient — if the random sampling did not retrieve a good set of hypotheses, the optimised outcome will not represent a good fit. To overcome this weakness we propose a new multi-structure fitting approach based on Reversible Jump MCMC. Instrumental in raising the effectiveness of our method is an adaptive hypothesis generator, whose proposal distribution is learned incrementally and online. We prove that this adaptive proposal satisfies the diminishing adaptation property crucial for ensuring ergodicity in MCMC. Our method effectively conducts hypothesis sampling and optimisation simultaneously, and yields superior computational efficiency over previous two-stage methods.	computation;curve fitting;ergodicity;mathematical optimization;model selection;monte carlo method;reversible-jump markov chain monte carlo;sampling (signal processing)	Trung-Thanh Pham;Tat-Jun Chin;Jin Yu;David Suter	2011			econometrics;mathematical optimization;mathematics;statistics	ML	27.99228069780068	-28.603411659781266	58303
46228baba46879357b619069a8ed7989237e6b1d	inferring parameters and structure of latent variable models by variational bayes	variational bayes;latent variable model	Current methods for learning graphical mod­ els with latent variables and a fixed structure estimate optimal values for the model param­ eters. Whereas this approach usually pro­ duces overfitting and suboptimal generaliza­ tion performance, carrying out the Bayesian program of computing the full posterior dis­ tributions over the parameters remains a dif­ ficult problem. Moreover, learning the struc­ ture of models with latent variables, for which the Bayesian approach is crucial, is yet a harder problem. In this paper I present the Variational Bayes framework, which pro­ vides a solution to these problems. This ap­ proach approximates full posterior distribu­ tions over model parameters and structures, as well as latent variables, in an analytical manner without resorting to sampling meth­ ods. Unlike in the Laplace approximation, these posteriors are generally non-Gaussian and no Hessian needs to be computed. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. I demonstrate that this algorithm can be applied to a large class of models in several domains, including unsupervised clustering and blind source sep­ aration.	approximation;cluster analysis;expectation–maximization algorithm;hessian;latent variable;overfitting;sampling (signal processing);variational principle	Hagai Attias	1999			latent class model;econometrics;mathematical optimization;computer science;machine learning;mathematics;probabilistic latent semantic analysis;latent variable model;statistics	ML	27.646064853381052	-30.356436430196986	58475
f1eca9bbdbb8eea8af00a12a7c9faf843ed45408	phd filter with approximate multiobject density measurement update	clutter;atmospheric measurements;standards;approximation methods density measurement atmospheric measurements particle measurements standards clutter target tracking;density measurement;particle measurements;cardinality estimate phd filter approximate multiobject density measurement update multiple target tracking problem particle approximation joint probabilistic data association multiobject particle approximation posterior multiobject density gaussian mixture approximation standard phd measurement update;target tracking filtering theory gaussian processes mixture models;approximation methods;target tracking	The PHD filter is a popular approach to the multiple target tracking problem, however, it suffers from the Poisson assumption which yields a cardinality estimate with too high variance. In recent work Le and Kaplan proposed to improve the performance of the PHD filter using a particle approximation of the predicted multiobject density and updating it using the multiobject measurement pdf. Following the work by Le and Kaplan, in this paper we use the predicted PHD to construct a particle approximation of the predicted multiobject density. Using joint probabilistic data association, the multiobject particle approximation can then be updated using the multiobject measurement likelihood, resulting in a particle approximation of the posterior multiobject density. The posterior multiobject particles are then used to approximate the posterior PHD, which is subsequently predicted using the standard PHD prediction. The proposed filter is implemented using a Gaussian mixture approximation of the PHD intensity, and a simulation study shows a significant performance improvement compared to using the standard PHD measurement update, especially in terms of the cardinality estimate.	approximation algorithm;computational complexity theory;correspondence problem;java platform debugger architecture;kaplan–meier estimator;portable document format;sampling (signal processing);simulation	Karl Granström;Peter Willett;Yaakov Bar-Shalom	2015	2015 18th International Conference on Information Fusion (Fusion)		econometrics;mathematical optimization;mathematics;statistics	Robotics	38.87942930084803	-26.741557538421834	58627
3b9b570e2841a77b9e38c35dd40cfd87f156ee23	multi-target tracking with poisson processes observations	poisson process;time varying;sequential monte carlo methods;ultrasound;sequential monte carlo method;bayesian inference;dynamic model;data association;multiple target tracking;physical characteristic;mixture model;particle filter;multi target tracking;probability hypothesis density;particle filters;target tracking;poisson point process;marked poisson process;random set	This paper considers the problem of Bayesian inference in dynamical models with time-varying dimension. These models have been studied in the context of multiple target tracking problems and for estimating the number of components in mixture models. Traditional solutions for the single target tracking problem becomes infeasible when the number of targets grows. Furthermore, when the number of targets is unknown and the number of observations is influenced by misdetections and clutter, then the problem is complex. In this paper, we consider a marked Poisson process for modeling the time-varying dimension problem. Another solution which has been proposed for this problem is the Probability Hypothesis Density (PHD) filter, which uses a random set formalism for representing the time-varying nature of the state and observation vectors. An important feature of the PHD and the proposed method is the ability to perform sensor data fusion by integrating the information from the multiple observations without an explicit data association step. However, the method proposed here differs from the PHD filter in that uses a Poisson point process formalism with discretized spatial intensity. The method can be implemented with techniques similar to the standard particle filter, but without the need for specifying birth and death probabilities for each target in the update and filtering equations. We show an example based on ultrasound acoustics, where the method is able to represent the physical characteristics of the problem domain.	bayesian approaches to brain function;clutter;correspondence problem;discretization;dynamical system;mixture model;nonlinear acoustics;particle filter;point process;problem domain;semantics (computer science)	Sergio Hernández;Paul D. Teal	2007		10.1007/978-3-540-77129-6_42	econometrics;mathematical optimization;particle filter;mathematics;statistics	Vision	38.37072862552177	-26.396805701303034	59254
3a336630ed5044bcf88f4126eb27a27b9188b409	clustering processes		The problem of clustering is considered, for the case when each data point is a sample generated by a stationary ergodic process. We propose a very natural asymptotic notion of consistency, and show that simple consistent algorithms exist, under most general non-parametric assumptions. The notion of consistency is as follows: two samples should be put into the same cluster if and only if they were generated by the same distribution. With this notion of consistency, clustering generalizes such classical statistical problems as homogeneity testing and process classification. We show that, for the case of a known number of clusters, consistency can be achieved under the only assumption that the joint distribution of the data is stationary ergodic (no parametric or Markovian assumptions, no assumptions of independence, neither between nor within the samples). If the number of clusters is unknown, consistency can be achieved under appropriate assumptions on the mixing rates of the processes. (again, no parametric or independence assumptions). In both cases we give examples of simple (at most quadratic in each argument) algorithms which are consistent.	algorithm;algorithmic efficiency;ambiguous name resolution;bibliothèque de l'école des chartes;cluster analysis;data compression;data point;ergodicity;k-means clustering;mutual information;online and offline;rate of convergence;stationary ergodic process;stationary process	Daniil Ryabko	2010			homogeneity (statistics);local consistency;ergodic theory;discrete mathematics;parametric statistics;stationary ergodic process;joint probability distribution;markov process;cluster analysis;mathematics	ML	31.592306294581594	-25.37197759798945	59658
61bbc8604a7514429184b4f80f4faa81e4aec0f3	mixture models for ordinal data: a pairwise likelihood approach	ordinal data;finite mixture models;composite likelihood;em algorithm	A latent Gaussian mixture model to classify ordinal data is proposed. The observed categorical variables are considered as a discretization of an underlying finite mixture of Gaussians. The model is estimated within the expectation-maximization (EM) framework maximizing a pairwise likelihood. This allows us to overcome the computational problems arising in the full maximum likelihood approach due to the evaluation of multidimensional integrals that cannot be written in closed form. Moreover, a method to cluster the observations on the basis of the posterior probabilities in output of the pairwise EM algorithm is suggested. The effectiveness of the proposal is shown comparing the pairwise likelihood approach with the full maximum likelihood and the maximum likelihood for continuous data ignoring the ordinal nature of the variables. The comparison is made by means of a simulation study; applications to real data are provided.	mixture model;ordinal data	Monia Ranalli;Roberto Rocci	2016	Statistics and Computing	10.1007/s11222-014-9543-4	ordinal regression;econometrics;likelihood principle;score test;expectation–maximization algorithm;machine learning;pattern recognition;mathematics;restricted maximum likelihood;likelihood function;quasi-maximum likelihood;maximum likelihood sequence estimation;ordinal data;statistics	ML	29.468715482517347	-24.880957922199684	59679
814752f7527a40e03f47725006bf481e5acfa6a7	estimation of spherical refractive errors using virtual reality headset		Refractive errors are the most common visual defects in humans. They are corrected using lenses whose power is determined using expensive and bulky devices operated by trained professionals. This limits the outreach of eye- health care. We exploit commercial virtual reality (VR) setup to create a portable and inexpensive system for subjective estimation of spherical refractive errors. In doing so, we aim to keep hardware additions simple and to a minimum. We add a plain reflecting mirror in a VR headset to project optotypes on programmable focal planes at varying distances from the subject’s eye. An interactive interface uses feedback from the user to estimate accommodation range and spherical refractive errors automatically. We compute the range and precision of our system, and validate them in a user trial study. The proposed setup strongly agrees with clinical subjective refraction.	add-ons for firefox;area striata structure;astigmatism;distance;experiment;focal (programming language);headset (audio);headset device component;health care;inborn errors of metabolism;preparation;refractive errors;simulation;user interface;vr - veterans rand health survey;virtual reality exposure therapy;virtual reality headset;visual accommodation	Ashish Goyal;Ajit S. Bopardikar;Vijay N. Tiwari	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8513209	headphones;computer vision;subjective refraction;adaptive optics;artificial intelligence;refraction;virtual reality;headset;accommodation;computer science;lens (optics)	Visualization	38.168710835217084	-37.85267080333559	59938
fdff2ba3ce0df37af58c3ff7aae7f79be2d0833e	bayesian approaches to gaussian mixture modeling	unsupervised learning;pattern clustering;bayesian approach;bayes methods;bayesian methods;bayesian method;bayesian methods parameter estimation roentgenium testing unsupervised learning probability density function equations distributed computing taylor series;gaussian mixture model;cluster analysis;covariance matrices;gaussian mixture models;parameter estimation;unsupervised learning bayes methods pattern clustering parameter estimation hessian matrices covariance matrices;optimization model;hessian matrices;optimal model selection bayesian approaches gaussian mixture modeling	A Bayesian-based methodology is presented which automatically penalizes overcomplex models being fitted to unknown data. We show that, with a Gaussian mixture model, the approach is able to select an “optimal” number of components in the model and so partition data sets. The performance of the Bayesian method is compared to other methods of optimal model selection and found to give good results. The methods are tested on synthetic and real data sets.	bayesian network;data visualization;die (integrated circuit);expectation–maximization algorithm;google map maker;heuristic;information theory;mdl (programming language);maxima and minima;mixture model;model selection;synthetic intelligence	Stephen J. Roberts;Dirk Husmeier;Iead Rezek;William D. Penny	1998	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.730550	unsupervised learning;bayesian average;bayesian probability;computer science;machine learning;pattern recognition;mathematics;bayesian statistics;statistics	ML	29.783649545339586	-31.024762557068634	59976
8d7500bbd5ff8132e1213c86fd173c9d3037860a	the conditional least squares method for thermocouples error modeling	least squares approximations;thermocouples;lsm estimations conditional least squares method thermocouples error modeling electrodes degradation processes;electrodes;mathematical model temperature measurement least squares methods temperature distribution equations measurement uncertainty nonhomogeneous media;thermocouples electrodes least squares approximations;acquired during operation time inhomogeneity of thermoelectrodes the least squares method lsm estimations of model coefficients thermocouples error of drift	The problems in modeling of consequences of thermocouples electrodes degradation processes are considered in this paper. It is showed that application of the least squares method (LSM) leads to bias in estimated model coefficients a when free member is equal to zero. Estimations of model coefficients made by LSM when a free member is not equal to zero have all properties of traditional LSM-estimations but haven't got any physical sense. The new method which has a physical sense is proposed in this paper. Its estimations of the coefficients are unbiased. Method is called the conditional least squares method.	coefficient;elegant degradation;linear least squares (mathematics)	Valeriy Yeromenko;Orest Kochan	2013	2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS)	10.1109/IDAACS.2013.6662661	total least squares;econometrics;mathematical optimization;non-linear iterative partial least squares;electrode;residual sum of squares;mathematics;explained sum of squares;non-linear least squares;thermocouple;least squares;statistics	Robotics	35.59018251749458	-26.54656199112606	60328
2d89761311e01a723e786ca9338f5e879a4c1507	tractability of interpretability via selection of group-sparse models	dynamic programming;graph theory;pareto optimisation;compressed sensing;convex programming;group sparse approximation interpretability tractability group sparse model linear regression problem signal recovery compressive sensing convex relaxation polynomial time solution algorithm np hard problem graph based understanding dynamic programming unimodular constraint pareto frontier;regression analysis compressed sensing convex programming dynamic programming graph theory pareto optimisation polynomials;polynomials;regression analysis;approximation methods polynomials cancer computational modeling linear regression standards compressed sensing	Group-based sparsity models [1], [2] are proven instrumental in linear regression problems for recovering signals from much fewer measurements than standard compressive sensing. A promise of these models is to lead to “interpretable” signals for which we identify its constituent groups, however we show that, in general, claims of correctly identifying the groups with convex relaxations would lead to polynomial time solution algorithms for an NP-hard problem. Instead, leveraging a graph-based understanding of group models, we describe group structures which enable correct model identification in polynomial time via dynamic programming. We also show that group structures that lead to totally unimodular constraints have tractable relaxations. Finally, we highlight the non-convexity of the Pareto frontier of group-sparse approximations.	algorithm;approximation;cobham's thesis;compressed sensing;dynamic programming;np-hardness;pareto efficiency;sparse matrix;system identification;time complexity;unimodular polynomial matrix	Nirav Bhan;Luca Baldassarre;Volkan Cevher	2013	2013 IEEE Global Conference on Signal and Information Processing	10.1109/GlobalSIP.2013.6736969	mathematical optimization;combinatorics;machine learning;mathematics	ML	27.51484653979314	-35.98349841576384	60600
23ef10f1f9eec1c7fab406ff864ad5bc40db21cd	sampling graphical networks via conditional independence coupling of markov chains	conditional independence;markov chain monte carlo;sampling online social networks;metropolis hastings algorithm;coupling of markov chains	Markov Chain Monte Carlo MCMC methods have been used for sampling Online SNs. The main drawbacks are that traditional MCMC techniques such as the Metropolis-Hastings Random Walk MHRW suffer from slow mixing rates, and the resulting sample is usually approximate. An appealing solution is to adapt the MHRW sampler to probability coupling techniques for perfect sampling. While this MHRW coupler is theoretically advanced, it is inapplicable for sampling large SNs in practice. We develop a new coupling algorithm, called Conditional Independence Coupler CIC, which improves existing coupling techniques by adopting a new coalescence condition, called Conditional Independence CI, for efficient coalescence detection. The proposed CIC algorithm is outstandingly scalable for sampling large SNs without any bias as compared to previous traditional MCMC sampling algorithms.	markov chain	Guichong Li	2016		10.1007/978-3-319-34111-8_36	metropolis–hastings algorithm;coupling from the past;conditional independence;markov chain monte carlo;computer science;slice sampling;machine learning;rejection sampling;statistics	ML	27.8351643950136	-28.568507048980187	60653
953df2f29a3217f70816d12ee65dd46a20bdb533	sparse multi-prototype classification		We present a new class of sparse multi-prototype classifiers, designed to combine the computational advantages of sparse predictors with the non-linear power of prototype-based classification techniques. This combination makes sparse multi-prototype models especially well-suited for resource constrained computational platforms, such as those found in IoT devices. We cast our supervised learning problem as a convex-concave saddle point problem and design a provably-fast algorithm to solve it. We complement our theoretical analysis with an empirical study that demonstrates the power of our methodology.	algorithm;complement system proteins;nonlinear system;prototype;sparse matrix;supervised learning	Vikas K. Garg	2018			supervised learning;computer science;machine learning;empirical research;artificial intelligence;internet of things;saddle point	AI	25.135856234664757	-37.267342681914684	60657
1b63197f51487781c72fa8a6f62b32c9790711d3	multi human trajectory estimation using stochastic sampling and its application to meeting recognition.	gibbs sampling;real time;energy minimization	In this paper we present a stochastic sampling approach to estimate multiple human trajectory in the meeting. The algorithm is formalized as a energy minimization problem based on stochastic sampling of deterministic trajectory, and has some effectiveness to the low frame data with jumps and switchings and it can estimate a near optimal result in 9 times faster then the real-time by using Gibbs sampling. Also experiment is shown using meeting data of real environment.	algorithm;energy minimization;gibbs sampling;real-time clock;sampling (signal processing)	Yosuke Matsusaka;Hideki Asoh;Futoshi Asano	2007			mathematical optimization;gibbs sampling;computer science;slice sampling;machine learning;umbrella sampling;energy minimization;statistics	Robotics	38.07351739641564	-27.372942583116252	60665
838ceba3dee81d95d3ee9849b5bbdd993429f969	fast learning of clusters and topics via sparse posteriors		Mixture models and topic models generate each observation from a single cluster, but standard variational posteriors for each observation assign positive probability to all possible clusters. This requires dense storage and runtime costs that scale with the total number of clusters, even though typically only a few clusters have significant posterior mass for any data point. We propose a constrained family of sparse variational distributions that allow at most $L$ non-zero entries, where the tunable threshold $L$ trades off speed for accuracy. Previous sparse approximations have used hard assignments ($L=1$), but we find that moderate values of $L>1$ provide superior performance. Our approach easily integrates with stochastic or incremental optimization algorithms to scale to millions of examples. Experiments training mixture models of image patches and topic models for news articles show that our approach produces better-quality models in far less time than baseline methods.	algorithm;approximation;baseline (configuration management);calculus of variations;data point;experiment;mathematical optimization;mixture model;sparse matrix;stochastic process;topic model;variational principle	Michael C. Hughes;Erik B. Sudderth	2016	CoRR		computer science;machine learning;data mining;statistics	ML	25.660432814673815	-32.56884943147956	60716
f09a929163e9663583978cb9ab269775a3fa0029	potential conditional mutual information: estimators, properties and applications		The conditional mutual information I(X;Y |Z) measures the average information that X and Y contain about each other given Z. This is an important primitive in many learning problems including conditional independence testing, graphical model inference, causal strength estimation and time-series problems. In several applications, it is desirable to have a functional purely of the conditional distribution pY |X,Z rather than of the joint distribution pX,Y,Z . We define the potential conditional mutual information as the conditional mutual information calculated with a modified joint distribution pY |X,ZqX,Z , where qX,Z is a potential distribution, fixed airport. We develop K nearest neighbor based estimators for this functional, employing importance sampling, and a coupling trick, and prove the finite k consistency of such an estimator. We demonstrate that the estimator has excellent practical performance and show an application in dynamical system inference.	causal filter;conditional mutual information;dynamical system;entropy (information theory);graphical model;importance sampling;sampling (signal processing);time series	Arman Rahimzamani;Sreeram Kannan	2017	CoRR		estimator;mathematics;importance sampling;discrete mathematics;conditional entropy;conditional mutual information;conditional independence;conditional probability distribution;joint probability distribution;graphical model	ML	25.60836038222688	-27.512449508107014	60723
21afc4e6eec1cd971435b15324b02f6136c4abd6	efficient sampling for bipartite matching problems		Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in realworld applications of these problems is intractable, making efficient approximation methods essential for learning and inference. In this paper we propose a novel sequential matching sampler based on a generalization of the PlackettLuce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difficult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems—ranking and image correspondence—which show that the sequential matching sampler efficiently approximates the target distribution, significantly outperforming other sampling approaches.	approximation;approximation algorithm;bmp file format;experiment;information retrieval;matching (graph theory);multimodal interaction;sampling (signal processing)	Maksims Volkovs;Richard S. Zemel	2012			mathematical optimization;combinatorics;3-dimensional matching;machine learning;mathematics	ML	27.032245977193583	-29.201574166388447	60789
d56636aade25dc760f69c24d5b5a0bdca2800d5d	converting svdd scores into probability estimates		To enable post-processing, the output of a support vector data description (SVDD) should be a calibrated probability as done for SVM. Standard SVDD does not provide such probabilities. To create probabilities, we first generalize the SVDD model and propose two calibration functions. The first one uses a sigmoid model and the other one is based on a generalized extreme distribution model. To estimate calibration parameters, we use the consistency property of the estimator associated with a single SVDD model. A synthetic dataset and datasets from the UCI repository are used to compare the performance against a robust kernel density estimator.	eisenstein's criterion;kernel density estimation;mathematical optimization;maxima and minima;optimality criterion;optimization problem;sigmoid function;synthetic intelligence;video post-processing	Meriem El Azami;Carole Lartizien;Stéphane Canu	2016			econometrics;computer science;data mining;statistics	Vision	30.297709464527497	-27.08975036435064	60803
23f774dedc77ceda987be8e1583b00371dd4c66c	testing ising models		Given samples from an unknown multivariate distribution p, is it possible to distinguish whether p is the product of its marginals versus p being far from every product distribution? Similarly, is it possible to distinguish whether p equals a given distribution q versus p and q being far from each other? These problems of testing independence and goodness-of-fit have received enormous attention in statistics, information theory, and theoretical computer science, with sample-optimal algorithms known in several interesting regimes of parameters [BFF01, Pan08, VV17, ADK15, DK16]. Unfortunately, it has also been understood that these problems become intractable in large dimensions, necessitating exponential sample complexity. Motivated by the exponential lower bounds for general distributions as well as the ubiquity of Markov Random Fields (MRFs) in the modeling of high-dimensional distributions, we initiate the study of distribution testing on structured multivariate distributions, and in particular the prototypical example of MRFs: the Ising Model. We demonstrate that, in this structured setting, we can avoid the curse of dimensionality, obtaining sample and time efficient testers for independence and goodness-of-fit. One of the key technical challenges we face along the way is bounding the variance of functions of the Ising model. ∗Supported by a Microsoft Research Faculty Fellowship, NSF CCF-1551875, CCF-1617730, CCF-1650733, and ONR N00014-12-1-0999. †Supported by NSF CCF-1551875, CCF-1617730, CCF-1650733, and ONR N00014-12-1-0999. ‡Supported by NSF CCF-1551875, CCF-1617730, CCF-1650733, and ONR N00014-12-1-0999. ISSN 1433-8092 Electronic Colloquium on Computational Complexity, Revision 2 of Report No. 6 (2017)	algorithm;belief revision;curse of dimensionality;electronic colloquium on computational complexity;ibm notes;information theory;international standard serial number;ising model;markov chain;markov random field;microsoft research;sample complexity;theoretical computer science;time complexity	Constantinos Daskalakis;Nishanth Dikkala;Gautam Kamath	2017		10.1137/1.9781611975031.130	econometrics;combinatorics;mathematics;algorithm;statistics	Theory	25.34754647545821	-24.16169667507071	61018
ff99b7e85195ef3d0bf4ea2f54e19aef94b693a0	parameter estimation in finite mixture models by regularized optimal transport: a unified framework for hard and soft clustering		In this short paper, we formulate parameter estimation for finite mixture models in the context of discrete optimal transportation with convex regularization. The proposed framework unifies hard and soft clustering methods for general mixture models. It also generalizes the celebrated k-means and expectation-maximization algorithms in relation to associated Bregman divergences when applied to exponential family mixture models.	akaike information criterion;bayesian information criterion;bregman divergence;cluster analysis;estimation theory;expectation–maximization algorithm;hierarchical clustering;k-means clustering;mixture model;model selection;supervised learning;time complexity;transportation theory (mathematics)	Arnaud Dessein;Nicolas Papadakis;Charles-Alban Deledalle	2017	CoRR		mixture model;mathematical optimization;regular polygon;fuzzy clustering;mathematics;estimation theory;exponential family;regularization (mathematics)	ML	27.942635338974807	-30.383654817289667	61159
1516c7ed7da727228579dd2b15576b8d79449e6c	dependence maximizing temporal alignment via squared-loss mutual information		The goal of temporal alignment is to establish time correspondence between two sequences, which has many applications in a variety of areas such as speech processing, bioinformatics, computer vision, and computer graphics. In this paper, we propose a novel temporal alignment method called least-squares dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes statistical dependency between sequences, measured by a squared-loss variant of mutual information. The benefit of this novel information-theoretic formulation is that LSDTW can align sequences with different lengths, different dimensionality, high nonlinearity, and non-Gaussianity in a computationally efficient manner. In addition, model parameters such as an initial alignment matrix can be systematically optimized by cross-validation. We demonstrate the usefulness of LSDTW through experiments on synthetic and real-world Kinect action recognition datasets.	algorithmic efficiency;align (company);bioinformatics;computer graphics;computer vision;cross-validation (statistics);dynamic time warping;expectation–maximization algorithm;experiment;information theory;kinect;least squares;mean squared error;mutual information;nonlinear system;sequence alignment;speech processing;synthetic intelligence	Makoto Yamada;Leonid Sigal;Michalis Raptis;Masashi Sugiyama	2012	CoRR		computer vision;computer science;theoretical computer science;data mining	Vision	29.638694293574076	-37.75764973514522	61239
2bb24cc4fea8afcb556336485beef1cab472b488	particle gibbs with refreshed backward simulation	markov kernels particle gibbs algorithm bayesian parameter estimation markovian state space models markov chains component particle filter refreshed backward simulation ancestor indexes;kernel;particle filtering numerical methods markov processes monte carlo methods;indexes;smoothing methods;trajectory;backward simulation sequential monte carlo particle markov chain monte carlo gibbs sampling;approximation methods;markov processes;monte carlo methods;markov processes indexes monte carlo methods trajectory kernel smoothing methods approximation methods	The particle Gibbs algorithm can be used for Bayesian parameter estimation in Markovian state space models. Sometimes the resulting Markov chains mix slowly when the component particle filter suffers from degeneracy. This effect can be somewhat alleviated using backward simulation. In this paper we show how a simple modification to this scheme, which we refer to as refreshed backward simulation, can further improve the mixing. This works by sampling new state values simultaneously with the corresponding ancestor indexes. Although the necessary conditional distributions cannot be sampled directly, we provide suitable Markov kernels which target them. The efficacy of this new scheme is demonstrated with a simulation example.	degeneracy (graph theory);estimation theory;gibbs algorithm;hidden markov model;markov chain;particle filter;sampling (signal processing);simulation;state space	Pete Bunch;Fredrik Lindsten;Sumeetpal Singh	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178745	database index;econometrics;markov chain;mathematical optimization;markov kernel;kernel;gibbs sampling;hybrid monte carlo;particle filter;markov chain monte carlo;markov property;trajectory;monte carlo molecular modeling;mathematics;markov process;markov chain mixing time;markov model;statistics;monte carlo method;variable-order markov model	Robotics	37.45888180080082	-26.122661038859707	61565
b4eaeb44c2abd685875e4c68ded728375428520c	graph clustering: block-models and model free results		Clustering graphs under the Stochastic Block Model (SBM) and extensions are well studied. Guarantees of correctness exist under the assumption that the data is sampled from a model. In this paper, we propose a framework, in which we obtain “correctness” guarantees without assuming the data comes from a model. The guarantees we obtain depend instead on the statistics of the data that can be checked. We also show that this framework ties in with the existing model-based framework, and that we can exploit results in model-based recovery, as well as strengthen the results existing in that area of research.	sampling (signal processing);stochastic block model;super bit mapping	Yali Wan;Marina Meila	2016			computer science;theoretical computer science;machine learning;data mining;database;statistics	DB	26.011535574034134	-33.327999836651216	61639
2a3cd278db41db7aec5933afc1aa5155cdd1551e	information theoretic properties of markov random fields, and their algorithmic applications		Markov random fields are a popular model for high-dimensional probability distributions. Over the years, many mathematical, statistical and algorithmic problems on them have been studied. Until recently, the only known algorithms for provably learning them relied on exhaustive search, correlation decay or various incoherence assumptions. Bresler [4] gave an algorithm for learning general Ising models on bounded degree graphs. His approach was based on a structural result about mutual information in Ising models. Here we take a more conceptual approach to proving lower bounds on the mutual information. Our proof generalizes well beyond Ising models, to arbitrary Markov random fields with higher order interactions. As an application, we obtain algorithms for learning Markov random fields on bounded degree graphs on n nodes with r-order interactions in n time and log n sample complexity. Our algorithms also extend to various partial observation models.	algorithm;brute-force search;interaction;ising model;machine learning;markov chain;markov random field;mutual information;sample complexity;theory	Linus Hamilton;Frederic Koehler;Ankur Moitra	2017			combinatorics;discrete mathematics;machine learning;mathematics;markov model;statistics;variable-order markov model	ML	25.871592217563997	-28.961186905094262	61646
053ff27aba868c64823dbbe2167a762dd3f33b53	probabilistic slow features for behavior analysis	probabilistic logic covariance matrices optimization inference algorithms feature extraction algorithm design and analysis heuristic algorithms;covariance matrices;heuristic algorithms;feature extraction;temporal alignment behavior analysis linear dynamical system lds slow feature analysis sfa;facial behavior analysis probabilistic slow features latent feature learning technique time varying dynamic phenomena analysis slow feature analysis deterministic component analysis technique multidimensional sequences first order time derivative approximation variance minimization latent variables deterministic sfa optimization frameworks probabilistic sfa optimization frameworks expectation maximization algorithm time varying data sequences slowest varying latent space dynamic time warping techniques robust sequence time alignment;inference algorithms;optimization;probabilistic logic;probability approximation theory behavioural sciences expectation maximisation algorithm learning artificial intelligence minimisation;algorithm design and analysis	A recently introduced latent feature learning technique for time-varying dynamic phenomena analysis is the so-called slow feature analysis (SFA). SFA is a deterministic component analysis technique for multidimensional sequences that, by minimizing the variance of the first-order time derivative approximation of the latent variables, finds uncorrelated projections that extract slowly varying features ordered by their temporal consistency and constancy. In this paper, we propose a number of extensions in both the deterministic and the probabilistic SFA optimization frameworks. In particular, we derive a novel deterministic SFA algorithm that is able to identify linear projections that extract the common slowest varying features of two or more sequences. In addition, we propose an expectation maximization (EM) algorithm to perform inference in a probabilistic formulation of SFA and similarly extend it in order to handle two and more time-varying data sequences. Moreover, we demonstrate that the probabilistic SFA (EM-SFA) algorithm that discovers the common slowest varying latent space of multiple sequences can be combined with dynamic time warping techniques for robust sequence time-alignment. The proposed SFA algorithms were applied for facial behavior analysis, demonstrating their usefulness and appropriateness for this task.	align (company);blast e-value;congenital contractural arachnodactyly;dynamic time warping;electron microscopy;expectation–maximization algorithm;experiment;feature learning;inference;latent variable;loudspeaker time alignment;mathematical optimization;norm (social);order of approximation;projections and predictions;sample variance;simple features;synthetic intelligence;unsupervised learning	Lazaros Zafeiriou;Mihalis A. Nicolaou;Stefanos P. Zafeiriou;Symeon Nikitidis;Maja Pantic	2016	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2015.2435653	algorithm design;mathematical optimization;probabilistic analysis of algorithms;feature extraction;computer science;machine learning;pattern recognition;mathematics;probabilistic logic;probabilistic latent semantic analysis;statistics	ML	30.378073335513182	-37.56394391427603	61673
067ecc216f0e10a04d062bfe3a325279335f8bf1	computationally efficient hyperspectral data learning based on the doubly stochastic dirichlet process	computational modeling mixture models computational efficiency feature extraction hyperspectral imaging markov processes;computational modeling;feature extraction;markov processes;hyperspectral imaging;mixture models;computational efficiency;article;remote sensing bayesian model computational efficiency hyperspectral image hsi land covers	The Dirichlet process (DP) prior is effective in modeling HSIs (HSI) and identifying land-cover classes. However, modeling a continuously varying intensity of these land covers elegantly and consistently is still a challenge. We propose a doubly stochastic DP (DSDP) as an efficient model of the global topic measurement space, which imposes a weaker assumption compared with the discrete Markov assumption, resulting in a lower computational cost than other DP-prior-based models. We also present a mixture model of DSDP, which is termed the marked sigmoidal Gaussian process (SGP) DSDP mixture model. It can be thinned from a DP mixture without massive auxiliary covariates, and the marked function prior makes the number of land-cover classes consistent, whereas the SGP function prior models the HSI land-cover variation globally. The consistency of the number of land covers is maintained for various HSIs with large-scale geographical areas. Experiments show that the model is robust and consistent on HSI identification with weak or even no supervision.	algorithm;algorithmic efficiency;central processing unit;cluster analysis;computation;doubly stochastic model;experiment;gaussian process;horizontal situation indicator;matlab;markov chain;mixture model;semi-supervised learning;sigmoid function;simplified perturbations models;stochastic gradient descent;symposium on geometry processing;system identification	Xing Sun;Nelson H. C. Yung;Edmund Y. Lam;Hayden Kwok-Hay So	2017	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2016.2606575	econometrics;feature extraction;hyperspectral imaging;machine learning;pattern recognition;mixture model;mathematics;markov process;computational model;statistics;remote sensing	ML	29.84244233881708	-32.386396585924174	61851
695e667c4543492a84f1e50acd9c722543bb8aa1	a new feature vector using selected bispectra for signal classification with application in radar target recognition	bispectra;traitement signal;translation invariant;image recognition;metodo espectral;reconocimiento imagen;deteccion blanco;time shift;detection signal;circularly integrated bispectra;axially integrated bispectra;rib;signal detection;aspect dependence feature vector bispectra signal classification radar target recognition radially integrated bispectra axially integrated bispectra circularly integrated bispectra rib aib cib integration paths maximum interclass separability range profiles time shift;cib;classification;detection cible;feature vector;smoothing methods;deteccion senal;radially integrated bispectra;target recognition;feature extraction;maximum interclass separability;signal processing;aspect dependence;spatial databases;signal classification;reconnaissance image;pattern classification radar applications target recognition feature extraction data acquisition laboratories radar signal processing automation smoothing methods spatial databases;feature extraction signal classification radar target recognition spectral analysis;spectral method;radar target recognition;pattern classification;integration paths;radar applications;methode spectrale;extraction caracteristique;spectral analysis;procesamiento senal;target detection;data acquisition;clasificacion;aib;radar signal processing;radar;range profiles;automation	Radially integrated bispectra (RIB), axially integrated bispectra (AIB), and circularly integrated bispectra (CIB) were used as feature vectors of signals, but many bispectra on integration paths may be redundant, and some bispectra are even baneful for signal classification. To avoid these problems, this paper proposes using selected bispectra with the maximum interclass separability as feature vectors of signals. In radar target recognition, range profiles are suitable feature vectors, but they have two main shortcomings: sensitivity to time shift and aspect dependence. Since the selected bispectra of range profiles are translation invariant and can avoid redundant and baneful bispectra as features, they are thus especially suitable for radar target recognition, which is shown by experiments.	controlled image base;experiment;feature vector;linear separability;radar;timeshift	Xianda Zhang;Yu Shi;Zheng Bao	2001	IEEE Trans. Signal Processing	10.1109/78.942617	computer vision;speech recognition;feature vector;feature extraction;biological classification;computer science;electrical engineering;automation;signal processing;pattern recognition;time shifting;data acquisition;radar;detection theory;spectral method	Robotics	37.534547378220296	-34.69566293167222	61947
a9208fcb74e23ae98ed6b2f230149f63aa4e8b03	statistical learning via manifold learning	manifold learning regression statistical learning manifold learning tangent bundle manifold learning;manifold learning;jacobian matrix estimation statistical learning nonlinear regression task predictive function q dimensional regression manifold manifold valued sample manifold learning regression equality mlr;statistical learning;manifold learning regression;regression analysis functions jacobian matrices learning artificial intelligence;manifolds kernel jacobian matrices zinc statistical learning training data principal component analysis;tangent bundle manifold learning	A new geometrically motivated method is proposed for solving the non-linear regression task consisting in constructing a predictive function which estimates an unknown smooth mapping f from q-dimensional inputs to m-dimensional outputs based on a given 'input-output' training pairs. The unknown mapping f determines q-dimensional Regression manifold M(f) consisting of all the (q+m)-dimensional 'input-output' vectors. The manifold is covered by a single chart, the training data set determines a manifold-valued sample from this manifold. Modern Manifold Learning technique is used for constructing the certain estimator M* of the Regression manifold from the sample which accurately approximates the Regression manifold. The proposed method called Manifold Learning Regression (MLR) finds the predictive function fMLR to ensure an equality M(fMLR) = M*. The MLR estimates also the m×q Jacobian matrix of the mapping f.	algorithm;experiment;jacobian matrix and determinant;kernel (operating system);kriging;learning to rank;machine learning;nonlinear dimensionality reduction;nonlinear system;numerical analysis;predictive modelling;stationary process;test set	Alexander V. Bernstein;Alexander P. Kuleshov;Yury Yanovich	2015	2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2015.26	statistical manifold;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;nonlinear dimensionality reduction;manifold alignment	Robotics	29.76597685461873	-34.97381611818044	62254
99c797e19e0f3367420d8e1e0cb417b20ad9c0e5	learning multi-label predictors under sparsity budget			multi-label classification;sparse matrix	Pekka Naula;Tapio Pahikkala;Antti Airola;Tapio Salakoski	2011		10.3233/978-1-60750-754-3-30	machine learning;artificial intelligence;computer science	NLP	24.887819435248698	-35.43744474877198	62344
0cd5f27935a4d009f6097b0f767ffaa4476252ef	spectral estimation of conditional random graph models for large-scale network data		Generative models for graphs have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are either only suitable for characterizing some particular network properties (such as degree distribution or clustering coefficient), or they are aimed at estimating joint probability distributions, which is often intractable in large-scale networks. In this paper, we first propose a novel network statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random graph model, switching the focus from the estimation of joint probability distributions to a more tractable conditional estimation setting. After analyzing the dependence structure characterizing Fiedler random graphs, we evaluate them experimentally in edge prediction over several real-world networks, showing that they allow to reach a much higher prediction accuracy than various alternative statistical models.	algorithmic inference;clustering coefficient;cobham's thesis;degree distribution;dirac delta function;experiment;random graph;sampling (signal processing);spectral density estimation;spectral graph theory;statistical model	Antonino Freno;Mikaela Keller;Gemma C. Garriga;Marc Tommasi	2012			random graph;combinatorics;machine learning;mathematics;statistics	ML	26.366150793698743	-28.163766358421576	62473
324aaf2d779747646355f145ffdb248907117c8e	variational particle approximations		Monte Carlo methods provide a powerful framework for approximating probability distributions with a set of stochastically sampled particles. In this paper, we rethink particle approximations from the perspective of variational inference, where the particles play the role of variational parameters. This leads to a deterministic version of Monte Carlo in which the particles are selected to optimize the Kullback-Leibler divergence between the approximation and the target distribution. Variational particle approximations overcome some of the weaknesses of Monte Carlo methods like particle filtering, leading to substantially improved performance on several synthetic and realworld datasets.	approximation;calculus of variations;kullback–leibler divergence;monte carlo method;particle filter;synthetic intelligence;variational method (quantum mechanics);variational principle	Tejas D. Kulkarni;Ardavan Saeedi;Samuel J Gershman	2017	Journal of Machine Learning Research		quantum monte carlo;econometrics;mathematical optimization;dynamic monte carlo method;hybrid monte carlo;particle filter;markov chain monte carlo;monte carlo molecular modeling;mathematics;monte carlo integration;statistics;monte carlo method	ML	27.464403997299172	-28.65273556996344	62598
08b1982b57d2c6ab9aac4171875642e9577707dd	unsupervised learning with non-ignorable missing data		In this paper we explore the topic of unsupervised learning in the presence of nonignorable missing data with an unknown missing data mechanism. We discuss several classes of missing data mechanisms for categorical data and develop learning and inference methods for two specific models. We present empirical results using synthetic data which show that these algorithms can recover both the unknown selection model parameters and the underlying data model parameters to a high degree of accuracy. We also apply the algorithms to real data from the domain of collaborative filtering, and report initial results.	algorithm;categorical variable;collaborative filtering;data model;latent variable model;missing data;synthetic data;unsupervised learning	Benjamin M. Marlin;Sam T. Roweis;Richard S. Zemel	2005			semi-supervised learning;missing data;machine learning;artificial intelligence;collaborative filtering;computer science;unsupervised learning;categorical variable;data model;inference;synthetic data	ML	28.268733367841566	-31.562215712559045	62750
4820b9b840382ed7f9609359560a2e18d28119fe	generalized outlier detection with flexible kernel density estimates		We analyse the interplay of density estimation and outlier detection in density-based outlier detection. By clear and principled decoupling of both steps, we formulate a generalization of density-based outlier detection methods based on kernel density estimation. Embedded in a broader framework for outlier detection, the resulting method can be easily adapted to detect novel types of outliers: while common outlier detection methods are designed for detecting objects in sparse areas of the data set, our method can be modified to also detect unusual local concentrations or trends in the data set if desired. It allows for the integration of domain knowledge and specific requirements. We demonstrate the flexible applicability and scalability of the method on large real world data sets.	anomaly detection;blueprint;coupling (computer programming);heuristic;kernel density estimation;requirement;scalability;sensor;sparse matrix	Erich Schubert;Arthur Zimek;Hans-Peter Kriegel	2014		10.1137/1.9781611973440.63	computer science;anomaly detection;pattern recognition;artificial intelligence;kernel (statistics);machine learning;scalability;outlier;density estimation;kernel density estimation;data set;variable kernel density estimation	ML	27.025342895439948	-37.61947731967831	63111
43265af53799be3ad18acc7ac73a56633f401b99	compressed least squares regression revisited		We revisit compressed least squares (CLS) regression as originally analyzed in Maillard and Munos (2009) and later on in Kaban (2014) with some refinements. Given a set of high-dimensional inputs, CLS applies a random projection and then performs least squares regression based on the projected inputs of lower dimension. This approach can be beneficial with regard to both computation (yielding a smaller least squares problem) and statistical performance (reducing the estimation error). We will argue below that the outcome of previous analysis of the procedure is not meaningful in typical situations, yielding a bound on the prediction error that is inferior to ordinary least squares while requiring the dimension of the projected data to be of the same order as the original dimension. As a fix, we subsequently present a modified analysis with meaningful implications that much better reflects empirical results with simulated and real data.	coefficient;common language infrastructure;computation;dimensionality reduction;linear model;linear programming;machine learning;mathematical optimization;nonlinear programming;nonlinear system;offset binary;ordinary least squares;random projection;singular value decomposition;sparse matrix	Martin Slawski	2017			statistics;mathematical optimization;partial least squares regression;least trimmed squares;explained sum of squares;computer science;total least squares;non-linear least squares;least squares;generalized least squares	ML	28.481300715717847	-25.603210675127347	63163
213fde329008446a6dc8d39b0277a053493d2d27	dual online inference for latent dirichlet allocation		Latent Dirichlet allocation (LDA) provides an efficient tool to analyze very large text collections. In this paper, we discuss three novel contributions: (1) a proof for the tractability of the MAP estimation of topic mixtures under certain conditions that might fit well with practices, even though the problem is known to be intractable in the worse case; (2) a provably fast algorithm (OFW) for inferring topic mixtures; (3) a dual online algorithm (DOLDA) for learning LDA at a large scale. We show that OFW converges to some local optima, but under certain conditions it can converge to global optima. The discussion of OFW is general and hence can be readily employed to accelerate the MAP estimation in a wide class of probabilistic models. From extensive experiments we find that DOLDA can achieve significantly better predictive performance and semantic quality, with lower runtime, than stochastic variational inference. Further, DOLDA enables us to easily analyze text streams or millions of documents.	converge;experiment;global optimization;latent dirichlet allocation;local optimum;machine learning;map;online algorithm;rate of convergence;topic model;variational principle	Khoat Than;Tung Doan	2014			computer science;machine learning;pattern recognition;data mining;statistics	ML	25.36714600322033	-32.551996545938785	63384
1edc9fb72f783026c4a65e935305611b83a80e88	online kernel-based learning for task-space tracking robot control	robots convex programming data handling learning artificial intelligence regression analysis;convex programming;abt scholkopf;task space tracking kernel methods online learning real time learning robot control;robots;robots data models kernel joints predictive models aerospace electronics torque;regression analysis;data handling;learning artificial intelligence;task space control mappings online kernel based learning task space tracking robot control redundant robot systems data driven model learning methods sampled data input data point output values nonconvex solution space regression methods	Task-space control of redundant robot systems based on analytical models is known to be susceptive to modeling errors. Data-driven model learning methods may present an interesting alternative approach. However, learning models for task-space tracking control from sampled data is an ill-posed problem. In particular, the same input data point can yield many different output values, which can form a nonconvex solution space. Because the problem is ill-posed, models cannot be learned from such data using common regression methods. While learning of task-space control mappings is globally ill-posed, it has been shown in recent work that it is locally a well-defined problem. In this paper, we use this insight to formulate a local kernel-based learning approach for online model learning for task-space tracking control. We propose a parametrization for the local model, which makes an application in task-space tracking control of redundant robots possible. The model parametrization further allows us to apply the kernel-trick and, therefore, enables a formulation within the kernel learning framework. In our evaluations, we show the ability of the method for online model learning for task-space tracking control of redundant robots.	data point;evaluation;feasible region;kernel (operating system);kernel method;robot (device);robot control;well-posed problem	Duy Nguyen-Tuong;Jan Peters	2012	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2012.2201261	semi-supervised learning;unsupervised learning;robot;robot learning;instance-based learning;mathematical optimization;convex optimization;simulation;computer science;artificial intelligence;online machine learning;machine learning;group method of data handling;competitive learning;active learning;regression analysis;iterative learning control;generalization error	ML	29.922055256887763	-35.13298022931006	63871
d95f81028b896b2080c05fcf7c9cafaa9e31c26b	a fault diagnosis method of planetary gearbox under variable speed condition using vold-kalman filter and laplacian score		Planetary gearboxes operation under non-stationary working condition exhibit complex time-varying modulations and spectral structures, resulting in difficulty in the fault diagnosis of planetary gearbox. In order to effectively remove the influence of rotating speed and extract the fault characteristics, this paper aims to develop a fault diagnosis scheme based on Void-Kalman filter (VKF) and Laplacian score (LS). In this method, VKF is firstly adopted to remove the fault-unrelated components and give a clear representation of the fault symptoms. Second, the time-domain features and frequency-domain features are used to extract fault features. Third, LS approach is introduced to refine the fault features by sorting the scale factors. Lastly, the selected features are fed into the LSSVM to automatically complete the fault pattern identifications. The combined method is experimentally demonstrated to be able to recognize the different sun gear fault types of planetary gearboxes.	experiment;feature extraction;kalman filter;laplacian matrix;least squares;planetary scanner;simulation;sorting;stationary process;traffic collision avoidance system	Yongbo Li;Xianzhi Wang;Zhiliang Liu;Shubin Si	2018	2018 IEEE International Conference on Prognostics and Health Management (ICPHM)	10.1109/ICPHM.2018.8448875	feature extraction;time–frequency analysis;kalman filter;control theory;vibration;transmission (mechanics);laplace operator;sorting;mathematics	Robotics	37.4454164383379	-30.77718332729486	63984
3432127c0af778a96391a0f43e5af75f65c0ad26	sparse representations for radar with matlab examples	compressive sensing;sparse representations;matlab;radar	Although the field of sparse representations is relatively new, research activities in academic and industrial research labs are already producing encouraging results. The sparse signal or parameter model motivated several researchers and practitioners to explore high complexity/wide bandwidth applications such as Digital TV, MRI processing, and certain defense applications. The potential signal processing advancements in this area may influence radar technologies. This book presents the basic mathematical concepts along with a number of useful MATLAB examples to emphasize the practical implementations both inside and outside the radar field.	matlab;radar;sparse	Peter Knee	2012		10.2200/S00445ED1V01Y201208ASE010	computer vision;simulation;speech recognition;computer science;electrical engineering;theoretical computer science;machine learning;compressed sensing;radar	EDA	34.64136909465316	-34.24163935435166	64023
d9fbb7ac8602cea6479854ccf412834199fdded4	development of embedded piezoelectric acoustic sensor array architecture	health monitoring;neural system;signal processing;sensor array;spatial filtering;acoustic emission;acoustic waves;high frequency	Abstract   This paper examines development of novel piezoelectric acoustic sensors, which are capable of sensing high frequency acoustic emissions in a composite/metallic plate. The fabrication of the piezoelectric acoustic sensors, made from piezoceramic ribbons, is described in the paper. An attempt was made to build directionality into the sensing system itself. Continuous sensors placed at right angles on a plate are discussed as a new approach to measure and locate the source of the acoustic waves. Novel signal processing algorithms based on bio-inspired neural systems for spatial filtering of large numbers of embedded sensor arrays in laminated composite media are presented. It is expected that the present work would help in the development of microelectronic sensing aiding diagnostics and prognostics techniques for highly efficient health monitoring of integrated aerospace vehicles and structures.	acoustic cryptanalysis;embedded system;piezoelectricity	Anindya Ghoshal;William H. Prosser;Heung Soo Kim;Aditi Chattopadhyay;Ben Copeland	2010	Microelectronics Reliability	10.1016/j.microrel.2010.01.037	acoustic wave;electronic engineering;acoustics;surface acoustic wave sensor;engineering;electrical engineering;acoustic emission;signal processing;high frequency;sensor array;physics;spatial filter	EDA	37.5487495277578	-33.296499886404334	64189
cc93a1ac139642e02dbe04fdd5605af137cd2545	density ratio estimation in machine learning	ratio estimator;machine learning	Machine learning is an interdisciplinary field of science and engineering that studies mathematical theories and practical applications of systems that learn. This book introduces theories, methods, and applications of density ratio estimation, which is a newly emerging paradigm in the machine learning community. Various machine learning problems such as non-stationarity adaptation, outlier detection, dimensionality reduction, independent component analysis, clustering, classification, and conditional density estimation can be systematically solved via the estimation of probability density ratios. The authors offer a comprehensive introduction of various density ratio estimators including methods via density estimation, moment matching, probabilistic classification, density fitting, and density ratio fitting as well as describing how these can be applied to machine learning. The book also provides mathematical theories for density ratio estimation including parametric and non-parametric convergence analysis and numerical stability analysis to complete the first and definitive treatment of the entire framework of density ratio estimation in machine learning.	anomaly detection;cluster analysis;dimensionality reduction;independent component analysis;kernel density estimation;machine learning;numerical stability;programming paradigm;stationary process;statistical classification;theory	Masashi Sugiyama;Taiji Suzuki;Takafumi Kanamori	2012			unsupervised learning;econometrics;density estimation;wake-sleep algorithm;computer science;online machine learning;machine learning;multivariate kernel density estimation;computational learning theory;statistics	ML	30.206298078847773	-31.1839972208998	64213
aa143bd5ffc8ef52030c94e7d5e48ac19c650f1c	map approximation to the variational bayes gaussian mixture model and application		The learning of variational inference can be widely seen as first estimating the class assignment variable and then using it to estimate parameters of the mixture model. The estimate is mainly performed by computing the expectations of the prior models. However, learning is not exclusive to expectation. Several authors report other possible configurations that use different combinations of maximization or expectation for the estimation. For instance, variational inference is generalized under the expectation–expectation (EE) algorithm. Inspired by this, another variant known as the maximization–maximization (MM) algorithm has been recently exploited on various models such as Gaussian mixture, Field-of-Gaussians mixture, and sparse-coding-based Fisher vector. Despite the recent success, MM is not without issue. Firstly, it is very rare to find any theoretical study comparing MM to EE. Secondly, the computational efficiency and accuracy of MM is seldom compared to EE. Hence, it is difficult to convince the use of MM over a mainstream learner such as EE or even Gibbs sampling. In this work, we revisit the learning of EE and MM on a simple Bayesian GMM case. We also made theoretical comparison of MM with EE and found that they in fact obtain near identical solutions. In the experiments, we performed unsupervised classification, comparing the computational efficiency and accuracy of MM and EE on two datasets. We also performed unsupervised feature learning, comparing Bayesian approach such as MM with other maximum likelihood approaches on two datasets.	approximation;calculus of variations;mixture model	Kart-Leong Lim;Han Wang	2018	Soft Comput.	10.1007/s00500-017-2565-z	econometrics;mathematical optimization;computer science;machine learning;statistics	ML	29.0051386653364	-31.987394661402277	64539
6b895e6ef9c02aa3844ad67cd776ba9fbcfe84cf	modelling of parametrized processes via regression in the model space of neural networks		We consider the modelling of parametrized processes, where the goal is to model the process for new parameter value combinations. We compare the classical regression approach to a modular approach based on regression in the model space: First, for each process parametrization a model is learned. Second, a mapping from process parameters to model parameters is learned. We evaluate both approaches on two synthetic and two real-world data sets and show the advantages of the regression in the model space.	artificial neural network	Witali Aswolinskiy;René Felix Reinhart;Jochen J. Steil	2017	Neurocomputing	10.1016/j.neucom.2016.12.086	econometrics;proper linear model;single-index model;machine learning;mathematics;ball-and-stick model;statistics	AI	29.512713341561458	-28.48819040447891	64580
43d1978f8188a3329e1de5975a40cd0648362fda	fpga implementation of a novel algorithm for on-line bar breakage detection on induction motors	vibration analysis;preventive maintenance;life cycle;operant conditioning;bar breakage;induction motor;failure detection;spectrum;fpga;line detection;embedded system;fpga implementation;embedded systems;condition monitoring;signal processing	Preventive maintenance is one of the major concerns in modern industry where failure detection on motors increases the useful life cycle on the machinery. Bar breakage is one of the most common failures on motors and their condition monitoring is a mandatory task for industries. Previous works for bar breakage detection are based on off-line current or vibration analysis through the spectrum, but their detectability is compromised under certain operating conditions. The novelty of this work is the proposal of a correlation algorithm that combines current and vibration spectra to enhance detectability where other works fail. Current and vibration monitoring are non-invasive methods and they are preferred for on-line analysis. The proposed correlation between current and vibration allows to detect the broken bar condition on induction motors when no load is applied to the motor, which is the condition where other reported works fail to detect. The contribution of this work is that the proposed algorithm, though complex in computational load, is implemented into a low-cost FPGA (Spartan-3 XC3S1000) that gives a special purpose SOC solution for on-line operation, thanks to the development of a special purpose hardware signal processing unit. The developed FPGA based algorithm was tested for different bar breakage conditions in the induction motors and with several loads, giving as a result a better detectability for the motor failure under conditions where other methodologies fail, with the additional advantage of being a low-cost SOC solution for on-line detection	algorithm;computation;edge detection;field-programmable gate array;mathematical induction;online and offline;signal processing;vertical bar	Jose de Jesus Rangel-Magdaleno;René de Jesús Romero-Troncoso;Luis Miguel Contreras-Medina;Arturo Garcia-Perez	2008	2008 IEEE Instrumentation and Measurement Technology Conference	10.1145/1344671.1344725	embedded system;spectrum;biological life cycle;preventive maintenance;real-time computing;computer science;vibration;signal processing;operant conditioning;induction motor;field-programmable gate array	Robotics	37.320011986720324	-30.528688250706427	64630
02219e75a6aa9448e7f39e80ca9c1781c059122c	a direct approach for sparse quadratic discriminant analysis		Quadratic discriminant analysis (QDA) is a standard tool for classification due to its simplicity and flexibility. Because the number of its parameters scales quadratically with the number of the variables, QDA is not practical, however, when the dimensionality is relatively large. To address this, we propose a novel procedure named DA-QDA for QDA in analyzing high-dimensional data. Formulated in a simple and coherent framework, DAQDA aims to directly estimate the key quantities in the Bayes discriminant function including quadratic interactions and a linear index of the variables for classification. Under appropriate sparsity assumptions, we establish consistency results for estimating the interactions and the linear index, and further demonstrate that the misclassification rate of our procedure converges to the optimal Bayes risk, even when the dimensionality is exponentially high with respect to the sample size. An efficient algorithm based on the alternating direction method of multipliers (ADMM) is developed for finding interactions, which is much faster than its competitor in the literature. The promising performance of DA-QDA is illustrated via extensive simulation studies and the analysis of four real datasets.	algorithm;augmented lagrangian method;coherence (physics);interaction;linear discriminant analysis;multi language virtual machine;quadratic classifier;simulation;sparse matrix	Binyan Jiang;Xiangyu Wang;Chenlei Leng	2018	Journal of Machine Learning Research			ML	27.082798630784204	-34.11786904683116	64806
2d7bcd4a2f888b61f5a3527d4f7c83d2a32dd31e	dimensionality reduction by bayesian eigenvalue-analysis for state prediction in large sensor systems: with application in wind turbines.		The potential of the theory of random matrices are presented and evaluated as a statistical tool to represent the empirical correlations in a study of multivariate time series. A new sub space state prediction framework is proposed, consisting of the combination of a Bayesian state prediction algorithm and the eigenvalues of the empirical correlation matrix. In an industrial use-case of wind turbines, remarkable agreement between the theoretical prediction (based on the assumption that the correlation matrix is random) and empirical data, concerning the density of eigenvalues associated with the time series of different sensors, are found. Finally, the proposed framework outperforms the existing Bayesian state prediction algorithm and is computationally more feasible than feeding unprocessed data.	algorithm;bayesian network;dimensionality reduction;feature vector;information;quantum fluctuation;sensor;time series	Jürgen Herp;Esmaeil S. Nadimi	2018		10.1145/3264746.3264753	real-time computing;random matrix;eigenvalues and eigenvectors;statistical signal processing;covariance matrix;dimensionality reduction;multivariate statistics;algorithm;bayesian inference;computer science;bayesian probability	ML	32.33724259620914	-28.909447443881376	65122
3e642358544f9a7a88a5a5e67252c8444eb97081	a lean model for performance assessment of machinery using second generation wavelet packet transform and fisher criterion	decision support;second generation wavelet packet transforms;second generation wavelet;data processing;condition monitoring;feature extraction;signal processing;assessment methods;fisher criterion;feature selection;lean model;fuzzy c means clustering;mechanical systems;data transfer;performance assessment;expert system	The development of efficient on-line data processing and decision support algorithms is one of future trends of expert systems for machine condition monitoring research. This paper contributes to a lean model for machine performance assessment by combining an efficient signal processing algorithm, an effective feature selection criterion, and an intelligent assessment method. In the proposed model, firstly, a second generation wavelet packet transform is used to project raw signals into the wavelet domain; secondly, the Fisher criterion is applied to reduce redundant dimensions; eventually, a fuzzy c-means clustering method is used to assess and classify the performance of mechanical systems. The vibration signals from a rolling element bearing experiment has been used to verify both efficiency and effectiveness of the lean model. Compared with conventional methods, the lean model can reduce the time consumption of feature extraction by 49.7% and storage space or data transfer load related to the feature dimensionality by 97.7%, which indicates a great improvement in efficiency. 2009 Elsevier Ltd. All rights reserved.	algorithm;algorithmic efficiency;biorthogonal wavelet;cluster analysis;computation;decision support system;dimensionality reduction;expert system;feature extraction;feature selection;fisher–yates shuffle;lifting scheme;multilinear subspace learning;network packet;online and offline;packet analyzer;reliability engineering;second generation multiplex plus;second-generation wavelet transform;signal processing;wavelet packet decomposition	Yixiang Huang;Chengliang Liu;Xuan F. Zha;Yanming Li	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.11.038	speech recognition;data processing;feature extraction;computer science;artificial intelligence;machine learning;signal processing;data mining;mechanical system;feature selection;expert system	AI	36.772946338316984	-30.802356333939105	65494
70bc7af1dcfcc3f2bd38f4d38108e19da32480e9	the research of acoustic emission signal classification	short time analysis technology acoustic emission signal classification acoustic emission signal monitoring device ae signal monitoring device ae source type ae signal monitoring accuracy time varying characteristics acoustic emission signal transient analysis technique signal compression signal characteristic extraction neural network technology;acoustic emission rocks signal processing algorithms time frequency analysis monitoring educational institutions;data compression;neural nets;acoustic signal processing;transient analysis;neural network acoustic emission short time analysis fisher criterion;monitoring;fisher criterion;signal classification;acoustic emission;rocks;transient analysis acoustic signal processing data compression neural nets signal classification;signal processing algorithms;time frequency analysis;neural network;short time analysis	When using acoustic emission (AE) monitoring of rock burst, the signals received by AE monitoring device are related to the type of AE source type, which influences predicting the AE monitoring accuracy. In view of the time-varying characteristics of acoustic emission signals, we adopt the Short-Time analysis technology, in other words, to acoustic emission signal transient analysis technique to extract the signal characteristics of effective, then the fisher criteria for the number of signal compression. Using neural network technology for signal classification, the classification results showed that this method is particularly effective in terms of the Acoustic Emission signal.	acoustic cryptanalysis;acoustic fingerprint;acoustic model;neural network software;signal compression;transient state	Xiaojing Meng;Weidong Liu;Enjie Ding	2011	2011 Seventh International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIHMSP.2011.101	data compression;speech recognition;time–frequency analysis;computer science;acoustic emission;machine learning;artificial neural network	Robotics	37.512250609926895	-31.731886083990183	65530
5e37846556e9d581529c7e4d7f619b26ceb00fb6	statistical mechanical assessment of a reconstruction limit of compressed sensing: toward theoretical analysis of correlated signals	replica method;compressed sensing;statistical mechanics;cost function;random measure;time series;europhysics letters;autoregressive model;epl;theoretical analysis;signal reconstruction;numerical experiment;perfect reconstruction	We provide a scheme for exploring the reconstruction limits of compressed sensing by minimizing the general cost function under the random measurement constraints for generic correlated signal sources. Our scheme is based on the statistical mechanical replica method for dealing with random systems. As a simple but non-trivial example, we apply the scheme to a sparse autoregressive model, where the first differences in the input signals of the correlated time series are sparse, and evaluate the critical compression rate for a perfect reconstruction. The results are in good agreement with a numerical experiment for a signal reconstruction. Introduction. – Compressed sensing (CS) is a novel technique for data compression and has been drawing a lot of attention recently from the viewpoints of both theory and application. The key idea behind CS is to utilize the sparsity of the original input signals as the prior knowledge during the signal reconstruction stage, which can significantly reduce the number of signal measurements required for a perfect reconstruction. This setup is realistic because we often have to face situations where we have to handle sparse signals in the real world. A lot of effort has been paid and significant progress has been made in investigating the properties of CS [1–3]. After the pioneering works, contribution to CS problem from statistical mechanics analysis is now growing rapidly [4–10]. The measurement process of CS is summarized in the following linear equation:	autoregressive model;compressed sensing;cross-correlation;data compression;formal system;javaserver pages;linear equation;loss function;nos;numerical analysis;signal reconstruction;sparse matrix;time series;universality probability	Koujin Takeda;Yoshiyuki Kabashima	2010	CoRR	10.1209/0295-5075/95/18006	signal reconstruction;random measure;statistical mechanics;time series;autoregressive model;compressed sensing	ML	31.841938122760908	-33.10093909646187	65687
c6cdbc5f6c42c547a49216624d0ea8923ed59ddb	lazy probability propagation on gaussian bayesian networks	silicon;directed graphs;belief networks;bayesian network;junction tree algorithms gaussian bayesian networks lazy propagation arc reversal;gaussian distribution lazy probability propagation gaussian bayesian network lazy lauritzen spiegelhalter algorithm lazy hugin algorithm lazy shafer shenoy algorithm junction tree algorithm directed graph message passing;gaussian bayesian network;lazy lauritzen spiegelhalter algorithm;lazy probability propagation;junctions inference algorithms message passing silicon probabilistic logic algorithm design and analysis particle separators;trees mathematics;junctions;junction tree algorithms;gaussian bayesian networks;directed graph;lazy propagation;message passing;junction tree algorithm;particle separators;inference algorithms;probabilistic logic;lazy shafer shenoy algorithm;lazy hugin algorithm;algorithm design and analysis;gaussian distribution;arc reversal;trees mathematics belief networks directed graphs gaussian distribution message passing	Novel lazy Lauritzen-Spiegelhalter (LS), lazy Hugin and lazy Shafer-Shenoy (SS) algorithms are devised for Gaussian Bayesian networks (BNs). In the lazy algorithms, the clique potentials and separator potentials are kept in combinable decomposed form instead of combined to be a single valuation in conventional junction tree algorithms. By employing decomposed form potentials, the independence relations between variables are explored online and the directed graph information is utilized in the message calculations. In the proposed algorithms, a consistent junction tree with the evidence entered can be obtained by a single round of message passing. The moments form parametrization of Gaussian distributions allows the deterministic relationships between variables. Preliminary analysis shows that the lazy LS algorithm and the lazy Hugin algorithm are more computationally efficient than the lazy SS algorithm, especially when there are multiple items of evidence to be incorporated.	algorithm;algorithmic efficiency;bayesian network;directed graph;hugin;lazy evaluation;least squares;message passing;schema (genetic algorithms);software propagation;tree decomposition;value (ethics)	Hua Mu;Meiping Wu;Hongxu Ma;Tim Bailey	2010	2010 22nd IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2010.51	directed graph;computer science;theoretical computer science;machine learning;algorithm	Robotics	24.944507293921106	-27.56616025057252	65976
1b5475cfbec29c57d40932a765fa0dcf874cb519	generalized analysis of a distribution separation method	information retrieval;distribution separation;kl divergence;mixture model	Separating two probability distributions from a mixture model that is made up of the combinations of the two is essential to a wide range of applications. For example, in information retrieval (IR), there often exists a mixture distribution consisting of a relevance distribution that we need to estimate and an irrelevance distribution that we hope to get rid of. Recently, a distribution separation method (DSM) was proposed to approximate the relevance distribution, by separating a seed irrelevance distribution from the mixture distribution. It was successfully applied to an IR task, namely pseudo-relevance feedback (PRF), where the query expansion model is often a mixture term distribution. Although initially developed in the context of IR, DSM is indeed a general mathematical formulation for probability distribution separation. Thus, it is important to further generalize its basic analysis and to explore its connections to other related methods. In this article, we first extend DSM’s theoretical analysis, which was originally based on the Pearson correlation coefficient, to entropy-related measures, including the KL-divergence (Kullback–Leibler divergence), the symmetrized KL-divergence and the JS-divergence (Jensen–Shannon divergence). Second, we investigate the distribution separation idea in a well-known method, namely the mixture model feedback (MMF) approach. We prove that MMF also complies with the linear combination assumption, and then, DSM’s linear separation algorithm can largely simplify the EM algorithm in MMF. These theoretical analyses, as well as further empirical evaluation results demonstrate the advantages of our DSM approach.	approximation algorithm;coefficient;expectation–maximization algorithm;incremental funding methodology;information retrieval;jensen's inequality;kullback–leibler divergence;mixture model;primitive recursive function;query expansion;relevance feedback;shannon (unit)	Peng Zhang;Qian Yu;Yuexian Hou;Dawei Song;Jingfei Li;Bin Hu	2016	Entropy	10.3390/e18040105	econometrics;categorical distribution;machine learning;mixture model;mathematics;kullback–leibler divergence;statistics;three-point estimation	ML	28.966568355603712	-29.1268295569829	66068
29fbb767e22d36757fedb5e730d1496d94697df6	weak conditions for shrinking multivariate nonparametric density estimators	parametric model;nonparametric estimation;62g20;62g07;shrinkage;integrated square error;kolmogorov asymptotics	Nonparametric density estimators on R may fail to be consistent when the sample size n does not grow fast enough relative to reduction in smoothing. For example a Gaussian kernel estimator with bandwidths proportional to some sequence hn is not consistent if nh K n fails to diverge to in nity. The paper studies shrinkage estimators in this scenario and shows that we can still meaningfully use in a sense to be speci ed in the paper a nonparametric density estimator in high dimensions, even when it is not asymptotically consistent. Due to the curse of dimensionality , this framework is quite relevant to many practical problems. In this context, unlike other studies, the reason to shrink towards a possibly misspeci ed low dimensional parametric estimator is not to improve on the bias, but to reduce the estimation error.	curse of dimensionality;smoothing	Alessio Sancetta	2013	J. Multivariate Analysis	10.1016/j.jmva.2012.09.009	econometrics;mathematical optimization;parametric model;multivariate kernel density estimation;mathematics;shrinkage;statistics	ML	28.895463120718386	-26.395680286491746	66084
9ea70e18c1db30c85179963fa5a1688e12fec4b0	analyzing and learning sparse and scale-free networks using gaussian graphical models		In this paper, we consider the problem of fitting a sparse precision matrix to multivariate Gaussian data. The zero elements in the precision matrix correspond to conditional independencies between variables. We focus on the estimation of a class of sparse precision matrix which represents the scale-free networks. It has been demonstrated that some of the important networks display features similar to scale-free graphs. We propose a new log-likelihood formulation, which promotes the sparseness of the precision matrix as well as the topological structure of scale-free networks. To optimize this new energy formulation, the alternating direction method of multipliers form is used with the general $$L_1$$ L 1 -regularized loss optimization. We tested our proposed method on various databases. Our proposed method exhibits better estimation performance with various number of samples, N, and different selection of sparsity parameter, $$\rho $$ ρ .	augmented lagrangian method;database;experiment;graphical model;graphical user interface;interaction;mathematical optimization;neural coding;sparse matrix;synthetic intelligence;usb hub	Melih S. Aslan;Xue-wen Chen;Hong Cheng	2016	International Journal of Data Science and Analytics	10.1007/s41060-016-0009-y	machine learning;pattern recognition;mathematics;statistics	ML	28.168134898397685	-32.8730189046415	66274
12cd8dc6b19766dd736502416d67293bca801447	partial logistic artificial neural network with automatic relevance determination and markov chain monte carlo methods applied in medical survival studies	artificial neural networks;logistics;markov processes;sampling methods;monte carlo methods;data models	This paper builds on previous work involving different Bayesian Neural Networks namely Partial Logistic Artificial Neural Network with Automatic Relevance Determination (PLANN-ARD) for Single Risk (SR) and Competing Risk (CR) [1, 6, 15, 16] and applied in the medical survival studies. The results obtained with these PLANN-ARD/PLANN-CR-ARD models are compared with the results obtained with a different set of Bayesian Neural Networks namely the Markov Chain Monte Carlo (MCMC) methods [19, 20]. This work is done in the recent context of evaluation of large number of classification models [25] on medical dataset(s) and more specifically in the context of evaluation of different types of Bayesian Neural Networks [23, 26-29] in the medical survival analysis domain. There is such an interest in studying various classification and regression models for outcome prediction in medical survival analysis. Two medical datasets are used herein: a node negative breast cancer dataset (SR analysis) and a Primary Biliary Cirrhosis (PBC) dataset (CR analysis). The PLANN-ARD/PLANN-CR-ARD models form a group of two neural network models which are based on gradient type of optimization algorithms for the calculus of the neural network parameters. The MCMC sampling methods represent another set of models which are used in this paper for SR study (MCMC-SR algorithm) and CR study (MCMC-CR algorithm) in medical survival domain. The MCMC sampling methods are implemented by a MCMC toolbox available in the literature [10, 11, 19-21]. The MCMC methods are sampling from the prior probability distributions of the model parameters and they include the Gibbs sampler, the Metropolis-Hastings sampler, the Hybrid Markov Chain Monte Carlo (HMCMC) sampler or the Reversible Jump Markov Chain Monte Carlo (RJMCMC) sampler. The results obtained by the four BNNs are compared with the non-parametric estimates obtained through the survival study of the two medical datasets from above. The results show a superiority of the PLANN-ARD/PLANN-CR-ARD models with regard to the MCMC-SR/MCMC-CR algorithms from the point of view of the model selection task which was less computational expensive for the PLANN-ARD/PLANN-CR-ARD models.	artificial neural network;benchmark (computing);computation;gibbs sampling;gradient;kaplan–meier estimator;machine learning;mathematical optimization;metropolis;metropolis–hastings algorithm;model selection;neural networks;numerical analysis;periodic boundary conditions;prognostic variable;real life;relevance;reversible-jump markov chain monte carlo;sampling (signal processing);statistical classification	Corneliu T. C. Arsene	2016	2016 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)	10.1109/CIBCB.2016.7758122	logistics;data modeling;sampling;econometrics;variable-order bayesian network;hybrid monte carlo;markov chain monte carlo;computer science;machine learning;markov process;artificial neural network;statistics;monte carlo method	ML	30.174826960335842	-26.494800503662155	66309
4b11a52c5e5bed78c5fbe6215c52e40a71f29eb3	a new minimum contrast approach for inference in single-index models	single index assumption;62f12;u;62j02;62g08;62g20;semiparametric regression;conditional law;kernel smoothing	Semiparametric single-index models represent an appealing compromise between parametric and nonparametric approaches and have been widely investigated in the literature. The underlying assumption in single-index models is that the information carried by the vector of covariates could be summarized by a one-dimensional projection. We propose a new, general inference approach for such models, based on a quadratic form criterion involving kernel smoothing. The approach could be applied with general single-index assumptions, in particular for mean regression models and conditional law models. The covariates could be unbounded and no trimming is necessary. A resampling method for building confidence intervals for the index parameter is proposed. Our empirical experiments reveal that the new method performs well in practice.	single-index model	Weiyu Li;Valentin Patilea	2017	J. Multivariate Analysis	10.1016/j.jmva.2017.03.009	econometrics;mathematics;haplogroup u;nonparametric regression;semiparametric model;statistics;semiparametric regression;kernel smoother	NLP	28.960358253777414	-24.479824598415966	66395
14cbe72236eaeebe75fc376e3f781981293d8cdc	optimization with em and expectation-conjugate-gradient	ucl;discovery;theses;conference proceedings;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;ucl research	We show a close relationship between the Expectation Maximization (EM) algorithm and direct optimization algorithms such as gradientbased methods for parameter learning. We identify analytic conditions under which EM exhibits Newton-like behavior, and conditions under which it possesses poor, first-order convergence. Based on this analysis, we propose two novel algorithms for maximum likelihood estimation of latent variable models, and report empirical results showing that, as predicted by theory, the proposed new algorithms can substantially outperform standard EM in terms of speed of convergence in certain cases.	conjugate gradient method;expectation–maximization algorithm;first-order predicate;latent variable;mathematical optimization;monte carlo method;newton;rate of convergence	Ruslan Salakhutdinov;Sam T. Roweis;Zoubin Ghahramani	2003			library science;computer science;media studies;engineering physics	ML	26.730559502925793	-30.893154483727375	66795
1f3799094e69705e11815482b85cf48c17f0a377	efficient variant of algorithm fastica for independent component analysis attaining the cram&#201;r-rao lower bound	algorithms artificial intelligence computer simulation computing methodologies data interpretation statistical models statistical pattern recognition automated principal component analysis signal processing computer assisted;asymptotic efficiency;borne erreur;separacion ciega;algorithm analysis;blind deconvolution;complexite calcul;blind source separation;r 8211;source separation gaussian distribution independent component analysis speech signal separation fastica independent component analysis cramer rao lower bound finite data samples generalized gaussian distributions bimodal distribution independent component analysis ica algorithm fastica blind deconvolution blind source separation cram 201;cramer rao lower bound crb;desconvolucion ciega;independent component analysis;indexing terms;blind separation;complejidad computacion;independent component analysis ica algorithm fastica blind deconvolution blind source separation cramer rao lower bound crb;borne inferieure cramer rao;rao lower bound crb;independent component analysis ica;computational complexity;probability distribution;algorithm fastica;separacion senal;independent component analysis signal processing algorithms signal processing information theory automation probability distribution computational complexity computational modeling speech deconvolution;cram er rao lower bound;speech signal separation fastica independent component analysis cramer rao lower bound finite data samples generalized gaussian distributions bimodal distribution;deconvolution aveugle;analyse composante independante;generalized gaussian distribution;separation aveugle;source separation gaussian distribution independent component analysis;analyse algorithme;separation source;error bound;reseau neuronal;analisis componente independiente;source separation;red neuronal;gaussian distribution;analisis algoritmo;lower bound;neural network;limite error	FastICA is one of the most popular algorithms for independent component analysis (ICA), demixing a set of statistically independent sources that have been mixed linearly. A key question is how accurate the method is for finite data samples. We propose an improved version of the FastICA algorithm which is asymptotically efficient, i.e., its accuracy given by the residual error variance attains the Cramer-Rao lower bound (CRB). The error is thus as small as possible. This result is rigorously proven under the assumption that the probability distribution of the independent signal components belongs to the class of generalized Gaussian (GG) distributions with parameter alpha, denoted GG(alpha) for alpha>2. We name the algorithm efficient FastICA (EFICA). Computational complexity of a Matlab implementation of the algorithm is shown to be only slightly (about three times) higher than that of the standard symmetric FastICA. Simulations corroborate these claims and show superior performance of the algorithm compared with algorithm JADE of Cardoso and Souloumiac and nonparametric ICA of Boscolo on separating sources with distribution GG(alpha) with arbitrary alpha, as well as on sources with bimodal distribution, and a good performance in separating linearly mixed speech signals	algorithm;computational complexity theory;computer simulation;epilepsy, generalized;fastica;gadu-gadu;independent computing architecture;independent component analysis;jade;matlab;normal statistical distribution;population parameter;sample variance	Zbynek Koldovský;Petr Tichavský;Erkki Oja	2006	IEEE Transactions on Neural Networks	10.1109/TNN.2006.875991	normal distribution;probability distribution;independent component analysis;fastica;speech recognition;index term;computer science;machine learning;pattern recognition;mathematics;blind signal separation;generalized normal distribution;blind deconvolution;upper and lower bounds;computational complexity theory;artificial neural network;statistics	ML	32.99574372335917	-32.08109305913085	66905
06d9c08bff6066f754db8f424fc97281e6a9ee20	bayesian inference on principal component analysis using reversible jump markov chain monte carlo	model selection;mcmc methods;bayesian inference;simulation experiment;reversible jump;markov chain monte carlo;principal component analysis;reversible jump markov chain monte carlo;principal component	Based on the probabilistic reformulation of principal component analysis (PCA), we consider the problem of determining the number of principal components as a model selection problem. We present a hierarchical model for probabilistic PCA and construct a Bayesian inference method for this model using reversible jump Markov chain Monte Carlo (MCMC). By regarding each principal component as a point in a one-dimensional space and employing only birthdeath moves in our reversible jump methodology, our proposed method is simple and capable of automatically determining the number of principal components and estimating the parameters simultaneously under the same disciplined framework. Simulation experiments are performed to demonstrate the effectiveness of our MCMC method. Introduction Principal component analysis (PCA) is a powerful tool for data analysis. It has been widely used for such tasks as dimensionality reduction, data compression and visualization. The original derivation of PCA is based on a standardized linear projection that maximizes the variance in the projected space. Recently, Tipping & Bishop (1999) proposed the probabilistic PCA which explores the relationship between PCA and factor analysis of generative latent variable models. This opens the door to various Bayesian treatments of PCA. In particular, Bayesian inference can now be employed to solve the central problem of determining the number of principal components that should be retained. Bishop (1999a; 1999b) addressed this by using automatic relevance determination (ARD) (Neal 1996) and Bayesian variational methods. Minka (2001), on the other hand, adopted a Bayesian method which is based on the Laplace approximation. In this paper, we propose a hierarchical model for Bayesian inference on PCA using the novel reversible jump Markov chain Monte Carlo (MCMC) algorithm of Green (1995). In brief, reversible jump MCMC is a random-sweep Metropolis-Hastings method for varying-dimension probCopyright c © 2004, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. lems. It constructs a dimension matching transform using the reversible jump methodology and estimates the parameters using Gibbs sampling. Richardson & Green (1997), by developing the split-merge and birth-death moves for the reversible jump methodology, performed a fully Bayesian analysis on univariate data generated from a finite Gaussian mixture (GM) with an unknown number of components. This was then further extended to univariate hidden Markov models (HMM) by Robert, Rydén, & Titterington (2000). In general, reversible jump MCMC is attractive in that it can perform parameter estimation and model selection simultaneously within the same framework. In contrast, the other methods mentioned above can only perform model selection separately. In recent years, reversible jump MCMC has also been successfully applied to neural networks (Holmes & Mallick 1998; Andrieu, Djurié, & Doucet 2001) and pattern recognition (Roberts, Holmes, & Denison 2001). Motivated by these successes, in this paper, we introduce reversible jump MCMC into the probabilistic PCA framework. This provides a disciplined method to perform parameter estimation simultaneously with choosing the number of principal components. In particular, we propose a hierarchical model for probabilistic PCA, together with a Bayesian inference procedure for this model using reversible jump MCMC. Note that PCA is considerably simpler than GMs and HMMs in the following ways. First, PCA has much fewer free parameters than GMs and HMMs. Second, unlike GMs and HMMs, no component in PCA can be empty. Third, using reversible jump MCMC in GMs and HMMs for high-dimensional data is still an open problem, while reversible jump MCMC for PCA is more manageable because, as to be discussed in more detail in later sections, each principal component can be regarded as a point in some onedimensional space. Because of these, we will only employ birth-death moves for the dimension matching transform in our reversible jump methodology. The rest of this paper is organized as follows. In the next section, we give a brief overview of probabilistic PCA and the corresponding maximum likelihood estimation problem. A hierarchical Bayesian model and the corresponding reversible jump MCMC procedure are then presented, followed by some experimental results on different data sets. The last section gives some concluding remarks. Probabilistic PCA Probabilistic PCA was proposed by Tipping & Bishop (1999). In this model, a high-dimensional random vector x is expressed as a linear combination of basis vectors (hj’s) plus noise ( ):	approximation;artificial intelligence;artificial neural network;basis (linear algebra);bayesian network;calculus of variations;data compression;dimensionality reduction;estimation theory;experiment;factor analysis;gibbs sampling;hidden markov model;hierarchical database model;latent variable;metropolis;metropolis–hastings algorithm;model selection;monte carlo method;pattern recognition;principal component analysis;probabilistic turing machine;relevance;reversible-jump markov chain monte carlo;richardson number;sampling (signal processing);selection algorithm;simulation;the moon is a harsh mistress	Zhihua Zhang;Kap Luk Chan;James T. Kwok;Dit-Yan Yeung	2004			hybrid monte carlo;markov chain monte carlo;pattern recognition;markov chain mixing time;statistics;monte carlo method;principal component analysis	AI	29.91662058129677	-28.509361262792744	66979
266b0b6adf7e0e65aedfa0e623bf4e43fd6755de	classification of time-frequency representations based on two-direction 2dlda for gear fault diagnosis	time frequency representation tfr;information hiding;signal analysis;time frequency;gear;linear discriminate analysis;short time fourier transform;discriminant analysis;feature vector;wavelet transform;two direction two dimensional line discriminant analysis td 2dlda;time frequency representation;feature extraction;s transform;automatic classification;time frequency analysis;fault diagnosis	Time–frequency representations (TFR) have been intensively employed for analyzing vibration signals in mechanical faults diagnosis. However, in many applications, time–frequency representations are simply utilized as a visual aid to be used for vibration signal analysis. It is very attractive to investigate the utility of TFR for automatic classification of vibration signals. A key step for this work is to extract discriminative parameters from TFR as input feature vector for classifiers. This paper contributes to this ongoing investigation by developing a two direction two dimensional linear discriminative analysis (TD-2DLDA) technique for feature extraction from TFR. The S transform, which combines the separate strengths of the short time Fourier transform and wavelet transforms, is chosen to perform the time–frequency analysis of vibration signals. Then, a novel feature extraction technique, named TD-2DLDA, is proposed to represent the time–frequency matrix. As opposed to traditional LDA, TD-2DLDA is directly conduct on 2D transform wo-direction two-dimensional line iscriminant analysis (TD-2DLDA) matrices rather than 1D vectors, so the time–frequency matrix does not need to be transformed into a vector prior to feature extraction. Therefore, the TD-2DLDA can reduce the computation cost and preserve more structure information hiding in original 2D matrices compared to the LDA. The promise of our method is illustrated by performing our procedure on vibration signals measured from a gearbox with five operating states. Experimental results indicate that the TD-2DLDA obviously outperforms related feature extraction schemes such as LDA, 2DLDA in gear fault diagnosis.	computation;discriminative model;feature extraction;feature vector;frequency analysis;s transform;signal processing;time–frequency analysis;time–frequency representation;wavelet transform	Bing Li;Peilin Zhang;Dongsheng Liu;Shuang-shan Mi;Peng-yuan Liu	2011	Appl. Soft Comput.	10.1016/j.asoc.2011.03.030	speech recognition;time–frequency analysis;computer science;machine learning;signal processing;pattern recognition;linear discriminant analysis	AI	37.323417934584874	-31.17818463780396	67384
25c8f86f245e279f7111ea57f6c929f07c077bc2	detecting manifold dependences of multivariate data with total correlation			total correlation	Yujian Li;Yahong Zhang	2018	Intell. Data Anal.	10.3233/IDA-163324	manifold;artificial intelligence;computer science;pattern recognition;total correlation;multivariate statistics	AI	30.610737424710326	-35.98443065851282	67566
c116f19a2e63f5fb188aaad05640d251fa7fac64	fault diagnosis method of ningxia photovoltaic inverter based on wavelet neural network		Accurate fault diagnosis is the premise to ensure the safe and reliable operation of photovoltaic three-level inverter. A fault diagnosis method based on wavelet neural network is researched in the paper. First of all, the topology and the fault characteristics of three-level inverter are analyzed, the fault features are analyzed for three-level inverter when single and double IGBTs fault, the eigenvectors of phase voltage, the upper bridge arm and the lower bridge arm voltage are extracted by three-layer Wavelet Package Transform, the BP neural network is designed for training data and testing. The simulation model is built by Matlab/Simulink, the simulation results show that the method can accurately diagnose for various fault circumstances.	artificial neural network;power inverter;solar inverter;wavelet	Guohua Yang;Pengzhen Wang;Bingxuan Li;Bo Lei;Hao Tang;Rui Li	2017		10.1007/978-981-10-6364-0_18	wavelet transform;wavelet;voltage;artificial neural network;eigenvalues and eigenvectors;inverter;control engineering;photovoltaic system;space vector modulation;electronic engineering;computer science	Robotics	36.59532193718938	-30.64306714847894	67730
c04db24e39fb152e0903839d119c735afc3c4c31	geometric sampling: an approach to uncertainty in high dimensional spaces	inverse problems;uncertainty.;high dimensional spa- ces;geometric sampling	Uncertainty is always present in inverse problems. The main reasons for that are noise in data and measurement error, solution nonuniqueness, data coverage and bandwidth limitations, physical assumptions and numerical approximations. In the context of nonlinear inversion, the uncertainty problem is that of quantifying the variability in the model space supported by prior information and the observed data. In this paper we outline a general nonlinear inverse uncertainty estimation method that allows for the comprehensive search of model posterior space while maintaining computational efficiencies similar to deterministic inversions. Integral to this method is the combination of model reduction techniques, a constrained mapping approach and a sparse sampling scheme. This approach allows for uncertainty quantification in inverse problems in high dimensional spaces and very costly forward evaluations. We show some results in non linear geophysical inversion (electromagnetic data).	boson sampling;converge;nonlinear system;numerical analysis;sampling (signal processing);spaces;sparse approximation;sparse matrix;spatial variability;uncertainty quantification	Juan Luis Fernández Martínez;Michael Tompkins;Tapan Mukerji;David Alumbaugh	2010		10.1007/978-3-642-14746-3_31	econometrics;mathematical optimization;mathematics;sensitivity analysis;statistics	Robotics	37.627317519567804	-24.64913705758757	68203
0b49df91449fcead05d6bb6099f508ad553f7b4d	differentially private variational inference for non-conjugate models		Many machine learning applications are based on data collected from people, such as their tastes and behaviour as well as biological traits and genetic data. Regardless of how important the application might be, one has to make sure individuals’ identities or the privacy of the data are not compromised in the analysis. Differential privacy constitutes a powerful framework that prevents breaching of data subject privacy from the output of a computation. Differentially private versions of many important Bayesian inference methods have been proposed, but there is a lack of an efficient unified approach applicable to arbitrary models. In this contribution, we propose a differentially private variational inference method with a very wide applicability. It is built on top of doubly stochastic variational inference, a recent advance which provides a variational solution to a large class of models. We add differential privacy into doubly stochastic variational inference by clipping and perturbing the gradients. The algorithm is made more efficient through privacy amplification from subsampling. We demonstrate the method can reach an accuracy close to nonprivate level under reasonably strong privacy guarantees, clearly improving over previous sampling-based alternatives especially in the strong privacy regime. ∗AH is also with the Department of Mathematics and Statistics and Department of Public Health, University of Helsinki.	academy;algorithm;approximation;bayesian approaches to brain function;calculus of variations;chroma subsampling;computation;differential privacy;doubly stochastic model;gradient;leftover hash lemma;machine learning;sampling (signal processing);time complexity;variational principle	Joonas Jälkö;Antti Honkela;Onur Dikmen	2017	CoRR		computer science;theoretical computer science;machine learning;data mining;mathematics;statistics	ML	27.305761287821664	-28.839159152984482	68297
6912297a9f404ff6dd23cb3bb3ff5c4981f68162	provable inductive matrix completion		Consider a movie recommendation system where apart from the ratings information, side information such as user’s age or movie’s genre is also available. Unlike standard matrix completion, in this setting one should be able to predict inductively on new users/movies. In this paper, we study the problem of inductive matrix completion in the exact recovery setting. That is, we assume that the ratings matrix is generated by applying feature vectors to a low-rank matrix and the goal is to recover back the underlying matrix. Furthermore, we generalize the problem to that of low-rank matrix estimation using rank-1 measurements. We study this generic problem and provide conditions that the set of measurements should satisfy so that the alternating minimization method (which otherwise is a non-convex method with no convergence guarantees) is able to recover back the exact underlying low-rank matrix. In addition to inductive matrix completion, we show that two other low-rank estimation problems can be studied in our framework: a) general low-rank matrix sensing using rank-1 measurements, and b) multi-label regression with missing labels. For both the problems, we provide novel and interesting bounds on the number of measurements required by alternating minimization to provably converges to the exact low-rank matrix. In particular, our analysis for the general low rank matrix sensing problem significantly improves the required storage and computational cost than that required by the RIP-based matrix sensing methods [1]. Finally, we provide empirical validation of our approach and demonstrate that alternating minimization is able to recover the true matrix for the above mentioned problems using a small number of measurements.	computation;computational complexity theory;emoticon;feature vector;inductive reasoning;multi-label classification;provable security;recommender system	Prateek Jain;Inderjit S. Dhillon	2013	CoRR		mathematical optimization;sparse matrix;machine learning;mathematics;state-transition matrix;algorithm;statistics	ML	27.34928543972576	-35.98625192986983	68750
90c7028daa3e4779e097a1dcfb55ebd5d1124209	double markov process blind estimation: application to communication in a long memory channel	blind estimation;double markov process;long memory channel;fisher information	Also called a Markov switching process, a Double Markov Process (DMP) is an extension of the Hidden Markov Chain (HMC) where the observed process conditionally to the hidden one is modelled by a Markov process. This paper focuses on the estimation of a DMP model, the goal being twofold. First we provide the Cramer–Rao bound of the covariance matrix of any unbiased estimator in order to assess the accuracy limitations. Then we develop a DMP blind estimator for communication through a long memory channel. The results show the benefit of such a modelling in terms of both performance and complexity.		Noura Dridi;Yves Delignon;Wadih Sawaya;Christelle Garnier	2014	Digital Signal Processing	10.1016/j.dsp.2014.02.016	econometrics;markov chain;maximum-entropy markov model;markov kernel;partially observable markov decision process;markov property;continuous-time markov chain;fisher information;machine learning;causal markov condition;hidden semi-markov model;markov blanket;mathematics;additive markov chain;markov process;markov model;hidden markov model;statistics;variable-order markov model	EDA	31.80050358446743	-26.786699260165516	69080
8ddcb3ea4b2291640aae8bde5895f73228ad4034	tactile sensor and algorithm to detect slip in robot grasping processes	robot sensing systems;humanoid robot;discrete short time fourier transform;robot hand;tactile sensor;spectrogram;pvdf sensor;silicone rubber surface;anthropomorphic robot hand;humanoid robot tactile sensor slip detection algorithm;data mining;short time fourier transform;tactile sensors discrete fourier transforms end effectors principal component analysis signal processing;k nearest neighbour classifier;signal processing;principal component analysis;tactile slip sensor;fourier transforms;robots;slip detection;detection algorithm;tactile sensors robot sensing systems signal processing signal processing algorithms grasping anthropomorphism circuits rubber fourier transforms spectrogram;tactile sensors;signal processing slip detection robot grasping process tactile slip sensor anthropomorphic robot hand measurement circuit silicone rubber surface pvdf sensor discrete short time fourier transform spectrogram principal component analysis k nearest neighbour classifier;k nearest neighbour;signal processing algorithms;discrete fourier transforms;robot grasping process;slip detection algorithm;end effectors;measurement circuit	In this paper we introduce a tactile slip sensor for an anthropomorphic robot hand, the measurement circuit and the corresponding algorithm to determine slip states. The main slip sensor components consist of a silicone rubber surface which covers a PVDF sensor. After the amplification of the signal it is processed by a discrete short-time Fourier transform. The resulting spectrogram is processed by a principal component analysis to determine the main signal components. For a classification of three states (‘slip’, ‘signal but no slip’ and ‘noise’) a k-nearest neighbour classifier has been trained with the main signal components and serves for discrimination of slip states on the sensor's surface. The build-up of the sensor and the experimental setup will be briefly explained, the signal processing and the results will be discussed in detail.	distortion;frequency analysis;k-nearest neighbors algorithm;lazy learning;online and offline;principal component analysis;robot;slip (programming language);short-time fourier transform;signal processing;spectrogram;tactile sensor;test set	Dirk Göger;Nico Ecker;Heinz Wörn	2008	2008 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2009.4913219	computer vision;electronic engineering;speech recognition;computer science;engineering;artificial intelligence;signal processing;tactile sensor	Robotics	37.75477450208493	-33.13598611405774	69121
553ddbdce7332378490e2c04b74dff6217909150	approximating bayesian confidence regions in convex inverse problems	high dimensional image restoration problem;bayesian high posterior density region approximation;proximal markov chain monte carlo;bayes methods uncertainty inverse problems signal processing estimation optimization approximation error;uncertainty quantification;statistical signal processing methods;inverse problems approximation theory bayes methods convex programming image restoration;approximation error;convex programming;uncertainty;bayes methods;bayesian inference;image restoration;approximation theory;convex optimisation;estimation;signal processing;standard convex optimisation techniques;impressive point estimation;optimization;bayesian confidence region approximation;proximal markov chain monte carlo convex inverse problems statistical signal processing methods impressive point estimation bayesian high posterior density region approximation bayesian confidence region approximation standard convex optimisation techniques high dimensional image restoration problem approximation error;convex inverse problems;convex optimisation inverse problems bayesian inference uncertainty quantification;inverse problems	Solutions to inverse problems that are ill-conditioned or ill-posed have significant intrinsic uncertainty. Unfortunately, analysing and quantifying this uncertainty is very challenging, particularly in high-dimensional settings. As a result, while most modern statistical signal processing methods achieve impressive point estimation results, they are generally unable to quantify the uncertainty in the solutions delivered. This work presents a new general methodology for approximating Bayesian high-posterior-density (confidence or credibility) regions in inverse problems that are convex and potentially very high-dimensional. A remarkable property of the approximations is that they can be computed very efficiently, even in large-scale problems, by using standard convex optimisation techniques. The proposed methodology is demonstrated on a high-dimensional image restoration problem, where the approximation error is assessed by using proximal Markov chain Monte Carlo as benchmark.	approximation error;benchmark (computing);circuit restoration;condition number;convex optimization;image restoration;markov chain monte carlo;mathematical optimization;monte carlo method;statistical signal processing;well-posed problem	Marcelo Pereyra	2016	2016 IEEE Statistical Signal Processing Workshop (SSP)	10.1109/SSP.2016.7551771	image restoration;econometrics;mathematical optimization;approximation error;estimation;uncertainty quantification;uncertainty;inverse problem;signal processing;mathematics;bayesian inference;statistics;approximation theory	ML	26.975966119279946	-33.45187610923483	69147
ecc6a8ad9d0444b0945ad95a140f10a17f33303e	non-parametric stochastic subset optimization utilizing multivariate boundary kernels and adaptive stochastic sampling		The implementation of NP-SSO (non-parametric stochastic subset optimization) to general design under uncertainty problems and its enhancement through various soft computing techniques is discussed. NP-SSO relies on iterative simulation of samples of the design variables from an auxiliary probability density, and approximates the objective function through kernel density estimation (KDE) using these samples. To deal with boundary correction in complex domains, a multivariate boundary KDE based on local linear estimation is adopted in this work. Also, a non-parametric characterization of the search space at each iteration using a framework based on support vector machine is formulated. To further improve computational efficiency, an adaptive kernel sampling density formulation is integrated and an adaptive, iterative selection of the number of samples needed for the KDE implementation is established. © 2015 Civil-Comp Ltd. and Elsevier Ltd. All rights reserved.	complex adaptive system;iteration;kernel (operating system);kernel density estimation;loss function;mathematical optimization;optimization problem;sampling (signal processing);simulation;soft computing;support vector machine	Gaofeng Jia;Alexandros A. Taflanidis	2015	Advances in Engineering Software	10.1016/j.advengsoft.2015.06.014	mathematical optimization;machine learning;mathematics;variable kernel density estimation;statistics	AI	29.502737460205232	-27.31502159216773	69815
69df16dd19a03c24d6b692c29d92944727b1a38f	analysis of massive emigration from poland: the model-based clustering approach	cluster algorithm;model based approach;statistical model;multivariate normal distribution;clustering method;probability distribution;number of clusters;finite mixture model;model based clustering;finite mixture	The model-based approach assumes that data is generated by a finite mixture of probability distributions such as multivariate normal distributions. In finite mixture models, each component of probability distribution corresponds to a cluster. The problem of determining the number of clusters and choosing an appropriate clustering method becomes the problem of statistical model choice. Hence, the model-based approach provides a key advantage over heuristic clustering algorithms, because it selects both the correct model and the number of clusters.	computer cluster	Ewa Witek	2008		10.1007/978-3-642-01044-6_57	correlation clustering;econometrics;determining the number of clusters in a data set;k-medians clustering;flame clustering;consensus clustering;pattern recognition;cure data clustering algorithm;mixture model;mathematics;cluster analysis;single-linkage clustering;statistics	ML	30.33666197686256	-30.467149475903295	69821
94f10095a58c135f275879c02535843628439924	a novel empirical bayes with reversible jump markov chain in user-movie recommendation system		In this article we select the unknown dimension of the feature by reversible jump MCMC inside a simulated annealing in bayesian set up of collaborative filter. We implement the same in MovieLens small dataset. We also tune the hyper parameter by using a modified empirical bayes. It can also be used to guess an initial choice for hyper-parameters in grid search procedure even for the datasets where MCMC oscillates around the true value or takes long time to converge.	algorithm;collaborative filtering;converge;movielens;recommender system;reversible-jump markov chain monte carlo;simulated annealing;transformation matrix	Arabin Kumar Dey;Himanshu Jhamb	2018	CoRR		artificial intelligence;machine learning;collaborative filtering;recommender system;mathematics;simulated annealing;hyperparameter optimization;movielens;markov chain monte carlo;bayes' theorem;markov chain	ML	26.737983841797345	-31.442496394949874	69959
a753937c09de1cdda122a2aa25ff3d0aaa95f950	canonical variate dissimilarity analysis for process incipient fault detection		Early detection of incipient faults in industrial processes is increasingly becoming important, as these faults can slowly develop into serious abnormal events, an emergency situation, or even failure of critical equipment. Multivariate statistical process monitoring methods are currently established for abrupt fault detection. Among these, the canonical variate analysis (CVA) was proven to be effective for dynamic process monitoring. However, the traditional CVA indices may not be sensitive enough for incipient faults. In this work, an extension of CVA, called the canonical variate dissimilarity analysis (CVDA), is proposed for process incipient fault detection in nonlinear dynamic processes under varying operating conditions. To handle the non-Gaussian distributed data, the kernel density estimation was used for computing detection limits. A CVA dissimilarity based index has been demonstrated to outperform traditional CVA indices and other dissimilarity-based indices, namely the dissimilarity analysis, recursive dynamic transformed component statistical analysis, and generalized canonical correlation analysis, in terms of sensitivity when tested on slowly developing multiplicative and additive faults in a continuous stirred-tank reactor under closed-loop control and varying operating conditions.		Karl Ezra Pilario;Yi Cao	2018	IEEE Transactions on Industrial Informatics	10.1109/TII.2018.2810822	computer science;random variate;process control;real-time computing;principal component analysis;generalized canonical correlation;multivariate statistics;kernel density estimation;statistical process control;fault detection and isolation;pattern recognition;artificial intelligence	SE	36.12923051796869	-29.058415843523168	69995
6e2279959046a81eb29803d6e5575f5d5db43350	modeling and unsupervised classification of multivariate hidden markov chains with copulas	modelizacion;traitement signal;processus gauss;parametric model;modelo markov oculto;evasion;chaine markov;cadena markov;analisis estadistico;image processing;maximum likelihood;modele markov cache;spherically invariant random vectors;copulas;classification non supervisee;ley n variables;hidden markov chain;hidden markov model;funcion densidad probabilidad;statistical classification;probability density function;maximum vraisemblance;copulae;procesamiento imagen;inference for margins;image classification;ruido no gaussiano;probabilistic approach;maximum likelihood estimation;non gaussian noise;traitement image;spherically invariant random vector sirv;nongaussian multidimensional probability density function;random vector;escape;modelisation;fonction densite probabilite;hidden markov models telecommunications multidimensional signal processing probability density function signal processing image processing pattern classification signal processing algorithms inference algorithms maximum likelihood estimation;hidden markov models;statistical analysis;enfoque probabilista;approche probabiliste;signal processing;clasificacion no supervisada;analyse statistique;multidimensional signal processing;inferencia;pattern classification;signal and image processing;algorithme em;unsupervised classification;invariante;statistical unsupervised classification unsupervised classification multivariate hidden markov chains parametric modeling nongaussian multidimensional probability density function image processing signal processing spherically invariant random vectors;multivariate hidden markov chains;multivariate distribution;inference algorithms;algoritmo em;vector aleatorio;multivariate modeling;gaussian process;signal processing algorithms;proceso gauss;statistical classification copulas em algorithm hidden markov chains hidden markov models inference for margins maximum likelihood multivariate modeling spherically invariant random vector sirv;em algorithm;loi n variables	Parametric modeling and estimation of non-Gaussian multidimensional probability density function is a difficult problem whose solution is required by many applications in signal and image processing. A lot of efforts have been devoted to escape the usual Gaussian assumption by developing perturbed Gaussian models such as spherically invariant random vectors (SIRVs). In this work, we introduce an alternative solution based on copulas that enables theoretically to represent any multivariate distribution. Estimation procedures are proposed for some mixtures of copula-based densities and are compared in the hidden Markov chain setting, in order to perform statistical unsupervised classification of signals or images. Useful copulas and SIRV for multivariate signal classification are particularly studied through experiments.	conditional comment;estimation theory;expectation–maximization algorithm;experiment;hidden markov model;hybrid memory cube;image processing;interaction-free measurement;iterative method;linear least squares (mathematics);linear search;markov chain;noise margin;parametric model;sensor;signal processing;triplet state;unsupervised learning;monotone	Nicolas Brunel;Jérôme Lapuyade-Lahorgue;Wojciech Pieczynski	2010	IEEE Transactions on Automatic Control	10.1109/TAC.2009.2034929	parametric model;machine learning;pattern recognition;mathematics;maximum likelihood;copula;hidden markov model;statistics	ML	28.89981405870682	-30.43054371732026	70209
73d20529d7eddc117243497d268f968618082917	latent class models for financial data analysis: some statistical developments	secs s 01 statistica;latent variable;financial data analysis;risk return profile;portfolio choice;secs p 05 econometria;latent class model	I exploit the potential of latent class models for proposing an innovative framework for financial data analysis. By stressing the latent nature of the most important financial variables, expected return and risk, I am able to introduce a new methodological dimension in the analysis of financial phenomena. In my proposal, (i) I provide innovative measures of expected return and risk, (ii) I suggest a financial data classification consistent with the latent risk-return profile, and (iii) I propose a set of statistical methods for detecting and testing the number of groups of the new data classification. The results lead to an improvement in both risk measurement theory and practice and, if compared to traditional methods, allow for new insights into the analysis of financial data. Finally, I illustrate the potentiality of my proposal by investigating the European stock market and detailing the steps for the appropriate choice of a financial portfolio.	akaike information criterion;bootstrapping (statistics);cluster analysis;diversification (finance);dow jones news/retrieval;jones calculus;k-means clustering;latent variable;model selection;relevance;risk assessment;sensor	Luca De Angelis	2013	Statistical Methods and Applications	10.1007/s10260-012-0214-3	latent class model;latent variable;econometrics;data mining;mathematics;statistics	ML	27.85852566855385	-24.15415735148718	70372
f00a85a3bd9c1294ec8cd6d0e96bc9164c8bff6f	a hybrid quasi monte carlo method for yield aware analog circuit sizing tool	mobile device	Efficient yield estimation methods are required by yield aware automatic sizing tools, where many iterative variability analyses are performed. Quasi Monte Carlo (QMC) is a popular approach, in which samples are generated more homogeneously, hence faster convergence is obtained compared to the conventional MC. However, since QMC is deterministic and has no natural variance, there is no convenient way to obtain estimation error bounds. To determine the confidence interval of the estimated yield, scrambled QMC, in which samples are randomly permuted, is run multiple times to obtain stochastic variance by sacrificing computational cost. To palliate this challenge, this paper proposes a hybrid method, where a single QMC is performed to determine infeasible solutions in terms of yield, which is followed by a few scrambled QMC analyses providing variance and confidence interval of the estimated yield. Yield optimization is performed considering the worst case of the current estimation, thus the optimizer guarantees that the solution will satisfy the confidence interval. Furthermore, a yield ranking mechanism is also developed to enforce the optimizer to search for more robust solutions.	algorithmic efficiency;analogue electronics;best, worst and average case;iterative method;mathematical optimization;monte carlo method;quantum monte carlo;quasi-monte carlo method;randomness;spatial variability	Engin Afacan;Gönenç Berkol;Ali Emre Pusane;Günhan Dündar;I. Faik Baskaya	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		mathematical optimization;computer science;operating system;mobile device;statistics	EDA	36.44394656068769	-25.976687664471886	70538
51b93e92fbc9122060e29c3e0f86006dd44234a0	comparison of methods for hyperspherical data averaging and parameter estimation	optimisation;robust estimator;computer vision;vectors;hyperspherical data averaging;pattern recognition;optimization;parameter estimation	Averaging is an important concept which has found numerous applications in general and in pattern recognition and computer vision in particular. In this paper we consider averaging directional vectors of arbitrary dimensions. Given a set of vectors, we intend to compute an average vector which optimally represents the input vectors according to some formal criterion. Several optimisation criteria are formulated. In particular, we present a class of robust estimators of up to 50% outlier tolerance. Furthermore, we propose a technique to estimate another distribution parameter. Experimental results on spherical data are presented to demonstrate the usefulness of the proposed methods	computer vision;estimation theory;mathematical optimization;pattern recognition	Kai Rothaus;Xiaoyi Jiang;Martin Lambers	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.391	robust statistics;computer vision;econometrics;mathematical optimization;computer science;mathematics;estimation theory;statistics	Vision	33.134154243822074	-30.35377017309348	70571
cf550cd2f97719b7a0f6bedf41f51ba73cf96500	projection predictive model selection for gaussian processes	gaussian processes;input variables;training;computational modeling;mathematical model;predictive models;data models	We propose a new method for simplification of Gaussian process (GP) models by projecting the information contained in the full encompassing model and selecting a reduced number of variables based on their predictive relevance. Our results on synthetic and real world datasets show that the proposed method improves the assessment of variable relevance compared to the automatic relevance determination (ARD) via the length-scale parameters. We expect the method to be useful for improving explainability of the models, reducing the future measurement costs and reducing the computation time for making new predictions.	computation;gaussian process;level of detail;model selection;relevance;synthetic intelligence;time complexity	Juho Piironen;Aki Vehtari	2016	2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2016.7738829	data modeling;computer science;machine learning;mathematical model;data mining;gaussian process;mathematics;predictive modelling;computational model;statistics	ML	29.41031333130173	-28.696236059587815	70661
5842ad95c39c4994c40610cfe5033018f9da516a	temporal regularized matrix factorization for high-dimensional time series prediction		Time series prediction problems are becoming increasingly high-dimensional in modern applications, such as climatology and demand forecasting. For example, in the latter problem, the number of items for which demand needs to be forecast might be as large as 50,000. In addition, the data is generally noisy and full of missing values. Thus, modern applications require methods that are highly scalable, and can deal with noisy data in terms of corruptions or missing values. However, classical time series methods usually fall short of handling these issues. In this paper, we present a temporal regularized matrix factorization (TRMF) framework which supports data-driven temporal learning and forecasting. We develop novel regularization schemes and use scalable matrix factorization methods that are eminently suited for high-dimensional time series data that has many missing values. Our proposed TRMF is highly general, and subsumes many existing approaches for time series analysis. We make interesting connections to graph regularization methods in the context of learning the dependencies in an autoregressive framework. Experimental results show the superiority of TRMF in terms of scalability and prediction quality. In particular, TRMF is two orders of magnitude faster than other methods on a problem of dimension 50,000, and generates better forecasts on real-world datasets such as Wal-mart E-commerce datasets.	autoregressive model;data mart;e-commerce;matrix regularization;missing data;scalability;signal-to-noise ratio;temporal logic;time series;write-ahead logging	Hsiang-Fu Yu;Nikhil Rao;Inderjit S. Dhillon	2016			econometrics;machine learning;data mining;mathematics;statistics	ML	28.702593559287063	-34.38924399829341	70799
a489d92a9d5f5236fa5d2b61252afce346e805d1	the mutual information principle and applications	mutual information	A general theory of prior probability models is presented, valid for both discrete and continuous random variables, even when the prior information about them has been obtained with errors. An example is included as an illustration.	database;mutual information	Nicolas S. Tzannes;Joseph P. Noonan	1973	Information and Control	10.1016/S0019-9958(73)90448-8	information theory and measure theory;variation of information;econometrics;computer science;multivariate mutual information;pattern recognition;mathematics;mutual information;conditional mutual information;interaction information;total correlation;statistics;pointwise mutual information	DB	32.950730587017034	-26.784201595202205	70846
eb9f29c7f7cc61e6c138507f205b964977e5122f	least square regression method for estimating gas concentration in an electronic nose system	least square regression;biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;classification;concentration estimation;gas sensor;machine learning;research articles;abstracts;open access;least square;life sciences;clinical guidelines;support vector machine;full text;electronic nose;rest apis;orcids;europe pmc;biomedical research;humidity sensor;bioinformatics;literature search	We describe an Electronic Nose (ENose) system which is able to identify the type of analyte and to estimate its concentration. The system consists of seven sensors, five of them being gas sensors (supplied with different heater voltage values), the remainder being a temperature and a humidity sensor, respectively. To identify a new analyte sample and then to estimate its concentration, we use both some machine learning techniques and the least square regression principle. In fact, we apply two different training models; the first one is based on the Support Vector Machine (SVM) approach and is aimed at teaching the system how to discriminate among different gases, while the second one uses the least squares regression approach to predict the concentration of each type of analyte.	class;classification;correctness (computer science);diabetes insipidus;electronic nose;estimated;gases;global optimization;heater, device;least squares;machine learning;optimization problem;primed in situ labeling;program optimization;providing (action);support vector machine;analyte;hemoglobin j calabria;sensor (device);voltage	Walaa Khalaf;Calogero Pace;Manlio Gaudioso	2009		10.3390/s90301678	electronic nose;support vector machine;text mining;biological classification;computer science;engineering;data science;machine learning;data mining;least squares	ML	36.65488329881218	-34.18031531622607	70894
11506a1f2b02814358f41f3a973573d308cfa502	a permutation-augmented sampler for dp mixture models	auxiliary variable;gibbs sampling;mixture model;dynamic program ming;dirichlet process mixture model;hill climbing;variational method;random projection	We introduce a new inference algorithm for Dirichlet process mixture models. While Gibbs sampling and variational methods focus on local moves, the new algorithm makes more global moves. This is done by introducing a permutation of the data points as an auxiliary variable. The algorithm is a blocked sampler which alternates between sampling the clustering and sampling the permutation. The key to the efficiency of this approach is that it is possible to use dynamic programming to consider all exponentially many clusterings consistent with a given permutation. We also show that random projections can be used to effectively sample the permutation. The result is a stochastic hill-climbing algorithm that yields burn-in times significantly smaller than those of collapsed Gibbs sampling.	algorithm;burn-in;calculus of variations;cluster analysis;data point;dynamic programming;gibbs sampling;mixture model;random projection;sampling (signal processing);stochastic hill climbing	Percy Liang;Michael I. Jordan;Ben Taskar	2007		10.1145/1273496.1273565	econometrics;mathematical optimization;random permutation;gibbs sampling;computer science;slice sampling;variational method;hill climbing;machine learning;mixture model;mathematics;statistics	ML	28.032158803199934	-28.88567861606368	70933
e59f21d7aa3439efac37359d2f3bc62607eb9d22	proportional data modeling via entropy-based variational bayes learning of mixture models	mixture models;entropy;variational bayes;3d objects;identity verification;document clustering;gene expression	During the last few decades, many statistical approaches that were developed in the fields of computer vision and pattern recognition are based on mixture models. A mixture-based representation has a number of advantages: mixture models are generative, flexible, plus they can take prior information into account to improve the generalization capability. The mixture models that we consider in this paper are based on the Dirichlet and generalized Dirichlet distributions that have been widely used to represent proportional data. The novel aspect of this paper is to develop an entropy-based framework to learn these mixture models. Specifically, we propose a Bayesian framework for model learning by means of a sophisticated entropy-based variational Bayes technique. We present experimental results to show that the proposed method is effective in several applications namely person identity verification, 3D object recognition, text document clustering, and gene expression categorization.	3d single-object recognition;categorization;cluster analysis;computer vision;data modeling;entropy (information theory);feature selection;gene co-expression network;identity verification service;mixture model;outline of object recognition;pattern recognition;real life;variational principle	Wentao Fan;Faisal R. Al-Osaimi;Nizar Bouguila;Ji-Xiang Du	2017	Applied Intelligence	10.1007/s10489-017-0909-0	mixture model;computer science;machine learning;artificial intelligence;generative grammar;pattern recognition;categorization;dirichlet distribution;data modeling;bayes' theorem;bayesian probability;document clustering	Vision	31.939057992985937	-36.6214253601838	71271
6dbbb86adf6a1b751fa1ae75818cdfd0796b3c1f	people tracking using hybrid monte carlo filtering	monte carlo methods particle filters state space methods filtering humans space exploration layout distributed computing probability distribution proposals;markov processes state estimation nonlinear dynamical systems tracking motion estimation monte carlo methods;people tracking monte carlo filtering hidden state estimation nonlinear dynamical systems 3 d human motion multiple markov chains hmc filter;high dimensionality;nonlinear dynamical systems;motion estimation;state estimation;hybrid monte carlo;particle filter;human motion;state space;nonlinear dynamics;people tracking;markov processes;monte carlo methods;nonlinear dynamic system;tracking;markov chain	Particle filters are used for hidden state estimation with nonlinear dynamical systems. The inference of 3-d human motion is a natural application, given the nonlinear dynamics of the body and the nonlinear relation between states and image observations. However, the application of particle filters has been limited to cases where the number of state variables is relatively small, because the number of samples needed with high dimensional problems can be prohibitive. We describe a filter that uses hybrid Monte Carlo (HMC) to obtain samples in high dimensional spaces. It uses multiple Markov chains that use posterior gradients to rapidly explore the state space, yielding fair samples from the posterior. We find that the HMC filter is several thousand times faster than a conventional particle filter on a 28D people tracking problem.	dynamical system;gradient;hybrid memory cube;hybrid monte carlo;kinesiology;markov chain;monte carlo method;nonlinear system;particle filter;state space	Kiam Choo;David J. Fleet	2001		10.1109/ICCV.2001.937643	markov chain;mathematical optimization;simulation;hybrid monte carlo;particle filter;nonlinear system;state space;motion estimation;mathematics;tracking;markov process;statistics;monte carlo method	Vision	37.83002996565782	-26.692656483748667	71298
d7c727142e9d6eecc724b8be67e3f0fe2749ad53	diagnosis of elevator faults with ls-svm based on optimization by k-cv		Several common elevator malfunctions were diagnosed with a least square support vector machine (LS-SVM). After acquiring vibration signals of various elevator functions, their energy characteristics and time domain indicators were extracted by theoretically analyzing the optimal wavelet packet, in order to construct a feature vector of malfunctions for identifying causes of the malfunctions as input of LS-SVM. Meanwhile, parameters about LS-SVM were optimized by K-fold cross validation (KCV). After diagnosing deviated elevator guide rail, deviated shape of guide shoe, abnormal running of tractor, erroneous rope groove of traction sheave, deviated guide wheel, and tension of wire rope, the results suggested that the LS-SVM based on K-CV optimization was one of effective methods for diagnosing elevator malfunctions.		Zhou Wan;Shilin Yi;Kun Li;Ran Tao;Min Gou;Xinshi Li;Shu Guo	2015	J. Electrical and Computer Engineering	10.1155/2015/935038	control engineering;simulation;engineering;forensic engineering	EDA	37.873379264600445	-29.859792921845177	71343
44619cca4f39f230f46a0aaa604b734062a8de19	automatic pattern identification based on the complex empirical mode decomposition of the startup current for the diagnosis of rotor asymmetries in asynchronous machines	rotors hidden markov models vectors filtering harmonic analysis bars fault detection;hybrid simulation experimental approach automatic pattern identification complex empirical mode decomposition startup current rotor asymmetries diagnosis asynchronous machines advanced signal processing method start up current complex intrinsic mode function pattern recognition stage automatic detection hidden markov models;hidden markov models;rotors;signal processing;pattern recognition;asynchronous machines;pattern recognition asynchronous rotating machines broken rotor bar detection complex empirical mode decomposition emd hidden markov models hmms;signal processing asynchronous machines hidden markov models pattern recognition rotors	This paper presents an advanced signal processing method applied to the diagnosis of rotor asymmetries in asynchronous machines. The approach is based on the application of complex empirical mode decomposition to the measured start-up current and on the subsequent extraction of a specific complex intrinsic mode function. Unlike other approaches, the method includes a pattern recognition stage that makes possible the automatic identification of the signature caused by the fault. This automatic detection is achieved by using a reliable methodology based on hidden Markov models. Both experimental data and a hybrid simulation-experimental approach demonstrate the effectiveness of the proposed methodology.	automatic identification and data capture;hidden markov model;hilbert–huang transform;markov chain;pattern recognition;r.o.t.o.r.;signal processing;simulation	George K. Georgoulas;Ioannis P. Tsoumas;Jos&#x00E9; Antonino-Daviu;Vicente Climente-Alarc&#x00F3;n;Chrysostomos D. Stylios;Epaminondas D. Mitronikas;Athanasios N. Safacas	2014	IEEE Transactions on Industrial Electronics	10.1109/TIE.2013.2284143	speech recognition;computer science;machine learning;signal processing;pattern recognition	Vision	37.65981073204851	-31.172642766221642	71501
749c5a95612898d886e0d3a8dc747c87ba193050	invariance properties of the likelihood ratio for covariance matrix estimation in some complex elliptically contoured distributions	expected likelihood;likelihood ratio;62h15;elliptically contoured distribution;62h10;probabilites;covariance matrix estimation	properties of the likelihood ratio for covariance matrix estimation in some complex elliptically contoured distributions. OATAO is an open access repository that collects the work of Toulouse researchers and makes it freely available over the web where possible. • We consider a class of complex elliptically contoured matrix distributions (ECD). • We investigate properties of the likelihood ratio (LR). • We derive stochastic representations of the LR for covariance matrix estimation (CME). • Its p.d.f. evaluated at the true CM R 0 does not depend on the latter. • This extends the expected likelihood approach for regularized CME. a b s t r a c t The likelihood ratio (LR) for testing if the covariance matrix of the observation matrix X is R has some invariance properties that can be exploited for covariance matrix estimation purposes. More precisely, it was shown in Abramovich et al. Gaussian case, LR(R 0 |X), where R 0 stands for the true covariance matrix of the observations X , has a distribution which does not depend on R 0 but only on known parameters. This paved the way to the expected likelihood (EL) approach, which aims at assessing and possibly enhancing the quality of any covariance matrix estimate (CME) by comparing its LR to that of R 0. Such invariance properties of LR(R 0 |X) were recently proven for a class of elliptically contoured distributions (ECD) in Abramovich and Besson (2013) and Besson and Abramovich (2013) where regularized CME were also presented. The aim of this paper is to derive the distribution of LR(R 0 |X) for other classes of ECD not covered yet, so as to make the EL approach feasible for a larger class of distributions.	energy citations database;explanatory combinatorial dictionary;lr parser;matrix regularization	Olivier Besson;Yuri I. Abramovich	2014	J. Multivariate Analysis	10.1016/j.jmva.2013.10.024	estimation of covariance matrices;econometrics;combinatorics;likelihood-ratio test;mathematics;statistics	AI	31.270370144292503	-24.866093465135265	71520
d8aa548c805039f8ab5b0d6469021e47c9567258	naïve bayes classifier based watermark detection in wavelet transform	transformation ondelette;high pass;bayes estimation;filigranage numerique;digital watermarking;bande passante;passband;analisis contenido;correlacion;steganographie;multimedia;banda pasante;securite informatique;low pass;blind;imagen nivel gris;probabilistic approach;classification;computer security;sucesion seudo aleatoria;steganography;estimacion bayes;content analysis;esteganografia;suite pseudoaleatoire;wavelet transform;enfoque probabilista;approche probabiliste;seguridad informatica;robustesse;filigrana digital;image niveau gris;pseudorandom sequence;robustness;image watermarking;transformacion ondita;analyse contenu;correlation;deteccion bayes;bayes detection;bayes classifier;random numbers;grey level image;clasificacion;ciego;wavelet transformation;detection bayes;estimation bayes;robustez;aveugle	Robustness is the one of the essential properties of watermarking schemes. It is the ability to detect the watermark after attacks. A DWT-based semi-blind image watermarking scheme leaves out the low pass band, and embeds a pseudo random number (PRN) sequence (i.e., the watermark) in the other three bands into the coefficients that are higher than a given threshold T1. During watermark detection, all the high pass coefficients above another threshold T2 (T2 ≥ T1) are used in correlation with the original watermark. In this paper, we embed a PRN sequence using the same procedure. In detection, however, we apply the Naïve Bayes Classifier, which can predict class membership probabilities, such as the probability that a given image belongs to class “Watermark Present” or “Watermark Absent”. Experimental results show that the Naïve Bayes Classifier gives very promising results for gray scale images in the wavelet domain watermark detection.	algorithm;blind signature;coefficient;digital watermarking;discrete wavelet transform;essence;grayscale;naive bayes classifier;pseudorandomness;random number generation;semiconductor industry	Ersin Elbasi;Ahmet M. Eskicioglu	2006		10.1007/11848035_32	computer vision;bayes classifier;speech recognition;content analysis;low-pass filter;biological classification;digital watermarking;computer science;machine learning;pattern recognition;mathematics;steganography;high-pass filter;programming language;passband;correlation;statistics;robustness;wavelet transform	ML	35.232736393221586	-37.48746508848787	71598
749d9a4f0d1217bb8d01e7f4ecf54d0b885afde8	kernel ridge regression via partitioning		In this paper, we investigate a divide and conquer approach to Kernel Ridge Regression (KRR). Given n samples, the division step involves separating the points based on some underlying disjoint partition of the input space (possibly via clustering), and then computing a KRR estimate for each partition. The conquering step is simple: for each partition, we only consider its own local estimate for prediction. We establish conditions under which we can give generalization bounds for this estimator, as well as achieve optimal minimax rates. We also show that the approximation error component of the generalization error is lesser than when a single KRR estimate is fit on the data: thus providing both statistical and computational advantages over a single KRR estimate over the entire data (or an averaging over random partitions as in other recent work, [30]). Lastly, we provide experimental validation for our proposed estimator and our assumptions.	approximation error;cluster analysis;computation;generalization error;kernel (operating system);mike lesser;minimax	Rashish Tandon;Si Si;Pradeep Ravikumar;Inderjit S. Dhillon	2016	CoRR		econometrics;mathematical optimization;machine learning;mathematics;statistics	ML	30.568230120348925	-29.709373321384113	71761
d7a0708a6be83facd5b101123c87d375078664a2	spherical minimum description length		We consider the problem of model selection using the Minimum Description Length (MDL) criterion for distributions with parameters on the hypersphere. Model selection algorithms aim to find a compromise between goodness of fit and model complexity. Variables often considered for complexity penalties involve number of parameters, sample size and shape of the parameter space, with the penalty term often referred to as stochastic complexity. Current model selection criteria either ignore the shape of the parameter space or incorrectly penalize the complexity of the model, largely because typical Laplace approximation techniques yield inaccurate results for curved spaces. We demonstrate how the use of a constrained Laplace approximation on the hypersphere yields a novel complexity measure that more accurately reflects the geometry of these spherical parameters spaces. We refer to this modified model selection criterion as spherical MDL. As proof of concept, spherical MDL is used for bin selection in histogram density estimation, performing favorably against other model selection criteria.	akaike information criterion;approximation;bayesian information criterion;blum axioms;computational complexity theory;conceptualization (information science);ibm notes;kolmogorov complexity;mdl (programming language);minimum description length;model selection;norm (social);occam's razor;selection algorithm;whole earth 'lectronic link	Trevor Herntier;Koffi Eddy Ihou;Anthony O. Smith;Anand Rangarajan;Adrian M. Peter	2018	Entropy	10.3390/e20080575	mathematical optimization;minimum description length;mathematics;mathematical analysis	ML	27.776836153522684	-27.39545432057012	71831
1c7ce96184883a80d1cb949253c52d1792b45159	advanced cyclostationary-based analysis for condition monitoring of complex systems		Wind energy experiences a significant growth during the last decades but the industry is still challenged by premature turbine component failures, which are quite expensive due to the increase of turbines size. The core of wind turbine drivetrains is a planetary gearbox and its rolling element bearings are often responsible for machinery breakdowns. The failure signs of an early bearing damage are usually weak compared to the gear excitation and are hardly detected. As a result there is a special need for advanced signal processing tools which can detect accurately bearing faults. Cyclic Spectral Coherence (CSC) appears to be a strong diagnostic tool but its interpretation is complicated for a non-expert. In this paper a novel CSC based methodology is proposed in order to extract an Improved Envelope Spectrum exploiting a specific domain of the CSC map optimally selected by a proposed criterion. The methodology is tested and validated on a wind turbine gearbox benchmarking dataset provided by the National Renewable Energy Laboratory (NREL), USA.		Konstantinos C. Gryllias;Alexandre Mauricio;Junyu Qi	2018	2018 26th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2018.8553568	wind power;reliability engineering;signal processing;condition monitoring;cyclostationary process;turbine;drivetrain;computer science;vibration;bearing (mechanical)	HPC	35.12904207384622	-30.59141774282686	71841
de042bfc91dd9ab2e2908f5b211210d6c40df781	automatic isosurface propagation using an extrema graph and sorted boundary cell lists	graph theory;visualization automatic isosurface propagation extrema graph sorted boundary cell lists high performance algorithm isosurface generation searching arcs sorted lists adjacent intersected cells benchmark;boundary;computational geometry;surface fitting;data visualisation;isosurface;extremum points;surface fitting data visualisation computational geometry graph theory;isosurfaces visualization displays numerical simulation temperature electronic mail costs intrusion detection benchmark testing power engineering computing;propagation	A high-performance algorithm for generating isosurfaces is presented. In our method, guides to searching for cells intersected by an isosurface are generated as a pre-process. These guides are two kinds of cell lists: an extrema graph, and sorted lists of boundary cells. In an extrema graph, extremum points are connected by arcs, and each arc has a list of cells through which it passes. At the same time, all boundary cells are sorted according to their minimum and maximum values, and two sorted lists are then generated. Isosurfaces are generated by visiting adjacent intersected cells in order. Here, the starting cells for this process are found by searching in an extrema graph and in sorted boundary cell lists. In this process, isosurfaces appear to propagate themselves. Our algorithm is efficient, since it visits only cells that are intersected by an isosurface and cells whose IDs are included in the guides. It is especially efficient when many isosurfaces are interactively generated in a huge volume. Some benchmark tests described in this paper show the efficiency of the algorithm.	cell lists;isosurface;schema (genetic algorithms)	Takayuki Itoh;Koji Koyamada	1995	IEEE Trans. Vis. Comput. Graph.	10.1109/2945.485619	combinatorics;computational geometry;computer science;graph theory;theoretical computer science;isosurface;mathematics;geometry;boundary;data visualization;algorithm	Visualization	36.155904138710504	-36.69015468580922	72213
c322821971390fcd85e389e0e5250ed42e5e4255	efficient estimation for shared latent space using multi-layer perceptron		There are quite a few high dimensional time-series data co-ocurring each other such as lip motions, voices, and face appearances and so on. When capturing the correspondent relationships among those time-series data with different dimensionality, we need to make the dimensionality all the same size so that they can be compared each other. To achieve this, Gaussian Process Latent Variable Models (GPLVM) is often used to reduce the size of high dimensional time-series data. In this study, we propose a method to introduce MLP to GPLVM-based methods in estimating latent states. We applied the proposed method to GPLVM, SharedGPLVM, GPDM, and SharedGPDM, and then confirmed that our method outperforms the conventional methods in terms of efficiency and precisely estimation.	dimensionality reduction;expectation–maximization algorithm;gaussian process;latent variable;mathematical optimization;memory-level parallelism;multilayer perceptron;nonlinear programming;nonlinear system;quad flat no-leads package;time series	Mariho Ohyama;Ichiro Kobayashi	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122986	machine learning;kernel (linear algebra);curse of dimensionality;artificial intelligence;computer science;data modeling;principal component analysis;multilayer perceptron;gaussian process;latent variable	DB	28.883136558995826	-34.44857300070827	72247
2cba1a1c436e1e2be94fe501e99d4b559b49ae10	asymptotic properties of nonparametric estimation on manifold		In many applications, the real high-dimensional data occupy only a very small part in the high dimensional ‘observation space’ whose intrinsic dimension is small. The most popular model of such data is Manifold model which assumes that the data lie on or near an unknown manifold (Data Manifold, DM) of lower dimensionality embedded in an ambient high-dimensional input space (Manifold Assumption about high-dimensional data). Manifold Learning is a Dimensionality Reduction problem under the Manifold assumption about the processed data, and its goal is to construct a low-dimensional parameterization of the DM (global low-dimensional coordinates on the DM) from a finite dataset sampled from the DM. Manifold Assumption means that local neighborhood of each manifold point is equivalent to an area of low-dimensional Euclidean space. Because of this, most of Manifold Learning algorithms include two parts: ‘local part’ in which certain characteristics reflecting low-dimensional local structure of neighborhoods of all sample points are constructed via nonparametric estimation, and ‘global part’ in which global low-dimensional coordinates on the DM are constructed by solving the certain convex optimization problem for specific cost function depending on the local characteristics. Both statistical properties of ‘local part’ and its average over manifold are considered in the paper. The article is an extension of the paper (Yanovich, 2016) for the case of nonparametric estimation.	algorithm;convex optimization;embedded system;intrinsic dimension;loss function;mathematical optimization;nonlinear dimensionality reduction;optimization problem	Yury Yanovich	2017			manifold;mathematical analysis;nonparametric statistics;mathematics;asymptotic analysis	ML	24.66110812621017	-37.74055524502834	72310
281866c177a7df126cf1950706091239e88c9fbe	learning bayesian networks with restricted causal interactions	bayesian network	A major problem for the learning of Bayesian networks (BNs) is the exponential number of parameters needed for conditional prob­ ability tables. Recent research reduces this complexity by modeling local structure in the probability tables. We examine the use of log-linear local models. While log-linear models in this context are not new (Whit­ taker, 1990; Buntine, 1991; Neal, 1992; Heck­ erman and Meek, 1997), it is generally sub­ sumed under a naive Bayes model. We de­ scribe an alternative using a Minimum Mes­ sage Length (MML) (Wallace and Freeman, 1987) metric for the selection of local mod­ els with causal independence, which we term a first-order model (FOM). We also combine FOMs and full conditional models on a node­ by-node basis.	bayesian network;causal filter;causality;discrepancy function;first-order predicate;http 404;interaction;linear model;log-linear model;logistic regression;machine learning;metropolis;metropolis–hastings algorithm;minimum description length;naive bayes classifier;sampling (signal processing);sion's minimax theorem;test data;the baseball network;time complexity;wallace tree	Julian R. Neil;Chris S. Wallace;Kevin B. Korb	1999			econometrics;variable-order bayesian network;computer science;artificial intelligence;machine learning;bayesian network;mathematics;statistics	AI	25.09354239016736	-26.6783139092532	72384
03e8fbee0f2cd7ec12de6b87bde838b245b22c54	a reservoir-driven non-stationary hidden markov model	dirichlet process;hidden markov model;journal article;pattern recognition;markov processes;computer science;article;reservoir	In this work, we propose a novel approach towards sequential data modeling that leverages the strengths of hidden Markov models and echo-state networks (ESNs) in the context of nonparametric Bayesian inference approaches. We introduce a non-stationary hidden Markov model, the time-dependent state transition probabilities of which are driven by a high-dimensional signal that encodes the whole history of the modeled observations, namely the state vector of a postulated observations-driven ESN reservoir. We derive an efficient inference algorithm for our model under the variational Bayesian paradigm, and we examine the efficacy of our approach considering a number of sequential data modeling applications.	algorithm;bayesian approaches to brain function;data modeling;echo state network;hidden markov model;markov chain;programming paradigm;state transition table;stationary process;variational principle	Sotirios P. Chatzis;Yiannis Demiris	2012	Pattern Recognition	10.1016/j.patcog.2012.04.018	forward algorithm;markov chain;maximum-entropy markov model;markov kernel;variable-order bayesian network;partially observable markov decision process;markov property;viterbi algorithm;computer science;machine learning;causal markov condition;hidden semi-markov model;markov blanket;forward–backward algorithm;pattern recognition;markov renewal process;markov algorithm;markov process;markov model;hidden markov model;statistics;variable-order markov model;reservoir	Vision	31.894403844854455	-28.210265876560513	72711
c3d59438df81ae05f8da4330337ea8ff45763efc	graph-based semisupervised learning	optimisation sous contrainte;unlabeled data;graph classifier;modelizacion;algorithms artificial intelligence cluster analysis computer simulation data interpretation statistical models statistical pattern recognition automated;constrained optimization;graph theory;optimisation;kernel;constraint optimization;iterative algorithms;nonparametric statistics;methode noyau;semisupervised graph based learning;analisis forma;graph based semisupervised learning;statistical methods;statistical method;intelligence artificielle;nonparametric statistic;fonction perte;funcion perdida;classification;constraint loss optimization;semi supervised learning;optimizacion con restriccion;modelisation;benchmark data sets;laplace equations;smoothing methods;machine learning;semisupervised learning clustering algorithms laplace equations matrix decomposition iterative algorithms clamps kernel smoothing methods constraint optimization labeling;matrix decomposition;loss function;smoothing;metodo nucleo;pattern classification graph theory learning artificial intelligence optimisation;pattern classification;alisamiento;statistical methods machine learning nonparametric statistics;clustering algorithms;artificial intelligence;kernel method;optimization graph based semisupervised learning graph classifier kernel smoothing benchmark data sets;optimization;modele donnee;pattern analysis;inteligencia artificial;learning artificial intelligence;labeling index;kernel smoothing;modeling;clamps;clasificacion;lissage;semisupervised learning;analyse forme;labeling;data models	Graph-based learning provides a useful approach for modeling data in classification problems. In this modeling scenario, the relationship between labeled and unlabeled data impacts the construction and performance of classifiers and, therefore, a semisupervised learning framework is adopted. We propose a graph classifier based on kernel smoothing. A regularization framework is also introduced and it is shown that the proposed classifier optimizes certain loss functions. Its performance is assessed on several synthetic and real benchmark data sets with good results, especially in settings where only a small fraction of the data are labeled.	algorithm;apache axis;appendix;assumed;axis vertebra;benchmark (computing);clamping (graphics);co-training;command & conquer:yuri's revenge;dhrystone;emoticon;entity name part qualifier - adopted;fingerprint;graph - visual representation;hearing loss, high-frequency;iteration;kr 66223;kernel;loss function;mathematical optimization;matrix regularization;pharmacology;population parameter;sftpa1 gene;sgta gene;semi-supervised learning;smoothing (statistical technique);software propagation;statistical model;synthetic data;time complexity;tracer;vertex;cisplatin/cyclophosphamide/etoposide protocol;cisplatin/cyclophosphamide/vindesine protocol;exponential	Mark Vere Culp;George Michailidis	2008	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2007.70765	semi-supervised learning;nonparametric statistics;data modeling;kernel method;labeling theory;constrained optimization;kernel;systems modeling;biological classification;computer science;machine learning;pattern recognition;data mining;mathematics;cluster analysis;matrix decomposition;statistics;smoothing;kernel smoother;loss function	ML	29.573524133824627	-31.599560955199586	72794
56c405663d6dbc8e2de96968d702ee65a739def1	hybrid real-time matrix factorization for implicit feedback recommendation systems		In this paper, we present a hybrid real-time incremental stochastic gradient descent (RI-SGD) updating technique for implicit feedback matrix factorization (MF) recommendation systems. Compared with explicit feedback evaluation scores, implicit feedback data are easier to obtain but pose challenges to MF recommendation systems because of the transformation procedures from raw data to user preference scores. Another challenge for MF recommendation systems is the accuracy issue when the speed of the new input data increases. The proposed RI-SGD is designed for computationally-efficient and accurate time-variant implicit feedback MF recommendation system, which consists of alternating least squares with weight regularization in the training phase and stochastic gradient descent in the updating phase. To demonstrate the advantages of the RI-SGD updating technique in terms of computational efficiency and accuracy, we implement the proposed updating techniques in a real-time music recommendation system. Compared with the method of retraining the entire model, our numerical results show that RI-SGD approach can achieve almost the same recommendation accuracy, but requires only about 0.02% of the retraining time.	feedback;least squares;numerical analysis;rs-232;real-time cmix;real-time clock;real-time computing;real-time transcription;recommender system;stochastic gradient descent	Chia-Yu Lin;Li-Chun Wang;Kun-Hung Tsai	2018	IEEE Access	10.1109/ACCESS.2018.2819428	kernel (linear algebra);recommender system;raw data;machine learning;distributed computing;least squares;computer science;stochastic gradient descent;matrix decomposition;data modeling;stochastic process;artificial intelligence	AI	28.957576487201084	-34.22138615144812	73017
af8a60c97218b734d92bcfbb7c8beb9d1007619e	computational advances in sparse l1-norm principal-component analysis of multi-dimensional data		We consider the problem of extracting a sparse Li-norm principal component from a data matrix X ∊ R<sup>D×N</sup> of N observation vectors of dimension D. Recently, an optimal algorithm was presented in the literature for the computation of sparse L<inf>1</inf>-norm principal components with complexity O(N<sup>S</sup>) where S is the desired sparsity. In this paper, we present an efficient suboptimal algorithm of complexity O(N<sup>2</sup>(N + D)). Extensive numerical studies demonstrate the near-optimal performance of the proposed algorithm and its strong resistance to faulty measurements/outliers in the data matrix.	algorithm;computation;numerical analysis;principal component analysis;sparse matrix;t-norm;taxicab geometry	Shubham Chamadia;Dimitris A. Pados	2017	2017 IEEE 7th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)	10.1109/CAMSAP.2017.8313159	data matrix;outlier;mathematical optimization;computation;principal component analysis;sparse matrix;mathematics	ML	25.69610401999498	-36.85857720216888	73344
179efe0e5945a553bbbf66499b05c7e2783580e5	practical bounds on the error of bayesian posterior approximations: a nonasymptotic approach		Bayesian inference typically requires the computation of an approximation to the posterior distribution. An important requirement for an approximate Bayesian inference algorithm is to output high-accuracy posterior mean and uncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain Monte Carlo, remain the gold standard for approximate Bayesian inference because they have a robust finite-sample theory and reliable convergence diagnostics. However, alternative methods, which are more scalable or apply to problems where Markov Chain Monte Carlo cannot be used, lack the same finite-data approximation theory and tools for evaluating their accuracy. In this work, we develop a flexible new approach to bounding the error of mean and uncertainty estimates of scalable inference algorithms. Our strategy is to control the estimation errors in terms of Wasserstein distance, then bound the Wasserstein distance via a generalized notion of Fisher distance. Unlike computing the Wasserstein distance, which requires access to the normalized posterior distribution, the Fisher distance is tractable to compute because it requires access only to the gradient of the log posterior density. We demonstrate the usefulness of our Fisher distance approach by deriving bounds on the Wasserstein error of the Laplace approximation and Hilbert coresets. We anticipate that our approach will be applicable to many other approximate inference methods such as the integrated Laplace approximation, variational inference, and approximate Bayesian computation.	approximation algorithm;approximation theory;bayesian approaches to brain function;bayesian network;cobham's thesis;computation;fisher information;gradient;markov chain monte carlo;monte carlo method;scalability;variational principle	Jonathan H. Huggins;Trevor Campbell;Mikolaj Kasprzak;Tamara Broderick	2018	CoRR		statistics;approximate inference;monte carlo method;markov chain monte carlo;inference;mathematics;approximate bayesian computation;laplace's method;posterior probability;bayesian inference	ML	27.682155499815863	-27.7356301457337	73397
0a47e562f03b46fdbfccae86a2d17cc904bd699f	an ordered lasso and sparse time-lagged regression	grupo de excelencia;penalized regression;ciencias basicas y experimentales;matematicas;feature selection;monotone coefficients	We consider a regression scenario where it is natural to impose an order constraint on the coefficients. We propose an order-constrained version of `1-regularized regression (lasso) for this problem, and show how to solve it efficiently using the well-known Pool Adjacent Violators Algorithm as its proximal operator. The main application of this idea is to time-lagged regression, where we predict an outcome at time t from features at the previous K time points. In this setting it is natural to assume that the coefficients decay as we move farther away from t, and hence the order constraint is reasonable. Potential application areas include financial time series and prediction of dynamic patient outcomes based on clinical measurements. We illustrate this idea on real and simulated data.	algorithm;c date and time functions;coefficient;lasso;proximal operator;sparse;time series;whole earth 'lectronic link	Robert Tibshirani;Xiaotong Suo	2016	Technometrics	10.1080/00401706.2015.1079245	econometrics;mathematics;feature selection;statistics	ML	29.07827560093288	-25.632257297963687	73402
7c3820366847404cc35d5a19b66b0df51fb0efb9	maximum likelihood for blind separation and deconvolution of noisy signals using mixture models	data augmentation;maximum likelihood;approximation theory maximum likelihood estimation deconvolution gaussian distribution noise;blind source separation;conditional expectation;maximum likelihood estimation;approximation theory;monte carlo integration;mixture model;deconvolution signal processing algorithms parameter estimation blind source separation distributed computing additive noise maximum likelihood estimation iterative algorithms gaussian distribution finite impulse response filter;deconvolution;mixture of gaussians;missing data;cumulant;conditional expectation data augmentation deconvolution noisy signals mixture model approximate maximum likelihood method blind source separation missing data unobserved input input signal distribution gaussian distributions posterior density;maximum likelihood method;gaussian distribution;noise	In this paper, an approximate maximum likelihood method for blind source separation and deconvolution of noisy signal is proposed. This technique relies upon a data augmentation scheme, where the (unobserved) input are viewed as the missing data. In the technique described in this contribution, the input signal distribution is modeled by a mixture of Gaussian distributions, enabling the use of explicit formula for computing the posterior density and conditional expectation and thus avoiding Monte-Carlo integrations. Because this technique is able to capture some salient features of the input signal distribution, it performs generally much better than third-order or fourth-order cumulant based techniques.	approximation algorithm;blind signal separation;convolutional neural network;deconvolution;missing data;mixture model;monte carlo;source separation	Eric Moulines;Jean-François Cardoso;Elisabeth Gassiat	1997		10.1109/ICASSP.1997.604649	econometrics;pattern recognition;mathematics;blind signal separation;blind deconvolution;maximum likelihood;statistics	ML	30.6540226851929	-28.716928229679453	73502
7734a9b8d99b03e9cc88d89690e24b54d37bba3f	bayesian robust pca of incomplete data	outliers;factor analysis;principal component analysis;robustness;missing values;variational bayesian methods	We present a probabilistic model for robust factor analysis and principal component analysis in which the observation noise is modeled by Student-t distributions in order to reduce the negative effect of outliers. The Student-t distributions are modeled independently for each data dimensions, which is different from previous works using multivariate Student-t distributions. We compare methods using the proposed noise distribution, the multivariate Student-t and the Laplace distribution. Intractability of evaluating the posterior probability density is solved by using variational Bayesian approximation methods. We demonstrate that the assumed noise model can yield accurate reconstructions because corrupted elements of a bad quality sample can be reconstructed using the other elements of the same data vector. Experiments on an artificial dataset and a weather dataset show that the dimensional independency and the flexibility of the proposed Student-t noise model can make it superior in some applications.	academy;approximation algorithm;data point;experiment;factor analysis;finnish meteorological institute;gaussian blur;gaussian process;machine learning;missing data;preprocessor;principal component analysis;spatial variability;state space;statistical model;variational principle;wave packet	Jaakko Luttinen;Alexander Ilin;Juha Karhunen	2012	Neural Processing Letters	10.1007/s11063-012-9230-4	econometrics;outlier;computer science;data mining;mathematics;factor analysis;statistics;robustness;principal component analysis	ML	30.91951173982897	-28.644791937754196	73639
bf709cc87a36dbbf25faae3e20c3f0d21c47364c	distributed differentially private algorithms for matrix and tensor factorization		In many signal processing and machine learning applications, datasets containing private information are held at different locations, requiring the development of distributed privacy-preserving algorithms. Tensor and matrix factorizations are key components of many processing pipelines. In the distributed setting, differentially private algorithms suffer because they introduce noise to guarantee privacy. This paper designs new and improved distributed and differentially private algorithms for two popular matrix and tensor factorization methods: principal component analysis and orthogonal tensor decomposition. The new algorithms employ a correlated noise design scheme to alleviate the effects of noise and can achieve the same noise level as the centralized scenario. Experiments on synthetic and real data illustrate the regimes in which the correlated noise allows performance matching with the centralized setting, outperforming previous methods and demonstrating that meaningful utility is possible while guaranteeing differential privacy.		Hafiz Imtiaz;Anand D. Sarwate	2018	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2018.2877842	differential privacy;tensor;computer science;principal component analysis;private information retrieval;signal processing;matrix (mathematics);algorithm;factorization	ML	26.12782829855459	-36.2226849848892	73713
48e79ddd445c7ceaf0798608cd8c9c3348c649ee	non-parametric approximate linear programming for mdps	journal article;jason;computer science non parametric approximate linear programming for mdps duke university ronald parr pazis	The Approximate Linear Programming (ALP) approach to value function approximation for MDPs is a parametric value function approximation method, in that it represents the value function as a linear combination of features which are chosen a priori. Choosing these features can be a difficult challenge in itself. One recent effort, Regularized Approximate Linear Programming (RALP), uses L1 regularization to address this issue by combining a large initial set of features with a regularization penalty that favors a smooth value function with few non-zero weights. Rather than using smoothness as a backhanded way of addressing the feature selection problem, this paper starts with smoothness and develops a non-parametric approach to ALP that is consistent with the smoothness assumption. We show that this new approach has some favorable practical and analytical properties in comparison to (R)ALP.	approximation;bellman equation;feature selection;linear programming;matrix regularization;selection algorithm	Jason Pazis;Ronald Parr	2011			mathematical optimization;artificial intelligence;machine learning;algorithm;statistics	AI	25.699716638838765	-33.93292357822351	73897
3b08f068ab47060829e15ac32bdf8b7cf53d6f63	"""better prediction using the super-serial scalability law explained by the least square error principle and the machine repairman model"""""""			scalability	Jayanta Choudhury	2013			scalability;theoretical computer science;machine learning;artificial intelligence;computer science;least squares	ML	25.658657102156187	-31.63204516095392	74126
36821fae840705e6f896ac9eb922024adc903c9a	kernel subspace pursuit for sparse regression	sparse function approximation;subspace pursuit;kernel methods;regression	This paper introduces a kernel version of the Subspace Pursuit algorithm.The proposed method, KSP, is a new iterative method for sparse regression.KSP outperforms and is less computationally intensive than related kernel methods. Recently, results from sparse approximation theory have been considered as a means to improve the generalization performance of kernel-based machine learning algorithms. In this paper, we present Kernel Subspace Pursuit (KSP), a new method for sparse non-linear regression. KSP is a low-complexity method that iteratively approximates target functions in the least-squares sense as a linear combination of a limited number of elements selected from a kernel-based dictionary. Unlike other kernel methods, by virtue of KSP's algorithmic design, the number of KSP iterations needed to reach the final solution does not depend on the number of basis functions used nor that of elements in the dictionary. We experimentally show that, in many scenarios involving learning synthetic and real data, KSP is less complex computationally and outperforms other kernel methods that solve the same problem, namely, Kernel Matching Pursuit and Kernel Basis Pursuit.	kernel (operating system);sparse matrix	Jad Kabbara;Ioannis N. Psaromiligkos	2016	Pattern Recognition Letters	10.1016/j.patrec.2015.09.018	principal component regression;kernel method;mathematical optimization;string kernel;kernel embedding of distributions;regression;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;matching pursuit;kernel smoother	Vision	25.17912538135701	-37.04008280820857	74157
2c3fb35ea44431a8ba69c0b8f4848c4ad2003897	the generalized reparameterization gradient		The reparameterization gradient has become a widely used method to obtain Monte Carlo gradients to optimize the variational objective. However, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit Gaussian distributions. In this paper, we introduce the generalized reparameterization gradient, a method that extends the reparameterization gradient to a wider class of variational distributions. Generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters. This results in new Monte Carlo gradients that combine reparameterization gradients and score function gradients. We demonstrate our approach on variational inference for two complex probabilistic models. The generalized reparameterization is e ective: even a single sample from the variational distribution is enough to obtain a low-variance gradient.	approximation;calculus of variations;gamma correction;gradient;latent variable;monte carlo method;software release life cycle;variational method (quantum mechanics);variational principle	Francisco J. R. Ruiz;Michalis K. Titsias;David M. Blei	2016			mathematical optimization;combinatorics;mathematics;statistics	ML	28.288119596378095	-28.626750346777467	74244
290ac75699d2bafb6d0a0b30014fb874414301ff	efficient variational inference in large-scale bayesian compressed sensing	point estimation;compressed sensing;statistical machine learning;time complexity;maximum likelihood;gaussian processes;latent variable;approximation algorithms;learning model;bayes methods;random sampling;estimation bayesian methods approximation methods computational modeling approximation algorithms monte carlo methods inference algorithms;image deblurring;bayesian methods;variational bayesian;inference mechanisms;heavy tail;gaussian markov random field;maximum likelihood estimation;large scale;computational modeling;posterior distribution;estimation;variational inference;bayesian computation;linear model;pattern recognition;maximum likelihood estimation bayes methods gaussian processes inference mechanisms markov processes;image deblurring variational inference bayesian compressed sensing probabilistic viewpoint sparse most probable solution deterministic approach maximum likelihood estimation exact posterior distribution sparse linear model variational bayesian technique gaussian variance gaussian markov random field variance estimation technique;inference algorithms;approximation methods;variance estimation;markov processes;exact sampling;monte carlo methods;large scale problem;information theory	We study linear models under heavy-tailed priors from a probabilistic viewpoint. Instead of computing a single sparse most probable (MAP) solution as in standard deterministic approaches, the focus in the Bayesian compressed sensing framework shifts towards capturing the full posterior distribution on the latent variables, which allows quantifying the estimation uncertainty and learning model parameters using maximum likelihood. The exact posterior distribution under the sparse linear model is intractable and we concentrate on variational Bayesian techniques to approximate it. Repeatedly computing Gaussian variances turns out to be a key requisite and constitutes the main computational bottleneck in applying variational techniques in large-scale problems. We leverage on the recently proposed Perturb-and-MAP algorithm for drawing exact samples from Gaussian Markov random fields (GMRF). The main technical contribution of our paper is to show that estimating Gaussian variances using a relatively small number of such efficiently drawn random samples is much more effective than alternative general-purpose variance estimation techniques. By reducing the problem of variance estimation to standard optimization primitives, the resulting variational algorithms are fully scalable and parallelizable, allowing Bayesian computations in extremely large-scale problems with the same memory and time complexity requirements as conventional point estimation techniques. We illustrate these ideas with experiments in image deblurring.	approximation algorithm;calculus of variations;compressed sensing;computation;deblurring;experiment;general-purpose modeling;kernel density estimation;latent variable;linear model;markov chain;markov random field;mathematical optimization;pseudo-random number sampling;requirement;scalability;sparse matrix;time complexity;variational principle	George Papandreou;Alan L. Yuille	2011	2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)	10.1109/ICCVW.2011.6130406	econometrics;information theory;variational message passing;pattern recognition;mathematics;bayesian linear regression;maximum likelihood;statistics	Vision	27.2262450932858	-30.572719431928455	74248
6fe3729b72fd006a3367035dbd49d3cc2227360c	online fault diagnosis method based on incremental support vector data description and extreme learning machine with incremental output structure	online fault diagnosis;extreme learning machine;multi scale principal component analysis;incremental support vector data description	Online fault diagnosis system should be able to detect faults, recognize fault types and update the discriminating ability and knowledge of itself automatically in real time. But the class number in fault diagnosis is not constant and it is in a dynamic state with new members enrolled. The traditional recognition algorithms are not able to update diagnosis system efficiently when the class number of failure modes is increasing. To solve the problem, an online fault diagnosis method based on Incremental Support Vector Data Description (ISVDD) and Extreme Learning Machine with incremental output structure (IOELM) is proposed. ISVDD is used to find a new failure mode quickly in the continuous condition monitoring of the equipments. The fixed structure of Extreme Learning Machine is changed into an elastic structure whose output nodes could be added incrementally to recognize the new fault mode efficiently. Recognition experiments on the diesel engine under eleven different conditions show that the online fault diagnosis method based on ISVDD and IOELM works well, and the method is also feasible in fault diagnosis of other mechanical equipments.		Gang Yin;Ying-Tang Zhang;Zhining Li;Guo-Quan Ren;Hong-Bo Fan	2014	Neurocomputing	10.1016/j.neucom.2013.01.061	real-time computing;fault coverage;stuck-at fault;machine learning;data mining;fault model	AI	36.81465215369438	-30.17826896280371	74496
5f272ca67829870304aa16d00e6f57a00d4f3642	error bounds for convex parameter estimation	compressed sensing;sparsity;estimation;bounds	We evaluate the accuracy of sparsity-based estimation methods inspired from compressed sensing. Typical estimation approaches consist of minimizing a non-convex cost function that exhibits local minima, and require excessive computational resources. A tractable alternative relies on a sparse representation of the observation vector using a large dictionary matrix and a convex cost function. This estimation approach converts the intractable high-dimensional non-convex problem into a simpler convex problem with reduced dimension. Unfortunately, the advantages come at the expense of increased estimation error. Therefore, an evaluation of the estimation error is of considerable interest. We consider the case of estimating a single parameter vector, and provide upper bounds on the achievable accuracy. The theoretical results are corroborated by simulations. & 2011 Elsevier B.V. All rights reserved.	cobham's thesis;compressed sensing;computational resource;convex optimization;dictionary;error detection and correction;estimation theory;loss function;maxima and minima;simulation;sparse approximation;sparse matrix	Joseph Shmuel Picard;Anthony J. Weiss	2012	Signal Processing	10.1016/j.sigpro.2011.11.031	econometrics;mathematical optimization;estimation;mathematics;compressed sensing;sparsity-of-effects principle;statistics	ML	26.88535468094169	-33.46718691706357	74892
f1ee5b1bd3331d047f2e8bccffa076e7308981a7	adaptive regularized canonical correlations in clustering sensor data	correlation noise clustering algorithms minimization pollution measurement standards covariance matrices;pattern clustering;non stationary data adaptive canonical correlation analysis sparsity;non stationary data;numerical test adaptive regularized canonical correlation scheme sensor data clustering sensor measurements adaptive clustering information content sparsity inducing regularization exponential weighing nonstationary settings distributed recursions coordinate descent techniques alternating direction method multipliers;adaptive;sparsity;canonical correlation analysis	A regularized canonical correlations scheme is proposed for adaptive clustering of sensor measurements according to their information content. A novel framework utilizing sparsity-inducing regularization and exponential weighing is designed to deal with nonstationary settings. Distributed recursions to minimize the proposed formulation are put forth by utilizing coordinate descent techniques combined with the alternating direction method of multipliers. Numerical tests demonstrate that the novel adaptive clustering framework is capable to deal with nonstationary settings while outperforming existing alternatives.	augmented lagrangian method;centralized computing;cluster analysis;coordinate descent;numerical method;recursion;self-information;sensor;sparse matrix;time complexity	Jia Chen;Ioannis D. Schizas	2014	2014 48th Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2014.7094738	correlation clustering;mathematical optimization;machine learning;mathematics;cluster analysis;statistics	AI	30.654417788248505	-34.668390931508156	75069
5f2c7af65a94ed5d875b26d686a10804b48b2868	multivariable regression model building by using fractional polynomials: description of sas, stata and r programs	non linear functional;metodo polinomial;ajustamiento modelo;function selection;corresponding author;ucl;51e24;analisis datos;modele mal specifie;selection variable;funcional no lineal;selection of variables;discovery;regression model;theses;conference proceedings;polynomial;statistical regression;linear functional;62jxx;fonctionnelle non lineaire;misspecified model;linear functionals;funcion escala;ajustement modele;variable selection;multivariable model building;62f07;data analysis;modelo regresion;digital web resources;fonction echelon;model building;polynomial method;ucl discovery;regresion estadistica;step function;modele regression;polinomio;model matching;open access;statistical computation;calculo estadistico;non linearite;no linealidad;analyse donnee;ucl library;nonlinearity;calcul statistique;book chapters;open access repository;funcional lineal;fractional polynomials;regression statistique;methode polynomiale;polynome;programs;multivariate regression;fonctionnelle lineaire;ucl research	In fitting regression models data analysts are often faced with many predictor variables which may influence the outcome. Several strategies for selection of variables to identify a subset of ‘important’ predictors are available for many years. A further issue to model building is how to deal with nonlinearity in the relationship between outcome and a continuous predictor. Traditionally, for such predictors either a linear functional relationship or a step function after grouping is assumed. However, the assumption of linearity may be incorrect, leading to a misspecified final model. For multivariable model building a systematic approach to investigate possible non-linear functional relationships based on fractional polynomials and the combination with backward elimination was proposed recently. So far a program was only available in Stata, certainly preventing a more general application of this useful procedure. The approach will be introduced, advantages will be shown in two examples, a new approach to present FP functions will be illustrated and a macro in SAS will be shortly introduced. Differences to Stata and R programs are noted. © 2005 Elsevier B.V. All rights reserved.	branch predictor;kerrison predictor;nonlinear system;polynomial;r language;sas;stata;stepwise regression	Willi Sauerbrei;C. Meier-Hirmer;A. Benner;Patrick Royston	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2005.07.015	econometrics;nonlinear system;mathematics;feature selection;algorithm;regression analysis;statistics	AI	34.61800040378102	-24.53063983158662	75106
f3c2eb1af5dc78c403a499b7eaa883f825ec9e22	feature extraction and sufficient statistics in detection and classification	sufficient statistic;neural networks;neural nets;probability density function;signal detection;feature extraction statistics statistical distributions statistical analysis testing probability density function equations training data neural networks computer vision;empirical method;testing;neural network classifier;computer vision;linear network classifiers sufficient statistic feature extraction signal detection signal classification empirical method training data performance enhancement real world data neural network classifiers;training data;real world data;linear network analysis;statistical distributions;statistical analysis;neural nets signal detection feature extraction statistical analysis linear network analysis;feature extraction;linear network classifiers;signal classification;statistics;performance enhancement;neural network classifiers	The effectiveness of sufficient statistics as features in the detectiodclassification process is studied. The concept of a sufficient statistic [ 1-31 is reviewed and an empirical method of developing an ‘apparent’ sufficient statistic from training data is offered. Examples of the performance enhancement achieved when using such statistics on real world data in both linear and neural network classifiers are given.	artificial neural network;feature extraction	Edward C. Real	1996		10.1109/ICASSP.1996.550519	probability distribution;training set;probability density function;sufficient statistic;feature extraction;computer science;machine learning;pattern recognition;data mining;mathematics;software testing;empirical research;artificial neural network;statistics;detection theory	ML	33.2871680018774	-32.66157803673781	75688
c2086934b5503eb772484509f60ca75284fb1eab	process monitoring and fault detection based on multivariate statistical projection analysis	chemical industry;performance monitoring;normal distribution;latent variable;confidence limit;statistical process control;independent component analysis;satisfiability;principal component analysis process monitoring fault detection multivariate statistical projection analysis multivariate statistical process control chemical process normal distribution independent component analysis;density estimation;process monitoring;principal component analysis;fault detection;multivariate statistics;multivariable control systems;density functional;chemical industry process monitoring fault location statistical process control multivariable control systems normal distribution independent component analysis principal component analysis;multivariate statistical process control;fault diagnosis;monitoring fault detection independent component analysis data mining gaussian distribution principal component analysis process control chemical processes density functional theory statistical distributions;fault location	Multivariate statistical process control (MSPC) has been applied to performance monitoring for chemical process. However, conventional methods of MSPC are based on the premise that the extracted latent variables must be subjected to normal distribution, which often can't be satisfied. In this paper, a new method based on independent component analysis (ICA) and principal component analysis (PCA) is presented for process performance monitoring, in which a two-step procedure is employed. At first step, process operating information with non-normal distribution is extracted by means of ICA, and the density function of this part of information is estimated by means of parzen density estimator for calculating the confidence limits. At the second step, the information with normal distribution is extracted from the underlying residual data sets by PCA, and the confidence limits are determined on Q and hotelling T/sup 2/ statistics. With the primary advantage of which no assumption of normal distribution on process data sets is needed, the proposed method is applied to a double-effect evaporator, and the simulation results verify it effective.	fault detection and isolation;independent computing architecture;independent component analysis;latent variable;principal component analysis;process architecture;simulation	Guo-jin Chen;Jun Liang;Ji-xin Qian	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1400742	normal distribution;latent variable;independent component analysis;econometrics;multivariate statistics;chemical industry;density estimation;confidence interval;computer science;pattern recognition;mathematics;fault detection and isolation;statistical process control;statistics;satisfiability;principal component analysis	Robotics	30.224727970050328	-24.50800926303445	75773
091a54b737136dce4da6aef9faafc353af93b890	crack propagation assessment for spur gears using model-based analysis and simulation	discrete wavelet transform;spur gears;crack propagation;vibration response simulation;root mean square;gearbox	Model-based gear dynamic analysis and simulation has been a promising way for developing effective gearbox vibration monitoring approaches. In this paper, based on the dynamic model of a one-stage gearbox with spur gears and one tooth crack, statistical indicators and the discrete wavelet transform (DWT) technique are investigated to identify effective and sensitive health indicators for reflecting the crack propagation level. The results suggest that the root mean square (RMS) indicator is a better statistical indicator than the Kurtosis indicator to reflect the crack propagation in the early stage; the RMS indicator based on the residual signal segments that are strongly affected by the crack is more sensitive; the proposed DWT approach can improve the sensitivity of the RMS indicator, and the RMS indicator becomes more sensitive with the increase of the DWT level up to a best DWT level, beyond which either the monotonicity is lost or the sensitivity decreases; the proposed approach is effective with the presence of noise; with the increase of the noise level, the DWT level at which the best performance is achieved, and thus the sensitivity, decreases. Gearbox systems with different sizes and different input shaft frequencies are also investigated, and it is found that the observations presented above hold for different gearbox system settings.	discrete wavelet transform;mathematical model;mean squared error;noise (electronics);signal processing;simulation;software propagation	Zhigang Tian;Ming Jian Zuo;Siyan Wu	2012	J. Intelligent Manufacturing	10.1007/s10845-009-0357-8	fracture mechanics;structural engineering;electronic engineering;root mean square;transmission;computer science;engineering;forensic engineering;discrete wavelet transform	AI	38.1570805663472	-30.303847929177216	75821
0e2faf9f6b809cf609602476e41b42be211b45d2	preconditioned temporal difference learning	temporal difference;efficient algorithm;artificial intelligent;science learning;policy evaluation;least square;condition number;asymptotic properties;stochastic model;temporal difference learning;absorbing markov chain;markov chain	This paper extends many of the recent popular policy evaluation algorithms to a generalized framework that includes least-squares temporal difference (LSTD) learning, least-squares policy evaluation (LSPE) and a variant of incremental LSTD (iLSTD). The basis of this extension is a preconditioning technique that solves a stochastic model equation. This paper also studies three significant issues of the new framework: it presents a new rule of step-size that can be computed online, provides an iterative way to apply preconditioning, and reduces the complexity of related algorithms to near that of temporal difference (TD) learning.	algorithm;incremental compiler;iterative method;least squares;preconditioner;temporal difference learning	Hengshuai Yao;Zhi-Qiang Liu	2008		10.1145/1390156.1390308	temporal difference learning;markov chain;mathematical optimization;computer science;artificial intelligence;stochastic modelling;theoretical computer science;machine learning;condition number;mathematics;absorbing markov chain;least squares;statistics	ML	25.528967007684678	-31.393106125718074	75916
c15e7708b9aaee4b8ca80272b73adec7eb61a7a4	bayesian anisotropic gaussian model for audio source separation		In audio source separation applications, it is common to model the sources as circular-symmetric Gaussian random variables, which is equivalent to assuming that the phase of each source is uniformly distributed. In this paper, we introduce an anisotropic Gaussian source model in which both the magnitude and phase parameters are modeled as random variables. In such a model, it becomes possible to promote a phase value that originates from a signal model and to adjust the relative importance of this underlying model-based phase constraint. We conduct Bayesian inference of the model through the derivation of an expectation-maximization algorithm for estimating the parameters. Experiments conducted on realistic music songs for a monaural source separation task, in an scenario where the variance parameters are assumed known, show that the proposed approach outperforms state-of-the-art techniques.	bayesian network;expectation–maximization algorithm;open-source software;source separation	Paul Magron;Tuomas Virtanen	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461741	mathematical optimization;magnitude (mathematics);expectation–maximization algorithm;gaussian network model;gaussian;computer science;random variable;source separation;bayesian inference;monaural	Robotics	33.50909469012116	-27.400938546426584	75996
0e040d13fcc6389c64ebf7afbf16f535a47c37cd	a flexible modeling framework for coupled matrix and tensor factorizations	unconstrained all at once optimization algorithms coupled matrix and tensor factorization problem flexible modeling framework data fusion cmtf problem general purpose optimization solver constrained optimization problem all factor matrices nonlinear objective function;tensors data mining matrix decomposition optimisation sensor fusion;tensile stress data models optimization data integration nuclear magnetic resonance chemicals joints;snopt data fusion tensor factorizations nonlinear optimization nonlinear constraints	Joint analysis of data from multiple sources has proved useful in many disciplines including metabolomics and social network analysis. However, data fusion remains a challenging task in need of data mining tools that can capture the underlying structures from multi-relational and heterogeneous data sources. In order to address this challenge, data fusion has been formulated as a coupled matrix and tensor factorization (CMTF) problem. Coupled factorization problems have commonly been solved using alternating methods and, recently, unconstrained all-at-once optimization algorithms. In this paper, unlike previous studies, in order to have a flexible modeling framework, we use a general-purpose optimization solver that solves for all factor matrices simultaneously and is capable of handling additional linear/nonlinear constraints with a nonlinear objective function. We formulate CMTF as a constrained optimization problem and develop accurate models more robust to overfactoring. The effectiveness of the proposed modeling/algorithmic framework is demonstrated on simulated and real data.	algorithm;constrained optimization;constraint (mathematics);data mining;general-purpose modeling;mathematical optimization;metabolomics;nonlinear system;optimization problem;social network analysis;solver	Evrim Acar;Mathias Nilsson;Michael Saunders	2014	2014 22nd European Signal Processing Conference (EUSIPCO)		mathematical optimization;combinatorics;machine learning;mathematics	ML	27.93073693426282	-33.70334961417439	76014
6d937532ad3317441a6cb34ca7597e3aec6ba62e	occupancy grid mapping with markov chain monte carlo gibbs sampling	probability;occupancy grid mapping methods sensor measurement error mcmc computational efficiency occupancy probability mobile robots markov chain monte carlo gibbs sampling;mobile robots;slam robots markov processes mobile robots monte carlo methods probability sampling methods;markov processes;sampling methods;robot sensing systems mathematical model uncertainty markov processes computational modeling equations bayes methods;slam robots;monte carlo methods	Occupancy grids have been widely used for mapping with mobile robots for nearly 30 years. Occupancy grids discretize the analog environment and seek to determine the occupancy probability of each cell. Traditional occupancy grid mapping methods make two assumptions for computational efficiency and it has been shown that the full posterior is computationally intractable without these assumptions. This paper employs a form of Markov Chain Monte Carlo (MCMC) known as Gibbs sampling to sample from the full posterior. By drawing many samples, we are able to capture the full posterior, which more accurately represents the uncertainty in the map due to sensor measurement error. The MCMC method is shown to compute the full posterior in a 1D toy example, and it is shown to be computationally tractable, though not online, for realistic 2D simulations.	benchmark (computing);cell (microprocessor);cobham's thesis;computation;computational complexity theory;discretization;facebook platform;gibbs sampling;ground truth;internationalization and localization;map;markov chain monte carlo;mobile robot;monte carlo method;online algorithm;polynomial;sampling (signal processing);simulation;time complexity	Rehman S. Merali;Tim D. Barfoot	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6631020	mobile robot;sampling;econometrics;simulation;hybrid monte carlo;markov chain monte carlo;computer science;slice sampling;occupancy grid mapping;probability;monte carlo molecular modeling;rejection sampling;markov process;markov model;statistics;monte carlo method	Robotics	37.826089379075455	-26.744992947052527	76026
922ce12f06eb581212cd752f35d668ded7b50915	wavelet shrinkage with double weibull prior	bayes estimation;double weibull distribution;wavelet shrinkage;larger posterior mode;nonparametric regression;primary 62;secondary 62g08	In this paper we propose a denoising methodology in the wavelet domain based on a Bayesian hierarchical model using Double Weibull prior. We propose two estimators, one based on posterior mean (DWWS ) and the other based on larger posterior mode (DWWSLPM ), and show how to calculate them efficiently. Traditionally, mixture priors have been used for modeling sparse wavelet coefficients. The interesting feature of this paper is the use of non-mixture prior. We show that the methodology provides good denoising performance, comparable even to state-of-the-art methods that use mixture priors and empirical Bayes setting of hyperparameters, which is demonstrated by extensive simulations on standardly used test functions. An application to real-word data set is also considered.	approximation;bayesian network;coefficient;computer simulation;distribution (mathematics);dyadic transformation;hierarchical database model;longest prefix match;matlab;noise reduction;sparse matrix;wavelet	Norbert Reményi;Brani Vidakovic	2015	Communications in Statistics - Simulation and Computation	10.1080/03610918.2013.765470	econometrics;mathematics;nonparametric regression;statistics	ML	30.123184099708997	-25.825079219051545	76237
bd5d2efb76ccb090213a8d5d72095fbff3e7d317	model-based clustering and classification of functional data		The problem of complex data analysis is a central topic of modern statistical science and learning systems and is becoming of broader interest with the increasing prevalence of highdimensional data. The challenge is to develop statistical models and autonomous algorithms that are able to acquire knowledge from raw data for exploratory analysis, which can be achieved through clustering techniques or to make predictions of future data via classification (i.e., discriminant analysis) techniques. Latent data models, including mixture model-based approaches are one of the most popular and successful approaches in both the unsupervised context (i.e., clustering) and the supervised one (i.e, classification or discrimination). Although traditionally tools of multivariate analysis, they are growing in popularity when considered in the framework of functional data analysis (FDA). FDA is the data analysis paradigm in which the individual data units are functions (e.g., curves, surfaces), rather than simple vectors. In many areas of application, including signal and image processing, functional imaging, bio-informatics, etc., the analyzed data are indeed often available in the form of discretized values of functions or curves (e.g., time series, waveforms) and surfaces (e.g., 2d-images, spatio-temporal data). This functional aspect of the data adds additional difficulties compared to the case of a classical multivariate (non-functional) data analysis. We review and present approaches for model-based clustering and classification of functional data. We derive well-established statistical models along with efficient algorithmic tools to address problems regarding the clustering and the classification of these high-dimensional data, including their heterogeneity, missing information, and dynamical hidden structure. The presented models and algorithms are illustrated on real-world functional data analysis problems from several application area. Normandie Univ, UNICAEN, UMR CNRS LMNO, Department of Mathematics and Computer Science, 14000 Caen, France Department of Mathematics and Statistics, La Trobe University, 3086 Bundoora, Victoria Australia.	algorithm;autonomous robot;bioinformatics;british informatics olympiad;cluster analysis;computer science;data model;discretization;functional data analysis;functional imaging;image processing;linear algebra;linear discriminant analysis;mixture model;programming paradigm;statistical model;time series;vector graphics;victoria (3d figure)	Faicel Chamroukhi;Hien D. Nguyen	2018	CoRR		raw data;mixture model;machine learning;data mining;artificial intelligence;linear discriminant analysis;functional data analysis;data modeling;computer science;cluster analysis;complex data type;statistical model	ML	31.512212549661278	-35.98286939702297	76295
0bdfe0cd9d60a0cc69cea1cc77e852ba44c387f3	inference of time-varying gene networks using constrained and smoothed kalman filtering	time varying networks biology computing cellular biophysics estimation theory genetics kalman filters smoothing methods state estimation time varying filters;forward backward smoothing procedure time varying gene networks constrained kalman filtering smoothed kalman filtering undersampled observation noisy observation cell condition environmental condition genetic profiles dynamic biological processes cell development cancer progression treatment recovery genetic interactions state space framework simultaneous compression state estimation sparsity property compressed kalman estimate	This paper tackles the problem of recovering time-varying gene networks from a series of undersampled and noisy observations. Gene regulatory networks evolve over time in response to functional requirements in the cell and environmental conditions. Collected genetic profiles from dynamic biological processes, such as cell development, cancer progression and treatment recovery, underlie genetic interactions that rewire over the course of time. We formulate the problem of estimating time-varying networks in a state-space framework. We show that, due to the small number of measurements, the system is unobservable; thus making the application of the standard Kalman filter ineffective. We remedy the problem by performing simultaneous compression and state estimation. The sparsity property of gene regulatory networks is incorporated as a constraint in the Kalman filter, leading to a compressed Kalman estimate and reducing the number of required observations for effective tracking of the network. Moreover, we improve the estimation accuracy by taking into account the entire sample set for each time instant estimate of the network through a forward-backward smoothing procedure. The proposed constrained and smoothed Kalman filter is shown to yield good tracking results for varying small and medium-size networks.	color gradient;extended kalman filter;functional requirement;gene regulatory network;interaction;kalman filter;nonlinear system;peterson's algorithm;simulation;smoothing;sparse matrix;state space;time-varying network;undersampling;vii	Ghulam Rasool;Nidhal Bouaynaya	2012	Proceedings 2012 IEEE International Workshop on Genomic Signal Processing and Statistics (GENSIPS)	10.1109/GENSIPS.2012.6507756	biology;econometrics;mathematical optimization;invariant extended kalman filter;fast kalman filter;mathematics;extended kalman filter;moving horizon estimation;statistics;alpha beta filter	Robotics	37.03243244441411	-27.398123307424356	76335
1d1a63dd6c7fbd9f9018b265ead45b8652944d5e	bayesian analysis of multivariate stable distributions using one-dimensional projections	multivariate stable distributions;bayesian inference;spectral measure;60e07;11k45;markov chain monte carlo;60e10	In this paper we take up Bayesian inference in general multivariate stable distributions. We exploit the representation of Matsui and Takemura (2009) for univariate projections, and the representation of the distributions in terms of their spectral measure. We present efficient MCMC schemes to perform the computations when the spectral measure is approximated discretely or, as we propose, by a normal distribution. Appropriate latent variables are introduced to implement MCMC. In relation to the discrete approximation, we propose efficient computational schemes based on the characteristic function.	approximation algorithm;bayesian network;characteristic function (convex analysis);computation;latent variable;markov chain monte carlo	Mike G. Tsionas	2016	J. Multivariate Analysis	10.1016/j.jmva.2015.09.005	econometrics;multivariate statistics;markov chain monte carlo;mathematics;bayesian statistics;bayesian inference;statistics	ML	30.77081385684403	-24.885318371520775	76479
6ad37e615e1fe538e2763ec59c8de0e3ac32ae0c	design and implementation of a power quality disturbance classifier: an ai approach	field programmable gate array;power quality;fuzzy logic;wavelet transform;design and implementation;artificial neural network	This paper presents a new intelligent system incorporating wavelet transform, artificial neural network and fuzzy logic to automate the classification of power quality disturbance. This novel and efficient method in hardware, based on FPGA technology showed improved performance over existing approaches for power quality disturbance detection and classification on six types of disturbances including sag, swell, transient, fluctuation, interruption and normal waveform. The approach obtained an average classification accuracy of 98.19%. The design was successfully implemented, tested and validated on Altera APEX EP20K200EBC652-1X FPGA utilizing 1209 logic cells and achieved a maximum frequency of 263.71 MHz.	electric power quality	Mamun Bin Ibne Reaz;Florence Choong;M. S. Sulaiman;Faisal Mohd-Yasin	2006	Journal of Intelligent and Fuzzy Systems		fuzzy logic;computer science;machine learning;artificial neural network;field-programmable gate array;wavelet transform	AI	36.27210231833383	-31.928195913811557	76711
3413af6c689eedb4fe3e7d6c5dc626647976307a	horizontally scalable submodular maximization		A variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization. Existing approaches for distributed submodular maximization have a critical drawback: The capacity – number of instances that can fit in memory – must grow with the data set size. In practice, while one can provision many machines, the capacity of each machine is limited by physical constraints. We propose a truly scalable approach for distributed submodular maximization under fixed capacity. The proposed framework applies to a broad class of algorithms and constraints and provides theoretical guarantees on the approximation factor for any available capacity. We empirically evaluate the proposed algorithm on a variety of data sets and demonstrate that it achieves performance competitive with the centralized greedy solution.	approximation;centralized computing;emulator;expectation–maximization algorithm;greedy algorithm;information security;machine learning;scalability;submodular set function	Mario Lucic;Olivier Bachem;Morteza Zadimoghaddam;Andreas Krause	2016			mathematical optimization;submodular set function;machine learning;mathematics	ML	25.239737063222023	-33.229470173775795	76748
d330071503637ec05a29b20083d194819bd88ddc	admm penalty parameter selection by residual balancing		Appropriate selection of the penalty parameter is crucial to obtaining good performance from the Alternating Direction Method of Multipliers (ADMM). While analytic results for optimal selection of this parameter are very limited, there is a heuristic method that appears to be relatively successful in a number of different problems. The contribution of this paper is to demonstrate that their is a potentially serious flaw in this heuristic approach, and to propose a modification that at least partially addresses it.	flaw hypothesis methodology;heuristic	Brendt Wohlberg	2017	CoRR		residual;mathematics;mathematical optimization;heuristic	ML	27.004322835107356	-35.005695105261225	76882
bcb1d170b45e5f93e86f65e9312a782bac54f584	enhanced online subspace estimation via adaptive sensing		This work investigates the problem of adaptive measurement design for online subspace estimation from compressive linear measurements. We study the previously proposed Grassmannian rank-one online subspace estimation (GROUSE) algorithm with adaptively designed compressive measurements. We propose an adaptive measurement scheme that biases the measurement vectors towards the current subspace estimate and prove a global convergence result for the resulting algorithm. Our experiments on synthetic data demonstrate the effectiveness of the adaptive measurement scheme over non-adaptive compressive random measurements.	algorithm;experiment;local convergence;synthetic data	Greg Ongie;David Hong;Dejiao Zhang;Laura Balzano	2017	2017 51st Asilomar Conference on Signals, Systems, and Computers	10.1109/ACSSC.2017.8335497	mathematical optimization;grassmannian;computer science;synthetic data;subspace topology;convergence (routing);data modeling	Metrics	25.667377448231978	-35.5842739164923	76940
02c1580617a9b1ccd809f06ae57773c00cf96647	online tensor methods for learning latent variable models	mixed membership stochastic blockmodel;large datasets;topic modeling;stochastic gradient descent;tensor method;parallel implementation	We introduce an online tensor decomposition based approach for two latent variable modeling problems namely, (1) community detection, in which we learn the latent communities that the social actors in social networks belong to, and (2) topic modeling, in which we infer hidden topics of text articles. We consider decomposition of moment tensors using stochastic gradient descent. We conduct optimization of multilinear operations in SGD and avoid directly forming the tensors, to save computational and storage costs. We present optimized algorithm in two platforms. Our GPU-based implementation exploits the parallelism of SIMD architectures to allow for maximum speed-up by a careful optimization of storage and data transfer, whereas our CPU-based implementation uses efficient sparse matrix computations and is suitable for large sparse datasets. For the community detection problem, we demonstrate accuracy and computational efficiency on Facebook, Yelp and DBLP datasets, and for the topic modeling problem, we also demonstrate good performance on the New York Times dataset. We compare our results to the state-of-the-art algorithms such as the variational method, and report a gain of accuracy and a gain of several orders of magnitude in the execution time.	algorithm;analysis of algorithms;calculus of variations;central processing unit;cloud computing;computation;data point;graphics processing unit;heuristic;ibm notes;iteration;iterative method;kohn–sham equations;latent variable;loss function;mathematical optimization;multigraph;parallel computing;randomized algorithm;run time (program lifecycle phase);simd;social network;software deployment;sparse matrix;spectral method;stochastic gradient descent;text corpus;the new york times;time complexity;topic model;variational method (quantum mechanics);variational principle;whitening transformation;eric	Furong Huang;U. N. Niranjan;Mohammad Umar Hakeem;Anima Anandkumar	2015	Journal of Machine Learning Research		computer science;theoretical computer science;machine learning;data mining;stochastic gradient descent;topic model;statistics	ML	25.96620101518205	-33.41080510774857	77300
7611d1df10e997ed2b80050274bfeeb54652405a	entropy-isomap: manifold learning for high-dimensional dynamic processes		Scientific and engineering processes deliver massive high-dimensional data sets that are generated as non-linear transformations of an initial state and few process parameters. Mapping such data to a low-dimensional manifold facilitates better understanding of the underlying processes, and enables their optimization. In this paper, we first show that off-theshelf non-linear spectral dimensionality reduction methods, e.g., Isomap, fail for such data, primarily due to the presence of strong temporal correlations. Then, we propose a novel method, Entropy-Isomap, to address the issue. The proposed method is successfully applied to large data describing a fabrication process of organic materials. The resulting lowdimensional representation correctly captures process control variables, allows for low-dimensional visualization of the material morphology evolution, and provides key insights to improve the process. Keywords-Large-scale Manifold Learning, Time Series, Dynamic Processes	algorithm;automatic identification and data capture;data point;etsi satellite digital radio;entropy (information theory);experiment;galaxy morphological classification;information exchange;isomap;latent variable;mathematical optimization;nonlinear dimensionality reduction;nonlinear system;numerical analysis;sampling (signal processing);semiconductor device fabrication;simulation;time series	Frank Schoeneman;Varun Chandola;Nils Napp;Olga Wodo;Jaroslaw Zola	2018	CoRR		artificial intelligence;machine learning;manifold;process control;data mining;isomap;computer science;visualization;dimensionality reduction;data set;nonlinear dimensionality reduction	AI	29.872904199290243	-35.67499185743045	77646
091b9453e56128430865b4674b562262c0afe80e	application of principal component analysis for mechanical coupling system modelling based on support vector machine	mechanical coupling systems;support vector machines;hydraulic valves;journal;characteristics prediction;machine learning;principal component analysis;svm;pca	This paper presents the results of a research into the application of principal component analysis (PCA) for the mechanical coupling system modelling based on support vector machine (SVM). Because of the impact of multiple geometric parameters, there are more input variables in the mechanical coupling system modelling process. The high-dimensional data poses an interesting challenge to machine learning, as the presence of high numbers of redundant or highly correlated variables can seriously degrade modelling accuracy. In this study, we use PCA as the preprocessor for mechanical coupling system modelling, so as to realise dimension reduction of the high-dimensional data and improve the predictive performance of machine learning method, and then SVM is used for mechanical coupling system modelling. Experiments are carried out on a typical mechanical coupling, hydraulic valve. The results show that the use of PCA method can improve the performance of machine learning method in the modelling of high-dimensio...	principal component analysis;support vector machine	Jian-Wei Ma;Fuji Wang;Zhenyuan Jia;Wei Liu	2011	IJMA	10.1504/IJMA.2011.040037	engineering;machine learning;pattern recognition;data mining	Robotics	36.96928696706275	-29.539078416903305	77919
72d1cef6b5f725924c9ef9056cd67d55b9b4beb1	method for extraction wavelet packets' coefficients in loudspeaker fault detection based on pca	neural nets;standard deviation;acoustics;wavelet transforms;artificial neural networks;packet node signals;principal component analysis acoustic signal detection fault diagnosis loudspeakers neural nets;loudspeaker fault detection;loudspeakers;wavelet packet transform;principal component analysis;fault detection;wavelet packets loudspeakers fault detection principal component analysis acoustic waves hardware fault diagnosis signal processing time frequency analysis data mining;acoustic signal detection;acoustics signal decomposition;correlation;wavelet packets;correlation coefficient;artificial neural network wavelet packet coefficients loudspeaker fault detection principal component analysis wavelet packet transformation acoustics signal decomposition packet node signals correlation coefficient;wavelet packet coefficients;wavelet packet transformation;artificial neural network;fault diagnosis	This paper presents a new method using principal component analysis (PCA) to eliminate data redundancy in loudspeaker fault detection. It uses wavelet packet transformation (WPT) to decompose the loudspeaker acoustics signal into 32 packet node signals. Then, get the mean, max, standard deviation and correlation coefficient of every node envelopment .With the way of observing, it gets 63 coefficients from 128 ones which are helpful for detection of fault. Using the new way above, 32 coefficients are removed from the 63. The failed loudspeaker can be found with the help of artificial neural network (ANN). It is proved that the method is very effective in experiment.	loudspeaker;wavelet packet decomposition	Hongxing Wang;Zengpu Xu;Congling Zhou;Lin Yang	2008		10.1109/PACIIA.2008.375	speech recognition;computer science;machine learning;wavelet packet decomposition;artificial neural network	EDA	37.36451119033105	-31.60258612575271	77955
03772cf602c28073518f3d0d24fe6e545cafad17	analyzing tensor power method dynamics in overcomplete regime	cs lg;stat ml	We present a novel analysis of the dynamics of tensor power iterations in the overcomplete regime where the tensor CP rank is larger than the input dimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in general. We consider the case where the tensor components are randomly drawn, and show that the simple power iteration recovers the components with bounded error under mild initialization conditions. We apply our analysis to unsupervised learning of latent variable models, such as multi-view mixture models and spherical Gaussian mixtures. Given the third order moment tensor, we learn the parameters using tensor power iterations. We prove it can correctly learn the model parameters when the number of hidden components k is much larger than the data dimension d, up to k = o(d). We initialize the power iterations with data samples and prove its success under mild conditions on the signal-to-noise ratio of the samples. Our analysis significantly expands the class of latent variable models where spectral methods are applicable. Our analysis also deals with noise in the input tensor leading to sample complexity result in the application to learning latent variable models.	latent variable;mixture model;np-hardness;power iteration;randomness;sample complexity;signal-to-noise ratio;spectral method;unsupervised learning	Anima Anandkumar;Rong Ge;Majid Janzamin	2017	Journal of Machine Learning Research		mathematical optimization;combinatorics;computer science;machine learning;calculus;mathematics;statistics	ML	26.676090420884226	-32.354752404845065	77994
7385ee5b902ebbcd81c1e72e913fd5ff4cc7f4a2	a modularized efficient framework for non-markov time series estimation		We present a compartmentalized approach to finding the maximum a posteriori (MAP) estimate of a latent time series that obeys a dynamic stochastic model and is observed through noisy measurements. We specifically consider modern signal processing problems with non-Markov signal dynamics (e.g., group sparsity) and/or non-Gaussian measurement models (e.g., point process observation models used in neuroscience). Through the use of auxiliary variables in the MAP estimation problem, we show that a consensus formulation of the alternating direction method of multipliers enables iteratively computing separate estimates based on the likelihood and prior and subsequently “averaging” them in an appropriate sense using a Kalman smoother. As such, this can be applied to a broad class of problem settings and only requires modular adjustments when interchanging various aspects of the statistical model. Under broad log-concavity assumptions, we show that the separate estimation problems are convex optimization problems and that the iterative algorithm converges to the MAP estimate. As such, this framework can capture non-Markov latent time series models and non-Gaussian measurement models. We provide example applications involving 1) group-sparsity priors, within the context of electrophysiologic specrotemporal estimation, and 2) non-Gaussian measurement models, within the context of dynamic analyses of learning with neural spiking and behavioral observations.	algorithm;augmented lagrangian method;concave function;convex optimization;estimation theory;iterative method;kalman filter;markov chain;mathematical optimization;motion estimation;point process;signal processing;sparse matrix;statistical model;time series	Gabriel Schamberg;Demba Ba;Todd P. Coleman	2018	IEEE Transactions on Signal Processing	10.1109/TSP.2018.2793870	stochastic modelling;iterative method;mathematical optimization;mathematics;convex optimization;signal processing;maximum a posteriori estimation;prior probability;statistical model;kalman filter	ML	30.077846163274426	-32.994497382887516	78074
7ed1e1c147896afc9a985cd11b8a1b0b5403ecd5	computing the least median of squares estimator in time o(nd)	metodo cuadrado menor;modelizacion;algoritmo aleatorizado;methode moindre carre;analisis estadistico;robust estimator;least squares method;least median of squares;robust statistics;computational geometry;outlier;plan randomise;algorithme randomise;probabilistic approach;mediane;median;identificacion sistema;modelisation;observacion aberrante;plan aleatorizado;statistical analysis;system identification;randomized design;enfoque probabilista;approche probabiliste;analyse statistique;estimacion parametro;randomized algorithm;observation aberrante;hiperplano;mediana;parameter estimation;estimation parametre;modeling;hyperplane;identification systeme;hyperplan	In modern statistics, the robust estimation of parameters of a regression hyperplane is a central problem, i. e., an estimation that is not or only slightly affected by outliers in the data. In this paper we will consider the least median of squares (LMS) estimator. For n points in d dimensions we describe a randomized algorithm for LMS running in O ( nd ) time and O(n) space, for d fixed, and in time O ( d3 · (2n)d) and O(dn) space, for arbitrary d.	donald becker;ingo wegener;least squares;randomized algorithm	Thorsten Bernholt	2005		10.1007/11424758_72	robust statistics;econometrics;mathematical optimization;computational geometry;mathematics;statistics	Theory	33.5511002961566	-24.731378450099328	78091
e8befadeebb8f0ed403b0a4fc3f1f18231b15330	fault classification for induction motor using hilbert-based bispectral analysis and probabilistic neural networks	supervised radial basis type neural network induction motor hilbert based bispectral analysis probabilistic neural networks signal processing approach hilbert transform rotor faults motor vibration systems principal component analysis vibration signatures;hilbert transforms;rotor faults;probability;vibrations;neural networks;neural nets;induction motor;training;signal detection;signal processing approach;radial basis function networks;hilbert transform;induction motors;feature extraction;signal processing;principal component analysis;supervised radial basis type neural network;probabilistic neural networks;vibrations electric machine analysis computing hilbert transforms induction motors neural nets principal component analysis probability radial basis function networks signal detection;motor vibration systems;vibration signatures;electric machine analysis computing;feature selection;probabilistic logic;probabilistic neural network;hilbert based bispectral analysis;induction motors vibrations training feature extraction probabilistic logic neural networks principal component analysis;neural network	This paper addresses the development of a new signal processing approach based on the fusion of Hilbert transform and bispectral analysis to extract features of defects in a number of induction motor conditions. The motor conditions considered are a normal motor and motors with outer, inner race and rotor faults. The signal processing techniques based on Hilbert transform have been used to extract the modulating components which are able to characterize the motor fault patterns. The use of bispectral analysis provides great capabilities for detection and characterization of nonlinearity in the motor vibration systems. Feature selection based on principal component analysis are used to extract from the vibration signatures so obtained and these features are used as inputs to probabilistic neural networks trained to identify the motor conditions. The results obtained show that the diagnostic system using a supervised radial basis type neural network is capable of classifying motor conditions with high accuracy recognition rate.	artificial neural network;bispectrum;feature selection;hilbert transform;nonlinear system;principal component analysis;r.o.t.o.r.;radial (radio);signal processing;type signature	D.-M. Yang	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019716	speech recognition;computer science;machine learning;pattern recognition;induction motor;artificial neural network	ML	37.057455725130616	-31.362987816735217	78244
0f4217089dadad7f06160aa5bfdf6ad3ced45bff	scalable label propagation for multi-relational learning on tensor product graph		Label propagation on the tensor product of multiple graphs can infer multi-relations among the entities across the graphs by learning labels in a tensor. However, the tensor formulation is only empirically scalable up to three graphs due to the exponential complexity of computing tensors. In this paper, we propose an optimization formulation and a scalable Lowrank Tensor-based Label Propagation algorithm (LowrankTLP). The optimization formulation minimizes the rank-k approximation error for computing the closed-form solution of label propagation on a tensor product graph with efficient tensor computations used in LowrankTLP. LowrankTLP takes either a sparse tensor of known multi-relations or pairwise relations between each pair of graphs as the input to infer unknown multirelations by semi-supervised learning on the tensor product graph. We also accelerate LowrankTLP with parallel tensor computation which enabled label propagation on a tensor product of 100 graphs of size 1000 within 150 seconds in simulation. LowrankTLP was also successfully applied to multi-relational learning for predicting author-paper-venue in publication records, alignment of several protein-protein interaction networks across species and alignment of segmented regions across up to 7 CT scan images. The experiments prove that LowrankTLP indeed well approximates the original label propagation with high scalability. Source code: https://github.com/kuanglab/LowrankTLP		Zhuliu Li;Raphael Petegrosso;Shaden Smith;David Sterling;George Karypis;Rui Kuang	2018	CoRR		tensor;mathematical optimization;source code;scalability;mathematics;pairwise comparison;approximation error;statistical relational learning;exponential function;tensor product	ML	25.854696651186533	-32.79717937513305	78247
7dd18106023f2fa5a76c3242b544db0857905829	understanding where your classifier does (not) work -- the scape model class for emm	loss measurement;telescopes protons terrestrial atmosphere data mining atmospheric modeling loss measurement radio frequency;cherenkov radiation;data mining;telescopes;exceptional model mining;radio frequency;technology and engineering;soft classifier;terrestrial atmosphere;search problems astronomical telescopes astronomy computing data mining monte carlo methods pattern classification probability;astrophysics;atmospheric modeling;astrophysical interpretation emm fact telescope g apd cherenkov telescope air shower detection high energetic cosmic particles shower classification gamma ray background particle monte carlo simulated data scape soft classifier performance evaluation model class exceptional model mining local pattern mining framework classifier probabilities binary column ranking loss search space;protons;soft classifier astrophysics exceptional model mining cherenkov radiation	FACT, the First G-APD Cherenkov Telescope, detects air showers induced by high-energetic cosmic particles. It is desirable to classify a shower as being induced by a gamma ray or a background particle. Generally, it is nontrivial to get any feedback on the real-life training task, but we can attempt to understand how our classifier works by investigating its performance on Monte Carlo simulated data. To this end, in this paper we develop the SCaPE (Soft Classifier Performance Evaluation) model class for Exceptional Model Mining, which is a Local Pattern Mining framework devoted to highlighting unusual interplay between multiple targets. In our Monte Carlo simulated data, we take as targets the computed classifier probabilities and the binary column containing the ground truth: which kind of particle induced the corresponding shower. Using a newly developed quality measure based on ranking loss, the SCaPE model class highlights subspaces of the search space where the classifier performs particularly well or poorly. These subspaces arrive in terms of conditions on attributes of the data, hence they come in a language a domain expert understands, which should aid him in understanding where his/her classifier does (not) work. Found subgroups highlight subspaces whose difficulty for classification is corroborated by astrophysical interpretation, as well as subspaces that warrant further investigation.	auditory processing disorder;cosmic;coherence (physics);data mining;experiment;gamma correction;ground truth;incidence matrix;monte carlo method;performance evaluation;random forest;real life;result set;runescape;subject-matter expert;whole earth 'lectronic link;worldwide telescope	Wouter Duivesteijn;Julia Thaele	2014	2014 IEEE International Conference on Data Mining	10.1109/ICDM.2014.10	atmospheric model;simulation;cherenkov radiation;data mining;radio frequency;proton	ML	36.16545129273101	-34.13740390622273	78412
af4a9949cf755e76077790b474e074c95cbad854	ensemble methods for structured prediction		We present a series of learning algorithms and theoretical guarantees for designing accurate ensembles of structured prediction tasks. This includes several randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones, and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels. We give a detailed study of all these algorithms, including the description of new on-line-to-batch conversions and learning guarantees. We also report the results of extensive experiments with these algorithms in several structured prediction tasks.	appendix h;directed graph;edit distance;experiment;loss function;online and offline;online machine learning;randomized algorithm;structured prediction	Corinna Cortes;Vitaly Kuznetsov;Mehryar Mohri	2014			natural language processing;speech recognition;computer science;machine learning	ML	24.691788356256726	-31.989859628875674	78429
0903fc3687f93861462aa12baba2dbc22ee7c054	learning hmms with nonparametric emissions via spectral decompositions of continuous matrices		Recently, there has been a surge of interest in using spectral methods for estimating latent variable models. However, it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family. In this paper, we study the estimation of an m-state hidden Markov model (HMM) with only smoothness assumptions, such as Hölderian conditions, on the emission densities. By leveraging some recent advances in continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric HMMs. Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as continuous matrices. We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices. We implement our method using Chebyshev polynomial approximations. Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient.	algorithm;algorithmic efficiency;approximation;baseline (configuration management);chebyshev polynomials;hidden markov model;latent variable;linear algebra;markov chain;mixture model;numerical analysis;observable;perturbation theory;polynomial;sample complexity;singular value decomposition;spectral method;synthetic intelligence	Kirthevasan Kandasamy;Maruan Al-Shedivat;Eric P. Xing	2016			econometrics;mathematical optimization;machine learning;mathematics;statistics	ML	28.054521328646665	-30.11716385826256	78440
64a3690992cdecf992ba296f3aaf847de6b97c10	on inclusion-driven learning of bayesian networks	heuristic search;search strategy;graphical markov model inclusion;bayesian network;conditional independency;bayesian networks;inclusion-driven learning;partial order;new learning algorithm;inclusion boundary condition;inclusion boundary;search space;bayesian network structure;inclusion order;structure learning;conditional independence;boundary condition;satisfiability;probability distribution;difference set	Two or more Bayesian network structures are Markov equivalent when the corresponding acyclic digraphs encode the same set of conditional independencies. Therefore, the search space of Bayesian network structures may be organized in equivalence classes, where each of them represents a different set of conditional independencies. The collection of sets of conditional independencies obeys a partial order, the so-called “inclusion order.” This paper discusses in depth the role that the inclusion order plays in learning the structure of Bayesian networks. In particular, this role involves the way a learning algorithm traverses the search space. We introduce a condition for traversal operators, the inclusion boundary condition, which, when it is satisfied, guarantees that the search strategy can avoid local maxima. This is proved under the assumptions that the data is sampled from a probability distribution which is faithful to an acyclic digraph, and the length of the sample is unbounded. The previous discussion leads to the design of a new traversal operator and two new learning algorithms in the context of heuristic search and the Markov Chain Monte Carlo method. We carry out a set of experiments with synthetic and real-world data that show empirically the benefit of striving for the inclusion order when learning Bayesian networks from data.	algorithm;approximation algorithm;bayesian network;climber (beam);courant–friedrichs–lewy condition;directed acyclic graph;directed graph;encode;experiment;exponent bias;graphical user interface;heuristic;machine learning;markov chain monte carlo;markov model;maxima and minima;monte carlo method;rate of convergence;synthetic data;synthetic intelligence;tree traversal;turing completeness	Robert Castelo;Tomas Kocka	2003	Journal of Machine Learning Research		partially ordered set;probability distribution;bayesian average;mathematical optimization;combinatorics;heuristic;conditional independence;variable-order bayesian network;boundary value problem;computer science;machine learning;causal markov condition;bayesian network;mathematics;graphical model;bayesian statistics;difference set;statistics;satisfiability	AI	25.551557620287166	-27.840635111339086	78598
04f16a93a932cd6bef27fe4f03c4c9e7d5399ddd	jump-means: small-variance asymptotics for markov jump processes		Markov jump processes (MJPs) are used to model a wide range of phenomena from disease progression to RNA path folding. However, maximum likelihood estimation of parametric models leads to degenerate trajectories and inferential performance is poor in nonparametric models. We take a small-variance asymptotics (SVA) approach to overcome these limitations. We derive the small-variance asymptotics for parametric and nonparametric MJPs for both directly observed and hidden state models. In the parametric case we obtain a novel objective function which leads to non-degenerate trajectories. To derive the nonparametric version we introduce the gamma-gamma process, a novel extension to the gamma-exponential process. We propose algorithms for each of these formulations, which we call JUMP-means. Our experiments demonstrate that JUMP-means is competitive with or outperforms widely used MJP inference approaches in terms of both speed and reconstruction accuracy.	algorithm;color gradient;experiment;inferential theory of learning;loss function;markov chain;modernist journals project;optimization problem;time complexity	Jonathan H. Huggins;Kumaravelu Narasimhan;Ardavan Saeedi;Vikash K. Mansinghka	2015			econometrics;mathematical optimization;mathematics;statistics	ML	27.556778656588207	-28.780022766696316	78943
b900dc6e45da047da8224cddf919fdc09edcce23	sparse regression with highly correlated predictors		We consider a linear regression y = Xβ + u where X ∈ Rn×p, p n, and β is s−sparse. Motivated by examples in financial and economic data, we consider the situation where X has highly correlated and clustered columns. To perform sparse recovery in this setting, we introduce the clustering removal algorithm (CRA), that seeks to decrease the correlation in X by removing the cluster structure without changing the parameter vector β. We show that as long as certain assumptions hold about X, the decorrelated matrix will satisfy the restricted isometry property (RIP) with high probability. We also provide examples of the empirical performance of CRA and compare it with other sparse recovery techniques.	algorithm;algorithmic efficiency;cluster analysis;column (database);compressed sensing;credit bureau;experiment;newton's method;regular language description for xml;restricted isometry property;sparse matrix;the matrix;with high probability;xslt/muenchian grouping	Behrooz Ghorbani;Özgür Yilmaz	2015	CoRR		econometrics;mathematical optimization;mathematics;statistics	ML	27.76099015568256	-34.96432957362702	78944
be61109967d1f0dfbe9392b3b46faf45a48eeb46	using a non-commutative bernstein bound to approximate some matrix algorithms in the spectral norm	non commutative;linear regression;matrix multiplication;sparse matrix;data structure;spectral norm	We focus on row sampling based approximations for matrix algorithms, in particular matrix multipication, sparse matrix reconstruction, and l2 regression. For A ∈ R (m points in d ≪ m dimensions), and appropriate row-sampling probabilities, which typically depend on the norms of the rows of the m × d left singular matrix of A (the leverage scores), we give row-sampling algorithms with linear (up to polylog factors) dependence on the stable rank of A. This result is achieved through the application of non-commutative Bernstein bounds.	algorithm;approximation;phil bernstein;sampling (signal processing);sparse matrix	Malik Magdon-Ismail	2011	CoRR		mathematical optimization;combinatorics;discrete mathematics;data structure;sparse matrix;matrix multiplication;linear regression;matrix norm;mathematics;algebra	ML	25.699168939495813	-35.20829003739956	79113
0052d0cd88442c794155cf01d9e4d64711cf65a1	a method based on total variation for network modularity optimization using the mbo scheme	community detection;91d30;graphs;94c15;data clustering;91c20;62h30;mbo scheme;social networks;modularity	The study of network structure is pervasive in sociology, biology, computer science, and many other disciplines. One of the most important areas of network science is the algorithmic detection of cohesive groups of nodes called “communities”. One popular approach to find communities is to maximize a quality function known as modularity to achieve some sort of optimal clustering of nodes. In this paper, we interpret the modularity function from a novel perspective: we reformulate modularity optimization as a minimization problem of an energy functional that consists of a total variation term and an `2 balance term. By employing numerical techniques from image processing and `1 compressive sensing—such as convex splitting and the Merriman-Bence-Osher (MBO) scheme—we develop a variational algorithm for the minimization problem. We present our computational results using both synthetic benchmark networks and real data.	adjacency matrix;algorithm;algorithmic efficiency;benchmark (computing);calculus of variations;cluster analysis;compressed sensing;computation;computer science;dhrystone;expanded memory;ground truth;image processing;iteration;laplacian matrix;mnist database;mathematical optimization;modularity (networks);network science;numerical analysis;pervasive informatics;rayleigh–ritz method;semi-supervised learning;sparse matrix;time complexity;two-phase commit protocol;unsupervised learning;γ-convergence	Huiyi Hu;Thomas Laurent;Mason A. Porter;Andrea L. Bertozzi	2013	SIAM Journal of Applied Mathematics	10.1137/130917387	mathematical optimization;combinatorics;modularity;modularity;mathematics;cluster analysis;graph;social network	ML	26.773590190515247	-37.98735601275975	79210
043849fe787415b8a5022139334eb531f2fb5621	bayesian false discovery rate wavelet shrinkage: theory and applications	shrinkage 65t60;transformation ondelette;test hypothese;bayes estimation;false discovery rate;metodo estadistico;analyse multivariable;relational data;analisis numerico;test statistique;ridge regression;covariance analysis;62h15;aplicacion;multivariate analysis;loi probabilite;ley probabilidad;regresion ridge;relacion orden;ondelette;test hipotesis;test estadistico;simulacion numerica;estimation non parametrique;variance analysis;bayes factor;statistical test;shrinkage estimator;ordering;statistical method;65t60;posterior probability;statistical regression;wavelet shrinkage;analyse numerique;estimation parametrique;non parametric estimation;relation ordre;shrinkage;estimacion bayes;regression pseudo orthogonale;numerical analysis;posterior distribution;42c40;analyse covariance;estimateur retrecissement;62g10;510 matematica;methode statistique;analisis variancia;regresion estadistica;bayesian local false discovery rate;62f15;probability distribution;simulation numerique;probabilite a posteriori;62j10;62f03;probabilidad a posteriori;ley a posteriori;multiple hypothesis testing;facteur bayes;statistical inference;analisis multivariable;scientific communication;multiple hypotheses testing;technical report;transformacion ondita;estimacion no parametrica;analisis covariancia;estimation statistique;regression statistique;application;62j07;estimacion estadistica;loi a posteriori;statistical estimation;wavelets;wavelet transformation;analyse variance;estimation bayes;variance;variancia;numerical simulation;massive data sets;hypothesis test	Statistical inference in the wavelet domain remains vibrant area of contemporary statistical research because desirable properties of wavelet representations and the need of scientific community to process, explore, and summarize massive data sets. Prime examples are biomedical, geophysical, and internet related data. In this paper we develop wavelet shrinkage methodology based on testing multiple hypotheses in the wavelet domain. The shrinkage/thresholding approach by implicit or explicit simultaneous testing of many hypotheses had been considered by many researchers and goes back to the early 1990’s. Even the early proposal, the universal thresholding, could be interpreted as a test of multiple hypotheses in the wavelet domain. We propose two new approaches to wavelet shrinkage/thresholding. (i) In the spirit of Efron and Tibshirani’s recent work on local false discovery rate, we propose the theoretical counterpart Bayesian Local False Discovery Rate, BLFDR, where the underlying model assumes unknown variances. This approach to wavelet shrinkage can also be connected with shrinkage based on Bayes factors. (ii) The second proposal to wavelet shrinkage explored in this paper is Bayesian False Discovery Rate, BaFDR. This proposal is based on ordering of posterior probabilities of hypotheses in Bayesian testing of multiple hypotheses. We demonstrate that both approaches result in a competitive shrinkage methods by contrasting them to some popular shrinkage techniques.	bayesian network;internet;naive bayes classifier;thresholding (image processing);wavelet	Ilya Lavrik;Yoon Young Jung;Fabrizio Ruggeri;Brani Vidakovic	2008	Communications in Statistics - Simulation and Computation	10.1080/03610910802049649	computer simulation;econometrics;statistical hypothesis testing;mathematics;posterior probability;statistics	ML	32.4254279464418	-24.41431946066738	79525
3e708edb6ce8feccc127ba6cd20f62affa12c604	variational consensus monte carlo		Practitioners of Bayesian statistics have long depended on Markov chain Monte Carlo (MCMC) to obtain samples from intractable posterior distributions. Unfortunately, MCMC algorithms are typically serial, and do not scale to the large datasets typical of modern machine learning. The recently proposed consensus Monte Carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel [22]. A fixed aggregation function then combines these samples, yielding approximate posterior samples. We introduce variational consensus Monte Carlo (VCMC), a variational Bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target. The resulting objective contains an intractable entropy term; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions. We illustrate the advantages of our algorithm on three inference tasks from the literature, demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step. Our algorithm achieves a relative error reduction (measured against serial MCMC) of up to 39% compared to consensus Monte Carlo on the task of estimating 300-dimensional probit regression parameter expectations; similarly, it achieves an error reduction of 92% on the task of estimating cluster comembership probabilities in a Gaussian mixture model with 8 components in 8 dimensions. Furthermore, these gains come at moderate cost compared to the runtime of serial MCMC—achieving near-ideal speedup in some instances.	aggregate function;algorithmic efficiency;approximation algorithm;approximation error;calculus of variations;concave function;latent variable;linear programming relaxation;machine learning;markov chain monte carlo;mathematical optimization;mixture model;monte carlo algorithm;monte carlo method;multimodal interaction;overhead (computing);probit model;sampling (signal processing);speedup;variational principle	Maxim Rabinovich;Elaine Angelino;Michael I. Jordan	2015			econometrics;mathematical optimization;hybrid monte carlo;markov chain monte carlo;machine learning;mathematics;statistics;monte carlo method	ML	25.610869861735704	-32.27721953927402	79595
e39e338c9317f4a62f7340680cb8107f9550de1a	practical privacy for expectation maximization		Expectation maximization (EM) is an iterative algorithm that computes maximum likelihood and maximum a posteriori estimates for models with unobserved variables. While widely used, the iterative nature of EM presents challenges for privacy-preserving estimation. Multiple iterations are required to obtain accurate parameter estimates, yet each iteration increases the amount of noise that must be added to achieve a reasonable degree of privacy. We propose a practical algorithm that overcomes this challenge and outputs EM parameter estimates that are both accurate and private. Our algorithm focuses on the frequent use case of models whose joint distribution over observed and unobserved variables remains in the exponential family. For these models, the EM parameters are functions of moments of the data. Our algorithm leverages this to preserve privacy by perturbing the moments, for which the amount of additive noise scales naturally with the data. In addition, our algorithm uses a relaxed notion of the differential privacy (DP) gold standard, called concentrated differential privacy (CDP). Rather than focusing on single-query loss, CDP provides high probability bounds for cumulative privacy loss, which is well suited for iterative algorithms. For mixture models, we show that our method requires a significantly smaller privacy budget for the same estimation accuracy compared to both DP and its ( , δ)-DP relaxation. Our general approach of moment perturbation equipped with CDP can be readily extended to many iterative machine learning algorithms, which opens up various exciting future directions.	additive white gaussian noise;differential privacy;expectation–maximization algorithm;iteration;linear programming relaxation;machine learning;mixture model;time complexity;utility functions on indivisible goods	Mijung Park;Jimmy Foulds;Kamalika Chaudhuri;Max Welling	2016	CoRR		econometrics;mathematical optimization;mathematics;statistics	ML	26.817497464317928	-31.56307260284081	79909
8d5f9cf3d97ec0538add0b5af63a6a5781d767bb	a cluster analysis of vote transitions	bayesian hierarchical model;spatial data;bayesian model checking;ecological inference;election data;article	To help settle the debate triggered the day after any election around the origin and destination of the vote of winners and losers, a Bayesian analysis of the results in a pair of consecutive elections is proposed. It is based on a model that simultaneously carries out a cluster analysis of the areas in which the results are broken into and links the results in the two elections of areas in a given cluster through a vote switch matrix. The number of clusters is chosen both through predictive checks as well as by testing whether the residuals are spatially correlated or not. The analysis is tried on the results in Barcelona of a pair of consecutive elections held just four months apart, in 2003 for the Catalan parliament and in 2004 for the Spanish parliament. The proposed approach, which reconstructs individual behavior from aggregated data, can be exported to be a solution for any ecological inference problem where one cannot assume that all the areas are exchangeable the way typically assumed by other ecological inference methods.	cluster analysis	Xavier Puig;Josep Ginebra	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.10.006	econometrics;data mining;mathematics;spatial analysis;bayesian hierarchical modeling;statistics	ML	26.44938287017075	-25.58151286641891	80049
3589030d5541a975f2efb5d52d2a4138965f38d1	dependence-maximization clustering with least-squares mutual information	model selection;kernel;squared loss mutual information;dependence maximization clustering;least squares mutual information;least square;mutual information	Recently, statistical dependence measures such as mutual information and kernelized covariance have been successfully applied to clustering, called dependencemaximization clustering. In this paper, we propose a novel dependencemaximization clustering method based on an estimator of a squared-loss variant of mutual information called least-squares mutual information. A notable advantage of the proposed method over existing ones is that hyperparameters such as kernel parameters and regularization parameters can be objectively optimized based on cross-validation. Thus, subjective manual-tuning of hyperparameters is not necessary in the proposed method, which is a highly useful property in unsupervised clustering scenarios. Through experiments, we illustrate the usefulness of the proposed approach.	cluster analysis;cross-validation (statistics);expectation–maximization algorithm;experiment;kernel (operating system);kernel method;least squares;mean squared error;mutual information;presto	Manabu Kimura;Masashi Sugiyama	2011	JACIII	10.20965/jaciii.2011.p0800	variation of information;correlation clustering;constrained clustering;data stream clustering;kernel;fuzzy clustering;machine learning;pattern recognition;cluster analysis;mutual information;conditional mutual information;least squares;interaction information;model selection;statistics;pointwise mutual information	AI	27.63795411137573	-36.52828271513586	80282
fffcdd2541018f8dfe806d7db653e36c88d8fcd9	graph learning from data under laplacian and structural constraints		Graphs are fundamental mathematical structures used in various fields to represent data, signals, and processes. In this paper, we propose a novel framework for learning/estimating graphs from data. The proposed framework includes (i) formulation of various graph learning problems, (ii) their probabilistic interpretations, and (iii) associated algorithms. Specifically, graph learning problems are posed as the estimation of graph Laplacian matrices from some observed data under given structural constraints (e.g., graph connectivity and sparsity level). From a probabilistic perspective, the problems of interest correspond to maximum a posteriori parameter estimation of Gaussian–Markov random field models, whose precision (inverse covariance) is a graph Laplacian matrix. For the proposed graph learning problems, specialized algorithms are developed by incorporating the graph Laplacian and structural constraints. The experimental results demonstrate that the proposed algorithms outperform the current state-of-the-art methods in terms of accuracy and computational efficiency.	algorithm;computation;concave function;connectivity (graph theory);convex function;convex set;coordinate descent;core opengl;diagonally dominant matrix;estimation theory;graph theory;iteration;laplacian matrix;linear function;markov chain;markov random field;mathematical structure;optimization problem;propositional calculus;slab allocation;social inequality;sorting;sparse matrix;turing completeness;vergence	Hilmi E. Egilmez;Eduardo Pavez;Antonio Ortega	2017	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2017.2726975	algebraic connectivity;mathematical optimization;directed graph;laplacian matrix;null graph;graph property;machine learning;pattern recognition;voltage graph;graph;spectral graph theory;quartic graph;strength of a graph	ML	26.498618466339995	-29.51045031893365	80485
a6e33d00a55c0742de82d9fdbc1ab4fe878fd530	tool wear monitoring based on localized fuzzy neural networks for turning operation	turning operation;fuzzy neural networks turning neural networks condition monitoring computerized monitoring machining fuzzy systems computer networks signal analysis time series analysis;optimisation;acoustic emission signals;fuzzy neural network;fuzzy neural nets;normal fuzzy neural networks;wear;turning;bp neural networks;frequency domain analysis;cutting tools;automatic machining process;time frequency;localized fuzzy neural network;integrated neural network;backpropagation;data mining;tool wear monitoring system;tool states classification;force;time domain analysis;wear acoustic emission testing backpropagation condition monitoring cutting tools frequency domain analysis fuzzy neural nets optimisation time domain analysis turning machining;wavelet transforms;artificial neural networks;monitoring system;adaptive learning algorithm;cutting force signals;condition monitoring;monitoring;feature extraction;tool wear;bp neural networks tool wear monitoring system localized fuzzy neural networks turning operation automatic machining process tool states classification normal fuzzy neural networks cutting force signals acoustic emission signals time domain analysis frequency domain analysis time frequency domain analysis integrated neural network adaptive learning algorithm;adaptive learning;localized fuzzy neural networks;time frequency domain analysis;localized fuzzy neural network tool wear turning;time domain;acoustic emission;acoustic emission testing;frequency domain;classification accuracy;fuzzy neural networks;cutting force;turning machining;neural network	On-line tool wear monitoring is essential to automatic machining process. In order to predict tool wear accurately and reliably under different cutting conditions, a novel tool wear monitoring system (TWMS) is proposed by using localized fuzzy neural networks(LFNN) in this study which may improve classification accuracy of tool states and the computing speed compared with BPNN and normal fuzzy neural networks in the process of turning. By analyzing cutting forces signals and acoustic emission signals in time domain, frequency domain, and time-frequency domain, a series of features that sensitive to tool states were selected as inputs of neural networks according to synthesis coefficient. The nonlinear relations between tool wear and features were modeled by using integrated neural network (INN) that constructed and optimized through LFNN trained by an adaptive learning algorithm. The experimental results show that the monitoring system based on LFNN is provided with high precision, rapid computing speed and good multiplication.	acoustic cryptanalysis;algorithm;artificial neural network;coefficient;network packet;nonlinear system;time–frequency analysis;wavelet	Hongli Gao;Mingheng Xu;Xiaohui Shi;HaiFeng Huang	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.754	computer science;artificial intelligence;acoustic emission;machine learning;frequency domain;artificial neural network	AI	37.65860639226101	-31.60030465732142	81286
57dcef186a71a3de08c6cce9a8410654a2d347a6	sparse representation-based feature extraction combined with support vector machine for sense-through-foliage target detection and recognition	sense through foliage target recognition;bpnn;feature extraction;principal component analysis;bp neural network;sparse representation bpnn bp neural network k nearest neighbour chaotic differential evolution optimisation pca principal component analysis svm backscattering multipath propagation sense through foliage target recognition sense through foliage target detection support vector machine feature extraction;svm;k nearest neighbour;sense through foliage target detection;multipath propagation;support vector machine;support vector machines backpropagation backscatter feature extraction image representation neural nets object detection optimisation principal component analysis;backscattering;sparse representation;pca;chaotic differential evolution optimisation	Owing to multipath propagation effects of rough surfaces, scattering from trees and ground tend to overwhelm the weak backscattering of targets, which makes it more difficult for sense-through-foliage target detection and recognition. In this study, a novel method to detect and recognise targets obscured by foliage based on sparse representation (SR) and support vector machine (SVM) is proposed. SR theory is applied to analysing the components of received radar signals and sparse coefficients are used to describe target features, the dimension of the sparse coefficients is reduced using principal component analysis (PCA). Then, an improved SVM classifier is developed to perform target detection and recognition. A chaotic differential evolution optimisation approach using tent map is developed to determine the parameters of SVM. The experimental results indicate that the proposed approach is an effective method for sense-through-foliage target detection and recognition, which can achieve higher accuracy than that of the differential evolution-optimised SVM, SVM, k-nearest neighbour and BP neural network (BPNN).	feature extraction;sparse approximation;support vector machine	Shijun Zhai;Ting Jiang	2014	IET Signal Processing	10.1049/iet-spr.2013.0281	support vector machine;speech recognition;computer science;machine learning;pattern recognition;principal component analysis	ML	38.41956603733761	-33.911847208849196	81294
2e764ffacf6d175d01c19a9f744f733ea86e2e1a	an integrated faults classification approach based on lw-mwpca and pnn	fault classification;te process fault detection and diagnosis fault classification lifting wavelets lw mpca and pnn;probability;moving window principal components analysis;neural nets;lifting wavelets transform;integrated faults classification;lifting scheme;wavelet transforms fault diagnosis neural nets principal component analysis probability production engineering computing;production engineering computing;pnn;wavelet transforms;artificial neural networks;artificial neural networks wavelet transforms principal component analysis classification algorithms fault diagnosis monitoring process control;wavelet transform;monitoring;lw mwpca;principal component analysis;classification algorithms;industrial system faults;process control;lw mpca and pnn;fault diagnosis integrated faults classification lw mwpca pnn moving window principal components analysis probabilistic neural network industrial system faults lifting wavelets transform;te process;process simulation;lifting wavelets;probabilistic neural network;fault detection and diagnosis;fault diagnosis	This paper presents the development of an algorithm based on lifting wavelets, moving window principal components analysis and probabilistic neural network (LW-MWPCA and PNN) for classifying the industrial system faults. The proposed technique consists of a pre-processing unit based on lifting wavelets transform in combination with moving window principal components analysis (MWPCA) and PNN. Firstly the data are pre-processed to remove noise through lifting scheme wavelets, which are faster than first generation wavelets, MWPCA is used to reduce the dimensionality, and then PNN is used to diagnose faults. To validate the performance and effectiveness of the proposed scheme, the method based on LW-MPCA and PNN is applied to diagnose the faults in TE Process. Simulation studies show that the proposed algorithm not only provides an accepted degree of accuracy in fault classification under different fault conditions, but also is reliable, fast and computationally efficient tool.	algorithm;algorithmic efficiency;artificial neural network;lambda lifting;lifting scheme;limewire;multilinear principal component analysis;preprocessor;probabilistic neural network;simulation;test engineer;wavelet	Qing Yang;Feng Tian;Dongsheng Wu;Dazhi Wang	2010	2010 Sixth International Conference on Natural Computation	10.1109/ICNC.2010.5583656	speech recognition;engineering;machine learning;pattern recognition	Robotics	36.946994980412256	-30.989057859505966	81374
2cad10bc4ece9ef2fbbecb16e00b5cb68ecdc240	volumes of logistic regression models and their use for model selection			logistic regression;model selection	James G. Dowty	2014	CoRR		generalised logistic function;logistic regression;logistic model tree;factor regression model;multinomial logistic regression	ML	30.423798314555523	-25.94267876463801	81560
e133d78cee71fa8c73ec7815b0d1dc59316f1619	rubbing fault diagnosis of rotary machinery based on wavelet and support vector machine	rubbing block;level 2;kernel;support vector machines;svm model;rotary machinery;wavelet transforms;learning systems;mechanical engineering;artificial neural networks;wavelet transform;wavelet transforms electric machine analysis computing electric machines fault diagnosis support vector machines;feature extraction;rotors;support vector machine acoustic emission rubbing fault wavelet transform fault diagnosis;electric machine analysis computing;support vector machine classification;acoustic emission data fault diagnosis rotary machinery wavelet transform support vector machine rubbing block svm model;acoustic emission;support vector machine;acoustic emission data;machinery;time frequency analysis;rubbing fault;fault diagnosis machinery support vector machines kernel support vector machine classification wavelet transforms chemical technology mechanical engineering feature extraction learning systems;level 1;fault diagnosis;chemical technology;electric machines	The diagnosis method of rubbing fault in rotary machinery was investigated by support vector machine combined with wavelet transform. The rubbing fault of a rotary machine was simulated with a rubbing-block. The decomposed signals at every level were continuous for the case without rubbing fault, while the decomposed signals were bursting signals at level 1, level 2 and level 3, and continuous signals at level 4, level 5 for the case with rubbing fault. The correct rate of test samples was more than 92%, which indicated that the method can be used to identify rubbing faults effectively. The complexity of the SVM model was decreased and the calculation was simplified by using wavelet transform.	cpu cache;rotary system;rotary woofer;support vector machine;wavelet transform	Zhihao Jin;Shangwei Ji;Wen Jin;Bangchun Wen	2009	2009 First International Workshop on Database Technology and Applications	10.1109/DBTA.2009.163	support vector machine;speech recognition;computer science;machine learning;pattern recognition;artificial neural network;wavelet transform	EDA	37.28428272843955	-31.23562573047892	81883
b16045d7cd820d358bf5b41d67ad387cd0c2d12d	gromov's method for bayesian stochastic particle flow: a simple exact formula for q	q measurement;standards;kalman filters;noise measurement;covariance matrices;mathematical model;particle filters	We describe several new algorithms for stochastic particle flow using Gromov's method. We derive a simple exact formula for Q in certain special cases. The purpose of using stochastic particle flow is two fold: improve estimation accuracy of the state vector and improve the accuracy of uncertainty quantification. Q is the covariance matrix of the diffusion for particle flow corresponding to Bayes' rule.	algorithm;autodesk 3ds max;smoothed-particle hydrodynamics;uncertainty quantification	Frederick E. Daum;Jim Huang;Arjang Noushin	2016	2016 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)	10.1109/MFI.2016.7849544	econometrics;mathematical optimization;mathematics;statistics	Robotics	38.11703442583268	-25.78139189782154	81926
75d96ce070fb08a68e7a06b03761b80b0481bd43	learning essential graph markov models from data	model selection;markov model;bayesian network	In a model selection procedure where many models are to be compared, computational efficiency is critical. For acyclic digraph (ADG) Markov models (aka DAG models or Bayesian networks), each ADG Markov equivalence class can be represented by a unique chain graph, called an essential graph (EG). This parsimonious representation might be used to facilitate selection among ADG models. Because EGs combine features of decomposable graphs and ADGs, a scoring metric can be developed for EGs with categorical (multinomial) data. This metric may permit the characterization of local computations directly for EGs, which in turn would yield a learning procedure that does not require transformation to representative ADGs at each step for scoring purposes, nor is the scoring metric constrained by Markov equivalence.	bayesian network;computation;directed acyclic graph;directed graph;markov chain;markov model;mixed graph;model selection;multinomial logistic regression;occam's razor;raid;turing completeness	Robert Castelo;Michael D. Perlman	2002			combinatorics;variable-order bayesian network;computer science;machine learning;pattern recognition;bayesian network;mathematics;markov model;model selection;statistics;variable-order markov model	ML	24.99241087374051	-27.929623336937183	82258
106bb0442fb686ca4ec39abd7ec2f5e76d995696	automated probabilistic modeling for relational data	databases;probabilistic modeling;bayesian models	Probabilistic graphical model representations of relational data provide a number of desired features, such as inference of missing values, detection of errors, visualization of data, and probabilistic answers to relational queries. However, adoption has been slow due to the high level of expertise expected both in probability and in the domain from the user. Instead of requiring a domain expert to specify the probabilistic dependencies of the data, we present an approach that uses the relational DB schema to automatically construct a Bayesian graphical model for a database. This resulting model contains customized distributions for the attributes, latent variables that cluster the records, and factors that reflect and represent the foreign key links, whilst allowing efficient inference. Experiments demonstrate the accuracy of the model and scalability of inference on synthetic and real-world data.	bayesian network;foreign key;graphical model;graphical user interface;high-level programming language;latent variable;missing data;relational database;scalability;subject-matter expert;synthetic intelligence;while	Sameer Singh;Thore Graepel	2013		10.1145/2505515.2507828	variable elimination;statistical relational learning;data model;probabilistic relevance model;relational database;computer science;probabilistic database;machine learning;data mining;database;graphical model;divergence-from-randomness model	ML	28.04008382216109	-31.48791442208028	82488
835d9366b246c13faf38f4b89dfb8ca06b79ff3d	mixture of distributions in the biparametric exponential family: a bayesian approach	62j12;mcmc methods;bayesian approach;mcmc simulation;bayesian methods;biparametric exponential family;exponential family;bayesian method;mixture model;markov chain monte carlo;62f15;gamma distribution;mixture models;mixture of distributions	In this article we propose mixture of distributions belonging to the biparametric exponential family, considering joint modeling of the mean and variance (or dispersion) parameters. As special cases we consider mixtures of normal and gamma distributions. A novel Bayesian methodology, using Markov Chain Monte Carlo (MCMC) methods, is proposed to obtain the posterior summaries of interest. We include simulations and real data examples to illustrate de performance of the proposal.		L. Garrido LilianaGarrido;Edilberto Cepeda	2012	Communications in Statistics - Simulation and Computation	10.1080/03610918.2011.592245	econometrics;bayesian probability;natural exponential family;mixture model;mathematics;bayesian statistics;statistics	ML	31.02221534572282	-24.79588934301406	82656
a50fadcae9226befd212e7095c0715a74c11063d	bayesian inference and online learning in poisson neuronal networks		Motivated by the growing evidence for Bayesian computation in the brain, we show how a two-layer recurrent network of Poisson neurons can perform both approximate Bayesian inference and learning for any hidden Markov model. The lower-layer sensory neurons receive noisy measurements of hidden world states. The higher-layer neurons infer a posterior distribution over world states via Bayesian inference from inputs generated by sensory neurons. We demonstrate how such a neuronal network with synaptic plasticity can implement a form of Bayesian inference similar to Monte Carlo methods such as particle filtering. Each spike in a higher-layer neuron represents a sample of a particular hidden world state. The spiking activity across the neural population approximates the posterior distribution over hidden states. In this model, variability in spiking is regarded not as a nuisance but as an integral feature that provides the variability necessary for sampling during inference. We demonstrate how the network can learn the likelihood model, as well as the transition probabilities underlying the dynamics, using a Hebbian learning rule. We present results illustrating the ability of the network to perform inference and learning for arbitrary hidden Markov models.	afferent neuron;approximation algorithm;bayesian approaches to brain function;computation (action);hebbian theory;hidden markov model;inference;learning rule;markov chain;monte carlo method;neural ensemble;neuronal plasticity;particle filter;probability;recurrent neural network;sampling (signal processing);spatial variability;synaptic package manager	Yanping Huang;Rajesh P. N. Rao	2016	Neural Computation	10.1162/NECO_a_00851	variable-order bayesian network;machine learning;hidden semi-markov model;free energy principle;pattern recognition;mathematics;bayesian statistics;dynamic bayesian network;statistics	ML	24.62901901821799	-29.989556383521016	82665
0887e7721f526e75488babf464bee51c5cbbcbda	bayesian nonparametric kernel-learning		Kernel methods are ubiquitous tools in machine learning. However, there is often little reason for the common practice of selecting a kernel a priori. Even if a universal approximating kernel is selected, the quality of the finite sample estimator may be greatly affected by the choice of kernel. Furthermore, when directly applying kernel methods, one typically needs to compute a N×N Gram matrix of pairwise kernel evaluations to work with a dataset of N instances. The computation of this Gram matrix precludes the direct application of kernel methods on large datasets, and makes kernel learning especially difficult. In this paper we introduce Bayesian nonparmetric kernel-learning (BaNK), a generic, data-driven framework for scalable learning of kernels. BaNK places a nonparametric prior on the spectral distribution of random frequencies allowing it to both learn kernels and scale to large datasets. We show that this framework can be used for large scale regression and classification tasks. Furthermore, we show that BaNK outperforms several other scalable approaches for kernel learning on a variety of real world datasets.	computation;generative model;gramian matrix;ibm notes;kernel (operating system);kernel density estimation;kernel method;machine learning;markov chain monte carlo;mixture model;sampling (signal processing);scalability;spectral density;statistical classification;eric	Junier B. Oliva;Kumar Avinava Dubey;Andrew Gordon Wilson;Barnabás Póczos;Jeff G. Schneider;Eric P. Xing	2016			kernel method;string kernel;kernel embedding of distributions;radial basis function kernel;computer science;machine learning;pattern recognition;data mining;graph kernel;tree kernel;variable kernel density estimation;polynomial kernel;statistics;kernel smoother	ML	27.226124813568894	-31.11952089935564	82822
82b8f5280334991f93b6199b3a018c026daa4484	gradient hyperalignment for multi-subject fmri data alignment		Multi-subject fMRI data analysis is an interesting and challenging problem in human brain decoding studies. The inherent anatomical and functional variability across subjects make it necessary to do both anatomical and functional alignment before classification analysis. Besides, when it comes to big data, time complexity becomes a problem that cannot be ignored. This paper proposes Gradient Hyperalignment (Gradient-HA) as a gradient-based functional alignment method that is suitable for multi-subject fMRI datasets with large amounts of samples and voxels. The advantage of Gradient-HA is that it can solve independence and high dimension problems by using Independent Component Analysis (ICA) and Stochastic Gradient Ascent (SGA). Validation using multiclassification tasks on big data demonstrates that Gradient-HA method has less time complexity and better or comparable performance compared with other state-of-the-art functional alignment methods.	algorithm;big data;conjugate gradient method;data structure alignment;experiment;independent computing architecture;independent component analysis;spatial variability;stochastic gradient descent;synthetic genetic array;time complexity;times ascent;voxel	Tonglin Xu;Muhammad Yousefnezhad;Daoqiang Zhang	2018		10.1007/978-3-319-97304-3_81	time complexity;artificial intelligence;voxel;machine learning;computer science;pattern recognition;data structure alignment;independent component analysis;big data;gradient descent;decoding methods	ML	29.299719199363782	-37.65784093562051	83242
8012c23e833e170a2f372110fac5a305be1a699f	ep-based infinite inverted dirichlet mixture learning: application to image spam detection		We propose in this paper a new fully unsupervised model based on a Dirichlet process prior and the inverted Dirichlet distribution that allows the automatic inferring of clusters from data. The main idea is to let the number of mixture components increases as new vectors arrive. This allows answering the model selection problem in a elegant way since the resulting model can be viewed as an infinite inverted Dirichlet mixture. An expectation propagation (EP) inference methodology is developed to learn this model by obtaining a full posterior distribution on its parameters. We validate the model on a challenging application namely image spam filtering to show the merits of the framework.	expectation propagation	Wentao Fan;Sami Bourouis;Nizar Bouguila;Fahd Aldosari;Hassen Sallay;K. M. Jamil Khayyat	2018		10.1007/978-3-319-92058-0_33	image spam;dirichlet distribution;model selection;filter (signal processing);inference;expectation propagation;posterior probability;artificial intelligence;pattern recognition;mathematics;dirichlet process	ML	27.768952171948975	-30.537444361220683	83276
54ba1431543999d6a0558c1cfc63f28c38cb9e4b	bayesian framework for unsupervised classification with application to target tracking	bayesian framework;unsupervised learning;probability density;bayesian approach;bayes methods;multidimensional data;iterative methods target tracking unsupervised learning maximum likelihood estimation bayes methods array signal processing signal classification;array signal processing;iterative method bayesian framework unsupervised classification target tracking multidimensional data bayesian estimation classes data partition parameter vectors density map estimates joint posterior probability density validating clusters probability density training samples descent algorithm;maximum likelihood estimation;data partitioning;iterative methods;signal classification;map estimation;unsupervised classification;bayesian estimator;target tracking;bayesian methods target tracking clustering algorithms partitioning algorithms application software contracts data engineering multidimensional systems iterative algorithms shape	W e have given a solution to the problem of unsupervised classification of multidimensional data. Our approach is based on Bayesian estimation which regards the number of classes, the data partition and the parameter vectors that describe the density of classes as unknowns. W e compute their M A P estimates simultaneously by maximizing their joint posterior probability density given the data. The concept of partition as a variable to be estimated is a unique feature of our method. This formulation also solves the problem of validating clusters obtained from various methods. Our method can also incorporate any additional information about a class while assigning its probability density. It can also utilize any available training samples that arise from different classes. We provide a descent algorithm that starts with an arbitrary partition of the data and iteratively computes the M A P estimates. The proposed method is applied to target tracking data. The results obtained demonstrate the power of Bayesian approach for unsupervised classification.	algorithm;bayesian network;unsupervised learning	Rangasami L. Kashyap;Srinivas Sista	1999		10.1109/ICASSP.1999.756332	unsupervised learning;probability density function;bayesian probability;machine learning;pattern recognition;mathematics;iterative method;maximum likelihood;statistics	ML	30.177523226710953	-31.787210713670202	83486
1afa533d6d02e217f04b3edfaeaa8d4e828c9cef	bayesian network refinement via machine learning approach	bayesian network;probability;bayesian methods machine learning uncertainty spaceborne radar data mining intelligent systems intelligent networks learning systems image recognition marine vehicles;inference mechanisms;uncertainty handling;data mining;machine learning;knowledge acquisition;minimum description length;uncertainty reasoning;localization scheme bayesian network refinement machine learning approach conditional probability parameters minimum description length principle;knowledge base refinement;uncertainty handling learning artificial intelligence probability knowledge acquisition inference mechanisms;network structure;learning artificial intelligence;conditional probability;bayesian networks	An approach to refining Bayesian network structures from new data is developed. Most previous work has only considered the refinement of the network's conditional probability parameters and has not addressed the issue of refining the network's structure. We tackle this problem by a machine learning approach based on a formalism known as the minimum description length (MDL) principle. The MDL principle is well suited to this task since it can perform tradeoffs between the accuracy, simplicity, and closeness to the existent structure. Another salient feature of this refinement approach is the capability of refining a network structure using partially specified data. Moreover, a localization scheme is developed for efficient computation of the description lengths since direct evaluation involves exponential time resources.	bayesian network;machine learning	Wai Lam	1998	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.667882	computer science;machine learning;pattern recognition;bayesian network;data mining;statistics	Vision	32.18154680884483	-32.04429104834218	83600
819f97745609ece548c5d0e37b318ef144781b82	filtering with state-observation examples via kernel monte carlo filter		This letter addresses the problem of filtering with a state-space model. Standard approaches for filtering assume that a probabilistic model for observations (i.e., the observation model) is given explicitly or at least parametrically. We consider a setting where this assumption is not satisfied; we assume that the knowledge of the observation model is provided only by examples of state-observation pairs. This setting is important and appears when state variables are defined as quantities that are very different from the observations. We propose kernel Monte Carlo filter, a novel filtering method that is focused on this setting. Our approach is based on the framework of kernel mean embeddings, which enables nonparametric posterior inference using the state-observation examples. The proposed method represents state distributions as weighted samples, propagates these samples by sampling, estimates the state posteriors by kernel Bayes’ rule, and resamples by kernel herding. In particular, the sampling and resampling procedures are novel in being expressed using kernel mean embeddings, so we theoretically analyze their behaviors. We reveal the following properties, which are similar to those of corresponding procedures in particle methods: the performance of sampling can degrade if the effective sample size of a weighted sample is small, and resampling improves the sampling performance by increasing the effective sample size. We first demonstrate these theoretical findings by synthetic experiments. Then we show the effectiveness of the proposed filter by artificial and real data experiments, which include vision-based mobile robot localization.	addresses (publication format);behavior;estimated;experiment;inference;kernel (operating system);mobile robot;monte carlo method;particle filter;quantity;resampling (statistics);robotic mapping;sample size;sampling (signal processing);sampling - surgical action;state space;statistical model;synthetic intelligence;uml state machine	Motonobu Kanagawa;Yu Nishiyama;Arthur Gretton;Kenji Fukumizu	2016	Neural Computation	10.1162/NECO_a_00806	econometrics;kernel embedding of distributions;machine learning;mathematics;variable kernel density estimation;statistics	ML	28.152088993568185	-26.561189808708406	83692
e9b4e0bc3cc67bdf13aae06f5d14121ff814f2c8	fiber optic perimeter detection based on human engineering	optical fibers optical fiber communication optical fiber sensors vibrations frequency domain analysis time domain analysis;noise suppression fiber optic perimeter security system time domain analysis system human engineering k means clustering cosine similarity;vibrations;optical fiber sensors;frequency domain analysis;event signal feature extraction fiber optic perimeter detection human engineering fiber optic perimeter security system mach zehnder interferometer human intrusion partial environmental noise fiber vibration signal classification system data extraction system k means clustering method time frequency domain antidisturbance ability improvement false alarm rate reduction;time domain analysis;optical fibers;optical fiber communication;time frequency analysis ergonomics feature extraction fibre optic sensors mach zehnder interferometers optical signal detection pattern clustering security of data signal classification	In the fiber optic perimeter security system, which is based on Mach-Zehnder interferometer, the human intrusion and partial environmental noise can cause the fiber vibration. Distinguishing intrusion from environmental events without reducing the efficiency is a key requirement for any perimeter intrusion detection systems. In this paper, an signal classification system is presented for detection and recognition. This system compares event signal features, which are based on frequency domain and time domain respectively. Firstly, this research analyzes data extraction system based on human engineering. This model preprocesses time-domain data according to the characteristics of human behavior. After an event detection and feature extraction proce-dure, a classification algorithm applies K-means clustering method and the cosine similarity. In this method, the combination of signal in time and frequency domain is used to classify and improve the anti disturbance ability of the system and reduce the false alarm rate.	algorithm;cluster analysis;conley–zehnder theorem;cosine similarity;feature extraction;human factors and ergonomics;intrusion detection system;k-means clustering;mathematical optimization;network packet;optical fiber;perimeter	Kun Peng;Qing Li;Min Zhang;Xiaoyue Kong;Youli Yuan	2016	2016 25th Wireless and Optical Communication Conference (WOCC)	10.1109/WOCC.2016.7506584	electronic engineering;telecommunications;computer science;engineering;optical fiber;fiber optic sensor;vibration;optics;frequency domain	EDA	38.430874567975344	-33.529506554415086	83883
ca8938549aa752220c5d20d0dd91100a1877dee7	a lasso-based diagnostic framework for multivariate statistical process control	bayesian methods least squares statistical process control spc variable selection;grupo de excelencia;least squares approximation;variable selection;high dimensional;ciencias basicas y experimentales;matematicas;statistical process control spc statistics;bic;fault isolation;consistency	In monitoring complex systems, apart from quick detection of abnormal changes of system performance and key parameters, accurate fault diagnosis of responsible factors has become increasingly critical in a variety of applications that involve rich process data. Conventional statistical process control (SPC) methods, such as interpretation and decomposition of Hotelling’s T 2-type statistic, suffer from such high-dimensional problems because they are often computationally expensive. In this paper, we frame fault isolation as a two-sample variable selection problem to provide a unified diagnosis framework based on Bayesian information criterion (BIC). We propose a practical LASSO-based diagnostic procedure which combines BIC with the popular adaptive LASSO variable selection method. Given the oracle property of LASSO and its algorithm, the diagnostic result can be obtained easily and quickly with a similar computational effort as least-squares regressions. More importantly, the proposed method does not require making any extra tests that are necessary in existing diagnosis methods. Under some mild conditions, the diagnostic consistency of the proposed method is established. Finally, we present several specific SPC examples, including multistage process control and profile monitoring, to demonstrate the effectiveness of our method.	analysis of algorithms;bayesian information criterion;complex systems;computation;fault detection and isolation;feature selection;interpretation (logic);lasso;least squares;multistage amplifier;selection algorithm	Changliang Zou;Wei Jiang;Fugee Tsung	2011	Technometrics	10.1198/TECH.2011.10034	econometrics;mathematics;consistency;feature selection;least squares;fault detection and isolation;statistics	AI	35.74189357175314	-28.523894741608828	83998
7309066076bcf60d089d4823b7ada8eac015c7d7	averaged least-mean-squares: bias-variance trade-offs and optimal sampling distributions		We consider the least-squares regression problem and provide a detailed asymptotic analysis of the performance of averaged constant-step-size stochastic gradient descent. In the strongly-convex case, we provide an asymptotic expansion up to explicit exponentially decaying terms. Our analysis leads to new insights into stochastic approximation algorithms: (a) it gives a tighter bound on the allowed step-size; (b) the generalization error may be divided into a variance term which is decaying as O(1/n), independently of the step-size γ, and a bias term that decays as O(1/γn); (c) when allowing non-uniform sampling of examples over a dataset, the choice of a good sampling density depends on the trade-off between bias and variance: when the variance term dominates, optimal sampling densities do not lead to much gain, while when the bias term dominates, we can choose larger step-sizes that lead to significant improvements.	approximation algorithm;biasing;converge;emoticon;fisher information;generalization error;gibbs sampling;global optimization;initial condition;least mean squares filter;least squares;logistic regression;nicolas jacobsen;nonuniform sampling;resampling (statistics);sampling (signal processing);stochastic approximation;stochastic gradient descent	Alexandre Défossez;Francis R. Bach	2015				ML	28.353918094279024	-27.245678636106778	84176
ff64b88b23dd2a32ca11b71ab19d6e5812598666	t-schatten-$p$ norm for low-rank tensor recovery		"""In this paper, we propose a new definition of tensor Schatten-<inline-formula><tex-math notation=""""LaTeX"""">$p$</tex-math></inline-formula> norm (t-Schatten-<inline-formula><tex-math notation=""""LaTeX"""">$p$</tex-math></inline-formula> norm) based on t-SVD, and prove that this norm has similar properties to matrix Schatten-<inline-formula><tex-math notation=""""LaTeX"""">$p$</tex-math></inline-formula> norm. More importantly, the t-Schatten-<inline-formula><tex-math notation=""""LaTeX"""">$p$</tex-math></inline-formula> norm can better approximate the <inline-formula><tex-math notation=""""LaTeX"""">$\ell _1$</tex-math></inline-formula> norm of the tensor multi-rank with <inline-formula><tex-math notation=""""LaTeX"""">$0 < p < 1$</tex-math></inline-formula>. Therefore, it can be used for the Low-Rank Tensor Recovery problems as a tighter regularizer. We further prove the tensor multi-Schatten-<inline-formula><tex-math notation=""""LaTeX"""">$p$</tex-math></inline-formula> norm surrogate theorem and give an efficient algorithm accordingly. By decomposing the target tensor into many small-scale tensors, the non-convex optimization problem <inline-formula><tex-math notation=""""LaTeX"""">$(0 < p < 1)$</tex-math></inline-formula> is transformed into many convex sub-problems equivalently, which can greatly improve the computational efficiency when dealing with large-scale tensors. Finally, we provide the theories on the conditions for exact recovery in the noiseless case and give the corresponding error bounds for the noise case. Experimental results on both synthetic and real-world datasets demonstrate the superiority of our t-Schattern-<italic>p</italic> norm in the Tensor Robust Principle Component Analysis and the Tensor Completion problems."""		Hao Kong;Xingyu Xie;Zhouchen Lin	2018	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2018.2879185	mathematical optimization;tensor;regular polygon;norm (mathematics);computational complexity theory;principal component analysis;computer science;matrix (mathematics);convex function	ML	26.59251967752775	-36.04030389009979	84334
3b5c7f7d20d475c7983474dfa26f7817f402d09d	knowledge discovery in power quality data using support vector machine and s-transform	directed graphs;stransform;directed acyclic graph;power quality data mining;support vector machines;power quality;wavelet transforms data mining directed graphs power engineering computing power system management support vector machines;directed acyclic graph knowledge discovery support vector machine s transform power quality data mining electrical power systems wavelet transform feature extraction;data mining;power quality knowledge discovery data mining svm stransform;wavelet transforms;feature vector;data analysis;electric power system;power engineering computing;wavelet transform;power system management;feature extraction;power quality support vector machines support vector machine classification data mining feature extraction wavelet transforms data analysis signal resolution frequency power system analysis computing;electrical power systems;signal resolution;support vector machine classification;power system analysis computing;svm;support vector machine;s transform;frequency;artificial neural network;knowledge discovery	In this paper, we investigate the potential of support vector machines (SVMs) for power quality data mining in electrical power systems. Modified wavelet transform, known as S-transform, has been used to extract unique features of the various power quality disturbances. Feature vectors from S-transform analysis are used to train the SVM classifier. Various multi-class SVM algorithms have been applied on the power quality data under study and the directed acyclic graph (DAGSVM) algorithm is found to be performing well. A comparison between the DAGSVM method and the one based on artificial neural network demonstrates the efficiency of the SVM method in classifying PQ disturbances	algorithm;artificial neural network;data mining;directed acyclic graph;electric power quality;ibm power systems;s transform;support vector machine;wavelet transform	K. Vivek;M. Gopa;Bijaya K. Panigrahi	2006	Third International Conference on Information Technology: New Generations (ITNG'06)	10.1109/ITNG.2006.86	support vector machine;computer science;machine learning;pattern recognition;data mining;artificial neural network	EDA	35.60911319334312	-32.10917605400501	84476
12eb2f8a3c4498cee31fb6536b5faa5ca9eaa0f7	a bayesian probability calculus for density matrices	bayes rule;bayesian method;upper bound;quantum physics;density matrix;map estimation	One of the main concepts in quantum physics is a density matrix, which is a symmetric positive definite matrix of trace one. Finite probability distributions can be seen as a special case when the density matrix is restricted to be diagonal. We develop a probability calculus based on these more general distributions that includes definitions of joints, conditionals and formulas that relate these, including analogs of the Theorem of Total Probability and various Bayes rules for the calculation of posterior density matrices. The resulting calculus parallels the familiar “conventional” probability calculus and always retains the latter as a special case when all matrices are diagonal. Whereas the conventional Bayesian methods maintain uncertainty about which model has the highest data likelihood, the generalization maintains uncertainty about which unit direction has the largest variance. Surprisingly the bounds also generalize: as in the conventional setting we upper bound the negative log likelihood of the data by the negative log likelihood of the MAP estimator.	density matrix;parallels desktop for mac;quantum mechanics	Manfred K. Warmuth;Dima Kuzmin	2006	CoRR		combinatorics;density estimation;conditional probability;bayesian probability;symmetric probability distribution;density matrix;calculus;mathematics;posterior probability;upper and lower bounds;bayes' theorem;physics;quantum mechanics;matrix;empirical probability;statistics	ML	29.372540826211406	-27.73088022328296	84687
2a2dfbd49cd2e1d7ae09c877a7dba0f041ce4bd3	sensor data air pollution prediction by kernel models	kernel;atmospheric measurements;neural networks;pollution measurement;hybrid methods;sensor fusion air pollution calibration environmental science computing evolutionary computation learning artificial intelligence neural nets;environmental modelling;statistics;kernel atmospheric measurements pollution measurement sociology statistics neural networks data models;air pollution sensor network calibration sensor data air pollution prediction kernel based neural network machine learning regularization network evolutionary learning algorithm;environmental modelling kernel models hybrid methods;kernel models;sociology;data models	Kernel-based neural networks are popular machine learning approach with many successful applications. Regularization networks represent a their special subclass with solid theoretical background and a variety of learning possibilities. In this paper, we focus on single and multi-kernel units, in particular, we describe the architecture of a product unit network, and describe an evolutionary learning algorithm for setting its parameters including different kernels from a dictionary, and optimal split of inputs into individual products. The approach is tested on real-world data from calibration of air-pollution sensor networks, and the performance is compared to several different regression tools.	algorithm;approximation;artificial neural network;cluster analysis;computer science;dictionary;embedded system;evolutionary algorithm;gradient descent;kernel (operating system);local search (optimization);machine learning;manifold regularization;mathematical optimization;matrix regularization;missing data;multikernel;polynomial;semi-supervised learning;semiconductor industry;sigmoid function;supervised learning;support vector machine	Petra Vidnerová;Roman Neruda	2016	2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)	10.1109/CCGrid.2016.80	data modeling;kernel method;kernel;computer science;machine learning;data mining;artificial neural network;statistics	ML	30.230285125411676	-34.564031199030076	85060
cc2a8585bd64b79c66bfb8ec83b94041392cf740	particle gibbs split-merge sampling for bayesian inference in mixture models		This paper presents an original Markov chain Monte Carlo method to sample from the posterior distribution of conjugate mixture models. This algorithm relies on a flexible split-merge procedure built using the particle Gibbs sampler introduced in Andrieu et al. (2009, 2010). The resulting so-called Particle Gibbs Split-Merge sampler does not require the computation of a complex acceptance ratio and can be implemented using existing sequential Monte Carlo libraries. We investigate its performance experimentally on synthetic problems as well as on geolocation data. Our results show that for a given computational budget, the Particle Gibbs Split-Merge sampler empirically outperforms existing split merge methods. The code and instructions allowing to reproduce the experiments is available at https://github.com/aroth85/pgsm.	algorithm;computation;dhrystone;experiment;geolocation;gibbs sampling;library (computing);markov chain monte carlo;mixture model;monte carlo method;sampling (signal processing)	Alexandre Bouchard-Côté;Arnaud Doucet;Andrew Roth	2017	Journal of Machine Learning Research		econometrics;gibbs sampling;particle filter;markov chain monte carlo;slice sampling;machine learning;mathematics;statistics	ML	27.737293874191117	-28.329376063840584	85544
9795234b5e3b6d821ec7a3b6af0538128954ad7b	wound-rotor induction generator inter-turn short-circuits diagnosis using a new digital neural network	winding short circuits backpropagation data preprocessing digital measurements fault diagnosis feedforward neural network induction generators rotor current stator current;modeling and simulation;electrical machine;devs formalism;artificial neural networks rotors stator windings training sensors neurons;devsimpy;wound rotor induction machine;electrical fault simulation;discrete event specification;python devs;fault diagnosis	This paper deals with a new transformation and fusion of digital input patterns used to train and test feedforward neural network for a wound-rotor three-phase induction machine windings short-circuit diagnosis. The single type of short-circuits tested by the proposed approach is based on turn-to-turn fault which is known as the first stage of insulation degradation. Used input/output data have been binary coded in order to reduce the computation complexity. A new procedure, namely addition and mean of the set of same rank, has been implemented to eliminate the redundancy due to the periodic character of input signals. However, this approach has a great impact on the statistical properties on the processed data in terms of richness and of statistical distribution. The proposed neural network has been trained and tested with experimental signals coming from six current sensors implemented around a setup with a prime mover and a 5.5 kW wound-rotor three-phase induction generator. Both stator and rotor windings have been modified in order to sort out first and last turns in each phase. The experimental results highlight the superiority of using this new procedure in both training and testing modes.	algorithm;artificial neural network;backpropagation;computation;data acquisition;data pre-processing;digital data;elegant degradation;feedforward neural network;field-programmable gate array;input/output;performance;preprocessor;r.o.t.o.r.;sensor;software propagation	Samuel Toma;Laurent Capocchi;Gérard-André Capolino	2013	IEEE Transactions on Industrial Electronics	10.1109/TIE.2012.2229675	control engineering;electronic engineering;real-time computing;computer science;engineering;electrical engineering;control theory;modeling and simulation	ML	36.58011813447113	-31.02879231347361	85546
f60f23bdc2ec695224e7df30ee3770b0c02e4028	restricted likelihood ratio tests for functional effects in the functional linear model	nonregular problem;functional data analysis nonparametric smoothing nonregular problem penalized splines variance components;functional data analysis;likelihood methods ratios linear models data smoothing spline functions variance components linear regression;bepress selected works;grupo de excelencia;ciencias basicas y experimentales;matematicas;nonparametric smoothing;statistics;variance components;penalized splines	The goal of our article is to provide a transparent, robust, and computationally feasible statistical approach for testing in the context of scalar-on-function linear regression models. In particular, we are interested in testing for the necessity of functional effects against standard linear models. Our methods are motivated by and applied to a large longitudinal study involving diffusion tensor imaging of intracranial white matter tracts in a susceptible cohort. In the context of this study, we conduct hypothesis tests that are motivated by anatomical knowledge and which support recent findings regarding the relationship between cognitive impairment and white matter demyelination. R-code and data are provided to reproduce the application.		Bruce J. Swihart;Jeff Goldsmith;Ciprian M. Crainiceanu	2014	Technometrics	10.1080/00401706.2013.863163	econometrics;mathematical optimization;functional data analysis;mathematics;statistics	ML	29.961891494051798	-24.237369740515234	85769
ab6ee7e3e4bc33d4212bd43143b33fd09a56ff9b	characterization of microarray data using wavelet power spectrum	microarray data;power spectrum	A door brace is disclosed adapted to interengage a door and a floor beneath the door and prevent unauthorized entry through the door. The brace comprises sections interconnected substantially end-to-end and collapsible with respect to each other into a compact form. One end section of the prop member has means adapted to engage the door, while the other end section has pivotally mounted base means adapted to engage the floor. Optionally, the prop member has lock means adapted to extend from the prop member to an exterior side of the door where the lock means is operable for locking or unlocking. The prop member may also have pressure-sensitive means adapted to signal a warning upon pressure upon the prop member due to unauthorized attempt to open the door.	microarray;spectral density;wavelet	S. Prabakaran;Rajendra Sahu;Sekher Verma	2006	KES Journal		microarray analysis techniques;computer science;spectral density	Networks	36.72864143204744	-36.41506936907912	85919
faeabe45fca6506be46294cf46f8520ac28d5562	autoconj: recognizing and exploiting conjugacy without a domain-specific language		Deriving conditional and marginal distributions using conjugacy relationships can be time consuming and error prone. In this paper, we propose a strategy for automating such derivations. Unlike previous systems which focus on relationships between pairs of random variables, our system (which we call Autoconj) operates directly on Python functions that compute log-joint distribution functions. Autoconj provides support for conjugacy-exploiting algorithms in any Pythonembedded PPL. This paves the way for accelerating development of novel inference algorithms and structure-exploiting modeling strategies.1	algorithm;cognitive dimensions of notations;domain-specific language;iterated function;marginal model;python	Matthew D. Hoffman;Matthew J. Johnson;Dustin Tran	2018			computer science;machine learning;theoretical computer science;conjugacy class;artificial intelligence;marginal distribution;python (programming language);domain-specific language;inference;random variable	ML	26.16136641421437	-28.892696913087523	85973
abbda3b047bc215498287646ca6405aea5b7e7cf	feature selection for steganalysis using the mahalanobis distance	dimensionalidad;mahalanobis distance;4230;steganographie;steganalysis;high dimensionality;learning;distance de mahalanobis;complexite calcul;0130c;dimensionality;imagerie;curse of dimensionality;feature space;algorithme;accuracy;distance measurement;steganography;apprentissage;imagery;esteganografia;compression image;precision;image compression;steganalyse;computational complexity;dimensionnalite;medicion distancia;feature extraction;estaganalisis;algorithms;feature selection;imagineria;extraction caracteristique;classification accuracy;mesure de distance;compresion imagen	"""Steganalysis is used to detect hidden content in innocuous images. Many successful steganalysis algorithms use a large number of features relative to the size of the training set and suffer from a """"curse of dimensionality"""": large number of feature values relative to training data size. High dimensionality of the feature space can reduce classification accuracy, obscure important features for classification, and increase computational complexity. This paper presents a filter-type feature selection algorithm that selects reduced feature sets using the Mahalanobis distance measure, and develops classifiers from the sets. The experiment is applied to a well-known JPEG steganalyzer, and shows that using our approach, reduced-feature steganalyzers can be obtained that perform as well as the original steganalyzer. The steganalyzer is that of Pevný et al. (SPIE, 2007) that combines DCT-based feature values and calibrated Markov features. Five embedding algorithms are used. Our results demonstrate that as few as 10-60 features at various levels of embedding can be used to create a classifier that gives comparable results to the full suite of 274 features."""	feature selection;steganalysis	Jennifer L. Davidson;Jaikishan Jalan	2010		10.1117/12.841074	computer vision;curse of dimensionality;machine learning;pattern recognition;accuracy and precision;feature selection	ML	35.090493613539074	-37.579160049409666	86076
d5fe9d77eaca5adf4751fcc5c54eaa1f72e84876	fast algorithms for structured sparsity		Sparse representations of signals (i.e., representations that have only few non-zero or large coefficients) have emerged as powerful tools in signal processing theory, algorithms, machine learning and other applications. However, real-world signals often exhibit rich structure beyond mere sparsity. For example, a natural image, once represented in the wavelet domain, often has the property that its large coefficients occupy a subtree of the wavelet hierarchy, as opposed to arbitrary positions. A general approach to capturing this type of additional structure is to model the support of the signal of interest (i.e., the set of indices of large coefficients) as belonging to a particular family of sets. Computing a sparse representation of the signal then corresponds to the problem of finding the support from the family that maximizes the sum of the squares of the selected coefficients. Such a modeling approach has proved to be beneficial in a number of applications including compression, de-noising, compressive sensing and machine learning. However, the resulting optimization problem is often computationally difficult or intractable, which is undesirable in many applications where large signals and datasets are commonplace. In this talk, I will outline some of the past and more recent algorithms for finding structured sparse representations of signals, including piecewise constant approximations, tree-sparse approximations and graph-sparse approximations. The algorithms borrow several techniques from combinatorial optimization (e.g., dynamic programming), graph theory, and approximation algorithms. For many problems the algorithms run in (nearly) linear time, which makes them applicable to very large datasets. Joint work with Chinmay Hegde and Ludwig Schmidt ∗Machine Learning External Seminar, Gatsby Unit, May 16, 2016.	approximation algorithm;coefficient;combinatorial optimization;compressed sensing;dynamic programming;graph theory;machine learning;mathematical optimization;optimization problem;schmidt decomposition;signal processing;sparse approximation;sparse matrix;time complexity;tree (data structure);wavelet	Chinmay Hegde;Piotr Indyk;Ludwig Schmidt	2015	Bulletin of the EATCS		mathematics;theoretical computer science	ML	30.06982191104834	-33.94640963127297	86229
7c94ab49fefb0e73155eae801595c8860c17a0d7	gaussian process regression for voice activity detection and speech enhancement	covariance functions gaussian process regression voice activity detection speech enhancement nonparametric bayesian method signal classification probabilistic method optimized hyperparameters;probability;nonparametric statistics;gaussian processes;probabilistic method;gaussian process regression;bayes methods;speech processing;covariance functions;signal detection;kalman filters;speech speech enhancement speech processing signal to noise ratio hidden markov models numerical models kalman filters;speech;regression model;speech enhancement;bayesian method;hidden markov models;signal classification;regression analysis;length scale;gaussian process;covariance function;numerical experiment;nonparametric bayesian method;voice activity detection;numerical models;signal to noise ratio;speech enhancement bayes methods gaussian processes nonparametric statistics probability regression analysis signal detection;optimized hyperparameters	Gaussian process (GP) model is a flexible nonparametric Bayesian method that is widely used in regression and classification. In this paper we present a probabilistic method where we solve voice activity detection (VAD) and speech enhancement in a single framework of GP regression, modeling clean speech by a GP smoother. Optimized hyperparameters in GP models lead us to a novel VAD method since learned length-scale parameters in covariance functions are much different between voiced and unvoiced frames. Clean speech is estimated by posterior means in GP models. Numerical experiments confirm the validity of our method.	bayesian network;experiment;gaussian process;kriging;numerical method;speech enhancement;voice activity detection	Sunho Park;Seungjin Choi	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4634203	speech recognition;computer science;pattern recognition;gaussian process;speech processing;regression analysis;statistics	Robotics	32.502494534015355	-26.905232317848213	86326
ebb13658a90dc3b9d90dfc884562bcecd0c17730	variable selection by stepwise slicing in nonparametric regression	input variables;probability density function;variance estimator;nonparametric test;random variables;data mining;variable selection;nonparametric regression model;stepwise slicing;smoothing methods;input variables smoothing methods educational institutions mathematics computer science multidimensional systems additives random variables analysis of variance testing;estimation;smoothing;asymptotic properties;nonparametric regression;regression analysis;smoothing variable nonparametric test;variance estimation;asymptotic properties variable selection stepwise slicing variance estimator nonparametric regression model;variable;data models	In this paper, variable selection issue is considered in a nonparametric regression setting. Two stepwise procedures based on variance estimators are proposed for selecting the significant variables in a general nonparametric regression model. These procedures do not require multidimensional smoothing at intermediate steps and they are based on formal rests of hypotheses as opposed to existing methods in the literature. Asymptotic properties are examined and empirical results are given.	feature selection;smoothing;stepwise regression	Jifu Nong	2009	2009 International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2009.304	nonparametric statistics;random variable;data modeling;econometrics;estimation;probability density function;computer science;variable;pattern recognition;mathematics;feature selection;nonparametric regression;regression analysis;statistics;smoothing	AI	28.76027428167166	-25.29522278966549	86518
6b6702c1cd050f8bfbac833726dc740119c47c69	regularization and model selection for quantile varying coefficient model with categorical effect modifiers	fused lasso;quantile regression;categorical effect modifiers;variable selection;varying coefficient model	A varying coefficient model with categorical effect modifiers is an effective modeling strategy when the data set includes categorical variables. With categorial predictors the number of parameters can become very large. This paper focuses on the model selection problem for varying coefficient model with categorical effect modifiers under the framework of quantile regression. After distinguishing between nominal and ordinal effect modifiers, a unified (adaptive-) Lasso-type regularization technique is proposed that allows for selection of covariates and fusion of categories of categorical effect modifiers, which can identify whether the coefficient functions are really varying with the level of a potentially effect modifying factor and provide a sparse model at different quantile levels. Moreover, the large sample properties are derived under appropriate conditions including a fixed bound on the number of parameters. The proposed methods are illustrated and investigated by extensive simulation studies and two real data evaluations. © 2014 Elsevier B.V. All rights reserved.	categorial grammar;coefficient;lasso;mathematical model;mcgurk effect;model selection;ordinal data;selection algorithm;simulation;sparse matrix	Weihua Zhao;Riquan Zhang;Jicai Liu	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2014.05.003	econometrics;quantile regression;categorical variable;machine learning;pattern recognition;mathematics;feature selection;statistics	AI	29.113934884644596	-24.548836057549625	86624
48f01507dae8dc433504c4d7db7a1977eff16147	analysis of crash simulation data using spectral embedding with histogram distances		Finite Element simulation of crash tests in the car industry generates huge amounts of high-dimensional numerical data. Methods from Machine Learning, especially from Dimensionality Reduction, can assist in analyzing and evaluating this data efficiently. Here we present a method that performs a two step dimensionality reduction in a novel manner: First the simulation data is represented as (normalized) histograms, then embedded into a low dimensional space using histogram distances and the nonlinear method of Spectral Embedding/Diffusion Maps, thus enabling a much easier data analysis. In particular, this method solves the problem of comparing simulation data with small changes in the Finite Element grids due to variations of geometry or unequally fine grid structures.	algorithm;diffusion map;dimensionality reduction;displacement mapping;embedded system;emile garcke;finite element method;level of measurement;machine learning;nonlinear system;numerical analysis;preprocessor;runtime system;simulation	Anna-Luisa Schwartz	2014			grid;dimensionality reduction;finite element method;theoretical computer science;histogram;mathematics;embedding;crash simulation;diffusion map;crash	ML	30.648505928873888	-36.55220081230848	86645
5c853bb6e4fde16d19039da17b7a9c147410c587	high-performance computing and application of zero-norm	sparseness;zero norm;non negative sparse coding;one norm;sparse matrix	-Whether sparseness can be effectively controlled is one of the key elements to measure the merits of the sparse coding algorithm. One-norm is primarily used in the sparse coding algorithm to control its sparseness currently, as well as by sparse approximation to control the sparseness of sparse coding model, but all these methods have led to slow convergence and low efficiency ultimately. In order to enhance the effectiveness of sparse coding algorithms, this paper selects zero-norm to control sparseness of the sparse coding model, and calculate after continuously extended at the discontinuous point of the model. We propose a highly efficient zero-norm sparse coding algorithm. This paper not only theoretically proves feasibility and efficiency of the algorithms which is capable of effectively controlling model sparseness, but also verifies the theoretical correctness of inference through experiments. These prove the operational efficiency of the algorithm is more efficient and stronger than existing algorithms.	algorithm;correctness (computer science);experiment;neural coding;sparse approximation;sparse matrix	Liying Lang;XueKe Jing	2012	JCP	10.4304/jcp.7.2.534-539	mathematical optimization;sparse matrix;machine learning;pattern recognition;sparse approximation;mathematics	AI	26.28849116755485	-36.04516647207061	87149
4545960df3dc27d2668ace408e9cd231d5ae21bb	automatic transaction of signal via statistical modeling	akaike information criterion;kullback leibler information;predictive distribution;prior knowledge;signal extraction;statistical model;generalized state space model;signal processing;information processing;akaike information criterion aic;statistical modeling;state space model;likelihood function;kullback leibler	The statistical information processing can be characterized by the likelihood function defined by giving an explicit form for an approximation to the true distribution. This mathematical representation, which is usually called a model, is built based on not only the current data but also prior knowledge on the object and the objective of the analysis. Akaike2,3) showed that the log-likelihood can be considered as an estimate of the Kullback-Leibler (K-L) information which measures the similarity between the predictive distribution of the model and the true distribution. Akaike information criterion (AIC) is an estimate of the K-L information and makes it possible to evaluate and compare the goodness of many models objectively. In consequence, the minimum AIC procedure allows us to develop automatic modeling and signal extraction procedures. In this article, we give a simple explanation of statistical modeling based on the AIC and demonstrate four examples of applying the minimum AIC procedure to an automatic transaction of signals observed in the earth sciences.	akaike information criterion;approximation;information processing;kullback–leibler divergence;statistical model	Genshiro Kitagawa;Tomoyuki Higuchi	2000	New Generation Computing	10.1007/BF03037565	statistical model;akaike information criterion;information processing;signal processing;pattern recognition;bayesian information criterion	ML	33.54415764660287	-26.58179690830034	87200
a0ea21049aae545953f0a47afdba4489fc6b4094	incorporating prior knowledge when learning mixtures of truncated basis functions from data			basis function	Antonio Fernández;Inmaculada Pérez-Bernabé;Rafael Rumí;Antonio Salmerón	2013		10.3233/978-1-61499-330-8-95	machine learning;artificial intelligence;computer science;basis function;pattern recognition	ML	30.885951648458352	-32.72703370599707	87414
23a8dc98eb38039f032d77a209751b25d3106c4b	memory efficient max flow for multi-label submodular mrfs		Multi-label submodular Markov Random Fields (MRFs) have been shown to be solvable using max-flow based on an encoding of the labels proposed by Ishikawa, in which each variable Xi is represented by l nodes (where l is the number of labels) arranged in a column. However, this method in general requires 2 l2 edges for each pair of neighbouring variables. This makes it inapplicable to realistic problems with many variables and labels, due to excessive memory requirement. In this paper, we introduce a variant of the max-flow algorithm that requires much less storage. Consequently, our algorithm makes it possible to optimally solve multi-label submodular problems involving large numbers of variables and labels on a standard computer.	algorithm;decision problem;energy, physics;experiment;graph - visual representation;ishikawa diagram;markov chain;markov random field;maximum flow problem;memory disorders;multi-label classification;numerous;submodular set function	Thalaiyasingam Ajanthan;Richard I. Hartley;Mathieu Salzmann	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/TPAMI.2018.2819675	mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm	Vision	25.394128995272126	-28.636797738774952	87446
1fc367ad1787e2c5c3a258290e3bb3b71a74bd6a	variational measure preserving flows		Probabilistic modelling is a general and elegant framework to capture the uncertainty, ambiguity and diversity of data. Probabilistic inference is the core technique for developing training and simulation algorithms on probabilistic models. However, the classic inference methods, like Markov chain Monte Carlo (MCMC) methods and mean-field variational inference (VI), are not computationally scalable for the recent developed probabilistic models with neural networks (NNs). This motivates many recent works on improving classic inference methods using NNs, especially, NN empowered VI. However, even with powerful NNs, VI still suffers its fundamental limitations. In this work, we propose a novel computational scalable general inference framework. With the theoretical foundation in ergodic theory, the proposed methods are not only computationally scalable like NN-based VI methods but also asymptotically accurate like MCMC. We test our method on popular benchmark problems and the results suggest that our methods can outperform NN-based VI and MCMC on deep generative models and Bayesian neural networks.	algorithm;approximation;artificial neural network;bayesian network;benchmark (computing);calculus of variations;ergodic theory;ergodicity;hybrid memory cube;kullback–leibler divergence;marginal model;markov chain monte carlo;mean field particle methods;measure-preserving dynamical system;modern portfolio theory;monte carlo method;probabilistic automaton;rate of convergence;rewrite (programming);scalability;simulation;statistical model;transform, clipping, and lighting;variational principle	Yichuan Zhang;José Miguel Hernández-Lobato;Zoubin Ghahramani	2018	CoRR		ergodicity;probabilistic logic;machine learning;ambiguity;spite;artificial neural network;inference;markov chain monte carlo;artificial intelligence;mathematics;bayesian probability	ML	25.479692720628286	-30.450999006458154	87620
e4bf4301a04b9fded019810604cf8ab588850b97	on simultaneously identifying outliers and heteroscedasticity without specific form	functional form;regression model;standard error;robust diagnostics;weighted least absolute deviation estimator;heteroscedasticity;least absolute deviation;covariance matrix	Assuming homogeneous variance in a normal regression model is not always appropriate as invalid standard inference procedures may result from the improper estimation of the standard error when the disturbance process in a regression model presents heteroscedasticity. When both outliers and heteroscedasticity exist, the inflation of the scale's estimate can deteriorate. Using graphical analysis, this study identifies outliers under heteroscedastic error without specifying a functional form. A jigsaw plot with two kinds of cut-off points differentiates both outlying and heteroscedastic characteristics for each observation in the data. The proposed approach is based on the concept of the weighted least absolute deviation estimator. Furthermore, plugging the resulting residuals into the estimation of the heteroscedasticity-consistent covariance matrix leads to a robust quasi-t test for the estimated coefficients.		Tsung-Chi Cheng	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2012.01.004	least absolute deviations;econometrics;covariance matrix;mathematical optimization;mathematics;heteroscedasticity;standard error;higher-order function;park test;regression analysis;statistics	ML	29.828422408059676	-24.313679440857584	88048
e0c08fc8f3791b874ded372a89fa3a028cf5273a	learning the number of gaussian components using hypothesis test	unsupervised learning;kolmogorov smirnow test gaussian components one dimensional statistical hypothesis test gaussian mixture projected gaussian means algorithms g means pg means wrapper algorithms k means expectation maximization algorithms extended projected gaussian means xpg means possibilistic fuzzy c means algorithm computational complexity fuzzy clustering;fuzzy c mean;expectation maximization algorithms;pg means;gaussian mixture;g means;extended projected gaussian means;neural networks;iterative algorithms;gaussian processes;k means;one dimensional statistical hypothesis test;bayesian methods;testing;gaussian components;maximum likelihood estimation;data mining;noise robustness;fuzzy set theory;statistical hypothesis testing;testing clustering algorithms gaussian processes computational complexity bayesian methods iterative algorithms noise robustness neural networks unsupervised learning maximum likelihood estimation;distance measurement;fuzzy clustering;statistical testing computational complexity expectation maximisation algorithm fuzzy set theory gaussian processes possibility theory;expectation maximization;computational complexity;principal component analysis;fast algorithm;number of clusters;possibilistic fuzzy c means algorithm;clustering algorithms;kolmogorov smirnow test;possibility theory;statistical testing;refining;xpg means;projected gaussian means algorithms;em algorithm;wrapper algorithms;noise;data models;hypothesis test;expectation maximisation algorithm	This paper addresses the problem of estimating the correct number of components in a Gaussian mixture given a sample data set. In particular, an extension of Gaussian-means (G-means) and Projected Gaussian-means (PG-means) algorithms is proposed. All these methods are based on one-dimensional statistical hypothesis test. G-means and PG-means are wrapper algorithms of the k-means and Expectation-Maximization (EM) algorithms, respectively. Although G-means is a simple and fast algorithm, it does not perform well when clusters overlap since it is based on k-means. PG-means can handle overlapped clusters but requires more computation and sometimes fails to find the right number of clusters. In this paper, we propose an extension, called Extended Projected Gaussian means (XPG-means). XPG-means is a wrapper algorithm of Possibilistic Fuzzy C-means (PFCM) algorithm. XPG-means integrates the advantages of both algorithms while resolving some of the disadvantages involving overlapped clusters, noise, and computational complexity. More specifically, XPG-means handles overlapped clusters better than G-means because of the use of fuzzy clustering, handles noise better than both algorithms because it uses possibilitistic clustering. XPG-means is less computationally expensive than PG-means because it uses local hypothesis testing scheme used by G-means that is specific to Gaussians wherease PG-means uses a more general Kolmogorov-Smirnow test on Gaussian mixtures. In addition, XPG-means demonstrates less variance in estimating the number of components than either of the other algorithms.	analysis of algorithms;cluster analysis;computation;computational complexity theory;expectation–maximization algorithm;fuzzy clustering;k-means clustering;x/open	Gyeongyong Heo;Paul D. Gader	2009	2009 International Joint Conference on Neural Networks	10.1109/IJCNN.2009.5178886	unsupervised learning;statistical hypothesis testing;computer science;machine learning;pattern recognition;mathematics;artificial neural network;statistics	ML	30.549366272340905	-31.415977154621043	88172
dd0d40abb879be1beb5d41d12c77c93adb3e197e	stationary point variational bayesian attribute-distributed sparse learning with ℓ1 sparsity constraints	distributed data;sparse bayesian learning attribute distributed learning variational bayesian inference;probability;bayesian methods training convergence inference algorithms optimization prediction algorithms probability density function;gaussian processes;sparse bayesian learning;probability density function;distributed processing;variational techniques;variational bayesian;variational techniques distributed processing gaussian processes learning artificial intelligence multi agent systems probability regression analysis;multi agent systems;distributed learning;variational bayesian inference;attribute distributed learning;flexible agent update protocols stationary point variational bayesian attribute distributed sparse learning sparsity constraints variational bayesian algorithm multivariate regression attribute distributed data variational bayesian version sage algorithm distributed fashion sparse bayesian learning sbl hierarchical sparsity prior modeling agent weights performing agents ensemble estimator gaussian product exponential probability density function marginalized prior laplace pdf hierarchical formulation variational update expressions prior parameters stationary points synthetic data mse noninformative agents distributed implementation;regression analysis;information agent;synthetic data;learning artificial intelligence;multivariate regression	The paper proposes a new variational Bayesian algorithm for ℓ1-penalized multivariate regression with attribute-distributed data. The algorithm is based on the variational Bayesian version of the SAGE algorithm that realizes a training of individual agents in a distributed fashion and sparse Bayesian learning (SBL) with hierarchical sparsity prior modeling of the agent weights. The SBL introduces constraints on the weights of individual agents, thus reducing the effects of overfitting and removing/suppressing poorly performing agents in the ensemble estimator. The ℓ1 constraint is introduced using a product of a Gaussian and an exponential probability density function with the resulting marginalized prior being a Laplace pdf. Such a hierarchical formulation of the prior allows for a computation of the stationary points of the variational update expressions for prior parameters, as well as deriving conditions that ensure convergence to these stationary points. Using synthetic data it is demonstrated that the proposed algorithm performs very well in terms of the achieved MSE, and outperforms other algorithms in the ability to sparsify non-informative agents, while at the same time allowing distributed implementation and flexible agent update protocols.	algorithm;calculus of variations;computation;general linear model;information;overfitting;portable document format;rate of convergence;relevance;sparse matrix;stationary process;synthetic data;time complexity;variational principle	Dmitriy Shutin;Sanjeev R. Kulkarni;H. Vincent Poor	2011	2011 4th IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)	10.1109/CAMSAP.2011.6136003	wake-sleep algorithm;computer science;machine learning;pattern recognition;statistics	ML	29.307086195742915	-32.01054728421556	88305
6ac9ebbb8c66ab813579549138f172dbfe6fccba	choice of basis for laplace approximation	laplace approximation;hidden markov model;marginal likelihood;bayesian inference;bayes factor;latent variable models;latent variable model;graphical models;hidden markov models;graphical model	Maximum a posteriori optimization of parameters and the Laplace approximation for the marginal likelihood are both basis-dependent methods. This note compares two choices of basis for models parameterized by probabilities, showing that it is possible to improve on the traditional choice, the probability simplex, by transforming to the 'softmax' basis.	approximation;marginal model;mathematical optimization;softmax function	David J. C. MacKay	1998	Machine Learning	10.1023/A:1007558615313	variable elimination;econometrics;expectation–maximization algorithm;marginal likelihood;computer science;machine learning;hidden semi-markov model;pattern recognition;mathematics;graphical model;hidden markov model;statistics	ML	27.462667962141207	-29.75213522310657	88354
c7ab406f9a937955fcb8309ffcbfce168aed2431	symbolic regression in multicollinearity problems	undesigned data;symbolic regression;multicollinearity;multiple regression	In this paper the potential of GP-generated symbolic regression for alleviating multicollinearity problems in multiple regression is presented with a case study in an industrial setting. The main advantage of this approach is the potential to produce a simple and stable polynomial model in terms of the original variables.	stable polynomial;symbolic regression	Flor A. Castillo;Carlos M. Villa	2005		10.1145/1068009.1068377	econometrics;multicollinearity;local regression;computer science;linear regression;machine learning;regression diagnostic;mathematics;variance inflation factor;statistics;cross-sectional regression	AI	27.479670729000606	-24.465381992680285	88441
a4e91cb755e1e63e1fd5b8d35c1f71e90568994c	target identification in foliage environment using selected bispectra and extreme learning machine	radar computing;ultra wideband radar;vegetation;foliage environment target identification radar recognition obstacle detection selected bispectra extreme learning machine;target identification method target classifier feature extraction selected bispectra algorithm radar sensor uwb ir transceiver ir equipment impulse radio equipment uwb equipment ultra wideband equipment signal waveforms extreme learning machine foliage environment;feature extraction;radar imaging;radar target recognition;learning artificial intelligence;training radar feature extraction testing accuracy support vector machine classification radar antennas;radio transceivers;object detection;vegetation feature extraction learning artificial intelligence object detection radar computing radar imaging radar target recognition radio transceivers ultra wideband radar	In this paper, a novel method of target identification in foliage environment is presented. This method takes the received signal waveforms to identify the targets between the communication transceivers, which are measured by Ultra WideBand (UWB) Impulse Radio (IR) equipment under foliage environment. In this way, most existing UWB-IR transceivers can be exploited as detecting radar sensors, which leads to a potential low-cost way to identify targets under foliage environment. The selected bispectra algorithm is applied to extract the feature vector, and Extreme Learning Machine is used as the target classifier. Experiments with real-world data samples indicate that this method has an excellent classification performance in foliage environment.	algorithm;experiment;feature vector;sensor;transceiver;ultra-wideband	Minglei You;Ting Jiang	2013	2013 IEEE International Conference on Communications Workshops (ICC)	10.1109/ICCW.2013.6649370	man-portable radar;computer vision;continuous-wave radar;radar tracker;radar engineering details;radar lock-on;radar configurations and types;feature extraction;computer science;fire-control radar;bistatic radar;low probability of intercept radar;pulse-doppler radar;radar imaging;vegetation	Robotics	38.45380898209346	-33.814766252335744	88629
440f07be3faf60a92e089be6d1ac08f219f2773c	using zero-norm constraint for sparse probability density function estimation	quadratic program;probability density function;endnotes;parzen window;pubications;cross validation;sparse modelling	A new sparse kernel probability density function (pdf) estimator based on zero-norm constraint is constructed using the classical Parzen window (PW) estimate as the target function. The so-called zero-norm of the parameters is used in order to achieve enhanced model sparsity, and it is suggested to minimize an approximate function of the zero-norm. It is shown that under certain condition, the kernel weights of the proposed pdf estimator based on the zero-norm approximation can be updated using the multiplicative nonnegative quadratic programming algorithm. Numerical examples are employed to demonstrate the efficacy of the proposed approach.	approximation algorithm;kernel (operating system);kernel density estimation;numerical method;portable document format;quadratic programming;sparse matrix;window function	Xia Hong;Sheng Chen;Christopher J. Harris	2012	Int. J. Systems Science	10.1080/00207721.2011.564673	kernel;kernel density estimation;mathematical optimization;probability density function;density estimation;pattern recognition;mathematics;quadratic programming;cross-validation;statistics	ML	29.798520583853847	-26.77339108174704	88678
31f47a776b0f486bf0e2cbbb6d44b7b0cad1247a	comparing the predictive ability of pls and covariance models	covariance analysis;predictive modeling;statistical methods;partial least squares;structural modeling structural equation modeling sem;simulation study	Partial Least Squares (PLS) is a statistical technique that is widely used in the Partial Least Squares (PLS) is a popular technique for estimating structural equation models with latent variables. It is frequently perceived as an alternative to covariance analysis of such models. While its proponents recognize the shortcomings of PLS for testing explanatory models in comparison to covariance models, PLS is instead positioned as a tool for prediction and argued to be preferable to covariance analysis for this purpose. In this paper, we present an initial study that compares the predictive ability of PLS and covariance analysis in a range of situations using a simulation study. Our results show that PLS does offer some advantages over covariance models, but that these are not the ones advocated by PLS proponents.	latent variable;partial least squares regression;simulation;structural equation modeling	Joerg Evermann;Mary Tate	2012			structural equation modeling;econometrics;predictive analytics;analysis of covariance;computer science;machine learning;partial least squares regression;statistics	ML	28.059170749628827	-24.36338900644817	89024
3c546b2aba06b93ef15c14d2abd7964d806a0534	on perturbed proximal gradient algorithms		We study a version of the proximal gradient algorithm for which the gradient is intractable and is approximated by Monte Carlo methods (and in particular Markov Chain Monte Carlo). We derive conditions on the step size and the Monte Carlo batch size under which convergence is guaranteed: both increasing batch size and constant batch size are considered. We also derive non-asymptotic bounds for an averaged version. Our results cover both the cases of biased and unbiased Monte Carlo approximation. To support our findings, we discuss the inference of a sparse generalized linear model with random effect and the problem of learning the edge structure and parameters of sparse undirected graphical models.	approximation algorithm;generalized linear model;graph (discrete mathematics);graphical model;markov chain monte carlo;monte carlo method;proximal gradient methods for learning;random effects model;sparse matrix	Yves F. Atchadé;Gersende Fort;Eric Moulines	2017	Journal of Machine Learning Research		quantum monte carlo;monte carlo method in statistical physics;quasi-monte carlo method;econometrics;mathematical optimization;diffusion monte carlo;dynamic monte carlo method;hybrid monte carlo;particle filter;markov chain monte carlo;monte carlo molecular modeling;mathematics;kinetic monte carlo;monte carlo integration;statistics;monte carlo method	ML	27.075527271898345	-28.7240635123094	89048
1a1871f1557bd767ff8048da89e267015b542fd3	feature hiding in 3d human body scans	keywords;feature recognition;search space;non linear regression;information visualization;radial basis function;data privacy;3d scanning;human body;3d scan;security;privacy	Received: 20 January 2006 Revised: 21 July 2006 Accepted: 14 September 2006 Abstract In this paper, we explore a privacy algorithm that detects human private parts in a 3D scan data set. The analogia graph is introduced to study the proportion of structures. The intrinsic human proportions are applied to reduce the search space in an order of magnitude. A feature shape template is constructed to match the model data points using Radial Basis Functions in a non-linear regression and the relative measurements of the height and area factors. The method is tested on 100 data sets from CAESAR database. Two surface rendering methods are studied for data privacy: blurring and transparency. It is found that test subjects normally prefer to have the most possible privacy in both rendering methods. However, the subjects adjusted their privacy measurement to a certain degree as they were informed of the context of security. Information Visualization (2006) 5, 271--278. doi:10.1057/palgrave.ivs.9500136	algorithm;data point;information privacy;information visualization;nonlinear system;radial (radio);radial basis function	Joseph Laws;Nathaniel Bauernfeind;Yang Cai	2006	Information Visualization	10.1057/palgrave.ivs.9500136	feature recognition;computer vision;radial basis function;human body;information visualization;computer science;pattern recognition;data mining;privacy;nonlinear regression	HCI	33.39048691402873	-31.289998875503784	89066
179604fb21c6cfdd65b6ac1201ea873a22755022	identification of sparse multivariate autoregressive models	sparse mvar models sparse ar matrix structures time series linear regression mdl formulation minimum description length formulation noise covariance multivariate autoregressive process heuristic search method;abstracts biological system modeling gold;model selection criteria;linear regression;auto regressive;time series;autoregressive model;heuristic search;minimum description length;synthetic data;time series autoregressive processes compressed sensing covariance matrices regression analysis sparse matrices	A heuristic search method is presented by which a multivariate auto-regressive (MVAR) process is identified such that its model order, sparse structure and noise covariance is accurately recovered. A novel minimum description length (MDL) formulation of time-series linear regression is derived and applied to the problem of identifying (and coding) sparse AR matrix structures such that sparsification is largely achieved in a single initial step and improved iteratively. The method was tested against synthetic data generated by known sparse MVAR processes, compared with commonly used model selection criteria (AIC, BIC) used for identification, suggesting that it is significantly more accurate and does not overfit.	akaike information criterion;autoregressive model;bayesian information criterion;heuristic;minimum description length;model selection;overfitting;sparse matrix;synthetic data;time series	Florin Popescu	2008	2008 16th European Signal Processing Conference		econometrics;star model;pattern recognition;sparse approximation;mathematics;statistics	ML	27.757598884013746	-34.08553733655479	89269
6d89992cf5854e0fb4e1be51af42ae19a70b03c1	new fault recognition method for rotary machinery based on information entropy and a probabilistic neural network	fault recognition;feature extraction;information entropy;probabilistic neural network;rotary machinery	Feature recognition and fault diagnosis plays an important role in equipment safety and stable operation of rotating machinery. In order to cope with the complexity problem of the vibration signal of rotating machinery, a feature fusion model based on information entropy and probabilistic neural network is proposed in this paper. The new method first uses information entropy theory to extract three kinds of characteristics entropy in vibration signals, namely, singular spectrum entropy, power spectrum entropy, and approximate entropy. Then the feature fusion model is constructed to classify and diagnose the fault signals. The proposed approach can combine comprehensive information from different aspects and is more sensitive to the fault features. The experimental results on simulated fault signals verified better performances of our proposed approach. In real two-span rotor data, the fault detection accuracy of the new method is more than 10% higher compared with the methods using three kinds of information entropy separately. The new approach is proved to be an effective fault recognition method for rotating machinery.	approximate entropy;artificial neural network;biological neural networks;entropy (information theory);equipment safety;fault detection and isolation;feature recognition;fuse device component;greater than;information theory;neural network simulation;performance;probabilistic neural network;r.o.t.o.r.;rotary system;rotary woofer;singular;span distance;spectral density;perineuronal net (cell component)	Quansheng Jiang;Yehu Shen;Hua Li;Fengyu Xu	2018		10.3390/s18020337	electronic engineering;entropy (information theory);feature extraction;engineering;approximate entropy;rotor (electric);feature recognition;fault detection and isolation;probabilistic neural network;spectral density;artificial intelligence;pattern recognition	Robotics	37.11678957424303	-31.1823308477081	89341
b01604391ce5a2b218588cea75605c3915aa018e	recursive algorithms for phylogenetic tree counting	biological patents;biomedical journals;text mining;europe pubmed central;citation search;physiological cellular and medical topics;citation networks;computational biology bioinformatics;research articles;abstracts;open access;life sciences;clinical guidelines;algorithms;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	In Bayesian phylogenetic inference we are interested in distributions over a space of trees. The number of trees in a tree space is an important characteristic of the space and is useful for specifying prior distributions. When all samples come from the same time point and no prior information available on divergence times, the tree counting problem is easy. However, when fossil evidence is used in the inference to constrain the tree or data are sampled serially, new tree spaces arise and counting the number of trees is more difficult. We describe an algorithm that is polynomial in the number of sampled individuals for counting of resolutions of a constraint tree assuming that the number of constraints is fixed. We generalise this algorithm to counting resolutions of a fully ranked constraint tree. We describe a quadratic algorithm for counting the number of possible fully ranked trees on n sampled individuals. We introduce a new type of tree, called a fully ranked tree with sampled ancestors, and describe a cubic time algorithm for counting the number of such trees on n sampled individuals. These algorithms should be employed for Bayesian Markov chain Monte Carlo inference when fossil data are included or data are serially sampled.	algorithm;bayesian inference in phylogeny;comefrom;computational phylogenetics;counting problem (complexity);fossil;markov chain monte carlo;monte carlo method;new type;phylogenetic tree;polynomial;sampling - surgical action;time complexity;trees (plant)	Alexandra Gavryushkina;David Welch;Alexei J. Drummond	2013		10.1186/1748-7188-8-26	random binary tree;segment tree;text mining;medical research;vantage-point tree;binary tree;computer science;bioinformatics;data science;range tree;tree rearrangement;k-d tree;incremental decision tree;data mining;k-ary tree;interval tree;fractal tree index;tree structure;search tree;tree traversal;metric tree;algorithm	ML	26.145156828562968	-25.578728859095644	89396
b1f644f4307e073fbfa00efd2cb5d5c2e17ff1f4	bayesian estimation of the global minimum variance portfolio	credible interval;global minimum variance portfolio;posterior distribution;wishart distribution;sannolikhetsteori och statistik	In this paper we consider the estimation of the weights of optimal portfolios from the Bayesian point of view under the assumption that the conditional distribution of the logarithmic returns is normal. Using the standard priors for the mean vector and the covariance matrix, we derive the posterior distributions for the weights of the global minimum variance portfolio. Moreover, we reparameterize the model to allow informative and non-informative priors directly for the weights of the global minimum variance portfolio. The posterior distributions of the portfolio weights are derived in explicit form for almost all models. The models are compared by using the coverage probabilities of credible intervals. In an empirical study we analyze the posterior densities of the weights of an international portfolio.	diffuse reflection;fundamental theorem of software engineering;information;maxima and minima;model transformation;numerical analysis	Taras Bodnar;Stepan Mazur;Yarema Okhrin	2017	European Journal of Operational Research	10.1016/j.ejor.2016.05.044	financial economics;econometrics;economics;modern portfolio theory;portfolio optimization;mathematics;wishart distribution;posterior probability;credible interval;statistics	ML	31.347901327291527	-23.99631353729685	89531
fc8aaef7a7ae8dadb83b6965475f4eaf45c438ac	vibration monitoring and faults detection using wavelet techniques	wavelet analysis;faults detection;vibrations;wavelet packet space;wavelet techniques;vibration signals;fault detection signal analysis wavelet transforms wavelet packets signal processing continuous wavelet transforms machinery fault diagnosis condition monitoring vibrations;wavelet packet;wavelet transforms;wavelet packet transform;wavelet transforms fault diagnosis turbomachinery vibrations;rotating machinery;fault detection;transforms;wavelet packets;time frequency analysis;vibration monitoring;wavelet packet space vibration monitoring faults detection wavelet techniques rotating machinery vibration signals wavelet packet transform;fault diagnosis;turbomachinery	This paper introduces an efficient approach for fault detection in rotating machinery by analyzing its vibration signals using wavelet techniques. Specifically our approach uses the wavelet packet transform (WPT) to decompose the vibration signals in the wavelet packet space, in order to reveal the transient information in these signals. Faults are efficiently detected by exploiting the mean values of the energy in the detail signals. The wavelet-based approach is also compared with the traditional Fourier-based one. Both analysis and an extensive simulation of the two approaches clearly show the superiority of the WPT-based approach over the Fourier-based one, in efficiently diagnosing faults from vibration signals.	energy level;exploit (computer security);fast fourier transform;fault detection and isolation;network packet;r.o.t.o.r.;simulation;wavelet packet decomposition	Basel Isayed;Lahouari Cheded;Fadi Al-Badour	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555438	speech recognition;mathematics;wavelet packet decomposition;quantum mechanics;statistics	Arch	37.63975084553418	-30.942070855620322	89673
f18099692cbfb39475bed67001c459820b592d82	generalized random utility model	latent variables;stated preference;functional form;generic model;estimation method;latent variable;combining datasets;unobserved heterogeneity;simulated maximum likelihood;logit kernel;discrete choice analysis;choice models;random parameters;random utility model;flexible disturbances;revealed preference;parameter estimation;mixed logit;choice behavior;latent class;kernel smoothing;covariance structure;choice model;latent classes	Researchers have long been focused on enriching Random Utility Models (RUMs) for a variety of reasons, including to better understand behavior, to improve the accuracy of forecasts, and to test the validity of simpler model structures. While numerous useful enhancements exist, they tend to be discussed and applied independently from one another. This paper presents a practical, generalized model that integrates many enhancements that have been made to RUM. In the generalized model, RUM forms the core, and then extensions are added that relax simplifying assumptions and enrich the capabilities of the basic model. The extensions that are included are:#R##N#•#TAB##R##N#Flexible Disturbances in order to allow for a rich covariance structure and enable estimation of unobserved heterogeneity through, for example, random parameters;#R##N##R##N#•#TAB##R##N#Latent Variables in order to provide a richer explanation of behavior by explicitly representing the formation and effects of latent constructs such as attitudes and perceptions;#R##N##R##N#•#TAB##R##N#Latent Classes in order to capture latent segmentation in terms of, for example, taste parameters, choice sets, and decision protocols; and#R##N##R##N#•#TAB##R##N#Combining Revealed Preferences and Stated Preferences in order to draw on the advantages of the two types of data, thereby reducing bias and improving efficiency of the parameter estimates.#R##N##R##N##R##N##R##N##R##N#The paper presents a unified framework that encompasses all models, describes each enhancement, and shows relationships between models including how they can be integrated. These models often result in functional forms composed of complex multidimensional integrals. Therefore, an estimation method consisting of Simulated Maximum Likelihood Estimation with a kernel smooth simulator is reviewed, which provides for practical estimation. Finally, the practicality and usefulness of the generalized model and estimation technique is demonstrated by applying it to a case study.		Joan L. Walker;Moshe E. Ben-Akiva	2002	Mathematical Social Sciences	10.1016/S0165-4896(02)00023-9	latent variable;econometrics;mathematics;statistics	Theory	28.489855182826293	-24.76323779058664	89775
2ad6516a50f898b56833c5f9409782ebc4686835	further rao-blackwellizing an already rao-blackwellized algorithm for jump markov state space systems	statistical distributions bayes methods kalman filters markov processes monte carlo methods particle filtering numerical methods;bayes methods;kalman filters;statistical distributions;id bayesian filtering jump markov state space system sequential monte carlo algorithm jmss jump markov linear system rbpf rao blackwellized particle filter kalman filter rbpf based moment estimation smc importance distribution;approximation methods computational modeling computational efficiency markov processes target tracking monte carlo methods mathematical model;markov processes;monte carlo methods;particle filtering numerical methods	Exact Bayesian filtering is impossible in Jump Markov State Space Systems (JMSS), even in the simple linear and Gaussian case. Suboptimal solutions include sequential Monte-Carlo (SMC) algorithms which are indeed popular, and are declined in different versions according to the JMSS considered. In particular, Jump Markov Linear Systems (JMLS) are particular JMSS for which a Rao-Blackwellized (RB) Particle Filter (PF) has been derived. The RBPF solution relies on a combination of PF and Kalman Filtering (KF), and RBPF-based moment estimators outperform purely SMC-based ones when the number of samples tends to infinity. In this paper, we show that it is possible to derive a new RBPF solution, which implements a further RB step in the already RBPF with optimal importance distribution (ID). The new RBPF-based moment estimator outperforms the classical RBPF one whatever the number of particles, at the expense of a reasonable extra computational cost.	algorithm;algorithmic efficiency;computation;kalman filter;markov chain;particle filter;state space	Yohan Petetin;François Desbouvries	2012	2012 11th International Conference on Information Science, Signal Processing and their Applications (ISSPA)	10.1109/ISSPA.2012.6310644	kalman filter;probability distribution;econometrics;mathematical optimization;markov chain monte carlo;mathematics;markov process;statistics;monte carlo method	ML	37.83671254882517	-25.977333317706876	90097
96ab746da1ceacc1af193da96e91b416796d7c86	exemplars, prototypes, similarities, and rules in category representation: an example of hierarchical bayesian analysis	cognitive science;model selection;inferences;bayesian statistics;abstract reasoning;data;bayesian inference;classification;bayesian method;evaluation methods;learning processes;hierarchical bayesian model;varying abstraction model;relational model;hierarchical bayesian models;category learning;parameter estimation;bayesian analysis;models;generalized context model;computation	This article demonstrates the potential of using hierarchical Bayesian methods to relate models and data in the cognitive sciences. This is done using a worked example that considers an existing model of category representation, the Varying Abstraction Model (VAM), which attempts to infer the representations people use from their behavior in category learning tasks. The VAM allows for a wide variety of category representations to be inferred, but this article shows how a hierarchical Bayesian analysis can provide a unifying explanation of the representational possibilities using 2 parameters. One parameter controls the emphasis on abstraction in category representations, and the other controls the emphasis on similarity. Using 30 previously published data sets, this work shows how inferences about these parameters, and about the category representations they generate, can be used to evaluate data in terms of the ongoing exemplar versus prototype and similarity versus rules debates in the literature. Using this concrete example, this article emphasizes the advantages of hierarchical Bayesian models in converting model selection problems to parameter estimation problems, and providing one way of specifying theoretically based priors for competing models.	bayesian network;cognitive science;concept learning;estimation theory;inference;model selection;population parameter;prototype;representation (action);rule (guideline);scientific publication	Michael D. Lee;Wolf Vanpaemel	2008	Cognitive science	10.1080/03640210802073697	concept learning;bayesian probability;computer science;machine learning;pattern recognition;data mining;statistics	ML	26.992431345756042	-26.149661329412066	90124
1406bd1454e5b07b4e808aee2bb64fc34433103d	registration of 3d - patterns and shapes with characteristic points		We study approximation algorithms for a matching problem that is motivated by medical applications. Given a small set of points P ⊂ R and a surface S, the optimal matching of P with S is represented by a rigid transformation which maps P as ‘close as possible’ to S. Previous solutions either require polynomial runtime of high degree or they make use of heuristic techniques which could be trapped in some local minimum. We propose a modification of the problem setting by introducing small subsets of so called characteristic points Pc ⊆ P and Sc ⊆ S, and assuming that points from Pc must be matched with points from Sc. We focus our attention on the first nontrivial case that occurs if |Pc| = 2, and show that this restriction results in new fast and reliable algorithms for the matching problem. In contrast to heuristic approaches our algorithm provides guarantees on the approximation factor of the matching. Experimental results are provided for surfaces reconstructed from real and synthetic data. 1 MOTIVATION AND RELATED	approximation algorithm;heuristic;map;matching (graph theory);maxima and minima;optimal matching;polynomial;regular expression;synthetic data;time complexity	Darko Dimitrov;Christian Knauer;Klaus Kriegel	2006			pattern recognition;computer vision;computer science;artificial intelligence	Theory	37.72095271596155	-37.185399574815776	90183
16611312448f5897c7a84e2f590617f4fa3847c4	a spectral algorithm for learning hidden markov models	modelo markov oculto;theorie automate;complexity theory;pac learning;modele markov cache;automata estado finito;hidden markov model;complexity;hidden markov models;criptografia;cryptography;probability distribution;finite automata;automata theory;cryptographie;teoria automata;finite automaton;automate fini;learning artificial intelligence;theorie complexite;apprentissage intelligence artificielle	Hidden Markov Models (HMMs) are one of the most fundamental a nd widely used statistical tools for modeling discrete time series. Typically, they are learned using sea rch heuristics (such as the Baum-Welch / EM algorithm), which suffer from the usual local optima issues. While in gen eral these models are known to be hard to learn with samples from the underlying distribution, we provide t h first provably efficient algorithm (in terms of sample and computational complexity) for learning HMMs under a nat ur l separation condition. This condition is roughly analogous to the separation conditions considered for lear ning mixture distributions (where, similarly, these model s are hard to learn in general). Furthermore, our sample compl exity results do not explicitly depend on the number of distinct (discrete) observations — they implicitly depend on this number through spectral properties of the underlyin g HMM. This makes the algorithm particularly applicable to se ttings with a large number of observations, such as those in natural language processing where the space of observati on is sometimes the words in a language. Finally, the algorithm is particularly simple, relying only on a singula r value decomposition and matrix multiplications.	baum–welch algorithm;computational complexity theory;expectation–maximization algorithm;heuristic (computer science);hidden markov model;local optimum;markov chain;natural language processing;network address translation;time series;welch's method	Daniel J. Hsu;Sham M. Kakade;Tong Zhang	2002		10.1007/3-540-45790-9_21	forward algorithm;probability distribution;markov chain;maximum-entropy markov model;markov kernel;complexity;markov property;viterbi algorithm;computer science;cryptography;continuous-time markov chain;artificial intelligence;machine learning;hidden semi-markov model;automata theory;mathematics;markov renewal process;markov process;markov model;finite-state machine;algorithm;hidden markov model;variable-order markov model	ML	25.785220938973	-29.929065056510623	90587
44de27180562162db3206f68d5097924aa0d9052	energy and entropy-based feature extraction for locating fault on transmission lines by using neural network and wavelet packet decomposition	energy;wavelet packet;feature vector;wavelet packet transform;feature extraction;wavelet packet decomposition;entropy;artificial neural network;neural network;transmission line;fault location	The aim of this paper is to estimate the fault location on transmission lines quickly and accurately. The faulty current and voltage signals obtained from a simulation are decomposed by wavelet packet transform (WPT). The extracted features are applied to artificial neural network (ANN) for estimating fault location. As data sets increase in size, their analysis become more complicated and time consuming. The energy and entropy criterion are applied to wavelet packet coefficients to decrease the size of feature vectors. The test results of ANN demonstrate that the applying of energy criterion to current signals after WPT is a very powerful and reliable method for reducing data sets in size and hence estimating fault locations on transmission lines quickly and accurately. 2007 Elsevier Ltd. All rights reserved.	artificial neural network;coefficient;feature extraction;feature vector;network packet;performance evaluation;signal processing;simulation;transmission line;wavelet packet decomposition;wavelet transform	Sami Ekici;Selcuk Yildirim;Mustafa Poyraz	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.05.011	speech recognition;second-generation wavelet transform;computer science;machine learning;pattern recognition;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;artificial neural network	Mobile	36.89784462528763	-31.426227054276758	90691
bac3cd3940f1d2f96c324a00b431da2a6076bc22	radar emitter identification based on deep convolutional neural network		Aiming at the identification and classification of radar radiation sources, this paper proposes a classification method based on the Convolutional Neural Network(CNN) for radar signal classification. Firstly, this paper sets the appropriate learning rate, batch size, iteration number, momentum and weight decay coefficient. Secondly, the time domain real part waveform signal is modeled and the network structure is selected for analysis. Finally, according to the spectrogram of the time domain waveform by Short Time Fourier Transform(STFT), design two different convolutional neural network models. The results show that the network learns more distinguishing feature representations and has better generalization capabilities after STFT. The Deep Learning of CNN has a greater advantage in extracting the feature representations in the spectrogram of the radar signal.		Mingxin Kong;Jing Zhang;Weifeng Liu;Guilin Zhang	2018	2018 International Conference on Control, Automation and Information Sciences (ICCAIS)	10.1109/ICCAIS.2018.8570480		Robotics	36.84842323759492	-32.51538124915312	90757
1e17d73e5578a091272825de5700d4972b4465d2	one-shot learning with a hierarchical nonparametric bayesian model	semi supervised learning	We develop a hierarchical Bayesian model that learns to lear n c tegories from single training examples. The model transfers acquired knowledge from prev iously learned categories to a novel category, in the form of a prior over category means and varia nces. The model discovers how to group categories into meaningful super-categories that express different priors for new classes. Given a single example of a novel category, we can efficiently i fer which super-category the novel category belongs to, and thereby estimate not only the new ca tegory’s mean but also an appropriate similarity metric based on parameters inherited from the su per-category. On MNIST and MSR Cambridge image datasets the model learns useful represent ations of novel categories based on just a single training example, and performs significantly b etter than simpler hierarchical Bayesian approaches. It can also discover new categories in a complet ely unsupervised fashion, given just one or a few examples.	bayesian network;coherence (physics);mnist database;microsoft research;one-shot learning;unsupervised learning	Ruslan Salakhutdinov;Joshua B. Tenenbaum;Antonio Torralba	2012			variable-order bayesian network;bayesian hierarchical modeling;nonparametric regression	ML	28.360628539079123	-31.750015471067346	90827
32a6ae50f2e8dbe01ae7a02d1c3440c2167b8abc	chain event graph map model selection	bayesian network;model selection;conference_paper;maximum a posteriori model;conjugate learning;qa mathematics;chain event graph	When looking for general structure from a finite discrete data set one can search over the class of Bayesian Networks (BNs). The class of Chain Event Graph (CEG) models is however much more expressive and is particularly suited to depicting hypotheses about how situations might unfold. Like the BN, the CEG admits conjugate learning on its conditional probability parameters using product Dirichlet priors. The Bayes Factors associated with different CEG models can therefore be calculated in an explicit closed form, which means that search for the maximum a posteriori (MAP) model in this class can be enacted by evaluating the score function of successive models and optimizing. Local search algorithms can be devised for the class of candidate models, but in this paper we concentrate on the process of scoring the members of this class.	bayesian network;consistency model;discrete mathematics;local search (optimization);model selection;naive bayes classifier;search algorithm	Peter A. Thwaites;Guy Freeman;Jim Q. Smith	2009			computer science;artificial intelligence;machine learning;pattern recognition;bayesian network;model selection	AI	24.84676611497244	-28.093111679839915	90900
99327486c76c8907b9c64562220a451056cd9b1e	black-box autoregressive density estimation for state-space models		Formally, a SSM is based on a latent Markov process Xti at times ti = 0,∆t, 2∆t, . . . , N for some ∆t > 0. The SSM has initial density p(xt0) and evolves through a transition density Xti | ( Xti−1 = xti−1 ) ∼ p(xti |xti−1 , θ). Observations Yti of the latent process are available according to an observation likelihood Yti | (Xti = xt) ∼ p(yti |xti , θ). Here θ denotes the set of global latent variables that govern the above densities.	autoregressive model;latent variable;markov chain;state space;x/open transport interface	Tom Ryder;Andrew Golightly;A. Stephen McGough;Dennis Prangle	2018	CoRR			ML	31.67022977858685	-25.804699777904503	90903
05e70f50a161b085ca3d9221fcb875784286b92c	submodular decomposition framework for inference in associative markov networks with global constraints	graph theory;associative pairwise terms;graph structure submodular decomposition inference framework associative markov networks global constraints discrete markov random field associative pairwise terms np hard problem;discrete markov random field;bayesian inference;discrete mathematics;inference mechanisms;global constraint;inference algorithms minimization optimization markov random fields labeling vectors;submodular decomposition inference framework;markov random field;computer vision;np hard problem;graph structure;machine learning;computational complexity;associative markov networks;pattern recognition;markov network;approximate inference;energy minimization;markov processes;global constraints;markov processes computational complexity graph theory inference mechanisms	In this paper we address the problem of finding the most probable state of discrete Markov random field (MRF) with associative pairwise terms. Although of practical importance, this problem is known to be NP-hard in general. We propose a new type of MRF decomposition, submod-ular decomposition (SMD). Unlike existing decomposition approaches SMD decomposes the initial problem into sub-problems corresponding to a specific class label while preserving the graph structure of each subproblem. Such decomposition enables us to take into account several types of global constraints in an efficient manner. We study theoretical properties of the proposed approach and demonstrate its applicability on a number of problems.	markov chain;markov random field;np-hardness;service mapping description;submodular set function	Anton Osokin;Dmitry P. Vetrov;Vladimir Kolmogorov	2011	CVPR 2011	10.1109/CVPR.2011.5995361	combinatorics;computer science;graph theory;machine learning;pattern recognition;np-hard;mathematics;markov process;computational complexity theory;bayesian inference;energy minimization;statistics	Vision	25.321646832289193	-28.53278613903533	90940
a6213ebccd768d369d237fbbc771c649d4063673	cvt fault diagnosis based on immune principle and oil spectrum analysis	mature detectors set cvt fault diagnosis immune principle oil spectrum analysis artificial immune principle negative selection method;detectors;negative selection;spectrum analysis;cvt fault diagnosis;artificial immune principle;iron;manganese;cvt artificial immune principle negative selection method oil spectrum analysis fault diagnosis;mature detectors set;petroleum industry;immune principle;negative selection method;cvt;manganese iron algorithm design and analysis detectors;petroleum industry artificial immune systems fault diagnosis;oil spectrum analysis;artificial immune systems;algorithm design and analysis;fault identification;fault diagnosis	By introducing the artificial immune principle into CVT fault diagnosis, using the negative selection method, generate the initial detectors set, then stimulate the set into mature detectors set. Utilizing the mature detectors set to diagnose CVT fault status. And it is proved that the detectors set are effective on CVT fault diagnosis and Fault identification.	sensor	Shuai Gao;Ying An;Yun-Shan Zhou;Chuan-Xue Song	2010	2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)	10.1109/BICTA.2010.5645100	structural engineering;control engineering;engineering;forensic engineering	Robotics	37.71418214496493	-29.15008210984474	91040
72b981aca3cfada21e23e6b92b4ff3dab6fbffef	natural gradients in practice: non-conjugate variational inference in gaussian process models		The natural gradient method has been used effectively in conjugate Gaussian process models, but the non-conjugate case has been largely unexplored. We examine how natural gradients can be used in non-conjugate stochastic settings, together with hyperparameter learning. We conclude that the natural gradient can significantly improve performance in terms of wall-clock time. For illconditioned posteriors the benefit of the natural gradient method is especially pronounced, and we demonstrate a practical setting where ordinary gradients are unusable. We show how natural gradients can be computed efficiently and automatically in any parameterization, using automatic differentiation. Our code is integrated into the GPflow package.		Hugh Salimbeni;Stefanos Eleftheriadis;James Hensman	2018			mathematical optimization;automatic differentiation;conjugate;machine learning;mathematics;hyperparameter;artificial intelligence;parametrization;gradient method;inference;gaussian process	ML	26.383387544876665	-30.5522076939631	91255
e53bcc47a37b093e35973b5ec12ea892bfe88b2e	automatic equipment fault fingerprint extraction for the fault diagnostic on the batch process data		Abstract Equipment condition monitoring in semiconductor manufacturing requires prompt, accurate, and sensitive detection and classification of equipment and process faults. Efficient and effective fault diagnostic is essential to minimizing scrapped wafers, reducing unscheduled equipment downtime, and consequently maintaining high production throughput and product yields. Through analyzing the equipment sensor signals as the batch process data, i.e., process timestamp × sensor × wafer, this paper firstly applies the well-known Support Vector Machine (SVM) classifier to detect the abnormal observations. In the second stage, the normal process dynamics are decomposed into different clusters by K -Means clustering. Each part of the process dynamics is further modelled by Principal Component Analysis (PCA). Fault fingerprints then can be extracted by consolidating the out of control scenarios after projecting the abnormal observations into the PCA models. An empirical study is conducted in collaboration with a local IC maker in France to validate the methodology. The result shows that the proposed approach can effectively detect abnormal observations as well as automatically classify the proper fault fingerprints to give evident guidelines in explaining the known faults.	batch processing;fingerprint	Hamideh Rostami;Jakey Blue;Claude Yugma	2018	Appl. Soft Comput.	10.1016/j.asoc.2017.10.029	mathematics;condition monitoring;batch processing;support vector machine;fault coverage;real-time computing;principal component analysis;fault indicator;cluster analysis;downtime	Robotics	37.06356639395158	-30.36617330055055	91262
ac107538037316fc24238dce7420692fbe8bb8c6	combinatorial stochastic processes and nonparametric bayesian modeling	stochastic process;bayesian model	"""Computer Science has historically been strong on data structures and weak on inference from data, whereas Statistics has historically been weak on data structures and strong on inference from data. One way to draw on the strengths of both disciplines is to develop """"inferential methods for data structures""""; i.e., methods that update probability distributions on recursively-defined objects such as trees, graphs, grammars and function calls. This is the world of """"nonparametric Bayes,"""" where prior and posterior distributions are allowed to be general stochastic processes. Both statistical and computational considerations lead one to certain classes of stochastic processes, and these tend to have interesting connections to combinatorics. I will give some examples of how this blend of ideas leads to useful models in some applied problem domains, including natural language parsing, computational vision, statistical genetics and protein structural modeling."""	bayesian network;stochastic process	Michael I. Jordan	2009			econometrics;mathematical optimization;combinatorics;computer science;machine learning;mathematics;bayesian statistics;bayesian inference;algorithm;statistics	Robotics	26.6646168640973	-26.91564574371523	91341
08493fc62a4b7559fb02339c6a2dd95a1d849f08	avoiding bias when aggregating relational data with degree disparity	relational data;aggregation function;relational learning	A common characteristic of relational data sets —degree disparity—can lead relational learning algorithms to discover misleading correlations. Degree disparity occurs when the frequency of a relation is correlated with the values of the target variable. In such cases, aggregation functions used by many relational learning algorithms will result in misleading correlations and added complexity in models. We examine this problem through a combination of simulations and experiments. We show how two novel hypothesis testing procedures can adjust for the effects of using aggregation functions in the presence of degree disparity.	aggregate function;algorithm;authorization;autocorrelation;binocular disparity;experiment;ibm notes;international conference on machine learning;logistic regression;machine learning;missing data;sampling (signal processing);simulation;spatial analysis	David D. Jensen;Jennifer Neville;Michael Hay	2003			statistical relational learning;relational database;computer science;machine learning;data mining;mathematics;statistics	ML	28.29821492092871	-31.43944673796933	91610
a399311fafe04860e553a7b93e3c327ac51b6274	an algorithm for clustering and classification of series data with constraint of contiguity	geotechnics;segmentation;classification;machine learning;boundary energy;constraint clustering	Clustering and classification of series-based data is an important issue in a number of engineering problems, in particular in geotechnics. In such problems intervals of measured seria (signals) are. to be attributed a class so that the constraint of contiguity have to be considered and standard classification methods could be inadequate. Classification in this case needs involvement of an expert who observes the magnitude and trends of the signals in addition to any a priori information that might be available. In this paper an approach for automating this classification procedure is presented. Firstly, a segmentation algorithm is applied to segment the measured signals. Secondly, the salient features of these segments are extracted using boundary energy method. Based on the measured data and extracted features classifiers to assign class values to the segments were built; they employ decision trees and artificial neural networks. The algorithm was tested in a case-study for classifying sub-surface soil using measured data from cone penetration testing and satisfactory results were obtained.	algorithm	Biswanath Bhattacharya;Dimitri P. Solomatine	2003			machine learning;pattern recognition;data mining;mathematics	DB	34.793029176031375	-35.97544134755839	91939
86d1619ecc6154a59ea7010e346151d4760fe43d	use of support vector regression in structural optimization: application to vehicle crashworthiness design	vehicle lightweight design;support vector regression;crashworthiness;structural optimization;metamodel	Metamodel is widely used to deal with analysis and optimization of complex system. Structural optimization related to crashworthiness is of particular importance to automotive industry nowadays, which involves highly nonlinear characteristics with material and structural parameters. This paper presents two industrial cases using support vector regression (SVR) for vehicle crashworthiness design. The first application aims to improve roof crush resistance force, and the other is lightweight design of vehicle front end structure subject to frontal crash, where SVR is utilized to construct crashworthiness responses. The use of multiple instances of SVR with different kernel types and hyper-parameters simultaneously and select the best accurate one for subsequent optimization is proposed. The case studies present the successful use of SVR for structural crashworthiness design. It is also demonstrated that SVR is a promising alternative for approximating highly nonlinear crash problems, showing a successfully alternative for metamodel-based design optimization in practice.	mathematical optimization;shape optimization;support vector machine	Ping Zhu;Feng Pan;Wei Chen;Siliang Zhang	2012	Mathematics and Computers in Simulation	10.1016/j.matcom.2011.11.008	metamodeling;support vector machine;simulation;machine learning	EDA	36.95526740386449	-29.311129170319788	91982
3aa47e092b948c675b97d65090da5b7cbf8a3990	comparison of kullback-leibler divergence approximation methods between gaussian mixture models for satellite image retrieval	photogrammetrie und bildanalyse;remote sensing image retrieval;unscented transformation kullback leibler divergence approximation methods gaussian mixture models satellite image retrieval change detection statistical models probability density functions approximation methods matched bond approximation gaussian product variational method;gaussian mixture model;image retrieval gaussian mixture model gmm kullback leibler divergence;yttrium;satellites;approximation methods;approximation methods monte carlo methods yttrium image retrieval satellites gaussian mixture model;monte carlo methods;image retrieval	In many applications, such as image retrieval and change detection, we need to assess the similarity of two statistical models. As a distance measure between two probability density functions, Kullback-Leibler divergence is widely used for comparing two statistical models. Unfortunately, for some models such as Gaussian Mixture Model (GMM), Kullback-Leibler divergence has no analytically tractable formula. We have to resort to approximation methods. In this paper, we compare seven methods, namely Monte Carlo method, matched bond approximation, product of Gaussian, variation-al method, unscented transformation, Gaussian approximation, and min-Gaussian approximation, for approximating the Kullback-Leibler divergence between two Gaussian mixture models for satellite image retrieval. Two image retrieval experiments based on two publicly available datasets have been performed. The comparison is carried out in terms of both retrieval performance and computational time.	approximation algorithm;cobham's thesis;computation;experiment;image retrieval;kullback–leibler divergence;maxima and minima;mixture model;monte carlo method;statistical model;time complexity	Shiyong Cui;Mihai Datcu	2015	2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2015.7326631	gaussian random field;econometrics;image retrieval;computer science;yttrium;pattern recognition;mixture model;mathematics;gaussian function;physics;satellite;statistics;monte carlo method;divergence-from-randomness model	Vision	28.59326865283338	-28.60375901448984	92032
47d6e8b73bc604d8655cc862b1395766b0839350	on the effect of the form of the posterior approximation in variational learning of ica models	bayes estimation;modelizacion;calculo de variaciones;tecnologia electronica telecomunicaciones;linear independence;computacion informatica;melangeage;dynamic model;modele lineaire;variational bayesian;ruido no gaussiano;modelo lineal;independent component analysis;independent component analysis variational bayesian learning;non gaussian noise;modelisation;calcul variationnel;estimacion bayes;ciencias basicas y experimentales;principal component analysis;linear model;variational bayesian learning;component analysis;analyse composante independante;mixture of gaussians;tecnologias;analisis componente independiente;grupo a;mixing;second order statistics;modeling;mezclado;variational calculus;estimation bayes;bruit non gaussien	We show that the choice of posterior approximation affects the solution found in Bayesian variational learning of linear independent component analysis models. Assuming the sources to be independent a posteriori favours a solution which has orthogonal mixing vectors. Linear mixing models with either temporally correlated sources or non-Gaussian source models are considered but the analysis extends to nonlinear mixtures as well.	approximation;calculus of variations;independent computing architecture;independent component analysis;nonlinear system;variational principle	Alexander Ilin;Harri Valpola	2005	Neural Processing Letters	10.1007/s11063-005-5265-0	independent component analysis;linear independence;econometrics;systems modeling;computer science;machine learning;calculus;linear model;mixture model;mathematics;mixing;statistics;calculus of variations;principal component analysis	ML	28.660170924421337	-26.89897670659577	92230
078393296ab4a0579e2c2913bb03033b61ee504e	smooth on-line learning algorithms for hidden markov models	cross entropy;learning algorithm;modelo markov;learning;maximization;hidden markov model;on line;en linea;generalized expectation maximization;baum welch;algorithme;aprendizaje;algorithm;apprentissage;markov model;expectation maximization;expectation;speech recognition;en ligne;modele markov;optimal algorithm;maximizacion;expectacion;maximisation;on line learning;algoritmo	A simple learning algorithm for Hidden Markov Models (HMMs) is presented together with a number of variations. Unlike other classical algorithms such as the Baum-Welch algorithm, the algorithms described are smooth and can be used on-line (after each example presentation) or in batch mode, with or without the usual Viterbi most likely path approximation. The algorithms have simple expressions that result from using a normalized-exponential representation for the HMM parameters. All the algorithms presented are proved to be exact or approximate gradient optimization algorithms with respect to likelihood, log-likelihood, or cross-entropy functions, and as such are usually convergent. These algorithms can also be casted in the more general EM (Expectation-Maximization) framework where they can be viewed as exact or approximate GEM (Generalized Expectation-Maximization) algorithms. The mathematical properties of the algorithms are derived in the appendix.	approximation algorithm;batch processing;baum–welch algorithm;cross entropy;expectation–maximization algorithm;gradient;hidden markov model;markov chain;mathematical optimization;online and offline;online machine learning;time complexity;welch's method	Pierre Baldi;Yves Chauvin	1994	Neural Computation	10.1162/neco.1994.6.2.307	randomized algorithms as zero-sum games;mathematical optimization;probabilistic analysis of algorithms;weighted majority algorithm;expectation–maximization algorithm;computer science;baum–welch algorithm;machine learning;mathematics;markov model;cross entropy;approximation algorithm;hidden markov model;expected value;statistics	ML	25.516702773160773	-30.9728444285151	92343
010c98c20dbae52f9adb5a95df2769f7939bde35	gp-sum. gaussian processes filtering of non-gaussian beliefs		This work studies the problem of stochastic dynamic filtering and state propagation with complex beliefs. The main contribution is GP-SUM, a filtering algorithm tailored to dynamic systems and observation models expressed as Gaussian processes (GP), that does not rely on linearizations or unimodal Gaussian approximations of the belief. The algorithm can be seen as a combination of a sampling-based filter and a probabilistic Bayes filter. GP-SUM operates by sampling the state distribution and propagating each sample through the dynamic system and observation models. Effective sampling and accurate probabilistic propagation are possible by relying on the GP form of the system, and a Gaussian mixture form of the belief. We show that GP-SUM outperforms several GP-Bayes and Particle Filters on a standard benchmark. We also illustrate its practical use in a pushing task, and demonstrate that it predicts heteroscedasticity, i.e., different amounts of uncertainty, and multi-modality when naturally occurring in pushing.	algorithm;approximation;benchmark (computing);dynamical system;gaussian orbital;gaussian process;low-pass filter;modality (human–computer interaction);particle filter;sampling (signal processing);software propagation	Maria Bauzá;Alberto Rodriguez	2017	CoRR		filter (signal processing);mathematical optimization;control engineering;engineering;sampling (statistics);heteroscedasticity;recursive bayesian estimation;probabilistic logic;gaussian;gaussian random field;gaussian process	ML	38.48838079415445	-26.28586937317962	92404
54d2a37b305d4abf302f68ef2af6ee3145ef9921	general linear mixed model and signal extraction problem with constraint	62j12;linear estimation;62j05;generalized linear mixed model;auto regressive;signal extraction;blue;auto regressive model;linear system;asymptotic property;maximum likelihood estimate;inverse problem;blup;maximum likelihood estimator;covariance matrices;asymptotic properties;generalized inverse;best linear unbiased predictor	We consider a noisy observed vector y = x + u ∈ Rn. The unobserved vector x is a solution of a non-invertible linear system Ax = v, where v is a forcing term. A unique solution of the system is obtained by considering additional constraint on the vector x. This constraint is defined by a triple (β, F,A), where β is a vector, F denotes a matrix whose range is equal to N (A) (the null space of A) and A is a generalized inverse of A. Each triple (β, F,A) defines the solution x = Fβ + Av and the general linear mixed model y = Fβ +Av + u. Given the covariance matrices of u and v, we will prove that the best linear unbiased predictor of x knowing y depends only on A. If β is a parameter and (F,A) is given, then we will study the asymptotic behavior of the best linear estimator of β. If the constraint (β, F,A) is not known, then we will estimate it using the data y. Some numerical results will be given. © 2011 Elsevier Inc. All rights reserved.	kernel (linear algebra);kerrison predictor;linear system;mixed model;numerical analysis	Azzouz Dermoune;Nadji Rahmania;Tianwen Wei	2012	J. Multivariate Analysis	10.1016/j.jmva.2011.10.007	econometrics;mathematical optimization;mathematics;maximum likelihood;statistics	Theory	29.924909006460567	-25.577640291785205	92436
6e21a7079825b0d6e87782da6851325829123952	on a hybrid data cloning method and its application in generalized linear mixed models	asymptotic normality;generalized linear mixed models;data cloning;integrated nested laplace approximation	Data cloning method is a new computational tool for computing maximum likelihood estimates in complex statistical models such as mixed models. The data cloning method is synthesized with integrated nested Laplace approximation to compute maximum likelihood estimates efficiently via a fast implementation in generalized linear mixed models. Asymptotic normality of the hybrid data cloning based distribution is established aided by modification of Stein’s Identity. The results are illustrated through a series of well known examples. It is shown that the proposed method as well as normal approximation perform very well and justify the theory.	approximation;mixed model;statistical model	Hossein Baghishani;Håvard Rue;Mohsen Mohammadzadeh	2012	Statistics and Computing	10.1007/s11222-011-9254-z	econometrics;generalized linear mixed model;mathematical optimization;mathematics;asymptotic distribution;statistics	ML	31.157526571559025	-25.90500741530266	92534
2b85c85b3b72220b178cc62fa07a35afad28798c	the kernel kalman rule - efficient nonparametric inference with recursive least squares		Nonparametric inference techniques provide promising tools for probabilistic reasoning in high-dimensional nonlinear systems. Most of these techniques embed distributions into reproducing kernel Hilbert spaces (RKHS) and rely on the kernel Bayes’ rule (KBR) to manipulate the embeddings. However, the computational demands of the KBR scale poorly with the number of samples and the KBR often suffers from numerical instabilities. In this paper, we present the kernel Kalman rule (KKR) as an alternative to the KBR. The derivation of the KKR is based on recursive least squares, inspired by the derivation of the Kalman innovation update. We apply the KKR to filtering tasks where we use RKHS embeddings to represent the belief state, resulting in the kernel Kalman filter (KKF). We show on a nonlinear state estimation task with high dimensional observations that our approach provides a significantly improved estimation accuracy while the computational demands are significantly decreased.	computation;expectation–maximization algorithm;hilbert space;kalman filter;kernel (operating system);key-based routing;mathematical optimization;nonlinear system;numerical analysis;numerical stability;partially observable system;recursion (computer science);recursive least squares filter;sum rule in quantum mechanics	Gregor H. W. Gebhardt;Andras Gabor Kupcsik;Gerhard Neumann	2017			kernel embedding of distributions;kernel (linear algebra);machine learning;computer science;artificial intelligence;mathematical optimization;recursive least squares filter;kalman filter;bayes' theorem;nonparametric statistics;inference;reproducing kernel hilbert space	AI	25.690188047880845	-31.33513937728929	92574
5f1eacc95dbb2cee662e9b5ca81b507e52e1dc33	a generalised labelled multi-bernoulli filter for extended multi-target tracking	clutter;standards;approximation algorithms;performance improvement generalised labelled multibernoulli filter extended multitarget tracking clutter target kinematics measurement rates multitarget state modelling gamma gaussian inverse wishart distribution ggiw distribution single extended target probability hypothesis density cardinalised phd filters ggiw mixtures glmb based approach extended target cphd filter;target tracking filtering theory gamma distribution gaussian distribution;conference paper;inverse wishart multi target tracking extended targets random finite sets;computational modeling;target tracking approximation methods clutter computational modeling standards approximation algorithms mathematical model;mathematical model;approximation methods;target tracking	This paper addresses extended multi-target tracking in clutter, i.e. tracking targets that may produce more than one measurement on each scan. We propose a new algorithm for solving this problem, that is capable of initiating and maintaining labelled estimates of the target kinematics, measurement rates and extents. Our proposed technique is based on modelling the multi-target state as a generalised labelled multi-Bernoulli (GLMB), combined with the gamma Gaussian inverse Wishart (GGIW) distribution for a single extended target. Previously, probability hypothesis density (PHD) and cardinalised PHD (CPHD) filters based on GGIW mixtures have been proposed to solve the extended target tracking problem. Although these are computationally cheaper, they involve significant approximations, as well as lacking the ability to maintain target tracks over time. Here, we compare our proposed GLMB-based approach to the extended target PHD/CPHD filters, and show that the GLMB has improved performance.	algorithm;approximation;bernoulli polynomials;clutter;euler–bernoulli beam theory;simulation;the australian	Michael Beard;Stephan Reuter;Karl Granström;Ba-Tuong Vo;Ba-Ngu Vo;Alexander Scheel	2015	2015 18th International Conference on Information Fusion (Fusion)		mathematical optimization;machine learning;mathematics;statistics	Robotics	39.13523441892963	-26.800682476627262	92664
9f03a0c1b22aec1d0d4f1300353c0bc45ff9338f	on renyi's entropy estimation with one-dimensional gaussian kernels	hierarchical clustering;entropy kernel estimation standards gaussian distribution clustering algorithms transforms;gaussian kernels;k nearest neigbour algorithm renyi s entropy estimation one dimensional gaussian kernels signal processing plug in kernel density estimation hermite expansion gaussian distribution;renyi s entropy estimation;hermite expansion;hierarchical clustering renyi s entropy estimation gaussian kernels hermite expansion;signal processing gaussian distribution	Rényi's entropies play a significant role in many signal processing applications. Plug-in kernel density estimation methods have been employed to estimate such entropies with good results. However, they become computationally intractable in higher dimensions, because of the requirement to store intermediate probability density values for a large number of data points. We propose a method to reduce the number of the samples in a plug-in kernel density estimation method for Rényi's entropies of real exponents and to improve the result of the standard plug-in kernel density method. To this end, we derive a univariate estimator, using an Hermite expansion of sums of Gaussian kernels and a hierarchical clustering of the samples. On simulated data from a univariate Gaussian distribution, our method performs better than a k-nearest neigbour algorithm and other kernel density estimation methods.	algorithm;cluster analysis;computational complexity theory;data point;entropy estimation;hierarchical clustering;kernel density estimation;plug-in (computing);requirement;signal processing	Septimia Sarbu	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472510	gaussian random field;kernel density estimation;mathematical optimization;computer science;machine learning;pattern recognition;multivariate kernel density estimation;mathematics;hierarchical clustering;gaussian function;variable kernel density estimation;statistics	Robotics	30.159412335616032	-29.634290621880805	92673
f5725d5db1f668c3d7322df4e34dc24bb992cd64	nonlinear multiple regression methods: a survey and extensions	multiple regression;piecewise linear	This paper reviews some nonlinear statistical procedures useful in function approximation, classifi cation, regression and time-series analysis. Primary emphasis is on piecewise linear models such as multivariate adaptive regression splines, adaptive logic networks, hinging hyperplanes and their conceptual differences. Potential and actual applications of these methods are cited. Software for implementation is discussed, and practical suggestions are given for improvement. Examples show the relative capabilities of the various methods, including their ability for universal approximation. Copyright © 2010 John Wiley & Sons, Ltd.	john d. wiley;linear model;multivariate adaptive regression splines;nonlinear system;piecewise linear continuation;smoothing spline;spline (mathematics);time series;universal approximation theorem	Kenneth O. Cogger	2010	Int. Syst. in Accounting, Finance and Management	10.1002/isaf.311	econometrics;mathematical optimization;piecewise linear function;multivariate adaptive regression splines;linear regression;machine learning;bayesian multivariate linear regression;polynomial regression;statistics	ML	27.468947378966945	-24.615241819724826	92823
43173b1bc1f0747190055c785c84b54b7a2337bc	hybrid approach based on temporal representation and classification techniques used to determine unstable conditions in a blast furnace	deslizamiento;signal analysis;stack;alto horno;pila;analisis de senal;intelligence artificielle;classification;glissement;blast furnace;slip;haut fourneau;artificial intelligence;inteligencia artificial;clasificacion;pile memoire;analyse signal	This paper discusses the analysis of differential pressure signals in a blast furnace stack, by a hybrid approach based on temporal representation of process trends and classification techniques. The objective is to determine whether these can be used to predict unstable conditions (slips). First, episode analysis is performed on each individual trend. Next, using the obtained episodes and variable magnitudes, the classification tool is trained to predict and detect the fault in a blast furnace. The proposed approach has been selected in this application, due to the best results obtained using the qualitative representations of process variables instead of only raw data.	control theory	Juan Mora;Joan Colomer-Llinas;Joaquím Meléndez;Francisco Gamero;Peter Warren	2003		10.1007/978-3-540-25945-9_40	stack;biological classification;computer science;artificial intelligence;signal processing;slip;programming language;operations research	NLP	36.14783994692146	-31.5319390681492	92867
116c030ab58bd4521db685806fc010da55d4cb23	relations between the conditional normalized maximum likelihood distributions and the latent information priors	density measurement;bayes methods;probability density function;maximum likelihood estimation;renyi divergence bayes projection conditional mutual information kullback leibler divergence least favorable prior regret;lips;mutual information;predictive models;bayes methods lips mutual information predictive models maximum likelihood estimation density measurement probability density function;conditional normalized maximum likelihood distributions lip bayesian predictive densities cnml3 latent information priors	We reveal the relations between the conditional normalized maximum likelihood (CNML) distributions and Bayesian predictive densities based on the latent information priors (LIPs). In particular, CNML3, which is one type of CNML distributions, is investigated. The Bayes projection of a predictive density, which is an information projection of the predictive density on a set of Bayesian predictive densities, is considered. We prove that the sum of the Bayes projection divergence of CNML3 and the conditional mutual information is asymptotically constant. This result implies that the Bayes projection of CNML3 (BPCNML3) is asymptotically identical to the Bayesian predictive density based on LIP. In addition, under some stronger assumptions, we show that BPCNML3 exactly coincides with the Bayesian predictive density based on LIP.	conditional mutual information;information projection	Mutsuki Kojima;Fumiyasu Komaki	2016	IEEE Transactions on Information Theory	10.1109/TIT.2015.2496581	econometrics;probability density function;bayes' rule;bayes factor;naive bayes classifier;pattern recognition;bayesian network;mathematics;predictive modelling;maximum likelihood;mutual information;bayes' theorem;statistics	ML	29.157320778953864	-27.919759157293413	92908
8c01a5bbee1a519bcbccf1b05f5cc06b74dd2099	scalable inference of overlapping communities		We develop a scalable algorithm for posterior inference of overlapping communities in large networks. Our algorithm is based on stochastic variational inference in the mixed-membership stochastic blockmodel (MMSB). It naturally interleaves subsampling the network with estimating its community structure. We apply our algorithm on ten large, real-world networks with up to 60,000 nodes. It converges several orders of magnitude faster than the state-of-the-art algorithm for MMSB, finds hundreds of communities in large real-world networks, and detects the true communities in 280 benchmark networks with equal or better accuracy compared to other scalable algorithms.	algorithm;benchmark (computing);chroma subsampling;scalability;variational principle	Prem Gopalan;David M. Mimno;Sean Gerrish;Michael J. Freedman;David M. Blei	2012			theoretical computer science;machine learning;data mining;mathematics	ML	25.74601905735374	-32.91703996399167	92944
861b91b72764326afc8445417c8918ffd6cdc712	decentralized k-means using randomized gossip protocols for clustering large datasets	pattern clustering;optimisation;protocols clustering algorithms convergence vectors optimization partitioning algorithms data models;randomised algorithms distributed processing optimisation pattern clustering;distributed processing;randomised algorithms;randomized gossip protocols;very large dataset clustering objective function centralized k means algorithm message exchange randomized gossip aggregation protocol network node codebook decentralized k means algorithm computational units;distributed clustering;randomized gossip protocols distributed clustering	In this paper, we consider the clustering of very large datasets distributed over a network of computational units using a decentralized K-means algorithm. To obtain the same codebook at each node of the network, we use a randomized gossip aggregation protocol where only small messages are exchanged. We theoretically show the equivalence of the algorithm with a centralized K-means, provided a bound on the number of messages each node has to send is met. We provide experiments showing that the consensus is reached for a number of messages consistent with the bound, but also for a smaller number of messages, albeit with a less smooth evolution of the objective function.	centralized computing;cluster analysis;codebook;computation;experiment;gossip protocol;k-means clustering;loss function;optimization problem;randomized algorithm;turing completeness	Jérôme Fellus;David Picard;Philippe Henri Gosselin	2013	2013 IEEE 13th International Conference on Data Mining Workshops	10.1109/ICDMW.2013.58	correlation clustering;data stream clustering;computer science;theoretical computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;distributed computing	ML	31.853277300548903	-35.073585082814205	93044
3f6e1701114c510ce23b6a9c0caaf970daab7a4d	interpretable nonnegative matrix decompositions	matrix decompositions;alternating least squares;column row decompositions;information technology;nonnegative matrix;nonnegative matrices;data analysis;matrix decomposition;least square;experimental evaluation;local search	A matrix decomposition expresses a matrix as a product of at least two factor matrices. Equivalently, it expresses each column of the input matrix as a linear combination of the columns in the first factor matrix. The interpretability of the decompositions is a key issue in many data-analysis tasks. We propose two new matrix-decomposition problems: the nonnegative CX and nonnegative CUR problems, that give naturally interpretable factors. They extend the recently-proposed column and column-row based decompositions, and are aimed to be used with nonnegative matrices. Our decompositions represent the input matrix as a nonnegative linear combination of a subset of its columns (or columns and rows).  We present two algorithms to solve these problems and provide an extensive experimental evaluation where we assess the quality of our algorithms' results as well as the intuitiveness of nonnegative CX and CUR decompositions. We show that our algorithms return intuitive answers with smaller reconstruction errors than the previously-proposed methods for column and column-row decompositions.	algorithm;column (database);digital molecular matter (dmm);experiment;k-means clustering;model selection;multi-factor authentication;nikon cx format;non-negative matrix factorization;singular value decomposition;synthetic intelligence	Saara Hyvönen;Pauli Miettinen;Evimaria Terzi	2008		10.1145/1401890.1401935	metzler matrix;mathematical optimization;combinatorics;discrete mathematics;nonnegative matrix;local search;mathematics;data analysis;matrix decomposition;information technology;least squares	ML	28.738764844421837	-35.72152510151614	93444
39567ff03c781202f8556ff6f0806d5b60850a23	characterizing response behavior in multisensory perception with conflicting cues	gaussian noise;heavy tail;conference paper;mixture model	We explore a recently proposed mixture model approach to understanding interactions between conflicting sensory cues. Alternative model formulations, differing in their sensory noise models and inference methods, are compared based on their fit to experimental data. Heavy-tailed sensory likelihoods yield a better description of the subjects’ response behavior than standard Gaussian noise models. We study the underlying cause for this result, and then present several testable predictions of these models.	interaction;mixture model	Rama Natarajan;Iain Murray;Ladan Shams;Richard S. Zemel	2008			gaussian noise;computer vision;speech recognition;heavy-tailed distribution;computer science;mixture model;mathematics;statistics	ML	27.184519935161436	-25.992874766797346	93509
24239aa9f271ebed0c8b273d9a8a8dd88f2898d2	fast multidimensional entropy estimation by $k$-d partitioning	recursive estimation;multidimensional systems entropy estimation multidimensional signal processing;recursive estimation adaptive estimation entropy multidimensional signal processing nonparametric statistics;image processing;nonparametric statistics;nonparametric estimation;multidimensional system;probability density function;multidimensional signal processing fast multidimensional differential entropy estimation k d partitioning nonparametric estimator recursive rectilinear partitioning adaptive partitioning method;recursive rectilinear partitioning;fast multidimensional differential entropy estimation;random variables;state estimation;journal article;limit set;genetics;entropy estimation;estimation;multidimensional signal processing;nonparametric estimator;entropy;adaptive partitioning method;k d partitioning;multidimensional systems entropy recursive estimation state estimation multidimensional signal processing random variables probability density function context image processing genetics;context;multidimensional systems;adaptive estimation	We describe a nonparametric estimator for the differential entropy of a multidimensional distribution, given a limited set of data points, by a recursive rectilinear partitioning. The estimator uses an adaptive partitioning method and runs in Theta(N log N) time, with low memory requirements. In experiments using known distributions, the estimator is several orders of magnitude faster than other estimators, with only modest increase in bias and variance.	binary space partitioning;data point;differential entropy;digital electronics;entropy estimation;equivalence partitioning;experiment;gnu;matlab;recursion;regular grid;requirement	Dan Stowell;Mark D. Plumbley	2009	IEEE Signal Processing Letters	10.1109/LSP.2009.2017346	minimax estimator;mathematical optimization;minimum-variance unbiased estimator;multidimensional systems;image processing;pattern recognition;mathematics;bias of an estimator;consistent estimator;statistics	ML	31.329371115606463	-28.934818399777445	94096
3fcbcb179fa9838c3a5ee2866b86e1ab7c1f95d6	compressive sampling of ensembles of correlated signals		We propose several sampling architectures for the efficient acquisition of an ensemble of correlated signals. We show that without prior knowledge of the correlation structure, each of our architectures (under different sets of assumptions) can acquire the ensemble at a sub-Nyquist rate. Prior to sampling, the analog signals are diversified using simple, implementable components. The diversification is achieved by injecting types of “structured randomness” into the ensemble, the result of which is subsampled. For reconstruction, the ensemble is modeled as a low-rank matrix that we have observed through an (undetermined) set of linear equations. Our main results show that this matrix can be recovered using standard convex programming techniques when the total number of samples is on the order of the intrinsic degree of freedom of the ensemble — the more heavily correlated the ensemble, the fewer samples are needed. To motivate this study, we discuss how such ensembles arise in the context of array processing.	analog signal;array processing;compressed sensing;convex optimization;diversification (finance);linear equation;nyquist rate;randomness;sampling (signal processing)	Ali Ahmed;Justin K. Romberg	2011	CoRR		econometrics;machine learning;mathematics;ensemble learning;statistics	ML	34.366848749502665	-29.241611713785627	94119
8689a6d5ee3c5ced892461aaffaf87f32bffc093	low-rank factorization of determinantal point processes		Determinantal point processes (DPPs) have garnered attention as an elegant probabilistic model of set diversity. They are useful for a number of subset selection tasks, including product recommendation. DPPs are parametrized by a positive semi-definite kernel matrix. In this work we present a new method for learning the DPP kernel from observed data using a low-rank factorization of this kernel. We show that this low-rank factorization enables a learning algorithm that is nearly an order of magnitude faster than previous approaches, while also providing for a method for computing product recommendation predictions that is far faster (up to 20x faster or more for large item catalogs) than previous techniques that involve a full-rank DPP kernel. Furthermore, we show that our method provides equivalent or sometimes better test loglikelihood than prior full-rank DPP approaches.	algorithm;association rule learning;digital photo professional (dpp);gal's accurate tables;high memory;kernel (operating system);low-rank approximation;peripheral;scalability;semiconductor industry;statistical model	Mike Gartrell;Ulrich Paquet;Noam Koenigstein	2017			mathematical optimization;computer science;discrete mathematics;rank factorization;factorization of polynomials;point process;determinantal point process	ML	25.635160335621244	-34.6321893997995	94313
afa6391db57a3757009142f5f897a9a36c851437	informative subspace learning for counterfactual inference		Inferring causal relations from observational data is widely used for knowledge discovery in healthcare and economics. To investigate whether a treatment can affect an outcome of interest, we focus on answering counterfactual questions of this type: what would a patient’s blood pressure be had he/she recieved a different treatment? Nearest neighbor matching (NNM) sets the counterfactual outcome of any treatment (control) sample to be equal to the factual outcome of its nearest neighbor in the control (treatment) group. Although being simple, flexible and interpretable, most NNM approaches could be easily misled by variables that do not affect the outcome. In this paper, we address this challenge by learning subspaces that are predictive of the outcome variable for both the treatment group and control group. Applying NNM in the learned subspaces leads to more accurate estimation of the counterfactual outcomes and therefore treatment effects. We introduce an informative subspace learning algorithm by maximizing the nonlinear dependence between the candidate subspace and the outcome variable measured by the Hilbert-Schmidt Independence Criterion (HSIC). We propose a scalable estimator of HSIC, called HSIC-RFF that reduces the quadratic computational and storage complexities (with respect to the sample size) of the naive HSIC implementation to linear through constructing random Fourier features. We also prove an upper bound on the approximation error of the HSIC-RFF estimator. Experimental results on simulated datasets and real-world datasets demonstrate our proposed approach outperforms existing NNM approaches and other commonly used regression-based methods for counterfactual inference.	algorithm;approximation error;causal filter;counterfactual conditional;information;nearest neighbor search;nonlinear system;scalability;schmidt decomposition;usb	Yale Chang;Jennifer G. Dy	2017			machine learning;counterfactual thinking;computer science;inference;subspace topology;artificial intelligence;pattern recognition	ML	24.761116126882868	-26.256164281929806	94582
6eb5caca5dbdcd4491263670740f3ba92981d335	on the stability the least squares monte carlo	stock price;least squares problem;least squares monte carlo;computational finance	Consider least squares Monte Carlo (LSM) algorithm, which is proposed by Longstaff and Schwartz (Rev Financial Studies 14:113–147, 2001) for pricing American style securities. This algorithm is based on the projection of the value of continuation onto a certain set of basis functions via the least squares problem. We analyze the stability of the algorithm when the number of exercise dates increases and prove that, if the underlying process for the stock price is continuous, then the regression problem is ill-conditioned for small values of the time parameter.	algorithm;basis function;condition number;continuation;least squares;monte carlo method;rev	Oleksii Mostovyi	2013	Optimization Letters	10.1007/s11590-011-0414-z	generalized least squares;total least squares;iteratively reweighted least squares;least squares support vector machine;simple linear regression;econometrics;mathematical optimization;non-linear iterative partial least squares;computational finance;least trimmed squares;lack-of-fit sum of squares;residual sum of squares;mathematics;partial least squares regression;explained sum of squares;non-linear least squares;least squares;linear least squares;nonlinear regression;recursive least squares filter	ML	34.73477910439196	-24.581833106621986	94596
991010fe69e3b77292df4fde52de0ae9cdc0adc7	multi-scan generalized labeled multi-bernoulli filter		This paper extends the generalized labeled multi-Bernoulli (GLMB) tracking filter to a batch multi-target tracker. In a labeled random finite set formulation, a multi-target tracking filter propagates the labeled multi-target filtering density while a batch multi-target tracker propagates the labeled multi-target posterior density. The GLMB filter is an analytic solution to the labeled multi-target filtering recursion. In this work, we show that the GLMB filter can be extended to an analytic multi-object posterior recursion.	bernoulli polynomials;euler–bernoulli beam theory;recursion	Ba-Tuong Vo;Ba-Ngu Vo	2018	2018 21st International Conference on Information Fusion (FUSION)	10.23919/ICIF.2018.8455419	artificial intelligence;machine learning;computer science;recursion;filter (signal processing);mathematical optimization;bernoulli's principle;finite set	Robotics	38.98036354460681	-26.7434242654833	94697
4f3965fe16384c3994bc6c6f98f61a156c4c33cf	data-driven compression and efficient learning of the choquet integral		"""The Choquet integral (ChI) is a parametric nonlinear aggregation function defined with respect to the fuzzy measure (FM). To date, application of the ChI has sadly been restricted to problems with relatively few numbers of inputs; primarily as the FM has <inline-formula><tex-math notation=""""LaTeX"""">$2^N$</tex-math></inline-formula> variables for <inline-formula><tex-math notation=""""LaTeX"""">$N$</tex-math></inline-formula> inputs and <inline-formula> <tex-math notation=""""LaTeX"""">$N(2^{N-1}-1)$</tex-math></inline-formula> monotonicity constraints. In return, the community has turned to density-based imputation (e.g., Sugeno <inline-formula><tex-math notation=""""LaTeX"""">$\lambda$ </tex-math></inline-formula>-FM) or the number of <italic>interactions</italic> (FM variables) are restricted (e.g., <inline-formula><tex-math notation=""""LaTeX"""">$k$</tex-math></inline-formula>-additivity). Herein, we propose a new scalable data-driven way to represent and learn the ChI, making learning computationally manageable for larger <inline-formula><tex-math notation=""""LaTeX"""">$N$</tex-math></inline-formula>. First, data supported variables are identified and used in optimization. Identification of these variables also allows us recognize future ill-posed fusion scenarios; ChIs involving variable subsets not supported by data. Second, we outline an imputation function framework to address data unsupported variables. Third, we present a lossless way to compress redundant variables and associated monotonicity constraints. Finally, we outline a lossy approximation method to further compress the ChI (if/when desired). Computational complexity analysis and experiments conducted on synthetic datasets with known FMs demonstrate the effectiveness and efficiency of the proposed theory."""	analysis of algorithms;approximation;computation;computational complexity theory;experiment;fm broadcasting;fuzzy measure theory;geo-imputation;lossless compression;lossy compression;mathematical optimization;nonlinear system;scalability;synthetic intelligence;well-posed problem	Muhammad Aminul Islam;Derek T. Anderson;Anthony J. Pinar;Timothy C. Havens	2018	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2017.2755002	artificial intelligence;lossy compression;mathematics;machine learning;control theory;fuzzy logic;imputation (statistics);nonlinear system;monotonic function;mathematical optimization;parametric statistics;choquet integral;lossless compression	ML	29.23510199850759	-35.86571557920061	94749
1d024ebc866489d64c9615358eabb70e03d03bfd	cartan-sync: fast and global se(d)-synchronization	maximum likelihood estimation;synchronization;optimization;laplace equations;simultaneous localization and mapping;context	"""This work addresses the fundamental problem of pose graph optimization (PGO), which is pervasive in the context of SLAM, and widely known as <inline-formula><tex-math notation=""""LaTeX"""">$\text{SE}(d)$</tex-math></inline-formula> -synchronization in the mathematical community. Our contribution is twofold. First, we provide a novel, elegant, and compact matrix formulation of the maximum likelihood estimation (MLE) for this problem, drawing interesting connections with the connection Laplacian of a graph object. Second, even though the MLE problem is nonconvex and computationally intractable in general, we exploit recent advances in convex relaxations of PGO and Riemannian techniques for low-rank optimization to yield an <italic>a posteriori certifiably globally optimal</italic> algorithm [A. Bandeira, “A note on probably certifiably correct algorithms,” <italic>Comptes Rendus Mathematique </italic>, vol. 354, pp. 329–333, 2016.] that is also <italic>fast</italic> and <italic>scalable</italic>. This work builds upon a fairly demanding mathematical machinery, but beyond the theoretical basis presented, we demonstrate its performance through extensive experimentation in common large-scale SLAM datasets. The proposed framework, <monospace>Cartan-Sync</monospace>, is up to one order of magnitude faster that the state-of-the-art <monospace>SE-Sync </monospace> [D. M. Rosen <italic>et al.</italic> “A certifiably correct algorithm for synchronization over the special Euclidean group,” in <italic>Proc. Int. Workshop Algorithmic Found. Robot.</italic>, 2016.] in some important scenarios (e.g., the KITTI dataset). We make the code for <monospace>Cartan-Sync</monospace> available at <uri>bitbucket.org/jesusbriales/cartan-sync</uri>, along with some examples and guides for a friendly use by researchers in the field, hoping to promote further adoption and exploitation of these techniques in the robotics community."""	algorithm;bitbucket;centre de morphologie mathématique;computational complexity theory;mathematical optimization;matrix mechanics;national lidar dataset;profile-guided optimization;robotics;simultaneous localization and mapping;uniform resource identifier	Jesus Briales;Javier Gonzalez-Jimenez	2017	IEEE Robotics and Automation Letters	10.1109/LRA.2017.2718661		Robotics	32.45864332065769	-35.49958555779859	94751
3ecad1ea9124304da308a6d096e3b68806782ed5	scoring bayesian networks of mixed variables	bayesian network structure learning;continuous and discrete variables;mixed variables	In this paper we outline two novel scoring methods for learning Bayesian networks in the presence of both continuous and discrete variables, that is, mixed variables. While much work has been done in the domain of automated Bayesian network learning, few studies have investigated this task in the presence of both continuous and discrete variables while focusing on scalability. Our goal is to provide two novel and scalable scoring functions capable of handling mixed variables. The first method, the Conditional Gaussian (CG) score, provides a highly efficient option. The second method, the Mixed Variable Polynomial (MVP) score, allows for a wider range of modeled relationships, including nonlinearity, but it is slower than CG. Both methods calculate log likelihood and degrees of freedom terms, which are incorporated into a Bayesian Information Criterion (BIC) score. Additionally, we introduce a structure prior for efficient learning of large networks and a simplification in scoring the discrete case which performs well empirically. While the core of this work focuses on applications in the search and score paradigm, we also show how the introduced scoring functions may be readily adapted as conditional independence tests for constraint-based Bayesian network learning algorithms. Lastly, we describe ways to simulate networks of mixed variable types and evaluate our proposed methods on such simulations.	algorithm;bayesian information criterion;bayesian network;conditional (computer programming);handling (psychology);machine learning;nonlinear system;normal statistical distribution;polynomial;programming paradigm;scalability;score;scoring functions for docking;simulation;text simplification;ovarian carcinosarcoma	Bryan Andrews;Joseph Ramsey;Gregory F. Cooper	2017	International Journal of Data Science and Analytics	10.1007/s41060-017-0085-7	scalability;nonlinear system;conditional independence;bayesian network;gaussian;bayesian information criterion;polynomial;pattern recognition;mathematics;artificial intelligence	ML	24.6627092749625	-29.982592475197475	94949
4b98b8c94ad762bf3ca7c5c3044226699e171238	time and sample efficient discovery of markov blankets and direct causal relations	bayesian network;local algorithm;statistical method;novel data mining algorithms;data mining;scaling up;variable selection;graph connectivity;data mining algorithm;causal relation;bayesian networks;robust and scalable statistical methods	Data Mining with Bayesian Network learning has two important characteristics: under conditions learned edges between variables correspond to casual influences, and second, for every variable T in the network a special subset (Markov Blanket) identifiable by the network is the minimal variable set required to predict T. However, all known algorithms learning a complete BN do not scale up beyond a few hundred variables. On the other hand, all known sound algorithms learning a local region of the network require an exponential number of training instances to the size of the learned region.The contribution of this paper is two-fold. We introduce a novel local algorithm that returns all variables with direct edges to and from a target variable T as well as a local algorithm that returns the Markov Blanket of T. Both algorithms (i) are sound, (ii) can be run efficiently in datasets with thousands of variables, and (iii) significantly outperform in terms of approximating the true neighborhood previous state-of-the-art algorithms using only a fraction of the training size required by the existing methods. A fundamental difference between our approach and existing ones is that the required sample depends on the generating graph connectivity and not the size of the local region; this yields up to exponential savings in sample relative to previously known algorithms. The results presented here are promising not only for discovery of local causal structure, and variable selection for classification, but also for the induction of complete BNs.	bayesian network;causal filter;connectivity (graph theory);data mining;feature selection;local algorithm;markov blanket;markov chain;time complexity	Ioannis Tsamardinos;Constantin F. Aliferis;Alexander R. Statnikov	2003		10.1145/956750.956838	computer science;machine learning;causal markov condition;bayesian network;data mining;mathematics;feature selection;statistics	ML	24.995171003596678	-28.423464573289696	95215
ad552295636437e00570c2c3d56b32395cb01ed8	second-order latent-space variational bayes for approximate bayesian inference	second order;variational approximation;belief networks;latent variables;model selection;variational bayes;variational techniques bayes methods belief networks gaussian processes inference mechanisms;approximate bayesian inference framework;gaussian processes;latent variable;variational bayesian expectation maximization;bayes methods;bayesian inference;variational techniques;bayesian methods;variational bayesian;inference mechanisms;exponential family;approximate bayesian inference;higher order statistics;gaussian mixture model;first order;hidden markov models;conjugate exponential family;expectation maximization;bayesian methods context modeling higher order statistics predictive models encoding computer science monte carlo methods convergence;variational inference;gaussian mixture models;conjugate exponential family models;gaussian mixture models latent space variational bayes approximate bayesian inference framework conjugate exponential family models latent variables variational inference variational bayesian expectation maximization;mixture of gaussians;approximation methods;latent space variational bayes;variational method;covariance matrix;variational method bayesian inference conjugate exponential family latent variable mixture of gaussians model selection	In this letter, we consider a variational approximate Bayesian inference framework, latent-space variational Bayes (LSVB), in the general context of conjugate-exponential family models with latent variables. In the LSVB approach, we integrate out model parameters in an exact way and then perform the variational inference over only the latent variables. It can be shown that LSVB can achieve better estimates of the model evidence as well as the distribution over the latent variables than the popular variational Bayesian expectation-maximization (VBEM). However, the distribution over the latent variables in LSVB has to be approximated in practice. As an approximate implementation of LSVB, we propose a second-order LSVB (SoLSVB) method. In particular, VBEM can be derived as a special case of a first-order approximation in LSVB (Sung). SoLSVB can capture higher order statistics neglected in VBEM and can therefore achieve a better approximation. Examples of Gaussian mixture models are used to illustrate the comparison between our method and VBEM, demonstrating the improvement.	approximation algorithm;calculus of variations;computation;convex function;expectation–maximization algorithm;first-order predicate;hessian;jensen's inequality;latent dirichlet allocation;latent variable;latent variable model;linear function;mixture model;monte carlo method;numerical analysis;order of approximation;social inequality;time complexity;variational principle	JaeMo Sung;Zoubin Ghahramani;Sung Yang Bang	2008	IEEE Signal Processing Letters	10.1109/LSP.2008.2001557	latent variable;econometrics;computer science;variational message passing;pattern recognition;mixture model;mathematics;hidden markov model;statistics	ML	28.450036297853018	-30.25408762569371	95682
a7f669a6dbb6adfd1aaf02cf77160f7a59aea299	the fastclime package for linear programming and large-scale precision matrix estimation in r	biological patents;biomedical journals;text mining;europe pubmed central;citation search;sparse precision matrix;citation networks;research articles;abstracts;open access;high dimensional data;life sciences;clinical guidelines;linear programming;undirected graphical model;full text;rest apis;orcids;europe pmc;biomedical research;parametric simplex method;bioinformatics;literature search	We develop an R package fastclime for solving a family of regularized linear programming (LP) problems. Our package efficiently implements the parametric simplex algorithm, which provides a scalable and sophisticated tool for solving large-scale linear programs. As an illustrative example, one use of our LP solver is to implement an important sparse precision matrix estimation method called CLIME (Constrained L1 Minimization Estimator). Compared with existing packages for this problem such as clime and flare, our package has three advantages: (1) it efficiently calculates the full piecewise-linear regularization path; (2) it provides an accurate dual certificate as stopping criterion; (3) it is completely coded in C and is highly portable. This package is designed to be useful to statisticians and machine learning researchers for solving a wide range of problems.	dual;linear programming;machine learning;scalability;simplex algorithm;solver;sparse matrix;xara flare	Haotian Pang;Han Liu;Robert J. Vanderbei	2014	Journal of machine learning research : JMLR		text mining;computer science;linear programming;data science;theoretical computer science;machine learning;data mining;mathematics;statistics;clustering high-dimensional data	ML	26.357575588460378	-33.83196886743551	95921
2508d9e84620775d1784586251e017a2e399ec36	learning nonlinear overcomplete representations for efficient coding	overcomplete representation;learning algorithm;probabilistic model;independent component	We derive a learning algorithm for inferring an overcomplete basis by viewing it as probabilistic model of the observed data. Overcomplete bases allow for better approximation of the underlying statistical density. Using a Laplacian prior on the basis coefficients removes redundancy and leads to representations that are sparse and are a nonlinear function of the data. This can be viewed as a generalization of the technique of independent component analysis and provides a method for blind source separation of fewer mixtures than sources. We demonstrate the utility of overcomplete representations on natural speech and show that compared to the traditional Fourier basis the inferred representations potentially have much greater coding efficiency. A traditional way to represent real-values signals is with Fourier or wavelet bases. A disadvantage of these bases, however, is that they are not specialized for any particular dataset. Principal component analysis (PCA) provides one means for finding an basis that is adapted for a dataset, but the basis vectors are restricted to be orthogonal. An extension of PCA called independent component analysis (Jutten and Herault, 1991; Comon et al., 1991; Bell and Sejnowski, 1995) allows the learning of non-orthogonal bases. All of these bases are complete in the sense that they span the input space, but they are limited in terms of how well they can approximate the dataset's statistical density. Representations that are overcomplete, i. e. more basis vectors than input variables, can provide a better representation, because the basis vectors can be specialized for Learning Nonlinear Overcomplete Representations for Efficient Coding 557 a larger variety of features present in the entire ensemble of data. A criticism of overcomplete representations is that they are redundant, i.e. a given data point may have many possible representations, but this redundancy is removed by the prior probability of the basis coefficients which specifies the probability of the alternative representations. Most of the overcomplete bases used in the literature are fixed in the sense that they are not adapted to the structure in the data. Recently Olshausen and Field (1996) presented an algorithm that allows an overcomplete basis to be learned. This algorithm relied on an approximation to the desired probabilistic objective that had several drawbacks, including tendency to breakdown in the case of low noise levels and when learning bases with higher degrees of overcompleteness. In this paper, we present an improved approximation to the desired probabilistic objective and show that this leads to a simple and robust algorithm for learning optimal overcomplete bases. 1 Inferring the representation The data, X 1 :L ' are modeled with an overcomplete linear basis plus additive noise:	additive white gaussian noise;algorithmic efficiency;approximation algorithm;basis (linear algebra);blind signal separation;coefficient;data point;independent component analysis;natural language;nonlinear system;principal component analysis;source separation;sparse matrix;statistical model;utility functions on indivisible goods;wavelet	Michael S. Lewicki;Terrence J. Sejnowski	1997			statistical model;theoretical computer science;machine learning;pattern recognition;mathematics;statistics	ML	29.584488185443337	-33.54459348464111	96086
68a3cd2d28100f0ef613271b8ca2c07908346181	an improved structural em to learn dynamic bayesian nets	bayesian network;bayes methods;expectation maximisation algorithm bayes methods;global maximizer structural expectation maximization dynamic bayesian network learning structure bayesian information criterion augmented bayesian network structural constraints structural method;incomplete data;expectation maximization;dynamic bayesian network;bayesian information criterion;bayesian methods approximation methods machine learning approximation algorithms equations uncertainty;expectation maximisation algorithm	This paper addresses the problem of learning structure of Bayesian and Dynamic Bayesian networks from incomplete data based on the Bayesian Information Criterion. We describe a procedure to map the problem of the dynamic case into a corresponding augmented Bayesian network through the use of structural constraints. Because the algorithm is exact and anytime, it is well suitable for a structural Expectation--Maximization (EM) method where the only source of approximation is due to the EM itself. We show empirically that the use a global maximizer inside the structural EM is computationally feasible and leads to more accurate models.	algorithmic efficiency;anytime algorithm;approximation;bayesian information criterion;dynamic bayesian network;expectation–maximization algorithm;local optimum	Cassio Polpo de Campos;Zhi Zeng;Qiang Ji	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.152	bayesian average;bayes factor;bayesian experimental design;variable-order bayesian network;wake-sleep algorithm;bayesian programming;expectation–maximization algorithm;computer science;machine learning;causal markov condition;pattern recognition;bayesian network;mathematics;bayesian linear regression;bayesian hierarchical modeling;bayesian statistics;bayesian econometrics;bayesian information criterion;dynamic bayesian network;statistics	Robotics	27.655554655475118	-30.314898467459724	96157
a7063d9d122d5d859fcb231bcf751615079733a8	an intelligent soft-computing texture classification system		"""The aim of this research work was to obtain a system that classifies texture. This so called Texture Classification System is not a system for one special task or group of tasks. It is a general approach that shows a way towards real artificial vision. Finding ways to enable computerised systems to visually recognise its surroundings is of increasing importance for the industry and society at large. To reach this goal not only objects but less well describable texture has to be identified within an image. To achieve this aim a number of objectives had to be met. At first a review of how natural vision works was done to better understand the complexity of visual systems. This is followed by a more detailed definition of what texture is. Next a review of image processing techniques, of statistical methods and of soft-computing methods was made to identify those that can be used or improved for the Texture Classification System. A major objective was to create the structure of the Texture Classification System. The design presented in this work is the framework for a multitude of modules arranged in groups and layers with multiple feedback and optimisation possibilities. The main achievement is a system for texture classification for which natural vision was used as a """" blue-print"""". A more detailed definition of what texture is was made and a new texture library was started. The close review of image processing techniques provided a variety of applicable methods, as did the review and enhancement of statistical methods. Some of those methods were improved or used in a new way. Neural networks and fuzzy clustering were applied for classification, while genetic algorithms provide a means for self optimisation. The concepts and methods have been used for a number of projects next to texture classification itself. This work presents applications for fault detection in glass container manufacturing, quality control of veneer, positioning control of steel blocks in a rotation oven, and measurement of hair gloss."""		Alexander Stolpmann	2005			computer vision;soft computing;artificial intelligence;computer science	Robotics	36.69949882196379	-37.873829665453464	96351
603a5c61a965cce9f9fb9456c5dad5e453db856a	local entropy map: a nonparametric approach to detecting spatially varying multivariate relationships	multiple testing;linear regression model;statistical test;spatial data mining;minimum spanning tree;linear quadratic;entropy;synthetic data;spatial analysis;scan statistic;scan statistics;multivariate data;local analysis	The relationship between two or more variables may change over the geographic space. The change can be in parameter values (e.g., regression coefficients) or even in relation forms (e.g., linear, quadratic, or exponential). Existing local spatial analysis methods often assume a relationship form (e.g., a linear regression model) for all regions and focus only on the change in parameter values. Therefore, they may not be able to discover local relationships of different forms simultaneously. This research proposes a nonparametric approach, a local entropy map, which does not assume a prior relationship form and can detect the existence of multivariate relationships regardless of their forms. The local entropy map calculates an approximation of the Rényi entropy for the multivariate data in each local region (in the geographic space). Each local entropy value is then converted to a p-value by comparing to a distribution of permutation entropy values for the same region. All p-values (one for each local region) are processed by several statistical tests to control the multiple-testing problem. Finally, the testing results are mapped and allow analysts to locate and interactively examine significant local relationships. The method is evaluated with a series of synthetic data sets and a real data set.	approximation;bivariate data;coefficient;dimensionality reduction;experiment;feature selection;file spanning;interactivity;mandelbrot set;minimum spanning tree;principal component analysis;quadratic function;rényi entropy;sensor;signal-to-noise ratio;spatial analysis;synthetic data;synthetic intelligence;time complexity;user interface;world wide web	Diansheng Guo	2010	International Journal of Geographical Information Science	10.1080/13658811003619143	econometrics;entropy;multivariate statistics;statistical hypothesis testing;binary entropy function;transfer entropy;maximum entropy probability distribution;linear regression;principle of maximum entropy;minimum spanning tree;pattern recognition;mathematics;spatial analysis;multiple comparisons problem;statistics;synthetic data	ML	31.876761765641774	-30.55214010141406	96395
7ba1c409455ccd3eb9f96da8771de6ebf41607be	latent space non-linear statistics		Given data, deep generative models, such as variational autoencoders (VAE) and generative adversarial networks (GAN), train a lower dimensional latent representation of the data space. The linear Euclidean geometry of data space pulls back to a nonlinear Riemannian geometry on the latent space. The latent space thus provides a low-dimensional nonlinear representation of data and classical linear statistical techniques are no longer applicable. In this paper we show how statistics of data in their latent space representation can be performed using techniques from the field of nonlinear manifold statistics. Nonlinear manifold statistics provide generalizations of Euclidean statistical notions including means, principal component analysis, and maximum likelihood fits of parametric probability distributions. We develop new techniques for maximum likelihood inference in latent space, and adress the computational complexity of using geometric algorithms with high-dimensional data by training a separate neural network to approximate the Riemannian metric and cometric tensor capturing the shape of the learned data manifold.	approximation algorithm;artificial neural network;autoencoder;computation;computational complexity theory;dataspaces;dimensionality reduction;fits;generative adversarial networks;mnist database;nonlinear programming;nonlinear system;principal component analysis;principal geodesic analysis;simulation;synthetic data;variational principle	Line Kühnel;Tom Fletcher;Sarang C. Joshi;Stefan Sommer	2018	CoRR		tensor;riemannian geometry;manifold;probability distribution;statistics;principal component analysis;artificial neural network;computer science;nonlinear system;parametric statistics	ML	29.19861466466778	-34.84647835418121	96420
4981740d8fb2328c08a908c62bb18777658cb80f	automatic transaction of signal via statistical modeling	akaike information criterion;traitement signal;kullback leibler information;predictive distribution;procesamiento informacion;analisis estadistico;analisis datos;maximum likelihood;maximum vraisemblance;prior knowledge;signal extraction;statistical model;data analysis;statistical analysis;signal processing;analyse statistique;information processing;analyse donnee;traitement information;procesamiento senal;likelihood function;maxima verosimilitud	The statistical information processing can be characterized by using the likelihood function defined by giving an explicit form for an approximation to the true distribution from which the data are generated. This mathematical representation as an approximation, which is usually called a model, is built based on not only the current data but also prior knowledge on the object and the objective of the analysis. Akaike ([2] and [3]) showed that the log-likelihood can be considered as an estimate of the Kullback-Leibler information which defines the similarity between the predictive distribution of the model and the true distribution and proposed the Akaike information criterion (AIC). By the use of this AIC, it becomes possible to evaluate and compare the goodness of many models objectively and it enables us to select the best model among many candidates. In consequence, the minimum AIC procedure allows us to develop automatic modeling and signal extraction procedures. In this study, we give a simple explanation of statistical modeling based on the AIC and demonstrate four examples of applying the minimum AIC procedure to an automatic transaction of signals observed in the earth sciences. In each case, the AIC plays an important role in making the procedure automatic and objective, and promises to realize a detail examination of a large amount of data sets, which provides us with an opportunity to discover new information.	statistical model	Genshiro Kitagawa;Tomoyuki Higuchi	1998		10.1007/3-540-49292-5_33	statistical model;econometrics;posterior predictive distribution;akaike information criterion;information processing;machine learning;signal processing;pattern recognition;mathematics;maximum likelihood;likelihood function;data analysis;statistics	EDA	33.625253126713766	-26.547467553948998	97085
428f479e90b785d0c6452f84b450bbdbdfc1180f	exploring multimodal data fusion through joint decompositions with flexible couplings	tensor decompositions;big data tensor decompositions coupled decompositions data fusion multimodal data cramer rao bound;tensile stress;approximation algorithms;bayes methods;data fusion;additive gaussian models multimodal data fusion flexible couplings bayesian framework joint posterior distributions conditional gaussian coupling bayesian cramer rao bound hybrid coupling models joint tensor decompositions joint maximum a posteriori estimator joint map estimator;big data;matrix decomposition;coupled decompositions;sensor fusion bayes methods gaussian distribution maximum likelihood estimation;couplings tensile stress data models data integration bayes methods matrix decomposition approximation algorithms;multimodal data;couplings;cramer rao bound;data integration;data models	A Bayesian framework is proposed to define flexible coupling models for joint tensor decompositions of multiple datasets. Under this framework, a natural formulation of the data fusion problem is to cast it in terms of a joint maximum a posteriori (MAP) estimator. Data-driven scenarios of joint posterior distributions are provided, including general Gaussian priors and non Gaussian coupling priors. We present and discuss implementation issues of algorithms used to obtain the joint MAP estimator. We also show how this framework can be adapted to tackle the problem of joint decompositions of large datasets. In the case of a conditional Gaussian coupling with a linear transformation, we give theoretical bounds on the data fusion performance using the Bayesian Cramér-Rao bound. Simulations are reported for hybrid coupling models ranging from simple additive Gaussian models to Gamma-type models with positive variables and to the coupling of data sets which are inherently of different size due to different resolution of the measurement devices.	algorithm;booting;charge-coupled device;cold start;column (database);computer simulation;condition number;coupling (computer programming);current–voltage characteristic;direct coupling;emoticon;experiment;gradient;hp 48 series;image scaling;markov chain monte carlo;mathematical optimization;monte carlo method;multimodal interaction;optimization problem;performance;sampling (signal processing);scalability;signal-to-noise ratio;sparse matrix;unfolding (dsp implementation);utility functions on indivisible goods	Rodrigo Cabral Farias;Jeremy E. Cohen;Pierre Comon	2016	IEEE Transactions on Signal Processing	10.1109/TSP.2016.2576425	data modeling;econometrics;cramér–rao bound;big data;computer science;data integration;pattern recognition;mathematics;sensor fusion;coupling;stress;matrix decomposition;statistics	ML	28.374751388664837	-32.737760546203745	97454
71fd617ff3087c2ff69322cfe78baf7969e62c1a	blazing fast time series segmentation based on update techniques for polynomial approximations	optimal segmentation;least squares approximations;piecewise linear approximation;standards;approximation algorithms;piecewise polynomial techniques;linear approximation;time series;polynomials time series analysis linear approximation approximation algorithms piecewise linear approximation standards;polynomials;blazing fast time series segmentation optimal segmentation offline optseg technique sliding window and bottom up online swab algorithm orthogonal polynomials growing time windows sliding time windows time series length computational complexity least squares linear approximations piecewise constant piecewise polynomial approximations time series analysis time series processing update techniques;swab;time series computational complexity least squares approximations piecewise polynomial techniques;time series analysis;computational complexity;sliding window and bottom up;orthogonal polynomials;time series segmentation;temporal data mining time series segmentation optimal segmentation swab sliding window and bottom up orthogonal polynomials;temporal data mining	Segmentation is an important step in processing and analyzing time series. In this article, we present an approach to speed up some standard time series segmentation techniques. Often, time series segmentation is based on piecewise polynomial approximations of the time series (including piecewise constant or linear approximations as special cases). Basically, a least-squares fit with a polynomial has a computational complexity that depends on the number of observations, i.e., the length of the time series. To improve the computational complexity of segmentation techniques we exploit the fact that approximations have to be repeated in sliding (moving) or growing time windows. Therefore, we suggest to use update techniques for the approximations that determine the approximating polynomial in a sliding or growing time window from an already existing one with a computational complexity that is independent of the number of observations, i.e., the length of the window. For that purpose bases of orthogonal polynomials must be used instead of standard bases such as monomials. We take two standard techniques for segmentation - the on-line algorithm SWAB (Sliding Window And Bottom-up) and the off-line technique OptSeg (Optimal Segmentation) - and show that the run-times can be reduced substantially for a given polynomial degree. If run-time constraints are given, e.g., in real-time applications, it would also be possible to adapt the degree of the approximating polynomials. Higher polynomial degrees typically result in lower modeling errors or longer segments. The various properties of the new realizations of segmentation techniques are outlined by means of some benchmark time series. The experimental results show that, depending on the chosen parameterization, OptSeg can be accelerated by some orders of magnitude, SWAB by a factor of up to about ten.	activity recognition;approximation algorithm;approximation theory;benchmark (computing);best, worst and average case;big data;c++;computational complexity theory;degree of a polynomial;embedded system;java;least squares;microsoft windows;monomial;numerical analysis;online algorithm;online and offline;real-time clock;real-time computing;time series;top-down and bottom-up design;unsupervised learning	Andre Gensler;Thiemo Gruber;Bernhard Sick	2013	2013 IEEE 13th International Conference on Data Mining Workshops	10.1109/ICDMW.2013.90	mathematical optimization;combinatorics;discrete mathematics;machine learning;time series;mathematics;approximation algorithm;statistics	DB	32.93264162801732	-34.52866074750554	97564
e5ba23e5379b2900f49eccd9fb85fa8077a04350	non-negative matrix factorization with exogenous inputs for modeling financial data		Non-negative matrix factorization (NMF) is an effective dimensionality reduction technique that extracts useful latent spaces from positive value data matrices. Constraining the factors to be positive values, and via additional regularizations, sparse representations, sometimes interpretable as part-based representations have been derived in a wide range of applications. Here we propose a model suitable for the analysis of multi-variate financial time series data in which the variation in data is explained by latent subspace factors and contributions from a set of observed macro-economic variables. The macro-economic variables being external inputs, the model is termed XNMF (eXogenous inputs NMF). We derive a multiplicative update algorithm to learn the factorization, empirically demonstrate that it converges to useful solutions on real data and prove that it is theoretically guaranteed to monotonically reduce the objective function. On share prices from the FTSE 100 index time series, we show that the proposed model is effective in clustering stocks in similar trading sectors together via the latent representations learned.		Steven Squires;Luis Montesdeoca;Adam Prügel-Bennett;Mahesan Niranjan	2017		10.1007/978-3-319-70096-0_89	machine learning;artificial intelligence;time series;multiplicative function;factorization;pattern recognition;dimensionality reduction;cluster analysis;matrix decomposition;latent class model;finance;mathematics;non-negative matrix factorization	ML	28.657510529822158	-35.8552110812536	97736
2750447dcafe61b6201e98672e83b77b3a1bbe20	iterative conditional fitting for gaussian ancestral graph models	structure learning;conditional independence;bayesian network;markov random eld;markov property;graph model;conditional distribution	Ancestral graph models, introduced by Richardson and Spirtes (2002), generalize both Markov random fields and Bayesian networks. A key feature of ancestral graph models is that the global Markov property is closed under conditioning and marginalization. The conditional independence structures that can be encoded by ancestral graphs coincide with the structures that can arise from a Bayesian network with selection and unobserved variables. Thus, association structures learned via ancestral graph models may be interpreted causally. In this paper, we consider Gaussian ancestral graph models and present an algorithm for maximum likelihood estimation. We call this new algorithm iterative conditional fitting since in each step of the procedure, a conditional distribution is estimated, subject to constraints, while a marginal distribution is held fixed. This approach is in duality to the well-known iterative proportional fitting algorithm, in which marginal distributions are fitted while conditional distributions are held fixed.	algorithm;ancestral graph;bayesian network;curve fitting;directed acyclic graph;entropy maximization;expectation–maximization algorithm;graph (discrete mathematics);ibm notes;iteration;iterative method;iterative proportional fitting;marginal model;markov chain;markov property;markov random field;mathematical optimization;richardson number;statistical model;turing completeness;whole earth 'lectronic link;windows firewall	Mathias Drton;Thomas S. Richardson	2004			conditional probability distribution;econometrics;conditional independence;markov property;computer science;conditional variance;factor graph;pattern recognition;bayesian network;chain rule;mathematics;graphical model;hidden markov model;discriminative model;statistics;belief propagation;variable-order markov model	ML	27.011829073776077	-29.51051809085536	97756
12036d2a347dfbd8507f20fa2e46005f2a851593	matrix completion and vector completion via robust subspace learning		Abstract Matrix completion is widely used in many practical applications such as computer vision and data mining. In this paper, we consider the following two issues arising in real scenarios. First, the collected datasets are damaged by sparse noise and column outliers simultaneously; second, the datasets are not static in nature due to the existing of out-of-sample. Both of them have been ignored by most existing methods. In contrast with the traditional matrix completion algorithms which aim at recovering the entire matrix directly, we in this paper aim to first learn a low dimensional subspace by recovering a subset of collected samples, and then utilize it to estimate the missing values of residual data. There are two important advantages about this transformation. First, weakening the deviation caused by column outliers. Second, providing a direction for efficiently solving the out-of-sample problem. Particularly, to further improve the robustness of proposed method to sparse noise, we present a novel robust matrix completion model and a robust vector completion model, and both of them are based on non-convex l p -norm (0  p In experiments, the proposed method and other state-of-the-art algorithms will be used to cope with two problems: matrix completion and vector completion. Numerical results on real datasets and artificial datasets demonstrate that our method can provide a significant performance advantage over alternative methods.		Zhe Liu;Zhanxuan Hu;Feiping Nie	2018	Neurocomputing	10.1016/j.neucom.2018.04.032	robustness (computer science);matrix completion;machine learning;residual;matrix (mathematics);outlier;missing data;artificial intelligence;mathematics;pattern recognition;subspace topology	ML	26.2720273931466	-37.76858645833226	98060
f8af399e78b966dcf0dcf68de0314b2f53e046c6	the licors cabinet: nonparametric algorithms for spatio-temporal prediction		For the task of unsupervised spatio-temporal forecasting (e.g., learning to predict video data without labels), we propose two new nonparametric predictive state algorithms, Moonshine and One Hundred Proof. The algorithms are conceptually simple and make few assumptions on the underlying spatio-temporal process yet have strong predictive performance and provide predictive distributions over spatio-temporal data. The latter property allows for likelihood estimation under the models, for classification and other probabilistic inference.	algorithm;baseline (configuration management);generative modelling language;iterative method;statistical classification	George D. Montanez;Cosma Rohilla Shalizi	2015	CoRR		econometrics;mathematical optimization;computer science;machine learning;statistics	ML	27.898225100924773	-31.4258866368787	98374
330358a375fb4824a9f860539ff5fb7ec2783050	a cluster elastic net for multivariate regression		We propose a method for simultaneously estimating regression coefficients and clustering response variables in a multivariate regression model, to increase prediction accuracy and give insights into the relationship between response variables. The estimates of the regression coefficients and clusters are found by using a penalized likelihood estimator, which includes a cluster fusion penalty, to shrink the difference in fitted values from responses in the same cluster, and an L1 penalty for simultaneous variable selection and estimation. We propose a two-step algorithm, that iterates between k-means clustering and solving the penalized likelihood function assuming the clusters are known, which has desirable parallel computational properties obtained by using the cluster fusion penalty. If the response variable clusters are known a priori then the algorithm reduces to just solving the penalized likelihood problem. Theoretical results are presented for the penalized least squares case, including asymptotic results allowing for p n. We extend our method to the setting where the responses are binomial variables. We propose a coordinate descent algorithm for the normal likelihood and a proximal gradient descent algorithm for the binomial likelihood, which can easily be extended to other generalized linear model (GLM) settings. Simulations and data examples from business operations and genomics are presented to show the merits of both the least squares and binomial methods.	algorithm;cluster analysis;coefficient;computation;computer simulation;coordinate descent;elastic map;elastic net regularization;feature selection;general linear model;generalized linear model;gradient descent;k-means clustering;least squares;proximal gradient methods for learning;smoothing spline	Bradley S. Price;Ben Sherwood	2017	Journal of Machine Learning Research		linear regression;mathematics;feature selection;cluster analysis;artificial intelligence;coordinate descent;pattern recognition;statistics;multivariate statistics;elastic net regularization;generalized linear model;least squares	ML	29.35653620955134	-25.5911971957017	98463
096d7d43e82f6a62004e54c6cae272c010f5c846	non-linear matrix factorization with gaussian processes	matrix factorization;approximation method;stochastic gradient descent;latent variable model;collaborative ltering;gaussian process	A popular approach to collaborative filtering is matrix factorization. In this paper we develop a non-linear probabilistic matrix factorization using Gaussian process latent variable models. We use stochastic gradient descent (SGD) to optimize the model. SGD allows us to apply Gaussian processes to data sets with millions of observations without approximate methods. We apply our approach to benchmark movie recommender data sets. The results show better than previous state-of-the-art performance.	approximation algorithm;benchmark (computing);collaborative filtering;gaussian process;latent variable;nonlinear system;recommender system;stochastic gradient descent	Neil D. Lawrence;Raquel Urtasun	2009		10.1145/1553374.1553452	mathematical optimization;computer science;machine learning;gaussian process;mathematics;stochastic gradient descent;matrix decomposition;latent variable model;non-negative matrix factorization;statistics	ML	26.06530852038736	-33.25090119521429	98519
7aff14433b74101e46e0d8b3c1c6d7ef76dd80dd	computable bayesian compression for uniformly discretizable statistical models	statistical model;exponential family	Supplementing Vovk and V’yugin’s ‘if’ statement, we show that Bayesian compression provides the best enumerable compression for parameter-typical data if and only if the parameter is Martin-Löf random with respect to the prior. The result is derived for uniformly discretizable statistical models, introduced here. They feature the crucial property that given a discretized parameter, we can compute how much data is needed to learn its value with little uncertainty. Exponential families and certain nonparametric models are shown to be uniformly discretizable.	bayesian network;computable function;discretization;random graph;statistical model	Lukasz Debowski	2009		10.1007/978-3-642-04414-4_9	econometrics;mathematical optimization;machine learning;mathematics;statistics	ML	26.879356851496933	-26.889468618866236	98754
3b33407d51bbb33c2786887c708d97bb1b3588fa	fast algorithms for reconstruction of sparse signals from cauchy random projections	regression analysis;signal reconstruction;cauchy random projections;convex relaxation;regularized coordinate-descent myriad regression;sparse signals;sparse matrices;iterative methods;approximation algorithms;noise	Recent work on dimensionality reduction using Cauchy random projections has emerged for applications where ℓ<sub>1</sub> distance preservation is preferred. An original sparse signal b ϵ ℝ<sup>n</sup> is multiplied by a Cauchy random matrix <b>R</b> ϵ ℝ<sup>n×k</sup> (k≪n), resulting in a projected vector c ϵ ℝ<sup>k</sup>. Two approaches for fast recover of b from the Cauchy vector c are proposed. The two algorithms are based on a regularized coordinate-descent Myriad regression using both ℓ<sub>0</sub> and convex relaxation as sparsity inducing terms. The key element is to start, in the first iteration, by finding the optimal estimate value for each coordinate, and selectively updating only the coordinates with rapid descent in subsequent iterations. For the particular case of the ℓ<sub>0</sub> regularized approach, an approximation function for the ℓ<sub>0</sub>-norm is given due to it is non-differentiable norm [1]. Performance comparisons of the proposed approaches to the original regularized coordinate-descent method are included.	algorithm;algorithmic efficiency;approximation;computation;coordinate descent;data mining;detection theory;dimensionality reduction;information retrieval;iteration;linear programming relaxation;locality-sensitive hashing;rate of convergence;run time (program lifecycle phase);sparse matrix	Ana B. Ramirez;Gonzalo R. Arce;Brian M. Sadler	2010	2010 18th European Signal Processing Conference		mathematical optimization;machine learning;sparse approximation;mathematics;statistics	ML	26.614724174214945	-34.58704996986749	98768
77d8ae2dfd306b7b5f72e1b6fe0b7262b60b4b8d	model structure learning: a support vector machine approach for lpv linear-regression models	structure learning;support vector machines learning artificial intelligence least squares approximations linear systems parameter estimation performance evaluation reduced order systems regression analysis sensitivity analysis;linear systems;least squares approximations;kernel;prediction error;performance evaluation;kernel estimation support vector machines dispersion noise data models computational modeling;support vector machines;linear parameter varying;degree of freedom;arx;computer model;linear regression model;model structure selection;representative examples model structure learning lpv linear regression models parametric identification linear parameter varying systems lpv systems optimal prior selection functional dependency model coefficients structural bias over parametrization bias variance trade off degree of freedom sensitivity underlying model structure measured data model orders least squares support vector machine approach ls svm approach dependency structure linear regression lpv models rational dynamic dependency performance evaluation;linear regression;model structure selection linear parameter varying support vector machines linear regression arx identification;functional dependency;data model;computational modeling;estimation;sensitivity analysis;identification;dependence structure;lpv system;regression analysis;support vector machine;parameter estimation;learning artificial intelligence;dispersion;reduced order systems;kernel estimate;noise;data models;least squares support vector machine	Accurate parametric identification of Linear Parameter-Varying (LPV) systems requires an optimal prior selection of a set of functional dependencies for the parametrization of the model coefficients. Inaccurate selection leads to structural bias while over-parametrization results in a variance increase of the estimates. This corresponds to the classical bias-variance trade-off, but with a significantly larger degree of freedom and sensitivity in the LPV case. Hence, it is attractive to estimate the underlying model structure of LPV systems based on measured data, i.e., to learn the underlying dependencies of the model coefficients together with model orders etc. In this paper a Least-Squares Support Vector Machine (LS-SVM) approach is introduced which is capable of reconstructing the dependency structure for linear regression based LPV models even in case of rational dynamic dependency. The properties of the approach are analyzed in the prediction error setting and its performance is evaluated on representative examples.	coefficient;functional dependency;hysteresis;least squares support vector machine;mathematical optimization;nonlinear system;semiconductor industry	Roland Tóth;Vincent Laurain;Wei Xing Zheng;Kameshwar Poolla	2011	IEEE Conference on Decision and Control and European Control Conference	10.1109/CDC.2011.6160564	computer simulation;support vector machine;econometrics;mathematical optimization;computer science;linear regression;machine learning;mathematics;statistics	ML	28.752408746814165	-25.47867912127896	98780
c23e2c3f03237dac214236ff8b6abc23af864135	model selection in gaussian graphical models: high-dimensional consistency of l1-regularized mle		We consider the problem of estimating the graph structure as sociated with a Gaussian Markov random field (GMRF) from i.i.d. samples. We study the p erformance of study the performance of thel1-regularized maximum likelihood estimator in the high-dim ensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufficient conditions on (n, p, d) for thel1-regularized MLE estimator to recover all the edges of the graph with high prob ability. Under some conditions on the model covariance, we show that model selection can be a chi ved for sample sizes n = Ω(d log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and sho w g od correspondences between the theoretical predictions and behavior in simulations.	graphical model;markov chain;markov random field;model selection;simulation	Pradeep Ravikumar;Garvesh Raskutti;Martin J. Wainwright;Bin Yu	2008				ML	26.469526770869642	-28.30676566504016	98811
dcdbfe344ff073494c452fab275a3f480db885fe	a novel relative entropy-posterior predictive model checking approach with limited information statistics for latent trait models in sparse 2k contingency tables	posterior predictive model checking;goodness of fit;relative entropy;latent trait model;parametric bootstrapping;limited information statistics	Limited information statistics have been recommended as the goodness-of-fit measures in sparse 2^k contingency tables, but the p-values of these test statistics are computationally difficult to obtain. A Bayesian model diagnostic tool, Relative Entropy-Posterior Predictive Model Checking (RE-PPMC), is proposed to assess the global fit for latent trait models in this paper. This approach utilizes the relative entropy (RE) to resolve possible problems in the original PPMC procedure based on the posterior predictive p-value (PPP-value). Compared with the typical conservatism of PPP-value, the RE value measures the discrepancy effectively. Simulated and real data sets with different item numbers, degree of sparseness, sample sizes, and factor dimensions are studied to investigate the performance of the proposed method. The estimates of univariate information and difficulty parameters are found to be robust with dual characteristics, which produce practical implications for educational testing. Compared with parametric bootstrapping, RE-PPMC is much more capable of evaluating the model adequacy.	consistency model;contingency table;kullback–leibler divergence;latent variable model;model checking;sparse matrix	Huiping Wu;Ka-Veng Yuen;Shing-On Leung	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2014.06.004	econometrics;machine learning;mathematics;goodness of fit;kullback–leibler divergence;statistics	AI	27.96287637632295	-24.50884539373692	98825
01f7e4897833c3f3c17268a0b3a423f60c3d6fd2	health condition monitoring and early fault diagnosis of bearings using sdf and intrinsic characteristic-scale decomposition	degradation;vibrations;circuit faults;health condition monitoring cusum cumulative sum spectral kurtosis method original local mean decomposition hilbert envelope spectrum fourier transform bearing fault detection principal product component icd method extracted abnormal signal decomposition fault feature extraction fault type recognition symbolic dynamic filtering machine downtime reduction intrinsic characteristic scale decomposition sdf bearing early fault diagnosis;symbolic dynamic filtering sdf cumulative sum cusum intrinsic characteristic scale decomposition icd roller bearing;feature extraction fault diagnosis circuit faults vibrations fault detection degradation rolling bearings;rolling bearings;mechanical engineering computing condition monitoring decomposition fault diagnosis feature extraction filtering theory fourier transforms hilbert transforms machine bearings;feature extraction;fault detection;fault diagnosis	Early fault diagnosis is crucial to reduce the machine downtime. This paper presents a novel method based on symbolic dynamic filtering (SDF) for early fault detection and intrinsic characteristic-scale decomposition (ICD) for fault type recognition. SDF is first applied to extract the fault feature for depicting bearing performance degradation. Then, a fault alarm is triggered using cumulative sum. Finally, the extracted abnormal signal is decomposed by the ICD method, and the kurtosis method is used to select a principal product component that contains most fault information for fault detection. The real life experimental results validate the effectiveness of the proposed method in early detection of bearing fault and fault diagnosis in comparison with Fourier transform, Hilbert envelope spectrum, original local mean decomposition and spectral kurtosis.	anomaly detection;downtime;elegant degradation;experiment;fault detection and isolation;markov chain;numerical analysis;real life;real-time clock;real-time computing	Yongbo Li;Minqiang Xu;Yu Wei;Wenhu Huang	2016	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2016.2564078	structural engineering;electronic engineering;degradation;feature extraction;computer science;engineering;vibration;forensic engineering;physics;fault detection and isolation	Robotics	37.61875555086418	-30.892660738339337	99177
16327e8b0e6fcee3751a7790e01b8427ca5deceb	bounded recursive decomposition: a search-based method for belief-network inference under limited resources	stochastic simulation;bayesian belief network;belief network	This paper presents a new inference algorithm for belief networks that combines a search-based algorithm with a simulation-based algorithm. The former is an extension of the recursive decomposition (RD) algorithm proposed by Cooper, which is here modified to compute interval bounds on marginal probabilities. We call the algorithm bounded-RD. The latter is a stochastic simulation method known as Pearl's Markov blanket algorithm. Markov simulation is used to generate highly probable instantiations of the network nodes to be used by bounded-RD in the computation of probability bounds. Bounded-RD has the anytime property, and produces successively narrower interval bounds, which converge in the limit to the exact value.	anytime algorithm;approximation;backward chaining;bayesian network;cluster analysis;computation;converge;decision problem;experiment;fastest;hybrid algorithm;jensen's inequality;marginal model;markov blanket;markov chain;message passing;modularity (networks);network topology;polynomial;recursion;recursion (computer science);route distinguisher;ruby document format;simulation;singular value decomposition;software propagation;time complexity;turing completeness	Stephano Monti;Gregory F. Cooper	1996	Int. J. Approx. Reasoning	10.1016/0888-613X(96)00012-6	forward algorithm;mathematical optimization;computer science;machine learning;bayesian network;mathematics;statistics	AI	26.328991884224877	-28.156067933757164	99204
ef73375a1c585f85f7c65eda8728792e79829310	diagnosis method of combing feature extraction based on time-frequency analysis and intelligent classifier	wavelet decomposition;feature extraction;bp neural network;time frequency analysis;fault diagnosis	In the process of using neural network to carry out intelligent fault type identification, how to extract sensitive fault features from the original data is quite important for an accurate diagnosis result. An intelligent fault diagnosis method was proposed, which combined time domain analysis and wavelet analysis method to extract features from vibration data of a motor bearing. The resulting vector obtained from the feature extraction was used as samples to train the BP neural network intelligent classifier to enable the classifier to identify fault type. The comparison of experiment results showed that the proposed diagnosis method was effective.	feature extraction;frequency analysis	Baolu Gao;Junjie Chen;Xiaoyan Xiong;Shibo Xiong	2011		10.1007/978-3-642-23971-7_53	time–frequency analysis;feature extraction;computer science;machine learning;pattern recognition;data mining	Robotics	37.07348702326263	-31.227907225106105	99251
ff06d6f8c7289feae038bccbb6f071795f2ad7cd	building bayesian networks from gwas statistics based on independence of causal influence	bayes methods;data mining;noise measurement;genetics;mathematical model;buildings;data models	Genome-wide association studies (GWASs) have received an increasing attention to understand genotype-phenotype relationships. In this paper, we study how to build Bayesian networks from publicly released GWAS statistics to explicitly reveal the conditional dependency between single-nucleotide polymorphisms (SNPs) and traits. The key challenge in building a Bayesian network is the specification of the conditional probability table (CPT) of an variable with multiple parent variables. We employ the Independence of Causal Influences (ICI) which assumes that the causal mechanism of each parent variable is mutually independent. Specifically, we derive a formulation from the Noisy-or model, one of the ICI models, to specify the CPT using the released GWAS statistics. We prove that the specified CPT is accurate as long as the underlying individual-level genotype and phenotype profile data follows the Noisy-or model. We empirically evaluate the Noisy-or model and its derived formulation using data from openSNP. Experimental results demonstrate the effectiveness of our approach.	bayesian network;cpt (file format);causal filter;causality;consistency model;ici (programming language)	Lu Zhang;Qiuping Pan;Xintao Wu;Xinghua Shi	2016	2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2016.7822576	data modeling;econometrics;computer science;bioinformatics;noise measurement;machine learning;mathematical model;data mining;mathematics;genetics;statistics	ML	28.478345850308433	-32.361179547278304	99329
43aa597bcadecdbb5739652fa96efaee88887e40	fast variational inference in the conjugate exponential family		We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our method unifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. We exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equations have been derived. Empirically we show significant speed-ups for probabilistic inference using our bound.	algorithm;calculus of variations;conjugate gradient method;information geometry;marginal model;mathematical optimization;time complexity;variational principle	James Hensman;Magnus Rattray;Neil D. Lawrence	2012			econometrics;mathematical optimization;mathematics;statistics	ML	26.595288929666204	-30.14864079495854	99404
531e56c16606476a7e87293bcabbf45af07d2799	stochastic discriminative em		Stochastic discriminative EM (sdEM) is an online-EM-type algorithm for discriminative training of probabilistic generative models belonging to the natural exponential family. In this work, we introduce and justify this algorithm as a stochastic natural gradient descent method, i.e. a method which accounts for the information geometry in the parameter space of the statistical model. We show how this learning algorithm can be used to train probabilistic generative models by minimizing different discriminative loss functions, such as the negative conditional log-likelihood and the Hinge loss. The resulting models trained by sdEM are always generative (i.e. they define a joint probability distribution) and, in consequence, allows to deal with missing data and latent variables in a principled way either when being learned or when making predictions. The performance of this method is illustrated by several text classification problems for which a multinomial naive Bayes and a latent Dirichlet allocation based classifier are learned using different discriminative loss functions.	discriminative model;document classification;expectation–maximization algorithm;generative model;gradient descent;hinge loss;hoc (programming language);information geometry;latent dirichlet allocation;latent variable;loss function;missing data;multinomial logistic regression;naive bayes classifier;statistical model;time complexity	Andrés R. Masegosa	2014			machine learning;pattern recognition;mathematics;generative model;discriminative model;statistics	ML	25.970571813017617	-31.068212083446436	99470
8105c83158488260327e319bd598fb3941887ad0	non-convex rank minimization via an empirical bayesian approach	variational approximation;point estimation;cost function;bayesian approach;low rank matrices;ranking function;optimization problem;empirical evidence;principal component analysis;convex relaxation	In many applications that require matrix solutions of minimal rank, the underlying cost function is non-convex leading to an intractable, NP-hard optimization problem. Consequently, the convex nuclear norm is frequently used as a surrogate penalty term for matrix rank. The problem is that in many practical scenarios there is no longer any guarantee that we can correctly estimate generative low-rank matrices of interest, theoretical special cases notwithstanding. Consequently, this paper proposes an alternative empirical Bayesian procedure build upon a variational approximation that, unlike the nuclear norm, retains the same globally minimizing point estimate as the rank function under many useful constraints. However, locally minimizing solutions are largely smoothed away via marginalization, allowing the algorithm to succeed when standard convex relaxations completely fail. While the proposed methodology is generally applicable to a wide range of low-rank applications, we focus our attention on the robust principal component analysis problem (RPCA), which involves estimating an unknown low-rank matrix with unknown sparse corruptions. Theoretical and empirical evidence are presented to show that our method is potentially superior to related MAP-based approaches, for which the convex principle component pursuit (PCP) algorithm (Candès et al., 2011) can be viewed as a special case.	algorithm;approximation;global optimization;loss function;mathematical optimization;np-hardness;optimization problem;robust principal component analysis;smoothing;sparse matrix;variational principle	David P. Wipf	2012			optimization problem;econometrics;mathematical optimization;empirical evidence;bayesian probability;computer science;machine learning;point estimation;mathematics;statistics;low-rank approximation;principal component analysis	ML	27.053866170066488	-35.007793141664116	99601
11829ff16bb3b82212455e4c5cbca0590ddba597	a gpu-accelerated approximate algorithm for incremental learning of gaussian mixture model	pattern clustering;mixture component equality testing gpu accelerated approximate algorithm incremental learning gaussian mixture model probabilistic clustering model data problem cluster quality incremental gmm learning algorithm algorithmic method gpu cpu hybrid system model evolution history hypothesis testing euclidean distance;probability;standards;gaussian processes;approximation algorithms;equality test;gmm;gpu;order identification;approximation theory;clustering algorithms data models approximation algorithms mathematical model standards graphics processing unit equations;equality test gmm incremental learning gpu order identification;incremental learning;graphics processing units;mathematical model;clustering algorithms;learning artificial intelligence;graphics processing unit;probability approximation theory gaussian processes graphics processing units learning artificial intelligence pattern clustering;data models	The Gaussian mixture model (GMM) is a widely used probabilistic clustering model. The incremental learning algorithm of GMM is the basis of a variety of complex incremental learning algorithms. It is typically applied to real-time or massive data problems where the standard Expectation Maximum (EM) algorithm does not work. But the output of the incremental learning algorithm may exhibit degraded cluster quality than the standard EM algorithm. In order to achieve a high-quality and fast incremental GMM learning algorithm, we develop an algorithmic method for incremental learning of GMM in a GPU-CPU hybrid system. Our method uses model evolution history to approximate the model order and adopts both hypothesis-test and Euclidean distance to do mixture component equality test. Through experiments we show that our method achieves high performance in terms of both cluster quality and speed.	approximation algorithm;bayesian information criterion;central processing unit;cluster analysis;computation;euclidean distance;expectation–maximization algorithm;experiment;google map maker;graphics processing unit;hybrid system;machine learning;mixture model;real-time locating system;scalability;speedup	Chunlei Chen;Dejun Mu;Huixiang Zhang;Bo Hong	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.236	data modeling;computer science;generalized method of moments;theoretical computer science;machine learning;pattern recognition;probability;mathematical model;gaussian process;cluster analysis;approximation algorithm;statistics;population-based incremental learning;approximation theory	DB	29.72933290901397	-30.854450647669267	99634
7de1f5079ed7a8a8a5690f72ad2099f52d697393	fast global alignment kernels		We propose novel approaches to cast the widely-used family of Dynamic Time Warping (DTW) distances and similarities as positive definite kernels for time series. To this effect, we provide new theoretical insights on the family of Global Alignment kernels introduced by Cuturi et al. (2007) and propose alternative kernels which are both positive definite and faster to compute. We provide experimental evidence that these alternatives are both faster and more efficient in classification tasks than other kernels based on the DTW formalism.	dynamic time warping;semantics (computer science);time series	Marco Cuturi	2011			mathematical optimization;theoretical computer science;machine learning;mathematics	ML	29.518078268924516	-37.78293438328002	99777
95ea0dcb17ef42267bb56232f308b367327a9ab7	variational bayesian inference with stochastic search		Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.	approximation algorithm;control variates;gradient;logistic regression;marginal model;mathematical optimization;stochastic optimization;variational principle	John William Paisley;David M. Blei;Michael I. Jordan	2012			econometrics;mathematical optimization;mathematics;bayesian linear regression;statistics	ML	27.19841760886014	-29.668125097670096	99817
f1fffa10219d91aa107184981a669b18e733b6cf	type 1 and 2 symmetric divergences for stochastic neighbor embedding		Stochastic neighbor embedding (SNE) is a method of dimensionality reduction (DR) that involves softmax similarities measured between all pairs of data points. In order to build a low-dimensional embedding, SNE tries to reproduce the similarities observed in the highdimensional data space. The capability of softmax similarities to fight the phenomenon of norm concentration has been studied in previous work. This paper investigates a complementary aspect, namely, the cost function that quantifies the mismatch between the highand low-dimensional similarities. We show experimentally that switching from a simple KullbackLeibler divergences to mixtures of dual divergences increases the quality of DR. This modification brings SNE to the performance level of its Student t-distributed variant, without the need to resort to non-identical similarity definitions in the highand low-dimensional spaces. These results allow us to conclude that future improvements in similarity-based DR will likely emerge from better definitions of the cost function.	bregman divergence;data point;dataspaces;dimensionality reduction;experiment;kullback–leibler divergence;loss function;nonlinear programming;softmax function;t-distributed stochastic neighbor embedding	John Aldo Lee	2012			combinatorics;embedding;computer science	ML	28.387051230173046	-37.26535565685381	100275
20ffc1947ace3e8234c5e8108ef661eefd120a98	sparse group inductive matrix completion		We consider the problem of matrix completion with side information (inductive matrix completion). In real-world applications many side-channel features are typically non-informative making feature selection an important part of the problem. We incorporate feature selection into inductive matrix completion by proposing a matrix factorization framework with group-lasso regularization on side feature parameter matrices. We demonstrate, that the theoretical sample complexity for the proposed method is much lower compared to its competitors in sparse problems, and propose an efficient optimization algorithm for the resulting low-rank matrix completion problem with sparsifying regularizers. Experiments on synthetic and real-world datasets show that the proposed approach outperforms other methods.	algorithm;experiment;feature selection;inductive reasoning;information;lasso;mathematical optimization;matrix regularization;sample complexity;side-channel attack;sparse matrix;synthetic intelligence	Ivan Nazarov;Boris Shirokikh;Maria Burkina;Gennady Fedonin;Maxim Panov	2018	CoRR		mathematics;matrix completion;mathematical optimization;sample complexity;oracle;matrix (mathematics);matrix decomposition;regularization (mathematics);coefficient matrix	ML	26.14190659529444	-37.22562235903331	100450
827bf27888ccbcc55a0540c89aadb8eb7093b3f6	new insights into the role of the head radius in model-based binaural speaker localization		In this work we evaluate the effects of the head radius on binaural localization algorithms. We employ a spherical head model and the null-steering beamforming localization method. The model characterizes the binaural cues in the form of HRTFs. One of the main parameters in this model is the head radius. We propose to optimize jointly for both the source location and the head radius. In contrast to the free-field configuration where it is difficult to estimate the source location and microphone distance simultaneously, the binaural algorithm yields a unique solution to the head radius. Moreover, for real recordings we show that the commonly-assumed size of the head achieves a fairly reliable performance. For applications with non-typical size of the head, e.g., hearing-impaired children the adaptation of the head radius using the proposed algorithm would improve the accuracy of the binaural localization algorithm.	algorithm;beamforming;binaural beats;direction of arrival;frequency band;humanoid robot;internet listing display;interpupillary distance;kernel density estimation;loss function;microphone;pc speaker	Mehdi Zohourian;Rainer Martin;Nilesh Madhu	2017	2017 25th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2017.8081201	computer vision;microphone;binaural recording;beamforming;artificial intelligence;speech recognition;computer science	Robotics	34.05745072835877	-36.7222113102561	100494
cd48ffdeea44a6b9681e41a6fb82fa8ce5616c86	bayesian noise model selection and system identification based on approximation of the evidence	computational modeling noise approximation methods bayes methods mathematical model numerical models monte carlo methods;mcmc system identification model selection bayesian strategy evidence;statistical signal processing bayesian noise model selection problem system identification evidence approximation second order system parameter identification noise distribution finite set closed form expression markov chain monte carlo techniques gibbs sampler per noise model posterior probability numerical assessment input signal;statistical distributions approximation theory bayes methods markov processes monte carlo methods parameter estimation probability signal processing	The purpose of this work is to identify the parameters of a second order system from noisy data in a context where the difficulty is twofold. First, the model is strongly non linear and possibly non Gaussian. Second, the noise distribution is unknown. It is nevertheless assumed to belong to a finite set, thus, the identification issue is coupled with a model selection problem. In a Bayesian framework, both the selection and the estimation are optimally designed and finally based upon the posterior distribution for the model index and the parameters. Since the latter does not admit a closed-form expression, we resort to Markov Chain Monte Carlo techniques: one Gibbs sampler is run per noise model so as to approximate the evidence and then compute the posterior probability. The model is then selected and the parameters are estimated given the selected model. A first numerical assessment of the method is given based on simulated data.	approximation algorithm;bayesian network;gibbs sampling;markov chain monte carlo;model selection;monte carlo method;numerical analysis;sampling (signal processing);selection algorithm;signal-to-noise ratio;system identification	Jean-François Giovannelli;Audrey Giremus	2014	2014 IEEE Workshop on Statistical Signal Processing (SSP)	10.1109/SSP.2014.6884591	monte carlo method in statistical physics;econometrics;gibbs sampling;hybrid monte carlo;particle filter;markov chain monte carlo;pattern recognition;monte carlo molecular modeling;mathematics;bayesian linear regression;bayesian statistics;bayesian inference;statistics;monte carlo method	ML	31.28384245406383	-26.25404320900494	100655
440691560ddbd30f5d7511d73919a5dce2064384	evolutionary dimensionality reduction for crack localization in ship structures using a hybrid computational intelligent approach	nondestructive testing methods;non destructive testing;evolutionary computation;marine vehicles computational intelligence intelligent structures competitive intelligence acoustic testing feature extraction working environment noise acoustic emission materials testing nondestructive testing;probability;neural nets;sensors;computational intelligence;working environment noise;ships acoustic emission testing crack detection evolutionary computation marine engineering neural nets nondestructive testing pattern classification probability;hybrid computational intelligent approach;materials testing;piezo sensors;acoustic testing;simulation experiment;artificial neural networks;ships;marine vehicles;feature selection phase evolutionary dimensionality reduction crack localization ship structures hybrid computational intelligent approach acoustic emission testing nondestructive testing methods classification problem feature extraction piezo sensors evolutionary algorithm linear transformation probabilistic neural network stiffened plate model;crack detection;feature extraction;classification algorithms;stiffened plate model;pattern classification;linear transformation;competitive intelligence;marine engineering;ship structures;feature selection phase;crack localization;feature selection;classification problem;acoustic emission;evolutionary algorithm;acoustic emission testing;intelligent structures;probabilistic neural network;dimensional reduction;nondestructive testing;gallium;noise;evolutionary dimensionality reduction	Acoustic Emission (AE) is one of the most important Non-Destructive Testing (NDT) methods for materials and constructions. Using AE testing, the location of a single event (crack) can be classified efficiently into three typical areas in a ship hull. The problem is a typical classification problem based on the use of features extracted from piezo-sensors' signal. As in most classification problems, the extraction and selection of the most appropriate set of features plays a major role in the overall performance of the system. In this research work we investigate the use of an evolutionary algorithm to extract new features from a set of primitive features in a lower dimensional space through a linear transformation. These features are subsequently fed into a Probabilistic Neural Network (PNN) that performs the classification. In simulation experiments, where a Stiffened Plate Model (SPM) is partially sank into water, the localization rate in noisy environments outperforms a recent work, where a feature selection phase alone was used before the classification phase. The proposed hybrid computational intelligent approach shows the potential merit of using it in real life situations where the signal is distorted by noise.	acoustic cryptanalysis;dimensionality reduction;evolutionary algorithm;experiment;feature selection;piezoelectricity;probabilistic neural network;real life;sensor;simulation;super paper mario	Vassilios A. Kappatos;George K. Georgoulas;Chrysostomos D. Stylios;Evangelos Dermatas	2009	2009 International Joint Conference on Neural Networks	10.1109/IJCNN.2009.5178852	nondestructive testing;computer science;acoustic emission;machine learning;artificial neural network	Robotics	37.288179096252485	-32.79849365926156	101239
47d9ad5ac0cc38510f16f1dfe57f13f0c6873f15	collusion-resistant spatial phenomena crowdsourcing via mixture of gaussian processes regression		With the rapid development of mobile devices, spatial location-based crowdsourcing applications have attracted much attention. These applications also introduce new security risks due to untrustworthy data sources. In the context of crowdsourcing applications for spatial interpolation (i.e. spatial regression) using crowdsourced data, the results can be seriously affected if malicious data sources initiate a colluding (collaborate) attacks which purposely alter some of the measurements. To combat this serious detrimental effect, and to mitigate such attacks, we develop a robust version via a Gaussian Process mixture model and develop a computationally efficient algorithm which utilises a Markov chain Monte Carlo (MCMC)-based methodology to produce an accurate predictive inference in the presence of collusion attacks. The algorithm is fully Bayesian and produces posterior predictive distribution for any point-of-interest in the input space. It also assesses the trustworthiness of each worker, i.e. the probability of each worker being honest (trustworthy). Simulation results demonstrate the accuracy of this algorithm.	algorithm;algorithmic efficiency;crowdsourcing;futures studies;gaussian process;malware;mixture model;mobile device;monte carlo method;multivariate interpolation;point of interest;reversible-jump markov chain monte carlo;sampling (signal processing);simulation;spatial analysis;synthetic intelligence;trust (emotion)	Qikun Xiang;Ido Nevat;Pengfei Zhang;Jie Zhang	2016			predictive inference;mixture model;collusion;multivariate interpolation;artificial intelligence;markov chain monte carlo;gaussian process;machine learning;crowdsourcing;bayesian probability;computer science	ML	31.428074141362714	-28.951770995511353	101467
5a36ef0f588a4cfcca31a1a5803b21532f40bdf5	density estimation via discrepancy based adaptive sequential partition		Given iid observations from an unknown absolute continuous distribution defined on some domain Ω, we propose a nonparametric method to learn a piecewise constant function to approximate the underlying probability density function. Our density estimate is a piecewise constant function defined on a binary partition of Ω. The key ingredient of the algorithm is to use discrepancy, a concept originates from Quasi Monte Carlo analysis, to control the partition process. The resulting algorithm is simple, efficient, and has a provable convergence rate. We empirically demonstrate its efficiency as a density estimation method. We also show how it can be utilized to find good initializations for k-means.	approximation algorithm;bit error rate;constant function;data compression;data visualization;discrepancy function;ibm notes;k-means clustering;kernel density estimation;monte carlo method;provable security;quasi-monte carlo method;rate of convergence	Dangna Li;Kun Yang;Wing Hung Wong	2016			econometrics;mathematical optimization;mathematics;piecewise;statistics	ML	30.06826275915938	-29.5492389946958	101481
5c9b4e652e47a168fd73e6e53410678194cd6b03	generative local metric learning for nearest neighbor classification	generative discriminative hybridization metric learning nearest neighbor classification f divergence;data models training training data extraterrestrial measurements kernel probability density function	We consider the problem of learning a local metric in order to enhance the performance of nearest neighbor classification. Conventional metric learning methods attempt to separate data distributions in a purely discriminative manner; here we show how to take advantage of information from parametric generative models. We focus on the bias in the information-theoretic error arising from finite sampling effects, and find an appropriate local metric that maximally reduces the bias based upon knowledge from generative models. As a byproduct, the asymptotic theoretical analysis in this work relates metric learning to dimensionality reduction from a novel perspective, which was not understood from previous discriminative approaches. Empirical experiments show that this learned local metric enhances the discriminative nearest neighbor performance on various datasets using simple class conditional generative models such as a Gaussian.	appendix;approximation algorithm;base;coefficient;dimensionality reduction;discrete laplace operator;discriminative model;equivalent weight;experiment;gaussian (software);generative model;hessian;hidden markov model;information theory;k-nearest neighbors algorithm;kernel method;machine learning;markov chain;missing data;mitolactol;mixture model;nx bit;nearest neighbour algorithm;normal statistical distribution;point of interest;probability density;radix angelicae sinensis/radix astragali herbal supplement;recurrent respiratory papillomatosis;sampling (signal processing);sampling - surgical action;single linkage cluster analysis;thickness (graph theory);bacterium rrp-e6	Yung-Kyun Noh;Byoung-Tak Zhang;Daniel D. Lee	2010	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2017.2666151	large margin nearest neighbor;speech recognition;computer science;machine learning;pattern recognition;cover tree;nearest neighbor search;generative model;k-nearest neighbors algorithm	ML	26.058236248167166	-30.945905294520724	101482
c7f3acc44ac879bcd460b24b0ef7dfd84b3b35e1	nonparametric splitting algorithm for detecting structural changes in predictive relationships		The problem of detecting structural changes in a regression study has become crucially important in a wide variety of fields, since data generating processes in a real world are usually unstable. Taking into account the fact that relationships within observed data are often in a continuous flux, it can be challenging to make any distributional assumptions. In the current paper, we propose a new nonparametric technique which allows estimation of an unknown number of structural change points in multivariate data having univariate response. The Nonparametric Splitting algorithm is a heuristic smart search for relationship changes based on a consequential division of the data into smaller parts. The approach utilizes a nonparametric change point test to find narrow regions of change locations. Our preliminary experiments are promising and suggest potential for the high efficiency and prediction accuracy of the introduced method.	algorithm;control theory;experiment;heuristic;sensor	Olga Gorskikh;Pekka Malo;Pauliina Ilmonen	2017		10.1145/3093241.3093282	flux;econometrics;statistics;structural change;algorithm;multivariate statistics;heuristic;nonparametric statistics;mathematics;univariate	SE	26.63678981982383	-25.06225215828082	101674
524c83631ba2e6d226b853bbd7adde0be59a7025	missing value imputation via copula and transformation methods, with applications to financial and economic data	quadrature;arellano valle and bolfarine s generalised t distribution;arellano valle;economic data;copula;financial data;data analysis;generalised t distribution;missing variable imputation;bolfarine;missing values;transformation;mixture models	We present new, tractable methods to impute missing values based on conditional probability density functions that we estimate via copula and mixture models. Our methods exploit known analytical results concerning conditional distributions for the Arellano-Valle and Bolfarine’s generalised t-distribution and fast, accurate quadrature methods. We also benchmark our approach on three financial/economic datasets (two of which are publicly available) and show that our methods outperform benchmark approaches on these data.	benchmark (computing);categorical logic;cobham's thesis;geo-imputation;missing data;mixture model	Craig Friedman;Jinggang Huang;Yangyong Zhang;Wenbo Cao	2012	IJDATS	10.1504/IJDATS.2012.050404	transformation;economic data;econometrics;quadrature;copula;mixture model;data mining;mathematics;data analysis;statistics	ML	28.0766228451995	-29.51970444110894	101776
39b579363cf8796060a7fe901bfb4b248b0f104c	on the consistency of graph-based bayesian learning and the scalability of sampling algorithms		A popular approach to semi-supervised learning proceeds by endowing the input data with a graph structure in order to extract geometric information and incorporate it into a Bayesian framework. We introduce new theory that gives appropriate scalings of graph parameters that provably lead to a well-defined limiting posterior as the size of the unlabeled data set grows. Furthermore, we show that these consistency results have profound algorithmic implications. When consistency holds, carefully designed graph-based Markov chain Monte Carlo algorithms are proved to have a uniform spectral gap, independent of the number of unlabeled inputs. Several numerical experiments corroborate both the statistical consistency and the algorithmic scalability established by the theory.	algorithm;experiment;markov chain monte carlo;monte carlo method;numerical analysis;scalability;semi-supervised learning;semiconductor industry;supervised learning	Nicolás García Trillos;Zachary Kaplan;Thabo Samakhoana;Daniel Sanz-Alonso	2017	CoRR		machine learning;mathematics;artificial intelligence;scalability;limiting;spectral gap;markov chain monte carlo;bayesian inference;pattern recognition;gibbs sampling;bayesian probability;graph	ML	26.081504722120584	-30.511122924632154	101816
4d9580d2e2afbac534cbd86b9dbdc31a9c3f4940	a fast and scalable joint estimator for integrating additional knowledge in learning multiple related sparse gaussian graphical models		We consider the problem of including additional knowledge in estimating sparse Gaussian graphical models (sGGMs) from aggregated samples, arising often in bioinformatics and neuroimaging applications. Previous joint sGGM estimators either fail to use existing knowledge or cannot scale-up to many tasks (large K) under a highdimensional (large p) situation. In this paper, we propose a novel Joint Elementary Estimator incorporating additional Knowledge (JEEK) to infer multiple related sparse Gaussian Graphical models from large-scale heterogeneous data. Using domain knowledge as weights, we design a novel hybrid norm as the minimization objective to enforce the superposition of two weighted sparsity constraints, one on the shared interactions and the other on the task-specific structural patterns. This enables JEEK to elegantly consider various forms of existing knowledge based on the domain at hand and avoid the need to design knowledgespecific optimization. JEEK is solved through a fast and entry-wise parallelizable solution that largely improves the computational efficiency of the state-of-the-art O(pK) to O(pK). We conduct a rigorous statistical analysis showing that JEEK achieves the same convergence rate O(log(Kp)/ntot) as the state-of-the-art estimators that are much harder to compute. Empirically, on multiple synthetic datasets and two real-world data, JEEK outperforms the speed of the state-ofarts significantly while achieving the same level of prediction accuracy. Department of Computer Science, University of Virginia, http://www.jointnets.org/ . Correspondence to: Beilun Wang <bw4mw@virginia.edu>, Yanjun Qi <yanjun@virginia.edu>. Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).	bioinformatics;computation;computer science;experiment;gene regulatory network;graphical model;interaction;international conference on machine learning;mathematical optimization;rate of convergence;sparse matrix;structural pattern;synthetic intelligence;usb hub	Beilun Wang;Arshdeep Sekhon;Yanjun Qi	2018			domain knowledge;estimator;rate of convergence;mathematical optimization;parallelizable manifold;machine learning;superposition principle;artificial intelligence;scalability;gaussian;mathematics;graphical model	ML	26.570743398826117	-33.315246809362854	101825
69b577768298964f7adc89f912e04b388822aa45	multi-stage optional unrelated question rrt model	parameter estimation andprivacy protection;unrelated question rrt models;simulation study;optional rrt models	Sihm et al. (2014) introduced modified optional unrelated question RRT model in both binary and quantitative response situations wherein the prevalence of the sensitive variable and the sensitivity level of the underlying sensitive question could be estimated simultaneously without using a split sample approach. In this study, we propose a three−stage optional unrelated question RRT model for both binary and quantitative response situations which combines the essence of Sihm et al. (2014) model and the three−stage optional additive RRT model proposed by Mehta et al. (2012). The efficiencies of Sihm et al. (2014) model and proposed three−stage optional unrelated question RRT model are compared using simulations. The privacy measures of the two models in question are also discussed. Comparisons for the binary models are based on Lanke (1976) measure while Yan et al. (2009) measure is used to compare the privacy measures for the quantitative models.	bitwise operation;privacy;simulation;utility functions on indivisible goods	Anu Chhabra;B. K. Dass;Samridhi Mehta	2016	JSTA	10.2991/jsta.2016.15.1.7	mathematics	NLP	25.743786554380634	-24.70642123500233	101871
337b91faf21e552cc52702380e040cbc41fb1546	variational gaussian copula inference		We utilize copulas to constitute a unified framework for constructing and optimizing variational proposals in hierarchical Bayesian models. For models with continuous and non-Gaussian hidden variables, we propose a semiparametric and automated variational Gaussian copula approach, in which the parametric Gaussian copula family is able to preserve multivariate posterior dependence, and the nonparametric transformations based on Bernstein polynomials provide ample flexibility in characterizing the univariate marginal posteriors.	bernstein polynomial;calculus of variations;hidden variable theory;marginal model;phil bernstein;semiparametric model;unified framework;variational principle	Shaobo Han;Xuejun Liao;David B. Dunson;Lawrence Carin	2016			econometrics;mathematical optimization;mathematics;statistics	ML	28.238026480014007	-29.219623113492624	101956
285e198170ccb4eb31de3482f5379fd05d33cacc	pull message passing for nonparametric belief propagation		We present a “pull” approach to approximate products of Gaussian mixtures within message updates for Nonparametric Belief Propagation (NBP) inference. Existing NBP methods often represent messages between continuous-valued latent variables as Gaussian mixture models. To avoid computational intractability in loopy graphs, NBP necessitates an approximation of the product of such mixtures. Sampling-based product approximations have shown effectiveness for NBP inference. However, such approximations used within the traditional “push” message update procedures quickly become computationally prohibitive for multi-modal distributions over high-dimensional variables. In contrast, we propose a “pull” method, as the Pull Message Passing for Nonparametric Belief propagation (PMPNBP) algorithm, and demonstrate its viability for efficient inference. We report results using an experiment from an existing NBP method, PAMPAS, for inferring the pose of an articulated structure in clutter. Results from this illustrative problem found PMPNBP has a greater ability to efficiently scale the number of components in its mixtures and, consequently, improve inference accuracy.	appletalk;approximation algorithm;belief propagation;casio loopy;clutter;computational complexity theory;gibbs sampling;latent variable;message passing;mixture model;modal logic;neutral body posture;software propagation	Karthik Desingh;Anthony Opipari;Odest Chadwicke Jenkins	2018	CoRR		machine learning;belief propagation;message passing;mixture model;pattern recognition;sampling (statistics);artificial intelligence;gaussian;computer science;inference;nonparametric statistics;latent variable	ML	26.841999755136133	-30.38553918680896	102050
55965deb48a2be417a25e1b97489f485b13cbddf	regularized tyler's scatter estimator: existence, uniqueness, and algorithms	covariance matrices robustness cost function signal processing algorithms estimation equations symmetric matrices;s matrix theory estimation theory signal processing;normalized scatter matrix regularized tyler scatter estimator elliptical distributions sufficient condition shrinkage tyler estimators majorization minimization framework numerical algorithms convergence analysis signal processing;majorization minimization tyler s scatter estimator shrinkage estimator existence uniqueness	This paper considers the regularized Tyler's scatter estimator for elliptical distributions, which has received considerable attention recently. Various types of shrinkage Tyler's estimators have been proposed in the literature and proved work effectively in the “large p small n” scenario. Nevertheless, the existence and uniqueness properties of the estimators are not thoroughly studied, and in certain cases the algorithms may fail to converge. In this work, we provide a general result that analyzes the sufficient condition for the existence of a family of shrinkage Tyler's estimators, which quantitatively shows that regularization indeed reduces the number of required samples for estimation and the convergence of the algorithms for the estimators. For two specific shrinkage Tyler's estimators, we also proved that the condition is necessary and the estimator is unique. Finally, we show that the two estimators are actually equivalent. Numerical algorithms are also derived based on the majorization-minimization framework, under which the convergence is analyzed systematically.	algorithm;converge;feasible region;goto;html tidy;heuristic;kl-one;kullback–leibler divergence;loss function;numerical linear algebra;simulation;sorting;tyler oakley;uniqueness type	Ying Sun;Prabhu Babu;Daniel Pérez Palomar	2014	IEEE Transactions on Signal Processing	10.1109/TSP.2014.2348944	econometrics;mathematical optimization;mathematics;statistics	ML	27.331773293630246	-34.65844424850113	102342
55c05a5e76086d71b86e5d3a2c2efd72e83f5bd2	rao-blackwell dimension reduction applied to hazardous source parameter estimation	bayesian estimation;monte carlo estimation;plume dispersion;chemical biological radiological defence	Parameter estimation of a source of chemical, biological or radiological emissions is a problem of great importance for public safety. The key parameters of interest are the source intensity and its location. This paper applies the concept of Rao–Blackwell dimension reduction to solve the posterior probability distribution function of source intensity, conditioned on source location, analytically. The paper is cast in the context of a source of a hazardous release of particles or gas and its turbulent transport through the medium. Numerical results, obtained by simulations and using an experimental dataset, demonstrate the statistical efficiency of the proposed method.	blackwell (series);dimensionality reduction;estimation theory	Branko Ristic;Ajith Gunatilaka;Yan Wang	2017	Signal Processing	10.1016/j.sigpro.2016.10.005	econometrics;simulation;bayes estimator;computer science;engineering;mathematics;statistics	ML	36.56334406322537	-24.876331260764097	102363
ba2f3c4dfb323553fd4c634eb1fbe1f113c712a3	general blending models for data from mixture experiments	biological patents;model selection;beckers models;biomedical journals;becker s models;nonlinear models;text mining;europe pubmed central;scheffe polynomials;citation search;grupo de excelencia;citation networks;research articles;ciencias basicas y experimentales;abstracts;open access;matematicas;life sciences;clinical guidelines;statistics;full text;beckerâs models;scheffa polynomials;mixture experiments design of experiments doe data polynomial model response surfaces;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	We propose a new class of models providing a powerful unification and extension of existing statistical methodology for analysis of data obtained in mixture experiments. These models, which integrate models proposed by Scheffé and Becker, extend considerably the range of mixture component effects that may be described. They become complex when the studied phenomenon requires it, but remain simple whenever possible. This article has supplementary material online.	alpha compositing;donald becker;electronic supplementary materials;experiment;unification (computer science)	L. Brown;Alexander N. Donev;A. C. Bissett	2015		10.1080/00401706.2014.947003	text mining;data mining;mathematics;model selection;statistics	ML	30.82489591669592	-26.214821422447145	102619
031ccf2a9672c47bdeab06bb211088bf37de25c9	decomposing local probability distributions in bayesian networks for improved inference and parameter learning	probability distribution;bayesian network;conditional probability;conditional probability table;parametric model	A major difficulty in building Bayesian network models is the size of conditional probability tables, which grow exponentially in the number of parents. One way of dealing with this problem is through parametric conditional probability distributions that usually require only a linear number of parameters in the number of parents. In this paper we introduce a new class of parametric models, the pICI models, that aim at lowering the number of parameters required to specify local probability distributions, but are still capable of modeling a variety of interactions. A subset of the pICI models are decomposable and this leads to significantly faster inference as compared to models that cannot be decomposed. We also show that the pICI models are especially useful for parameter learning from small data sets and this leads to higher accuracy than learning CPTs.	bayesian network;consistency model;interaction	Adam Zagorecki;Mark Voortman;Marek J. Druzdzel	2006			generative model;machine learning;parametric model;artificial intelligence;empirical probability;computer science;regular conditional probability;chain rule (probability);bayesian statistics;mathematical statistics;bayesian network	ML	26.1896779005428	-29.07558915956088	102829
4fd9bbc352e30a4f16258f09222f5ebbcd4af1f8	fast and scalable bayesian deep learning by weight-perturbation in adam		Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximumlikelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.	algorithm;computation;deep learning;gradient;information geometry;mathematical optimization;reinforcement learning;stochastic optimization;variational principle	Mohammad Emtiyaz Khan;Didrik Nielsen;Voot Tangkaratt;Wu Lin;Yarin Gal;Akash Srivastava	2018			stochastic optimization;mathematical optimization;artificial intelligence;computation;reinforcement learning;scalability;machine learning;deep learning;computer science;inference;gaussian;bayesian probability	ML	24.676752020869447	-30.82183010091215	102889
f1c3199edbb3cdc4d484f5da482530f3cf7177ef	a distributed frank-wolfe algorithm for communication-efficient sparse learning		Learning sparse combinations is a frequent theme in machine learning. In this paper, we study its associated optimization problem in the distributed setting where the elements to be combined are not centrally located but spread over a network. We address the key challenges of balancing communication costs and optimization errors. To this end, we propose a distributed Frank-Wolfe (dFW) algorithm. We obtain theoretical guarantees on the optimization error and communication cost that do not depend on the total number of combining elements. We further show that the communication cost of dFW is optimal by deriving a lowerbound on the communication cost required to construct an -approximate solution. We validate our theoretical analysis with empirical studies on synthetic and real-world data, which demonstrate that dFW outperforms both baselines and competing methods. We also study the performance of dFW when the conditions of our analysis are relaxed, and show that dFW is fairly robust.	approximation;baseline (configuration management);dantzig–wolfe decomposition;distributed element model;frank–wolfe algorithm;machine learning;mathematical optimization;optimization problem;overhead (computing);sparse matrix;synthetic intelligence	Aurélien Bellet;Yingyu Liang;Alireza Bagheri Garakani;Maria-Florina Balcan;Fei Sha	2015		10.1137/1.9781611974010.54	parallel computing;computer science;theoretical computer science;machine learning;data mining;distributed computing;algorithm	ML	25.444751612874494	-35.86948088254461	103001
cdbbf48008c11dfa55a37cdd4bc8a77a4f07bba8	very sparse stable random projections, estimators and tail bounds for stable random projections	rate of convergence;data stream computation;data stream;data mining;machine learning;science learning;pareto distribution;stable distribution;random projection;data structure;information theory;regularity condition	The method of stable random projections [39, 41] is popular for data streaming computations, data mi ning, and machine learning. For example, in data streaming, stabl e r ndom projections offer a unified, efficient, and elegant methodology for approximating the lα norm of a single data stream, or the lα distance between a pair of streams, for any 0 < α ≤ 2. [18] and [20] applied stable random projections for approx imating the Hamming norm and the max-dominance norm, respectively, using very small α. Another application is to approximate all pairwise lα distances in a data matrix to speed up clustering, classifica tion, or kernel computations. Given that stable random projections have be en successful in various applications, this paper will focus on three different aspects in improving the current practice of stable random projections.	approximation algorithm;cluster analysis;computation;machine learning;random projection;sparse;window function	Ping Li	2006	CoRR		econometrics;mathematical optimization;combinatorics;data structure;information theory;computer science;stable distribution;pareto distribution;mathematics;rate of convergence;statistics	ML	29.65898955648569	-29.496936037609856	103079
0be3f7cd55e5a1cc965944b63aebdbf164f564dd	structured bayesian compressive sensing with spatial location dependence via variational bayesian inference		Abstract In this paper, a novel non-parametric Bayesian compressive sensing algorithm is proposed to enhance reconstruction performance of sparse entries with a continuous structure by exploiting the location dependence of entries. An approach is proposed which involves the logistic model and location-dependent Gaussian kernel. The variational Bayesian inference scheme is used to perform the posterior distributions and acquire an approximately analytical solution. Compared to the conventional clustered based methods, which only exploit the information of neighboring pixels, the proposed approach takes the relationship between the pixels of the entire image into account to enable the utilization of the underlying sparse signal structure. It significantly reduces the required number of observations for sparse reconstruction. Both real-valued signal applications, including one-dimension signal and two-dimension image, and complex-valued signal applications, including single-snapshot direction-of-arrival (DOA) estimation of distributed sources and inverse synthetic aperture radar (ISAR) imaging with a limited number of pluses, demonstrate the superiority of the proposed algorithm.	bayesian programming;compressed sensing;variational principle	Qisong Wu;Shiliang Fang	2017	Digital Signal Processing	10.1016/j.dsp.2017.08.007	compressed sensing;pixel;pattern recognition;inverse synthetic aperture radar;mathematics;gaussian function;artificial intelligence;logistic regression;machine learning;bayesian inference;bayesian probability	ML	31.403435633644527	-29.48789256441453	103117
bd4eaaa64305194be689c776429292cd16090853	theoretical foundations of equitability and the maximal information coefficient		The maximal information coefficient (MIC) is a tool for finding the strongest pairwise relationships in a data set with many variables [1]. MIC is useful because it gives similar scores to equally noisy relationships of different types. This property, called equitability, is important for analyzing high-dimensional data sets. Here we formalize the theory behind both equitability and MIC in the language of estimation theory. This formalization has a number of advantages. First, it allows us to show that equitability is a generalization of power against statistical independence. Second, it allows us to compute and discuss the population value of MIC, which we call MIC∗. In doing so we generalize and strengthen the mathematical results proven in [1] and clarify the relationship between MIC and mutual information. Introducing MIC∗ also enables us to reason about the properties of MIC more abstractly: for instance, we show that MIC∗ is continuous and that there is a sense in which it is a canonical “smoothing” of mutual information. We also prove an alternate, equivalent characterization of MIC∗ that we use to state new estimators of it as well as an algorithm for explicitly computing it when the joint probability density function of a pair of random variables is known. Our hope is that this paper provides a richer theoretical foundation for MIC and equitability going forward. This paper will be accompanied by a forthcoming companion paper that performs extensive empirical analysis and comparison to other methods and discusses the practical aspects of both equitability and the use of MIC and its related statistics.	algorithm;estimation theory;maximal information coefficient;maximal set;mutual information;smoothing	Yakir A Reshef;David N. Reshef;Pardis C Sabeti;Michael Mitzenmacher	2014	CoRR		econometrics;mathematics;statistics	ML	29.083402873501566	-29.279999707381602	103183
c80b36aa167c00d542071d3bfb0008aff3bf8245	a probabilistic framework for multi-view feature learning with many-to-many associations via neural networks		A simple framework Probabilistic Multi-view Graph Embedding (PMvGE) is proposed for multi-view feature learning with many-to-many associations so that it generalizes various existing multi-view methods. PMvGE is a probabilistic model for predicting new associations via graph embedding of the nodes of data vectors with links of their associations. Multi-view data vectors with many-to-many associations are transformed by neural networks to feature vectors in a shared space, and the probability of new association between two data vectors is modeled by the inner product of their feature vectors. While existing multi-view feature learning techniques can treat only either of many-to-many association or non-linear transformation, PMvGE can treat both simultaneously. By combining Mercer’s theorem and the universal approximation theorem, we prove that PMvGE learns a wide class of similarity measures across views. Our likelihoodbased estimator enables efficient computation of non-linear transformations of data vectors in largescale datasets by minibatch SGD, and numerical experiments illustrate that PMvGE outperforms existing multi-view methods.	algorithm;artificial neural network;computation;experiment;feature learning;feature vector;graph embedding;many-to-many;nonlinear system;numerical analysis;statistical model;universal approximation theorem	Akifumi Okuno;Tetsuya Hada;Hidetoshi Shimodaira	2018			machine learning;mathematics;graph embedding;canonical correlation;pattern recognition;universal approximation theorem;multiset;artificial intelligence;probabilistic logic;artificial neural network;feature vector;feature learning	ML	28.82495053727511	-34.72253869098791	103730
c1c953a44aa855c2522a161dd8e5134215a03a7a	how to best sample a periodic probability distribution, or on the accuracy of hamiltonian finding strategies	adaptive tomography;experiment design;quantum physics;quantum mechanics;probability distribution;relaxation time;quantum process tomography;parameter estimation;cramer rao bound;hamiltonian estimation;experience design	Projective measurements of a single two-level quantum mechanical system (a qubit) evolving under a time-independent Hamiltonian produce a probability distribution that is periodic in the evolution time. The period of this distribution is an important parameter in the Hamiltonian. Here, we explore how to design experiments so as to minimize error in the estimation of this parameter. While it has been shown that useful results may be obtained by minimizing the risk incurred by each experiment, such an approach is computationally intractable in general. Here, we motivate and derive heuristic strategies for experiment design that enjoy the same exponential scaling as fully optimized strategies. We then discuss generalizations to the case of finite relaxation times, T 2 < ?.	hamiltonian (quantum mechanics)	Christopher Ferrie;Christopher E. Granade;David G. Cory	2013	Quantum Information Processing	10.1007/s11128-012-0407-6	probability distribution;mathematical optimization;combinatorics;cramér–rao bound;experience design;relaxation;mathematics;estimation theory;design of experiments;quantum mechanics;adiabatic quantum computation	ML	28.209657770854353	-27.539391169385137	104237
10034476ab37df6511759053d6e45b2d411d133a	on the application of statistical learning approaches to construct inverse probability weights in marginal structural cox models: hedging against weight-model misspecification	marginal structural models;primary 97k80 applied statistics;causal inference;machine learning;inverse probability weighting;multiple sclerosis;secondary 92d30 epidemiology;model misspecification	The marginal structural Cox model (MSCM) estimates can be highly sensitive to weightmodel misspecification. We assess the performance of various popular statistical learners, such as LASSO, support vector machines, CART, bagged CART, and boosted CART, in estimating MSCM weights. When weight-models are misspecified, we find that the weights computed from boosted CART generally lead to less MSE and better coverage for the MSCM estimates. This study is motivated by the investigation of the impact of beta-interferon treatment on disability progression in subjects with multiple sclerosis from British Columbia, Canada (19952008).	color gradient;columbia (supercomputer);decision tree learning;lasso;machine learning;marginal model;proportional hazards model;support vector machine	Mohammad Ehsanul Karim;John Petkau;Paul Gustafson;Helen Tremlett;The Beams Study Group	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2016.1248574	econometrics;causal inference;marginal structural model;statistics	ML	28.844480157330516	-26.477240087694792	104368
a9a54c9775b2609ff0541f16e929cafdbee65d5c	mean-field inference of hawkes point processes		We propose a fast and efficient estimation method that is able to accurately recover the parameters of a d-dimensional Hawkes point-process from a set of observations. We exploit a mean-field approximation that is valid when the fluctuations of the stochastic intensity are small. We show that this is notably the case in situations when interactions are sufficiently weak, when the dimension of the system is high or when the fluctuations are self-averaging due to the large number of past events they involve. In such a regime the estimation of a Hawkes process can be mapped on a least-squares problem for which we provide an analytic solution. Though this estimator is biased, we show that its precision can be comparable to the one of the Maximum Likelihood Estimator while its computation speed is shown to be improved considerably. We give a theoretical control on the accuracy of our new approach and illustrate its efficiency using synthetic datasets, in order to assess the statistical estimation error of the parameters.	approximation;computation;estimation theory;interaction;least squares;point process;synthetic intelligence	Emmanuel Bacry;Stéphane Gaïffas;Iacopo Mastromatteo;Jean-François Muzy	2015	CoRR		econometrics;mathematical optimization;mathematics;statistics	ML	28.19471851235594	-27.696828625707052	104384
6ef271709f9b863e065262524918c988ecb799e8	failure detection, isolation, and recovery of multifunctional self-validating sensor	relevance vector machine rvm failure recovery multifunctional self validating sensor principal component analysis pca;principal component analysis training predictive models failure analysis circuit faults accuracy;sensor fusion error statistics fault trees prediction theory principal component analysis;prediction theory;principal component analysis;error statistics;squared prediction error statistic failure detection failure isolation failure recovery multifunctional self validating sensor relevance vector machine principal component analysis pca fdir online updating algorithm rvm predictor fault free signal tracking;sensor fusion;fault trees	A novel strategy based on a relevance vector machine (RVM) coupled with principal component analysis (PCA) is proposed for failure detection, isolation, and recovery (FDIR) of a multifunctional self-validating sensor. The working principle and the online updating algorithm of the RVM predictor are emphasized to identify and recover faults. The proposed predictor can effectively isolate multiple simultaneous faults of multifunctional sensors and accomplish failure recovery with high accuracy and good timeliness. Further, it also possesses a good ability of tracking fault-free signals with sudden changes. Failure detection is carried out by using PCA-based squared prediction error statistics. The PCA-RVM method can distinguish the normal signals with sudden changes from faulty signals. The performance of the strategy is compared with other different predictors, and it is evaluated in a real multifunctional self-validating sensor experimental system. Results demonstrate that the proposed methodology provides a better solution to the FDIR of multifunctional self-validating sensors.	experiment;experimental system;failure analysis;fault detection and isolation;kerrison predictor;mehrotra predictor–corrector method;multi-function printer;online algorithm;principal component analysis;relevance vector machine;sensor;simulation;sparse matrix	Zhengguang Shen;Qi Wang	2012	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2012.2205509	fault tree analysis;computer science;engineering;machine learning;pattern recognition;data mining;sensor fusion;principal component analysis	Mobile	37.431608923635785	-29.08495845687767	104542
8f280f0b1a191317bad55f584fc6144a732667f5	a fuzzy adaptive network approach to parameter estimation in cases where independent variables come from an exponential distribution	analisis numerico;fuzzy adaptive network;exponential distribution;anfis;fuzzy set;computacion informatica;matematicas aplicadas;mathematiques appliquees;generic model;fonction repartition;11r29;adaptive network based fuzzy inference system;regression model;systeme adaptatif;analyse numerique;algorithme;data clustering;algorithm;funcion distribucion;fuzzy clustering;modelo regresion;distribution function;numerical analysis;ciencias basicas y experimentales;modele regression;matematicas;adaptive system;estimacion parametro;membership function;sistema adaptativo;switching regression;regression analysis;sistema difuso;systeme flou;parameter estimation;estimation parametre;grupo a;applied mathematics;validity criterion;fuzzy system;algoritmo	"""In a regression analysis, it is assumed that the observations come from a single class in a data cluster and the simple functional relationship between the dependent and independent variables can be expressed using the general model; Y=f(X)+@e. However; a data cluster may consist of a combination of observations that have different distributions that are derived from different clusters. When faced with issues of estimating a regression model for fuzzy inputs that have been derived from different distributions, this regression model has been termed the 'switching regression model' and it is expressed with Y^L=f^L(X)+@e^L(L=@?""""i""""=""""1^pl""""i). Here l""""i indicates the class number of each independent variable and p is indicative of the number of independent variables [J.R. Jang, ANFIS: Adaptive-network-based fuzzy inference system, IEEE Transaction on Systems, Man and Cybernetics 23 (3) (1993) 665-685; M. Michel, Fuzzy clustering and switching regression models using ambiguity and distance rejects, Fuzzy Sets and Systems 122 (2001) 363-399; E.Q. Richard, A new approach to estimating switching regressions, Journal of the American Statistical Association 67 (338) (1972) 306-310]. In this study, adaptive networks have been used to construct a model that has been formed by gathering obtained models. There are methods that suggest the class numbers of independent variables heuristically. Alternatively, in defining the optimal class number of independent variables, the use of suggested validity criterion for fuzzy clustering has been aimed. In the case that independent variables have an exponential distribution, an algorithm has been suggested for defining the unknown parameter of the switching regression model and for obtaining the estimated values after obtaining an optimal membership function, which is suitable for exponential distribution."""	estimation theory;time complexity	Türkan Erbay Dalkiliç;Aysen Apaydin	2009	J. Computational Applied Mathematics	10.1016/j.cam.2008.07.057	segmented regression;econometrics;fuzzy number;adaptive system;mathematics;regression analysis;statistics	ML	33.46113847943225	-24.112204571869835	104690
18d3fc1644affced131686c7d2eecf1fbc57a0bd	evolutionary soft co-clustering		We consider the mining of hidden block structures from time-varying data using evolutionary co-clustering. Existing methods are based on the spectral learning framework, thus lacking a probabilistic interpretation. To overcome this limitation, we develop a probabilistic model for evolutionary co-clustering in this paper. The proposed model assumes that the observed data are generated via a two-step process that depends on the historic co-clusters, thereby capturing the temporal smoothness in a probabilistically principled manner. We develop an EM algorithm to perform maximum likelihood parameter estimation. An appealing feature of the proposed probabilistic model is that it leads to soft co-clustering assignments naturally. To the best of our knowledge, our work represents the first attempt to perform evolutionary soft co-clustering. We evaluate the proposed method on both synthetic and real data sets. Experimental results show that our method consistently outperforms prior approaches based on spectral method.	authorization;biclustering;cluster analysis;estimation theory;expectation–maximization algorithm;spectral method;statistical model;synthetic intelligence;unsupervised learning	Shuiwang Ji;Wenlu Zhang;Rui Zhang	2013		10.1137/1.9781611972832.14	artificial intelligence;expectation–maximization algorithm;machine learning;pattern recognition;computer science;smoothness;statistical model;probabilistic logic;biclustering;estimation theory;data set;spectral method	ML	29.85822405293688	-31.218471033517535	105022
4499ccf356e7a91a0294aaf38e6bfb54e8e0cbc7	what cannot be learned with bethe approximations		We address the problem of learning the parameters in graphical models when inference is intractable. A common strategy in this case is to replace the partition function with its Bethe approximation. We show that there exists a regime of empirical marginals where such Bethe learning will fail. By failure we mean that the empirical marginals cannot be recovered from the approximated maximum likelihood parameters (i.e., moment matching is not achieved). We provide several conditions on empirical marginals that yield outer and inner bounds on the set of Bethe learnable marginals. An interesting implication of our results is that there exists a large class of marginals that cannot be obtained as stable fixed points of belief propagation. Taken together our results provide a novel approach to analyzing learning with Bethe approximations and highlight when it can be expected to work or fail. Probabilistic graphical models [8, 23] are a powerful tool for describing complex multivariate distributions. They have been used successfully in a wide range of fields, from computational biology to machine vision and natural language processing. To use such a model in practice, one typically needs to solve two related tasks. The first is the inference task which involves calculating probabilities of events under the model. The second task involves learning the parameters of the model from empirical data. Unfortunately, in many models of interest the inference problem is computationally hard, and cannot be solved exactly in practice. This has motivated extensive research into approximate inference schemes, some of which have been quite successful empirically. Perhaps the most well known of these is the belief propagation (BP) algorithm, which is closely related to variational approximations based on Bethe free energies [26]. Another variational approach, which uses convex free energies is the tree-reweighted (TRW) method [22]. Although the TRW approach results in convex optimization problems for inference, it sometimes yields marginals that are inferior to those obtained by BP (e.g., see [9]). How should one learn the parameters of a model when inference is intractable? The typical approach to parameter learning is likelihood maximization, but when inference is intractable it is also hard to maximize the likelihood. Because of this difficulty, many methods have been devised to approximate the learning problem. One elegant approach is to approximate the likelihood using the same variational approximation that is employed during inference [5, 14, 16, 19]. Analyzing the performance of approximate learning schemes is challenging, since even the accuracy of the underlying variational approximations is hard to analyze. Furthermore, we do not generally expect the learned model to be similar to the one obtained using exact maximum likelihood. One approach, which has recently been introduced by Wainwright [19] is to use the notion of moment matching. In exact maximum likelihood learning, the learned model has a nice property: some if its marginals are guaranteed to be identical to those of the empirical data. This property is often referred to as moment matching. Wainwright [19, 21] has shown that when using convex variational approximations such as TRW, the learned model also has the moment matching property in the following sense: if one applies approximate inference to it (using the same variational approach that was used during learning), the resulting marginals will be equal to When the data are known to be generated by a graphical model of the same structure, pseudo-likelihood [1] can be used and is consistent. However, this assumption is rarely met in practice, and pseudo-likelihood often does not perform well in these cases. the empirical ones. However, these results cannot be applied to learning with Bethe approximations, since the latter are not convex. Because of the success of Bethe approximations in a wide array of applications, it is important to understand the advantages and limitations of learning with those. This is precisely the goal of our work. It may initially seem like learning with Bethe approximations would also result in a moment matching property. In other words, if we use Bethe approximations during both learning and inference, our learned model will agree with the empirical marginals. However, as we show here, the situation is considerably more complex. In the current work we provide some surprising results with respect to moment matching and Bethe approximations, that shed light on the performance of learning with such approximations, and on properties of the BP algorithm. Our main results are: • We show that there exist empirical distributions for which Bethe approximations cannot perform moment matching. In other words, if we run BP on the optimal Bethe parameters, we will not recover the empirical marginals. Such empirical distributions are thus bad inputs for Bethe approximations, since the learned parameters cannot be used to reconstruct the original marginals. • We provide inner and outer bounds on the set of marginals for which Bethe moment matching is possible, and show that they agree with empirical behavior of Bethe learning. Surprisingly, we show that binary attractive models cannot be learned with Bethe approximations for certain graphs. • Our results also provide a novel characterization of BP fixed points. Specifically, we show that there is a large class of marginals that cannot be obtained as stable fixed points of BP. Taken together, our results provide a novel way of analyzing learning with Bethe approximations. 1 Maximum Likelihood in Graphical Models We focus on pairwise Markov random fields for simplicity. That is, we consider random variables X1, . . . , XNV and pairwise functions θij(xi, xj) corresponding to edges E in a graph G with NV nodes. The MRF corresponding to these parameters is given by: p(x;θ) = 1 Z(θ) exp ∑ ij∈E θij(xi, xj) + NV ∑	approximation algorithm;backpropagation;belief propagation;calculus of variations;computational biology;computational complexity theory;convex optimization;exptime;existential quantification;expectation–maximization algorithm;graphical model;machine vision;markov chain;markov random field;mathematical optimization;natural language processing;nv network;partition function (mathematics);software propagation;variational principle	Uri Heinemann;Amir Globerson	2011			mathematical optimization;combinatorics;mathematics;statistics	ML	24.654491814549246	-29.577461887394605	105201
ef7db3295c81b87e6a1f51d80c5d03f4c7054868	minimal shrinkage for noisy data recovery using schatten-p norm objective		Noisy data recovery is an important problem in machine learning field, which has widely applications for collaborative prediction, recommendation systems, etc. One popular model is to use trace norm model for noisy data recovery. However, it is ignored that the reconstructed data could be shrank (i.e., singular values could be greatly suppressed). In this paper, we present novel noisy data recovery models, which replaces the standard rank constraint (i.e., trace norm) using Schatten-p Norm. The proposed model is attractive due to its suppression on the shrinkage of singular values at smaller parameter p. We analyze the optimal solution of proposed models, and characterize the rank of optimal solution. Efficient algorithms are presented, the convergences of which are rigorously proved. Extensive experiment results on 6 noisy datasets demonstrate the good performance of proposed minimum shrinkage models.	algorithm;data recovery;ibm notes;low-rank approximation;machine learning;noisy-storage model;recommender system;signal-to-noise ratio;zero suppression	Deguang Kong;Miao Zhang;Chris H. Q. Ding	2013		10.1007/978-3-642-40991-2_12	mathematical optimization;machine learning;mathematics;statistics	ML	26.173067591359665	-37.029755966548294	105299
355a6b3f8edf672cf990573d220c034fba134b1f	fast computation of the principal components of genotype matrices in julia		Finding the largest few principal components of a matrix of genetic data is a common task in genome-wide association studies (GWASs), both for dimensionality reduction and for identifying unwanted factors of variation. We describe a simple random matrix model for matrices that arise in GWASs, showing that the singular values have a bulk behavior that obeys a MarchenkoPastur distributed with a handful of large outliers. We also implement Golub-Kahan-Lanczos (GKL) bidiagonalization in the Julia programming language, providing thick restarting and a choice between full and partial reorthogonalization strategies to control numerical roundoff. Our implementation of GKL bidiagonalization is up to 36 times faster than software tools used commonly in genomics data analysis for computing principal components, such as EIGENSOFT and FlashPCA, which use dense LAPACK routines and randomized subspace iteration respectively.	arpack;bidiagonalization;computation;dimensionality reduction;geo-imputation;introspection;iteration;iterative method;julia;lapack;lanczos resampling;missing data;numerical analysis;programming language;programming tool;randomized algorithm	Jiahao Chen;Andreas Noack;Alan Edelman	2018	CoRR		singular value;mathematical optimization;dimensionality reduction;computation;principal component analysis;mathematics;simple random sample;subspace topology;matrix (mathematics);bidiagonalization	ML	33.343772084097026	-28.848141378915425	105371
74b14526611136cc34951a80885ef53ca70c81dc	communication-efficient distributed optimization for sparse learning via two-way truncation		We propose a communication and computation efficient algorithm for high-dimensional distributed sparse learning, motivated by the approach of (Wang et al., 2016). At each iteration, local machines compute local gradients on their own local data and using these, a master machine solves a shifted l\ regularized minimization problem. Here, our contribution reduces the communication cost per transmission from the order of the parameter dimension to the order of the number of nonzero entries in the parameter via a Two-Way Truncation procedure. Theoretically, we prove that the estimation error of the proposed algorithm decreases exponentially and matches that of the centralized method under mild conditions. Extensive experiments on both simulated data and real data support that the proposed algorithm is efficient and has statistical performance comparable with the centralized method.	algorithm;centralized computing;computation;experiment;gradient;iteration;mathematical optimization;sparse matrix;truncation;word lists by frequency	Jineng Ren;Xingguo Li;Jarvis D. Haupt	2017	2017 IEEE 7th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)	10.1109/CAMSAP.2017.8313109	distributed algorithm;truncation;computation;mathematical optimization;exponential growth;mathematics	ML	25.518361701799577	-35.6560379975172	105411
ad55be79fa972a7997e370a323ac2ff5ac291d25	absolute penalty and shrinkage estimation in partially linear models	model selection;treatment effect;partially linear model;shrinkage estimation;linear regression model;semiparametric estimation;adaptive lasso;absolute penalty estimation;james stein estimator;b spline approximation;semiparametric model;partial linear model;parameter estimation;monte carlo simulation;lasso	In the context of a partially linear regression model, shrinkage semiparametric estimation is considered based on the Stein-rule. In this framework, the coefficient vector is partitioned into two sub-vectors: the first sub-vector gives the coefficients of interest, i.e., main effects (for example, treatment effects), and the second sub-vector is for variables that may or may not need to be controlled. When estimating the first sub-vector, the best estimate may be obtained using either the full model that includes both sub-vectors, or the reduced model which leaves out the second sub-vector. It is demonstrated that shrinkage estimators which combine two semiparametric estimators computed for the full model and the reduced model outperform the semiparametric estimator for the full model. Using the semiparametric estimate for the reduced model is best when the second sub-vector is the null vector, but this estimator suffers seriously from bias otherwise. The relative dominance picture of suggested estimators is investigated. In particular, suitability of estimating the nonparametric component based on the B-spline basis function is explored. Further, the performance of the proposed estimators is compared with an absolute penalty estimator through Monte Carlo simulation. Lasso and adaptive lasso were implemented for simultaneous model selection and parameter estimation. A real data example is given to compare the proposed estimators with lasso and adaptive lasso estimators.	linear model	S. M. Enayetur Raheem;S. Ejaz Ahmed;Kjell A. Doksum	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2011.09.021	econometrics;mathematical optimization;james–stein estimator;linear regression;lasso;mathematics;average treatment effect;estimation theory;model selection;semiparametric model;statistics;monte carlo method;semiparametric regression	ML	29.371284774799967	-24.69892481414229	105663
b3bd57b648bd6099e821265a7739dd3aa166196d	a sequential sampling framework for spectral k-means based on efficient bootstrap accuracy estimations: application to distributed clustering	matrix perturbation theory;sampling;article letter to editor;clustering;distributed clustering;spectral;bootstrapping;asymptotic convergence	The scalability of learning algorithms has always been a central concern for data mining researchers, and nowadays, with the rapid increase in data storage capacities and availability, its importance has increased. To this end, sampling has been studied by several researchers in an effort to derive sufficiently accurate models using only small data fractions. In this article we focus on spectral k-means, that is, the k-means approximation as derived by the spectral relaxation, and propose a sequential sampling framework that iteratively enlarges the sample size until the k-means results (objective function and cluster structure) become indistinguishable from the asymptotic (infinite-data) output. In the proposed framework we adopt a commonly applied principle in data mining research that considers the use of minimal assumptions concerning the data generating distribution. This restriction imposes several challenges, mainly related to the efficiency of the sequential sampling procedure. These challenges are addressed using elements of matrix perturbation theory and statistics. Moreover, although the main focus is on spectral k-means, we also demonstrate that the proposed framework can be generalized to handle spectral clustering.  The proposed sequential sampling framework is consecutively employed for addressing the distributed clustering problem, where the task is to construct a global model for data that resides in distributed network nodes. The main challenge in this context is related to the bandwidth constraints that are commonly imposed, thus requiring that the distributed clustering algorithm consumes a minimal amount of network load. This illustrates the applicability of the proposed approach, as it enables the determination of a minimal sample size that can be used for constructing an accurate clustering model that entails the distributional characteristics of the data. As opposed to the relevant distributed k-means approaches, our framework takes into account the fact that the choice of the number of clusters has a crucial effect on the required amount of communication. More precisely, the proposed algorithm is able to derive a statistical estimation of the required relative sizes for all possible values of k. This unique feature of our distributed clustering framework enables a network administrator to choose an economic solution that identifies the crude cluster structure of a dataset and not devote excessive network resources for identifying all the “correct” detailed clusters.	algorithm;approximation;cluster analysis;computer data storage;data mining;estimation theory;internet access;k-means clustering;linear programming relaxation;loss function;machine learning;nyquist–shannon sampling theorem;optimization problem;perturbation theory;sampling (signal processing);scalability;spectral clustering	Dimitrios Mavroeidis;Panagis Magdalinos	2012	TKDD	10.1145/2297456.2297457	correlation clustering;constrained clustering;sampling;data stream clustering;k-medians clustering;fuzzy clustering;computer science;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;bootstrapping;statistics;clustering high-dimensional data	ML	26.984704236974547	-33.888001774746954	105743
f70b6be3d37c25d84c4cd2efd667c185629fc673	power load event detection and classification based on edge symbol analysis and support vector machine	nova;research repository;university of newcastle;institutional repository;research online	Energy signature analysis of power appliance is the core of nonintrusive load monitoring (NILM) where the detailed data of the appliances used in houses are obtained by analyzing changes in the voltage and current. This paper focuses on developing an automatic power load event detection and appliance classification based on machine learning. In power load event detection, the paper presents a new transient detection algorithm. By turn-on and turn-off transient waveforms analysis, it can accurately detect the edge point when a device is switched on or switched off. The proposed load classification technique can identify different power appliances with improved recognition accuracy and computational speed. The load classification method is composed of two processes including frequency feature analysis and support vector machine. The experimental results indicated that the incorporation of the new edge detection and turn-on and turn-off transient signature analysis into NILM revealed more information than traditional NILM methods. The load classification method has achieved more than ninety percent recognition rate.	support vector machine	Lei Jiang;Jiaming Li;Suhuai Luo;Sam West;Glenn Platt	2012	Applied Comp. Int. Soft Computing	10.1155/2012/742461	real-time computing;speech recognition;computer science;machine learning;data mining;nova;computer security	HPC	35.529765900272395	-31.801714892075882	105845
3af01ea6b915185dce21040eb3fa1865e417c48b	logistic normal priors for unsupervised probabilistic grammar induction	normal distribution;hidden markov model;probabilistic context free grammar;natural language;dependency parsing;em algorithm;grammar induction;bayesian model;probabilistic grammar	We explore a new Bayesian model for probabilistic grammars, a family of distributions over discrete structures that includes hidden Markov models and probabilistic context-free grammars. Our model extends the correlated topic model framework to probabilistic grammars, exploiting the logistic normal distribution as a prior over the grammar parameters. We derive a variational EM algorithm for that model, and then experiment with the task of unsupervised grammar induction for natural language dependency parsing. We show that our model achieves superior results over previous models that use different priors.	bayesian network;calculus of variations;context-free grammar;context-free language;discrete mathematics;expectation–maximization algorithm;experiment;grammar induction;hidden markov model;markov chain;natural language;parsing;part-of-speech tagging;probabilistic turing machine;probabilistic automaton;topic model;treebank;unsupervised learning	Shay B. Cohen;Kevin Gimpel;Noah A. Smith	2008			normal distribution;natural language processing;link grammar;expectation–maximization algorithm;parsing expression grammar;grammar induction;probabilistic relevance model;computer science;machine learning;pattern recognition;stochastic grammar;mathematics;natural language;bayesian inference;stochastic context-free grammar;hidden markov model;statistics;dependency grammar;divergence-from-randomness model	ML	26.13905416846965	-30.090145950911275	105981
59ff76f99d56f67b38b3bdc1db4550300a316d90	random features for sparse signal classification		Random features is an approach for kernel-based inference on large datasets. In this paper, we derive performance guarantees for random features on signals, like images, that enjoy sparse representations and show that the number of random features required to achieve a desired approximation of the kernel similarity matrix can be significantly smaller for sparse signals. Based on this, we propose a scheme termed compressive random features that first obtains low-dimensional projections of a dataset and, subsequently, derives random features on the low-dimensional projections. This scheme provides significant improvements in signal dimensionality, computational time, and storage costs over traditional random features while enjoying similar theoretical guarantees for achieving inference performance. We support our claims by providing empirical results across many datasets.	approximation;kernel (operating system);similarity measure;sparse matrix;time complexity	Jen-Hao Rick Chang;Aswin C. Sankaranarayanan;B. V. K. Vijaya Kumar	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.583	random field;machine learning;random function;pattern recognition;data mining;mathematics;statistics	Vision	26.239503047310652	-35.164702177238	106311
4067679f07fd7474906068fee61d60258d8b3c2e	robust clustering of acoustic emission signals using neural networks and signal subspace projections	signal image and speech processing;mechanical failure;neural networks;acoustic signals;rbf;classification;quantum information technology spintronics;subspace projections;som;acoustic emission;pca;neural network	Acoustic emission-based techniques are being used for the nondestructive inspection of mechanical systems. For reliable automatic fault monitoring related to the generation and propagation of cracks, it is important to identify the transient crack-related signals in the presence of strong time-varying noise and other interferences. A prominent difficulty is the inability to differentiate events due to crack growth from noise of various origins. This work presents a novel algorithm for automatic clustering and separation of acoustic emission (AE) events based on multiple features extracted from the experimental data. The algorithm consists of two steps. In the first step, the noise is separated from the events of interest and subsequently removed using a combination of covariance analysis, principal component analysis (PCA), and differential time delay estimates. The second step processes the remaining data using a self-organizing map (SOM) neural network, which outputs the noise and AE signals into separate neurons. To improve the efficiency of classification, the short-time Fourier transform (STFT) is applied to retain the time-frequency features of the remaining events, reducing the dimension of the data. The algorithm is verified with two sets of data, and a correct classification ratio over 95% is achieved.	acoustic cryptanalysis;acoustic fingerprint;algorithm;basis function;broadcast delay;central processing unit;cluster analysis;experiment;neural networks;organizing (structure);principal component analysis;self-organization;self-organizing map;short-time fourier transform;signal subspace;software propagation;test case;test data	Vahid Emamian;Mostafa Kaveh;Ahmed H. Tewfik;Zhiqiang Shi;Laurence J. Jacobs;Jacek Jarzynski	2003	EURASIP J. Adv. Sig. Proc.	10.1155/S1110865703210027	speech recognition;failure causes;biological classification;computer science;acoustic emission;machine learning;pattern recognition;artificial neural network;principal component analysis	ML	37.549616704356474	-31.875657678445954	106432
54ef33f61863d07db1c1fa2fdff6e6370f035722	sequential graph matching with sequential monte carlo		We develop a novel probabilistic model for graph matchings and develop practical inference methods for supervised and unsupervised learning of the parameters of this model. The framework we develop admits joint inference on the parameters and the matchings. Furthermore, our framework generalizes naturally to K-partite hypergraph matchings or set packing problems. The sequential formulation of the graph matching process naturally leads to sequential Monte Carlo algorithms which can be combined with various parameter inference methods. We apply our method to image matching problems, document ranking, and our own novel quadripartite matching problem arising from the field of computational forestry.	algorithm;bayesian network;estimation theory;image registration;markov chain monte carlo;matching (graph theory);monte carlo method;particle filter;ranking (information retrieval);set packing;statistical model;supervised learning;unsupervised learning;while	Seong-Hwan Jun;Samuel W. K. Wong;James V. Zidek;Alexandre Bouchard-Côté	2017			matching (graph theory);mathematical optimization;monte carlo method;particle filter;monte carlo integration;computer science;quasi-monte carlo method;markov chain monte carlo;hybrid monte carlo	ML	26.88934260360285	-29.621939972914937	106482
0f6db9d705ac9bc95d1c1f2e092446f0899ebaa2	distributed frank-wolfe algorithm: a unified framework for communication-efficient sparse learning			frank–wolfe algorithm;sparse	Aurélien Bellet;Yingyu Liang;Alireza Bagheri Garakani;Maria-Florina Balcan;Fei Sha	2014	CoRR		distributed algorithm;theoretical computer science;machine learning;pattern recognition;sparse approximation	NLP	25.85394145371556	-36.574932706388715	106493
0842de6ba31b2dcfdc2a3e79dc59c8e71137b84c	data fusion in electronic tongue for qualitative analysis of beers	discrete wavelet transforms;brewing industry;beer style data fusion electronic tongue electrochemical sensors pattern recognition;beverages;statistical analysis;feature extraction;signal classification;manufacturing process data fusion electronic tongue beer qualitative analysis electrochemical sensor array potentiometric sensor voltammetric sensor beer style identification electrochemical measurement representative feature extraction classification ability single sensor data discrete wavelet transform statistical procedure classification model linear discriminant analysis leave one out cross validation procedure;electronic tongues;voltammetry chemical analysis beverages brewing industry discrete wavelet transforms electronic tongues feature extraction sensor fusion signal classification statistical analysis;sensor fusion;voltammetry chemical analysis;electrodes sensor phenomena and characterization electric potential sensor fusion	This paper presents the development of an Electronic Tongue based on two different arrays of electrochemical sensors (i.e. potentiometric and voltammetric) for the identification of three styles of beer. Conventionally, electrochemical measurements contain hundreds of records and cannot be processed directly, due to its high data dimension. Therefore, information obtained from both sensor families was prepossessed in order to extract representative features and then fused to improve the classification ability regarding to the use of single sensor data. On the one hand, Discrete Wavelet Transform and statistical procedures were employed as feature extraction techniques. On the other hand, classification model was build using Linear Discriminant Analysis and validated by Leave-one-out cross-validation procedure. Final results demonstrate that the ET employing data fusion is able to distinguish 100% of the types of beer as well as its manufacturing process.	cross-validation (statistics);discrete wavelet transform;feature extraction;linear discriminant analysis;potentiometer;sensor	Juan Manuel Gutiérrez-Salgado;Laura Moreno-Barón;X. Ceto;A. Mimendia;Manel del Valle	2012	2012 Fourth World Congress on Nature and Biologically Inspired Computing (NaBIC)	10.1109/NaBIC.2012.6402240	brewing;speech recognition;feature extraction;computer science;machine learning;sensor fusion	AI	36.94641981421987	-33.91439221195081	106759
987660899624223779a2f2209f9ea5bfbc9d4c75	a comparison of new and old algorithms for a mixture estimation problem	second order;rate of convergence;supervised learning;maximum likelihood;exponentiated gradient;taylor expansion;iterative algorithm;relative entropy;mixture model;stochastic approximation;exponentiated gradient algorithms;mixture models;em algorithm;em;lower bound	We investigate the problem of estimating the proportion vector which maximizes the likelihood of a given sample for a mixture of given densities. We adapt a framework developed for supervised learning and give simple derivations for many of the standard iterative algorithms like gradient projection and EM. In this framework, the distance between the new and old proportion vectors is used as a penalty term. The square distance leads to the gradient projection update, and the relative entropy to a new update which we call the exponentiated gradient update (EG_\eta). Curiously, when a second order Taylor expansion of the relative entropy is used, we arrive at an update EM_\eta which, for \eta=1, gives the usual EM update. Experimentally, both the EM_{\eta}-update and the EG_\eta-update for \eta > 1 outperform the EM algorithm and its variants. We also prove a polynomial bound on the rate of convergence of the EG_\eta algorithm.	algorithm;estimation theory	David P. Helmbold;Yoram Singer;Robert E. Schapire;Manfred K. Warmuth	1995		10.1145/225298.225306	stochastic approximation;econometrics;mathematical optimization;machine learning;mixture model;mathematics;supervised learning;statistics	Theory	29.46951025469864	-27.722206013082808	106774
f2338653bc9073219267e34298cb260cacb78bf9	a bounded p-norm approximation of max-convolution for sub-quadratic bayesian inference on additive factors	hidden markov model;bayesian inference;fast fourier transform;p norm;maximum a posteriori;null space projection;polynomial matrix;l p space;max convolution	Max-convolution is an important problem closely resembling standard convolution; as such, max-convolution occurs frequently across many fields. Here we extend the method with fastest known worst-case runtime, which can be applied to nonnegative vectors by numerically approximating the Chebyshev norm ‖ · ‖∞, and use this approach to derive two numerically stable methods based on the idea of computing pnorms via fast convolution: The first method proposed, with runtime in O(k log(k) log(log(k))) (which is less than 18k log(k) for any vectors that can be practically realized), uses the p-norm as a direct approximation of the Chebyshev norm. The second approach proposed, with runtime in O(k log(k)) (although in practice both perform similarly), uses a novel null space projection method, which extracts information from a sequence of p-norms to estimate the maximum value in the vector (this is equivalent to querying a small number of moments from a distribution of bounded support in order to estimate the maximum). The p-norm approaches are compared to one another and are shown to compute an approximation of the Viterbi path in a hidden Markov model where the transition matrix is a Toeplitz matrix; the runtime of approximating the Viterbi path is thus reduced from O(nk) steps to O(nk log(k)) steps in practice, and is demonstrated by inferring the U.S. unemployment rate from the S&P 500 stock index. 1 ar X iv :1 50 5. 07 51 9v 2 [ st at .C O ] 2 2 O ct 2 01 5	additive model;approximation;best, worst and average case;convolution;fastest;hidden markov model;kernel (linear algebra);markov chain;numerical analysis;numerical stability;stochastic matrix;toeplitz hash algorithm;viterbi algorithm	Julianus Pfeuffer;Oliver Serang	2016	Journal of Machine Learning Research		norm;fast fourier transform;mathematical optimization;polynomial matrix;combinatorics;discrete mathematics;computer science;maximum a posteriori estimation;machine learning;mathematics;bayesian inference;hidden markov model;statistics	ML	27.08060178489378	-27.310870001050716	107052
0b1d9c4e1dd82e6ea442b3f9f2fcc95d594800b9	individual radio frequency interference identification on vhf radar based on time-frequency and chaotic characteristics	tree classifier individual radio frequency interference identification time frequency and chaotic characteristics;time frequency analysis radiofrequency interference feature extraction radar countermeasures reconnaissance;tree classifier;tree classifier radio frequency interference identification vhf radar time frequency characteristic chaotic characteristic radar reconnaissance electronic warfare radar countermeasures very high frequency radar transient signal;trees mathematics;radar interference;time frequency and chaotic characteristics;individual radio frequency interference identification;trees mathematics electronic countermeasures radar interference radar signal processing;radar signal processing;electronic countermeasures	The radar reconnaissance plays an important role in modern electronic warfare as a critical link of radar countermeasures. We study individual radio frequency interference identification on the very high frequency (VHF) radar in the radar reconnaissance. The time-frequency and chaotic characteristics of the transient signals are extracted and based on these characteristics the tree classifier are designed. Real data experiments prove that individual radio frequency interference identification on VHF radar can be achieved through the method we propose.	chaos theory;experiment;interference (communication);radar;radio frequency;radio-frequency identification	Ligang Huang;Jia Xu;Li-Chang Qian	2012	2012 IEEE 12th International Conference on Computer and Information Technology	10.1109/CIT.2012.110	man-portable radar;continuous-wave radar;radar engineering details;radar lock-on;telecommunications;fire-control radar;electronic countermeasure;passive radar;bistatic radar;low probability of intercept radar;frequency agility;pulse repetition frequency;pulse-doppler radar;digital radio frequency memory;radar imaging;radar display;radar;radar jamming and deception;low-frequency radar	Mobile	38.54295429085901	-33.6947852659393	107186
4671a2a03461ad7ab4fc6c392c817e2eaaf8f3d3	research of micro-seismic signal extraction method based on rough set		Micro-seismic monitoring as a regional monitoring means to predict important dynamic disaster of the mine, meanwhile, micro-seismic signal with abundant components of spectrum and frequency bandwidth char- acteristics. Obtain sudden change time of micro-seismic abnormal signal and frequency components corresponding to sudden change time are the key is- sues in micro-seismic monitoring which to be solved urgently. In view of the variable precision rough set can not identify random rule which is supported by only a few examples, study a new hybrid model which combines rough set with Bayesian probability. In order to get more general and more reliable classification rule, make full use of intrinsic characteristic of micro-seismic monitoring knowledge system, more powerful in coping with noise data, de- rive a reliable and simple classification rule, it is more efficient to analyze a large number of micro-seismic data.	rough set	Changpeng Ji;Mo Gao	2010		10.1007/978-3-642-14880-4_74	computer science;machine learning;pattern recognition;data mining	EDA	36.67861764611403	-31.407687050740424	107266
e6f98ca55c84a30803f7e9538382e0f5063c1241	sequential monte carlo methods for parameter estimation in nonlinear state-space models	maximum likelihood;bayesian inference;expectation maximization;markov chain monte carlo	Stochastic nonlinear state-space models (SSMs) are prototypical mathematical models in geoscience. Estimating unknown parameters in nonlinear SSMs is an important issue for environmental modeling. In this paper, we present two recently developed methods that are based on the sequential Monte Carlo (SMC) method for parameter estimation in nonlinear SSMs. The first method, which belongs to classical statistics, is the SMC-based maximum likelihood estimation. The second method, belonging to Bayesian statistics, is Particle Markov Chain Monte Carlo (PMCMC). With a low-dimensional nonlinear SSM, the implementations of the two methods are demonstrated. It is concluded that these SMC-based parameter estimation methods are applicable to environmental modeling and geoscience. & 2012 Elsevier Ltd. All rights reserved.	estimation theory;markov chain monte carlo;mathematical model;monte carlo method;nonlinear system;state space	Meng Gao;Hui Zhang	2012	Computers & Geosciences	10.1016/j.cageo.2012.03.013	econometrics;mathematical optimization;dynamic monte carlo method;hybrid monte carlo;particle filter;markov chain monte carlo;expectation–maximization algorithm;monte carlo molecular modeling;mathematics;maximum likelihood;bayesian inference;statistics;monte carlo method	ML	31.632587685920022	-26.503762680954168	107283
39ad845f9cbafdba5dd6f8f6db340c3fba9abd16	predicting the evolution of stationary graph signals		One way of tackling the dimensionality issues arising in the modeling of a multivariate process is to assume that the inherent data structure can be captured by a graph. We here focus on the problem of predicting the evolution of a process that is time and graph stationary, i.e., a time-varying signal whose first two statistical moments are invariant over time and correlated to a known graph topology. This stationarity assumption allows us to regularize the estimation problem, reducing the variance and computational complexity, two common issues plaguing high-dimensional vector autoregressive models. In addition, our method compares favorably to state-of-the-art graph and time-based methods: it outperforms previous graph causal models as well as a purely time-based method.	autoregressive model;causal filter;computation;computational complexity theory;coupling (computer programming);data structure;graph (discrete mathematics);laplacian matrix;stationary process;stochastic process;topological graph theory;wikipedia	Andreas Loukas;Nathanael Perraudin	2017	2017 51st Asilomar Conference on Signals, Systems, and Computers	10.1109/ACSSC.2017.8335136	econometrics;computer science;machine learning;critical graph;statistics	ML	26.52403554314845	-28.082966586482183	107451
5c41ab70d853bf21240d96052de41e19d377d566	the analog formulation of sparsity implies infinite divisibility and rules out bernoulli-gaussian priors	biological system modeling;stochastic processes;signal processing;continuous time stochastic process analog sparsity formulation infinite divisibility bernoulli gaussian priors real world signals continuous time random processes linear transformations bernoulli gaussian distribution;random processes;gaussian distribution;stochastic processes gaussian distribution random processes signal processing	Motivated by the analog nature of real-world signals, we investigate continuous-time random processes. For this purpose, we consider the stochastic processes that can be whitened by linear transformations and we show that the distribution of their samples is necessarily infinitely divisible. As a consequence, such a modeling rules out the Bernoulli-Gaussian distribution since we are able to show in this paper that it is not infinitely divisible. In other words, while the Bernoulli-Gaussian distribution is among the most studied priors for modeling sparse signals, it cannot be associated with any continuous-time stochastic process. Instead, we propose to adapt the priors that correspond to the increments of compound Poisson processes, which are both sparse and infinitely divisible.	bernoulli polynomials;compressed sensing;discretization;euler–bernoulli beam theory;infinite divisibility;simulation;sparse matrix;stochastic process;synthetic data	Arash Amini;Ulugbek Kamilov;Michael Unser	2012	2012 IEEE Information Theory Workshop	10.1109/ITW.2012.6404765	mathematical optimization;discrete mathematics;compound poisson distribution;normal-inverse gaussian distribution;mathematics;infinite divisibility;statistics	ML	31.69295240009219	-26.874901249566058	107653
104ec5771ab9cd92d705432ca05c8a1735bc3a69	estimating join selectivities using bandwidth-optimized kernel density models		Accurately predicting the cardinality of intermediate plan operations is an essential part of any modern relational query optimizer. The accuracy of said estimates has a strong and direct impact on the quality of the generated plans, and incorrect estimates can have a negative impact on query performance. One of the biggest challenges in this field is to predict the result size of join operations. Kernel Density Estimation (KDE) is a statistical method to estimate multivariate probability distributions from a data sample. Previously, we introduced a modern, self-tuning selectivity estimator for range scans based on KDE that outperforms state-of-the-art multidimensional histograms and is efficient to evaluate on graphics cards. In this paper, we extend these bandwidth-optimized KDE models to estimate the result size of single and multiple joins. In particular, we propose two approaches: (1) Building a KDE model from a sample drawn from the join result. (2) Efficiently combining the information from base table KDE models. We evaluated our KDE-based join estimators on a variety of synthetic and real-world datasets, demonstrating that they are superior to state-of-the art join estimators based on sketching or sampling.	graphics;join (sql);kernel density estimation;mathematical optimization;query optimization;relational database;sampling (signal processing);selectivity (electronic);self-tuning;synthetic intelligence;video card	Martin Kiefer;Max Heimel;Sebastian Breß;Volker Markl	2017	PVLDB	10.14778/3151106.3151112	probability distribution;data mining;estimator;sampling (statistics);kernel density estimation;computer science;multivariate statistics;query optimization;joins;histogram;pattern recognition;artificial intelligence	DB	33.140605676630294	-31.2151067676101	107839
f0a08d6a4052edd087dd408f3c63399db9c0f8ef	joint design of gaussianized spectrum-based features and least-square linear classifier for automatic acoustic environment classification in hearing aids	classification algorithm;spectrum;sound classification;spectrum gaussianization;mean square error;feature extraction;digital hearing aids;least square;audio signals;error probability;power law;gaussian distribution;hearing aid;neural network	In this paper we propose a method to generate a novel set of features in order to improve sound classification in digital hearing aids. The approach is based on the fact that those classification algorithms whose design consists in minimizing the mean squared error work better when the data to be classified exhibit a Gaussian distribution. The novel features we propose are thus based on sound spectral magnitudes that, prior to the feature calculation itself, are Gaussianized by a power law parametrized by a design parameter, @a. The explored method allows to jointly design the sound features and a least-square linear classifier, whose design parameters are also parametrized by @a. The experimental work suggests that there is a proper value of @a for which the so-designed classifier, fed with the novel features, exhibits a low error probability. Moreover, we have found that the method can be extended to nonlinear classifiers also trained by minimizing the mean squared error, such as, for instance, neural networks.	acoustic cryptanalysis;linear classifier	Lucas Cuadra;Roberto Gil-Pita;Enrique Alexandre;Manuel Rosa-Zurera	2010	Signal Processing	10.1016/j.sigpro.2010.02.024	normal distribution;spectrum;power law;speech recognition;feature extraction;computer science;probability of error;machine learning;audio signal;pattern recognition;mathematics;mean squared error;least squares;artificial neural network;statistics	EDA	36.87985478505285	-35.10695817394613	108089
8269d98d864a46ea1b8fb4f88a6a2da96543a34f	arc mat, a toolbox for using arcview shape files for spatial econometrics and statistics	computadora;maps;software;programa;population;base donnee;systeme information geographique;analisis estadistico;mapa;geographic information system;north america;logiciel;america del norte;amerique du nord;amerique;analisis espacial;ordinateur;computer graphics;statistical software;chine;polygone;database;base dato;econometria;cartographie;probabilistic approach;etats unis;computer;estados unidos;carte;computer graphic;algorithme;polygon;algorithm;modelo;cartografia;statistical analysis;asie;enfoque probabilista;approche probabiliste;analyse statistique;poblacion;crecimiento;poligono;cartography;croissance;modele;econometrics;spatial econometrics;computer hardware;spatial analysis;america;china;growth;grafico computadora;materiel informatique;infographie;models;analyse spatiale;sistema informacion geografica;spatial model;econometrie;asia;hardware;algoritmo	Professor, Dept. of Economics University of Toledo 1995-2006 Associate Professor, Dept. of Economics University of Toledo 1989-1994 Assistant Professor, Dept. of Economics University of Toledo 1988-1989 Associate Professor, Dept. of Economics Bowling Green State University 1987-1988 Assistant Professor, Dept. of Economics Bowling Green State University 1982-1987 Full-time Instructor, Dept. of Economics Bowling Green State University 1977-1978	arcview	James P. LeSage;R. Kelley Pace	2004		10.1007/978-3-540-30231-5_12	computer science;polygon;mathematics;geographic information system;spatial econometrics;operations research;cartography;statistics	ML	34.8568352146127	-35.16655445511951	108333
f08d958ac56944612f19a7f19b7b3d589a2bb8c3	sparse multivariate factor regression	alternating minimization sparse multivariate factor regression algorithm dimensionality reduction parameter estimation long matrix wide matrix nonconvex optimization problem greedy optimization algorithm;sparse multivariate regression;factor regression;sparse principal component analysis;low rank;signal processing algorithms matrix decomposition prediction algorithms sparse matrices multivariate regression algorithm design and analysis optimization;signal processing matrix algebra minimisation regression analysis;sparse principal component analysis sparse multivariate regression factor regression low rank	We introduce a sparse multivariate regression algorithm which simultaneously performs dimensionality reduction and parameter estimation. We decompose the coefficient matrix into two sparse matrices: a long matrix mapping the predictors to a set of factors and a wide matrix estimating the responses from the factors. We impose an elastic net penalty on the former and an ℓ1 penalty on the latter. Our algorithm simultaneously performs dimension reduction and coefficient estimation and automatically estimates the number of latent factors from the data. Our formulation results in a non-convex optimization problem, which despite its flexibility to impose effective low-dimensional structure, is difficult, or even impossible, to solve exactly in a reasonable time. We specify a greedy optimization algorithm based on alternating minimization to solve this non-convex problem and provide theoretical results on its convergence and optimality. Finally, we demonstrate the effectiveness of our algorithm via experiments on simulated and real data.	coefficient;convex optimization;dimensionality reduction;elastic map;elastic net regularization;estimation theory;experiment;factor regression model;general linear model;greedy algorithm;latent variable;mathematical optimization;optimization problem;sparse dictionary learning;sparse matrix	Milad Kharratzadeh;Mark Coates	2016	2016 IEEE Statistical Signal Processing Workshop (SSP)	10.1109/SSP.2016.7551732	matrix t-distribution;mathematical optimization;sparse pca;sparse matrix;machine learning;sparse approximation;mathematics;factor regression model;statistics	ML	27.459446280517763	-34.67946363823057	108437
2c6f911bef5e2c88306569f076171a3ccd2abc8b	computational implications of reducing data to sufficient statistics		Given a large dataset and an estimation task, it is common to pre-process the data by reducing them to a set of sufficient statistics. This step is often regarded as straightforward and advantageous (in that it simplifies statistical analysis). I show that –on the contrary– reducing data to sufficient statistics can change a computationally tractable estimation problem into an intractable one. I discuss connections with recent work in theoretical computer science, and implications for some techniques to estimate graphical models.	cobham's thesis;computation;graphical model;preprocessor;statistical model;theoretical computer science	Andrea Montanari	2014	CoRR		econometrics;computer science;data mining;computational statistics;statistics	ML	26.882736728492105	-31.32938644932719	108521
ece2a11ba766ab47df2395081aaa03ccfd3fe729	tsd based framework for mining the induction rules	alligator s egg incubation;natural computing;temperature dependent sex determination;data classification	Sex determination mainly encompasses two aspects: genotypic sex determination (GSD) and temperature-dependent sex determination (TSD). Genotypic sex determination performs its task by observing the presence of sex chromosomes. In many reptiles sex determination is greatly influenced by the environmental conditions such as temperature of the nest, weight and size of eggs. A nature vailable online 4 December 2013 eywords: atural computing lligator’s egg incubation emperature dependent sex determination ata classification inspired algorithm which mimics the mechanism of temperature dependent sex determination (TSD) has been introduced for mining the classification rules from datasets. A comparison of proposed TSD algorithm with other well known rule induction algorithms like PRISM, C4.5, 1-R, CN2, and NN has been evaluated on some bench mark datasets. © 2013 Elsevier B.V. All rights reserved.	c4.5 algorithm;prism (surveillance program);rule induction;tsd	Subhash Chandra Pandey;Gora Chand Nandi	2014	J. Comput. Science	10.1016/j.jocs.2013.11.005	natural computing;computer science;machine learning	AI	35.85511847577864	-35.6542220661857	108539
5a3d5994089ae7941360678a0306f240a941760d	stochastic gradient methods for principled estimation with large datasets		14.	gradient	Panos Toulis;Edoardo M. Airoldi	2016		10.1201/b19567-20	data mining;machine learning;artificial intelligence;computer science	Vision	26.18412832720692	-31.554467085710744	109078
e50677bf441fac0b452d97e4d644809308f88cc2	visualizing time-varying particle flows with diffusion geometry		The tasks of identifying separation structures and clusters in flow data are fundamental to flow visualization. Significant work has been devoted to these tasks in flow represented by vector fields, but there are unique challenges in addressing these tasks for time-varying particle data. The unstructured nature of particle data, nonuniform and sparse sampling, and the inability to access arbitrary particles in space-time make it difficult to define separation and clustering for particle data. We observe that weaker notions of separation and clustering through continuous measures of these structures are meaningful when coupled with user exploration. We achieve this goal by defining a measure of particle similarity between pairs of particles. More specifically, separation occurs when spatially-localized particles are dissimilar, while clustering is characterized by sets of particles that are similar to one another. To be robust to imperfections in sampling we use diffusion geometry to compute particle similarity. Diffusion geometry is parameterized by a scale that allows a user to explore separation and clustering in a continuous manner. We illustrate the benefits of our technique on a variety of 2D and 3D flow datasets, from particles integrated in fluid simulations based on time-varying vector fields, to particle-based simulations in astrophysics.	cluster analysis;particle swarm optimization;sampling (signal processing);simulation;sparse matrix	Matthew Berger;Joshua A. Levine	2017	CoRR		vector field;computer science;cluster analysis;sampling (statistics);parameterized complexity;flow (psychology);cluster (physics);flow visualization;geometry;particle	Visualization	33.260793350819064	-37.57026234287784	109091
38d750b29c69296f4edb84ae26510c63aa13e8fd	sensorimotor learning of sound localization from an auditory evoked behavior	unsupervised learning;robot sensing systems;azimuth;laplace equation;manifolds;sound localization;acoustic generators;acoustic signal processing;online learning;auditory evoked potentials;laplace equations;ear;vectors;estimation;sensor placement;azimuthal sources direction auditory evoked behavior self supervised sensorimotor learning sound source localization simulated listener auditorimotor map sensorimotor experience auditory space azimuthal direction estimate nonlinear dimensionality reduction sensorimotor data online learning;sound source localization;unsupervised learning acoustic generators acoustic signal processing auditory evoked potentials sensor placement;dimensional reduction;manifolds robot sensing systems estimation azimuth vectors ear laplace equations	A new method for self-supervised sensorimotor learning of sound source localization is presented, that allows a simulated listener to learn online an auditorimotor map from the sensorimotor experience provided by an auditory evoked behavior. The map represents the auditory space and is used to estimate the azimuthal direction of sound sources. The learning mainly consists in non-linear dimensionality reduction of sensorimotor data. Our results show that an auditorimotor map can be learned, both from real and simulated data, and that the online learning leads to accurate estimations of azimuthal sources direction.	binaural beats;covox speech thing;nonlinear dimensionality reduction;online machine learning;piaget's theory of cognitive development	Mathieu Bernard;Patrick Pirim;Alain de Cheveigné;Bruno Gas	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6224972	unsupervised learning;computer vision;estimation;speech recognition;acoustics;sound localization;manifold;computer science;azimuth;acoustic source localization;statistics;laplace's equation	Robotics	33.863597621223995	-36.68769903570076	109282
8e597460557d44de07ec570738cd2b42cdcc2580	sparse greedy gaussian process regression	sample size;approximation error;gaussian process regression;conference paper;stopping criterion;gaussian process;large scale problem	Peter Bartlett RSISE Australian National University Canberra, ACT, 0200 Peter.Bartlett@anu.edu.au We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n2m), storage is O(nm), the cost for prediction is 0 ( n) and the cost to compute confidence bounds is O(nm), where n «: m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems.	approximation algorithm;approximation error;bartlett's bisection theorem;bartlett's method;gaussian process;greedy algorithm;image scaling;kriging;requirement;sparse matrix	Alexander J. Smola;Peter L. Bartlett	2000			gaussian random field;sample size determination;econometrics;mathematical optimization;approximation error;gaussian process;mathematics;kriging;statistics	ML	26.03638786369106	-24.125372802779694	109285
271d9fa1a3a4691582777eaf080b87d2b8117412	designing optimal sequential experiments for a bayesian classifier	experimental design;optimisation;bayesian classifier;computational complexity pattern classification bayes methods design of experiments optimisation;bayes methods;logistic model;design optimization;decision boundary uncertainty optimal sequential experiment design bayesian classifier parameter function variance approximation minimization computationally expensive discrete approximation logistic model;design of experiments;discrete approximation;bayesian classifiers;machine learning;computational complexity;pattern classification;bayesian methods design for experiments uncertainty cost function computational efficiency design methodology testing logistics performance analysis humans	As computing power has grown, the trend in experimental design has been from techniques requiring little computation towards techniques providing better, more general results at the cost of additional computation. This paper continues this trend presenting three new methods for designing experiments. A summary of previous work in experimental design is provided and used to show how these new methods generalize previous criteria and provide a more accurate analysis than prior methods. The first method generates experimental designs by maximizing the uncertainty of the experiment's result, while the remaining two methods minimize an approximation of the variance of a function of the parameters. The third method uses a computationally expensive discrete approximation to determine the variance. The methods are tested and compared using the logistic model and a Bayesian classifier. The results show that at the expense of greater computation, experimental designs more effective at reducing the uncertainty of the decision boundary of the Bayesian classifier can be generated.	naive bayes classifier	Robert Davis;Armand Prieditis	1999	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.754585	econometrics;naive bayes classifier;computer science;machine learning;mathematics;design of experiments;statistics	Vision	25.1518092577283	-24.941518814770536	109299
70450753646398013bd7f94518a05821ba403ad2	stochastic discrete clenshaw-curtis quadrature		The partition function is fundamental for probabilistic graphical models—it is required for inference, parameter estimation, and model selection. Evaluating this function corresponds to discrete integration, namely a weighted sum over an exponentially large set. This task quickly becomes intractable as the dimensionality of the problem increases. We propose an approximation scheme that, for any discrete graphical model whose parameter vector has bounded norm, estimates the partition function with arbitrarily small error. Our algorithm relies on a near minimax optimal polynomial approximation to the potential function and a Clenshaw-Curtis style quadrature. Furthermore, we show that this algorithm can be randomized to split the computation into a high-complexity part and a low-complexity part, where the latter may be carried out on small computational devices. Experiments confirm that the new randomized algorithm is highly accurate if the parameter norm is small, and is otherwise comparable to methods with unbounded error.	approximation;clenshaw–curtis quadrature;computation;coupling (computer programming);estimation theory;experiment;graphical model;minimax;model selection;multi-core processor;numerical integration;partition function (mathematics);polynomial;randomized algorithm;weight function	Nico Piatkowski;Katharina Morik	2016			mathematical optimization;combinatorics;discrete mathematics;machine learning;mathematics;statistics	ML	27.176685706741637	-28.43879218529235	109316
065b3c4d20b8af68e76b2f61b1eba247a8c21602	manifold representations for state estimation in contact manipulation		We investigate the problem of using contact sensors to estimate the configuration of an object during manipulation. Contact sensing is very discriminative by nature and, therefore, the set of object configurations that a sensor constitutes a lower-dimensional manifold in the state space of the object. This causes conventional state estimation methods, such as particle filters, to perform poorly during periods of contact. The manifold particle filter addresses this problem by sampling particles directly from the contact manifold. When it exists, we can sample these particles from an analytic representation of the contact manifold. We present two alternative sample-based contact manifold representations that make no assumptions about the object-hand geometry: rejection sampling and trajectory rollouts. The rejection sampling representation distributes uniformly in the space surrounding the manifold, while the trajectory rollout representation concentrates samples on the regions of the manifold that we are most likely to encounter during execution. We discuss theoretical considerations behind these three representations and compare their performance in an extensive suite of simulation experiments. We show that all three representations enable the manifold particle filter to outperform the conventional particle filter. Additionally, we show that the trajectory rollout representation performs similarly to the analytic method despite its relative simplicity.	analytic signal;experiment;hand geometry;particle filter;rejection sampling;sampling (signal processing);sensor;simulation;state space	Michael C. Koval;Nancy S. Pollard;Siddhartha S. Srinivasa	2013		10.1007/978-3-319-28872-7_22	computer vision;topology;geometry	Robotics	39.05686885280179	-26.235143984673797	109593
c9de8fd42a5a4d5beaf19526535c5b3cd36b16dd	danco: dimensionality from angle and norm concentration		In the last decades the estimation of the intrinsic dimen-sionality of a dataset has gained considerable importance. Despite the great deal of research work devoted to this task, most of the proposed solutions prove to be unreliable when the intrinsic dimensionality of the input dataset is high and the manifold where the points lie is nonlin-early embedded in a higher dimensional space. In this paper we propose a novel robust intrinsic dimensionality estimator that exploits the twofold complementary information conveyed both by the normalized nearest neighbor distances and by the angles computed on couples of neighboring points, providing also closed-forms for the Kullback-Leibler divergences of the respective distributions. Experiments performed on both synthetic and real datasets highlight the robustness and the effectiveness of the proposed algorithm when compared to state of the art methodologies.	embedded system;estimation of distribution algorithm;kullback–leibler divergence;nonlinear system;portable document format;synthetic intelligence	Claudio Ceruti;Simone Bassis;Alessandro Rozza;Gabriele Lombardi;Elena Casiraghi;Paola Campadelli	2012	CoRR		machine learning;data mining;mathematics;statistics	ML	28.334965421380556	-37.29981234223982	109786
a1d5da6054ffa957e27ec6ef18486d26ceff6d15	tensor decomposition for signal processing and machine learning	tensile stress;tutorials;matrix decomposition;signal processing;optimization;signal processing algorithms	"""Tensors or <italic>multiway arrays</italic> are functions of three or more indices <inline-formula> <tex-math notation=""""LaTeX"""">$(i,j,k,\ldots)$</tex-math></inline-formula>—similar to matrices (two-way arrays), which are functions of two indices <inline-formula><tex-math notation=""""LaTeX"""">$(r,c)$</tex-math></inline-formula> for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth <italic>and depth</italic> that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning."""	algorithm;collaborative filtering;confluence;data mining;gradient;machine learning;mathematical optimization;multilinear subspace learning;multiway branch;signal processing;source separation;statistical model;topic model	Nikos D. Sidiropoulos;Lieven De Lathauwer;Xiao Fu;Kejun Huang;Evangelos E. Papalexakis;Christos Faloutsos	2017	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2690524	computer vision;computer science;theoretical computer science;machine learning;signal processing;mathematics;stress;matrix decomposition;statistics	ML	30.782140102081495	-31.638241355606933	109843
63c4af141496edc0ab1b94a837bbc9878a0c1bf5	dynamic textures clustering using a hierarchical pitman-yor process mixture of dirichlet distributions	dynamic textures mixture models pitman yor dirichlet distribution variational inference;variational techniques bayes methods image texture mixture models pattern clustering statistical distributions;mixture models modeling bayes methods video sequences inference algorithms indexes dynamics;variational bayes approach dynamic texture clustering hierarchical pitman yor process mixture dirichlet distributions hpy process mixture dirichlet process mixtures infinite dirichlet mixture models	This paper proposes a hierarchical Pitman-Yor (HPY) process mixture of Dirichlet distributions. It can be viewed as an extension of our previous works that have considered Dirich-let process mixtures of Dirichlet distributions (i.e. infinite Dirichlet mixture models). The proposed model is learned via a variational Bayes approach and applied to the challenging problem of dynamic textures clustering.	cluster analysis;mixture model;variational principle	Wentao Fan;Nizar Bouguila	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7350807	latent dirichlet allocation;econometrics;pattern recognition;mixture model;mathematics;statistics;hierarchical dirichlet process	Vision	32.229377035509316	-36.553262908895405	109844
bbebd9e0599f28f9830679a563459265799c5037	nonparametric multiple change point estimation in highly dependent time series	unsupervised learning;stationary ergodic time series;change point analysis;consistency	Given a heterogeneous time-series sample, the objective is to find points in time, called change points, where the probability distribution generating the data has changed. The data are assumed to have been generated by arbitrary unknown stationary ergodic distributions. No modelling, independence or mixing assumptions are made. A novel, computationally efficient, nonparametric method is proposed, and is shown to be asymptotically consistent in this general framework. The theoretical results are complemented with experimental evaluations.	time series	Azadeh Khaleghi;Daniil Ryabko	2016	Theor. Comput. Sci.	10.1016/j.tcs.2015.10.041	unsupervised learning;econometrics;mathematical optimization;computer science;stationary ergodic process;mathematics;consistency;statistics	HCI	30.49325744327357	-24.98118339126504	109921
5483f2eb4b830c04ce0c674ffd4909e37ec25fae	carob moth, ectomyelois ceratoniae, detection in pomegranate using visible/near infrared spectroscopy	pomegranate;carob moth;simca;pls da;vis nir spectroscopy	In pomegranate, carob moth infestation is a postharvest problem. In most cases, damage develops inside the fruit i.e. without affecting the rind. Consequently, visual inspection is not adequate for identification of carob moth in pomegranate fruit because of the lack of external symptoms. In this study, the feasibility of using VIS/NIR spectroscopy as to detect carob moth infestation in pomegranate fruits is demonstrated. The samples included intact as well as infected pomegranate fruits at four different stages of maturity. Discriminant analysis of the samples was performed by soft independent modeling of class analogy (SIMCA) and partial least squares discriminant analysis (PLS-DA). The results showed that in all sample groups when the samples were classified by PLS-DA, the high values were found in comparison with the SIMCA models. All the discriminate analyses were accomplished for three different sample sets: standard (only the samples handpicked at standard harvest time consist of stage 1, stage 2 and stage 3), last (only the handpicked at last harvest time including stage 3 and stage 4) and combined (all four studied maturity stages together). The total discriminant power of PLS-DA classes was approximately 88%, 90%, and 86% for standard, last and combined sample sets, respectively. The comparison among different sample groups indicated that the last sample group was predicted with the best prediction accuracy followed by standard and then combined sample group. 2016 Elsevier B.V. All rights reserved.	capability maturity model;linear discriminant analysis;partial least squares regression;performance;sensor;soft independent modelling of class analogies;statistical classification;visual instruction set;visual inspection	Rasool Khodabakhshian;Bagher Emadi;Mehdi Khojastehpour;Mahmood Reza Golzarian	2016	Computers and Electronics in Agriculture	10.1016/j.compag.2016.09.006	botany;engineering	ML	34.79115025378459	-30.73946279255849	109999
9165151dc212c1b1f03368b4e5b3b558f92882c9	deep convolutional factor analyser for multivariate time series modeling	analytical models;convolution;training;hidden markov models;time series analysis;gaussian distribution;data models	Deep generative models can perform dramatically better than traditional graphical models in a number of machine learning tasks. However, training such models remains challenging because their latent variables typically do not have an analytical posterior distribution, largely due to the nonlinear activation nodes. We present a deep convolutional factor analyser (DCFA) for multivariate time series modeling. Our network is constructed in a way that bottom layer nodes are independent. Through a process of up-sampling and convolution, higher layer nodes gain more temporal dependency. Our model can thus give a time series different representations at different depths. DCFA only consists of linear Gaussian nodes. Therefore, the posterior distributions of latent variables are also Gaussian and can be estimated easily using standard variational Bayes algorithm. We show that even without nonlinearity the proposed deep model can achieve state-of-the-art results in anomaly detection, classification and clustering using both synthetic and real-world datasets.	activation function;algorithm;anomaly detection;calculus of variations;cluster analysis;convolution;deep learning;feature learning;graphical model;latent variable;machine learning;nonlinear system;relevance;sampling (signal processing);synthetic intelligence;time series;unsupervised learning;variational principle;vergence	Chao Yuan;Amit Chakraborty	2016	2016 IEEE 16th International Conference on Data Mining (ICDM)	10.1109/ICDM.2016.0180	normal distribution;data modeling;computer science;machine learning;time series;pattern recognition;data mining;mathematics;convolution;hidden markov model;statistics	ML	29.11204286972411	-34.33661566304913	110199
b59005f86864b13ddb545e45d7d3550500e4bc62	irrelevance and independence relations in quasi-bayesian networks	bayesian network;probability distribution;graphical model;convex set	This paper analyzes irrelevance and independence re­ lations in graphical models associated with convex sets of probability distributions (called Quasi-Bayesian networks) . The basic question in Quasi-Bayesian networks is, How can irrelevance/independence rela­ tions in Quasi-Bayesian networks be detected, enforced and exploited? This paper addresses these questions through Walley's definitions of irrelevance and inde­ pendence. Novel algorithms and results are presented for inferences with the so-called natural extensions us­ ing fractional linear programming, and the properties of the so-called type-1 extensions are clarified through a new generalization of d-separation.	algorithm;bayesian network;graphical model;graphical user interface;linear programming;relevance	Fábio Gagliardi Cozman	1998			probability distribution;econometrics;combinatorics;computer science;machine learning;bayesian network;mathematics;graphical model;convex set;statistics	AI	25.62820230862244	-27.34658044786485	110232
12f09631989a3f3260f819ab5d9b2c46d731a9b0	bethe learning of graphical models via map decoding		Many machine learning tasks require fitting probabilistic models over structured objects, such as pixel grids, matchings, and graph edges. Maximum likelihood estimation (MLE) for such domains is challenging due to the intractability of computing partition functions. One can resort to approximate marginal inference in conjunction with gradient descent, but such algorithms require careful tuning. Alternatively, in frameworks such as the structured support vector machine (SVM-Struct), discriminative functions are learned by iteratively applying efficient maximum a posteriori (MAP) decoders. We introduce MLE-Struct, a method for learning discrete exponential family models using the Bethe approximation to the partition function. Remarkably, this problem can also be reduced to iterative (MAP) decoding. This connection emerges by combining the Bethe approximation with the Frank-Wolfe (FW) algorithm on a convex dual objective, which circumvents the intractable partition function. Our method can learn both generative and conditional models and is substantially faster and easier to implement than existing MLE approaches while still relying on the same black-box interface to MAP decoding as SVM-Struct. We perform competitively on problems in denoising, segmentation, matching, and new datasets of roommate assignments and news and financial time series.	approximation algorithm;black box;frank–wolfe algorithm;gradient descent;graphical model;iterative method;machine learning;marginal model;matching (graph theory);noise reduction;partition function (mathematics);pixel;struct (c programming language);structured support vector machine;time complexity;time series	Kui Tang;Nicholas Ruozzi;David Belanger;Tony Jebara	2016			computer science;artificial intelligence;theoretical computer science;machine learning	ML	24.66121639313997	-32.45790313846712	110240
5a2be735da31f88fb70a3235b9ac61ad295b164c	neural classification of lamb wave ultrasonic weld testing signals using wavelet coefficients	aluminium;neural nets;ultrasonic testing;ultrasonic materials testing;signal analysis;image classification;acoustic signal processing;lamb wave;wavelet transforms;welding acoustic testing signal analysis neural networks aluminum signal processing automatic testing system testing nondestructive testing wavelet transforms;structural testing;wavelet transform;feature extraction;test methods;al neural classification lamb wave ultrasonic weld testing signals wavelet coefficients wavelet transform inspection signals neural network em acoustic transducer metallic welds al weld feature extraction us testing classification process neural classifier automated system feasibility efficiency;aluminium surface acoustic waves ultrasonic materials testing wavelet transforms neural nets image classification acoustic signal processing feature extraction;surface acoustic waves;neural network	This paper presents an ultrasonic nondestructive weld testing method based on the wavelet transform (WT) of inspection signals and their classification by a neural network (NN). The use of Lamb waves generated by an electromagnetic acoustic transducer (EMAT) as a probe allows us to test metallic welds. In this work, the case of an aluminum weld is treated. The feature extraction is made by using a method of analysis based on the WT of the ultrasonic testing signals; a classification process of the features based on a neural classifier to interpret the results in terms of weld quality concludes the process. The aim of this complete process of analysis and classification of the testing ultrasonic signals is to lead to an automated system of weld or structure testing. Results of real-world ultrasonic Lamb wave signal analysis and classifications for an aluminum weld are presented; these demonstrate the feasibility and efficiency of the proposed method.	acoustic cryptanalysis;artificial neural network;coefficient;electromagnetic acoustic transducer;feature extraction;signal processing;wavelet transform	Sylvie Legendre;Daniel Massicotte;Jacques Goyette;Tapan K. Bose	2001	IEEE Trans. Instrumentation and Measurement	10.1109/19.930439	electronic engineering;speech recognition;acoustics;computer science;engineering;machine learning;artificial neural network;wavelet transform	SE	37.78937891166933	-32.84231128004038	110629
39a62eb9b0a44740743841658653a38d7149c93e	diagnosis of incipient faults in weak nonlinear analog circuits	volterra series;nonlinear circuits;hidden markov model hmm;fractional correlation;fault diagnosis	Aiming at the problem to diagnose incipient faults in weak nonlinear analog circuits, an approach is presented in this paper. The approach calculates the fractional Volterra correlation functions beforehand. The next step is to use the fractional Volterra correlation functions and different angle parameters of the fractional wavelet packet transform (FRWPT) to extract the fault signatures. Meanwhile, the computational complexity is analyzed. Then the variables of the fault signatures are constructed, which are used to form the observation sequences of the hidden Markov model (HMM). HMM is used to accomplish the fault diagnosis. The simulations show that the presented method can significantly improve the incipient fault diagnosis capability.	integrated circuit	Yibing Shi;Yong Deng;Wei Zhang	2013	CSSP	10.1007/s00034-013-9589-0	electronic engineering;machine learning;control theory;mathematics	EDA	37.195985655974866	-31.194377936204848	111004
e37bb83b713e3b83706dfc0ee807bb497ed3873f	adaptive mcmc with bayesian optimization		This paper proposes a new randomized strategy for adaptive MCMC using Bayesian optimization. This approach applies to nondifferentiable objective functions and trades off exploration and exploitation to reduce the number of potentially costly objective function evaluations. We demonstrate the strategy in the complex setting of sampling from constrained, discrete and densely connected probabilistic graphical models where, for each variation of the problem, one needs to adjust the parameters of the proposal mechanism automatically to ensure efficient mixing of the Markov chains.	bayesian optimization;computation;graphical model;hamiltonian (quantum mechanics);hybrid monte carlo;image scaling;ising model;loss function;markov chain monte carlo;mathematical optimization;monte carlo method;optimization problem;performance tuning;randomized algorithm;restricted boltzmann machine;sampling (signal processing);test data;eric	Nimalan Mahendran;Ziyu Wang;Firas Hamze;Nando de Freitas	2012			mathematical optimization;bayesian optimization;artificial intelligence;machine learning;sampling (statistics);computer science;markov chain monte carlo;markov chain;graphical model	ML	26.94002040044848	-28.710053978222803	111914
20080375d2459f4d0c250b8f230cd0f7b23f6d1e	piecewise linear discriminant functions and classification errors for multiclass problems (corresp.)	piecewise linear;probability;piecewise linear techniques;integral equations;contracts;discriminant function;marine vehicles;vectors;pattern classification;classification error;gaussian distribution;monte carlo methods;covariance matrix	First Page of the Article	discriminant;multiclass classification	Keinosuke Fukunaga;David R. Olsen	1970	IEEE Trans. Information Theory	10.1109/TIT.1970.1054392	normal distribution;covariance matrix;mathematical optimization;piecewise linear function;pattern recognition;probability;optimal discriminant analysis;discriminant function analysis;mathematics;linear discriminant analysis;integral equation;statistics;monte carlo method	Theory	30.954243588315034	-27.78758715780781	112064
1ec2e9cd5d94b75f5e15cbf6201950d22724aa97	structured gaussian processes with twin multiple kernel learning		Vanilla Gaussian processes (GPs) have prohibitive computational needs for very large data sets. To overcome this difficulty, special structures in the covariance matrix, if exist, should be exploited using decomposition methods such as the Kronecker product. In this paper, we integrated the Kronecker decomposition approach into a multiple kernel learning (MKL) framework for GP regression. We first formulated a regression algorithm with the Kronecker decomposition of structured kernels for spatiotemporal modeling to learn the contribution of spatial and temporal features as well as learning a model for out-of-sample prediction. We then evaluated the performance of our proposed computational framework, namely, structured GPs with twin MKL, on two different real data sets to show its efficiency and effectiveness. MKL helped us extract relative importance of input features by assigning weights to kernels calculated on different subsets of temporal and spatial features.	algorithm;computation;gaussian process;kernel (operating system);math kernel library;multiple kernel learning;normal statistical distribution;weight	Çiğdem Ak;Önder Ergönül;Mehmet Gönen;Ichiro Takeuchi	2018			machine learning;artificial intelligence;gaussian process;multiple kernel learning;computer science	ML	28.418426285544616	-34.47348886783533	112144
39749ff7446e567eb3100f12cc0a49263f930798	dynamic bayesian networks with deterministic latent tables	latent variable;probabilistic model;dynamic bayesian network;hidden variables;conditional probability table	The application of latent/hidden variable Dynamic Bayesian Networks is constrained by the complexity of marginalising over latent variables. For this reason either small latent dimensions or Gaussian latent conditional tables linearly dependent on past states are typically considered in order that inference is tractable. We suggest an alternative approach in which the latent variables are modelled using deterministic conditional probability tables. This specialisation has the advantage of tractable inference even for highly complex non-linear/non-Gaussian visible conditional probability tables. This approach enables the consideration of highly complex latent dynamics whilst retaining the benefits of a tractable probabilistic model.	cobham's thesis;dynamic bayesian network;hash table;hidden variable theory;latent variable;nonlinear system;statistical model;system dynamics;while	David Barber	2002			latent class model;latent dirichlet allocation;latent variable;conditional probability distribution;statistical model;econometrics;computer science;machine learning;bayesian network;mathematics;probabilistic latent semantic analysis;local independence;latent variable model;hidden variable theory;dynamic bayesian network;statistics;conditional probability table	ML	27.18880424071982	-29.70508229564949	112513
305d20df1a440a38a34c752572c56852bd364836	regularized gaussian belief propagation	belief propagation;approximate inference;gaussian distributions;regularization;convergence	Belief propagation (BP) has been applied in a variety of inference problems as an approximation tool. BP does not necessarily converge in loopy graphs, and even if it does, is not guaranteed to provide exact inference. Even so, BP is useful in many applications due to its computational tractability. In this article, we investigate a regularized BP scheme by focusing on loopy Markov graphs (MGs) induced by a multivariate Gaussian distribution in canonical form. There is a rich literature surrounding BP on Gaussian MGs (labelled Gaussian belief propagation or GaBP), and this is known to experience the same problems as general BP on graphs. GaBP is known to provide the correct marginal means if it converges (this is not guaranteed), but it does not provide the exact marginal precisions. We show that our adjusted BP will always converge, with sufficient tuning, while maintaining the exact marginal means. As a further contribution we show, in an empirical study, that our GaBP variant can accelerate GaBP and compares well with other GaBP-type competitors in terms of convergence speed and accuracy of approximate marginal precisions. These improvements suggest that the principle of regularized BP should be investigated in other inference problems. The selection of the degree of regularization is addressed through the use of two heuristics. A by-product of GaBP is that it can be used to solve linear systems of equations; the same is true for our variant and we make an empirical comparison with the conjugate gradient method.	belief propagation;software propagation	Francois Kamper;Johan A. du Preez;Sarel Steel;Stephan N Wagner	2018	Statistics and Computing	10.1007/s11222-017-9753-7	mathematical optimization;machine learning;mathematics;statistics	ML	26.297273240398702	-30.38725402392525	112535
0503ced8e92c62719a00c7a051ea095dc352dac5	scalable moment-based inference for latent dirichlet allocation		Topic models such as Latent Dirichlet Allocation have been useful text analysis methods of wide interest. Recently, moment-based inference with provable performance has been proposed for topic models. Compared with inference algorithms that approximate the maximum likelihood objective, moment-based inference has theoretical guarantee in recovering model parameters. One such inference method is tensor orthogonal decomposition, which requires only mild assumptions for exact recovery of topics. However, it suffers from scalability issue due to creation of dense, high-dimensional tensors. In this work, we propose a speedup technique by leveraging the special structure of the tensors. It is efficient in both time and space, and only requires scanning the corpus twice. It improves over the state-of-the-art inference algorithm by one to three orders of magnitude, while preserving equal inference ability.	approximation algorithm;latent dirichlet allocation;provable security;scalability;speedup;text corpus;text mining	Chi Wang;Xueqing Liu;Yanglei Song;Jiawei Han	2014		10.1007/978-3-662-44845-8_19	machine learning;pattern recognition;data mining;mathematics;statistics	ML	25.869763355952923	-33.31555550165015	112588
bc8c876646e8f22285f44d5d5bddbe4b9d982dfd	prediction to the weak electrical signal in chrysanthemum by rbf neural networks	automatic control;agricultural production;dendranthema morifolium;maximum amplitude;plastic lookum weak electrical signal prediction chrysanthemum dendranthema morifolium time series gaussian radial base function neural network intelligent radial base function forecast system wavelet soft threshold de noising plant electrical signal forecasting intelligent automatic control system adaptive characteristic energy saving agricultural production greenhouse;greenhouses;intelligent radial base function forecast system;chrysanthemum weak electrical signal rbf neural network intelligent automatic control;gaussian processes;low frequency;adaptive control;intelligent automatic control;time series;adaptive characteristic;intelligent automatic control system;radial basis function networks;weak electrical signal;neural networks load forecasting intelligent systems frequency delay effects propagation delay timing intelligent control automatic control programmable control;forecasting theory;prediction theory;rbf neural network;weak electrical signal prediction;wavelet soft threshold de noising;radial base function;chrysanthemum;agriculture;greenhouse;neurocontrollers;gaussian radial base function neural network;energy saving;time series adaptive control agriculture forecasting theory gaussian processes greenhouses neurocontrollers prediction theory radial basis function networks signal denoising;signal denoising;plant electrical signal forecasting;plastic lookum	Taking electrical signals in the chrysanthemum (Dendranthema morifolium) as the time series and using the Gaussian radial base function (RBF) and a delayed input window chosen at 50, an intelligent RBF forecast system is set up to forecast signals by the wavelet soft-threshold de-noised backward. It is obvious that the electrical signal in chrysanthemum is a sort of weak, unstable and low frequency signals. There is the maximum amplitude at 1093.44 muV, minimum -605.35 muV, average value -11.94 muV; and below 0.3 Hz at frequency in the chrysanthemum respectively. A result shows that it is feasible to forecast plant electrical signals for the timing by using of the RBF neural network. The forecast data can be used as the important preferences for the intelligent automatic control system based on the adaptive characteristic of plants to achieve the energy saving on the agricultural production in the greenhouse and/or the plastic lookum.	artificial neural network;automatic control;control system;control theory;radial (radio);radial basis function;time series;wavelet	Jinli Ding;Miao Wang;Lanzhou Wang;Qiao Li	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.565	control engineering;engineering;artificial intelligence;machine learning	Robotics	37.41694686677256	-31.566790164026056	112641
d103f94f48ba47fee63b87116700d7644117317a	a unified particle-optimization framework for scalable bayesian sampling		There has been recent interest in developing scalable Bayesian sampling methods such as stochastic gradient MCMC (SG-MCMC) and Stein variational gradient descent (SVGD) for big-data analysis. A standard SG-MCMC algorithm simulates samples from a discrete-time Markov chain to approximate a target distribution, thus samples could be highly correlated, an undesired property for SG-MCMC. In contrary, SVGD directly optimizes a set of particles to approximate a target distribution, and thus is able to obtain good approximations with relatively much fewer samples. In this paper, we propose a principle particle-optimization framework based on Wasserstein gradient flows to unify SG-MCMC and SVGD, and to allow new algorithms to be developed. Our framework interprets SG-MCMC as particle optimization on the space of probability measures, revealing a strong connection between SG-MCMC and SVGD. The key component of our framework is several particle-approximate techniques to efficiently solve the original partial differential equations on the space of probability measures. Extensive experiments on both synthetic data and deep neural networks demonstrate the effectiveness and efficiency of our framework for scalable Bayesian sampling.		Changyou Chen;Ruiyi Zhang;Wenlin Wang;Bai Li;Liqun Chen	2018			mathematical optimization;mathematics;artificial intelligence;machine learning;artificial neural network;sampling (statistics);probability measure;markov chain monte carlo;gradient descent;synthetic data;markov chain;bayesian probability	ML	25.132124228828097	-30.52167671254384	112888
b52b4d46aa2ff7775ec36fc0f3d3fcc02ea237f1	learning factor graphs in polynomial time and sample complexity	structure learning;bayesian network;factor graph;learning model;probabilistic graphical model;maximum likelihood estimate;polynomial time;markov network;graphical model;network structure;parameter estimation;sample complexity	We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion.1	algorithm;bayesian network;estimation theory;factor graph;fault tolerance;graphical model;graphical user interface;hidden markov model;kullback–leibler divergence;markov chain;markov random field;p (complexity);polynomial;sample complexity;time complexity	Pieter Abbeel;Daphne Koller;Andrew Y. Ng	2006	Journal of Machine Learning Research		time complexity;variable elimination;combinatorics;computer science;machine learning;factor graph;bayesian network;mathematics;maximum likelihood;graphical model;estimation theory;computational learning theory;statistics	ML	25.916747303172613	-29.404156129643535	112904
7f0e9a827f81f844a09ccdccf29d64b885282708	blind source separation for non-stationary mixing	blind source separation;markov process;article	Blind source separation attempts to recover independent sources which have been linearly mixed to produce observations. We consider blind source separation with non-stationary mixing, but stationary sources. The linear mixing of the independent sources is modelled as evolving according to a Markov process, and a method for tracking the mixing and simultaneously inferring the sources is presented. Observational noise is included in the model. The technique may be used for online filtering or retrospective smoothing. The tracking of mixtures of temporally correlated is examined and sampling from within a sliding window is shown to be effective for destroying temporal correlations. The method is illustrated with numerical examples.	blind signal separation;source separation;stationary process	Richard M. Everson;Stephen J. Roberts	2000	VLSI Signal Processing	10.1023/A:1008183014430	econometrics;computer science;machine learning;mathematics;blind signal separation;markov process;statistics	ML	34.78756384111235	-26.923373467374788	113053
8d16b2830cb37eb5e573d8edf8e716c41cbed178	on local linear approximations to diffusion processes		Diffusion models have been used extensively in many applications. These models, such as those used in the financial engineering, usually contain unknown parameters which we wish to determine. One way is to use the maximum likelihood method with discrete samplings to devise statistics for unknown parameters. In general, the maximum likelihood functions for diffusion models are not available, hence it is difficult to derive the exact maximum likelihood estimator MLE . There are many different approaches proposed by various authors over the past years, see, for example, the excellent books and Kutoyants 2004 , Liptser and Shiryayev 1977 , Kushner and Dupuis 2002 , and Prakasa Rao 1999 , and also the recent works by Aı̈t-Sahalia 1999 , 2004 , 2002 , and so forth. Shoji and Ozaki 1998; see also Shoji and Ozaki 1995 and Shoji and Ozaki 1997 proposed a simple local linear approximation. In this paper, among other things, we show that Shoji’s local linear Gaussian approximation indeed yields a good MLE.		X. L. Duan;Zhongmin Qian;Weian Zheng	2011	Int. J. Math. Mathematical Sciences	10.1155/2011/906846	econometrics;mathematical optimization;mathematics;statistics	ML	32.650577617762224	-26.62893355392107	113304
bb360bdebe84b014c3d3096c950a0ef3fe3dbd7e	bayes error estimation using parzen and k-nn procedures	bayes estimation;nearest neighbor searches;metodo estadistico;finite sample;kernel;bayes error estimation;parzen;nickel;statistical method;classification;regle decision;density functional theory;error analysis;density estimation;distance measurement;vecino mas cercano;artificial neural networks;estimacion bayes;computer aided software engineering;estimation erreur;shape;estimation;error estimation;methode statistique;k nn;nonparametric error estimation;nearest neighbor;estimacion error;error analysis nearest neighbor searches kernel density functional theory shape pattern recognition multidimensional systems;pattern recognition;plus proche voisin;nearest neighbour;k nearest neighbor;reconnaissance forme;regla decision;reconocimiento patron;error estimate;clasificacion;parzen bayes error estimation finite sample k nn nearest neighbor nonparametric error estimation;multidimensional systems;decision rule;estimation bayes	The use of k nearest neighbor (k-NN) and Parzen density estimates to obtain estimates of the Bayes error is investigated under limited design set conditions. By drawing analogies between the k-NN and Parzen procedures, new procedures are suggested, and experimental results are given which indicate that these procedures yield a significant improvement over the conventional k-NN and Parzen procedures. We show that, by varying the decision threshold, many of the biases associated with the k-NN or Parzen density estimates may be compensated, and successful error estimation may be performed in spite of these biases. Experimental results are given which demonstrate the effect of kernel size and shape (Parzen), the size of k (k-NN), and the number of samples in the design set.	computer;control system;converge;convergence (action);eighty;electrical engineering;emoticon;error message;estimated;experiment;generalization (psychology);government agencies;k-nearest neighbors algorithm;kappa calculus;mean squared error;norm (social);normal statistical distribution;pattern recognition;single linkage cluster analysis;test case;test set;timation;usability;vii	Keinosuke Fukunaga;Donald M. Hummels	1987	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.1987.4767958	computer science;machine learning;pattern recognition;mathematics;k-nearest neighbors algorithm;artificial neural network;statistics	ML	32.70330536106611	-24.661061016164798	113591
554016ba8159d047f4e298037b6d2a9dbe0e36a9	polynomial-time algorithm for learning optimal bfs-consistent dynamic bayesian networks		Dynamic Bayesian networks (DBN) are powerful probabilistic representations that model stochastic processes. They consist of a prior network, representing the distribution over the initial variables, and a set of transition networks, representing the transition distribution between variables over time. It was shown that learning complex transition networks, considering both intraand inter-slice connections, is NP-hard. Therefore, the community has searched for the largest subclass of DBNs for which there is an efficient learning algorithm. We introduce a new polynomial-time algorithm for learning optimal DBNs consistent with a breadth-first search (BFS) order, named bcDBN. The proposed algorithm considers the set of networks such that each transition network has a bounded in-degree, allowing for p edges from past time slices (inter-slice connections) and k edges from the current time slice (intra-slice connections) consistent with the BFS order induced by the optimal tree-augmented network (tDBN). This approach increases exponentially, in the number of variables, the search space of the state-of-the-art tDBN algorithm. Concerning worst-case time complexity, given a Markov lag m, a set of n random variables ranging over r values, and a set of observations of N individuals over T time steps, the bcDBN algorithm is linear in N, T and m; polynomial in n and r; and exponential in p and k. We assess the bcDBN algorithm on simulated data against tDBN, revealing that it performs well throughout different experiments.	best, worst and average case;breadth-first search;cluster analysis;directed graph;dynamic bayesian network;expectation–maximization algorithm;experiment;f1 score;hidden markov model;hidden variable theory;markov chain;markov property;polynomial;preemption (computing);state transition table;stationary process;stochastic process;time complexity	Margarida Sousa;Alexandra M. Carvalho	2018	Entropy	10.3390/e20040274	time complexity;mathematical optimization;dynamic bayesian network;mathematics;ranging;stochastic process;bounded function;lag;random variable;algorithm;markov chain	ML	25.263378178708606	-28.3729737672448	113684
02e3a9e4b4ecaf78dece52eeca713e89e71885b7	infinite latent harmonic allocation: a nonparametric bayesian approach to multipitch analysis		Maximum a posteriori estimation (MAP) Point estimates Manually specified Manually specified Modest Classical Bayesian estimation Posterior distributions Manually specified Manually specified Good (1) Nonparametric Bayesian estimation Posterior distributions Posterior distributions ―――― Excellent (2) Hierarchical Posterior distributions ―――― Posterior distributions Excellent Parameters Complexity Hyperparameters Robustness Maximum likelihood estimation (ML) The values of F0s? How many F0s? How optimize prior distributions?		Kazuyoshi Yoshii;Masataka Goto	2010			mathematical optimization;machine learning;calculus;mathematics;statistics	ML	31.180254653998844	-25.44192817356355	114057
c7b82be246bc02f392dbf5b245adc90cce547ab9	testing problems with nuisance parameters: linear models under non-classical assumptions	nuisance parameter;indexation;linear model;analysis of covariance	Most testing problems involve a multidimensional parameter, only one component of which characterizes the hypotheses. In a mathematically strong sense, there are only two methods to cope with the remaining component, the so called nuisance parameter. Firstly, conditioning on a sufficient and complete statistic and, secondly, reduction by invariance. Both methods require strong assumptions on the underlying class of distributions. Therefore the local asymptotic approach is reviewed, which extends Neyman's theory ofC(α)-tests and yields, in a unified way, approximate optimal test statistics for smoothly indexed families. That device is applied to non-normal heteroscedastic linear models.	linear model	Ulrich Müller-Funk;Hermann Witting	1989	ZOR - Meth. & Mod. of OR	10.1007/BF01415513	nuisance variable;econometrics;mathematical optimization;analysis of covariance;nuisance parameter;linear model;mathematics;statistics	Theory	29.297962527911707	-24.562985577908805	114086
900e5256c097e30919615b379707e7af886507ff	a novel loo based two-stage method for automatic model identification of a class of nonlinear dynamic systems	linear systems;autoregressive moving average processes;nonlinear dynamical systems;computational modeling data models numerical models training complexity theory vectors mathematical model;nonlinear dynamical systems autoregressive moving average processes identification linear systems;identification;stepwise model selection algorithms loo based two stage method automatic model identification nonlinear dynamic systems linear systems model terms orthogonal least squares ols fast recursive algorithm fra stepwise forward methods model complexity model compactness leave one out criteria forward selection algorithms two stage algorithm	This paper investigates the construction of models for a class of nonlinear systems that can be represented by linear in parameter models. This is not a trivial problem, as there are many possible combinations of model terms and exhaustive search is not an option when the number of possible model terms is large. Most existing fast approaches such as orthogonal least squares (OLS), fast recursive algorithm (FRA) and their variants serve the purpose of fast selection. However, these stepwise forward methods are greedy approaches in general and the resultant models are not optimal. Further, they do not control the model complexity (i.e. automatically stop the model selection). The two stage algorithm may improve the compactness of models obtained from forward algorithms, again, it does not determine how many model terms are necessary. Recently, some cross validation based methods have been proposed for automatic model construction, based on leave-one-out (LOO) criteria and OLS or FRA, however the issues related to the forward selection algorithms still exist. Further, LOO based methods are computationally expensive as the model often has to be trained N times (N is the number of samples) for just only one evaluation of the LOO criterion. In this paper, a novel and fast two stage algorithm is proposed for automatic construction of linear in parameter models for a class of nonlinear systems using LOO criterion, overcoming the disadvantages of stepwise model selection algorithms and reducing the computational complexity in applying the LOO criteria. Two numerical examples are presented to confirm its effectiveness.	analysis of algorithms;brute-force search;computational complexity theory;dynamical system;greedy algorithm;lasso;linear least squares (mathematics);model selection;nonlinear system;numerical analysis;oracle flashback;ordinary least squares;recursion (computer science);refinement (computing);resultant;stepwise regression;system identification	Long Zhang;Kang Li;Er-Wei Bai	2013	52nd IEEE Conference on Decision and Control	10.1109/CDC.2013.6760549	identification;econometrics;mathematical optimization;computer science;machine learning;control theory;mathematics;linear system;statistics	Vision	30.082199333677142	-26.8917357369146	114154
5d5347b167015a2f76554c1fc7065b6ba09927e6	multiscale method based on spline regression for comparing multiple nonparametric curves		SiZer (SIgnificant ZERo crossing of the derivatives) is a powerful scale-space visualization technique for exploratory data analysis. In this paper a new version of SiZer based on regression spline is proposed for comparing multiple regression curves. The new SiZer is constructed on the basis of p-values for testing the equality of multiple regression functions at different locations and scales. Fiducial inference and regression spline are applied to gain the p-values. In addition, multiple testing adjustments are carried out to control the row-wise false discovery rate and family-wise error rate of the proposed SiZer, respectively. The new SiZer is more powerful even in the case of small sample size case due to the good properties of p-value and FDR control. Simulation results show that the new SiZer performs well compared with the existing SiZers. Finally, a real data example is carried out to illustrate its usage in applications.	cosmo-rs;fiducial marker;numerical analysis;performance;reed–solomon error correction;scale space;simulation;smoothing spline;spline (mathematics);zero crossing	Na Li;Xuhua Liu	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393375	computer science;mathematical optimization;multiple comparisons problem;linear regression;statistics;false discovery rate;nonparametric statistics;fiducial inference;sample size determination;spline (mathematics);exploratory data analysis	Robotics	29.559784233130294	-24.962230141247975	114425
0863cbbe919a409b09baec8cfefb5d403a595465	independently interpretable lasso: a new regularizer for sparse regression with uncorrelated variables		Sparse regularization such as l1 regularization is a quite powerful and widely used strategy for high dimensional learning problems. The effectiveness of sparse regularization has been supported practically and theoretically by several studies. However, one of the biggest issues in sparse regularization is that its performance is quite sensitive to correlations between features. Ordinary l1 regularization can select variables correlated with each other, which results in deterioration of not only its generalization error but also interpretability. In this paper, we propose a new regularization method, “Independently Interpretable Lasso” (IILasso). Our proposed regularizer suppresses selecting correlated variables, and thus each active variable independently affects the objective variable in the model. Hence, we can interpret regression coefficients intuitively and also improve the performance by avoiding overfitting. We analyze theoretical property of IILasso and show that the proposed method is much advantageous for its sign recovery and achieves almost minimax optimal convergence rate. Synthetic and real data analyses also indicate the effectiveness of IILasso. Proceedings of the 21 International Conference on Artificial Intelligence and Statistics (AISTATS) 2018, Lanzarote, Spain. PMLR: Volume 84. Copyright 2018 by the author(s).	algorithm;artificial intelligence;coefficient;coordinate descent;dhrystone;generalization error;global optimization;lasso;local optimum;manifold regularization;matrix regularization;minimax;overfitting;presto;rate of convergence;sparse matrix;synthetic data	Masaaki Takada;Taiji Suzuki;Hironori Fujisawa	2018			mathematics;machine learning;lasso (statistics);rate of convergence;mathematical optimization;overfitting;linear regression;regularization perspectives on support vector machines;minimax;interpretability;regularization (mathematics);artificial intelligence;pattern recognition	AI	26.491630049487206	-34.41676133786471	115176
8389851adf47d1f2407bbc603090d4c272dcb17e	bayesian inference for a new class of distributions on equivalence classes of three-dimensional orientations with applications to materials science	cone shaped region;grupo de excelencia;texture analysis;ciencias basicas y experimentales;matematicas;statistics;bayesian methods matrix dimensional measurement probability distributions equivalency crystallography orientation;rotation matrix;uniform axis random spin	Experiments in materials science investigating cubic crystalline structures often collect data which are in truth equivalence classes of crystallographically symmetric orientations. These intend to represent how lattice structures of particles are orientated relative to a reference coordinate system. Motivated by a materials science application, we formulate parametric probability models for “unlabeled orientation data.” This amounts to developing models on equivalence classes of three-dimensional rotations. We use a flexible existing model class for random rotations (called uniform-axis-random-spin models) to induce probability distributions on the equivalence classes of rotations. We develop one-sample Bayesian inference for the parameters in these models, and compare this methodology to some likelihood-based approaches. We also contrast the new parametric analysis of unlabeled orientation data with other analyses that proceed as if the data have been preprocessed into honest orientation data. Supplemen...	turing completeness	Chuanlong Du;Daniel J. Nordman;Stephen B. Vardeman	2016	Technometrics	10.1080/00401706.2015.1017610	econometrics;combinatorics;rotation matrix;mathematics;statistics	Logic	35.62533705653081	-26.032255766154623	115276
6338925ffe77bc9da55481d999b6bdf930227f5b	efficient bayesian estimation of the multivariate double chain markov model	data augmentation;gibbs sampler;hidden markov model;double chain markov model;ratings migration;markov chain monte carlo	The Double Chain Markov Model (DCMM) is used to model an observable process $Y = \{Y_{t}\}_{t=1}^{T}$ as a Markov chain with transition matrix, $P_{x_{t}}$ , dependent on the value of an unobservable (hidden) Markov chain $\{X_{t}\}_{t=1}^{T}$ . We present and justify an efficient algorithm for sampling from the posterior distribution associated with the DCMM, when the observable process Y consists of independent vectors of (possibly) different lengths. Convergence of the Gibbs sampler, used to simulate the posterior density, is improved by adding a random permutation step. Simulation studies are included to illustrate the method. The problem that motivated our model is presented at the end. It is an application to real data, consisting of the credit rating dynamics of a portfolio of financial companies where the (unobserved) hidden process is the state of the broader economy.	markov chain;markov model	Matthew C Fitzpatrick;Dobrin Marchev	2013	Statistics and Computing	10.1007/s11222-012-9323-y	econometrics;markov chain;gibbs sampling;markov chain monte carlo;markov property;continuous-time markov chain;balance equation;machine learning;mathematics;additive markov chain;markov chain mixing time;markov model;hidden markov model;statistics;variable-order markov model	ML	31.39393392217596	-25.884334390559694	115323
b835e6cff8b4afece51851a59e4abf60c9f6df6a	a novel approach to extracting casing status features using data mining	tsallis wavelet singularity entropy;data mining;feature extraction;shannon wavelet time entropy;casing coupling location signal	Casing coupling location signals provided by the magnetic localizer in retractors are typically used to ascertain the position of casing couplings in horizontal wells. However, the casing coupling location signal is usually submerged in noise, which will result in the failure of casing coupling detection under the harsh logging environment conditions. The limitation of Shannon wavelet time entropy, in the feature extraction of casing status, is presented by analyzing its application mechanism, and a corresponding improved algorithm is subsequently proposed. On the basis of wavelet transform, two derivative algorithms, singular values decomposition and Tsallis entropy theory, are proposed and their physics meanings are researched. Meanwhile, a novel data mining approach to extract casing status features with Tsallis wavelet singularity entropy is put forward in this paper. The theoretical analysis and experiment results indicate that the proposed approach can not only extract the casing coupling features accurately, but also identify the characteristics of perforation and local corrosion in casings. The innovation of the paper is in the use of simple wavelet entropy algorithms to extract the complex nonlinear logging signal features of a horizontal well tractor.	algorithm;data mining;feature extraction;nonlinear system;shannon (unit);shannon wavelet;tsallis entropy;wavelet transform	Jikai Chen;Haoyu Li;Yanjun Wang;Ronghua Xie;Xingbin Liu	2014	Entropy	10.3390/e16010389	speech recognition;feature extraction;data mining	ML	37.702283707829544	-31.14457428240139	115376
2acbc1612d8c680b48a335b3e7057adf9d36900e	bayesian sparse tucker models for dimension reduction and tensor completion		Tucker decomposition is the cornerstone of modern machine learning on tensorial data analysis, which have attracted considerable attention for multiway feature extraction, compressive sensing, and tensor completion. The most challenging problem is related to determination of model complexity (i.e., multilinear rank), especially when noise and missing data are present. In addition, existing methods cannot take into account uncertainty information of latent factors, resulting in low generalization performance. To address these issues, we present a class of probabilistic generative Tucker models for tensor decomposition and completion with structural sparsity over multilinear latent space. To exploit structural sparse modeling, we introduce two group sparsity inducing priors by hierarchial representation of Laplace and Student-t distributions, which facilitates fully posterior inference. For model learning, we derived variational Bayesian inferences over all model (hyper)parameters, and developed efficient and scalable algorithms based on multilinear operations. Our methods can automatically adapt model complexity and infer an optimal multilinear rank by the principle of maximum lower bound of model evidence. Experimental results and comparisons on synthetic, chemometrics and neuroimaging data demonstrate remarkable performance of our models for recovering ground-truth of multilinear rank and missing entries.	algorithm;bitcoin;block truncation coding;chemometrics;compressed sensing;dimensionality reduction;feature extraction;feature learning;ground truth;machine learning;missing data;peak signal-to-noise ratio;scalability;sparse matrix;synthetic data;synthetic intelligence;tucker decomposition;variational principle	Qibin Zhao;Liqing Zhang;Andrzej Cichocki	2015	CoRR		mathematical optimization;multilinear principal component analysis;machine learning;pattern recognition;mathematics;multilinear subspace learning;statistics	ML	26.74024962396998	-35.04021197819134	115380
98f995133de844d814e5491abb705d0bc23c685d	an algorithm for the minimization of mixed l1 and l2 norms with application to bayesian estimation	minimisation;bayes estimation;quadratic programming;minimization;map;bayesian estimation;estimation problem;mathematics;metodo monte carlo;l 2 norms;gaussian processes;numerical computations;bayes methods;simulacion numerica;prior information;methode monte carlo;bayesian methods;prior distribution;descent type algorithm;least squares approximation;estimation a posteriori;square norms;modele physique;minimization methods;likelihood distributions;a posteriori estimation;books;algorithm;estimacion bayes;maximum a posteriori estimator;computational modeling;prior distributions;stochastic processes;functional;estimacion a posteriori;signal processing;monte carlo method;numerical computation;simulation numerique;modelo fisico;map estimation;regularizing functional approach;bayesian estimator;signal processing functional equations minimisation bayes methods stochastic processes monte carlo methods parameter estimation;functional equations;minimization methods bayesian methods gaussian processes books quadratic programming gaussian distribution computational modeling mathematics equations least squares approximation;physical model;parameter estimation;map l sub 1 norms l sub 2 norms bayesian estimation minimization algorithm regularizing functional approach estimation problem functional maximum a posteriori estimator likelihood distributions prior distributions laplace distribution gaussian distribution square norms absolute norm descent type algorithm numerical computations monte carlo simulations;monte carlo simulation;monte carlo simulations;gaussian distribution;monte carlo methods;absolute norm;estimation bayes;laplace distribution;l 1 norms;numerical simulation	The regularizing functional approach is widely used in many estimation problems. In practice, the solution is defined as one minimum point of a suitable functional, the main part of which accounts for the underlying physical model, whereas the regularizing part represents some prior information about the unknowns. In the Bayesian interpretation, one has a maximum a posteriori (MAP) estimator in which the main and regularizing parts are represented, respectively, by likelihood and prior distributions. When either the prior or likelihood is a Laplace distribution and the other is a Gaussian distribution, one is led to consider functionals that include both absolute and square norms. The authors present a characterization of the minimum points of such functionals, together with a descent-type algorithm for numerical computations. The results of Monte-Carlo simulations are also reported. >	algorithm	Stefano Alliney;S. A. Ruzinsky	1994	IEEE Trans. Signal Processing	10.1109/78.277854	econometrics;mathematical optimization;signal processing;mathematics;quadratic programming;statistics;monte carlo method	Visualization	30.7495549851121	-28.014326780152825	115435
261c827c4408416b9e1a9003b17f319b05fc7914	convex and spectral relaxations for phase retrieval, seriation and ranking. (relaxations convexes et spectrales pour les problèmes de reconstruction de phase, seriation et classement)		Optimization is often the computational bottleneck in disciplines such as statistics, biology, physics, finance or economics. Many optimization problems can be directly cast in the wellstudied convex optimization framework. For non-convex problems, it is often possible to derive convex or spectral relaxations, i.e., derive approximations schemes using spectral or convex optimization tools. Convex and spectral relaxations usually provide guarantees on the quality of the retrieved solutions, which often transcribes in better performance and robustness in practical applications, compared to naive greedy schemes. In this thesis, we focus on the problems of phase retrieval, seriation and ranking from pairwise comparisons. For each of these combinatorial problems we formulate convex and spectral relaxations that are robust, flexible and scalable. • Phase retrieval seeks to reconstruct a complex signal, given a number of observations on the magnitude of linear measurements. In Chapter 2, we focus on problems arising in diffraction imaging, where various illuminations of a single object, e.g., a molecule, are performed through randomly coded masks. We show that exploiting structural assumptions on the signal and the observations, such as sparsity, smoothness or positivity, can significantly speed-up convergence and improve recovery performance. • The seriation problem seeks to reconstruct a linear ordering of items based on unsorted, possibly noisy, pairwise similarity information. The underlying assumption is that items can be ordered along a chain, where the similarity between items decreases with their distance within this chain. In Chapter 3, we first show that seriation can be formulated as a combinatorial minimization problem over the set of permutations, and then derive several convex relaxations that improve the robustness of seriation solutions in noisy settings compared to the spectral relaxation of Atkins et al. (1998). As an additional benefit, these convex relaxations allow to impose a priori constraints on the solution, hence solve semisupervised seriation problems. We establish new approximation bounds for some of these relaxations and present promising numerical experiments on archeological data, Markov chains and DNA assembly from shotgun gene sequencing data. • Given pairwise comparisons between n items, the ranking problem seeks to find the most consistent global order of these items, e.g., ranking players in a tournament. In practice, the information about pairwise comparisons is usually incomplete, especially when the set of items is very large, and the data may also be noisy, that is some pairwise comparisons could be incorrectly measured and inconsistent with a total order. In Chapter 4, we formulate this ranking problem as a seriation problem, by constructing an adequate similarity matrix from pairwise comparisons. Intuitively, ordering items based on this similarity	approximation;convex optimization;experiment;greedy algorithm;linear programming relaxation;markov chain;mathematical optimization;numerical analysis;phase retrieval;randomness;robustness (computer science);scalability;semi-supervised learning;similarity measure;sparse matrix;variable shadowing	Fajwel Fogel	2015				ML	27.800432067875125	-36.53389308549395	115600
3828c9d1833ea18ac4d07b219d17388c0d301bf4	efficient and consistent robust time series analysis		We study the problem of robust time series analysis under the standard auto-regressive (AR) time series model in the presence of arbitrary outliers. We devise an efficient hard thresholding based algorithm which can obtain a consistent estimate of the optimal AR model despite a large fraction of the time series points being corrupted. Our algorithm alternately estimates the corrupted set of points and the model parameters, and is inspired by recent advances in robust regression and hard-thresholding methods. However, a direct application of existing techniques is hindered by a critical difference in the time-series domain: each point is correlated with all previous points rendering existing tools inapplicable directly. We show how to overcome this hurdle using novel proof techniques. Using our techniques, we are also able to provide the first efficient and provably consistent estimator for the robust regression problem where a standard linear observation model with white additive noise is corrupted arbitrarily. We illustrate our methods on synthetic datasets and show that our methods indeed are able to consistently recover the optimal parameters despite a large fraction of points being corrupted.	additive white gaussian noise;algorithm;autoregressive model;robustness (computer science);synthetic intelligence;thresholding (image processing);time series;utility functions on indivisible goods	Kush Bhatia;Prateek Jain;Parameswaran Kamalaruban;Purushottam Kar	2016	CoRR		econometrics;mathematical optimization;mathematics;statistics	ML	30.241960154197436	-29.501891418345817	115673
d8fd17dc7d27c6e82c5b5142a765e34f734d1218	dimensionality reduction of high-dimensional data with a nonlinear principal component aligned generative topographic mapping	62g07;classification;additive model;density estimation;dimensionality reduction;62h30;principal component analysis;generative topographic mapping;62h25	Most high-dimensional real-life data exhibit some dependencies such that data points do not populate the whole data space but lie approximately on a lower-dimensional manifold. A major problem in many data mining applications is the detection of such a manifold and the expression of the given data in terms of a moderate number of latent variables. We present a method which is derived from the generative topographic mapping (GTM) and can be seen as a non-linear generalization of the Principal Component Analysis (PCA). It can detect certain non-linearities in the data but does not suffer from the curse of dimension with respect to the latent space dimension as the original GTM and thus allows for higher embedding dimensions. We provide experiments that show that our approach leads to an improved data reconstruction compared to the purely linear PCA and that it can furthermore be used for classification.	curse of dimensionality;data mining;data point;dataspaces;dimensionality reduction;experiment;generative topographic map;latent variable;nonlinear dimensionality reduction;nonlinear system;polynomial;population;preprocessor;principal component analysis;real life;topography	Michael Griebel;Alexander Hullmann	2014	SIAM J. Scientific Computing	10.1137/130931382	diffusion map;generative topographic map;density estimation;biological classification;machine learning;pattern recognition;mathematics;nonlinear dimensionality reduction;additive model;statistics;dimensionality reduction;principal component analysis	ML	30.128316787084042	-36.301993167548765	115787
916717cefb3e648bfa75f1264a2389febb8003e3	multivariate regression and machine learning with sums of separable functions	62j02;large data sets;curse of dimensionality;scattered data;68t05;65d15;machine learning;function approximation;separation of variables;adaptive system;62h99;high dimension;multivariate regression;nonlinear regression	Abstract. We present an algorithm for learning (or estimating) a function of many variables from scattered data. The function is approximated by a sum of separable functions, following the paradigm of separated representations. The central fitting algorithm is linear in both the number of data points and the number of variables, and thus is suitable for large data sets in high dimensions. We present numerical evidence for the utility of these representations. In particular, we show that our method outperforms other methods on several benchmark data sets.	approximation algorithm;benchmark (computing);data point;general linear model;machine learning;numerical analysis;programming paradigm	Gregory Beylkin;Jochen Garcke;Martin J. Mohlenkamp	2009	SIAM J. Scientific Computing	10.1137/070710524	multivariate statistics;mathematical optimization;curse of dimensionality;function approximation;adaptive system;machine learning;pattern recognition;mathematics;separation of variables;data matrix;regression analysis;nonlinear regression;statistics	ML	29.211276643738675	-26.59784991942983	116181
17c8dd7c9c2a1bb9ac82b64c6910a932e72c100d	a comparative study of wpd and emd for shaft fault diagnosis		Fault diagnosis of incipient crack failure in rotating shafts allows the detection and identification of performance degradation as early as possible in industrial plants, such as downtime and potential injury to personnel. The present work studies the performance and effectiveness of crack fault detection by means of applying wavelet packet decomposition (WPD) and empirical mode decomposition (EMD) on fault diagnosis of rotating shafts using multiscale entropy (MSE). After WPD and EMD, the most sensitive reconstruction vectors and intrinsic mode functions (IMFs) are selected using Shannon entropy. Then, these feature vectors are fed into support vector machine (SVM) for fault classification, where the entropy features represent the complexity of vibration signals with different scales. Experimental results have demonstrated that WPD combined with MSE can achieve an accuracy of 97.3% for crack fault detection in rotating shafts, whilst EMD combined with MSE has shown a higher detection rate of 98.5%.		Zhiqiang Huo;Yu Zhang;Lei Shu	2017	IECON 2017 - 43rd Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2017.8217482	hilbert–huang transform;entropy (information theory);time series;control theory;support vector machine;engineering;fault detection and isolation;feature extraction;feature vector;wavelet packet decomposition	Robotics	37.32858530097589	-30.731908282425803	116495
d4930a59b064188f59eaac0f96c01e74344241ad	a fast normalized maximum likelihood algorithm for multinomial data	model selection;time complexity;normalized maximum likelihood;discrete fourier transform;exponential sum;stochastic complexity;recursive algorithm	Stochastic complexity of a data set is defined as the shortest possible code length for the data obtainable by using some fixed set of models. This measure is of great theoretical and practical importance as a tool for tasks such as model selection or data clustering. In the case of multinomial data, computing the modern version of stochastic complexity, defined as the Normalized Maximum Likelihood (NML) criterion, requires computing a sum with an exponential number of terms. Furthermore, in order to apply NML in practice, one often needs to compute a whole table of these exponential sums. In our previous work, we were able to compute this table by a recursive algorithm. The purpose of this paper is to significantly improve the time complexity of this algorithm. The techniques used here are based on the discrete Fourier transform and the convolution theorem.	algorithm;cluster analysis;convolution;discrete fourier transform;kolmogorov complexity;model selection;multinomial logistic regression;recursion (computer science);time complexity	Petri Kontkanen;Petri Myllymäki	2005			time complexity;mathematical optimization;discrete mathematics;computer science;discrete fourier transform;mathematics;exponential sum;model selection;statistics;recursion	ML	29.38654075973598	-29.403590958535542	116612
8a93c3caa278c5103a1d168daa95f5ff65cc2ca9	robust and sparse regression via γ-divergence		In high-dimensional data, many sparse regression methods have been proposed. However, they may not be robust against outliers. Recently, the use of density power weight has been studied for robust parameter estimation, and the corresponding divergences have been discussed. One such divergence is the γ-divergence, and the robust estimator using the γ-divergence is known for having a strong robustness. In this paper, we extend the γ-divergence to the regression problem, consider the robust and sparse regression based on the γ-divergence and show that it has a strong robustness under heavy contamination even when outliers are heterogeneous. The loss function is constructed by an empirical estimate of the γ-divergence with sparse regularization, and the parameter estimate is defined as the minimizer of the loss function. To obtain the robust and sparse estimate, we propose an efficient update algorithm, which has a monotone decreasing property of the loss function. Particularly, we discuss a linear regression problem with L1 regularization in detail. In numerical experiments and real data analyses, we see that the proposed method outperforms past robust and sparse methods.	algorithm;estimation theory;experiment;iterative method;loss function;matrix regularization;numerical analysis;sparse matrix;monotone	Takayuki Kawashima;Hironori Fujisawa	2017	Entropy	10.3390/e19110608	mathematical optimization;robustness (computer science);statistics;mathematics;linear regression;outlier;divergence;sparse approximation;estimation theory;mm algorithm;robust statistics	ML	27.09815574968484	-34.01802501366897	116617
d8fcfb25195b71b85cec72edf160f00f160cbe70	global data association for the probability hypothesis density filter using network flows	target tracking filtering theory probability sensor fusion;global data association pedestrian tracking dataset transition probabilities measurement likelihoods phd recursion multitarget tracker multitarget state estimation network flows probability hypothesis density filter;target tracking trajectory time measurement state estimation optimization markov processes	The Probability Hypothesis Density (PHD) filter is an efficient formulation of multi-target state estimation that circumvents the combinatorial explosion of the multi-target posterior by operating on single-target space without maintaining target identities. In this paper, we propose a multi-target tracker based on the PHD filter that provides instantaneous state estimation and delayed decision on data association. For this purpose, we reformulate the PHD recursion in terms of single-target track hypotheses and solve a min-cost flow network for trajectory estimation where measurement likelihoods and transition probabilities are based on multi-target state estimates. In this manner, the presented approach combines global data association with efficient multi-target filtering. We evaluate the approach on a publicly available pedestrian tracking dataset to present state estimation and data association capabilities.	correspondence problem;experiment;flow network;markov chain;maxima and minima;minimum-cost flow problem;programming paradigm;recursion	Nicolai Wojke;Dietrich Paulus	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487180	econometrics;data mining;mathematics;statistics	Robotics	38.737159782788645	-26.926991142620196	116693
e17286a8b724e26b1c2718c9d7b9f1b852ff0394	link prediction using probabilistic group models of network structure	complex networks;metabolic network;gibbs sampling;complex network;bayesian inference;link prediction;prior distribution;prior knowledge;social network;social science;mixture model;multinomial mixture models;network structure	Modeling of complex networks is a crucial task such as in biology and social sciences. A large number of researches have been conducted for such a problem; however, most of them require explicit, specific prior knowledge on target networks. On the other hand, a few recent works on multinomial mixture models presented that those models do not require such explicit prior knowledge and turned out to be effective for the task of group detection of vertices such as in social networks. This paper focuses on another task, link prediction in such complex networks, using a Bayesian multinomial mixture model, which assumes unobservable prior distributions over multinomial mixtures based on network structure and are estimated using Bayesian inference via Gibbs sampling. We demonstrate that link prediction performance was significantly improved using this method, compared to five conventional methods, through experiments using a metabolic network and a co-authorship network.	complex network;consistency model;experiment;gibbs sampling;mixture model;multinomial logistic regression;protein structure prediction;sampling (signal processing);social network	Akira Ninagawa;Koji Eguchi	2010		10.1145/1774088.1774323	variable-order bayesian network;computer science;dynamic network analysis;machine learning;complex network;statistics	ML	27.822570652436568	-32.32420469108871	116743
3dcbd4589c17e7edcf29f22d82f2a59ca2f172f1	intelligent systems applied on the estimation of bearing faults in inverter-fed induction motors	bearing faults;induction motor;intelligent system;diagnosis	This paper proposes an approach based on intelligent systems for the classification and diagnosis of bearing fault evolution in inverter-fed induction motors, operating at steady state under a wide range of frequencies and load torque. Due to its robustness and low cost, induction motors are used in various industrial applications. In this work, the classifiers Fuzzy Artmap (FAM), Support Vector Machine - Sequential Minimal Optimization (SVM/SMO), k-Nearest Neighbours (k-NN) and Multilayer Perceptron (MLP) are used for the diagnosis and classification of bearing faults. Results obtained from 1173 experimental tests collected in the laboratory are presented to validate this proposal. The obtained results shows that this approach can accurately classify healthy and bearing defects in inverter-fed induction motors.	araucaria;calculus of variations;complement (complexity);fuzzy associative matrix;hybrid intelligent system;inverter (logic gate);k-nearest neighbors algorithm;mathematical induction;memory-level parallelism;multilayer perceptron;nearest neighbor search;power inverter;sequential minimal optimization;steady state;support vector machine;variable-frequency drive	Wagner Fontes Godoy;Rodrigo Henrique Cunha Palácios;I. N. da Silva;Alessandro Goedtel;P. P. D. da Silva	2016	IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2016.7793019	control engineering;engineering;machine learning;control theory	Robotics	36.38696416155252	-30.86603171503365	117177
be36e08308157849a985fa6b9e25508cd8bbf588	a web-based database on methanogenic potential of crops and wastes	methane;chemical composition;crop;anaerobic digestion;bioenergy;manure;waste	The Methanogenic Potential Database (BMP Database) provides engineers and scientists with specific and standardized information on the chemical composition and biochemical methane potential of crops, manures, wastes, as well as of mixed substrates. Currently, the BMP database contains data selected from more than 80 sources and covers more than 180 different substrates.	web application	Mario Alberto Luna Del Risco;Henri-Charles Dubourguier	2010	Environmental Modelling and Software	10.1016/j.envsoft.2010.02.002	anaerobic digestion;crop;chemical composition;methane;biotechnology;waste;agronomy;manure;waste management;ecology;bioenergy	SE	35.31357743888684	-36.51302319333431	117311
3dbc640f40014abf97b8e65c89e34b5011e82d23	regression-based estimation of covariance matrix of stock returns				Saejoon Kim;Soong Ho Kim	2018		10.3233/978-1-61499-927-0-182		ML	30.61674944775268	-24.416791537440826	117439
e11a67a0e5cd853367194d21bff8f1e10e4ac199	bayesian clustering of fuzzy feature vectors using a quasi-likelihood approach	bayesian framework;bayes estimation;unsupervised learning;modelizacion;processus gauss;discretisation;pattern clustering;analyse amas;gaussian mixture;bayesian clustering;mathematics;probability;analisis estadistico;analisis datos;melange loi probabilite;gaussian processes;classification non supervisee;quasi likelihood;bayes methods;mathematics and statistics;logique floue;supervised classification;discretization;bayesian methods;logica difusa;mixed distribution;discretizacion;fuzzy feature vector;spectrum;posterior probability;probabilistic approach;fuzzy set theory;binomial quasilikelihood approach;discrete model structure;fuzzy logic;fuzzy modeling;modelisation;feature vector;unsupervised learning bayes methods fuzzy set theory gaussian processes pattern classification pattern clustering probability;data analysis;estimacion bayes;cluster analysis;discrete model;statistical analysis;machine learning;stochastic processes;statistical power;information theory bayesian clustering fuzzy feature vector unsupervised classification supervised classification gaussian mixture classifier discrete model structure binomial quasilikelihood approach probability;computational complexity;gaussian mixture classifier;enfoque probabilista;approche probabiliste;clasificacion no supervisada;probabilite a posteriori;analyse statistique;probabilidad a posteriori;pattern classification;continuous data bayesian clustering quasi likelihood fuzzy modeling;pattern recognition;unsupervised classification;bayesian methods machine learning context modeling pattern recognition entropy computational complexity stochastic processes mathematics;mezcla ley probabilidad;analyse donnee;analisis cluster;modele donnee;entropy;gaussian process;theorie information;proceso gauss;modeling;information theoretic;context modeling;algorithms artificial intelligence bayes theorem computer simulation fuzzy logic likelihood functions models statistical pattern recognition automated;information theory;fuzzy model	Bayesian model-based classifiers, both unsupervised and supervised, have been studied extensively, and their value and versatility have been demonstrated on a wide spectrum of applications within science and engineering. A majority of the classifiers are built on the assumption of intrinsic discreteness of the considered data features or on their discretization prior to the modeling. On the other hand, Gaussian mixture classifiers have also been utilized to a large extent for continuous features in the Bayesian framework. Often, the primary reason for discretization in the classification context is the simplification of the analytical and numerical properties of the Bayesian models. However, the discretization can be problematic due to its ad hoc nature and the decreased statistical power to detect the correct classes (or clusters) in the resulting procedure. Here, we introduce an unsupervised classification approach for fuzzy feature vectors that utilizes a discrete model structure while preserving the continuous characteristics of data. This goal is achieved by replacing the ordinary likelihood by a binomial quasi-likelihood to yield an analytical expression for the posterior probability of a given clustering solution. The resulting model can also be justified from an information-theoretic perspective. Our method is shown to yield highly accurate clusterings for challenging synthetic and empirical data sets and to perform favorably compared to some alternative approaches.	bayesian network;class;cluster analysis;discretization;hoc (programming language);information theory;level of detail;mixture model;normal statistical distribution;numerical analysis;supervised learning;synthetic intelligence;unsupervised learning;statistical cluster	Pekka Marttinen;Jing Tang;Bernard De Baets;Peter Dawyndt;Jukka Corander	2009	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2008.53	unsupervised learning;information theory;computer science;machine learning;pattern recognition;discretization;gaussian process;mathematics;statistics	ML	30.909350848467767	-30.060051541337756	117637
0bb11a5ecf844673622d9ebe8ccdf37eb2e79e31	sequential sampling algorithms: unified analysis and lower bounds	extraction information;metodo adaptativo;algorithm analysis;generic algorithm;analisis datos;information extraction;random sampling;echantillonnage;chernoff bound;metodo secuencial;methode adaptative;chernoff bounds;sequential method;data mining;sampling;data analysis;fouille donnee;adaptive method;adaptive sampling;cherno bounds;methode sequentielle;analyse donnee;analyse algorithme;external research report;muestreo;busca dato;extraccion informacion;borne chernoff;analisis algoritmo;lower bound;sequential sampling	Sequential sampling algorithms have recently attracted interest as a way to design scalable algorithms for Data mining and KDD processes. In this paper, we identify an elementary sequential sampling task (estimation from examples), from which one can derive many other tasks appearing in practice. We present a generic algorithm to solve this task and an analysis of its correctness and running time that is simpler and more intuitive than those existing in the literature. For two specific tasks, frequency and advantage estimation, we derive lower bounds on running time in addition to the general upper bounds.	algorithm;correctness (computer science);data mining;generic programming;sampling (signal processing);scalability;time complexity	Ricard Gavaldà;Osamu Watanabe	2001		10.1007/3-540-45322-9_12	sampling;econometrics;computer science;mathematics;chernoff bound;information extraction;algorithm;statistics	ML	29.685866242424325	-29.46883066464011	117754
694246d5294cd896bc116c374eeb0d1ab3997e06	scale-space based weak regressors for boosting	regression model;wavelet decomposition;data mining;scale space;machine learning;loss function;power modeling	Boosting is a simple yet powerful modeling technique that is used in many machine learning and data mining related applications. In this paper, we propose a novel scale-space based boosting framework which applies scale-space theory for choosing the optimal regressors during the various iterations of the boosting algorithm. In other words, the data is considered at different resolutions for each iteration in the boosting algorithm. Our framework chooses the weak regressors for the boosting algorithm that can best fit the current resolution and as the iterations progress, the resolution of the data is increased. The amount of increase in the resolution follows from the wavelet decomposition methods. For regression modeling, we use logitboost update equations based on first derivative of the loss function. We clearly manifest the advantages of using this scale-space based framework for regression problems and show results on different real-world regression datasets.	abalone;adaboost;algorithm;boosting (machine learning);curve fitting;data mining;emoticon;iteration;logitboost;loss function;machine learning;scale space;wavelet transform	Jin Hyeong Park;Chandan K. Reddy	2007		10.1007/978-3-540-74958-5_66	econometrics;machine learning;pattern recognition;mathematics;gradient boosting	ML	26.333937410421505	-34.6088432141435	117756
ef2d539b9cfd656b6546b984d619a9df2f8a6da9	on-line tool wear monitoring via sparse coding based on dct and wpd		The adaptive and on-line tool wear monitoring is of great importance to improve the milling precision and efficiency. In traditional tool wear monitoring, feature extraction of cutting force signal by time-frequency method was usually off-line and needed signal reconstruction. In this paper, a novel online tool wear monitoring method is proposed. In the method, the sparse coefficients is measured by sparse coding based on DCT and WPD and then utilized to indicate the tool wear level without signal reconstruction. Experiments of tool wear monitoring are conducted for high speed CNC manufacturing. The simulation results show that the proposed method is capable to indicate tool wear level and robust to cutting conditions		Xiaolong Yu;Rongchuan Wang;Yungao Shi;Kunpeng Zhu	2018	2018 IEEE 14th International Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2018.8560437	discrete cosine transform;feature extraction;neural coding;computer vision;tool wear;signal reconstruction;computer science;artificial intelligence	Robotics	38.110442026593326	-31.336905653554478	117989
6cbd3a7f2b3be1a421f02cadf072c5c494d0b9be	convergence theorems for generalized alternating minimization procedures	rate of convergence;convergence theorem;practical reasoning;information geometry;alternating minimization;parameter estimation;em algorithm	The EM algorithm is widely used to develop iterative parameter estimation procedures for statistical models. In cases where these procedures strictly follow the EM formulation, the convergence properties of the estimation procedures are well understood. In some instances there are practical reasons to develop procedures that do not strictly fall within the EM framework. We study EM variants in which the E-step is not performed exactly, either to obtain improved rates of convergence, or due to approximations needed to compute statistics under a model family over which E-steps cannot be realized. Since these variants are not EM procedures, the standard (G)EM convergence results do not apply to them. We present an information geometric framework for describing such algorithms and analyzing their convergence properties. We apply this framework to analyze the convergence properties of incremental EM and variational EM. For incremental EM, we discuss conditions under these algorithms converge in likelihood. For variational EM, we show how the E-step approximation prevents convergence to local maxima in likelihood.	approximation;calculus of variations;converge;estimation theory;expectation–maximization algorithm;iterative method;maxima and minima;statistical model;variational principle	Asela Gunawardana;William J. Byrne	2005	Journal of Machine Learning Research		practical reason;mathematical optimization;combinatorics;expectation–maximization algorithm;modes of convergence;compact convergence;mathematics;convergence tests;rate of convergence;estimation theory;normal convergence;information geometry;statistics	ML	31.45059056501738	-27.974782302828892	118080
9a5da62a123e8d4ced5b9fcb9659b31b8827aa0e	the failure analysis of extreme learning machine on big data and the counter measure	singular value decomposition big data failure analysis feedforward neural nets learning artificial intelligence matrix algebra;large scale data learning failure analysis counter measure extreme learning machine elm single hidden layer feedforward neural networks slfn big data hidden layer output matrix singular value decomposition svd high order matrix high calculation complexity normal equation matrix equation;extreme learning machine;svd;generalized inverse;normal equation extreme learning machine generalized inverse svd;normal equation	Extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) was known for its extremely fast learning speed while maintaining acceptable generalization. Unfortunately, the failure of ELM on big data occurs frequently. The course is, the main computation of ELM focus on the calculation of generalized inverse of hidden layer output matrix, which depends on singular value decomposition (SVD) and has very low efficiency especially on high order matrix. In view of this high calculation complexity directly courses the failure of ELM on big data, normal equation extreme learning machine is proposed, which use the normal equation to reduce the size of the matrix equation and overcome the failure. The experiments on benchmarks show that the new proposed model has better performance than the ELM, so as to have more potential for large scale data learning.	artificial neural network;benchmark (computing);big data;computation;computational complexity theory;elm;experiment;failure analysis;feedforward neural network;ordinary least squares;singular value decomposition;the matrix	Pm-Zhou Zhang;Shi-Xin Zhao;Xizhao Wang	2015	2015 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2015.7340664	mathematical optimization;generalized inverse;artificial intelligence;theoretical computer science;machine learning;mathematics;singular value decomposition;linear least squares;statistics	ML	25.021655771133027	-36.51641808341344	118218
32c1efd2a449b1050630fdf46c59b7244045216c	variography for model selection in local polynomial regression with spatial data	model selection;spatial data;local polynomial regression;spatial statistics;multivariate data	In this work, we apply variographic techniques from spatial statistics to the problem of model selection in local polynomial regression with multivariate data. These techniques permit selection of the kernel and smoothing matrix with less computational load and interpretation of the regularity of the regression function in different directions. Moreover, they may represent the only feasible alternative for problems of a certain dimensionality. Mathematics Subject Classifications (2000): 62G08, 62H11, 65C60, 86A32, 91B72.	model selection;polynomial;smoothing;spatial analysis	José M. Matías;Wenceslao González-Manteiga;Mario Francisco-Fernández;C. Ordóñez Galán	2005	J. Math. Model. Algorithms	10.1007/s10852-005-9001-6	econometrics;polynomial regression;pattern recognition;mathematics;spatial analysis;statistics	ML	29.366023194965962	-26.33839137341961	118490
be09a793d5c965ed0014238e30d5e241afefc828	nonconvex optimization meets low-rank matrix factorization: an overview		Substantial progress has been made recently on developing provably accurate and efficient algorithms for low-rank matrix factorization via nonconvex optimization. While conventional wisdom often takes a dim view of nonconvex optimization algorithms due to their susceptibility to spurious local minima, simple iterative methods such as gradient descent have been remarkably successful in practice. The theoretical footings, however, had been largely lacking until recently. In this tutorial-style overview, we highlight the important role of statistical models in enabling efficient nonconvex optimization with performance guarantees. We review two contrasting approaches: (1) two-stage algorithms, which consist of a tailored initialization step followed by successive refinement; and (2) global landscape analysis and initialization-free algorithms. Several canonical matrix factorization problems are discussed, including but not limited to matrix sensing, phase retrieval, matrix completion, blind deconvolution, robust principal component analysis, phase synchronization, and joint alignment. Special care is taken to illustrate the key technical insights underlying their analyses. This article serves as a testament that the integrated thinking of optimization and statistics leads to fruitful research findings.	algorithm;blind deconvolution;gradient descent;iterative method;mathematical optimization;maxima and minima;phase retrieval;refinement (computing);robust principal component analysis;statistical model;synchronization (computer science);testament	Yuejie Chi;Yue M. Lu;Yuxin Chen	2018	CoRR		machine learning;iterative method;matrix completion;mathematical optimization;low-rank approximation;robust principal component analysis;matrix (mathematics);artificial intelligence;mathematics;matrix decomposition;factorization;gradient descent	ML	28.111865376655576	-37.736119186331315	118509
9c8108428feae3f28a60323628092963a50dacad	composite likelihood data augmentation for within-network statistical relational learning	data augmentation;approximation algorithms;nonlinear dynamical systems;mixing rate statistical relational learning data augmentation nonlinear dynamical systems;joints;estimation;stochastic processes approximation theory data handling expectation maximisation algorithm learning artificial intelligence network theory graphs;collective rml algorithm composite likelihood data augmentation within network statistical relational learning relational machine learning statistical correlations linked nodes rml methods node feature prediction network relationships partially labeled network relational versions expectation maximization r em methods predictive performance fixed point methods approximate learning sparsely labeled networks relational stochastic em method r sem method stochastic parameters approximation errors relational data augmentation method r da method relational inference;statistical relational learning;approximation methods inference algorithms joints approximation algorithms estimation markov processes;inference algorithms;approximation methods;markov processes;mixing rate	The prevalence of datasets that can be represented as networks has recently fueled a great deal of work in the area of Relational Machine Learning (RML). Due to the statistical correlations between linked nodes in the network, many RML methods focus on predicting node features (i.e., labels) using the network relationships. However, many domains are comprised of a single, partially-labeled network. Thus, relational versions of Expectation Maximization (i.e., R-EM), which jointly learn parameters and infer the missing labels, can outperform methods that learn parameters from the labeled data and apply them for inference on the unlabeled nodes. Although R-EM methods can significantly improve predictive performance in networks that are densely labeled, they do not achieve the same gains in sparsely labeled networks and can perform worse than RML methods. In this work, we show the fixed-point methods that R-EM uses for approximate learning and inference result in errors that prevent convergence in sparsely labeled networks. We then propose two methods that do not experience this problem. First, we develop a Relational Stochastic EM (R-SEM) method, which uses stochastic parameters that are not as susceptible to approximation errors. Then we develop a Relational Data Augmentation (R-DA) method, which integrates over a range of stochastic parameter values for inference. R-SEM and R-DA can use any collective RML algorithm for learning and inference in partially labeled networks. We analyze their performance with two RML learners over four real world datasets, and show that they outperform independent learning, RML and R-EM -- particularly in sparsely labeled networks.	approximation algorithm;authorization;em intermediate language;exptime;expectation–maximization algorithm;fixed point (mathematics);gibbs sampling;ibm notes;machine learning;maximal set;sampling (signal processing);statistical relational learning;stochastic modelling (insurance)	Joseph J. Pfeiffer;Jennifer Neville;Paul N. Bennett	2014	2014 IEEE International Conference on Data Mining	10.1109/ICDM.2014.151	estimation;statistical relational learning;computer science;machine learning;pattern recognition;data mining;mathematics;markov process;statistics	ML	26.087021483808986	-29.69320310341526	118542
4f1e85f6158eeeeb910e21e673b2cf9b352f4f0a	recursive slow feature analysis for adaptive monitoring of industrial processes		Recently, a new process monitoring and fault diagnosis method based on slow feature analysis has been developed, which enables concurrent monitoring of both operating point and process dynamics. In this paper, a recursive slow feature analysis algorithm for adaptive process monitoring is put forward to accommodate time-varying processes by updating model parameters and monitoring statistics once a new sample arrives. An important algebraic property of slow feature analysis is first established. We then show that such a property can be violated by online updating with a forgetting factor used, and a remedy is suggested. A novel algorithm based on the rank-one modification and the orthogonal iteration procedure is proposed to recursively adjust the solution to the generalized eigenvalue problem, model parameters, and associated monitoring statistics in a cost-efficient way. In addition, an improved stopping criterion for model updating is proposed based on the statistics relevant to process dynamics, which yields an intelligent maintenance mechanism of monitoring systems. The efficacy of the proposed method is finally evaluated on a real crude heating furnace system.	cost efficiency;iteration;online algorithm;operating point;recursion (computer science)	C. G. Shang;Fan Yang;Biao Huang;Dexian Huang	2018	IEEE Transactions on Industrial Electronics	10.1109/TIE.2018.2811358	algebraic number;control theory;recursion;eigendecomposition of a matrix;operating point;principal component analysis;pattern recognition (psychology);engineering;forgetting;control system	ML	36.05114085570208	-28.694994611639974	118607
39979dd47d3aca504f063949319627886e96794d	dynamic sparse coding with smoothing proximal gradient method	technological innovation encoding smoothing methods approximation methods optimization kalman filters noise;optimization procedure dynamic sparse coding smoothing proximal gradient method time varying sparse signal estimation under sampled observation sequence;smoothing methods encoding gradient methods optimisation signal sampling;proximal methods state space sparse coding dynamics	In this work we focus on the problem of estimating time-varying sparse signals from a sequence of under-sampled observations. We formulate this problem as estimating hidden states in a dynamic model and exploit the underlying temporal structure to find a more accurate solution, particularly when the information in the observations is at scarce. We propose an optimization procedure based on smoothing proximal gradient method to estimate these hidden states. We show that the proposed model is efficient and more robust to the noise in the system.	compressed sensing;image noise;mathematical model;mathematical optimization;neural coding;proximal gradient method;proximal gradient methods for learning;smoothing;sparse matrix	Rakesh Chalasani;José Carlos Príncipe	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854995	mathematical optimization;machine learning;sparse approximation;mathematics;proximal gradient methods;statistics;smoothing	Robotics	30.13620921391838	-33.05595823301115	118821
1b173e6484fa91f13fa36539a89e051303829304	the heteroskedasticity test implementation for linear regression model using matlab				Ludmyla Malyarets;Katerina Kovaleva;Irina Lebedeva;Ievgeniia Misiura;Oleksandr Dorokhov	2018	Informatica (Slovenia)		data mining;machine learning;matlab;heteroscedasticity;artificial intelligence;linear regression;computer science	EDA	30.19718727775809	-24.02463678732404	118881
4a7cd1b49f8e7ddc4729865446e5e18c1ddc8430	d-trace estimation of a precision matrix using adaptive lasso penalties		The accurate estimation of a precision matrix plays a crucial role in the current age of high-dimensional data explosion. To deal with this problem, one of the prominent and commonly used techniques is the \(\ell _1\) norm (Lasso) penalization for a given loss function. This approach guarantees the sparsity of the precision matrix estimate for properly selected penalty parameters. However, the \(\ell _1\) norm penalization often fails to control the bias of obtained estimator because of its overestimation behavior. In this paper, we introduce two adaptive extensions of the recently proposed \(\ell _1\) norm penalized D-trace loss minimization method. They aim at reducing the produced bias in the estimator. Extensive numerical results, using both simulated and real datasets, show the advantage of our proposed estimators.	lasso	Vahe Avagyan;Andrés M. Alonso;Francisco J. Nogales	2018	Adv. Data Analysis and Classification	10.1007/s11634-016-0272-8	econometrics;mathematical optimization;mathematics;statistics	ML	26.96355491581886	-34.50512252939886	119361
4e944436b0458b9d53cde5edb9905e41d01a2ced	multicollinearity and correlation among local regression coefficients in geographically weighted regression	analyse risque;maps;correlacion;distribucion espacial;diagnostic test;population;experimental studies;mapa;coeficiente correlacion;north america;cancer;america del norte;amerique du nord;experimental spatial design;etude experimentale;controlled experiment;mortalite;etats unis;estados unidos;carte;modelo;spatial pattern;echantillon reference;spatial distribution;regresion estadistica;disease mapping;poblacion;spatial eigenvectors;risk assessment;sante publique;regression analysis;modele;geografia;distribution spatiale;correlation;communities;extension;regression statistique;correlation coefficient;geographie;coefficient correlation;models;local regression diagnostics;standard samples;roca patron;multicollinearity;geographically weighted regression;eigenvectors;data generation process;geography	Present methodological research on geographically weighted regression (GWR) focuses primarily on extensions of the basic GWR model, while ignoring well-established diagnostics tests commonly used in standard global regression analysis. This paper investigates multicollinearity issues surrounding the local GWR coefficients at a single location and the overall correlation between GWR coefficients associated with two different exogenous variables. Results indicate that the local regression coefficients are potentially collinear even if the underlying exogenous variables in the data generating process are uncorrelated. Based on these findings, applied GWR research should practice caution in substantively interpreting the spatial patterns of local GWR coefficients. An empirical disease-mapping example is used to motivate the GWR multicollinearity problem. Controlled experiments are performed to systematically explore coefficient dependency issues in GWR. These experiments specify global models that use eigenvectors from a spatial link matrix as exogenous variables. This study was supported by grant number 1 R1 CA95982-01, Geographic-Based Research in Cancer Control and Epidermiology, from the National Cancer Institute. The author thank the anonymous reviewers and the editor for their helpful comments. D. Wheeler (&) Department of Geography, The Ohio State University, 1036 Derby Hall, Columbus, OH 43210, USA E-mail: wheeler.173@osu.edu M. Tiefelsdorf School of Social Sciences, University of Texas at Dallas, Richardson, TX 75083, USA E-mail: tiefelsdorf@utdallas.edu J Geograph Syst (2005) 7: 161–187 DOI: 10.1007/s10109-005-0155-6	autocorrelation;bandwidth (signal processing);bayesian network;columbus;coefficient;emoticon;experiment;global variable;guinness world records;interdependence;map;object-relational database;qr decomposition;reference model;richardson number;spatial analysis;xfig	David Wheeler;Michael Tiefelsdorf	2005	Journal of Geographical Systems	10.1007/s10109-005-0155-6	risk assessment;econometrics;multicollinearity;common spatial pattern;geography;eigenvalues and eigenvectors;mathematics;correlation;diagnostic test;cartography;regression analysis;statistics;cancer;population	ML	35.02444210331292	-24.7244067168983	119618
3196de075bd00eaac9fcd9be0c2400533a87f36e	near-separable non-negative matrix factorization with ℓ1- and bregman loss functions		Recently, a family of tractable NMF algorithms have been proposed under the assumption that the data matrix satisfies a separability condition Donoho & Stodden (2003); Arora et al. (2012). Geometrically, this condition reformulates the NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. In this paper, we develop several extensions of the conical hull procedures of Kumar et al. (2013) for robust (l1) approximations and Bregman divergences. Our methods inherit all the advantages of Kumar et al. (2013) including scalability and noise-tolerance. We show that on foreground-background separation problems in computer vision, robust near-separable NMFs match the performance of Robust PCA, considered state of the art on these problems, with an order of magnitude faster training time. We also demonstrate applications in exemplar selection settings.	algorithm;approximation;bregman divergence;cobham's thesis;computer vision;foreground-background;linear separability;loss function;non-negative matrix factorization;scalability	Abhishek Kumar;Vikas Sindhwani	2015		10.1137/1.9781611974010.39		ML	25.97645486979492	-37.89433560199869	119822
39187b6cb8ddd60d4c57d7136e1d7e8d2534e134	theoretical model of the fld ensemble classifier based on hypothesis testing theory	detectors;training;testing;ensemble classifier hypothesis testing theory information hiding optimal detection multi class classification;testing detectors payloads feature extraction training gaussian distribution;feature extraction;steganography learning artificial intelligence pattern classification probability statistical testing;payloads;probability theoretical model fld ensemble classifier hypothesis testing theory machine learning tool steganalysis digital media optimal detection statistical model base learner projection;gaussian distribution	The FLD ensemble classifier is a widely used machine learning tool for steganalysis of digital media due to its efficiency when working with high dimensional feature sets. This paper explains how this classifier can be formulated within the framework of optimal detection by using an accurate statistical model of base learners' projections and the hypothesis testing theory. A substantial advantage of this formulation is the ability to theoretically establish the test properties, including the probability of false alarm and the test power, and the flexibility to use other criteria of optimality than the conventional total probability of error. Numerical results on real images show the sharpness of the theoretically established results and the relevance of the proposed methodology.	authorization;digital media;ensemble learning;experiment;lr parser;machine learning;numerical method;relevance;statistical model;steganalysis	Rémi Cogranne;Tomás Denemark;Jessica J. Fridrich	2014	2014 IEEE International Workshop on Information Forensics and Security (WIFS)	10.1109/WIFS.2014.7084322	quadratic classifier;computer science;machine learning;pattern recognition;statistics	ML	29.01533735850047	-32.41674867931671	120126
36d163ac3706ff4753fb196bbe6199debcf2e56d	hierarchical double dirichlet process mixture of gaussian processes	gaussian processes;mixtures;nonlinear;nonstationary;hierarchical dirichlet process	We consider an infinite mixture model of Gaussian processes that share mixture components between nonlocal clusters in data. Meeds and Osindero (2006) use a single Dirichlet process prior to specify a mixture of Gaussian processes using an infinite number of experts. In this paper, we extend this approach to allow for experts to be shared non-locally across the input domain. This is accomplished with a hierarchical double Dirichlet process prior, which builds upon a standard hierarchical Dirichlet process by incorporating local parameters that are unique to each cluster while sharing mixture components between them. We evaluate the model on simulated and real data, showing that sharing Gaussian process components non-locally can yield effective and useful models for richly clustered non-stationary, non-linear data.	gaussian process;mixture model;nonlinear system;nonlocal lagrangian;simulation;stationary process	Aditya Tayal;Pascal Poupart;Yuying Li	2012			latent dirichlet allocation;nonlinear system;machine learning;pattern recognition;mixture model;gaussian process;mixture;statistics;hierarchical dirichlet process	AI	30.092852815552586	-32.1984561217131	120156
2465efe3e74e4be62314eecff77af207a8993615	bayesian filtering with online gaussian process latent variable models		In this paper we present a novel non-parametric approach to Bayesian filtering, where the prediction and observation models are learned in an online fashion. Our approach is able to handle multimodal distributions over both models by employing a mixture model representation with Gaussian Processes (GP) based components. To cope with the increasing complexity of the estimation process, we explore two computationally efficient GP variants, sparse online GP and local GP, which help to manage computation requirements for each mixture component. Our experiments demonstrate that our approach can track human motion much more accurately than existing approaches that learn the prediction and observation models offline and do not update these models with the incoming data stream.	algorithmic efficiency;bayesian network;computation;computer performance;experiment;gaussian process;kinesiology;latent variable;mixture model;multimodal interaction;naive bayes spam filtering;online and offline;particle filter;requirement;sparse matrix;standard of good practice;time series	Yali Wang;Marcus A. Brubaker;Brahim Chaib-draa;Raquel Urtasun	2014			computer science;machine learning;data mining;statistics	ML	30.298568350293003	-32.76737702809259	120641
9ae851d8fdf858dc8b360bb7d96eda44325e28a6	a nonlinear population monte carlo scheme for bayesian parameter estimation in a stochastic intercellular network model	standards;approximation algorithms;systems biology nonlinear population monte carlo scheme npmc algorithm bayesian parameter estimation stochastic intercellular network model continuous time stochastic version posterior probability distribution weight approximation particle filtering;computational modeling;proteins;stochastic processes;proteins computational modeling monte carlo methods stochastic processes approximation algorithms standards artificial intelligence;stochastic processes approximation theory bayes methods biology continuous time systems modelling monte carlo methods nonlinear systems parameter estimation particle filtering numerical methods statistical distributions;artificial intelligence;monte carlo methods	We investigate the application of a novel method termed nonlinear population Monte Carlo (NPMC) to the Bayesian estimation of a subset of the static parameters of a dynamic model that captures some of the features of intercellular networks, including intercell communication. The model we propose is a continuous-time stochastic version of a coupled-repressilator system that has enjoyed popularity in the last few years. To compute the posterior probability distributions of the unknown parameters, we first convert the model into a discrete-time state-space system and then apply an NPMC algorithm, with the importance weights being approximated via particle filtering (as they are analytically intractable). We show that the resulting parameter estimates converge asymptotically in the number of Monte Carlo samples in the parameter space, even if the computational budget of the particle filters used to approximate the weights of these samples is fixed. Some illustrative numerical results are also shown.	approximation algorithm;computer simulation;converge;estimation theory;mathematical model;monte carlo method;network model;nonlinear system;numerical analysis;particle filter;repressilator;state space	Joaquín Míguez;Inés P. Mariño	2015	2015 IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)	10.1109/CAMSAP.2015.7383845	quantum monte carlo;monte carlo method in statistical physics;quasi-monte carlo method;econometrics;mathematical optimization;dynamic monte carlo method;hybrid monte carlo;particle filter;markov chain monte carlo;computer science;monte carlo molecular modeling;kinetic monte carlo;monte carlo integration;stochastic;gillespie algorithm;statistics;monte carlo method	Vision	31.655458358868714	-26.746076220391352	120711
3315dad1763c96cd81a35e9202674db4dee773a3	vehicle detection and recognition based on a mems magnetic sensor	magnetic signal detection;magnetic field;support vector machines;vehicle recognition;target recognition mems magnetic sensor vehicle detection feature extraction;magnetic sensors;vehicle detection;magnetic sensor;magnetoacoustic effects;micro electro mechanical system;low power;ground vehicle targets;microelectromechanical system technology;target recognition;traffic engineering computing feature extraction magnetic sensors microsensors object detection road vehicles signal classification support vector machines;feature extraction;signal processing;magnetic resonance imaging;signal classification;classification algorithms;detection algorithm;traffic engineering computing;vehicles;noise removal solution;vehicle detection micromechanical devices magnetic sensors target recognition land vehicles feature extraction support vector machines road vehicles signal generators signal processing;support vector machine;mems magnetic sensor;magnetic perturbation detection;magnetic perturbation detection vehicle detection vehicle recognition mems magnetic sensor microelectromechanical system technology noise removal solution short time transform support vector machine magnetic signal detection magnetic signal recognition feature extraction ground vehicle targets target recognition signal processing;magnetic signal recognition;short time transform;microsensors;noise removal;object detection;road vehicles	Because vehicles moving over ground can generate a succession of impacts on the earth's magnetic field, we can detect them by means of detecting magnetic perturbation using a magnetic sensor, and automatically recognize them by advanced signal processing and recognition method. Comparing to traditional devices, magnetic sensors fabricated with Micro-electro-mechanical system (MEMS) technology is a promising apparatus for vehicle detection, because it is low cost, low power, small volume and light weight. This paper proposes a vehicle detection system structure together with its installation method. The system is based on a MEMS magnetic sensor. This paper also proposes a vehicle detection and noise removal solution based on short-time transform (STFT). Magnetic signals of typical vehicles are researched in order to extract features and recognize targets. In order to enhance the recognition speed, a technique of improved Support Vector Machine is used for recognition. The system and the algorithm have been used for detection and recognition of magnetic signals of vehicle targets in outdoor environment. From experimental results, it can be proven that magnetic signals of moving ground vehicles can be sensed by the MEMS magnetic sensor and detected by the detection system and the detection algorithm. The feature extraction of target magnetic signal is correct and SVM classifier is effective to solve the recognition problem for moving ground targets.	algorithm;feature extraction;microelectromechanical systems;sensor;short-time fourier transform;signal processing;succession;support vector machine	Jinhui Lan;Yuqiao Shi	2009	2009 4th IEEE International Conference on Nano/Micro Engineered and Molecular Systems	10.1109/NEMS.2009.5068605	support vector machine;magnetic resonance imaging;signal processing	Robotics	38.40249717129783	-33.7530730678164	120809
2a5b4e7640008dd0605a25b0c06d74351a9da159	using supervised learning to improve monte carlo integral estimation	supervised learning;quasi monte carlo;importance sampling;numerical analysis;monte carlo integration;monte carlo;uncertainty quantification;cross validation;fitness function;statistical computing	Monte Carlo (MC) techniques are often used to estimate integrals of a multivariate function using randomly generated samples of the function. In light of the increasing interest in uncertainty quantification and robust design applications in aerospace engineering, the calculation of expected values of such functions (e.g. performance measures) becomes important. However, MC techniques often suffer from high variance and slow convergence as the number of samples increases. In this paper we present Stacked Monte Carlo (StackMC), a new method for post-processing an existing set of MC samples to improve the associated integral estimate. StackMC is based on the supervised learning techniques of fitting functions and cross validation. It should reduce the variance of any type of Monte Carlo integral estimate (simple sampling, importance sampling, quasi-Monte Carlo, MCMC, etc.) without adding bias. We report on an extensive set of experiments confirming that the StackMC estimate of an integral is more accurate than both the associated unprocessed Monte Carlo estimate and an estimate based on a functional fit to the MC samples. These experiments run over a wide variety of integration spaces, numbers of sample points, dimensions, and fitting functions. In particular, we apply StackMC in estimating the expected value of the fuel burn metric of future commercial aircraft and in estimating sonic boom loudness measures. We compare the efficiency of StackMC with that of more standard methods and show that for negligible additional computational cost significant increases in accuracy are gained. Nomenclature b Bias of M Dx Set of samples of f (x) Dx(i) ith Sample of f (x) Dx Subset of Samples in the ith Testing Set Dx Subset of Samples in the ith Training Set E[·] Expected Value f (x) Objective Function f̂ True Expected Value of f (x) f̂M Estimate of f̂ from M f (Dx(i)) Function Value at the ith sample g(x) Fitting Algorithm gi(x) ith Fit to f (x) gi ( Dx(i) ) Prediction of Fit gi at Dx(i) ĝ Expected Value of g(x) k Number of Folds 1 ar X iv :1 10 8. 48 79 v1 [ st at .M L ] 2 4 A ug 2 01 1 Li Likelihood of the Expected Value of the ith Fold M An Estimator of f̂ mi Number of Samples in the ith Testing Set N Number of Samples Ni Number of Samples in the ith Training Set p(x) Probability Distribution of x q(x) Alternate Sample Distribution r(x) Fitting Algorithm for p(x) v Variance of M x Input Parameters α Free Parameter of StackMC β Free Parameter in the Fitting Algorithm ρ Correlation σ Standard Deviation	algorithm;computation;computational complexity theory;cross-validation (statistics);curve fitting;emoticon;experiment;exponent bias;importance sampling;linear least squares (mathematics);loss function;markov chain monte carlo;monte carlo method;procedural generation;quasi-monte carlo method;sampling (signal processing);sonic boom;supervised learning;uncertainty quantification;video post-processing	Brendan Tracey;David H. Wolpert;Juan J. Alonso	2011	CoRR		quasi-monte carlo method;econometrics;mathematical optimization;uncertainty quantification;numerical analysis;importance sampling;supervised learning;monte carlo integration;fitness function;cross-validation;statistics;monte carlo method	ML	34.45883630917924	-26.397193945435937	120948
173ebfaa4091926e336569674d5d39d44dc2e954	gaussian process regression for structured data sets		Approximation algorithms are widely used in many engineer- ing problems. To obtain a data set for approximation a factorial design of experiments is often used. In such case the size of the data set can be very large. Therefore, one of the most popular algorithms for approxima- tion — Gaussian Process regression — can hardly be applied due to its computational complexity. In this paper a new approach for a Gaussian Process regression in case of a factorial design of experiments is pro- posed. It allows to efficiently compute exact inference and handle large multidimensional and anisotropic data sets.	kriging	Mikhail Belyaev;Evgeny Burnaev;Yermek Kapushev	2015		10.1007/978-3-319-17091-6_6	econometrics;mathematical optimization;mathematics;statistics	ML	29.338344047881026	-28.372272442593566	122143
ac84df3884865a9bdc8f5154c9bed369d3f0e1cd	ekens: a learning on nonlinear blindly mixed signals	nonlinear mapping;learning algorithm;independent component analysis blind source separation learning artificial intelligence multilayer perceptrons gradient methods series mathematics statistical distributions parameter estimation;blind source separation;multilayer perceptrons;polynomials probability density function probability distribution distribution functions random variables information technology kernel cancer vectors gaussian distribution;kernel density;multilayer perceptron;independent component analysis;statistical distributions;291700 communications technologies;series mathematics;linear ica nonlinear blindly mixed signals blind source separation equivariant kernel nonlinear separation natural gradient algorithm gram charlier series nonlinear mapping statistical distributions kernel density distribution estimation nonlinear generative multilayer perceptron analysis learning algorithm linear independent component analysis;gradient methods;700302 telecommunications;parameter estimation;learning artificial intelligence;statistical distribution	We present experimental results of the blind separation of independent sources from their nonlinear mixtures. The proposed EKENS (equivariant kernel nonlinear separation) algorithm is a generalization of a natural gradient algorithm and the Gram-Charlier series, which is extended in two ways: (1) to deal with nonlinear mapping; (2) to be able to adapt to the actual statistical distributions of the sources by estimating the kernel density distribution at the output signals. The observations are modelled based on nonlinear generative multilayer perceptron analysis. The theory of the EKENS learning algorithm is discussed. Simulations show that the EKENS algorithm is able to find the underlying sources from the observation, even though the data generating mapping is nonlinear and unknown.	algorithm;blind signal separation;gradient;information geometry;kernel (operating system);multilayer perceptron;nonlinear system	Wai Yie Leong;John Homer	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415950	probability distribution;computer science;machine learning;pattern recognition;mathematics;statistics	Robotics	30.074150063616713	-32.034457154847935	122289
e84e94c52ad6a475bb1201d444953e8b81305e2a	gaussian flow sigma point filter for nonlinear gaussian state-space models		We propose a deterministic recursive algorithm for approximate Bayesian filtering. The proposed filter uses a function referred to as the approximate Gaussian flow transformation that transforms a Gaussian prior random variable into an approximate posterior random variable. Given a Gaussian filter prediction distribution, the succeeding filter prediction is approximated as Gaussian by applying sigma point moment-matching to the composition of the Gaussian flow transformation and the state transition function. This requires linearising the measurement model at each sigma point, solving the linearised models analytically, and introducing the measurement information gradually to improve the linearisation points progressively. Computer simulations show that the proposed method can provide higher accuracy and better posterior covariance matrix approximation than some state-of-the art computationally light approximative filters when the measurement model function is nonlinear but differentiable and the noises are additive and Gaussian. We also present a highly nonlinear scenario where the proposed filter occasionally diverges. In the accuracy-computational complexity axis the proposed algorithm is between Kalman filter extensions and Monte Carlo methods.	apache axis;approximation algorithm;bayesian network;computational complexity theory;computer simulation;finite-state machine;kalman filter;monte carlo method;nonlinear system;numerical analysis;numerical integration;recursion (computer science);singular value decomposition;solver;state space;state transition table;utility functions on indivisible goods;wave packet	Henri Nurminen;Robert Piché;Simon J. Godsill	2017	2017 20th International Conference on Information Fusion (Fusion)	10.23919/ICIF.2017.8009682	gaussian filter;gaussian function;ensemble kalman filter;gaussian random field;invariant extended kalman filter;extended kalman filter;mathematical optimization;gaussian process;mathematics;gaussian blur	Robotics	38.103587837906154	-25.756150114032536	122607
118b3ae69ab1aec93398e43e726af0c8cc2670e7	pattern generation using likelihood inference for cellular automata	neighborhood selection;stochastic automaton;discrete dynamical system;mdl criterion;stochastic cellular automata binary patterns cellular automata maximum likelihood estimation minimum description length principle neighborhood selection rule estimation;model selection;maximum likelihood;binary image;systeme discret;noisy data;pattern generation;maximum vraisemblance;automata estocastico;selection modele;automata stochastic processes maximum likelihood estimation lattices statistics pattern recognition councils stochastic systems stochastic resonance genetic algorithms;indexing terms;maximum likelihood estimation;critere mdl;dynamical system;systeme dynamique;automate stochastique;stochastic cellular automata;maximum likelihood estimate;seleccion modelo;automate cellulaire;image binaire;binary patterns;imagen binaria;minimum description length principle;algorithms artificial intelligence biomimetics cell physiology computer simulation image enhancement image interpretation computer assisted information storage and retrieval likelihood functions models statistical pattern recognition automated signal processing computer assisted;binary images pattern generation likelihood inference cellular automata discrete dynamical systems maximum likelihood estimation minimum description length principle;rule estimation;mdl criterio;sistema dinamico;sistema discreto;cellular automata;cellular automata maximum likelihood estimation;cellular automaton;maxima verosimilitud;discrete system;automata celular	Cellular automata are discrete dynamical systems which evolve on a discrete grid. Recent studies have shown that cellular automata with relatively simple rules can produce highly complex patterns. We develop likelihood-based methods for estimating rules of cellular automata aimed at the re-generation of observed regular patterns. Under noisy data, our approach is equivalent to estimating the local map of a stochastic cellular automaton. Direct computations of the maximum likelihood estimates are possible for regular binary patterns. The likelihood formulation of the problem is congenial with the use of the minimum description length principle as a model selection tool. We illustrate our method with a series of examples using binary images.	automata theory;binary image;cardiac arrest;computation;dynamical system;estimated;grayscale;inference;large;minimum description length;model selection;rule (guideline);signal-to-noise ratio;stochastic cellular automaton	Radu V. Craiu;Thomas C. M. Lee	2006	IEEE Transactions on Image Processing	10.1109/TIP.2006.873472	stochastic cellular automaton;cellular automaton;combinatorics;continuous spatial automaton;quantum finite automata;computer science;asynchronous cellular automaton;machine learning;ω-automaton;mathematics;maximum likelihood;algorithm;statistics	Comp.	34.500477810919705	-27.960830988960492	122750
de88263f2bab34e0671b4391669138a275f2f617	veiled attributes of the variational autoencoder		Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds obscured by gross corruptions. However, this previously unexplored feature comes with the cost of potential model collapse to a degenerate distribution that may be less suitable as the basis for generating new samples.	application domain;approximation;autoencoder;cobham's thesis;dimensionality reduction;generative model;information theory;maxima and minima;nonlinear system;stochastic process;variational principle	Bin Dai;Yu Wang;John Aston;Gang Hua;David P. Wipf	2017	CoRR		autoencoder;mathematics;artificial intelligence;pattern recognition	ML	27.351738751459013	-28.79239298153629	122962
e152e92d74d1f2375af103ffb168091bf77c5918	automatic kernel regression modelling using combined leave-one-out test score and regularised orthogonal least squares	kernel regression;ridge regression;regularisation;structure identification;endnotes;orthogonal least square;model evaluation;sum of squares;model development;nonlinear identification;pubications;cross validation;orthogonal forward regression;leave one out	This paper introduces an automatic robust nonlinear identification algorithm using the leave-one-out test score also known as the PRESS (Predicted REsidual Sums of Squares) statistic and regularised orthogonal least squares. The proposed algorithm aims to achieve maximised model robustness via two effective and complementary approaches, parameter regularisation via ridge regression and model optimal generalisation structure selection. The major contributions are to derive the PRESS error in a regularised orthogonal weight model, develop an efficient recursive computation formula for PRESS errors in the regularised orthogonal least squares forward regression framework and hence construct a model with a good generalisation property. Based on the properties of the PRESS statistic the proposed algorithm can achieve a fully automated model construction procedure without resort to any other validation data set for model evaluation.	algorithm;computation (action);controllers;goto;index;kernel (operating system);least squares;mandibular right second molar tooth;model selection;nonlinear system;press statistic;population parameter;recursion;resultant;rupture of appendix;signal processing;sparse matrix;statistic (data);subgroup;the matrix	Xia Hong;Sheng Chen;Paul M. Sharkey	2004	International journal of neural systems	10.1142/S0129065704001875	generalized least squares;principal component regression;total least squares;kernel regression;simple linear regression;econometrics;press statistic;computer science;machine learning;residual sum of squares;mathematics;explained sum of squares;non-linear least squares;tikhonov regularization;cross-validation;linear least squares;nonlinear regression;statistics	ML	30.02866853321516	-26.592777435466576	122964
199b705508c68d875a88ae3ab5be5911f536d30b	robust dynamic trajectory regression on road networks: a multi-task learning framework	training;trajectory regression;acceleration;trajectory;structured sparsity;roads;multi task learning;robustness;optimization;dynamic;data models	Trajectory regression, which aims to predict the travel time of arbitrary trajectories on road networks, attracts significant attention in various applications of traffic systems these years. In this paper, we tackle this problem with a multitask learning (MTL) framework. To take the temporal nature of the problem into consideration, we divide the regression problem into a set of sub-tasks of distinct time periods, then the problem can be treated in a multi-task learning framework. Further, we propose a novel regularization term in which we exploit the block sparse structure to augment the robustness of the model. In addition, we incorporate the spatial smoothness over road links and thus achieve a spatial-temporal framework. An accelerated proximal algorithm is adopted to solve the convex but non-smooth problem, which will converge to the global optimum. Experiments on both synthetic and real data sets demonstrate the effectiveness of the proposed method.	algorithm;computer multitasking;converge;experiment;global optimization;matrix regularization;multi-task learning;reinforcement learning;sparse matrix;synthetic data	Aiqing Huang;Linli Xu;Yitan Li;Enhong Chen	2014	2014 IEEE International Conference on Data Mining	10.1109/ICDM.2014.132	acceleration;data modeling;multi-task learning;mathematical optimization;simulation;computer science;trajectory;machine learning;data mining;robustness	Robotics	28.887163402505948	-34.7119486664489	122984
578fb1db6229c0b990d577d109f9eb52bd629e44	an overview of particle methods for random finite set models	bearings only measurements;sequential monte carlo estimation;random set models;signal processing;target tracking;stochastic nonlinear filtering	This overview paper describes the particle methods developed for the implementation of the a class of Bayes filters formulated using the random finite set formalism. It is primarily intended for the readership already familiar with the particle methods in the context of the standard Bayes filter. The focus in on the Bernoulli particle filter, the probability hypothesis density (PHD) particle filter and the generalised labelled multi-Bernoulli (GLMB) particle filter. The performance of the described filters is demonstrated in the context of bearings-only target tracking application.	bernoulli polynomials;particle filter;semantics (computer science)	Branko Ristic;Michael Beard;Claudio Fantacci	2016	Information Fusion	10.1016/j.inffus.2016.02.004	computer vision;econometrics;mathematical optimization;particle filter;auxiliary particle filter;computer science;signal processing;mathematics;statistics	Vision	39.005344140094266	-26.599279927776116	122996
6ab81bb6ef6a5c078852e0b4ec4c8384ff10ecc5	accelerating pseudo-marginal mcmc using gaussian processes	gaussian processes;010405 statistical theory;010400 statistics;pseudo marginal methods;010401 applied statistics;markov processes;010406 stochastic analysis and modelling;likelihood free methods;particle markov chain monte carlo;state space models	The grouped independence Metropolis-Hastings (GIMH) and Markov chain within Metropolis (MCWM) algorithms are pseudo-marginal methods used to perform Bayesian inference in latent variable models. These methods replace intractable likelihood calculations with unbiased estimates within Markov chain Monte Carlo algorithms. The GIMH method has the posterior of interest as its limiting distribution, but suffers from poor mixing if it is too computationally intensive to obtain high-precision likelihood estimates. The MCWM algorithm has better mixing properties, but tends to give conservative approximations of the posterior and is still expensive. A new method is developed to accelerate the GIMH method by using a Gaussian process (GP) approximation to the log-likelihood and train this GP using a short pilot run of the MCWM algorithm. This new method called GP-GIMH is illustrated on simulated data from a stochastic volatility and a gene network model. The new approach produces reasonable posterior approximations in these examples with at least an order of magnitude improvement in computing time. Code to implement the method for the gene network example can be found at http://www.runmycode.org/companion/view/2663.		Christopher C. Drovandi;Matthew T. Moores;Richard J. Boys	2018	Computational Statistics & Data Analysis	10.1016/j.csda.2017.09.002	econometrics;markov chain;mathematical optimization;markov kernel;coupling from the past;particle filter;markov chain monte carlo;markov property;marginal likelihood;mathematics;markov renewal process;additive markov chain;markov process;markov chain mixing time;markov model;hidden markov model;statistics;variable-order markov model	ML	27.897558848215862	-28.058878714628438	123055
b3a928b19dbf36be9623155e3cc3a63ddd86e3f8	selective attention mechanisms in a vision system based on neural networks	vision system;object recognition;bottom up;theoretical framework;top down;multilayer perceptrons;machine vision layout humans visual system data analysis information analysis artificial neural networks backpropagation algorithms sampling methods object recognition;object recognition selective attention mechanisms vision system neural networks attentional spotlight fixed dimension icon bottom up path bottom up path five layer artificial neural network backpropagation algorithm;human visual system;backpropagation algorithm;selective attention;artificial neural network;neural network	A system for visual recognition derived from a previously developed theoretical framework on the overall organization of the human visual system is proposed. The system operates dynamically by analyzing different parts of the input scene at variable levels of resolution through an attentional spotlight. A constant amount of information is gathered from the scene and a fixed dimension icon is produced, so that a trade-off occurs between the extension of the examined area and the level of resolution at which data are analyzed. The position of the spotlight and its dimensions are determined on the basis of the evolution of the recognition process. The icon is processed by a bottom-up path composed of a five-layer artificial neural network. The results of this net are analyzed by a planning module which determines if recognition has been achieved, or which action to undertake next. A top-down path, including a set of nets trained by the backpropagation algorithm, evaluates the parameters of the next sampling of information. The application of the system to object recognition with varying viewpoint and range from the camera is investigated.	artificial neural network	Michele Rucci;Paolo Dario	1993		10.1109/IROS.1993.583872	computer vision;computer science;artificial intelligence;machine learning;top-down and bottom-up design;time delay neural network;artificial neural network	AI	37.30680423058075	-36.6973165908094	123182
31513081fdd1f7f646e265aa8f5f61cc5153db75	pairwise likelihood estimation for multivariate mixed poisson models generated by gamma intensities	multinomial distribution;change detection;image processing;maximum likelihood;traitement du signal et de l image;multivariate mixed poisson models;gamma distribution;method of moment;negative multinomial distributions;synthetic data;multivariate gamma distributions;asymptotic normal;poisson model;pairwise likelihood estimation	Estimating the parameters of multivariate mixed Poisson models is an important problem in image processing applications, especially for active imaging or astronomy. The classical maximum likelihood approach cannot be used for these models since the corresponding masses cannot be expressed in a simple closed form. This paper studies a maximum pairwise likelihood approach to estimate the parameters of multivariate mixed Poisson models when the mixing distribution is a multivariate Gamma distribution. The consistency and asymptotic normality of this estimator are derived. Simulations conducted on synthetic data illustrate these results and show that the proposed estimator outperforms classical estimators based on the method of moments. An application to change detection in low-flux images is also investigated.	computer simulation;image processing;synthetic data	Florent Chatelain;Sophie Lambert-Lacroix;Jean-Yves Tourneret	2009	Statistics and Computing	10.1007/s11222-008-9092-9	gamma distribution;econometrics;multivariate statistics;normal-wishart distribution;expectation–maximization algorithm;image processing;pattern recognition;generalized linear model;mathematics;restricted maximum likelihood;poisson regression;maximum likelihood;poisson distribution;maximum likelihood sequence estimation;multivariate stable distribution;change detection;multinomial distribution;statistics;compound poisson process;synthetic data	ML	31.14632934097261	-24.766562184712974	123354
ace3b71f9467d38054564d9bf019c941b88e728b	comparison on confidence bands of decision boundary between svm and logistic regression	bootstrap method;sample size;classification algorithm;confidence band;bootstrap;support vector machines;logistic regression;bootstrap confidence bands of decision boundary support vector machine logistic regression;stability;support vector machines logistics support vector machine classification stability statistics fasteners statistical distributions classification algorithms covariance matrix training data;support vector machines covariance matrices pattern classification regression analysis stability;covariance matrices;pattern classification;regression analysis;confidence bands of decision boundary;decision boundary svm support vector machine logistic regression classification algorithm bootstrap methods stability sample size ratio central location covariance matrix confidence bands;support vector machine;covariance matrix	Support Vector Machine (SVM) and Logistic Regression (LR) are two popular classification models. The main purpose of a classification algorithm is to figure out the estimator for the decision boundary. In this paper, we considered confidence bands of decision boundary generated from SVM and LR. Confidence bands of decision boundary are estimated through bootstrap methods. We compared the confidence band estimator of SVM with the estimator of the conventional LR. Our main result is that sample size of the observations makes effect on the stability of both SVM and LR, sample size ratio, central location and covariance matrix of the data bring less effects on the stability of SVM than that of LR.	algorithm;booting;decision boundary;lr parser;logistic regression;resampling (statistics);support vector machine	Xing Wang;Xin Wang;Zhaonan Sun	2009	2009 Fifth International Joint Conference on INC, IMS and IDC	10.1109/NCM.2009.281	support vector machine;computer science;machine learning;pattern recognition;statistics	ML	32.56900897517004	-24.936759035290553	123415
3950431cf8628a9076c6eba3129d4f2bba0ba090	maximum likelihood model selection for 1-norm soft margin svms with multiple parameters	modelizacion;ajustamiento modelo;fonction vraisemblance;selection problem;model selection;optimisation;kernel;problema seleccion;maximum likelihood support vector machines model selection regularization;gradient based optimization maximum likelihood model selection 1 norm soft margin svm binary classification gradient ascent logistic regression;analisis estadistico;support vector machines optimisation pattern classification regression analysis;optimizacion;support vector machines;analisis datos;maximum likelihood;complexite calcul;probabilidad condicional;analisis forma;probabilite conditionnelle;maximum vraisemblance;bayesian methods;prior distribution;1 norm soft margin svm;gradient ascent;intelligence artificielle;selection modele;maximum likelihood estimation;logistic regression;ley a priori;reduction donnee;funcion verosimilitud;regularization;ajustement modele;modelisation;classification a vaste marge;data analysis;regresion logistica;complejidad computacion;statistical learning;support vector machines kernel support vector machine classification maximum likelihood estimation logistics robustness optimization methods pattern recognition bayesian methods statistical learning;logistics;statistical analysis;seleccion modelo;computational complexity;model matching;regression logistique;analyse statistique;pattern classification;pattern recognition;artificial intelligence;support vector machine classification;reduccion datos;analyse donnee;robustness;regression analysis;optimization;binary classification;pattern analysis;clasificacion binaria;data reduction;inteligencia artificial;support vector machine;maquina ejemplo soporte;vector support machine;computational efficiency;modeling;conditional probability;classification binaire;maximum likelihood model selection;likelihood function;maxima verosimilitud;analyse forme;loi a priori;gradient based optimization;optimization methods;probleme selection	Adapting the hyperparameters of support vector machines (SVMs) is a challenging model selection problem, especially when flexible kernels are to be adapted and data are scarce. We present a coherent framework for regularized model selection of 1-norm soft margin SVMs for binary classification. It is proposed to use gradient-ascent on a likelihood function of the hyperparameters. The likelihood function is based on logistic regression for robustly estimating the class conditional probabilities and can be computed efficiently. Overfitting is an important issue in SVM model selection and can be addressed in our framework by incorporating suitable prior distributions over the hyperparameters. We show empirically that gradient-based optimization of the likelihood function is able to adapt multiple kernel parameters and leads to better models than four concurrent state-of-the-art methods.	approximation algorithm;binary classification;coherence (physics);computation;electronic supplementary materials;estimated;expectation–maximization algorithm;genetic selection;gradient descent;inference;likelihood functions;logistic regression;loss function;mathematical optimization;model selection;optimization problem;overfitting;probability;selection algorithm;sigmoid function;support vector machine;times ascent;benefit	Tobias Glasmachers;Christian Igel	2010	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2010.95	support vector machine;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;quasi-maximum likelihood;statistics	ML	29.250311981424975	-31.570102583011856	123563
98f3f88c309b5d8b10c22a973c844cc076d6bf8d	mixtures of regression models with incomplete and noisy data	maximum likelihood;mixtures of regression models;outliers;missing values;em algorithm	AbstractThe estimation of the mixtures of regression models is usually based on the normal assumption of components and maximum likelihood estimation of the normal components is sensitive to noise, outliers or high-leverage points. Missing values are inevitable in many situations and parameter estimates could be biased if the missing values are not handled properly. In this paper, we propose the mixtures of regression models for contaminated incomplete heterogeneous data. The proposed models provide robust estimates of regression coefficients varying across latent subgroups even under the presence of missing values. The methodology is illustrated through simulation studies and a real data analysis.	signal-to-noise ratio	Byoung Cheol Jung;Sooyoung Cheon;Hwa Kyung Lim	2018	Communications in Statistics - Simulation and Computation	10.1080/03610918.2017.1283700	econometrics;outlier;expectation–maximization algorithm;missing data;mathematics;maximum likelihood;statistics	ML	29.15219138288747	-24.806388138163516	123789
2b29d172c7a801d64d7d54718fd8504e94fe5f8e	multi-task classification with infinite local experts	kernel stick breaking process multi task learning classification expert dirichlet process;model selection;kernel;dirichlet process;support vector machines;kernel stick breaking process;probability density function;training;dirichlet process multitask classification infinite local experts multitask learning nonlinear classification kernel stick breaking process;ionosphere;data mining;feature space;classification;ieee;infinite local experts;multi task learning;kernel machine learning bayesian methods;pattern classification;pattern classification learning artificial intelligence;multitask classification;expert;learning artificial intelligence;multitask learning;nonlinear classification	We propose a multi-task learning (MTL) framework for non-linear classification, based on an infinite set of local experts in feature space. The usage of local experts enables sharing at the expert-level, encouraging the borrowing of information even if tasks are similar only in subregions of feature space. A kernel stick-breaking process (KSBP) prior is imposed on the underlying distribution of class labels, so that the number of experts is inferred in the posterior and thus model selection issues are avoided. The MTL is implemented by imposing a Dirichlet process (DP) prior on a layer above the task-dependent KSBPs.	computer multitasking;experiment;feature vector;kernel (operating system);linear classifier;model selection;multi-task learning;nonlinear system;supervised learning	Chunping Wang;Qi An;Lawrence Carin;David B. Dunson	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959897	multi-task learning;computer science;machine learning;pattern recognition;data mining;mathematics;statistics	Robotics	29.98106561533955	-32.408712532534	123805
e008bb6a881a9193d86dfaa948c7caa1c35695c8	a general strategy for hidden markov chain parameterisation in composite feature-spaces	correlacion;modelo markov oculto;chaine markov;cadena markov;analisis estadistico;modelo markov;melange loi probabilite;modele markov cache;hidden markov chain;hidden markov model;in series;modele markov variable cachee;mixed distribution;general techniques;independent component analysis;time series;probabilistic approach;feature space;baum welch;gaussian mixture model;markov model;hidden markov models;statistical analysis;enfoque probabilista;approche probabiliste;analyse statistique;serie temporelle;en serie;serie temporal;pattern recognition;analyse composante independante;mezcla ley probabilidad;feature selection;teoria mezcla;reconnaissance forme;correlation;modele markov;reconocimiento patron;analisis componente independiente;mixture theory;theorie melange;markov chain	A general technique for the construction of hidden Markov models (HMMs) from multiple-variable time-series observations in noisy experimental environments is set out. The proposed methodology provides an ICA-based feature-selection technique for determining the number, and the transition sequence, of underlying hidden states, along with the statistics of the observed-state emission characteristics. In retaining correlation information between features, the method is potentially far more general than Gaussian mixture model HMM parameterisation methods such as Baum-Welch re-estimation, to which we demonstrate our method reduces when an arbitrary separation of features, or an experimentally-limited feature-space is imposed.	hidden markov model;markov chain;spaces	David Windridge;Richard Bowden;Josef Kittler	2004		10.1007/978-3-540-27868-9_118	independent component analysis;econometrics;markov chain;feature vector;computer science;baum–welch algorithm;machine learning;hidden semi-markov model;time series;pattern recognition;mixture model;mathematics;markov model;correlation;hidden markov model;statistics	ML	31.23929971845633	-32.532759625586266	124030
34cfadea9d39d608c72fa93774d6e4f963ef4c68	efficient parallel estimation for markov random fields		We present a new , deterministic, distributed MAPes­ timation algorithm for Markov Random Fields called Local Highest Confidence First (Local HCF). The al­ gorithm has been applied to segmentation problems in computer vision and its performance compared with stochastic algorithms. The experiments show that Local HCF finds better estimates than stochas­ tic algorithms with much less computation.	algorithm;computation;computer vision;experiment;halt and catch fire;markov chain;markov random field;timation	Michael J. Swain;Lambert E. Wixson;Paul B. Chou	1989			markov chain;mathematical optimization;markov kernel;random field;markov property;computer science;machine learning;mathematics;markov algorithm;markov process;markov model;statistics;variable-order markov model	Vision	34.497811958872525	-33.161067082683545	124169
2a922fead4463324549d3256553f9999cc90c0a9	a generalized kernel approach to structured output learning	structured outputs;kernel dependency estimation;function valued rkhs;operator valued kernel	We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. Our formulation overcomes the two main limitations of the original KDE approach, namely the decoupling between outputs in the image space and the inability to use a joint feature space. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach on three structured output problems, and compare it to the state-of-the-art kernelbased structured output regression methods.	coupling (computer programming);feature vector;interaction;kernel (operating system);structured prediction	Hachem Kadri;Mohammad Ghavamzadeh;Philippe Preux	2013			kernel;principal component regression;kernel regression;kernel method;mathematical optimization;string kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;machine learning;pattern recognition;graph kernel;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother	ML	28.064191438186914	-34.84424519178944	124316
6ff937436804ff079c2a7fd57a3c465eb9845381	tradeoff analysis between knowledge assessment approaches	adaptive testing;bayesian network;student model;bayesian inference;graphical models;irt;cat;graphical model;student models;bayesian networks	The problem of modeling and assessing an individual’s ability level is central to learning environments. Numerous approaches exists to this end. Computer Adaptive Testing (CAT) techniques, such as IRT and Bayesian posterior updating, are amongst the early approaches. Bayesian networks and graphs models are more recent approaches to this problem. These frameworks differ on their expressiveness and on their ability to automate model building and calibration with empirical data. We discuss the implication of expressiveness and data-driven properties of different frameworks, and analyze how it affects the applicability and accuracy of the knowledge assessment process. We conjecture that although expressive models such as Bayesian networks provide better cognitive diagnostic ability, their applicability, reliability, and accuracy is strongly affected by the knowledge engineering effort they require. We conclude with a comparative analysis of data driven approaches and provide empirical estimates of their respective performance for two data sets.	bayesian network;item response theory;knowledge engineering;qualitative comparative analysis	Michel C. Desmarais;Shunkai Fu;Xiaoming Pu	2005			computer-assisted translation;econometrics;variable-order bayesian network;computer science;machine learning;bayesian network;graphical model;statistics	AI	24.62040344626734	-25.15080614939395	124325
25fbdf097a2040785ec183b79ada0ca776c1ffb4	yggdrasil-a statistical package for learning split models	statistical package;statistical framework;split models;conditional independency;statistical inference;yggdrasil-a statistical package;specific context;graphical model;independencies holding;split model;specific value;markov property;context specific independence structure	There are two main objectives of this paper. The first is to present a statistical framework for models with context specific independence structures, i.e. conditional independencies holding only for specific values of the conditioning variables. This framework is constituted by the class of split models. Split models are an extension of graphical models for contingency tables and allow for a more sophisticated modelling than graphical models. The treatment of split models include estimation, representation and a Markov property for reading off those independencies holding in a specific context. The second objective is to present a software package named YGGDRASIL which is designed for statistical inference in split models, i.e. for learning such models on the basis of data.		Søren Højsgaard	2000			machine learning;artificial intelligence;markov property;contingency table;computer science;software;statistical inference;conditioning;graphical model	ML	26.3914767628693	-26.68852409522877	124401
72b8209d9d804b880b76dc594ed66fbefe11eec5	recursive algorithms for approximating probabilities in graphical models	graphical model;boltzmann machine;recursive algorithm;belief network	We develop a recursive node-elimination for-malismfor eeciently approximatinglarge prob-abilistic networks. No constraints are set on the network topologies. Yet the formalism can be straightforwardly integrated with exact methods whenever they are/become applicable. The approximations we use are controlled: they maintain consistently upper and lower bounds on the desired quantities at all times. We show that Boltzmann machines, sigmoid belief networks , or any combination (i.e., chain graphs) can be handled within the same framework. The accuracy of the methods is veriied experimentally .	approximation;bayesian network;boltzmann machine;experiment;graphical model;network topology;recursion (computer science);semantics (computer science);sigmoid function	Tommi S. Jaakkola;Michael I. Jordan	1996			boltzmann machine;mathematical optimization;combinatorics;computer science;machine learning;bayesian network;mathematics;graphical model;recursion	ML	25.452812444267945	-29.385893398567482	124514
07389de2f6d8ca155580eedb70f6e38c0a12dd5e	partially labeled classification with markov random walks	time scale;closed form solution;text classification;random walk	To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems.	algorithm;document classification;markov chain;matrix regularization;synthetic intelligence	Martin Szummer;Tommi S. Jaakkola	2001			closed-form expression;combinatorics;pattern recognition;mathematics;random walk;statistics	ML	27.603209328526447	-30.974402258764844	124538
268e5f1f5807709034116df91853d8d0067c28ab	non-convex penalized estimation in high-dimensional models with single-index structure	scad;minimax concave penalty;high dimensionality;shrinkage estimation;oracle property;index structure;62g20;link function;penalized least square;variable selection;data analysis;penalty method;penalized least squares;linear model;semi parametric model;simulation study;high dimensional variable selection;single index model;62h12	As promising alternatives to the LASSO, non-convex penalized methods, such as the SCAD and the minimax concave penalty method, produce asymptotically unbiased shrinkage estimates. By adopting non-convex penalties, in this paper we investigate uniformly variable selection and shrinkage estimation for several parametric and semi-parametric models with single-index structure. The new method does not need to estimate the involved nonparametric transformation or link function. The resulting estimators enjoy the oracle property even in the ''large p, small n'' scenario. The theoretical results for linear models are in parallel extended to general single-index models with no distribution constraint for the error at the cost of mild conditions on the predictors. Simulation studies are carried out to examine the performance of the proposed method and a real data analysis is also presented for illustration.	single-index model	Tao Wang;Peirong Xu;Lixing Zhu	2012	J. Multivariate Analysis	10.1016/j.jmva.2012.03.009	econometrics;mathematical optimization;single-index model;linear model;penalty method;mathematics;data analysis;feature selection;statistics	ML	29.296060543242934	-25.22612473712131	124677
d42fca2bd93bb30060091a17b3f03652a7097ebd	the two-state implicit filter recursive estimation for mobile robots	hidden markov models;kalman filters;robot sensing systems;bayes methods;mathematical model;mobile communication	This letter deals with recursive filtering for dynamic systems where an explicit process model is not easily devisable. Most Bayesian filters assume the availability of such an explicit process model, and thus may require additional assumptions or fail to properly leverage all available information. In contrast, we propose a filter that employs a purely residual-based modeling of the available information and thus achieves higher modeling flexibility. While this letter is related to the descriptor Kalman filter, it also represents a step toward batch optimization and allows the integration of further techniques, such as robust weighting for outlier rejection. We derive recursive filter equations that exhibit similar computational complexity when compared to their Kalman filter counterpart—the extended information filter. The applicability of the proposed approach is experimentally confirmed on two different real mobile robotic state estimation problems.	bayesian network;computational complexity theory;dynamical system;experiment;kalman filter;mathematical optimization;process modeling;recursion (computer science);recursive filter;rejection sampling;robot	Michael Bloesch;Michael Burri;Hannes Sommer;Roland Siegwart;Marco Hutter	2018	IEEE Robotics and Automation Letters	10.1109/LRA.2017.2776340	alpha beta filter;control theory;recursive bayesian estimation;invariant extended kalman filter;kernel adaptive filter;ensemble kalman filter;fast kalman filter;extended kalman filter;machine learning;adaptive filter;mathematics;artificial intelligence	Robotics	38.984107728847654	-26.633267129330317	124735
e8bcd54ac55570c2353c527ae4c8f2cd1366bf78	gaussian time error: a new index for fault detection in semiconductor processes	standards;sensors;semiconductor process modeling;indexes;indexes principal component analysis semiconductor device modeling semiconductor process modeling data models standards sensors;semiconductor device modeling;principal component analysis;semiconductor process fault detection multi way principal component analysis detection index;semiconductor technology fault diagnosis gaussian processes principal component analysis;square prediction error gaussian time error semiconductor processes fault detection index multiway principal component analysis principal spaces residual spaces gaussian model;data models	We present a new fault detection index, based on Multi-way Principal Component Analysis, which requires no selection of principal and residual spaces. This detection index is called Gaussian Time Error, since a Gaussian model is learned at each measurement instant. This index is used on real data from semiconductor processes to detect faults, providing a better detection than Square Prediction Error in the studied cases.	fault detection and isolation;multilinear principal component analysis;principal component analysis;relevance;semiconductor;software maintenance	Julien Marino;Francesco Rossi;Mustapha Ouladsine;Jacques Pinaton	2016	2016 American Control Conference (ACC)	10.1109/ACC.2016.7525416	database index;data modeling;electronic engineering;semiconductor device modeling;computer science;sensor;machine learning;statistics;principal component analysis	ML	35.75212732802099	-29.026186367890947	124815
7fd32a2597a8a15712fda86f0d1294f54397e1aa	generalized ridge estimator and model selection criteria in multivariate linear regression		Abstract We propose new model selection criteria based on generalized ridge estimators dominating the maximum likelihood estimator under the squared risk and the Kullback–Leibler risk in multivariate linear regression. Our model selection criteria have the following desirable properties: consistency, unbiasedness, and uniformly minimum variance. Consistency is proven under an asymptotic structure p ∕ n → c , where n is the sample size and p is the parameter dimension of the response variables. In particular, our proposed class of estimators dominates the maximum likelihood estimator under the squared risk, even when the model does not include the true model. Experimental results show that the risks of our model selection criteria are smaller than those based on the maximum likelihood estimator, and that our proposed criteria specify the true model under some conditions.	general linear model;model selection	Yuichi Mori;Taiji Suzuki	2018	J. Multivariate Analysis	10.1016/j.jmva.2017.12.006	mathematics;estimator;econometrics;minimax estimator;minimum-variance unbiased estimator;statistics;efficient estimator;bias of an estimator;model selection;estimation theory;mean squared error	ML	29.717577579707015	-24.416821762529743	125107
ea7fc8d1af0c638d739c9078b83ace5e486a7908	best: bayesian estimation of species trees under the coalescent model	species;estimacion;espece;tree;arbol;bioinformatique;modelo;estimation;especie;arbre;bayesian estimator;modele;bioinformatica;models;bioinformatics	UNLABELLED BEST implements a Bayesian hierarchical model to jointly estimate gene trees and the species tree from multilocus sequences. It provides a new option for estimating species phylogenies within the popular Bayesian phylogenetic program MrBayes. The technique of simulated annealing is adopted along with Metropolis coupling as performed in MrBayes to improve the convergence rate of the Markov Chain Monte Carlo algorithm.   AVAILABILITY http://www.stat.osu.edu/~dkp/BEST.	entity name part qualifier - adopted;estimated;hierarchical database model;list of phylogenetics software;markov chain monte carlo;metropolis;monte carlo algorithm;monte carlo method;rate of convergence;simulated annealing;trees (plant)	Liang Liu	2008	Bioinformatics	10.1093/bioinformatics/btn484	biology;econometrics;estimation;computer science;bioinformatics;tree;statistics	ML	32.25393271265094	-26.978384630059928	125270
1ee011c73e292fb25e682e79b4219138dc853b70	community detection and classification in hierarchical stochastic blockmodels	social network services;community detection;classification;neuroscience;symmetric matrices;stochastic processes;stochastic blockmodel;clustering algorithms;robustness;inference algorithms;hierarchical random graphs	In disciplines as diverse as social network analysis and neuroscience, many large graphs are believed to be composed of loosely connected smaller graph primitives, whose structure is more amenable to analysis We propose a robust, scalable, integrated methodology for community detection and community comparison in graphs. In our procedure, we first embed a graph into an appropriate Euclidean space to obtain a low-dimensional representation, and then cluster the vertices into communities. We next employ nonparametric graph inference techniques to identify structural similarity among these communities. These two steps are then applied recursively on the communities, allowing us to detect more fine-grained structure. We describe a hierarchical stochastic blockmodel—namely, a stochastic blockmodel with a natural hierarchical structure—and establish conditions under which our algorithm yields consistent estimates of model parameters and motifs, which we define to be stochastically similar groups of subgraphs. Finally, we demonstrate the effectiveness of our algorithm in both simulated and real data. Specifically, we address the problem of locating similar sub-communities in a partially reconstructed Drosophila connectome and in the social network Friendster.	algorithm;approximation algorithm;computation;drosophila connectome;graph (discrete mathematics);graphic art software;induced subgraph;random graph;recursion;scalability;sequence motif;situated;social network analysis;stochastic block model;structural similarity;time complexity	Vince Lyzinski;Minh Tang;Avanti Athreya;Youngser Park;Carey E. Priebe	2017	IEEE Transactions on Network Science and Engineering	10.1109/TNSE.2016.2634322	stochastic process;mathematical optimization;combinatorics;biological classification;computer science;theoretical computer science;machine learning;mathematics;cluster analysis;programming language;statistics;robustness;symmetric matrix	ML	27.87166732642032	-32.62270578061634	125309
a43838c0d7548574c791dc120dcc172cdcb8bdeb	bandwidth selection for backfitting estimation of semiparametric additive models: a simulation study	probability theory and statistics;semiparametric additive model;statistik;backfitting estimation;bandwidth selection;statistics;nonparametric regression;sannolikhetsteori och statistik;mean squared error	A data-driven bandwidth selection method for backfitting estimation of semiparametric additive models, when the parametric part is of main interest, is proposed. The proposed method is a double smoothing estimator of the mean-squared error of the backfitting estimator of the parametric terms. The performance of the proposed method is evaluated and compared with existing bandwidth selectors by means of a simulation study. © 2013 Elsevier B.V. All rights reserved.	additive model;mean squared error;semiparametric model;simulation;smoothing	Jenny Häggström	2013	Computational Statistics & Data Analysis	10.1016/j.csda.2013.01.010	econometrics;pattern recognition;mathematics;mean squared error;backfitting algorithm;nonparametric regression;semiparametric model;statistics;semiparametric regression	AI	29.90786163808337	-24.131966496182336	125342
935c880c2babd997d6b5836e867913660b525845	a power variance test for nonstationarity in complex-valued signals	statistical analysis fast fourier transforms signal processing;bootstrap;oceanography stochastic processes time series analysis nonstationary processes bootstrap;computational modeling trajectory discrete fourier transforms fast fourier transforms heuristic algorithms frequency domain analysis monte carlo methods;stochastic processes;time series analysis;nonstationary processes;fluid dynamics power variance test signal nonstationarity complex valued signals fast fourier transform second order structure stationary signals bootstrap method turbulent flow data;oceanography	We propose a novel algorithm for testing the hypothesis of nonstationarity in complex-valued signals. The implementation uses both the bootstrap and the Fast Fourier Transform such that the algorithm can be efficiently implemented in O(NlogN) time, where N is the length of the observed signal. The test procedure examines the second-order structure and contrasts the observed power variance -- i.e. the variability of the instantaneous variance over time -- with the expected characteristics of stationary signals generated via the bootstrap method. Our algorithmic procedure is capable of learning different types of nonstationarity, such as jumps or strong sinusoidal components. We illustrate the utility of our test and algorithm through application to turbulent flow data from fluid dynamics.	algorithm;booting;bootstrapping (statistics);fast fourier transform;heart rate variability;stationary process;turbulence	Thomas E. Bartlett;Adam M. Sykulski;Sofia C. Olhede;Jonathan M. Lilly;Jeffrey J. Early	2015	2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2015.122	stochastic process;econometrics;mathematical optimization;computer science;time series;mathematics;statistics	Robotics	36.919226382476175	-25.72298123029408	125435
2a748c7daa6ba4575c4bf86a905a2e8dcc0d3c03	robust-bd estimation and inference for general partially linear models		The classical quadratic loss for the partially linear model (PLM) and the likelihood function for the generalized PLM are not resistant to outliers. This inspires us to propose a class of “robust-Bregman divergence (BD)” estimators of both the parametric and nonparametric components in the general partially linear model (GPLM), which allows the distribution of the response variable to be partially specified, without being fully known. Using the local-polynomial function estimation method, we propose a computationally-efficient procedure for obtaining “robust-BD” estimators and establish the consistency and asymptotic normality of the “robust-BD” estimator of the parametric component βo. For inference procedures of βo in the GPLM, we show that the Wald-type test statistic Wn constructed from the “robust-BD” estimators is asymptotically distribution free under the null, whereas the likelihood ratio-type test statistic Λn is not. This provides an insight into the distinction from the asymptotic equivalence (Fan and Huang 2005) between Wn and Λn in the PLM constructed from profile least-squares estimators using the non-robust quadratic loss. Numerical examples illustrate the computational effectiveness of the proposed “robust-BD” estimators and robust Wald-type test in the appearance of outlying observations.	blu-ray;bregman divergence;computation;computational complexity theory;least squares;linear model;numerical method;polynomial;turing completeness	Chunming Zhang;Zhengjun Zhang	2017	Entropy	10.3390/e19110625	mathematics;statistics;estimator;mathematical optimization;test statistic;likelihood function;wald test;generalized linear model;parametric statistics;asymptotic distribution;nonparametric statistics	ML	29.974722223162853	-24.643043808522854	125467
c817e06a6212e7e41e97029b77305e5058fc087e	neural networks application for induction motor faults diagnosis	reseau kohonen;neural network application;induction motor fault;neural networks;induction motor;multilayer perceptron network;diagnostico;panne electrique moteur asynchrone;internal structure;multilayer perceptron;stator;electronica potencia;rotor;power electronics;electronique puissance;monitoring;estator;kohonen network;self organization;reseau perceptron multicouche;monitorage;reseau neuronal;monitoreo;diagnosis;red neuronal;induction motor faults;fault diagnosis;neural network;diagnostic	The paper deals with diagnosis problems of the induction motors in the case of rotor, stator and rolling bearing faults. Two kinds of neural networks (NN) were proposed for diagnostic purposes: multilayer perceptron networks and self organizing Kohonen networks. Neural networks were trained and tested using measurement data of stator current and mechanical vibration spectra. The efficiency of developed neural detectors was evaluated. Feedforward NN with very simple internal structure, used for the detection of all fault kinds, gave satisfactory results, which is very important in practical realization. Experiments with Kohonen networks indicated that they could be used for the initial classification of motor faults, as an introductory step before the proper neural detector based on multiplayer perceptron is used. The obtained results lead to a conclusion that neural detectors for rotor and stator faults as well as for rolling bearings and supply asymmetry faults can be developed based on measurement data acquired on-line in the drive system.	artificial neural network	Czeslaw T. Kowalski;Teresa Orlowska-Kowalska	2003	Mathematics and Computers in Simulation	10.1016/S0378-4754(03)00087-9	self-organization;rotor;artificial intelligence;machine learning;power electronics;induction motor;multilayer perceptron;artificial neural network	ML	36.329155132131255	-31.295231097031113	125737
76f86c540e07568a72d124aa8ae9fdce7edbc07c	autoregressive tensor factorization for spatio-temporal predictions		Analysis of spatio-temporal data is a common research topic that requires the interpolations of unknown locations and the predictions of feature observations by utilizing information about where and when the data were observed. One of the most difficult problems is to make predictions of unknown locations. Tensor factorization methods are popular in this field because of their capability of handling multiple types of spatio-temporal data, dealing with missing values, and providing computationally efficient parameter estimation procedures. However, unlike traditional approaches such as spatial autoregressive models, the existing tensor factorization methods have not tried to learn spatial autocorrelations. These methods employ previously inferred spatial dependencies, often resulting in poor performances on the problem of making interpolations and predictions of unknown locations. In this paper, we propose a new tensor factorization method that estimates low-rank latent factors by simultaneously learning the spatial and temporal autocorrelations. We introduce new spatial autoregressive regularizers based on existing spatial autoregressive models and provide an efficient estimation procedure. With experiments on publicly available traffic transporting data, we demonstrate that our proposed method significantly improves the predictive performances in our problems in comparison to the existing state-of-the-art spatio-temporal analysis methods.	algorithmic efficiency;autoregressive model;big data;estimation theory;experiment;interpolation;latent variable;missing data;nonlinear system;performance;sharing economy;spatial analysis;stationary process	Koh Takeuchi;Hisashi Kashima;Naonori Ueda	2017	2017 IEEE International Conference on Data Mining (ICDM)	10.1109/ICDM.2017.146	computer science;data mining;tensor;missing data;interpolation;autoregressive model;estimation theory;factorization;matrix decomposition;stress (mechanics)	Vision	28.840502368073626	-34.42126367401173	125930
2ff5af9c3c960a73206ea7795d4d084380a432a8	explicit probabilistic models for databases and networks	prior information;statistical significance;data type;satisfiability;data mining;artificial intelligent;probabilistic model;null model;maximum entropy;information theory	Recent work in data mining and related areas has highlighted the importance of the statistical assessment of data mining results. Crucial to this endeavour is the choice of a non-trivial null model for the data, to which the found patterns can be contrasted. The most influential null models proposed so far are defined in terms of invariants of the null distribution. Such null models can be used by computation intensive randomization approaches in estimating the statistical significance of data mining results. Here, we introduce a methodology to construct non-trivial probabilistic models based on the maximum entropy (MaxEnt) principle. We show how MaxEnt models allow for the natural incorporation of prior information. Furthermore, they satisfy a number of desirable properties of previously introduced randomization approaches. Lastly, they also have the benefit that they can be represented explicitly. We argue that our approach can be used for a variety of data types. However, for concreteness, we have chosen to demonstrate it in particular for databases and networks.	computation;convex optimization;data mining;database;endeavour (supercomputer);experiment;laptop;markov chain monte carlo;mathematical optimization;multinomial logistic regression;null model;principle of maximum entropy;randomized algorithm;randomness;sampling (signal processing)	Tijl De Bie	2009	CoRR		statistical model;null model;data type;information theory;computer science;artificial intelligence;principle of maximum entropy;machine learning;data mining;mathematics;statistical significance;statistics;satisfiability	ML	26.29197758771176	-27.598321158952494	126010
4e3f8c9952a7c7793330f3aee3e794e6ca08597f	a real-time interference monitoring technique for gnss based on a twin support vector machine method	interference monitoring;twin support vector machine;global navigation satellite system	Interferences can severely degrade the performance of Global Navigation Satellite System (GNSS) receivers. As the first step of GNSS any anti-interference measures, interference monitoring for GNSS is extremely essential and necessary. Since interference monitoring can be considered as a classification problem, a real-time interference monitoring technique based on Twin Support Vector Machine (TWSVM) is proposed in this paper. A TWSVM model is established, and TWSVM is solved by the Least Squares Twin Support Vector Machine (LSTWSVM) algorithm. The interference monitoring indicators are analyzed to extract features from the interfered GNSS signals. The experimental results show that the chosen observations can be used as the interference monitoring indicators. The interference monitoring performance of the proposed method is verified by using GPS L1 C/A code signal and being compared with that of standard SVM. The experimental results indicate that the TWSVM-based interference monitoring is much faster than the conventional SVM. Furthermore, the training time of TWSVM is on millisecond (ms) level and the monitoring time is on microsecond (μs) level, which make the proposed approach usable in practical interference monitoring applications.	algorithm;cns disorder;global positioning system;interference (communication);least squares;objectivity/db;real-time clock;real-time transcription;satellite navigation;support vector machine	Wutao Li;Zhigang Huang;Rongling Lang;Honglei Qin;Kai Zhou;Yongbin Cao	2016	Sensors	10.3390/s16030329	electronic engineering;telecommunications;engineering;remote sensing	ML	38.74825345948353	-33.905453523142214	126091
181bef55321b315c053d17800f248442f38d15fa	dynamic graphs, community detection, and riemannian geometry	community detection;dynamic graphs;riemannian geometry	A community is a subset of a wider network where the members of that subset are more strongly connected to each other than they are to the rest of the network. In this paper, we consider the problem of identifying and tracking communities in graphs that change over time – dynamic community detection – and present a framework based on Riemannian geometry to aid in this task. Our framework currently supports several important operations such as interpolating between and averaging over graph snapshots. We compare these Riemannian methods with entry-wise linear interpolation and find that the Riemannian methods are generally better suited to dynamic community detection. Next steps with the Riemannian framework include producing a Riemannian least-squares regression method for working with noisy data and developing support methods, such as spectral sparsification, to improve the scalability of our current methods.	least squares;linear interpolation;scalability;signal-to-noise ratio;strongly connected component	Craig Bakker;Mahantesh Halappanavar;Arun V. Sathanur	2018		10.1007/s41109-018-0059-2	snapshot (computer storage);interpolation;riemannian geometry;noisy data;scalability;linear interpolation;mathematical optimization;mathematics;graph;strongly connected component	Vision	29.904940324535385	-37.280206035935336	126489
00649e743e744c8046158fe988562bcb78982030	nonnegative dictionary learning in the exponential noise model for adaptive music signal representation		In this paper we describe a maximum likelihood approach for d ictionary learning in the multiplicative exponential noise model. This model i s prevalent in audio signal processing where it underlies a generative composit e model of the power spectrogram. Maximum joint likelihood estimation of the di ctionary and expansion coefficients leads to a nonnegative matrix factorizati on problem where the Itakura-Saito divergence is used. The optimality of this ap proach is in question because the number of parameters (which include the expansion coefficients) grows with the number of observations. In this paper we describe a v ari tional procedure for optimization of the marginal likelihood, i.e., the like lihood of the dictionary where the activation coefficients have been integrated out ( given a specific prior). We compare the output of both maximum joint likelihood estim ation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood es timation (MMLE) on real and synthetical datasets. The MMLE approach is shown t embed automatic model order selection, akin to automatic relevance de t rmination.	algorithm;audio signal processing;calculus of variations;coefficient;column (database);computational complexity theory;dictionary;dynamic music;experiment;itakura–saito distance;machine learning;marginal model;mathematical optimization;non-negative matrix factorization;relevance;spectrogram;timation;time complexity	Onur Dikmen;Cédric Févotte	2011			machine learning;pattern recognition;mathematics;likelihood function;quasi-maximum likelihood;maximum likelihood sequence estimation;statistics	ML	29.777804912801578	-33.32843118204854	127363
95fd628d894d932e4c48b409a5731ec9a1b18e67	a novel method of fault detection for solenoid valves based on vibration signal measurement	magnetic fields;vibrations;indexes;solenoids;noise reduction;electromagnetics;valves	Solenoid valves are vital pieces of equipment which have been widely used in a variety of industries. Fault detection methods of solenoid valves play important role for improving their reliability and maintenance efficiency due to their high failure rates in the field. Therefore, a novel method of fault detection for solenoid valves based on vibration signal measurement is proposed in this paper. The vibration sensor is place on the top of the solenoid valve and the vibration signal is collected by Labview SignalExpress system. The amplitude of the signal is extracted by denoising algorithm based on Wavelet and analyzed to detect the working state with the help of health index. The method is not interfered by the power supply system, and the method enhance the ability to resist interference. Experimental platform was set up to validate the proposed method by injecting faults into the solenoid valve. Experimental results show that the method proposed can provide not only indications of whether or not the valve is functioning, but also the information of how well the valve is functioning, thus empowering condition-based maintenance operations.	aharonov–bohm effect;algorithm;elegant degradation;experiment;failure cause;fault detection and isolation;interference (communication);labview;noise reduction;power supply;wavelet	Haifeng Guo;Kai Wang;He Cui;Aidong Xu;Jin Jiang	2016	2016 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)	10.1109/iThings-GreenCom-CPSCom-SmartData.2016.179	database index;embedded system;magnetic field;electromagnetism;computer science;vibration;noise reduction;solenoid	Robotics	37.825212817756416	-30.606340545762976	127494
324cd4bdb9227f73cb59981ef0ff181b999c5e23	assessing data mining results on matrices with randomization	data sharing;convergence;randomised algorithms;testing;general techniques;binary matrices;matrix algebra;measurement uncertainty;binary matrices data mining data analysis randomization approach;data mining;randomization approach;data analysis;randomised algorithms data analysis data mining matrix algebra;random matrices;principal component analysis;sparse matrices data mining testing measurement uncertainty convergence markov processes principal component analysis;missing values;markov processes;sparse matrices	Randomization is a general technique for evaluating the significance of data analysis results. In randomization-based significance testing, a result is considered to be interesting if it is unlikely to obtain as good result on random data sharing some basic properties with the original data. Recently, the randomization approach has been applied to assess data mining results on binary matrices and limited types of real-valued matrices. In these works, the row and column value distributions are approximately preserved in randomization. However, the previous approaches suffer from various technical and practical shortcomings. In this paper, we give solutions to these problems and introduce a new practical algorithm for randomizing various types of matrices while preserving the row and column value distributions more accurately. We propose a new approach for randomizing matrices containing features measured in different scales. Compared to previous work, our approach can be applied to assess data mining results on different types of real-life matrices containing dissimilar features, nominal values, non-Gaussian value distributions, missing values and sparse structure. We provide an easily usable implementation that does not need problematic manual tuning as theoretically justified parameter values are given. We perform extensive experiments on various real-life datasets showing that our approach produces reasonable results on practically all types of matrices while being easy and fast to use.	data mining;experiment;missing data;randomized algorithm;randomness;real life;sparse matrix	Markus Ojala	2010	2010 IEEE International Conference on Data Mining	10.1109/ICDM.2010.20	econometrics;convergence;sparse matrix;random matrix;machine learning;data mining;mathematics;software testing;markov process;data analysis;statistics;measurement uncertainty;principal component analysis	DB	28.190728163038877	-24.306737243668753	128072
02fdd00cde10a96efcfe42a1a8bf7851dffb287b	dissimilarity measures in feature space	signal processing statistical analysis gaussian distribution learning artificial intelligence estimation theory support vector machines;estimation theory;support vector machines;extraterrestrial measurements support vector machines probability density function signal processing algorithms character generation statistical distributions density measurement detection algorithms independent component analysis pattern recognition;dissimilarity measure;noisy data;probability density function;feature space;quantile estimation;signal processing applications dissimilarity measures feature space statistical behavior machine learning quantile estimation single class support vector machine svm fisher ratio radial gaussian probability density functions nonconnected quantiles noisy data sets;statistical analysis;machine learning;signal processing;support vector machine;learning artificial intelligence;gaussian distribution;asymptotic equivalence	We present a study of the statistical behavior of the dissimilarity measure, /spl Dscr//sub 5/ proposed previously (Desobry, F. and Davy, M., Proc. IEEE ICASSP, 2003), and which results from a machine learning-based quantile estimation approach, namely, a single-class support vector machine. This dissimilarity measure possesses the interesting property of being asymptotically equivalent to the Fisher ratio when dealing with radial Gaussian probability density functions. More generally, it can be efficiently applied to non-connected quantiles, and to noisy data sets, as outliers are taken into account by the SVM. A generalisation of /spl Dscr//sub 5/ is then proposed, which results in the design of a more general class of dissimilarity measures, also defined in feature space and with the same properties.	feature vector;international conference on acoustics, speech, and signal processing;machine learning;radial (radio);signal-to-noise ratio;support vector machine;waist–hip ratio	Frédéric Desobry;Manuel Davy	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1327150	support vector machine;computer science;machine learning;signal processing;pattern recognition;mathematics;statistics	Robotics	33.12735640739124	-32.47733765355204	128091
520c436b909f662e6c4e5aa76d0cd122e6a90800	gaussian process models for hrtf based sound-source localization and active-learning.		From a machine learning perspective, the human ability localize sounds can be modeled as a non-parametric and non-linear regression problem between binaural spectral features of sound received at the ears (input) and their sound-source directions (output). The input features can be summarized in terms of the individual’s head-related transfer functions (HRTFs) which measure the spectral response between the listener’s eardrum and an external point in 3D. Based on these viewpoints, two related problems are considered: how can one achieve an optimal sampling of measurements for training sound-source localization (SSL) models, and how can SSL models be used to infer the subject’s HRTFs in listening tests. First, we develop a class of binaural SSL models based on Gaussian process regression and solve a forward selection problem that finds a subset of inputoutput samples that best generalize to all SSL directions. Second, we use an active-learning approach that updates an online SSL model for inferring the subject’s SSL errors via headphones and a graphical user interface. Experiments show that only a small fraction of HRTFs are required for 5◦ localization accuracy and that the learned HRTFs are localized closer to their intended directions than non-individualized HRTFs.	binaural beats;gaussian process;graphical user interface;head-related transfer function;headphones;kriging;machine learning;nonlinear system;online and offline;randomized algorithm;sampling (signal processing);selection algorithm;simulation;stepwise regression;transport layer security	Yuancheng Luo;Dmitry N. Zotkin;Ramani Duraiswami	2015	CoRR		active learning;speech recognition;computer science;machine learning;artificial intelligence;headphones;transfer function;binaural recording;acoustic source localization;kriging;gaussian process	ML	33.86584468127243	-36.68860989441931	128105
1572cfd2ecf977865767ac17e0e4aff72c42d26a	geodesic gaussian kernels for value function approximation	reinforcement learning;least squares policy iteration;gaussian kernel;value function approximation;markov decision process;article	The least-squares policy iteration algorithm (Lagoudakis & Parr, JMLR 2003) works extremely well in value function approximation, given appropriate basis functions. Recent studies showed that value function approximation can be more effectively carried out by making use of the non-linear manifold structure of the state space induced by the Markov decision processes. In this paper, we propose using geodesic Gaussian kernels defined on the non-linear manifold for value function approximation. We show that geodesic Gaussian kernels have a number of preferable properties and compare favorably with other basis functions through numerical examples.	algorithm;approximation;basis function;bellman equation;gaussian blur;iteration;iterative method;journal of machine learning research;least squares;markov chain;markov decision process;nonlinear system;numerical analysis;state space	Masashi Sugiyama;Hirotaka Hachiya;Christopher Towell;Sethu Vijayakumar	2008	Auton. Robots	10.1007/s10514-008-9095-6	markov decision process;mathematical optimization;computer science;artificial intelligence;machine learning;pattern recognition;gaussian function;reinforcement learning;q-learning	ML	25.466763472358505	-31.645852874751736	128153
af06c0a6fd30e84b3448a9180ed9e08cd1d9428a	score-matching estimators for continuous-time point-process regression models	cost function;maximum likelihood estimation;computational modeling;writing;correlation;data models	We introduce a new class of efficient estimators based on score matching for probabilistic point process models. Unlike discretised likelihood-based estimators, score matching estimators operate on continuous-time data, with computational demands that grow with the number of events rather than with total observation time. Furthermore, estimators for many common regression models can be obtained in closed form, rather than by iteration. This new approach to estimation may thus expand the range of tractable models available for event-based data.	cobham's thesis;computation;discretization;iteration;point process	Maneesh Sahani;Gergo Bohner;Arne Meyer	2016	2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2016.7738848	econometrics;extremum estimator;pattern recognition;m-estimator;mathematics;statistics	ML	31.631799729334567	-26.760775210438364	128198
88250c7836375a7e2a46be4bedf8d54e8a08a35f	nonparametric estimation of multivariate scale mixtures of uniform densities	health research;uk clinical guidelines;rate of convergence;biological patents;minimax;multivariate;62g05;mixture;nonparametric estimation;maximum likelihood;europe pubmed central;citation search;62g20;62g07;maximum likelihood estimate;strong consistency;uk phd theses thesis;monotonicity;62f20;life sciences;uniform;smu;uk research reports;medical journals;consistency;lower bound;europe pmc;62h12;biomedical research;bioinformatics	Suppose that U = (U(1), … , U(d)) has a Uniform ([0, 1](d)) distribution, that Y = (Y(1), … , Y(d)) has the distribution G on [Formula: see text], and let X = (X(1), … , X(d)) = (U(1)Y(1), … , U(d)Y(d)). The resulting class of distributions of X (as G varies over all distributions on [Formula: see text]) is called the Scale Mixture of Uniforms class of distributions, and the corresponding class of densities on [Formula: see text] is denoted by [Formula: see text]. We study maximum likelihood estimation in the family [Formula: see text]. We prove existence of the MLE, establish Fenchel characterizations, and prove strong consistency of the almost surely unique maximum likelihood estimator (MLE) in [Formula: see text]. We also provide an asymptotic minimax lower bound for estimating the functional f ↦ f(x) under reasonable differentiability assumptions on f ∈ [Formula: see text] in a neighborhood of x. We conclude the paper with discussion, conjectures and open problems pertaining to global and local rates of convergence of the MLE.	estimated;fenchel's duality theorem;graph coloring;maximum likelihood estimation;minimax;strong consistency;density;mixture	Marios G. Pavlides;Jon A. Wellner	2012	Journal of multivariate analysis	10.1016/j.jmva.2012.01.001	econometrics;function composition;data mining;mathematics;maximum likelihood;statistics	ML	31.877564507202532	-24.51771071531658	128343
75df5bf6f26bba8dc83f5d685ad9f3ac65f2459f	metropolized independent sampling with comparisons to rejection sampling and importance sampling	asymptotic efficiency;delta method;eigenvalues and eigenvectors;markov chain monte carlo methods;relative efficiency;importance sampling;rejection sampling;metropolis hastings algorithm;total variation distance;markov chain	Although Markov chain Monte Carlo methods have been widely used in many disciplines, exact eigen analysis for such generated chains has been rare. In this paper, a special MetropolisHastings algorithm, Metropolized independent sampling, proposed first in Hastings (1970), is studied in full detail. The eigenvalues and eigenvectors of the corresponding Markov chain, as well as a sharp bound for the total variation distance between the nth updated distribution and the target distribution, are provided. Furthermore, the relationship between this scheme, rejection sampling, and importance sampling are studied with emphasis on their relative efficiencies. It is shown that Metropolized independent sampling is superior to rejection sampling in two respects: asymptotic efficiency and ease of computation.	algorithm;computation;eigen (c++ library);importance sampling;markov chain monte carlo;monte carlo method;rejection sampling;sampling (signal processing)	Jun S. Liu	1996	Statistics and Computing	10.1007/BF00162521	metropolis–hastings algorithm;econometrics;markov chain;mathematical optimization;delta method;multiple-try metropolis;coupling from the past;gibbs sampling;hybrid monte carlo;markov chain monte carlo;eigenvalues and eigenvectors;importance sampling;slice sampling;efficiency;mathematics;rejection sampling;umbrella sampling;total variation;statistics	ML	31.48182722487078	-24.683531102279872	128627
dd54fd11b782d5a4bd68cb22be7338dfa2315c00	feature-based steganalysis for jpeg images and its implications for future design of steganographic schemes	steganographie;analisis estadistico;transformation cosinus discrete;blind;probabilistic approach;classification;feature vector;detectabilidad;steganography;esteganografia;statistical analysis;detectabilite;detectability;discrete cosine transforms;enfoque probabilista;approche probabiliste;analyse statistique;clasificacion;ciego;aveugle	In this paper, we introduce a new feature-based steganalytic method for JPEG images and use it as a benchmark for comparing JPEG steganographic algorithms and evaluating their embedding mechanisms. The detection method is a linear classifier trained on feature vectors corresponding to cover and stego images. In contrast to previous blind approaches, the features are calculated as an L1 norm of the difference between a specific macroscopic functional calculated from the stego image and the same functional obtained from a decompressed, cropped, and recompressed stego image. The functionals are built from marginal and joint statistics of DCT coefficients. Because the features are calculated directly from DCT coefficients, conclusions can be drawn about the impact of embedding modifications on detectability. Three different steganographic paradigms are tested and compared. Experimental results reveal new facts about current steganographic methods for JPEGs and new design principles for more secure JPEG steganography.	algorithm;benchmark (computing);coefficient;discrete cosine transform;jpeg;linear classifier;marginal model;programming paradigm;steganalysis;steganography;t-norm;taxicab geometry	Jessica J. Fridrich	2004		10.1007/978-3-540-30114-1_6	computer vision;observability;feature vector;biological classification;computer science;theoretical computer science;pattern recognition;mathematics;steganography;statistics	ML	35.167197890329675	-37.518527100569315	128654
0f414ca03daf9f557028d250dad867a71ae65e0a	distributed low rank approximation of implicit functions of a matrix	probability approximation theory matrix algebra;servers principal component analysis approximation algorithms additives protocols partitioning algorithms computational modeling;additive error approximation distributed low rank approximation implicit functions rank k projection matrix probability gaussian kernel expansion m estimators	We study distributed low rank approximation in which the matrix to be approximated is only implicitly represented across the different servers. For example, each of s servers may have an n × d matrix A<sup>t</sup>, and we may be interested in computing a low rank approximation to A = f(Σ<sub>t=1</sub><sup>s</sup>A<sup>t</sup>), where f is a function which is applied entrywise to the matrix Σ<sub>t=1</sub><sup>s</sup>A<sup>t</sup>. We show for a wide class of functions f it is possible to efficiently compute a d × d rank-k projection matrix P for which ∥A - AP∥<sub>F</sub><sup>2</sup> ≤ ∥A - [A]<sub>k</sub>∥<sub>F</sub><sup>2</sup> + ε ∥A∥<sub>F</sub><sup>2</sup>, where AP denotes the projection of A onto the row span of P, and [A]<sub>k</sub> denotes the best rank-k approximation to A given by the singular value decomposition. The communication cost of our protocols is d·(sk=ε)<sup>O(1)</sup>, and they succeed with high probability. Our framework allows us to efficiently compute a low rank approximation to an entry-wise softmax, to a Gaussian kernel expansion, and to M-Estimators applied entrywise (i.e., forms of robust low rank approximation). We also show that our additive error approximation is best possible, in the sense that any protocol achieving relative error for these problems requires significantly more communication. Finally, we experimentally validate our algorithms on real datasets.	approximation algorithm;approximation error;big data;experiment;low-rank approximation;singular value decomposition;softmax function;the matrix;utility functions on indivisible goods;with high probability	David P. Woodruff;Peilin Zhong	2016	2016 IEEE 32nd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2016.7498295	mathematical optimization;approximation error;combinatorics;discrete mathematics;rank;mathematics;low-rank approximation	DB	25.977844124280164	-35.65217210895196	128733
17a728adc32f6b1f2f65aea810fe0355fd7c5b60	theory and computations for the dirichlet process and related models: an overview	density estimation;conditional density estimation;random probability distributions;hierarchical model	Abstract Data analysis sometimes requires the relaxation of parametric assumptions in order to gain modeling flexibility and robustness against mis-specification of the probability model. In the Bayesian context, this is accomplished by placing a prior distribution on an infinite-dimensional space, referred to as Bayesian nonparametric models. We provide an overview on the most popular Bayesian nonparametric models for probability distributions and for collections of predictor-dependent probability distributions. The intention of is not to be complete or exhaustive, but rather to touch on areas of interest for the practical use of the priors in the context of a hierarchical model. We give an overview covering the main properties of the basic models and the algorithms for fitting them.	computation	Alejandro Jara	2017	Int. J. Approx. Reasoning	10.1016/j.ijar.2016.11.008	nonparametric statistics;dirichlet distribution;econometrics;density estimation;categorical distribution;convolution of probability distributions;computer science;regular conditional probability;machine learning;mathematics;posterior probability;joint probability distribution;bayesian hierarchical modeling;bayesian statistics;bayes' theorem;hierarchical database model;empirical probability;statistics	SE	26.858728678723324	-26.694035058042665	129071
c5f58011f66fe4567c047559e065eea7ef303681	asymptotic marginal likelihood on linear dynamical systems	kalman filter;bayesian learning		dynamical system;marginal model	Takuto Naito;Keisuke Yamazaki	2014	IEICE Transactions		kalman filter;econometrics;invariant extended kalman filter;ensemble kalman filter;fast kalman filter;marginal likelihood;computer science;machine learning;pattern recognition;extended kalman filter;moving horizon estimation;bayesian inference;statistics	ML	31.30184654439888	-26.68883273169021	129139
3d8cba5414804d409edd7dc9c28681abb270cea3	a new look at reweighted message passing	biological patents;trees mathematics estimation theory message passing;biomedical journals;convergence;graphical models map estimation message passing algorithms;estimation theory message passing trees mathematics;text mining;europe pubmed central;message passing algorithms;citation search;tree decomposition sequential reweighted message passing srmp map estimation graphical model;citation networks;sequential reweighted message passing srmp map estimation graphical model tree decomposition;graphical models;vectors;research articles;abstracts;message passing algorithms graphical models map estimation;probability distribution;open access;life sciences;clinical guidelines;linear programming;message passing;map estimation;full text;vectors message passing graphical models labeling probability distribution linear programming convergence;rest apis;orcids;europe pmc;labeling;biomedical research;bioinformatics;literature search	We propose a new family of message passing techniques for MAP estimation in graphical models which we call Sequential Reweighted Message Passing (SRMP). Special cases include well-known techniques such as Min-Sum Diffusion (MSD) and a faster Sequential Tree-Reweighted Message Passing (TRW-S). Importantly, our derivation is simpler than the original derivation of TRW-S, and does not involve a decomposition into trees. This allows easy generalizations. The new family of algorithms can be viewed as a generalization of TRW-S from pairwise to higher-order graphical models. We test SRMP on several real-world problems with promising results.	derivation procedure;generalization (psychology);graphical model;message passing;multiple sulfatase deficiency disease;trees (plant);algorithm	Vladimir Kolmogorov	2015	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2014.2363465	probability distribution;labeling theory;text mining;message passing;convergence;computer science;linear programming;theoretical computer science;machine learning;data mining;distributed computing;graphical model;statistics	Vision	27.04798259695632	-29.73489518706095	129144
4badc69f9017273d4ed0332b78e391dfbce47f0a	sequential bayesian nonparametric multimodal topic models for video data analysis			multimodal interaction	Jianfei Xue;Koji Eguchi	2018	IEICE Transactions		pattern recognition;computer science;artificial intelligence;topic model;nonparametric statistics;bayesian probability	ML	27.327840409067747	-31.678132571461376	129202
5bcaf226943a04cc218000cd95163003ea241c48	outlier-resistant l1 orthogonal regression via the reformulation-linearization technique	reformulation linearization technique	Assessing the linear relationship between a set of continuous predictors and a continuous response is a well-studied problem in statistics and data mining. L2-based methods such as ordinary least squares and orthogonal regression can be used to determine this relationship. However, both of these methods become impaired when influential values are present. This problem becomes compounded when outliers confound standard diagnostics. This work proposes an L1-norm orthogonal regression method L1OR formulated as a nonconvex optimization problem. Solution strategies for finding globally optimal solutions are presented. Simulation studies are conducted to assess the resistance of the method to outliers and the consistency of the method. The method is also applied to real-world data arising from an environmental science application.	data mining;mathematical optimization;maxima and minima;optimization problem;ordinary least squares;simulation;taxicab geometry;total least squares	J. Paul Brooks;Edward L. Boone	2011	Adv. Operations Research	10.1155/2011/263762	econometrics;mathematical optimization;computer science;mathematics;statistics	ML	27.584475093718847	-34.47158329949686	129566
48376956d91528b3b85bc1c61bba47de2e56db00	performance comparison of two text marking methods	defence management;data journal;image coding;maximum likelihood detection detectors watermarking facsimile correlation prototypes performance analysis humans laboratories;performance comparison;correlation methods;indexing terms;knowledge;foresight innovation policy;it security;correlation methods document image processing word processing image coding maximum likelihood detection;experimental results text marking methods performance comparison illicit distribution regular structures words lines paragraphs document image watermark formatted text documents reliable document identification distortions photocopying facsimile transmission correlation method document profiles centroid method maximum likelihood detectors line shift word shift detection correlation detector centroid detector;maximum likelihood detection;consult;document image processing;maximum likelihood detector;risk architecture;process improvement;business intelligence;word processing	A text document typically consists of a collection of regular structures such as words, lines, and paragraphs, a slight movement of which seems less perceptible than, say, dithering of the document image. In this paper we exploit this property to watermark formatted text documents by shifting slightly certain lines and words in order to discourage illicit distribution. We analyze two methods for reliable document identification in the presence of severe distortions introduced by photocopying, facsimile transmission, and other processing. The correlation method uses document profiles directly for detection. To eliminate the effect of certain distortions, the centroid method bases its decision on the distances between the centroids of adjacent profile blocks. We present the maximum likelihood detectors for both methods and evaluate their relative performance. Our analysis indicates that line-shift generally has a smaller error than wordshift detection, and that the correlation detector outperforms the centroid detector provided certain distortions can be accurately compensated for before detection is attempted. These results have been applied to implement a marking and identification system and preliminary experimental results have been very promising.	distortion;dither;fax;item unique identification;photocopier;sensor	Steven H. Low;Nicholas F. Maxemchuk	1998	IEEE Journal on Selected Areas in Communications	10.1109/49.668978	speech recognition;telecommunications;computer science;pattern recognition;data mining;knowledge;business intelligence;statistics	Web+IR	38.33574454979636	-35.865948846743755	129581
902da465fbc7ae36f296608cc35c04466ff402ae	construction of balanced equivalent estimation second-order split-plot designs	second order;metodo cuadrado menor;modelizacion;composite design d optimality estimation least squares randomization tests split plot design second order designs;methode moindre carre;linear estimation;optimisation;analisis estadistico;least squares method;optimizacion;concepcion optimal;optimal estimation;estimacion lineal;conception optimale;catalogs;grupo de excelencia;inicializacion;probabilistic approach;catalogue;surface reponse;modelisation;estimation lineaire;aleatorizacion;split plot design;statistical analysis;central composite design;ciencias basicas y experimentales;restricted randomization;enfoque probabilista;approche probabiliste;matematicas;analyse statistique;superficie respuesta;randomisation;optimal design;optimization;catalogo;estimation optimale;equivalent estimation design;randomization;response surface;modeling;initialization;initialisation;variance;variancia;estimacion optima	Practical restrictions on randomization are commonplace in industrial experiments due to the presence of hard-to-change or costly-to-change factors. Using a split-plot design (SPD) structure reduces the number of times that these hard-to-change factors are reset during the experiment. A class of second-order response surface SPDs has been proposed in which the ordinary least squares estimates of the model are equivalent to the generalized least squares estimates. Equivalent estimation designs provide best linear unbiased estimates that are independent of the variance components and can be obtained with standard statistical software. Moreover, design selection is robust to model misspecification and does not require previous knowledge of the variance components. This article expands the conditions to obtain equivalent estimation designs and outlines two systematic design construction techniques for building balanced versions of the central composite design. In addition, it presents an approach to generatin...		Peter A. Parker;Scott M. Kowalski;G. Geoffrey Vining	2007	Technometrics	10.1198/004017006000000462	econometrics;restricted randomization;mathematics;algorithm;statistics	EDA	33.72265543685862	-24.5652835789428	129630
27654f7e3e2962a4bd1d1f0ef1190fd3aae9cb71	an aspect oriented framework to applying markov chain monte carlo methods with dynamic models	dynamic models;markov chain monte carlo mcmc;aspects	Both dynamic modeling and Bayesian Markov Chain Monte Carlo (MCMC) methods are established as increasingly popular approaches in their own domains. Dynamic modeling, although widely used to address complex situations, often suffers shortage of empirical data for model parameterization. Dynamic modelers thus use calibration to estimate parameters for which direct evidence is lacking. Unfortunately calibration suffers limitations in capturing the global (for multi-modal distribution) structure of parameter distributions, and a lack of a means of translating uncertainty in parameter estimates directly into uncertainty with respect to model outcomes. We present here a generic user-friendly aspect-based implementation of a theoretically grounded approach to address these limitations by combining Bayesian MCMC methods with dynamic models to estimate model parameters by sampling from joint posterior parameter distributions. The framework is enriched by a user interface to enable the parameter selection at run-time and an interactive runtime graphical visualization of parameter traceplots is generated during MCMC operation. To enable this, a probabilistic model - including a prior distribution and a likelihood function - needs to be specified within the dynamic model. The framework, when enabled, performs MCMC experiments using the dynamic and probabilistic models. We describe here the framework, experiments conducted, and the results obtained.	algorithm;aspect-oriented programming;bayesian network;computation;data point;estimation theory;experiment;machine learning;markov chain monte carlo;mathematical model;modal logic;monte carlo method;overhead (computing);sampling (signal processing);sensor;statistical model;unavailability;usability;user interface	Priyasree Bhowmik;Christopher Dutchyn;Nathaniel D. Osgood	2016	2016 Symposium on Theory of Modeling and Simulation (TMS-DEVS)		econometrics;simulation;markov chain monte carlo;computer science;statistics	SE	28.030471780399115	-24.661326120206752	129662
ce8bf846687986a090a93968a28ac0e52fd61d35	unsupervised learning of parsimonious mixtures on large spaces with integrated feature and component selection	alternative method;unsupervised learning;document clustering;modelizacion;belief networks;unsupervised feature selection;traitement signal;appareillage essai;unsupervised learning bayesian methods testing clustering algorithms length measurement estimation error;learning;gaussian processes;classification non supervisee;gaussian processes unsupervised learning belief networks expectation maximisation algorithm;generalized expectation maximization;bayesian information criterion bic;feature space;model order selection;model complexity;aprendizaje;modelisation;naive bayes classifier;apprentissage;minimum message length;gaussian process unsupervised learning parsimonious mixtures component selection bayesian information criterion informative feature subset generalized expectation maximization learning;mixture model;methode alternative;feature extraction;aparato ensayo;signal processing;metodo alternativo;clasificacion no supervisada;unsupervised feature selection bayesian information criterion bic document clustering em algorithm mixture models model order selection;signal classification;defaillance;classification error;testing equipment;algorithme em;classification signal;unsupervised classification;feature selection;teoria mezcla;algoritmo em;failures;extraction caracteristique;classification automatique;mixture models;automatic classification;mixture theory;em algorithm;bayesian information criterion;procesamiento senal;modeling;clasificacion automatica;data fitting;theorie melange;fallo;expectation maximisation algorithm	"""Estimating the number of components (the order) in a mixture model is often addressed using criteria such as the Bayesian information criterion (BIC) and minimum message length. However, when the feature space is very large, use of these criteria may grossly underestimate the order. Here, it is suggested that this failure is not mainly attributable to the criterion (e.g., BIC), but rather to the lack of """"structure"""" in standard mixtures-these models trade off data fitness and model complexity only by varying the order. The authors of the present paper propose mixtures with a richer set of tradeoffs. The proposed model allows each component its own informative feature subset, with all other features explained by a common model (shared by all components). Parameter sharing greatly reduces complexity at a given order. Since the space of these parsimonious modeling solutions is vast, this space is searched in an efficient manner, integrating the component and feature selection within the generalized expectation-maximization (GEM) learning for the mixture parameters. The quality of the proposed (unsupervised) solutions is evaluated using both classification error and test set data likelihood. On text data, the proposed multinomial version-learned without labeled examples, without knowing the """"true"""" number of topics, and without feature preprocessing-compares quite favorably with both alternative unsupervised methods and with a supervised naive Bayes classifier. A Gaussian version compares favorably with a recent method introducing """"feature saliency"""" in mixtures."""	bayesian information criterion;bioinformatics;collaborative filtering;complexity;expectation–maximization algorithm;feature selection;feature vector;gaussian (software);ground truth;maxima and minima;minimum message length;mixture model;multinomial logistic regression;naive bayes classifier;network switch;occam's razor;overfitting;preprocessor;semi-supervised learning;simulated annealing;structural integrity and failure;test set;text corpus;unsupervised learning	Michael W. Graham;David J. Miller	2006	IEEE Transactions on Signal Processing	10.1109/TSP.2006.870586	unsupervised learning;document clustering;computer science;machine learning;signal processing;pattern recognition;mixture model;mathematics;feature selection;statistics	ML	29.121415855839427	-31.51794501594679	129848
6409b8879c7e61acf3ca17bcc62f49edca627d4c	learning finite beta-liouville mixture models via variational bayes for proportional data clustering	finite mixture modeling;data analysis;finite beta-liouville mixture model;bayesian inference procedure;proportional data;artificial data set;finite mixture model;proposed approach;mixture model;proposed algorithm;variational bayes	During the past decade, finite mixture modeling has become a well-established technique in data analysis and clustering. This paper focus on developing a variational inference framework to learn finite Beta-Liouville mixture models that have been proposed recently as an efficient way for proportional data clustering. In contrast to the conventional expectation maximization (EM) algorithm, commonly used for learning finite mixture models, the proposed algorithm has the advantages that it is more efficient from a computational point of view and by preventing overand under-fitting problems. Moreover, the complexity of the mixture model (i.e. the number of components) can be determined automatically and simultaneously with the parameters estimation in a closed form as part of the Bayesian inference procedure. The merits of the proposed approach are shown using both artificial data sets and two interesting and challenging real applications namely dynamic textures clustering and facial expression recognition.	calculus of variations;cluster analysis;computation;dynamic data;estimation theory;expectation–maximization algorithm;experiment;mixture model;point of view (computer hardware company);variational principle	Wentao Fan;Nizar Bouguila	2013			correlation clustering;constrained clustering;econometrics;data stream clustering;canopy clustering algorithm;machine learning;cure data clustering algorithm;mathematics;cluster analysis;statistics	ML	29.937683901771987	-30.86624564891258	129891
4ed5f16da55b45e2e80c4084279422fb4c0959d3	on the use of adaptive fuzzy wavelet filter in the speech enhancement	support vector machines;wavelet thresholding;speech enhancement;spectral subtraction;fuzzy;voice activity detection	This paper proposes an adaptive fuzzy wavelet filter that is based on a fuzzy inference system for enhancing speech signals and improving the accuracy of speech recognition. In the last two decades, the basic wavelet thresholding algorithm has been extensively used for noise filtering. In the proposed method, adaptive wavelet thresholds are generated and controlled according to the fuzzy rules about the presence of speech in contaminated signals. In this adaptive fuzzy wavelet filter, the relationships between speech and noise are summarized into seven fuzzy rules using four linguistic variables, which are used to determine the state of a signal. A hybrid filter is proposed here, which combines an adaptive fuzzy wavelet filter and the spectral subtraction method to filter contaminated signals. An amplified voice activity detector in the proposed hybrid filter is designed to improve performance when the signal-to-noise ratio (SNR) is lower than 5 dB. The filtering that is performed using the adaptive fuzzy wavelet filter and the spectral subtraction method is controlled by support vector machines. Experimental results demonstrate that the proposed system effectively increases the SNR and the speech recognition rate.	algorithm;inference engine;signal-to-noise ratio;speech enhancement;speech recognition;support vector machine;thresholding (image processing);wavelet	Chih-Chia Yao;Ming-Hsun Tsai;Yuan-Tain Chang	2014	JCP	10.4304/jcp.9.11.2501-2513	voice activity detection;fuzzy logic;adaptive filter;support vector machine;speech recognition;kernel adaptive filter;computer science;machine learning;root-raised-cosine filter;pattern recognition;mathematics;wavelet packet decomposition;stationary wavelet transform	Robotics	37.45790654413075	-35.848193445114944	129982
b735113ec5d740be7905a73d79ef6e94adf5f1d1	monitoring and fault detection in a reverse osmosis plant using principal component analysis	desalination;reverse osmosis;process monitoring;monitoring principal component analysis cleaning fault detection covariance matrix trajectory vectors;batch processing industrial;trajectory;reverse osmosis batch processing industrial desalination principal component analysis process monitoring;vectors;monitoring;false alarms fault detection system principal component analysis techniques simulated reverse osmosis desalination plant cleaning cycles correct plan running operating mode pca approach batch processes monitoring;principal component analysis;fault detection;cleaning;covariance matrix	This paper presents a monitoring and fault detection system based on principal component analysis techniques (PCA) for a simulated reverse osmosis desalination plant.	fault detection and isolation;plasma cleaning;principal component analysis;recursion;stationary state	Diego Garcia-Alvarez;Maria J. Fuente;Luis G. Palacín	2011	IEEE Conference on Decision and Control and European Control Conference	10.1109/CDC.2011.6160345	control engineering;covariance matrix;real-time computing;reverse osmosis;engineering;trajectory;process engineering;mathematics;fault detection and isolation;statistics;principal component analysis	Robotics	36.26928739021644	-28.939549751319124	129995
a207fe48f40140041b901f098976e3697b3133d8	rapid probabilistic source inversion using pattern recognition	earthquake early warning;displacement observations;artificial neural networks;source inversion;inverse theory	Numerous problems in the field of seismology require the determination of parameters of a physical model that are compatible with a set of observations and prior assumptions. This type of problem is generally termed inverse problem. While, in many cases, we are able to predict observations, given a particular set of parameters with high accuracy and precision using numerical and analytical methods, often we are unable to directly infer model parameters from observations. This has lead to the development of a large number of inverse methodologies. For many applications it is necessary not only to obtain a set of best fitting parameters, but also an estimate of the attached uncertainties, which arise from the fact that any observation is subject to measurement uncertainties and from other sources of uncertainty that may be present due to numerical limitations or simplifying assumptions. Often, the situation is further complicated by the non-linear nature of many inverse problems, leading to non-Gaussian and possibly non-unique posterior distributions. An important seismological inverse problem is the determination of earthquake source parameters. Source parameters are on one hand needed as an input for subsequent investigations, such as tomographic imaging or finite fault studies. On the other hand source parameter estimates are required in order to provide information to the public as quickly as possible after a new earthquake has initiated - a field known as earthquake early warning. We have developed a methodology for rapid probabilistic moment tensor point source inversions able to cope with a wide variety of data types. Accurate and computationally expensive synthetic forward modelling can be incorporated without significantly altering the time required for the inversions. Our methodology is based on finding an approximation to the conditional posterior probability of source models given observations by smoothly interpolating a set of prior samples. The interpolation is obtained using ensembles of Mixture Density Networks (MDNs) - a class of artificial neural networks, whose output are the parameters of a Gaussian mixture model. Once an ensemble has been constructed, new observations can be inverted within the fraction of a second on a standard desktop computer. The method is therefore well suited for earthquake early warning, where new observations must be inverted routinely and rapidly. We apply the method to two scenarios in Southern California using both synthetic and observed datasets. We invert regional static and dynamic GPS displacement data for the 2010 M 7.2 El Mayor Cucapah earthquake in Baja California to obtain estimates of magnitude, centroid location and depth, and focal mechanism. Moreover, we show how accurate 3-D spectral element simulations can be incorporated into the rapid inversion scheme by means of a model scenario focussing on the 2008 M 5.4 Chino Hills earthquake. We discuss general implications, advantages and limitations of the method and suggest directions for potential future research.	pattern recognition	Paul J. Käufl	2015			econometrics;geography;artificial intelligence;statistics	Vision	37.74813997637794	-24.19910477767925	130288
b1ea6ef3f8b02d82d531e3bbcde5e6b94ae1b89f	compressed video coding with multi-kernel gaussian process regression	video coding;gaussian process;unscented kalman filter	From sparse low-quality samples over heterogeneous devices, this paper is dedicated to video coding through inverse learning based reconstruction. Given a subset of degraded frames with smooth filtering, the restoration inference process is considered as a learning equivalent optimization problem from online training examples in coded key frames. To accommodate the state of arbitrary nonlinear systems on non-stationary video statistics, non-parametric system models on motion estimation are investigated by the Unscented Kalman Filter as the state estimator. The spatio-temporal variation regularity is taken to maximize the state transition probability with multi-kernel Gaussian Process for regression. It is comprehended as a convex combination of kernel matrices that best explains the data and improve the generalization on unseen data. This construction is equivalent to imposing a product of heavy-tailed process priors over function space. It is capable of improving an inference quality than texture synthesis and guaranteeing a graceful change with increased model uncertainty. Over a variety of test sequences, the proposed approach is validated on both compression efficiency and restoration performance.	circuit restoration;data compression;gaussian process;kalman filter;kernel (operating system);key frame;kriging;markov chain;mathematical optimization;motion estimation;nonlinear system;optimization problem;sparse matrix;stationary process;texture synthesis	Jieyu Tian;Yong Li;Hongkai Xiong	2016		10.1145/3007669.3007726	machine learning;pattern recognition;mathematics;statistics	ML	31.002712556726916	-33.9243512408569	130495
39ea162de74d55db56796be71b1abafb94b1d035	assignment of multiplicative mixtures in natural images	assignment problem;natural image statistics;bottom up;ucl;gibbs sampling;top down;discovery;natural images;theses;conference proceedings;gaussian scale mixture;statistical model;multi dimensional;local structure;digital web resources;ucl discovery;open access;ucl library;image analysis;book chapters;open access repository;gain control;ucl research	In the analysis of natural images, Gaussian scale mixtures (GSM) have been used to account for the statistics of filter responses, and to inspire hierarchical cortical representational learning schemes. GSMs pose a critical assignment problem, working out which filter responses were generated by a common multiplicative factor. We present a new approach to solving this assignment problem through a probabilistic extension to the basic GSM, and show how to perform inference in the model using Gibbs sampling. We demonstrate the efficacy of the approach on both synthetic and image data.	assignment problem;coefficient;gibbs sampling;sampling (signal processing);synthetic data	Odelia Schwartz;Terrence J. Sejnowski;Peter Dayan	2004			image analysis;computer science;artificial intelligence;data science;machine learning;top-down and bottom-up design;data mining;mathematics;statistics	ML	31.130774196328797	-34.449664443235484	130534
f3968d1ec24310f8bcb181122309cf70c90a632c	bayesian learning for sparse signal reconstruction	sparse bayesian learning;bayes methods;learning automata;fixed point;support vector machine model sparse bayesian learning sparse signal reconstruction relevance vector machines signal representation signal regression signal classification basis selection overcomplete dictionaries basis pursuit;bayesian learning;signal representation;relevance vector machine;signal classification;success rate;simulation study;signal reconstruction;basis pursuit;learning artificial intelligence;learning automata signal reconstruction signal representation signal classification bayes methods learning artificial intelligence;signal reconstruction bayesian methods solids pursuit algorithms signal representations support vector machines support vector machine classification electronic mail machine learning dictionaries	Sparse Bayesian learning and specifically relevance vector machines have received much attention as a means of achieving parsimonious representations of signals in the context of regression and classification. We provide a simplified derivation of this paradigm from a Bayesian evidence perspective and apply it to the problem of basis selection from overcomplete dictionaries. Furthermore, we prove that the stable fixed points of the resulting algorithm are necessarily sparse, providing a solid theoretical justification for adapting the methodology to basis selection tasks. We then include simulation studies comparing sparse Bayesian learning with Basis Pursuit and the more recent FOCUSS class of basis selection algorithms, empirically demonstrating superior performance in terms of average sparsity and success rate of recovering generative bases.	algorithm;basis pursuit;dictionary;occam's razor;programming paradigm;relevance;signal reconstruction;simulation;sparse matrix	David P. Wipf;Bhaskar D. Rao	2003		10.1109/ICASSP.2003.1201753	signal reconstruction;basis pursuit;computer science;machine learning;pattern recognition;sparse approximation;data mining;mathematics;fixed point;relevance vector machine;bayesian inference;statistics	ML	29.36134791684992	-32.097020563832004	130960
f00ab72c394cb6dae89eaa665cbe19dc4d79ec58	optimum designs for parameter estimation in a mixture experiment with two correlated responses	optimum designs;62j05;d optimality criterion;correlated responses;linear and quadratic mixture models;62k99;a optimality criterion	AbstractIn this paper we investigate a mixture problem with two responses, which are functions of the mixing proportions, and are correlated with known dispersion matrix. We obtain D- and A- optimal designs for estimating the parameters of the response functions, when none or some of the regression coefficients of the two functions are the same. It is shown that when no prior knowledge about the regression coefficients is available, the D-optimal design is independent of the dispersion matrix, while the A-optimal design depends on it, provided the response functions are of different degree. On the other hand, when some of the regression coefficients are known to be the same for both the functions, the D-optimal design depends on the dispersion matrix when the two response functions are not of the same degree.	estimation theory	Manisha Pal;Nripes Kumar Mandal	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2016.1248575	econometrics;mathematical optimization;mathematics;statistics	ML	30.610748475495402	-24.05373877966756	131194
31defd1c1b5c13c5b09548b297e24bf8f60279c5	comments on “joint bayesian model selection and estimation of noisy sinusoids via reversible jump mcmc”	signal sampling;bayes methods;trans dimensional problems signal decomposition bayesian inference markov chain monte carlo methods;markov processes;markov chain monte carlo method joint bayesian model selection noisy sinusoid estimation reversible jump mcmc parameter estimation problem signal processing rj mcmc sampling technique metropolis hastings green ratio mhg ratio;signal sampling bayes methods markov processes monte carlo methods;monte carlo methods	Reversible jump MCMC (RJ-MCMC) sampling techniques, which allow to jointly tackle model selection and parameter estimation problems in a coherent Bayesian framework, have become increasingly popular in the signal processing literature since the seminal paper of Andrieu and Doucet [“Joint Bayesian model selection and estimation of noisy sinusoids via reversible jump MCMC,” IEEE Trans. Signal Process, vol. 47, no. 10, pp. 2667-2676, 1999]. Crucial to the implementation of any RJ-MCMC sampler is the computation of the so-called Metropolis-Hastings-Green (MHG) ratio, which determines the acceptance probability for the proposed moves. It turns out that the expression of the MHG ratio that was given in the paper of Andrieu and Doucet for “Birth-or-Death” moves is erroneous and has been reproduced in many subsequent papers dealing with RJ-MCMC sampling in the signal processing literature. This note fixes the erroneous expression and briefly discusses its cause and consequences.	bayes factor;coherence (physics);computation;estimation theory;metropolis;metropolis–hastings algorithm;model selection;registered jack;regular expression;reversible-jump markov chain monte carlo;sampling (signal processing);signal processing;turing jump	Alireza Roodaki;Julien Bect;Gilles Fleury	2013	IEEE Transactions on Signal Processing	10.1109/TSP.2013.2261992	econometrics;markov chain monte carlo;computer science;machine learning;mathematics;markov process;statistics;monte carlo method	Vision	38.347444384725705	-26.32072965399474	131379
9df20a5edb165015f9810e391d8fce2f88b3d831	likelihood-based inference for the power half-normal distribution		In this paper we consider an extension of the half-normal distribution based on the distribution of the maximum of a random sample. It is shown that this distribution belongs to the family of beta generalized half-normal distributions. Properties of its density are investigated, maximum likelihood estimation is discussed and the Fisher information matrix is derived. A real data illustration is presented, and comparisons with alternative extensions of the half-normal distribution reveal good performance of the proposed model.	fisher information;formation matrix	Yolanda M. Gómez;Heleno Bolfarine	2015	JSTA	10.2991/jsta.2015.14.4.4	gumbel distribution;exponential distribution;noncentral chi-squared distribution;beta-binomial distribution;econometrics;mathematical optimization;posterior predictive distribution;normal-gamma distribution;categorical distribution;product distribution;log-cauchy distribution;maximum a posteriori estimation;inverse-chi-squared distribution;mathematics;variance-gamma distribution;generalized integer gamma distribution;ratio distribution;compound probability distribution;sampling distribution;uniform distribution;asymptotic distribution;statistics;three-point estimation;distribution fitting	ML	31.179376959700242	-24.55277007651197	131503
1c80e6ddb0d2b7ac393218806e8b1f28381cd272	probabilistic approximations of signaling pathway dynamics	bayesian inference;dynamic bayesian network;time domain;signaling pathway;ordinary dierential equation	Systems of ordinary differential equations (ODEs) are often used to model the dynamics of complex biological pathways. We construct a discrete state model as a probabilistic approximation of the ODE dynamics by discretizing the value space and the time domain. We then sample a representative set of trajectories and exploit the discretization and the structure of the signaling pathway to encode these trajectories compactly as a dynamic Bayesian network. As a result, many interesting pathway properties can be analyzed efficiently through standard Bayesian inference techniques. We have tested our method on a model of EGF-NGF signaling pathway [1] and the results are very promising in terms of both accuracy and efficiency.	approximation;bayesian approaches to brain function;discretization;dynamic bayesian network;encode;gene regulatory network	Bing Liu;P. S. Thiagarajan;David Hsu	2009		10.1007/978-3-642-03845-7_17	econometrics;mathematical optimization;time domain;computer science;mathematics;bayesian inference;signal transduction;dynamic bayesian network;statistics	Theory	32.318866138054716	-28.152982608822988	131550
e03e669a4016deb4588b0be2f4ba57ed0767e686	high-dimensional range profile geometrical visualization and performance estimation of radar target classification via a gaussian mixture model		In this paper, a method of data visualization and classification performance estimation applied to target classification is proposed. The objective of this paper is to propose a mathematical tool for data characterization. The principle is to use a non linear dimensionality reduction technique to describe our data in a low-dimensional space and to model embedding data by Gaussian mixture model (GMM) to estimate classification performance graphically and analytically.		Thomas Boulay;Ali Mohammad-Djafari;Nicolas Gac;Julien Lagoutte	2013		10.1007/978-3-642-40020-9_93	pattern recognition;data mining;mathematics;statistics	ML	30.761562752968146	-36.4516831669964	131627
b4b73888cf9ba8fb9a0cd4a4b35dc419f8f747b8	differentially private least squares: estimation, confidence and rejecting the null hypothesis		Linear regression is one of the most prevalent techniques in data analysis. Given a large collection of samples composed of features x x x and a label y, linear regression is used to find the best prediction of the label as a linear combination of the features. However, it is also common to use linear regression for its explanatory capabilities rather than label prediction. Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. income) in the presence of other (potentially correlated) features. OLS uses linear regression in order to estimate the correlation between the label and a feature x j on a given dataset; and then, under the assumption of a certain random generative model for the data, OLS outputs an interval on the reals that is likely to contain the correlation between y and x j in the underlying distribution (a confidence interval). When this interval does not intersect the origin, we can reject the null hypothesis as it is likely that x j indeed has a non-zero correlation with y. Our work aims at achieving similar guarantees on data under differential privacy. We use the Gaussian Johnson-Lindenstrauss transform, which has been shown to satisfy differential privacy if the given data has large singular values [BBDS12]. We analyze the result of projecting the data using the JL transform under the OLS model and show how to approximate confidence intervals using only the projected data. We also bound the number of samples needed to reject the null hypothesis with differential privacy, when the data is drawn i.i.d from a multivariate Gaussian. When not all singular values of the data are sufficiently large, we alter the input and increase its singular values and then project it using a JL transform. Thus our projected data yields an approximation for the Ridge Regression problem — a variant of the linear regression that uses a l 2-regularization term. We give conditions under which the regularized problem is still helpful in establishing correlations.	approximation algorithm;differential privacy;discrete fourier transform;generative model;ordinary least squares	Or Sheffet	2015	CoRR		econometrics;pattern recognition;statistics	ML	27.870017126059118	-25.762030246344484	131810
880708ac41527dc4f0af61682c9062fb3ada4e57	a gradient byy harmony learning rule on gaussian mixture with automated model selection	model selection;gaussian mixture;bayesian ying yang learning;simulation experiment;bayesian ying yang	One important feature of Bayesian Ying–Yang (BYY) harmony learning is that model selection can be made automatically during parametric learning. In this paper, BYY harmony learning with a bi-directional architecture is studied for Gaussian mixture modelling via a gradient learning rule. It has been demonstrated by simulation experiments that the number of Gaussians can be determined automatically during learning the parameters of the Gaussian mixture. c © 2003 Published by Elsevier B.V.	bayesian network;experiment;gradient;learning rule;model selection;simulation;yang	Jinwen Ma;Taijun Wang;Lei Xu	2004	Neurocomputing	10.1016/j.neucom.2003.10.009	artificial intelligence;machine learning;pattern recognition;mathematics;model selection;statistics	ML	25.422085873444946	-30.349994035817044	131953
14c7d66a3ae24949ec01d51e4095e5141550f824	coherence pursuit: fast, simple, and robust subspace recovery		A remarkably simple, yet powerful, algorithm termed Coherence Pursuit for robust Principal Component Analysis (PCA) is presented. In the proposed approach, an outlier is set apart from an inlier by comparing their coherence with the rest of the data points. As inliers lie in a low dimensional subspace, they are likely to have strong mutual coherence provided there are enough inliers. By contrast, outliers do not typically admit low dimensional structures, wherefore an outlier is unlikely to bear strong resemblance with a large number of data points. As Coherence Pursuit only involves one simple matrix multiplication, it is significantly faster than the stateof-the-art robust PCA algorithms. We provide a mathematical analysis of the proposed algorithm under a random model for the distribution of the inliers and outliers. It is shown that the proposed method can recover the correct subspace even if the data is predominantly outliers. To the best of our knowledge, this is the first provable robust PCA algorithm that is simultaneously noniterative, can tolerate a large number of outliers and is robust to linearly dependent outliers.	algorithm;cache coherence;data point;matrix multiplication;mutual coherence (linear algebra);provable security;robust principal component analysis	Mostafa Rahmani;George Atia	2017			machine learning;subspace topology;computer vision;computer science;coherence (physics);artificial intelligence	ML	26.634834056137915	-36.43303146590888	131961
9e5c6847ae7d658edb78f95c18efad9bc4e55287	bayesian smoothing spline analysis of variance	zellner siow prior;bayesian;reproducing kernel;smoothing spline anova	Smoothing spline ANOVA (SSANOVA) provides an approach to semiparametric function estimation based on an ANOVA type of decomposition. Wahba et al. (1995) decomposed the regression function based on a tensor sum decomposition of inner product spaces into orthogonal subspaces, so the effects of the estimated functions from each subspace can be viewed independently. Recent research related to smoothing spline ANOVA focuses on either frequentist approaches or a Bayesian framework for variable selection and prediction. In our approach, we seek ‘‘objective’’ priors especially suited to estimation. The prior for linear terms including level effects is a variant of the Zellner–Siow prior (Zellner and Siow, 1980), and the prior for a smooth effect is specified in terms of effective degrees of freedom. We study this fully Bayesian SSANOVAmodel for Gaussian response variables, and the method is illustrated with a real data set. © 2012 Elsevier B.V. All rights reserved.	b-spline;estimation theory;feature selection;semiparametric model;smoothing spline	Chin-I Cheng;Paul L. Speckman	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2012.05.020	econometrics;mathematical optimization;bayesian probability;mathematics;statistics	ML	29.282586006340864	-25.060848179565323	132305
099d9c522e3d446abab668ae16b09b92420c9ee7	gas classification using binary decision tree classifier	gas classification metal oxide gas sensor test data vectors training phase gas distribution distance metric tree decision node gas data sample splitting multisensor array sensor sensitivity gas identification pattern recognition algorithm binary decision tree classifier;sensor arrays computerised instrumentation decision trees gas sensors learning artificial intelligence pattern classification;distance metric multi sensors array gas concentration gas classification binary decision tree;gases sensor arrays gas detectors sensor phenomena and characterization decision trees	Gas classification with an array of sensors is challenging for real life applications due to the limited amount of available training data of gases. Different pattern recognition algorithms are successfully used for gases identification, but their performance is degraded when the training and testing of these algorithms is done with different concentrations data. In this paper, we are using a binary decision tree approach for gas classification, and we are considering difference in the sensitivities of the sensors in every pair of a multi-sensor array as an input attribute for the tree. Suitable pairs of sensors are found by exploring their capability to split the available gases data samples at the decision node of the tree into two branches. A distance metric is used to select a single sensor pair in the case of more than one pair of sensors for the gases distribution at the decision node. The selected pairs of sensors learned during the training phase at the decision nodes are applied on the test data vectors. The effectiveness of our algorithm is successfully verified on the acquired data set with an array of seven metal oxide gas sensors for five different gases.	algorithm;decision tree;influence diagram;pattern recognition;real life;sensor;test data	Muhammad Hassan;Amine Bermak	2014	2014 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2014.6865700	machine learning;pattern recognition;data mining;mathematics	Embedded	36.62444835863029	-33.979999944191725	132368
6b8dff59f1e7431a99211ee99c2c29155187f7ee	exact bayesian inference via data augmentation	multinomial distribution;data augmentation;bayesian statistics;reed frost epidemic;integer valued autoregressive process	Data augmentation is a common tool in Bayesian statistics, especially in the application of MCMC. Data augmentation is used where direct computation of the posterior density, π(θ |x), of the parameters θ , given the observed data x, is not possible. We show that for a range of problems, it is possible to augment the data by y, such that, π(θ |x,y) is known, and π(y|x) can easily be computed. In particular, π(y|x) is obtained by collapsing π(y, θ |x) through integrating out θ . This allows the exact computation of π(θ |x) as a mixture distribution without recourse to approximating methods such as MCMC. Useful byproducts of the exact posterior distribution are the marginal likelihood of the model and the exact predictive distribution.	benchmark (computing);computation;convolutional neural network;desktop computer;emoticon;fortran;linkage (software);marginal model;markov chain monte carlo;time complexity	Peter Neal;Theodore Kypraios	2015	Statistics and Computing	10.1007/s11222-013-9435-z	econometrics;machine learning;mathematics;bayesian statistics;multinomial distribution;statistics	ML	27.842206335304365	-28.09993836683886	132384
cb9ddcb36a02ad8e70d32f099984bdd04fa21989	evaluating the variance of likelihood-ratio gradient estimators		The likelihood-ratio method is often used to estimate gradients of stochastic computations, for which baselines are required to reduce the estimation variance. Many types of baselines have been proposed, although their degree of optimality is not well understood. In this study, we establish a novel framework of gradient estimation that includes most of the common gradient estimators as special cases. The framework gives a natural derivation of the optimal estimator that can be interpreted as a special case of the likelihood-ratio method so that we can evaluate the optimal degree of practical techniques with it. It bridges the likelihood-ratio method and the reparameterization trick while still supporting discrete variables. It is derived from the exchange property of the differentiation and integration. To be more specific, it is derived by the reparameterization trick and local marginalization analogous to the local expectation gradient. We evaluate various baselines and the optimal estimator for variational learning and show that the performance of the modern estimators is close to the optimal estimator.	baseline (configuration management);bayesian network;bernoulli polynomials;computation;euler–bernoulli beam theory;gradient;sigmoid function;variational principle	Seiya Tokui;Issei Sato	2017			estimator;pattern recognition;artificial intelligence;one-way analysis of variance;computer science;m-estimator	ML	26.312800748478267	-30.64606278109162	132621
fd03e7c383d86b0abeb353cbb7be224bfe05717e	higher-fidelity frugal and accurate quantile estimation using a novel incremental discretized paradigm		"""Traditional pattern classification works with the moments of the distributions of the features and involves the estimation of the means and variances. As opposed to this, more recently, research has indicated the power of using the quantiles of the distributions because they are more robust and applicable for non-parametric methods. The estimation of the quantiles is even more pertinent when one is mining data streams. However, the complexity of quantile estimation is much higher than the corresponding estimation of the mean and variance, and this increased complexity is more relevant as the size of the data increases. Clearly, in the context of infinite data streams, a computational and space complexity that is linear in the size of the data is definitely not affordable. In order to alleviate the problem complexity, recently, a very limited number of studies have devised <italic>incremental</italic> quantile estimators <xref ref-type=""""bibr"""" rid=""""ref1"""">[1]</xref>, <xref ref-type=""""bibr"""" rid=""""ref2"""">[2]</xref>. Estimators within this class resort to updating the quantile estimates based on the most recent observation(s), and this yields updating schemes with a very small computational footprint–a constant-time (i.e., <inline-formula> <tex-math notation=""""LaTeX"""">$O(1)$ </tex-math></inline-formula>) complexity. In this paper, we pursue this research direction and present an estimator that we refer to as a higher-fidelity frugal <xref ref-type=""""bibr"""" rid=""""ref1"""">[1]</xref> quantile estimator. First, it guarantees a substantial advancement of the family of Frugal estimators introduced in <xref ref-type=""""bibr"""" rid=""""ref1"""">[1]</xref>. The highlight of the present scheme is that it works in the discretized space, and it is thus a pioneering algorithm within the theory of discretized algorithms.<xref ref-type=""""fn"""" rid=""""fn1""""><sup>1</sup></xref> The convergence results that we have proven are based on the theory of stochastic point location <xref ref-type=""""bibr"""" rid=""""ref3"""">[3]</xref>, which we advocate as a new tool for solving a large class of online estimation problems. Extensive simulation results show that our estimator outperforms the original Frugal algorithm in terms of both speed and accuracy.<fn id=""""fn1""""><label><sup>1</sup></label><p>The fact that discretized Learning Automata schemes are superior to their continuous counterparts has been clearly demonstrated in the literature. This is the first paper, to our knowledge, that proves the advantages of discretization within the domain of quantile estimation.</p></fn>"""	algorithm;automaton;computation;cross-reference;dspace;discretization;learning automata;point location;relevance;simulation	Anis Yazidi;Hugo Lewi Hammer;B. John Oommen	2018	IEEE Access	10.1109/ACCESS.2018.2820501	estimator;point location;mathematical optimization;quantile;robustness (computer science);data stream mining;discretization;computer science;distributed computing;convergence (routing);learning automata	DB	29.399093555127106	-29.21912223131143	132629
9cf067e461935d65530831a54bfdee57d18dea1b	v-matrix method of solving statistical inference problems	support vector machines;conditional density;regression;density ratio;interpolation function;reproducing kernel hilbert space;ill posed problem;mutual information;data adaptation;conditional probability;data balancing;function estimation	This paper presents direct settings and rigorous solutions of the main Statistical Inference problems. It shows that rigorous solutions require solving multidimensional Fredholm integral equations of the first kind in the situation where not only the right-hand side of the equation is an approximation, but the operator in the equation is also defined approximately. Using Stefanuyk-Vapnik theory for solving such ill-posed operator equations, constructive methods of empirical inference are introduced. These methods are based on a new concept called V -matrix. This matrix captures geometric properties of the observation data that are ignored by classical statistical methods.	approximation;matrix method;well-posed problem	Vladimir Vapnik;Rauf Izmailov	2015	Journal of Machine Learning Research		conditional probability distribution;support vector machine;mathematical optimization;mathematical analysis;regression;conditional probability;machine learning;reproducing kernel hilbert space;mathematics;mutual information;statistics	AI	27.417853484193472	-33.299847748285174	132733
75186d89b7fbc1a8b09a8d8bdcdcab440ae3c226	on discriminative joint density modeling	modelo lineal generalizado;discriminative joint density model;fonction vraisemblance;cost function;learning;modele lineaire generalise;logistic regression;funcion coste;funcion verosimilitud;aprendizaje;discriminant analysis;analyse discriminante;generalized linear models;analisis discriminante;regresion logistica;apprentissage;regression logistique;conditional likelihood;fonction cout;generalized linear model;missing data;mixture models;likelihood function	We study discriminative joint density models, that is, generative models for the joint density p(c,x) learned by maximizing a discriminative cost function, the conditional likelihood. We use the framework to derive generative models for generalized linear models, including logistic regression, linear discriminant analysis, and discriminative mixture of unigrams. The benefits of deriving the discriminative models from joint density models are that it is easy to extend the models and interpret the results, and missing data can be treated using justified standard methods.	discriminative model;emoticon;generalized linear model;generative model;linear discriminant analysis;logistic regression;loss function;missing data	Jarkko Salojärvi;Kai Puolamäki;Samuel Kaski	2005		10.1007/11564096_34	econometrics;computer science;machine learning;pattern recognition;generalized linear model;mathematics;linear discriminant analysis;generative model;discriminative model;statistics	ML	27.61286873726855	-30.361998525090037	132869
f7a6f7285b4ec032ca4cf32355faaec03d080ae3	variational bayes group sparse time-adaptive parameter estimation with either known or unknown sparsity pattern	intrinsic conditional autoregression model variational bayes group sparse time adaptive parameter estimation unknown sparsity pattern time adaptive group sparse signal estimation online variational bayes schemes known group sparsity pattern bayesian adaptive group lasso hierarchical bayesian model;parameter estimation adaptive estimation adaptive signal processing autoregressive processes bayes methods;bayes methods;estimation;covariance matrices;signal processing;mathematical model;generalized inverse gaussian distribution group sparsity online variational bayes conditional autoregressive model;signal processing algorithms;adaptation models;bayes methods estimation adaptation models signal processing algorithms mathematical model covariance matrices signal processing	In this paper, we study the problem of time-adaptive group sparse signal estimation from a Bayesian viewpoint. We propose two online variational Bayes schemes that are specifically designed to estimate and track group sparse signals in time. The proposed schemes address both the cases where the grouping information of the signal is either known or not. For the case of known group sparsity pattern, the proposed scheme builds on a novel hierarchical model for the Bayesian adaptive group lasso. Utilizing the variational Bayes framework, update equations for all model parameters are given, for both the batch and time adaptive estimation scenarios. To address the case where the group sparsity pattern is unknown, the hierarchical Bayesian model of the former scheme is extended by organizing the penalty parameters of the Bayesian lasso in a conditional autoregressive model. Intrinsic conditional autoregression is exploited to penalize the signal coefficients in a structured manner and thus obtain group sparse solutions automatically. Again, a robust and computationally efficient online variational Bayes estimator is developed, capitalizing on the conjugacy of the proposed hierarchical Bayesian formulation. Experimental results are reported that corroborate the superior estimation performance of the proposed online schemes, when compared with state-of-the-art methods.	algorithm;algorithmic efficiency;autoregressive model;bayesian network;calculus of variations;coefficient;digimon;estimation theory;experiment;global information grid;hp 48 series;hierarchical database model;lasso;mathematical optimization;optimization problem;organizing (structure);portable document format;recursion;sparse matrix;variational principle	Konstantinos Themelis;Athanasios A. Rontogiannis;Konstantinos D. Koutroumbas	2016	IEEE Transactions on Signal Processing	10.1109/TSP.2016.2543204	econometrics;estimation;bayes factor;recursive bayesian estimation;bayesian programming;signal processing;pattern recognition;mathematical model;mathematics;bayesian hierarchical modeling;statistics	ML	29.959368060395544	-32.55394200720377	132948
20a8b3834987813e2818a36207cdb04acaad18b7	asynchronous parallel algorithms for nonconvex big-data optimization: model and convergence		We propose a novel asynchronous parallel algorithmic framework for the minimization of the sum of a smooth nonconvex function and a convex nonsmooth regularizer, subject to both convex and nonconvex constraints. The proposed framework hinges on successive convex approximation techniques and a novel probabilistic model that captures key elements of modern computational architectures and asynchronous implementations in a more faithful way than current state of the art models. Key features of the proposed framework are: i) it accommodates inconsistent read, meaning that components of the vector variables may be written by some cores while being simultaneously read by others; ii) it covers in a unified way several different specific solution methods, and iii) it accommodates a variety of possible parallel computing architectures. Almost sure convergence to stationary solutions is proved. Numerical results, reported in the companion paper [5], on both convex and nonconvex problems show our method can consistently outperform existing parallel asynchronous algorithms.	approximation;big data;convex function;numerical method;parallel algorithm;parallel computing;stationary process;statistical model	Loris Cannelli;Francisco Facchinei;Vyacheslav Kungurtsev;Gesualdo Scutari	2016	CoRR		mathematical optimization;theoretical computer science;control theory;mathematics;algorithm	ML	25.185620894688867	-33.51729589731144	133057
104ebf501c227d71c82ea1b1d4865d5ef7684937	distributed geometric nonnegative matrix factorization and hierarchical alternating least squares-based nonnegative tensor factorization with the mapreduce paradigm			least squares;mapreduce;non-negative matrix factorization;programming paradigm	Rafal Zdunek;Krzysztof Fonal	2018	Concurrency and Computation: Practice and Experience	10.1002/cpe.4473	distributed computing;computer science;mathematical optimization;tensor;factorization;least squares;non-negative matrix factorization	ML	25.96174687647141	-36.72359989861216	133183
93b5260493d12777a1915b729852db9121929dfb	towards affordable on-track testing for autonomous vehicle — a kriging-based statistical approach		This paper discusses the use of Kriging model in Automated Vehicle evaluation. We explore how a Kriging model can help reduce the number of experiments or simulations in the Accelerated Evaluation procedure. We also propose an adaptive sampling scheme for selecting samples to construct the Kriging model. Application examples in the lane change scenario are presented to illustrate the proposed methods.	adaptive sampling;autonomous robot;experiment;kriging;sampling (signal processing);simulation	Zhiyuan Huang;Henry Lam;Ding Zhao	2017	2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2017.8317656	simulation;mathematics;mathematical optimization;kriging;adaptive sampling	Robotics	34.569818421477024	-26.426852155103003	133373
0b34482fcdb2bf74910dbb5a827e41cfea65a4f6	the infinite hierarchical factor regression model	statistical machine learning;regression model;science learning;factor analysis;number of factors;indian buffet process;gene expression data analysis;hierarchical model	We propose a nonparametric Bayesian factor regression mode l that accounts for uncertainty in the number of factors, and the relationship b etween factors. To accomplish this, we propose a sparse variant of the Indian Bu ffet Process and couple this with a hierarchical model over factors, based on Kingman’s coalescent. We apply this model to two problems (factor analysis and fact or regression) in gene-expression data analysis.	algorithm;bayesian network;factor analysis;factor regression model;hierarchical database model;kingman's subadditive ergodic theorem;regular expression;sampling (signal processing);sparse matrix;the matrix	Piyush Rai;Hal Daumé	2008			econometrics;computer science;machine learning;regression diagnostic;factor analysis;factor regression model;hierarchical database model;regression analysis;statistics	ML	30.333279266617083	-25.984046962197166	133742
32c9089e1e8ef7219e5db9036eb9dd34223d4766	consistent collective matrix completion under joint low rank structure		We address the collective matrix completion problem of jointly recovering a collection of matrices with shared structure from partial (and potentially noisy) observations. To ensure well– posedness of the problem, we impose a joint low rank structure, wherein each component matrix is low rank and the latent space of the low rank factors corresponding to each entity is shared across the entire collection. We first develop a rigorous algebra for representing and manipulating collective–matrix structure, and identify sufficient conditions for consistent estimation of collective matrices. We then propose a tractable convex estimator for solving the collective matrix completion problem, and provide the first non–trivial theoretical guarantees for consistency of collective matrix completion. We show that under reasonable assumptions stated in Sec. 3.1, with high probability, the proposed estimator exactly recovers the true matrices whenever sample complexity requirements dictated by Theorem 1 are met. The sample complexity requirement derived in the paper are optimum up to logarithmic factors, and significantly improve upon the requirements obtained by trivial extensions of standard matrix completion. Finally, we propose a scalable approximate algorithm to solve the proposed convex program, and corroborate our results through simulated and real life experiments.	approximation algorithm;cobham's thesis;convex optimization;experiment;real life;requirement;sample complexity;scalability;with high probability	Suriya Gunasekar;Makoto Yamada;Dawei Yin;Yi Chang	2015	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics;statistics	ML	27.0041986951993	-35.71680989288734	134050
1a16e09cf14c4146300213db32e07fd74f87d811	modelling from spatiotemporal data : a dynamic systems approach		Several natural phenomena manifest themselves as spatiotemporal evolution processes. The study of these processes, which aims to increase our understanding of the spatiotemporal phenomena for their prediction and control, requires analysis tools to infer models and their parameters from collected data. Whilst several studies exist on how to model from highly complex patterns characteristic of spatiotemporal processes, an approach which may be readily employed in a wide range of scenarios, such as with systems with different forms of observation processes or time-varying systems, is lacking. This work fills this void by providing a systems approach to spatiotemporal modelling which can be used with continuous observations, point process observations, systems exhibiting spatially varying dynamics and time-varying systems. The developed methodology builds on the stochastic partial differential equation as a suitable class of models for dynamic spatiotemporal modelling which can easily cater for spatially varying dynamics. A dimensionality reduction mechanism employing frequency methods is proposed; this is used to bring the spatiotemporal system, coupled with the observation process, into conventional state-space form. The work also provides a series of joint field-parameter inference methods which can cater for the vast range of problems under study. Variational techniques are found to be particularly amenable to these kinds of problem and hence a novel dual variational filter is developed to cater for time-varying spatiotemporal systems. The filter is seen to compare favourably with other conventional approaches and to work well on real temporal data sets. The potential of adopting a systems approach to spatiotemporal modelling is shown on the large-scale Wikileaks data set, the Afghan War Diary, where it is found that reliable predictions are possible even in complex scenarios. The encouraging results are a strong indication that the adopted approach may be used for large-scale spatiotemporal systems across several disciplines and thus provide a mechanism by which stochastic models are made available for spatiotemporal control purposes.	algorithm;calculus of variations;communicating sequential processes;control flow;control theory;coupled map lattice;dimensionality reduction;dynamical system;emoticon;frequency response;integrodifference equation;kalman filter;machine learning;nonlinear system;point process;process architecture;recurrence relation;signal processing;simulation;smoothing;spatiotemporal database;state space;stochastic process;system identification;the filter;timation;variational principle;vii;while;wikileaks	Andrew Zammit Mangion	2011				AI	32.67145214805723	-28.348310037379097	134528
e33117e515f6191084f6aa683230eb5381a6ee3c	hierarchical bayesian level set inversion	qa mathematics	The level set approach has provenwidely successful in the study of inverse problems for interfaces, since its systematic development in the 1990s. Recently it has been employed in the context of Bayesian inversion, allowing for the quantification of uncertainty within the reconstruction of interfaces. However, the Bayesian approach is very sensitive to the length and amplitude scales in the prior probabilistic model. This paper demonstrates how the scale-sensitivity can be circumvented by means of a hierarchical approach, using a single scalar parameter. Together with careful consideration of the development of algorithms which encode probabilitymeasure equivalences as the hierarchical parameter is varied, this leads to well-defined Gibbs-based MCMC methods found by alternating Metropolis–Hastings updates of the level set function and the hierarchical parameter. These methods demonstrably outperform non-hierarchical Bayesian level set methods.		Matthew M. Dunlop;Marco A. Iglesias;Andrew M. Stuart	2017	Statistics and Computing	10.1007/s11222-016-9704-8	bayesian average;econometrics;machine learning;mathematics;bayesian hierarchical modeling;statistics	ML	26.7297288759693	-26.276990627936247	134687
23cbbfc2b420611f0da7cb401a9a4e9c094ff916	eigengp: gaussian process models with adaptive eigenfunctions		Gaussian processes (GPs) provide a nonparametric representation of functions. However, classical GP inference suffers from high computational cost for big data and it is difficult to design nonstationary GP priors in practice. In this paper, we propose a sparse Gaussian process model, EigenGP, based on data-dependent eigenfunctions of a GP prior. The data-dependent eigenfunctions make the Gaussian process nonstationary and can be viewed as dictionary elements. We learn these dictionary elements in an empirical Bayesian framework; more specifically, we learn all hyperparameters associated with these elements—including basis points and lengthscales—from data by maximizing the model marginal likelihood. We explore computational linear algebra to simplify the gradient computation significantly. Our experimental results demonstrate improved predictive performance of EigenGP over alternative sparse GP methods such as the Nyströ method and sparse spectrum Gaussian process regression.	algorithmic efficiency;bayesian network;big data;computation;computer multitasking;data dependency;dictionary;gaussian process;gradient;kriging;linear algebra;marginal model;multi-task learning;nyström method;process modeling;semi-supervised learning;sparse matrix	Hao Peng;Yuan Qi	2015			machine learning;pattern recognition;mathematics;statistics	ML	27.76188012764949	-32.64592445461973	134700
3775d612df98c5b83921691be05994ab9f3932f5	pareto cascade modeling of diffusion networks		Time plays an essential role in the diffusion of information, influence and disease over networks. Usually we are only able to collect cascade data in which an infection (receiving) time of each node is recorded but without any transmission information over the network. In this paper, we infer the transmission rates among nodes by Pareto distributions. Pareto modeling has several advantages. It is naturally motivated and has a nice interpretation. The scale parameter of a Pareto distribution naturally fits into the starting time of a transition, i.e., the infection time of a parent node in the cascade data is the starting point for a transition from the parent to its receiver. The shape parameter (alpha) serves as the transition rate. The larger the alpha is, the faster the transition is and there is a higher probability for disease or information to spread in a short time period. Pareto modeling is mathematically simple and computationally easy. It has explicit solutions for the optimization problem that maximizes time-dependent pairwise transmission likelihoods between all pairs of nodes. We present three modelings with a common transmission rate, with different transmission rates and with different infection rates. Experiments on real and synthetic data show that our models accurately estimate the transmission rates and perform better than the existing method.	bayesian information criterion;experiment;fits;mathematical optimization;missing data;nice (unix);optimization problem;pareto efficiency;synthetic data;tree (data structure)	Christopher Ma;Xin Dang;Yixin Chen;Dawn Wilkins	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489509	pareto distribution;transition rate matrix;probability density function;pattern recognition;artificial intelligence;scale parameter;pareto principle;shape parameter;optimization problem;computer science;pairwise comparison	ML	33.054689638689894	-28.05689471220151	135243
c742d6aba2d2b6884b82b7a845dff8f8284682c5	the impact of missing values on pls model fitting		The analysis of interactive marketing campaigns frequently requires the investigation of latent constructs. Consequently, structural equation modeling is well established in this domain. Noticeably, the Partial-Least-Squares (PLS) algorithm is gaining popularity in the analysis of interactive marketing applications which may be attributed to its accuracy and robustness when data are not normally distributed. Moreover, the PLS algorithm also appraises incomplete data. This study reports from a simulation experiment in which a set of complete observations is blended with different patterns of missing values. We consider the impacts on the overall model fit, the outer model fit, and the assessment of significance by bootstrapping. Our results cast serious doubts on PLS algorithms’ ability to cope with missing values in a data set.	algorithm;curve fitting;latent variable;missing data;partial least squares regression;simulation;structural equation modeling	Moritz Parwoll;Ralf Wagner	2010		10.1007/978-3-642-24466-7_55	path coefficient;econometrics;robustness (computer science);missing data;structural equation modeling;popularity;mathematics;bootstrapping	Comp.	28.166245128112642	-23.997780364663573	135256
9aee66d71f23925077cb8cd4937c54d65ddf5c66	low-rank matrix decomposition in l1-norm by dynamic systems	dynamic system;low rank matrix approximation;l1 norm;l 1 norm;computational efficiency	"""Low-rank matrix approximation is used in many applications of computer vision, and is frequently implemented by singular value decomposition under L""""2-norm sense. To resist outliers and handle matrix with missing entries, a few methods have been proposed for low-rank matrix approximation in L""""1 norm. However, the methods suffer from computational efficiency or optimization capability. Thus, in this paper we propose a solution using dynamic system to perform low-rank approximation under L""""1-norm sense. From the state vector of the system, two low-rank matrices are distilled, and the product of the two low-rank matrices approximates to the given measurement matrix with missing entries, in L""""1 norm. With the evolution of the system, the approximation accuracy improves step by step. The system involves a parameter, whose influences on the computational time and the final optimized two low-rank matrices are theoretically studied and experimentally valuated. The efficiency and approximation accuracy of the proposed algorithm are demonstrated by a large number of numerical tests on synthetic data and by two real datasets. Compared with state-of-the-art algorithms, the newly proposed one is competitive."""	dynamical system;taxicab geometry	Yiguang Liu;Bingbing Liu;Yi-Fei Pu;Xiaohui Chen;Hong Cheng	2012	Image Vision Comput.	10.1016/j.imavis.2012.06.012	mathematical optimization;combinatorics;discrete mathematics;taxicab geometry;matrix norm;dynamical system;machine learning;mathematics;state-transition matrix;matrix;low-rank approximation	Robotics	25.946590161915235	-36.010310005540816	135289
4fdb7c1f0018013b0715c8bdf80608064cb9c2f9	stochastic convex sparse principal component analysis	signal image and speech processing;convex pca;systems biology;computational biology bioinformatics;biomedical engineering;sparse pca;proximal mapping	Principal component analysis (PCA) is a dimensionality reduction and data analysis tool commonly used in many areas. The main idea of PCA is to represent high-dimensional data with a few representative components that capture most of the variance present in the data. However, there is an obvious disadvantage of traditional PCA when it is applied to analyze data where interpretability is important. In applications, where the features have some physical meanings, we lose the ability to interpret the principal components extracted by conventional PCA because each principal component is a linear combination of all the original features. For this reason, sparse PCA has been proposed to improve the interpretability of traditional PCA by introducing sparsity to the loading vectors of principal components. The sparse PCA can be formulated as an ℓ1 regularized optimization problem, which can be solved by proximal gradient methods. However, these methods do not scale well because computation of the exact gradient is generally required at each iteration. Stochastic gradient framework addresses this challenge by computing an expected gradient at each iteration. Nevertheless, stochastic approaches typically have low convergence rates due to the high variance. In this paper, we propose a convex sparse principal component analysis (Cvx-SPCA), which leverages a proximal variance reduced stochastic scheme to achieve a geometric convergence rate. We further show that the convergence analysis can be significantly simplified by using a weak condition which allows a broader class of objectives to be applied. The efficiency and effectiveness of the proposed method are demonstrated on a large-scale electronic medical record cohort.	addresses (publication format);computation (action);convergence (action);courant–friedrichs–lewy condition;dimensionality reduction;electronic health records;electronics, medical;extraction;f7 wt allele;iteration;mathematical optimization;medical records;optimization problem;principal component analysis;proximal gradient method;proximal gradient methods for learning;rate of convergence;sample variance;sparse pca;sparse matrix	Inci M. Baytas;Kaixiang Lin;Fei Wang;Anil K. Jain;Jiayu Zhou	2016		10.1186/s13637-016-0045-x	biology;sparse pca;computer science;bioinformatics;machine learning;pattern recognition;biological engineering;systems biology;principal component analysis	ML	27.007680403575375	-37.125187697554	135316
272e48dcee2d24e114869cb9e54567e5fe899d94	using markov blankets for causal structure learning	structure learning;conditional independence;statistical test;support vector regression;data distribution;feature selection;recursive feature elimination	We show how a generic feature-selection algorithm returning strongly relevant variables can be turned into a causal structure-learning algorithm. We prove this under the Faithfulness assumption for the data distribution. In a causal graph, the strongly relevant variables for a node X are its parents, children, and children’s parents (or spouses), also known as the Markov blanket of X . Identifying the spouses leads to the detection of the V-structure patterns and thus to causal orientations. Repeating the task for all variables yields a valid partially oriented causal graph. We first show an efficient way to identify the spouse links. We then perform several experiments in the continuous domain using the Recursive Feature Elimination feature-selection algorithm with Support Vector Regression and empirically verify the intuition of this direct (but computationally expensive) approach. Within the same framework, we then devise a fast and consistent algorithm, Total Conditioning (TC), and a variant, TCbw, with an explicit backward feature-selection heuristics, for Gaussian data. After running a series of comparative experiments on five artificial networks, we argue that Markov blanket algorithms such as TC/TCbw or Grow-Shrink scale better than the reference PC algorithm and provides higher structural accuracy.	analysis of algorithms;causal filter;causal graph;experiment;feature selection;heuristic (computer science);markov blanket;markov chain;recursion (computer science);selection algorithm;support vector machine	Jean-Philippe Pellet;André Elisseeff	2008	Journal of Machine Learning Research	10.1145/1390681.1442776	support vector machine;statistical hypothesis testing;conditional independence;computer science;machine learning;pattern recognition;mathematics;feature selection;statistics	ML	24.622885489378003	-28.313210108049823	135892
05f59c1d7d7b152125193253c3b3f580d0d9a351	a geometric view of conjugate priors	bayes estimation;exponential distribution;learning algorithm;generic model;supervised learning;modelo generativo;ley exponencial;loi exponentielle;modelo hibrido;bregman divergence;conjugate prior;intelligence artificielle;algorithme apprentissage;divergence bregman;modele generatif;modele hybride;exponential family;semi supervised learning;famille exponentielle;hybrid model;estimacion bayes;divergencia de bregman;machine learning;exponential families;conjugacion;generative model;artificial intelligence;familia exponencial;inteligencia artificial;generative models;apprentissage supervise;learning artificial intelligence;aprendizaje supervisado;algoritmo aprendizaje;conjugation;conjugaison;estimation bayes;apprentissage intelligence artificielle	In Bayesian machine learning, conjugate priors are popular, mostly due to mathematical convenience. In this paper, we show that there are deeper reasons for choosing a conjugate prior. Specifically, we formulate the conjugate prior in the form of Bregman divergence and show that it is the inherent geometry of conjugate priors that makes them appropriate and intuitive. This geometric interpretation allows one to view the hyperparameters of conjugate priors as the effective sample points, thus providing additional intuition. We use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the generative and discriminative components of a hybrid model for semi-supervised learning.	bayesian network;bregman divergence;machine learning;semi-supervised learning;semiconductor industry;supervised learning	Arvind Agarwal;Hal Daumé	2010	Machine Learning	10.1007/s10994-010-5203-x	exponential family;computer science;artificial intelligence;machine learning;mathematics;supervised learning;statistics	AI	27.13251931805403	-30.366662325219348	136144
f305bbb469ae81d7221066f77ab4272c73feee03	a framework for optimal matching for causal inference		We propose a novel framework for matching estimators for causal effect from observational data that is based on minimizing the dual norm of estimation error when expressed as an operator. We show that many popular matching estimators can be expressed as optimal in this framework, including nearest-neighbor matching, coarsened exact matching, and mean-matched sampling. This reveals their motivation and aptness as structural priors formulated by embedding the effect in a particular functional space. This also gives rise to a range of new, kernel-based matching estimators that arise when one embeds the effect in a reproducing kernel Hilbert space. Depending on the case, these estimators can be found using either quadratic optimization or integer optimization. We show that estimators based on universal kernels are universally consistent without model specification. In empirical results using both synthetic and real data, the new, kernel-based estimators outperform all standard causal estimators in estimation error.	causal filter;causal inference;optimal matching	Nathan Kallus	2017			econometrics;mathematical optimization;mathematics;statistics	Vision	26.471490492107446	-27.81611880792216	136204
8261bde131675048f2d80dacdec526418b3944ef	application of scattered data approximation to a rotorcraft health monitoring problem	scattered data;health monitoring	An adaptive and matrix–free scheme has been developed for interpolating and approximating sparse multi–dimensional scattered data and has been applied to a timeseries problem in rotorcraft. The Sequential Function Approximation (SFA) method is based on a sequential Galerkin approach to artificial neural networks and requires neither ad–hoc parameters for the user to tune, nor rescaling of the inputs. It is linear in storage with respect to the number of samples. The SFA method has been used to model and extrapolate the time series data from a critical temperature sensor in a 1/4 scale model of the V-22 Osprey. The SFA regression model, constructed with radial basis functions, has also been used satisfactorily to evaluate the sensitivity to 74 system health and safety–of–flight parameters during a series of wind–tunnel tests. An upper bound on the error convergence rate that is exponential and does not explicitly depend on the dimensionality of the approximation was derived and confirmed for the time series data.	algorithmic efficiency;approximation;approximation algorithm;artificial neural network;b-spline;black box;computation;curse of dimensionality;emoticon;extrapolation;field electron emission;galerkin method;health management system;hoc (programming language);interpolation;mathematical model;mathematical optimization;nl (complexity);polynomial;radial (radio);radial basis function;rate of convergence;simple features;sparse matrix;time complexity;time series;x86 memory segmentation	Andrew J. Meade	2004	JACIC	10.2514/1.1978	simulation;computer science;engineering	ML	33.734899347747664	-30.09374560782381	136254
6ea1c4559411a93554b0c679827ddce0fb06a494	sparse time-frequency representation for incipient fault diagnosis of wind turbine drive train		As wind power attracts increasing attention and wind turbines (WTs) capacity expands, fault diagnosis of WT is playing a more and more important role in improving reliability, minimizing down time, reducing maintenance costs, and providing reliable power generation. In this paper, a novel sparse time-frequency representation (STFR) method is proposed to increase the diagnostic precision of incipient faults. The proposed method can be applied once the condition is detected as abnormal according to the VDI3834 vibration threshold standard in WT fault diagnosis systems. The proposed method is a novel signal representation method based on the sparse representation theory and Wigner–Ville distribution (WVD), which can overcome the limitations of traditional basis functions expansion and time-frequency analysis methods. In this method, a union of redundant dictionary (URD) is constructed on the basis of the underlying prior information of the oscillate characteristics with multicomponent coupling effect and different morphological waveforms. Therefore, the vibration signal can be sparsely represented over the URD. Then, the sparse coefficients and corresponding atoms can be obtained by solving the basis pursuit denoising problem via alternating direction method of multipliers. Based on the combination of the WVD of each atom and corresponding sparse coefficient, the time-frequency distribution of the vibration signal can be obtained. To verify the effectiveness of the STFR method, a simulation and two field tests in the wind farm are performed. The comparison results with state-of-the-art methods illustrate the superiority and robustness of the proposed method in the engineering applications.	atom;augmented lagrangian method;basis function;basis pursuit denoising;coefficient;dictionary;downtime;experiment;frequency analysis;information extraction;interference (communication);noise reduction;simulation;sparse approximation;sparse matrix;time–frequency analysis;time–frequency representation;wigner distribution function;wigner quasiprobability distribution	Boyuan Yang;Ruonan Liu;Xuefeng Chen	2018	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2018.2828739	algorithm;wind power;electronic engineering;robustness (computer science);time–frequency analysis;mathematics;sparse approximation;waveform;time–frequency representation;basis function;basis pursuit denoising	AI	37.41046036289135	-30.92953679624659	136257
e31643e920c7cfadc68be9265f2bfd1e1c582134	on a dimension reduction regression with covariate adjustment	covariate adjusted regression;dimension reduction;62g08;62g20;variable selection;central subspace;linear model;central subspace covariate adjusted regression dimension reduction;semiparametric model;simulation study;asymptotic normal;62h12	In this paper, we consider a semiparametric modeling with multi-indices when neither the response nor the predictors can be directly observed and there are distortions from some multiplicative factors. In contrast to the existing methods in which the response distortion deteriorates estimation efficacy even for a simple linear model, the dimension reduction technique presented in this paper interestingly does not have to account for distortion of the response variable. The observed response can be used directly whether distortion is present or not. The resulting dimension reduction estimators are shown to be consistent and asymptotically normal. The results can be employed to test whether the central dimension reduction subspace has been estimated appropriately and whether the components in the basis directions in the space are significant. Thus, the method provides an alternative for determining the structural dimension of the subspace and for variable selection. A simulation study is carried out to assess the performance of the proposed method. The analysis of a real dataset demonstrates the potential usefulness of distortion removal.	bundle adjustment;dimensionality reduction	Jun Zhang;Li-Ping Zhu;Li-Xing Zhu	2012	J. Multivariate Analysis	10.1016/j.jmva.2011.06.004	sufficient dimension reduction;econometrics;mathematical optimization;linear model;mathematics;feature selection;semiparametric model;statistics;dimensionality reduction	NLP	29.110401799853523	-25.0327334773282	136333
37c458226e412cdcebb432d67dad72e458aa3fed	sambaten: sampling-based batch incremental tensor decomposition		Tensor decompositions are invaluable tools in analyzing multimodal datasets. In many real-world scenarios, such datasets are far from being static, to the contrary they tend to grow over time. For instance, in an online social network setting, as we observe new interactions over time, our dataset gets updated in its “time” mode. How can we maintain a valid and accurate tensor decomposition of such a dynamically evolving multimodal dataset, without having to re-compute the entire decomposition after every single update? In this paper we introduce SAMBATEN, a Sampling-based Batch Incremental Tensor Decomposition algorithm, which incrementally maintains the decomposition given new updates to the tensor dataset. SAMBATEN is able to scale to datasets that the state-of-theart in incremental tensor decomposition is unable to operate on, due to its ability to effectively summarize the existing tensor and the incoming updates, and perform all computations in the reduced summary space. We extensively evaluate SAMBATEN using synthetic and real datasets. Indicatively, SAMBATEN achieves comparable accuracy to state-of-the-art incremental and non-incremental techniques, while being 2530 times faster. Furthermore, SAMBATEN scales to very large sparse and dense dynamically evolving tensors of dimensions up to 100K × 100K × 100K where state-of-the-art incremental approaches were not able to operate.	algorithm;approximation;computation;dynamic problem (algorithms);gibbs sampling;multimodal interaction;scalability;social network;sparse matrix;synthetic intelligence	Ekta Gujral;Ravdeep Pasricha;Evangelos E. Papalexakis	2018		10.1137/1.9781611975321.44	artificial intelligence;computer science;tensor;machine learning;sampling (statistics);theoretical computer science;computation	ML	25.553765649345625	-33.27769641454035	136433
37ca074471530bfc93cda54bc57a4ec24abf12fd	does hamiltonian monte carlo mix faster than a random walk on multimodal densities?		Hamiltonian Monte Carlo (HMC) is a very popular and generic collection of Markov chain Monte Carlo (MCMC) algorithms. One explanation for the popularity of HMC algorithms is their excellent performance as the dimension d of the target becomes large: under conditions that are satisfied for many common statistical models, optimally-tuned HMC algorithms have a running time that scales like d. In stark contrast, the running time of the usual Random-Walk Metropolis (RWM) algorithm, optimally tuned, scales like d. This superior scaling of the HMC algorithm with dimension is attributed to the fact that it, unlike RWM, incorporates the gradient information in the proposal distribution. In this paper, we investigate a different scaling question: does HMC beat RWM for highly multimodal targets? We find that the answer is often no. We compute the spectral gaps for both the algorithms for a specific class of multimodal target densities, and show that they are identical. The key reason is that, within one mode, the gradient is effectively ignorant about other modes, thus negating the advantage the HMC algorithm enjoys in unimodal targets. We also give heuristic arguments suggesting that the above observation may hold quite generally. Our main tool for answering this question is a novel simple formula for the conductance of HMC using Liouville’s theorem. This result allows us to compute the spectral gap of HMC algorithms, for both the classical HMC with isotropic momentum and the recent Riemannian HMC, for multimodal targets.	algorithm;conductance (graph);gradient descent;heuristic;hybrid memory cube;hybrid monte carlo;image scaling;markov chain monte carlo;metropolis;monte carlo method;multimodal interaction;statistical model;time complexity	Oren Mangoubi;Natesh S. Pillai;Aaron Smith	2018	CoRR			Theory	27.025962469310553	-29.088942172173763	136496
e9181c0703567368b458a0a3730f98eb822d4da8	conditionally gaussian hypermodels for cerebral source localization	finite element methods;65c05;source localization;computer model;regularization method;prior information;finite element method;iterative algorithm;magnetoencephalography;computer experiment;markov chain monte carlo;74s05;map estimation;92c55;electroencephalography;electroencephalography magnetoencephalography;hierarchical model;bayesian model	Bayesian modeling and analysis of the MEG and EEG modalities provide a flexible framework for introducing prior information complementary to the measured data. This prior information is often qualitative in nature, making the translation of the available information into a computational model a challenging task. We propose a generalized gamma family of hyperpriors which allows the impressed currents to be focal and we advocate a fast and efficient iterative algorithm, the Iterative Alternating Sequential (IAS) algorithm for computing maximum a posteriori (MAP) estimates. Furthermore, we show that for particular choices of the scalar parameters specifying the hyperprior, the algorithm effectively approximates popular regularization strategies such as the Minimum Current Estimate and the Minimum Support Estimate. The connection between priorconditioning and adaptive regularization methods is also pointed out. The posterior densities are explored by means of a Markov Chain Monte Carlo (MCMC) strategy suitable for this family of hypermodels. The computed experiments suggest that the known preference of regularization methods for superficial sources over deep sources is a property of the MAP estimators only, and that estimation of the posterior mean in the hierarchical model is better adapted for localizing deep sources.	algorithm;bayesian network;computational model;electroencephalography;experiment;focal (programming language);hierarchical database model;internet authentication service;iterative method;magnetoencephalography;markov chain monte carlo;monte carlo method;the superficial	Daniela Calvetti;Harri Hakula;Sampsa Pursiainen;Erkki Somersalo	2009	SIAM J. Imaging Sciences	10.1137/080723995	computer simulation;mathematical optimization;maximum a posteriori estimation;machine learning;finite element method;mathematics;statistics	ML	28.59258988622706	-32.33086851669164	136652
8e7ab0279dd931a54e362e8c56b900fdb5ebf5f9	an entropy based method for causal discovery in linear acyclic model	linear acyclic model;causal inference;entropy	A new algorithm for causal discovery in linear acyclic graphic model is proposed in this paper. The algorithm measures the entropy of observed data sequences by estimating the parameters of its approximate distribution to a generalized Gaussian family. Causal ordering can be discovered by an entropy base method. Compared with previous method, the sample complexity of the proposed algorithm is much lower, which means the causal relationship can be correctly discovered by a smaller number of samples. An explicit requirement of data sequences for correct causal inference in linear acyclic graphic model is discussed. Experiment results for both artificial data and real-world data are presented.	causal filter;directed acyclic graph	Yulai Zhang;Guiming Luo	2013		10.1007/978-3-642-42042-9_32	econometrics;entropy;causal inference;machine learning;mathematics;statistics	ML	29.73241630433517	-30.009356531250067	136734
76f94129395ab33f6de57f87b906efdccc499d76	demscale: large scale mds accounting for a ridge operator and demographic variables	ridge operator;ridge regression;optimal method;large scale;visualization;mds;pca	In this paper, a method called DEMScale is introduced for large scale MDS. DEMScale can be used to reduce MDS problems into manageable sub-problems, which are then scaled separately. The MDS items can be split into sub-problems using demographic variables in order to choose the sections of the data with optimal and sub-optimal mappings. The lower dimensional solutions from the scaled sub-problems are recombined by taking sample points from each sub-problem, scaling the sample points, and using an affine mapping with a ridge operator to map the non-sample points. DEMScale builds upon the methods of distributional scaling and FastMDS, which are used to split and recombine MDS mappings. The use of a ridge regression parameter enables DEMScale to achieve stronger solution stability than the basic distributional scaling and FastMDS techniques. The DEMScale method is general, and is independent of the MDS technique and optimization method used.		Stephen L. France;J. Douglas Carroll	2009		10.1007/978-3-642-03915-7_5	econometrics;visualization;computer science;machine learning;data mining;mathematics;tikhonov regularization;statistics;principal component analysis	Vision	28.531027961541966	-34.60122919252142	136892
0771ea183435f00d0074ba5ade76207d73bb0ab8	a pure l1l1-norm principal component analysis	health research;uk clinical guidelines;biological patents;l1 regression;l 1 regression;europe pubmed central;citation search;l 1;uk phd theses thesis;principal component analysis;life sciences;linear programming;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	The L 1 norm has been applied in numerous variations of principal component analysis (PCA). An? L 1 -norm PCA is an attractive alternative to traditional L 2 -based PCA because it can impart robustness in the presence of outliers and is indicated for models where standard Gaussian assumptions about the noise may not apply. Of all the previously-proposed PCA schemes that recast PCA as an optimization problem involving the L 1 norm, none provide globally optimal solutions in polynomial time. This paper proposes an L 1 -norm PCA procedure based on the efficient calculation of the optimal solution of the L 1 -norm best-fit hyperplane problem. We present a procedure called L 1 - PCA ? based on the application of this idea that fits data to subspaces of successively smaller dimension. The procedure is implemented and tested on a diverse problem suite. Our tests show that L 1 - PCA ? is the indicated procedure in the presence of unbalanced outlier contamination.	principal component analysis	J. Paul Brooks;José H. Dulá;Edward L. Boone	2013	Computational Statistics & Data Analysis	10.1016/j.csda.2012.11.007	econometrics;sparse pca;linear programming;data mining;mathematics;statistics;principal component analysis	ML	27.556661838721524	-34.53907380065302	136934
86923c955791aefdb90dbc891d4bb8fc31781dc8	model selection and minimax estimation in generalized linear models	minimax estimation structural constraints minimax lower bounds kullback leibler risk general nonasymptotic upper bound complexity penalty penalized maximum likelihood estimation model selection criteria high dimensional data glm generalized linear models;complexity theory;maximum likelihood estimation;upper bound;computational modeling;minimax techniques maximum likelihood estimation;sparsity complexity penalty generalized linear models kullback leibler risk minimax estimator model selection;mathematical model;adaptation models;data models;maximum likelihood estimation data models upper bound complexity theory adaptation models computational modeling mathematical model	We consider model selection in generalized linear models (GLM) for high-dimensional data and propose a wide class of model selection criteria based on penalized maximum likelihood with a complexity penalty on the model size. We derive a general nonasymptotic upper bound for the Kullback-Leibler risk of the resulting estimators and establish the corresponding minimax lower bounds for the sparse GLM. For the properly chosen (nonlinear) penalty, the resulting penalized maximum likelihood estimator is shown to be asymptotically minimax and adaptive to the unknown sparsity. We also discuss possible extensions of the proposed approach to model selection in the GLM under additional structural constraints and aggregation.	generalized linear model;kullback–leibler divergence;minimax;model selection;nonlinear system;sparse matrix	Felix Abramovich;Vadim Grinshtein	2016	IEEE Transactions on Information Theory	10.1109/TIT.2016.2555812	data modeling;econometrics;mathematical optimization;mathematical model;mathematics;maximum likelihood;upper and lower bounds;maximum likelihood sequence estimation;computational model;statistics	ML	28.47785602527186	-30.095532318953857	137496
1f9ac4bb9527d47deec00f24efd72d7c1796ca0c	structural segmentation with the variable markov oracle and boundary adjustment	variable markov oracle;self similarity matrix music structure segmentation variable markov oracle;music structure segmentation;self similarity matrix;time series audio coding markov processes music;beatles iso dataset structural segmentation variable markov oracle boundary adjustment music structure segmentation suffix automaton multivariate time series repeating segments self similarity matrix global repetitive structure encoding music piece	For the music structure segmentation task, one wants to solve two co-existing but sometimes contradicting problems; find global repetitive/homogenous structures and locate accurate local change points. In this paper we propose two algorithms to address these two problems. The algorithms can independently or jointly be plugged into various existing structural segmentation algorithms to improve their results. One algorithm utilizes the Variable Markov Oracle, a suffix automaton for multi-variate time series capable of finding repeating segments, is proposed to obtain a self-similarity matrix which encodes the global repetitive structure of a music piece. The other proposed algorithm is an iterative boundary adjustment algorithm refining boundary locations. The algorithms are evaluated against the Beatles-ISO dataset and achieve comparable performance to state-of-the-art.	algorithm;iterative method;markov chain;regular expression;self-similarity matrix;similarity measure;suffix automaton;time series	Cheng-i Wang;Gautham J. Mysore	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7471683	speech recognition;machine learning;pattern recognition;mathematics;scale-space segmentation	Robotics	31.44919872925263	-30.972016290990922	137587
ae894e14c62b4de514f30b7e4cb19347964f6a2d	the linearization of belief propagation on pairwise markov random fields		Belief Propagation (BP) is a widely used approximation for exact probabilistic inference in graphical models, such as Markov Random Fields (MRFs). In graphs with cycles, however, no exact convergence guarantees for BP are known, in general. For the case when all edges in the MRF carry the same symmetric, doubly stochastic potential, recent works have proposed to approximate BP by linearizing the update equations around default values, which was shown to work well for the problem of node classification. The present paper generalizes all prior work and derives an approach that approximates loopy BP on any pairwise MRF with the problem of solving a linear equation system. This approach combines exact convergence guarantees and a fast matrix implementation with the ability to model heterogenous networks. Experiments on synthetic graphs with planted edge potentials show that the linearization has comparable labeling accuracy as BP for graphs with weak potentials, while speeding-up inference by orders of magnitude.	approximation algorithm;belief propagation;casio loopy;christos faloutsos;computation;doubly stochastic model;experiment;fixed point (mathematics);graphical model;ground truth;ibm notes;iterative method;linear algebra;linear equation;markov chain;markov random field;software propagation;source lines of code;synthetic data;synthetic intelligence;system of linear equations	Wolfgang Gatterbauer	2017			mathematical optimization;combinatorics;discrete mathematics;markov model;belief propagation;variable-order markov model	AI	26.3595407418224	-29.895519596490818	137856
04aad8371ec9cd565c5d59be88de5741a582017e	a model for learning variance components of natural images	distributed representation;variance component;natural images;independent component analysis;higher order;hierarchical bayesian model	We present a hierarchical Bayesian model for learning effici nt codes of higher-order structure in natural images. The model, a nonlinear generalization of independent component analysis, replaces t he tandard assumption of independence for the joint distribution of coef ficients with a distribution that is adapted to the variance structure of t he coefficients of an efficient image basis. This offers a novel description o f higherorder image structure and provides a way to learn coarse-cod d, sparsedistributed representations of abstract image properties such as object location, scale, and texture.	bayesian network;code;coefficient;independent component analysis;nonlinear system;random effects model;texture mapping	Yan Karklin;Michael S. Lewicki	2002			independent component analysis;higher-order logic;computer science;machine learning;pattern recognition;mathematics;statistics	ML	30.664748171072805	-33.67377108376719	137942
287214c4519a9a9399c231e1fc0594cab15e4abf	learning heat diffusion graphs	heat diffusion;representation theory;graph signal processing;sparse prior;laplacian matrix learning	Information analysis of data often boils down to properly identifying their hidden structure. In many cases, the data structure can be described by a graph representation that supports signals in the dataset. In some applications, this graph may be partly determined by design constraints or predetermined sensing arrangements. In general though, the data structure is not readily available nor easily defined. In this paper, we propose to represent structured data as a sparse combination of localized functions that live on a graph. This model is more appropriate to represent local data arrangements than the classical global smoothness prior. We focus on the problem of inferring the connectivity that best explains the data samples at different vertices of a graph that is a priori unknown. We concentrate on the case where the observed data are actually the sum of heat diffusion processes, which is a widely used model for data on networks or other irregular structures. We cast a new graph learning problem and solve it with an efficient nonconvex optimization algorithm. Experiments on both synthetic and real world data finally illustrate the benefits of the proposed graph learning framework and confirm that the data structure can be efficiently learned from data observations only. We believe that our algorithm will help solving key questions in diverse application domains such as social and biological network analysis where it is crucial to unveil proper data structure for understanding and inference.	algorithm;biological network;data structure;experiment;graph (abstract data type);mathematical optimization;social network analysis;sparse matrix;synthetic intelligence	Dorina Thanou;Xiaowen Dong;Daniel Kressner;Pascal Frossard	2017	IEEE Transactions on Signal and Information Processing over Networks	10.1109/TSIPN.2017.2731164	mathematical optimization;combinatorics;representation theory;null model;computer science;graph partition;machine learning;mathematics;heat equation;critical graph;intersection graph;statistics	ML	28.731700858907566	-36.973212316356005	137967
69b5f4b09cc9d399f6ccf41de104e214c6850d65	nonparametric von mises estimators for entropies, divergences and mutual informations		We propose and analyse estimators for statistical functionals of one or more distributions under nonparametric assumptions. Our estimators are derived from the von Mises expansion and are based on the theory of influence functions, which appear in the semiparametric statistics literature. We show that estimators based either on data-splitting or a leave-one-out technique enjoy fast rates of convergence and other favorable theoretical properties. We apply this framework to derive estimators for several popular information theoretic quantities, and via empirical evaluation, show the advantage of this approach over existing estimators.	big data;cluster analysis;ibm notes;information theory;semiparametric model	Kirthevasan Kandasamy;Akshay Krishnamurthy;Barnabás Póczos;Larry A. Wasserman;James M. Robins	2015			econometrics;extremum estimator;pattern recognition;m-estimator;mathematics;statistics	ML	30.808197750393806	-24.855791444065822	138119
3bcdc0ba3bfa9209a0c9fdbfd063705792cc2ed9	feature extraction analysis using filter banks for faults classification in induction motors		Different studies have been worked about induction motor bearings fault detection using digital signal processing and pattern recognition techniques. However, performance of these techniques is related with the use of correct features. This paper presents an analysis of the use of filter banks with uniform and nonuniform frequency subbands to features extraction from vibration signals. Classification was developed by an artificial neural network with feedforward connections. Results identifies that the employment of filter banks improve the accuracy in 23% for six considered classes related with faults in bearings.	artificial neural network;digital signal processing;fault detection and isolation;feature extraction;feedforward neural network;filter bank;mathematical induction;pattern recognition	David R Holmes Jr;Alvaro David Orjuela-Cañón;Oscar D. Florez	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489736	frequency domain;machine learning;digital signal processing;feature extraction;artificial neural network;feed forward;artificial intelligence;induction motor;fault detection and isolation;computer science;bearing (mechanical)	Robotics	36.887993339628956	-31.599738587327007	138172
21c380b5481814fd0a9608e9f6afbb03e9d6aee8	instrument learning and sparse nmd for automatic polyphonic music transcription		"""In this paper, an automatic music transcription (AMT) algorithm based on a supervised non-negative matrix decomposition (NMD) is discussed. In particular, a novel approach for enhancing the sparsity of the solution is proposed. It consists of a two-step processing in which the NMD is solved joining a <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _2$</tex-math></inline-formula> regularization and a threshold filtering. In the first step, the NMD is performed with the <inline-formula><tex-math notation=""""LaTeX"""">$\ell _2$</tex-math></inline-formula> regularization in order to get an overall selection of the notes most likely appearing in the monotimbral musical excerpt. In the second step, a threshold filtering followed by another <inline-formula><tex-math notation=""""LaTeX""""> $\ell _2$</tex-math></inline-formula> regularized NMD are repeatedly performed in order to progressively reduce the dictionary matrix and to refine the notes transcription. Furthermore, a user-oriented instrument learning procedure has been conceived and proposed. The proposed AMT system has been tested upon the dataset collected by the LabROSA laboratories considering the transcription of three different pianos. Moreover, it has been validated through a comparison with a regularized NMD and with three open source AMT software. The results prove the effectiveness of the proposed two-step processing in enhancing the sparsity of the solution and in improving the transcription accuracy. Moreover, the proposed system shows promising performance in both multi-F0 and note tracking tasks, obtaining better transcription accuracy than the competing algorithms in most tests."""	algorithm;dictionary;medical transcription;non-negative matrix factorization;open-source software;sparse matrix;transcription (software)	Antonello Rizzi;Mario Antonelli;Massimiliano Luzi	2017	IEEE Transactions on Multimedia	10.1109/TMM.2017.2674603	computer science;pattern recognition;artificial intelligence;filter (signal processing);hidden markov model;matrix decomposition;regularization (mathematics);machine learning;matrix (mathematics);transcription (biology)	Vision	31.39033110342923	-37.46456248574294	138253
c8ae7dca78e4b781a686919f5e0bbefe69d077f1	matrix factorization with side and higher order information		The problem of predicting unobserved entries of a partially observed matrix has found wide applicability in several areas, such as recommender systems, computational biology, and computer vision. Many scalable methods with rigorous theoretical guarantees have been developed for algorithms where the matrix is factored into low-rank components, and embeddings are learned for the row and column entities. While there has been recent research on incorporating explicit side information in the low-rank matrix factorization setting, often implicit information can be gleaned from the data, via higher order interactions among entities. In this paper, we design a method to make use of this implicit information, via random walks on graphs. We show that the problem we intend to solve can be cast as factoring a nonlinear transform of the (partially) observed matrix and develop an efficient coordinate descent based algorithm for the same. Experiments on several datasets show that the method we propose outperforms vanilla matrix factorization, and also those methods that use explicitly available side information.	algorithm;computational biology;computer vision;coordinate descent;entity;integer factorization;interaction;mathematical optimization;nonlinear system;optimization problem;recommender system;scalability;the matrix	Vatsal Shah;Nikhil Rao;Weicong Ding	2017	CoRR		mathematical optimization;combinatorics;theoretical computer science;machine learning;mathematics;statistics	ML	27.11153988681076	-35.364407442671535	138349
0d91db3e0b13f6d96a08fa8fa4c8bb5aaf775d75	statistical translation, heat kernels and expected distances		High dimensional structured data such as text and images is often poorly understood and misrepresented in statistical modeling. The standard histogram representation suffers from high variance and performs poorly in general. We explore novel connections between statistical translation, heat kernels on manifolds and graphs, and expected distances. These connections provide a new framework for unsupervised metric learning for text documents. Experiments indicate that the resulting distances are generally superior to their more standard counterparts.	experiment;statistical machine translation;statistical model	Joshua V. Dillon;Yi Mao;Guy Lebanon;Jian Zhang	2007			combinatorics;machine learning;pattern recognition;mathematics;statistics	ML	25.96980212575268	-31.19146779263804	138617
1beeaee23a0af6334c8332100baec64def2954a4	a parameter-free kernel design based on cumulative distribution function for correntropy	statistics entropy functions;cdf parameter free kernel design cumulative distribution function translation invariant kernel positive definite kernel autocorrentropy function generalized similarity measure spectral density estimator gaussian kernel;statistics;kernel noise correlation estimation fourier transforms random processes;entropy;functions	This paper proposes a parameter-free kernel that is translation invariant and positive definite. The new kernel is based on the data cumulative distribution function (CDF) that provides all the statistical information about the observed samples. Without an explicit kernel size parameter, this novel kernel is used to define the autocorrentropy function, which is a generalized similarity measure, and spectral density estimator. Numerical examples show that the proposed method provides comparable performance to the existing Gaussian kernel with optimized kernel size.	autocorrelation;kernel (operating system);numerical method;similarity measure;spectral density estimation;stationary process	Jongmin Lee;Pingping Zhu;José Carlos Príncipe	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6707021	kernel;poisson kernel;principal component regression;kernel regression;kernel density estimation;kernel method;entropy;mathematical optimization;kernel fisher discriminant analysis;kernel embedding of distributions;radial basis function kernel;mean-shift;kernel principal component analysis;pattern recognition;multivariate kernel density estimation;mathematics;variable kernel density estimation;polynomial kernel;function;statistics;kernel smoother	ML	31.229075816500128	-27.379919121970765	138953
0b16f89de93917fb6297c69f61fcadf0fcc65b0b	fuzzy — weighted averaging for high-resolution ecg based on exploratory data analysis		In this work we introduce a method for the enhancement of Late Potentials in the Signal Averaged electrocardiography. The method involves computation of weights prior averaging. Two fuzzy control techniques are proposed for the derivation of weights. The experimental results indicate the contribution of the method to a more reliable prognosis.	computation;fuzzy control system;image resolution	Nikolaos A. Laskaris;Spiros Fotopoulos;A. Bezerianos;A. Manolis	1996	1996 8th European Signal Processing Conference (EUSIPCO 1996)		electronic engineering;noise measurement;pattern recognition;mathematics;statistics	Robotics	37.152040667985915	-36.10397625399668	139015
35a901607caf974d0ca21d3778f5aa8d7d79a66d	orion: online regularized multi-task regression and its application to ensemble forecasting	predictive models forecasting computational modeling equations prediction algorithms manganese data models;online multi task learning;forecasting;prediction algorithms;online learning orion online regularized multitask regression nonlinear dynamic system ensemble forecasting task;manganese;ensemble forecasting online multi task learning;computational modeling;weather forecasting geophysics computing learning artificial intelligence regression analysis;predictive models;ensemble forecasting;data models	Ensemble forecasting is a well-known numerical prediction technique for modeling the evolution of nonlinear dynamic systems. The ensemble member forecasts are generated from multiple runs of a computer model, where each run is obtained by perturbing the starting condition or using a different model representation of the dynamic system. The ensemble mean or median is typically chosen as the consensus point estimate of the aggregated forecasts for decision making purposes. These approaches are limited in that they assume each ensemble member is equally skill ful and do not consider their inherent correlations. In this paper, we cast the ensemble forecasting task as an online, multi-task regression problem and present a framework called ORION to estimate the optimal weights for combining the ensemble members. The weights are updated using a novel online learning with restart strategy as new observation data become available. Experimental results on seasonal soil moisture predictions from 12 major river basins in North America demonstrate the superiority of the proposed approach compared to the ensemble median and other baseline methods.	baseline (configuration management);computer multitasking;computer simulation;dynamical system;ensemble forecasting;laplacian matrix;nonlinear system;numerical analysis;numerical weather prediction;online machine learning	Jianpeng Xu;Pang-Ning Tan;Lifeng Luo	2014	2014 IEEE International Conference on Data Mining	10.1109/ICDM.2014.90	probabilistic forecasting;forecasting;computer science;data science;manganese;machine learning;ensemble forecasting;data mining;ensemble learning;statistics	Robotics	28.635760719986372	-33.909477188499714	139344
0d0cb7cb25dc19636eb5c27000b2190988502104	differentially private density estimation via gaussian mixtures model	gmm differentially private density estimation gaussian mixtures model probability density function data analysis dpgmm parametric density estimation algorithm differential privacy expectation maximization algorithm em algorithm noise adding step calibrated noise l 1 sensitivities post processing step;noise measurement;data analysis;estimation;data privacy;covariance matrices;mixture models data analysis data privacy expectation maximisation algorithm gaussian processes;privacy gaussian distribution estimation data privacy covariance matrices noise measurement data analysis;gaussian distribution;privacy	Density estimation can construct an estimate of the probability density function from the observed data. However, such a function may compromise the privacy of individuals. A notable paradigm for offering strong privacy guarantees in data analysis is differential privacy. In this paper, we propose DPGMM, a parametric density estimation algorithm using Gaussian mixtures model (GMM) under differential privacy. GMM is a well-known model that could approximate any distribution and can be solved via Expectation-Maximization (EM) algorithm. The main idea of DPGMM is to add two extra steps after getting the estimated parameters in the M step of each iteration. The first step is the noise adding step, which injects calibrated noise to the estimated parameters according to their L1-sensitivities and privacy budgets. The second step is the post-processing step, which post-processes those noisy parameters that might break their intrinsic characteristics. Extensive experiments using both real and synthetic datasets evaluate the performance of DPGMM, and demonstrate that the proposed method outperforms a state-of-art approach.	approximation algorithm;differential privacy;expectation–maximization algorithm;experiment;google map maker;iteration;programming paradigm;synthetic intelligence;video post-processing	Yuncheng Wu;Yao Wu;Hui Peng;Juru Zeng;Hong Chen;Cuiping Li	2016	2016 IEEE/ACM 24th International Symposium on Quality of Service (IWQoS)	10.1109/IWQoS.2016.7590445	normal distribution;gaussian noise;estimation;information privacy;computer science;noise measurement;data mining;data analysis;privacy;statistics	DB	30.74661029765434	-29.61478163267905	139373
7aecc1724bb7b63eda1464e1200c0b21734343f6	learning mixtures of random utility models		We tackle the problem of identifiability and efficient learning of mixtures of Random Utility Models (RUMs). We show that when the PDFs of utility distributions are symmetric, the mixture of k RUMs (denoted by k-RUM) is not identifiable when the number of alternatives m is no more than 2k − 1. On the other hand, when m ≥ max{4k − 2, 6}, any k-RUM is generically identifiable. We then propose three algorithms for learning mixtures of RUMs: an EM-based algorithm, which we call E-GMM, a direct generalized-methodof-moments (GMM) algorithm, and a sandwich (GMM-EGMM) algorithm that combines the other two. Experiments on synthetic data show that the sandwich algorithm achieves the highest statistical efficiency and GMM is the most computationally efficient. Experiments on real-world data at Preflib show that Gaussian k-RUMs provide better fitness than a single Gaussian RUM, the Plackett-Luce model, and mixtures of Plackett-Luce models w.r.t. commonly-used model fitness criteria. To the best of our knowledge, this is the first work on learning mixtures of general RUMs.	algorithm;algorithmic efficiency;computation;experiment;fitness function;google map maker;portable document format;synthetic data	Zhibing Zhao;Tristan Villamil;Lirong Xia	2018			machine learning;artificial intelligence;computer science	AI	27.483567811120885	-29.907961644411326	139627
b26c3005509e8445f7355f3ffa527d684eedc007	kernel bayes' rule	parametric model;statistical machine learning;reproducing kernel hilbert space;state space model;conditional probability	A nonparametric kernel-based method for realizing Bayes’ r ule is proposed, based on kernel representations of probabilities in reproducing kernel Hilbert spaces. The prior and conditional probabilities are expressed as em pirical kernel mean and covariance operators, respectively, and the kernel mea n of the posterior distribution is computed in the form of a weighted sample. The ke rnel Bayes’ rule can be applied to a wide variety of Bayesian inference proble ms: we demonstrate Bayesian computation without likelihood, and filtering wit h a nonparametric statespace model. A consistency rate for the posterior estimate i s established.	computation;hilbert space;kernel (operating system);kernel density estimation	Kenji Fukumizu;Le Song;Arthur Gretton	2011			kernel;principal component regression;kernel regression;kernel density estimation;kernel method;kernel fisher discriminant analysis;bayes' rule;parametric model;kernel embedding of distributions;radial basis function kernel;conditional probability;kernel principal component analysis;state-space representation;machine learning;pattern recognition;reproducing kernel hilbert space;mathematics;variable kernel density estimation;polynomial kernel;nonparametric regression;statistics;representer theorem;kernel smoother	ML	30.121131461056525	-26.960422751028204	139650
8a6f7b4dc7cf2b5156786d71f898bc37ab6184aa	detection of unexploded ordnance via efficient semisupervised and active learning	unlabeled data;electromagnetic induction emi detectors;levantamiento electromagnetico;detectors;magnetic sensors electromagnetic interference magnetometers magnetic field measurement electromagnetic induction weapons explosives semisupervised learning algorithm design and analysis mutual information;leve electromagnetique;probability;remote sensing electromagnetic induction landmine detection learning artificial intelligence magnetometers markov processes random processes;landmine detection;excavations;active learning;magnetic sensors;performance;magnetometers;detecteur;magnetometro;detection;unexploded ordnance detection;electromagnetic induction emi;classification;semi supervised learning;algorithme;apprentissage;magnetometer;magnetic field measurement;excavacion;remote sensing;induccion electromagnetica;electromagnetic induction;probabilidad;explosives;random processes;probabilite;mutual information;algorithms;electromagnetic interference;magnetometer unexploded ordnance detection semi supervised learning active learning buried item random markov walk excavation electromagnetic induction;performances;markov processes;learning artificial intelligence;unexploded ordnance;magnetometre;electromagnetic surveys;induction electromagnetique;buried item;clasificacion;random markov walk;algorithm design and analysis;weapons;semisupervised learning;munition;excavation;explosif;algoritmo;uxo	Semi supervised learning and active learning are considered for unexploded ordnance (UXO) detection. Semi supervised learning algorithms are designed using both labeled and unlabeled data, where here labeled data correspond to sensor signatures for which the identity of the buried item (UXO/non-UXO) is known; for unlabeled data, one only has access to the corresponding sensor data. Active learning is used to define which unlabeled signatures would be most informative to improve the classifier design if the associated label could be acquired (where for UXO sensing, the label is acquired by excavation). A graph-based semi supervised algorithm is applied, which employs the idea of a random Markov walk on a graph, thereby exploiting knowledge of the data manifold (where the manifold is defined by both the labeled and unlabeled data). The algorithm is used to infer labels for the unlabeled data, providing a probability that a given unlabeled signature corresponds to a buried UXO. An efficient active-learning procedure is developed for this algorithm, based on a mutual information measure. In this manner, one initially performs excavation with the purpose of acquiring labels to improve the classifier, and once this active-learning phase is completed, the resulting semi supervised classifier is then applied to the remaining unlabeled signatures to quantify the probability that each such item is a UXO. Example classification results are presented for an actual UXO site, based on electromagnetic induction and magnetometer data. Performance is assessed in comparison to other semi supervised approaches, as well as to supervised algorithms.	active learning (machine learning);algorithm;antivirus software;electronic signature;ibm naval ordnance research calculator;machine learning;markov chain;mutual information;semi-supervised learning;semiconductor industry;supervised learning	Qiuhua Liu;Xuejun Liao;Lawrence Carin	2008	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2008.920468	semi-supervised learning;stochastic process;excavation;magnetometer;speech recognition;performance;machine learning;pattern recognition;mathematics;physics;statistics	ML	35.55826151767545	-34.10471968561054	139699
88d8cee186e2d47bedd3b54f74e0ce7209dd28e0	study on adaptive wavelet de-noising for measurement signals and its application	eigenvalues and eigenfunctions;wavelet transforms correlation methods eigenvalues and eigenfunctions fault diagnosis feature extraction optimisation signal denoising;constructive theory;optimisation;correlation dimensions;correlative arithmetic;optimal method;mechanical fault feature extraction adaptive wavelet de noising measurement signals correlation dimensions fault diagnosis constructive theory orthogonal binary wavelet basis parameter expression equation genetic optimization method correlative arithmetic fault eigenvalue;measurement signals;mechanical fault feature extraction;correlation methods;genetic optimization method;eigenvalues;correlation dimension;genetics;wavelet transforms;noise reduction wavelet analysis fractals fault diagnosis signal processing feature extraction space technology monitoring genetics mechanical systems;adaptive wavelet de noising;feature extraction;fault eigenvalue;orthogonal binary wavelet basis;parameter expression equation;fault diagnosis;signal denoising	In order to reduce the negative influence of noise on the extracting fault feature (correlation dimensions) in fault diagnosis, an adaptive wavelet de-noising method is presented in this paper. Based on the constructive theory of orthogonal binary wavelet basis, a parameter expression equation of orthogonal wavelet basis is constructed and a adaptive goal function of de-noised effect is defined. By applying genetic optimization method, the best wavelet basis was obtained, and the correlative arithmetic is presented. Applying the optimal wavelet basis to eliminate noises from signals, and computed the correlation dimension of the de-noised signals as fault eigenvalue. Simulation and experiments show that the adaptive wavelet de-noising makes the mechanical fault feature extraction more reliable	correlation dimension;experiment;feature extraction;mathematical optimization;orthogonal wavelet;simulation	Zhonghui Luo;Qijun Xiao	2006	Sixth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2006.253713	wavelet;mathematical optimization;correlation dimension;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;feature extraction;eigenvalues and eigenvectors;computer science;machine learning;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;wavelet transform	Robotics	38.472064950483194	-31.372647912198694	140060
8b93f4218923b0ef8f671d843308d0fe4d03f2c1	using simulation to measure bias in principal components regression	performance monitoring;principal component regression;active radar tracking;distributed processing;network operating system;regression analysis;simulation emulation;technical report;microprogramming;principal component	Multicollinearity is a serious problem in regression analysis. High correlation among predictor variables can lead to unstable estimates of the regression coefficients. Principal components offers one approach to reducing the sampling variance of the coefficient estimates. However, this method produces biased estimates and the bias cannot be measured without knowledge of the true coefficients. This paper uses simulation to study the degree of bias in a model employing real economic data.	coefficient;control theory;kerrison predictor;ordinary least squares;principal component regression;sampling (signal processing);simulation	Philip G. Enns	1979			principal component regression;econometrics;multicollinearity;computer science;technical report;linear predictor function;machine learning;microcode;partial least squares regression;path coefficient;regression analysis;statistics;principal component analysis	ML	30.076507740708724	-24.04469949830728	140480
9258a85bec0ae67cb04d17a1dbabe3acf0526805	dirichlet bayesian network scores and the maximum entropy principle		A classic approach for learning Bayesian networks from data is to select the maximum a posteriori (MAP) network. In the case of discrete Bayesian networks, the MAP network is selected by maximising one of several possible Bayesian Dirichlet (BD) scores; the most famous is the Bayesian Dirichlet equivalent uniform (BDeu) score from Heckerman et al. (1995). The key properties of BDeu arise from its underlying uniform prior, which makes structure learning computationally efficient; does not require the elicitation of prior knowledge from experts; and satisfies score equivalence. In this paper we will discuss the impact of this uniform prior on structure learning from an information theoretic perspective, showing how BDeu may violate the maximum entropy principle when applied to sparse data and how it may also be problematic from a Bayesian model selection perspective. On the other hand, the BDs score proposed in Scutari (2016) arises from a piecewise prior and it does not appear to violate the maximum entropy principle, even though it is asymptotically equivalent to BDeu.	algorithmic efficiency;bayes factor;bayesian network;blu-ray;f1 score;model selection;principle of maximum entropy;sparse matrix;theory;turing completeness	Marco Scutari	2017			principle of maximum entropy;applied mathematics;bayesian network;maximum entropy thermodynamics;statistics;maximum entropy probability distribution;dirichlet distribution;mathematics;generalized dirichlet distribution	ML	27.52098593337073	-30.030004771313433	140921
4069a244f28be81c2fcd24c95916c24d730d7929	discussion: latent variable graphical model selection via convex optimization		We want to congratulate the authors for a thought-provoking and very interesting paper. Sparse modeling of the concentration matrix has enjoyed popularity in recent years. It has been framed as a computationally convenient convex 1constrained estimation problem in Yuan and Lin (2007) and can be applied readily to higher-dimensional problems. The authors argue—we think correctly—that the sparsity of the concentration matrix is for many applications more plausible after the effects of a few latent variables have been removed. The most attractive point about their method is surely that it is formulated as a convex optimization problem. Latent variable fitting and sparse graphical modeling of the conditional distribution of the observed variables can then be obtained through a single fitting procedure.	convex optimization;graphical model;latent variable;mathematical optimization;model selection;optimization problem;sparse matrix	Steffen L. Lauritzen;Nicolai Meinshausen	2012	CoRR	10.1214/12-AOS980	econometrics;mathematical optimization;mathematics;statistics	ML	27.09319143954852	-32.843706997371484	141340
94d561e23829186c0788c299608c81acaa29e179	distributed anomaly detection using minimum volume elliptical principal component analysis	distributed learning anomaly detection outlier detection principal component analysis;machine learning problem distributed anomaly detection minimum volume elliptical principal component analysis residual error centralized processing unit communication cost hierarchical network infrastructure data aggregation neighboring nodes small matrices;anomaly detection;principal component analysis distributed databases robustness peer to peer computing training data models biological system modeling;training;biological system modeling;outlier detection;distributed learning;principal component analysis;distributed databases;principal component analysis data aggregation learning artificial intelligence matrix algebra;robustness;peer to peer computing;data models	Principal component analysis and the residual error is an effective anomaly detection technique. In an environment where anomalies are present in the training set, the derived principal components can be skewed by the anomalies. A further aspect of anomaly detection is that data might be distributed across different nodes in a network and their communication to a centralized processing unit is prohibited due to communication cost. Current solutions to distributed anomaly detection rely on a hierarchical network infrastructure to aggregate data or models; however, in this environment, links close to the root of the tree become critical and congested. In this paper, an algorithm is proposed that is more robust in its derivation of the principal components of a training set containing anomalies. A distributed form of the algorithm is then derived where each node in a network can iterate towards the centralized solution by exchanging small matrices with neighboring nodes. Experimental evaluations on both synthetic and real-world data sets demonstrate the superior performance of the proposed approach in comparison to principal component analysis and alternative anomaly detection techniques. In addition, it is shown that in a variety of network infrastructures, the distributed form of the anomaly detection model is able to derive a close approximation of the centralized model.	aggregate data;algorithm;anomaly detection;approximation;centralized computing;iteration;principal component analysis;synthetic intelligence;test set;tree network	Colin O'Reilly;Alexander Gluhak;Muhammad Ali Imran	2016	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2016.2555804	data modeling;anomaly detection;computer science;machine learning;pattern recognition;data mining;database;robustness;principal component analysis	ML	31.811865521485363	-35.08953456736365	141878
34ada50ccb7527ac9ac8f359b29d1587900a24fc	the misalignment fault model building for rotating machinery rotor based on bp network	biological neural networks rotors training vibrations neurons fault diagnosis;rotor misalignment bp network rotating machinery fault recognition;nonlinear mapping;mechanical engineering computing;neural nets;rotor misalignment;backpropagation;condition monitoring;rotors;rotating machinery;bp network;fault recognition;fault model;machinery;rotors backpropagation condition monitoring fault diagnosis machinery mechanical engineering computing neural nets;neural network diagnostic misalignment fault model rotating machinery rotor bp neural network mechanical diagnosis mechanical recognition nonlinear mapping blower;fault diagnosis;neural network	BP neural network has successful experience in dealing with both mechanical diagnosis and recognition. This article introduces the use of the BP network in nonlinear mapping to diagnose and recognize the rotor of blower as well as the method of neural network diagnostic and BP algorithm. The test of the network show that the result is satisfactory and it has very important significance and good application prospect on the recognition of the rotating machinery rotor misalignment fault.	algorithm;artificial neural network;backpropagation;fault model;nonlinear system;r.o.t.o.r.	Xueping Ren;Xiusong Hou	2012	2012 8th International Conference on Natural Computation	10.1109/ICNC.2012.6234570	structural engineering;control engineering;engineering;control theory	Robotics	36.590713583163364	-30.581716216070895	142269
17a4a1144c61794ac61adb249b8d4228b104a7d9	dimensionality reduction and (bucket) ranking: a mass transportation approach		Whereas most dimensionality reduction techniques (e.g. PCA, ICA, NMF) for multivariate data essentially rely on linear algebra to a certain extent, summarizing ranking data, viewed as realizations of a random permutation Σ on a set of items indexed by i ∈ {1, . . . , n}, is a great statistical challenge, due to the absence of vector space structure for the set of permutations Sn. It is the goal of this article to develop an original framework for possibly reducing the number of parameters required to describe the distribution of a statistical population composed of rankings/permutations, on the premise that the collection of items under study can be partitioned into subsets/buckets, such that, with high probability, items in a certain bucket are either all ranked higher or else all ranked lower than items in another bucket. In this context, Σ’s distribution can be hopefully represented in a sparse manner by a bucket distribution, i.e. a bucket ordering plus the ranking distributions within each bucket. More precisely, we introduce a dedicated distortion measure, based on a mass transportation metric, in order to quantify the accuracy of such representations. The performance of buckets minimizing an empirical version of the distortion is investigated through a rate bound analysis. Complexity penalization techniques are also considered to select the shape of a bucket order with minimum expected distortion. Beyond theoretical concepts and results, numerical experiments on real ranking data are displayed in order to provide empirical evidence of the relevance of the approach promoted.	dimensionality reduction;distortion;experiment;independent computing architecture;linear algebra;non-negative matrix factorization;numerical analysis;penalty method;random permutation;relevance;sparse matrix;with high probability	Mastane Achab;Anna Korba;Stéphan Clémençon	2018	CoRR		permutation;discrete mathematics;linear algebra;mathematics;random permutation;dimensionality reduction;distortion;ranking;statistical population;non-negative matrix factorization	ML	27.70994789340014	-25.39027827455179	142391
01877c3ab4decc2884f7f38d7fcf21029fd1b7e9	mixture density mercer kernels: a method to learn kernels directly from data	mixture density estimation;kernel methods;image seg- mentation;unsupervised learning;clustering;mixture model;kernel method;density estimation;kernel function;synthetic data;multispectral images	This paper presents a method of generating Mercer Kernels from an ensemble of probabilistic mixture models, where each mixture model is generated from a Bayesian mixture density estimate. We show how to convert the ensemble estimates into a Mercer Kernel, describe the properties of this new kernel function, and give examples of the performance of this kernel on unsupervised clustering of synthetic data and also in the domain of unsupervised multispectral image understanding.	cluster analysis;computer vision;kernel (operating system);mixture model;multispectral image;separation kernel;synthetic data	Ashok N. Srivastava	2004		10.1137/1.9781611972740.34	pattern recognition;computer science;mixture model;machine learning;artificial intelligence;kernel (statistics);kernel method;kernel embedding of distributions;tree kernel;cluster analysis;unsupervised learning;variable kernel density estimation	ML	29.74609179223032	-32.59983577086519	142550
67de303bf8525d6478579bcdf611b0f224917737	incipient fault online estimation based on kullback-leibler divergence and fast moving window pca		The realization of early detection of incipient faults makes great sense for the guarantee of system performance and security operation. Therefore, It is necessary to estimate the fault amplitude especially when the system security assessment is the main goal. Regarding the incipient fault with low Fault-Noise-ratio (FNR), in this paper, a practical online fault estimation method is presented for multivariate processes. Statistics based on Kullback-Leibler Divergence (KLD), that is characterized with a high sensitivity to incipient fault, are utilized for the realization of small deviation estimation. Based on the Gaussian assumption, KLDs of different principal components, that characterizes the difference between free-fault and fault case, are computed online through updating the mean and variance of scores. The simulation confirms the feasibility and effectiveness of the proposed incipient fault diagnosis method.	computer security;fibrosis of extraocular muscles, congenital, with synergistic divergence;kullback–leibler divergence;normal statistical distribution;sample variance;simulation	Songbing Tao;Yi Chai;Ngo Quang Vi	2017	IECON 2017 - 43rd Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2017.8217415	kullback–leibler divergence;control theory;divergence;engineering;multivariate statistics;amplitude;gaussian;principal component analysis;pattern recognition;artificial intelligence	EDA	36.23849061050728	-29.235039368150375	142851
3cd93c773acfd220b6bcc496ba103deba1eaa294	bayesian methods in global optimization	bayesian method;global optimization	This paper reviews methods which have been proposed for solving global optimization problems in the framework of the Bayesian paradigm.	global optimization;mathematical optimization;monte carlo method;programming paradigm	Bruno Betrò	1991	J. Global Optimization	10.1007/BF00120661	econometrics;mathematical optimization;variable-order bayesian network;bayesian probability;mathematics;statistics;global optimization	Vision	28.460123105231204	-29.43471864265926	142943
99c1007f74025a3efd065e67e22ccdc1db37460d	robust nonparametric regression by controlling sparsity*	nonparametric method;robustness tuning approximation methods estimation linear regression context training;approximation method;training;linear regression;variable selection;statistical learning;lasso robustness nonparametric regression outlier rejection sparsity;sparsity;tuning;estimation;nonparametric regression;robustness;least trimmed squares;approximation methods;convex relaxation;lasso;smoothing spline;context;outlier rejection;compressive sampling	Nonparametric methods are widely applicable to statistical learning problems, since they rely on a few modeling assumptions. In this context, the fresh look advocated here permeates benefits from variable selection and compressive sampling, to robustify nonparametric regression against outliers. A variational counterpart to least-trimmed squares regression is shown closely related to an ℓ0-(pseudo)norm-regularized estimator, that encourages sparsity in a vector explicitly modeling the outliers. This connection suggests efficient (approximate) solvers based on convex relaxation, which lead naturally to a variational M-type estimator equivalent to Lasso. Outliers are identified by judiciously tuning regularization parameters, which amounts to controlling the sparsity of the outlier vector along the whole robustification path of Lasso solutions. An improved estimator with reduced bias is obtained after replacing the ℓ0-(pseudo)norm with a nonconvex surrogate, as corroborated via simulated tests on robust thin-plate smoothing splines.	approximation algorithm;calculus of variations;compressed sensing;feature selection;lasso;linear programming relaxation;machine learning;matrix regularization;robustification;sampling (signal processing);smoothing spline;sparse matrix;thin plate spline;variational principle	Gonzalo Mateos;Georgios B. Giannakis	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947199	econometrics;estimation;smoothing spline;least trimmed squares;computer science;linear regression;lasso;pattern recognition;mathematics;compressed sensing;sparsity-of-effects principle;feature selection;nonparametric regression;statistics;robustness	ML	26.977603050697443	-33.371754877768076	143032
3f68941bf1b9e7b230f6af13875991f2105974bb	choice of v for v-fold cross-validation in least-squares density estimation	model selection;penalization;density estimation;resampling penalties;monte carlo cross validation;v fold cross validation;leave p out;leave one out	This paper studies V -fold cross-validation for model selection in least-squares density estimation. The goal is to provide theoretical grounds for choosing V in order to minimize the least-squares loss of the selected estimator. We first prove a non-asymptotic oracle inequality for V -fold cross-validation and its bias-corrected version (V -fold penalization). In particular, this result implies that V -fold penalization is asymptotically optimal in the nonparametric case. Then, we compute the variance of V -fold crossvalidation and related criteria, as well as the variance of key quantities for model selection performance. We show that these variances depend on V like 1 + 4/(V − 1), at least in some particular cases, suggesting that the performance increases much from V = 2 to V = 5 or 10, and then is almost constant. Overall, this can explain the common advice to take V = 5 —at least in our setting and when the computational power is limited—, as supported by some simulation experiments. An oracle inequality and exact formulas for the variance are also proved for Monte-Carlo cross-validation, also known as repeated cross-validation, where the parameter V is replaced by the number B of random splits of the data.	advice (programming);asymptotically optimal algorithm;computation;cross-validation (statistics);experiment;least squares;model selection;monte carlo method;oracle nosql db;penalty method;simulation;social inequality	Sylvain Arlot;Matthieu Lerasle	2016	Journal of Machine Learning Research		econometrics;mathematical optimization;density estimation;mathematics;model selection;statistics	Web+IR	29.222237723205076	-24.62208202553122	143062
90ed58ff3e404492d25d202d572816df4b26fab7	fast conical hull algorithms for near-separable non-negative matrix factorization		The separability assumption (Donoho & Stodden, 2003; Arora et al., 2012a) turns non-negative matrix factorization (NMF) into a tractable problem. Recently, a new class of provably-correct NMF algorithms have emerged under this assumption. In this paper, we reformulate the separable NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. From this geometric perspective, we derive new separable NMF algorithms that are highly scalable and empirically noise robust, and have several other favorable properties in relation to existing methods. A parallel implementation of our algorithm demonstrates high scalability on sharedand distributedmemory machines.	algorithm;cobham's thesis;correctness (computer science);linear separability;non-negative matrix factorization;scalability	Abhishek Kumar;Vikas Sindhwani;Prabhanjan Kambadur	2013			mathematical optimization;combinatorics;machine learning;mathematics	AI	26.007165450308957	-37.74629138224834	143427
4bcb8d2d53cc4bfd3053c6e77991604b5b545758	the labeled multi-bernoulli filter	clutter;bayesian estimation;radar tracking;δ generalized labeled multibernoulli filter output target tracking approximation theory labeled random finite set particle implementation state estimation;conjugate prior;target tracking approximation methods clutter radar tracking vectors current measurement materials;materials;journal article;bayesian estimation conjugate prior marked point process random finite set target tracking;current measurement;vectors;marked point process;output target tracking approximation theory labeled random finite set particle implementation state estimation δ generalized labeled multibernoulli filter;random finite set;approximation theory estimation theory particle filtering numerical methods random processes state estimation target tracking;approximation methods;target tracking approximation theory estimation theory particle filtering numerical methods random processes state estimation;target tracking;target tracking bayesian estimation conjugate prior marked point process random finite set	This paper proposes a generalization of the multi- Bernoulli filter called the labeled multi-Bernoulli filter that outputs target tracks. Moreover, the labeled multi-Bernoulli filter does not exhibit a cardinality bias due to a more accurate update approximation compared to the multi-Bernoulli filter by exploiting the conjugate prior form for labeled Random Finite Sets. The proposed filter can be interpreted as an efficient approximation of the δ-Generalized Labeled Multi-Bernoulli filter. It inherits the advantages of the multi-Bernoulli filter in regards to particle implementation and state estimation. It also inherits advantages of the δ-Generalized Labeled Multi-Bernoulli filter in that it outputs (labeled) target tracks and achieves better performance.	algorithm;approximation;bernoulli polynomials;cobham's thesis;computational resource;euler–bernoulli beam theory;line mode browser;mathematical structure;remote file sharing;time complexity	Stephan Reuter;Ba-Tuong Vo;Ba-Ngu Vo;Klaus C. J. Dietmayer	2014	IEEE Transactions on Signal Processing	10.1109/TSP.2014.2323064	adaptive filter;econometrics;mathematical optimization;ensemble kalman filter;radar tracker;bayes estimator;kernel adaptive filter;conjugate prior;mathematics;clutter;statistics	Visualization	38.92003738503509	-26.732294248517125	143484
0a1303be0745ec862adba77c8e79b56fde27355e	parsimonious reduction of gaussian mixture models with a variational-bayes approach	bayes estimation;modelizacion;distributed system;processus gauss;bayesian modelling;bayesian estimation;systeme reparti;variational bayes;procesamiento informacion;diminution cout;complexite calcul;gaus sian mixture model;probabilistic approach;classification;model complexity;scaling up;modelisation;estimacion bayes;complejidad computacion;gaussian mixture model;sistema repartido;analyse parcimonie;mixture model;computational complexity;enfoque probabilista;approche probabiliste;information processing;parcimony analysis;bayesian estimator;teoria mezcla;gaussian process;reduccion costes;traitement information;mixture models;proceso gauss;analisis parsimonia;mixture theory;modeling;theorie melange;clasificacion;cost lowering;variational methods;estimation bayes;model aggregation;methode variationnelle	Aggregating statistical representations of classes is an important task for current trends in scaling up learning and recognition, or for addressing them in distributed infrastructures. In this perspective, we address the problem of merging probabilistic Gaussian mixture models in an efficient way, through the search for a suitable combination of components from mixtures to be merged. We propose a new Bayesian modelling of this combination problem, in association to a variational estimation technique, that handles efficiently the model complexity issue. A main feature of the present scheme is that it merely resorts to the parameters of the original mixture, ensuring low computational cost and possibly communication, should we operate on a distributed system. Experimental results are reported on real data. ∗This work was funded by ANR Safimage, in particular through P. Bruneau’s Ph.D. grant	algorithmic efficiency;ambiguous name resolution;bayesian network;calculus of variations;cluster analysis;computation;cost efficiency;distributed computing;image scaling;mixture model;occam's razor;pattern recognition;semi-supervised learning;semiconductor industry;variational principle	Pierrick Bruneau;Marc Gelgon;Fabien Picarougne	2010	Pattern Recognition	10.1016/j.patcog.2009.08.006	econometrics;information processing;computer science;machine learning;mixture model;mathematics;statistics	ML	29.16056101368791	-31.307679267519543	143734
3bc797df6d39e6033f7bccb605b6f002669badbf	minimizing sparse high-order energies by submodular vertex-cover		Inference in high-order graphical models has become important in recent years. Several approaches are based, for example, on generalized message-passing, or on transformation to a pairwise model with extra ‘auxiliary’ variables. We focus on a special case where a much more efficient transformation is possible. Instead of adding variables, we transform the original problem into a comparatively small instance of submodular vertex-cover. These vertex-cover instances can then be attacked by existing algorithms (e.g. belief propagation, QPBO), where they often run 4–15 times faster and find better solutions than when applied to the original problem. We evaluate our approach on synthetic data, then we show applications within a fast hierarchical clustering and model-fitting framework.	algorithm;belief propagation;cluster analysis;graphical model;hierarchical clustering;message passing;software propagation;sparse;submodular set function;synthetic data;vertex cover	Andrew Delong;Olga Veksler;Anton Osokin;Yuri Boykov	2012			mathematical optimization;combinatorics;machine learning;mathematics;statistics	ML	26.025797028161964	-33.46943878727933	143745
9c66fefe6512202760bb4649c11ebc3ca7eaa0e0	initializing normal mixtures of densities	matrix algebra maximum likelihood estimation pattern recognition optimisation iterative methods probability;nonparametric density estimation;optimisation;convergence;probability;iterative algorithms;nonparametric density estimation log likelihood function smoothed kernel estimate optimal weighting optimization em algorithm maximum likelihood estimation iterative algorithm normal density mixtures probability density function pattern recognition matrix algebra;normal density mixtures;smoothed kernel estimate;probability density function;tellurium;matrix algebra;maximum likelihood estimation;iterative algorithm;books;iterative methods;maximum likelihood estimate;normal mixture;optimal weighting;pattern recognition;optimization;parameter estimation;log likelihood function;content addressable storage;em algorithm;iterative algorithms information theory automation content addressable storage tellurium pattern recognition maximum likelihood estimation parameter estimation convergence books;likelihood function;information theory;kernel estimate;finite mixture;automation	It is well known that log-likelihood function for finite mixtures usually has local maxima and therefore the iterative EM algorithm for maximum-likelihood estimation of mixtures may be starting-point dependent. In the present paper we propose a method of choosing initial parameters of mixtures which includes two stages: (a) computation of nonparametric optimally smoothed kernel estimate of the unknown density, (b) optimal weighting of the smoothed kernel estimate and using essential kernels as the initial estimate of the mixture. All the optimization tasks make use of a suitably modified EM algorithm. The properties and computational aspects of the proposed method are illustrated by a numerical example and some application possibilities are considered.	computation;emoticon;expectation–maximization algorithm;iterative method;kernel (operating system);mathematical optimization;maxima and minima;numerical analysis;smoothing	Jirí Grim;Jana Novovicová;Pavel Pudil;Petr Somol;Francesc J. Ferri	1998		10.1109/ICPR.1998.711292	econometrics;mathematical optimization;information theory;mathematics;iterative method;maximum likelihood;likelihood function;statistics	ML	31.314381101912655	-28.35642065624269	143784
4bb8127bf81dbdf25abc03c38b2b07e3c411ab64	large-scale strategic games and adversarial machine learning	kernel;game theory;cost function;support vector machines;games;scalability;security	Decision making in modern large-scale and complex systems such as communication networks, smart electricity grids, and cyber-physical systems motivate novel game-theoretic approaches. This paper investigates big strategic (non-cooperative) games where a finite number of individual players each have a large number of continuous decision variables and input data points. Such high-dimensional decision spaces and big data sets lead to computational challenges, relating to efforts in non-linear optimization scaling up to large systems of variables. In addition to these computational challenges, real-world players often have limited information about their preference parameters due to the prohibitive cost of identifying them or due to operating in dynamic online settings. The challenge of limited information is exacerbated in high dimensions and big data sets. Motivated by both computational and information limitations that constrain the direct solution of big strategic games, our investigation centers around reductions using linear transformations such as random projection methods and their effect on Nash equilibrium solutions. Specific analytical results are presented for quadratic games and approximations. In addition, an adversarial learning game is presented where random projection and sampling schemes are investigated.	adversarial machine learning;algorithm;approximation;big data;complex systems;computation;convex function;cyber-physical system;data point;decision theory;game theory;image scaling;linear programming;mathematical optimization;nash equilibrium;nonlinear programming;nonlinear system;problem domain;random projection;sampling (signal processing);telecommunications network	Tansu Alpcan;Benjamin I. P. Rubinstein;Christopher Leckie	2016	2016 IEEE 55th Conference on Decision and Control (CDC)	10.1109/CDC.2016.7798940	games;game theory;support vector machine;kernel;scalability;simulation;computer science;theoretical computer science;machine learning;mathematics;mathematical economics;algorithm	ML	25.038195621613234	-31.422875347702732	143912
42172e81691cfb3cb4105b0d71cc8110794aaef6	semi-supervised learning -- a statistical physics approach	statistical mechanics;semi supervised learning;energy function;gene expression;statistical physics;science learning;markov chain monte carlo;pattern recognition	We present a novel approach to semisupervised learning which is based on statistical physics. Most of the former work in the field of semi-supervised learning classifies the points by minimizing a certain energy function, which corresponds to a minimal k-way cut solution. In contrast to these methods, we estimate the distribution of classifications, instead of the sole minimal k-way cut, which yields more accurate and robust results. Our approach may be applied to all energy functions used for semi-supervised learning. The method is based on sampling using a Multicanonical Markov chain Monte-Carlo algorithm, and has a straightforward probabilistic interpretation, which allows for soft assignments of points to classes, and also to cope with yet unseen class types. The suggested approach is demonstrated on a toy data set and on two real-life data sets of gene expression.	markov chain monte carlo;mathematical optimization;monte carlo algorithm;real life;sampling (signal processing);semi-supervised learning;semiconductor industry;supervised learning	Gad Getz;Noam Shental;Eytan Domany	2004	CoRR		semi-supervised learning;algorithmic learning theory;gene expression;markov chain monte carlo;statistical mechanics;computer science;machine learning;pattern recognition;mathematics;ensemble learning;statistics;generalization error	ML	27.527535726546642	-30.63644847784966	144032
371ebdcc627b0ba5ca49d3bc3268ec41a6bdc5bd	sparse gaussian process regression model based on ℓ 1/2 regularization	gaussian process;real-world datasets;sparse solution;sparse gp model;large datasets;sparse gaussian;new sparse;generalized linear regression model;gp model;proposed model;proposed model converges	Gaussian Process (GP) model is an elegant tool for the probabilistic prediction. However, the high computational cost of GP prohibits its practical application on large datasets. To address this issue, this paper develops a new sparse GP model, referred to as GPHalf. The key idea is to sparsify the GP model via the newly introduced ℓ 1/2 regularization method. To achieve this, we represent the GP as a generalized linear regression model, then use the modified ℓ 1/2 half thresholding algorithm to optimize the corresponding objective function, thus yielding a sparse GP model. We proof that the proposed model converges to a sparse solution. Numerical experiments on both artificial and real-world datasets validate the effectiveness of the proposed model.	algorithm;algorithmic efficiency;approximation;computation;computational complexity theory;experiment;gaussian process;generalized linear model;kriging;loss function;numerical method;optimization problem;singular value decomposition;sparse matrix;thresholding (image processing);variational principle	Peng Kou;Feng Gao	2013	Applied Intelligence	10.1007/s10489-013-0482-0	mathematical optimization;machine learning	AI	26.153703836781087	-33.57500515318472	144364
3d52f5bfb55a95f7664cf628efa9b1f9c7376b57	general directional regression	sliced average variance estimation;sliced inverse regression;nonlinear dimension reduction;general empirical directions;62h12;permutation test	Directional regression is an effective sufficient dimension reduction method which implicitly synthesizes the first two conditional moments. In this paper, we extend directional regression to a general family of estimators via the notion of general empirical directions. Data-driven method is used to identify the optimal estimator within this family. Based on the proposed general directional regression estimators, we develop a new methodology for nonlinear dimension reduction. Improvement of general directional regression over classical directional regression is demonstrated via simulation studies and an empirical study with the wine recognition data.		Zhou Yu;Yuexiao Dong;Mian Huang	2014	J. Multivariate Analysis	10.1016/j.jmva.2013.10.016	econometrics;mathematical optimization;local regression;resampling;sliced inverse regression;polynomial regression;mathematics;statistics	Vision	28.542473517538525	-25.250054484073967	144923
3819fbc7141e9518262b786fee2df06655b19b84	nonlinear structural equation modeling with distribution-free method	standard error estimation;bootstrap;latent variable modeling;normal mixture;deconvolution;mcem algorithm	Structural equation models (SEM) are widely used in many fields including economics and social science. Typical nonlinear SEMs consist of two parts: a linear measurement model relating observed measurements to underlying latent variables, and a nonlinear structural model describing relationships among the latent variables. For such models, we propose a pseudo likelihood approach based on a hypothetical normal mixture assumption on the latent variables. To obtain pseudo likelihood parameter estimates, a Monte Carlo EM algorithm is developed. Standard errors for the structural parameter estimates are obtained by combining an empirical observed information matrix and a bootstrap estimated covariance matrix. For nonlinear SEMs with latent variables with various distributions, we conduct simulations to show our approach produced unbiased parameter estimates and confidence intervals with nominal coverage.	structural equation modeling	Yan D. Zhao;Dewi Rahardja	2011	MASA	10.3233/MAS-2011-0162	econometrics;mathematical optimization;statistics	Vision	28.74792401918932	-24.004220725986663	145695
c34627fd2cb6d98a45e3bf000c89e9c4c165cf06	neural variational inference and learning in undirected graphical models		Many problems in machine learning are naturally expressed in the language of undirected graphical models. Here, we propose black-box learning and inference algorithms for undirected models that optimize a variational approximation to the log-likelihood of the model. Central to our approach is an upper bound on the logpartition function parametrized by a function q that we express as a flexible neural network. Our bound makes it possible to track the partition function during learning, to speed-up sampling, and to train a broad class of hybrid directed/undirected models via a unified variational inference framework. We empirically demonstrate the effectiveness of our method on several popular generative modeling datasets.	algorithm;approximation;artificial neural network;black box;calculus of variations;generative modelling language;generative model;graph (discrete mathematics);graphical model;ibm notes;importance sampling;latent variable;machine learning;partition function (mathematics);sampling (signal processing);variational principle	Volodymyr Kuleshov;Stefano Ermon	2017			machine learning;artificial intelligence;generative grammar;mathematical optimization;artificial neural network;parametrization;sampling (statistics);mathematics;partition function (statistical mechanics);inference;upper and lower bounds;graphical model	ML	25.366448259843565	-30.081104817651138	145700
544024538db20c4a608e2edf4166852b3b92a7f5	reducing statistical time-series problems to binary classification		We show how binary classification methods developed to work on i.i.d. data can be used for solving statistical problems that are seemingly unrelated to classification and concern highly-dependent time series. Specifically, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. The algorithms that we construct for solving these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods. Universal consistency of the proposed algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data.	algorithm;binary classification;cluster analysis;experiment;synthetic intelligence;time series	Daniil Ryabko;Jérémie Mary	2012			econometrics;machine learning;classification rule;data mining;mathematics;statistics	ML	29.045366704574967	-29.69163955342495	146017
96d28e0de2e2418797f63b792905aa6be9307a79	multivariate composite distributions for coefficients in synthetic optimization problems	composition;optimization problem;probability distribution;random variable;correlation;joint probability distribution	In most cases, coefficients in synthetic optimization problems are randomly generated based on specified univariate marginal distributions. Additionally, the various types of coefficients are assumed to be mutually independent, even though coefficients in practical problems may be correlated. In this paper, multivariate composite distributions with specified marginal distributions and a specified Pearson product–moment population correlation structure are characterized. The generation of synthetic optimization problems is the principal motivation for characterizing these composite distributions, but they are also useful for many other simulation applications. Type L composite distributions are composed of the extreme-correlation distributions for a multivariate random variable only, while Type U composite distributions are based on the extreme-correlation distributions and the joint distribution under independence. Closed-form composition probabilities for distributions of trivariate random variables are presented. Methods for identifying correlation structures that are amenable to representation by composite distributions are discussed.	coefficient;mathematical optimization;synthetic intelligence	Raymond R. Hill;Charles H. Reilly	2000	European Journal of Operational Research	10.1016/S0377-2217(99)00012-0	probability distribution;dirichlet distribution;inverse distribution;random variable;optimization problem;composition;econometrics;combinatorics;stability;convolution of probability distributions;heavy-tailed distribution;statistical parameter;mathematics;joint probability distribution;k-distribution;correlation;statistics	ML	31.30111337001488	-24.77519912990602	146018
12305deab00f3ede9e128845bb98c721cec64194	a survey of robust statistics	multivariate location;scatter;estimators;linear regression;variance	Robust statistics has multiple goals, which are not always aligned. Robust thinking grew out of data analysis and the realisation that empirical evidence is at times supported merely by one or a few observations. The paper examines the outgrowth from this criticism of the statistical method over the last few decades.		Stephan Morgenthaler	2007	Statistical Methods and Applications	10.1007/s10260-007-0057-5	robust statistics;econometrics;mathematical optimization;estimator;linear regression;mathematics;variance;statistics	EDA	28.396571791616427	-24.0227638196516	146038
54296290d7b961b480b86fed1161529459ca06c3	acoustic space learning for sound-source separation and localization on binaural manifolds	sound localization;manifold learning;binaural hearing;mixture of regressors;sound source separation;em inference	In this paper, we address the problems of modeling the acoustic space generated by a full-spectrum sound source and using the learned model for the localization and separation of multiple sources that simultaneously emit sparse-spectrum sounds. We lay theoretical and methodological grounds in order to introduce the binaural manifold paradigm. We perform an in-depth study of the latent low-dimensional structure of the high-dimensional interaural spectral data, based on a corpus recorded with a human-like audiomotor robot head. A nonlinear dimensionality reduction technique is used to show that these data lie on a two-dimensional (2D) smooth manifold parameterized by the motor states of the listener, or equivalently, the sound-source directions. We propose a probabilistic piecewise affine mapping model (PPAM) specifically designed to deal with high-dimensional data exhibiting an intrinsic piecewise linear structure. We derive a closed-form expectation-maximization (EM) procedure for estimating the model parameters, followed by Bayes inversion for obtaining the full posterior density function of a sound-source direction. We extend this solution to deal with missing data and redundancy in real-world spectrograms, and hence for 2D localization of natural sound sources such as speech. We further generalize the model to the challenging case of multiple sound sources and we propose a variational EM framework. The associated algorithm, referred to as variational EM for source separation and localization (VESSL) yields a Bayesian estimation of the 2D locations and time-frequency masks of all the sources. Comparisons of the proposed approach with several existing methods reveal that the combination of acoustic-space learning with Bayesian inference enables our method to outperform state-of-the-art methods.	acoustic cryptanalysis;audio media;blast e-value;bayesian approaches to brain function;binaural beats;body of uterus;calculus of variations;covox speech thing;estimated;exhibits as topic;expectation–maximization algorithm;full-spectrum light;inference;internationalization and localization;masks;missing data;model selection;nonlinear dimensionality reduction;piecewise linear continuation;pixel;programming paradigm;robot;selection algorithm;source separation;sparse matrix;spectrogram;theory;unsupervised learning;manifold;mixture	Antoine Deleforge;Florence Forbes;Radu Horaud	2015	International journal of neural systems	10.1142/S0129065714400036	speech recognition;sound localization;machine learning;mathematics	ML	33.92045954439628	-36.68872406688417	146089
8b1d5f48dd2fa74694e4d13f39d95aa5d226aaa2	sampling in computer vision and bayesian nonparametric mixtures		The field of computer vision focuses on understanding and reasoning about the visual world. Due to the complexity of this problem, researchers often focus on one specific component of this large task, such as segmentation or recognition. This modularized approach necessitates the combination of each separate component, which Bayesian formulations handle in a mathematically consistent framework. Unfortunately, probabilistic formulations are often difficult in computer vision due to the complexity and large dimensionality of data. In this thesis, we demonstrate how efficient Markov chain Monte Carlo (MCMC) sampling techniques can address a subset of these problems. In the first half of this thesis, we consider the problem of inference in discrete Markov random fields (MRFs) that often occur in segmentation and tracking. We develop the Permutation-based Gibbs-Inspired Metropolis-Hasting (PGIMH) sampling algorithm and show its applicability to a variety of formulations (including curve-length penalties and topology priors). In particle filtering, PGIMH precludes the need to update particle weights or use of sequential importance resampling. Empirical results demonstrate that PGIMH is approximately 104 times faster than previous shape sampling approaches and that it improves results in segmentation, boundary detection, and object tracking. In the second half of this thesis, we focus on inference in the Dirichlet process mixture model (DPMM), which is often slow and cumbersome due to the infinite number of mixture components. We develop a parallel algorithm that samples from the posterior distribution of a DPMM without requiring finite model approximations. This method, called DP Sub-Clusters, essentially fits a two-component mixture model to each regular cluster. These “sub-clusters” are then used to propose splits and merges, resulting in the efficient exploration of the sample space. We show how the developed framework extends to other mixture models, such as the hierarchical Dirichlet process, often used in document analysis. Additionally, we develop the spatially-varying Dirichlet process Gaussian mixture model (SV-DPGMM), which achieves state-of-the-art results in intrinsic image decomposition by leveraging the DP Sub-Cluster algorithm. By addressing these problems, we demonstrate the applicability of MCMC methods to computer vision, and highlight the importance of designing fast sampling algorithms. Thesis Supervisor: John W. Fisher III Title: Senior Research Scientist of Electrical Engineering and Computer Science	approximation;computer science;computer vision;fits;markov chain monte carlo;markov random field;memory segmentation;metropolis;mixture model;parallel algorithm;particle filter;sampling (signal processing);systemverilog	Jason Chang	2014				Vision	32.13243588773431	-33.75581429976902	146149
56c8779d8581fed2e8d28931c17f9f1cd1720f24	combining regression and estimation by analogy in a semi-parametric model for software cost estimation	parametric model;empirical study;software cost estimation;statistical test;estimation by analogy;non parametric estimation;least square;prediction accuracy;semi parametric model;regression analysis;least squares regression	Software Cost Estimation is the task of predicting the effort or productivity required to complete a software project. Two of the most known techniques appeared in literature so far are Regression Analysis and Estimation by Analogy. The results of the empirical studies show the lack of convergence in choosing the best prediction technique between the parametric Regression Analysis and the non-parametric Estimation by Analogy models. In this paper, we introduce the use of a semi-parametric model that achieves to incorporate some parametric information into a non-parametric model combining in this way regression and analogy. Furthermore, we demonstrate the procedure of building such a model on two well-known datasets and we present the comparative results based on the predictive accuracy of the new technique using several accuracy measures. We also perform statistical tests on the residuals in order to assess the improvement in the predictions attained through the new semi-parametric model in comparison to the accuracy of Regression Analysis and Estimation by Analogy when applied separately. Our results show that the semi-parametric model provides more accurate predictions than each one of the parametric and non-parametric approaches.	mathematical model;parametric model;semiconductor industry;semiparametric model;software development effort estimation;software project management	Nikolaos Mittas;Lefteris Angelis	2008		10.1145/1414004.1414017	econometrics;computer science;machine learning;regression diagnostic;least squares;statistics	ML	27.437142792312446	-24.38945511637743	146334
1968ea618407384305c84c90af2438baaf8dd7a1	privacy and statistical risk: formalisms and minimax bounds		We explore and compare a variety of definitions for privacy and disclosure limitation in statistical estimation and data analysis, including (approximate) differential privacy, testingbased definitions of privacy, and posterior guarantees on disclosure risk. We give equivalence results between the definitions, shedding light on the relationships between different formalisms for privacy. We also take an inferential perspective, where—building off of these definitions— we provide minimax risk bounds for several estimation problems, including mean estimation, estimation of the support of a distribution, and nonparametric density estimation. These bounds highlight the statistical consequences of different definitions of privacy and provide a second lens for evaluating the advantages and disadvantages of different techniques for disclosure limitation.	approximation algorithm;differential privacy;estimation theory;inferential theory of learning;minimax;turing completeness	Rina Foygel Barber;John C. Duchi	2014	CoRR		econometrics;data mining;mathematics;statistics	ML	27.049705328139346	-25.412914685653643	146385
d2de470136427e34b33ebefdb0a3bf583c884a0f	entropy penalized learning for gaussian mixture models	model selection algorithms entropy penalized learning gaussian mixture models gmm minimum message length criterion;gaussian processes;learning artificial intelligence entropy gaussian processes;gaussian mixture model;entropy;learning artificial intelligence;entropy computational modeling data models covariance matrix complexity theory fitting estimation	In this paper, we propose an entropy penalized approach to address the problem of learning the parameters of Gaussian mixture models (GMMs) with components of small weights. In addition, since the method is based on minimum message length (MML) criterion, it can also determine the number of components of the mixture model. The simulation results demonstrate that our method outperform several other state-of-art model selection algorithms especially for the mixtures with components of very different weights.	algorithm;minimum message length;mixture model;model selection;simulation	Boyu Wang;Feng Wan;Peng Un Mak;Pui-in Mak;Mang I Vai	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033481	gaussian random field;entropy;joint entropy;binary entropy function;transfer entropy;computer science;machine learning;pattern recognition;mixture model;gaussian process;mathematics;statistics	ML	29.342166334481252	-31.30444377984502	146788
174142a34d0920acc3dca9931132b3cdd22e2730	probably approximate bayesian computation: nonasymptotic convergence of abc under misspecification		Abstract. Approximate Bayesian computation (ABC) is a widely used inference method in Bayesian statistics to bypass the point-wise computation of the likelihood. In this paper we develop theoretical bounds for the distance between the statistics used in ABC. We show that some versions of ABC are inherently robust to mispecification. The bounds are given in the form of oracle inequalities for a finite sample size. The dependence on the dimension of the parameter space and the number of statistics is made explicit. The results are shown to be amenable to oracle inequalities in parameter space. We apply our theoretical results to given prior distributions and data generating processes, including a non-parametric regression model. In a second part of the paper, we propose a sequential Monte Carlo (SMC) to sample from the pseudo-posterior, improving upon the state of the art samplers.	approximation algorithm;calculus of variations;computation;goto;kernel (operating system);monte carlo method;oracle nosql db;time complexity;variational principle	James Ridgway	2017	CoRR		parameter space;oracle;statistics;econometrics;regression analysis;mathematics;sample size determination;particle filter;bayesian statistics;inference;mathematical optimization;approximate bayesian computation	ML	27.80285072377763	-28.273509701273625	146957
7b8f8890bd151b1895ac0cb66f134e87d90b1525	strongly hierarchical factorization machines and anova kernel regression		High-order parametric models that include terms for feature interactions are applied to various data mining tasks, where ground truth depends on interactions of features. However, with sparse data, the highdimensional parameters for feature interactions often face three issues: expensive computation, difficulty in parameter estimation and lack of structure. Previous work has proposed approaches which can partially resolve the three issues. In particular, models with factorized parameters (e.g. Factorization Machines) and sparse learning algorithms (e.g. FTRL-Proximal) can tackle the first two issues but fail to address the third. Regarding to unstructured parameters, constraints or complicated regularization terms are applied such that hierarchical structures can be imposed. However, these methods make the optimization problem more challenging. In this work, we propose Strongly Hierarchical Factorization Machines and ANOVA kernel regression where all the three issues can be addressed without making the optimization problem more difficult. Experimental results show the proposed models significantly outperform the state-of-the-art in two data mining tasks: cold-start user response time prediction and stock volatility prediction.	algorithm;cold start;computation;dspace;data mining;estimation theory;experiment;generalized linear model;ground truth;interaction;machine learning;mathematical optimization;matrix regularization;optimization problem;recommender system;response time (technology);sha-2;sparse matrix;time complexity;volatility	Ruocheng Guo;Hamidreza Alvari;Paulo Shakarian	2018		10.1137/1.9781611975321.82	parametric model;mathematics;pattern recognition;machine learning;artificial intelligence;sparse matrix;analysis of variance;estimation theory;ground truth;factorization;optimization problem;kernel regression	ML	28.433593479623095	-33.941127819989234	146958
58a0a4b8b88496659ea96718d2d9f36a5603a1e7	isosurface visualization of data with nonparametric models for uncertainty	isosurface extraction;biological patents;uncertainty quantification;kernel;biomedical journals;nonparametric statistics;text mining;europe pubmed central;uncertainty;citation search;uncertainty kernel isosurfaces random variables joints polynomials;random variables;isosurfaces;joints;citation networks;marching cubes;polynomials;nonparametric statistical framework data isosurface visualization nonparametric uncertainty models isosurface extraction process probability distribution data uncertainty topology extraction algorithm geometry extraction algorithm edge crossing probability based approach midpoint decider topological configuration probability density function monte carlo sampling;research articles;linear interpolation;abstracts;nonparametric statistics uncertainty quantification linear interpolation isosurface extraction marching cubes;open access;life sciences;clinical guidelines;full text;statistical distributions data visualisation;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	The problem of isosurface extraction in uncertain data is an important research problem and may be approached in two ways. One can extract statistics (e.g., mean) from uncertain data points and visualize the extracted field. Alternatively, data uncertainty, characterized by probability distributions, can be propagated through the isosurface extraction process. We analyze the impact of data uncertainty on topology and geometry extraction algorithms. A novel, edge-crossing probability based approach is proposed to predict underlying isosurface topology for uncertain data. We derive a probabilistic version of the midpoint decider that resolves ambiguities that arise in identifying topological configurations. Moreover, the probability density function characterizing positional uncertainty in isosurfaces is derived analytically for a broad class of nonparametric distributions. This analytic characterization can be used for efficient closed-form computation of the expected value and variation in geometry. Our experiments show the computational advantages of our analytic approach over Monte-Carlo sampling for characterizing positional uncertainty. We also show the advantage of modeling underlying error densities in a nonparametric statistical framework as opposed to a parametric statistical framework through our experiments on ensemble datasets and uncertain scalar fields.	algorithm;anatomy, regional;asymptotic decider;bonsai;computation;data point;experiment;graph drawing;handling (psychology);ibm notes;image noise;imagery;information extraction;interpolation imputation technique;isosurface;linear interpolation;meshlab;monte carlo method;nephrogenic systemic fibrosis;probability density;sampling (signal processing);silo (dataset);statistical model;statistics, nonparametric;uncertain data;vertex	Tushar Athawale;Elham Sakhaee;Alireza Entezari	2016	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2015.2467958	nonparametric statistics;random variable;text mining;uncertainty quantification;kernel;uncertainty;computer science;machine learning;data mining;mathematics;geometry;marching cubes;linear interpolation;statistics;polynomial	Visualization	32.67741216943069	-29.989763561915435	146976
d67787761ad313e40c56268f3d8aeb6c6f923321	aging detection of electrical point machines based on support vector data description		Electrical point machines (EPM) must be replaced at an appropriate time to prevent the occurrence of operational safety or stability problems in trains resulting from aging or budget constraints. However, it is difficult to replace EPMs effectively because the aging conditions of EPMs depend on the operating environments, and thus, a guideline is typically not be suitable for replacing EPMs at the most timely moment. In this study, we propose a method of classification for the detection of an aging effect to facilitate the timely replacement of EPMs. We employ support vector data description to segregate data of “aged” and “not-yet-aged” equipment by analyzing the subtle differences in normalized electrical signals resulting from aging. Based on the before and after-replacement data that was obtained from experimental studies that were conducted on EPMs, we confirmed that the proposed method was capable of classifying machines based on exhibited aging effects with adequate accuracy.	deep learning;electronic counter-countermeasure;one-class classification;random forest;unbalanced circuit	Younchang Choi;Yongwha Chung;Jonguk Lee;Daihee Park	2017	Symmetry	10.3390/sym9120290	combinatorics;signal;normalization (statistics);data mining;support vector machine;control engineering;mathematics;guideline;budget constraint	AI	36.558645829356074	-30.565751722977144	147074
41308a0015961a2946454a0194d2d0ed4f6c362c	entropy and inference, revisited	discrete distribution;dirichlet prior;phase space;data analysis;probability distribution	We study properties of popular near–uniform (Dirichlet) priors for learning undersampled probability distributions on discrete nonmetric spaces and show that they lead to disastrous results. However, an Occam–style phase space argument expands the priors into their infinite mixture and resolves most of the observed problems. This leads to a surprisingly good estimator of entropies of discrete distributions. Learning a probability distribution from examples is one of the basic problems in data analysis. Common practical approaches introduce a family of parametric models, leading to questions about model selection. In Bayesian inference, computing the total probability of the data arising from a model involves an integration over parameter space, and the resulting “phase space volume” automatically discriminates against models with larger numbers of parameters—hence the description of these volume terms as Occam factors [1, 2]. As we move from finite parameterizations to models that are described by smooth functions, the integrals over parameter space become functional integrals and methods from quantum field theory allow us to do these integrals asymptotically; again the volume in model space consistent with the data is larger for models that are smoother and hence less complex [3]. Further, at least under some conditions the relevant degree of smoothness can be determined self–consistently from the data, so that we approach something like a model independent method for learning a distribution [4]. The results emphasizing the importance of phase space factors in learning prompt us to look back at a seemingly much simpler problem, namely learning a distribution on a discrete, nonmetric space. Here the probability distribution is just a list of numbers {qi}, i = 1, 2, · · · ,K, where K is the number of bins or possibilities. We do not assume any metric on the space, so that a priori there is no reason to believe that any qi and qj should be similar. The task is to learn this distribution from a set of examples, which we can describe as the number of times ni each possibility is observed in a set of N = ∑K i=1 ni samples. This problem arises in the context of language, where the index i might label words or phrases, so that there is no natural way to place a metric on the space, nor is it even clear that our intuitions about similarity are consistent with the constraints of a metric space. Similarly, in bioinformatics the index i might label n–mers of the the DNA or amino acid sequence, and although most work in the field is based on metrics for sequence comparison one might like an alternative approach that does not rest on such assumptions. In the analysis of neural responses, once we fix our time resolution the response becomes a set of discrete “words,” and estimates of the information content in the response are determined by the probability distribution on this discrete space. What all of these examples have in common is that we often need to draw some conclusions with data sets that are not in the asymptotic limit N K. Thus, while we might use a large corpus to sample the distribution of words in English by brute force (reaching N K with K the size of the vocabulary), we can hardly do the same for three or four word phrases. In models described by continuous functions, the infinite number of “possibilities” can never be overwhelmed by examples; one is saved by the notion of smoothness. Is there some nonmetric analog of this notion that we can apply in the discrete case? Our intuition is that information theoretic quantities may play this role. If we have a joint distribution of two variables, the analog of a smooth distribution would be one which does not have too much mutual information between these variables. Even more simply, we might say that smooth distributions have large entropy. While the idea of “maximum entropy inference” is common [5], the interplay between constraints on the entropy and the volume in the space of models seems not to have been considered. As we shall explain, phase space factors alone imply that seemingly sensible, more or less uniform priors on the space of discrete probability distributions correspond to disastrously singular prior hypotheses about the entropy of the underlying distribution. We argue that reliable inference outside the asymptotic regimeN K requires a more uniform prior on the entropy, and we offer one way of doing this. While many distributions are consistent with the data when N ≤ K, we provide empirical evidence that this flattening of the entropic prior allows us to make surprisingly reliable statements about the entropy itself in this regime. At the risk of being pedantic, we state very explicitly what we mean by uniform or nearly uniform priors on the space of distributions. The natural “uniform” prior is given by Pu({qi}) = 1 Zu δ ( 1 − K ∑	asymptote;bayesian approaches to brain function;bioinformatics;brute-force search;entropy (information theory);heaps' law;information theory;model selection;mutual information;parametric polymorphism;peptide sequence;quantum field theory;self-information;text corpus;vocabulary;occam	Ilya Nemenman;F. Shafee;William Bialek	2001			probability distribution;dirichlet distribution;econometrics;mathematical optimization;concentration parameter;probability mass function;categorical distribution;dirichlet-multinomial distribution;mathematics;statistics	ML	27.043931352360183	-26.934006746109453	147566
777c82a9833629e313cc098bea921fa5e87278f5	time-series segmentation: a model and a method	iterations;time series;maximum likelihood estimation;probability distribution functions;time series analysis;mathematical models;parameters;observation;workshops;algorithms;relaxation;boundary value problems;segmented;labels	The problem of partitioning time series into segments is treated. The segments are considered as falling into classes, A different probability distribution is associated with each class of segment. Parametric families of distributions are considered, a set of parameter values being associated with each class. With each observation is associated an unobservable label, indicating from which class the observation arose. The label process is modeled as a Markov chain. Segmentation algorithms are obtained by applying a relaxation method to maximize the resulting likelihood function. Special attention is given to the situation in which the observations are conditionally independent, given the labels. A numerical example, segmentation of the U.S. gross national product, is given. Choice of the number of classes, using statistical model selection criteria, is illustrated.	algorithm;image segmentation;linear programming relaxation;markov chain;model selection;numerical analysis;relaxation (approximation);relaxation (iterative method);statistical model;time series	Stanley L. Sclove	1983	Inf. Sci.	10.1016/0020-0255(83)90007-5	econometrics;mathematical optimization;machine learning;time series;mathematics;algorithm;statistics	Vision	31.383090084556386	-25.3468943884405	147620
b9a2ebe415848bb34560d59167061b4356cd9f5f	foot flare and foot axis	eje;axis;membre inferieur;chine;morfometria;axe;hombre;foot;conception;courbure;morphometry;chaussure;asie;principal component analysis;human;diseno;turning point;curvatura;lower limb;design;curvature;hong kong;china;pie;morphometrie;shoe;pied;miembro inferior;asia;calzado;homme	Most commercial footwear is designed and manufactured on a curved last, although the amount of curvature of the last and the turning point of the last centerline have not been formally determined. In this study, we used principal component analysis to determine the foot axis so that lasts that match feet can be produced, resulting in a good fit. In evaluating 50 Hong Kong Chinese participants, we found that the center of the foot is located at approximately 52% of the foot length measuring from the back of the foot (SD = 0.65%) and that Hong Kong participants have a mean inflare (inward curvature) of 3.2 degrees. The foot center and inflare measures will help determine the fit between footwear and feet. Applications of this research include the ability to incorporate foot flare into the design and manufacture of footwear.		Ravindra S. Goonetilleke;Ameersing Luximon	1999	Human factors	10.1518/001872099779656761	design;engineering;soft foot;chine;curvature;forensic engineering;engineering drawing;china;foot;principal component analysis	HCI	38.05592026822132	-36.70164268905694	147650
e4a367f8f4046268e49457b0ff3f1755834d2743	convexity of proper composite binary losses	forecasting;conference paper;weight functions;artificial intelligence;keywords class probabilities	A composite loss assigns a penalty to a realvalued prediction by associating the prediction with a probability via a link function then applying a class probability estimation (CPE) loss. If the risk for a composite loss is always minimised by predicting the value associated with the true class probability the composite loss is proper. We provide a novel, explicit and complete characterisation of the convexity of any proper composite loss in terms of its link and its “weight function” associated with its proper CPE loss.	constant phase element;convex function;generalized linear model;weight function	Mark D. Reid;Robert C. Williamson	2010			mathematical optimization;forecasting;computer science;calculus;mathematics;statistics	AI	25.162128904532356	-25.333111498544792	147733
46b2a477eae0d9a7fbe0fad7b69e10f7ccdff20b	estimation of non-normalized statistical models by score matching	probability density function;natural images;independent component analysis;statistical model;objective function;markov chain monte carlo;markov chain monte carlo methods;statistical estimation;density functional	One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.	approximation;gradient;independent component analysis;loss function;markov chain monte carlo;monte carlo method;optimization problem;statistical model	Aapo Hyvärinen	2005	Journal of Machine Learning Research		monte carlo method in statistical physics;independent component analysis;statistical model;econometrics;markov chain;mathematical optimization;probability density function;density estimation;hybrid monte carlo;markov chain monte carlo;monte carlo molecular modeling;mathematics;additive markov chain;markov chain mixing time;statistics;monte carlo method;variable-order markov model	ML	28.57966441128558	-28.19113121943733	147981
a1bc29c089a29e6dd00aa8dd45d1f3efba152839	scale-based clustering with latent variables	analytical models;maximum likelihood;latent variable;dimension reduction;biological system modeling;clustering algorithms biological system modeling data models algorithm design and analysis analytical models probabilistic logic covariance matrices;statistical model;covariance matrices;cluster system;number of clusters;clustering algorithms;mixture of gaussians;probabilistic logic;human perception;algorithm design and analysis;data models	The use of clustering systems is very important in those real-word applications where an efficient, both accurate and economical, representation of the data to be processed is necessary. When dealing with statistical models, such a problem is usually related to the estimate of their parameters in the Maximum Likelihood context. At this regard, we propose an EM-based algorithm that uses a hierarchical growing approach, based on a given splitting procedure, to determine in an efficient way the parameters of a mixture of Gaussian clusters. The splitting procedure and the determination of the correct number of clusters are based on a scale-based approach, which imitates the human perception of images. Moreover, each cluster is modelled by means of latent variables, which also ensure a local linear dimension reduction of the data being processed.	algorithm;cluster analysis;dimensionality reduction;latent variable;statistical model	F. M. Frattale Mascioli;Massimo Panella;Antonello Rizzi;Giuseppe Martinelli	2000	2000 10th European Signal Processing Conference		machine learning;pattern recognition;mathematics;statistics	ML	30.174337782375158	-30.789746151284007	148014
0d5c07da8a0e6ddba072f83279f040992f8b11fc	tractability in structured probability spaces		Recently, the Probabilistic Sentential Decision Diagram (PSDD) has been proposed as a framework for systematically inducing and learning distributions over structured objects, including combinatorial objects such as permutations and rankings, paths and matchings on a graph, etc. In this paper, we study the scalability of such models in the context of representing and learning distributions over routes on a map. In particular, we introduce the notion of a hierarchical route distribution and show how they can be leveraged to construct tractable PSDDs over route distributions, allowing them to scale to larger maps. We illustrate the utility of our model empirically, in a route prediction task, showing how accuracy can be increased significantly compared to Markov models.	approximation;cobham's thesis;diagram;map;markov chain;markov model;scalability;spaces	Arthur Choi;Yujia Shen;Adnan Darwiche	2017			artificial intelligence;permutation;discrete mathematics;machine learning;probabilistic logic;scalability;markov model;influence diagram;graph;mathematics	ML	24.706606666055396	-27.588887974832243	148109
0dcdcc97ba4bf0301d1887791ce5ac8682e39128	constrained matrix factorization for financial data clustering	matrix factorization;data clustering;financial data analysis;trend extraction	Financial time series analysis is crucial to a successful assets allocation. Applying a matrix factorization technique can generate genuine grouping knowledge for the allocation of assets according to their association with a number of underlying bases. A constrained nonnegative matrix factorization NSMF is proposed to incorporate three penalties in order to compute a solution which can maximize between-base disjointness and volatility difference. A series of quantitative measures are designed for evaluation of bases and their volatility. Different types of real data are used in the experiments and compared regarding clustering consistency. Experimental analysis of historical prices of US blue chip stocks indicates that NSMF is superior to agglomerative clustering and independent component analysis and NSMF can extract bases with a higher discrepancy of volatility. The non-stochasticity constraint increases the dissimilarity of bases and it governs basis deviation over smoothness and sparseness. The clustering results of bases and persistent pairs, which are gained from NSMF, can consolidate our understanding of financial data properties and they provide meaningful knowledge in the construction of a well risk-balanced and diversified portfolio.	cluster analysis;discrepancy function;experiment;financial modeling;independent component analysis;neural coding;non-negative matrix factorization;time series;volatility	Jie Wang	2015	Intell. Data Anal.	10.3233/IDA-150719	econometrics;mathematical optimization;computer science;machine learning;data mining;mathematics;cluster analysis;matrix decomposition;statistics	ML	28.827088012804175	-35.90705200775899	148702
c58fe69617cfa490b9bb6f54c27fdfab5171ad2f	disturbance ratio for optimal multi-event classification in power distribution networks	discrete wavelet transforms;support vector machines;power quality;transient analysis;single event signal optimal multievent classification power distribution network power quality event identification pq event identification ieee std 11592009 intermittent power source renewable energy wavelet based global disturbance ratio index gdr index instantaneous precursor instantaneous transient disturbance index itd t index pdn intelligent disturbance classification support vector machine svm minimum input vector multievent signal;indexes;feature extraction;wavelet transform ieee std 1159 2009 power quality pq singleevent and multi event power quality svm;indexes support vector machines discrete wavelet transforms transient analysis power quality feature extraction;wavelet transforms power distribution faults power engineering computing power supply quality power system transients support vector machines	This paper presents an effective approach to identify power quality (PQ) events based on IEEE Std 1159-2009 caused by intermittent power sources like those of renewable energy. An efficient characterization of these disturbances is granted by the use of two useful wavelet-based indices. For this purpose, a wavelet-based global disturbance ratio (GDR) index, defined through its instantaneous precursor [instantaneous transient disturbance ITD(t) index], is used in power distribution networks (PDNs) under steady-state and/or transient conditions. An intelligent disturbance classification is done using a support vector machine (SVM) with a minimum input vector based on the GDR index. The effectiveness of the proposed technique is validated using a real-time experimental system with single event and multi-event signals.	electric power quality;experimental system;real-time clock;steady state;support vector machine;wavelet	Maria Dolores Borrás;Juan Carlos Bravo;Juan-Carlos Montaño	2016	IEEE Transactions on Industrial Electronics	10.1109/TIE.2016.2521615	control engineering;database index;support vector machine;electronic engineering;speech recognition;feature extraction;computer science;engineering;machine learning	Visualization	35.64304767806438	-32.01300567229679	149186
7a79171f3d3bfce7d5c5c5ac3b2346ff2ac79473	a rapid, non-parametric clustering scheme for flow cytometric data	traitement liste;pascal language;analyse amas;citometria flujo;tratamiento lista;microordinateur;microcomputer;algorithme;algorithm;histogram;algorritmo;cluster analysis;histogramme;microcomputadora;cytometrie flux;pattern recognition;pascal;reconnaissance forme;reconocimiento patron;histograma;flow cytometry;list processing	Abstract   A rapid, non-parametric program to identify clusters in multidimensional space is presented. The local event density, based on a low resolution histogram count, is stored on hashcoded, sorted, linked lists. Regions of monotonically decreasing density around a given mode are grouped into a cluster. Nearby clusters may be merged.  The algorithm can locate populations of only 1%, even in diffuse data, and gives percentages and locations of populations in real flow cytometric data in good agreement with results of conventional hand analysis. This is done in less than half a minute on a 68000 based microcomputer.	cluster analysis	Morgan P. Conrad	1987	Pattern Recognition	10.1016/0031-3203(87)90056-2	pascal;computer science;machine learning;pattern recognition;database;histogram;microcomputer;cluster analysis;flow cytometry;algorithm;statistics;pascal	Vision	34.79788137708055	-35.195627849788714	149207
72c3d430f25ace66f85fc70217084462dbc8bae2	bnstruct: an r package for bayesian network structure learning in the presence of missing data		Motivation A Bayesian Network is a probabilistic graphical model that encodes probabilistic dependencies between a set of random variables. We introduce bnstruct, an open source R package to (i) learn the structure and the parameters of a Bayesian Network from data in the presence of missing values and (ii) perform reasoning and inference on the learned Bayesian Networks. To the best of our knowledge, there is no other open source software that provides methods for all of these tasks, particularly the manipulation of missing data, which is a common situation in practice.   Availability and Implementation The software is implemented in R and C and is available on CRAN under a GPL licence.   Contact francesco.sambo@unipd.it.   Supplementary information Supplementary data are available at Bioinformatics online.		Alberto Franzin;Francesco Sambo;Barbara Di Camillo	2017	Bioinformatics	10.1093/bioinformatics/btw807	variable-order bayesian network;pattern recognition;data mining;statistics	Comp.	28.10864376503428	-31.327534673325538	149336
71ec7cbc61bf14b2a6d8b5166972b9acc1df52ef	localized incomplete multiple kernel k-means		The recently proposed multiple kernel k-means with incomplete kernels (MKKM-IK) optimally integrates a group of pre-specified incomplete kernel matrices to improve clustering performance. Though it demonstrates promising performance in various applications, we observe that it does not sufficiently consider the local structure among data and indiscriminately forces all pairwise sample similarity to equally align with their ideal similarity values. This could make the incomplete kernels less effectively imputed, and in turn adversely affect the clustering performance. In this paper, we propose a novel localized incomplete multiple kernel k-means (LI-MKKM) algorithm to address this issue. Different from existing MKKM-IK, LIMKKM only requires the similarity of a sample to its k-nearest neighbors to align with their ideal similarity values. This helps the clustering algorithm to focus on closer sample pairs that shall stay together and avoids involving unreliable similarity evaluation for farther sample pairs. We carefully design a three-step iterative algorithm to solve the resultant optimization problem and theoretically prove its convergence. Comprehensive experiments demonstrate that our algorithm significantly outperforms the state-of-the-art comparable algorithms proposed in the recent literature, verifying the advantage of considering local structure.	align (company);benchmark (computing);cluster analysis;computational complexity theory;experiment;iterative method;k-means clustering;k-nearest neighbors algorithm;kernel (operating system);mathematical optimization;optimization problem;resultant;robotics;verification and validation	Xinzhong Zhu;Xinwang Liu;Miaomiao Li;En Zhu;Li Liu;Zhiping Cai;Jianping Yin;Wen Gao	2018		10.24963/ijcai.2018/454	discrete mathematics;artificial intelligence;kernel (linear algebra);k-means clustering;machine learning;computer science	AI	27.710734358841965	-37.501864745676365	149475
d8e8f98e309c71cb135d06466336dbb3e33d9bb0	misspecified nonconvex statistical optimization for phase retrieval		Existing nonconvex statistical optimization theory and methods crucially rely on the correct specification of the underlying “true” statistical models. To address this issue, we take a first step towards taming model misspecification by studying the high-dimensional sparse phase retrieval problem with misspecified link functions. In particular, we propose a simple variant of the thresholded Wirtinger flow algorithm that, given a proper initialization, linearly converges to an estimator with optimal statistical accuracy for a broad family of unknown link functions. We further provide extensive numerical experiments to support our theoretical findings.	algorithm;experiment;mathematical optimization;maximum flow problem;numerical analysis;phase retrieval;program optimization;sparse matrix;statistical model	Zhuoran Yang;Lin F. Yang;Ethan X. Fang;Tuo Zhao;Zhaoran Wang;Matey Neykov	2017	CoRR		mathematics;simple variant;mathematical optimization;estimator;phase retrieval;initialization;statistical model	ML	26.929979436098183	-33.174066518333824	150044
cf27669eb3a0611815854770ef65fd1402353790	non-mercer hybrid kernel for linear programming support vector regression in nonlinear systems identification	quadratic program;nonlinear system identification;kernel function;support vector regression;support vector;hybrid kernel functions;system identification;statistical learning theory;loss function;linear programming;linear program;computational efficiency;nonlinear dynamic system;nonlinear systems identification	As a new sparse kernel modeling method, support vector regression (SVR) has been regarded as the stateof-the-art technique for regression and approximation. In [V.N. Vapnik, The Nature of Statistical Learning Theory, second ed., Springer-Verlag, 2000], Vapnik developed the e-insensitive loss function for the support vector regression as a trade-off between the robust loss function of Huber and one that enables sparsity within the support vectors. The use of support vector kernel expansion provides us a potentia avenue to represent nonlinear dynamical systems and underpin advanced analysis. However, in the standard quadratic programming support vector regression (QP-SVR), its implementation is often computationally expensive and sufficient model sparsity cannot be guaranteed. In an attempt to mitigate these drawbacks, this article focuses on the application of the soft-constrained linear programming support vector regression (LP-SVR) with hybrid kernel in nonlinear black-box systems identification. An innovative non-Mercer hybrid kernel is explored by leveraging the flexibility of LP-SVR in choosing the kernel functions. The simulation results demonstrate the ability to use more general kernel function and the inherent performance advantage of LP-SVR to QP-SVR in terms of model sparsity and computationa efficiency. 2008 Elsevier B.V. All rights reserved	analysis of algorithms;approximation;black box;data (computing);decision problem;dynamical system;genetic algorithm;hybrid kernel;lp-type problem;linear programming;loss function;nonlinear system;online and offline;qp state machine frameworks;quadratic programming;robotic arm;simulation;sparse matrix;springer (tank);statistical learning theory;support vector machine	Zhao Lu;Jing Sun	2009	Appl. Soft Comput.	10.1016/j.asoc.2008.03.007	principal component regression;support vector machine;least squares support vector machine;kernel method;mathematical optimization;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;linear programming;machine learning;polynomial regression;pattern recognition;graph kernel;mathematics;relevance vector machine;variable kernel density estimation;polynomial kernel;kernel smoother	AI	24.95737157676835	-33.9674369998212	150058
5b97c1b8239231aaebf5f917d07340d56d4d209a	batched stochastic gradient descent with weighted sampling	lipschitz constant;batching;stochastic gradient descent;numerical analysis;objective functions	"""We analyze a batched variant of Stochastic Gradient Descent (SGD) with weighted sampling distribution for smooth and non-smooth objective functions. We show that by distributing the batches computationally, a significant speedup in the convergence rate is provably possible compared to either batched sampling or weighted sampling alone. We propose several computationally efficient schemes to approximate the optimal weights, and compute proposed sampling distributions explicitly for the least squares and hinge loss problems. We show both analytically and experimentally that substantial gains can be obtained. 1. MATHEMATICAL FORMULATION We consider minimizing an objective function of the form F (x) = 1 n n ∑ i=1 fi (x) = E fi (x). (1.1) One important such objective function is the least squares objective for linear systems. Given an n ×m matrix A with rows a1, . . . , an and a vector b ∈Rn , one searches for the least squares solution xLS given by xLS def = argmin x∈Rm 1 2 ‖Ax −b‖2 = argmin x∈Rm 1 n n ∑ i=1 n 2 (bi −〈ai , x〉)2 = argmin x∈Rm E fi (x), (1.2) where the functionals are defined by fi (x) = 2 (bi −〈ai , x〉)2. Another important example is the setting of support vector machines where one wishes to minimize the hinge loss objective given by xHL def = argmin w∈Rm 1 n n ∑ i=1 [1− yi 〈w , xi 〉]++ λ 2 ‖w‖2. (1.3) Here, the data is given by the matrix X with rows x1, . . . , xn and the labels yi ∈ {−1,1}. The function [z]+ def = max(0, z) denotes the positive part. We view the problem (1.3) in the form (1.1) with fi (w ) = [1− yi 〈w , xi 〉]+ and regularizer λ2 ‖w‖2. The stochastic gradient descent (SGD) method solves problems of the form (1.1) by iteratively moving in the gradient direction of a randomly selected functional. SGD can be described succinctly by the update rule: xk+1 ← xk −γ∇ fik (xk ), where index ik is selected randomly in the kth iteration, and an initial estimation x0 is chosen arbitrarily. Typical implementations of SGD select the functionals uniformly at random, although if the problem at hand allows a one-pass preprocessing of the functionals, certain weighted sampling distributions preferring functionals with larger variation can provide better convergence (see e.g. [NSW16, ZZ15] and references therein). In particular, Needell et al. show that selecting a functional with probability proportional to the Lipschitz constant of its gradient yields a convergence rate depending on the average of all such Lipschitz constants, rather than the supremum [NSW16]. An analogous result in the same work shows that for non-smooth functionals, the probabilities should be chosen proportional to the Lipschitz constant of the functional itself. Date: August 30, 2016. 1 ar X iv :1 60 8. 07 64 1v 1 [ m at h. N A ] 2 7 A ug 2 01 6 2 DEANNA NEEDELL AND RACHEL WARD Another variant of SGD utilizes so-called mini-batches; in this variant, a batch of functionals is selected in each iteration rather than a single one [CSSS11, AD11, DGBSX12, TBRS13]. The computations over the batches can then be run in parallel and speedups in the convergence are often quite significant. Contribution. Our main contribution is to propose a weighted sampling scheme to be used in minibatch SGD. We show that when the batches can be implemented in parallel, significant speedup in convergence is possible. In particular, we analyze the convergence using computed distributions for the least squares and hinge loss objectives, the latter being especially challenging since it is non-smooth. We demonstrate theoretically and empirically that weighting the distribution and utilizing batches of functionals per iteration together form a complementary approach to accelerating convergence. Organization. We next briefly discuss some related work on SGD, weighted distributions, and batching methods. We then combine these ideas into one cohesive framework and discuss the benefits in various settings. Section 2 focuses on the impact of weighting the distribution. In Section 3 we analyze SGD with weighting and batches for smooth objective functions, considering the least squares objective as a motivating example. We analyze the non-smooth case along with the hinge loss objective function in Section 4. We display experimental results for the least squares problem in Section 5 that serve to highlight the relative tradeoffs of using both batches and weighting, along with different computational approaches. We conclude in Section 6. Related work. Stochastic gradient descent, stemming from the work [RM51], has recently received renewed attention for its effectiveness in treating large-scale problems arising in machine learning [BB11, Bot10, NJLS09, SSS08]. Importance sampling in stochastic gradient descent, as in the case of minibatching (which we also refer to simply as batching here), also leads to variance reduction in stochastic gradient methods and, in terms of theory, leads to improvement of the leading constant in the complexity estimate, typically via replacing the maximum of certain data-dependent quantities by their average. Such theoretical guarantees were shown for the case of solving least squares problems where stochastic gradient descent coincides with the randomized Kaczmarz method in [SV09]. This method was extended to handle noisy linear systems in [Nee10]. Later, this strategy was extended to the more general setting of smooth and strongly convex objectives in [NSW16], building on an analysis of stochastic gradient descent in [BM11]. Later, [ZZ15] considered a similar importance sampling strategy for convex but not necessarily smooth objective functions. Importance sampling has also been considered in the related setting of stochastic coordinate descent/ascent methods [Nes12, RT15, QRZ15, CQR15]. Other papers exploring advantages of importance sampling in various adaptations of stochastic gradient descent include but are not limited to [LS13, SRB13, XZ14, DB15]. Mini-batching in stochastic gradient methods refers to pooling together several random examples in the estimate of the gradient, as opposed to just a single random example at a time, effectively reducing the variance of each iteration [SSSSC11]. On the other hand, each iteration also increases in complexity as the size of the batch grows. However, if parallel processing is available, the computation can be done concurrently at each step, so that the “per-iteration cost"""" with batching is not higher than without batching. Ideally, one would like the consequence of using batch size b to result in a convergence rate speed-up by factor of b, but this is not always the case [BCNW12]. Still, [TBRS13] showed that by incorporating parallelization or multiple cores, this strategy can only improve on the convergence rate over standard stochastic gradient, and can improve the convergence rate by a factor of the batch size in certain situations, such as when the matrix has nearly orthonormal rows. Other recent papers exploring the advantages of mini-batching in different settings of stochastic optimization include [CSSS11, DGBSX12, NW13, KLRT16, LZCS14]. The recent paper [CR16] also considered the combination of importance sampling and mini-batching for a stochastic dual coordinate ascent algorithm in the general setting of empirical risk minimization, wherein the function to minimize is smooth and convex. There the authors provide a theoretical optimal sampling strategy that is not practical to implement but can be approximated via alternating minimization. They also provide a computationally efficient formula that yields better sample complexity than"""	algorithmic efficiency;approximation algorithm;compactrisc;computation;coordinate descent;d-subminiature;data dependency;emoticon;empirical risk minimization;experiment;hinge loss;importance sampling;iteration;kaczmarz method;least squares;linear system;machine learning;mathematical optimization;optimization problem;parallel computing;preprocessor;randomized algorithm;randomness;rate of convergence;sample complexity;sampling (signal processing);speedup;stemming;stochastic gradient descent;stochastic optimization;support vector machine;the matrix;times ascent;variance reduction	Deanna Needell;Rachel Ward	2016	CoRR		mathematical optimization;combinatorics;numerical analysis;mathematics;stochastic gradient descent;algorithm;statistics	ML	24.819695217802977	-33.143005797718914	150283
614081fe6d12b3aff5dd8d66bdf6108549c4c504	post lasso stability selection for high dimensional linear models		Lasso and sub-sampling based techniques (e.g. Stability Selection) are nowadays most commonly used methods for detecting the set of active predictors in high-dimensional linear models. The consistency of the Lassobased variable selection requires the strong irrepresentable condition on the design matrix to be fulfilled, and repeated sampling procedures with large feature set make the Stability Selection slow in terms of computation time. Alternatively, two-stage procedures (e.g. thresholding or adaptive Lasso) are used to achieve consistent variable selection under weaker conditions (sparse eigenvalue). Such two-step procedures involve choosing several tuning parameters that seems easy in principle, but difficult in practice. To address these problems efficiently, we propose a new two-step procedure, called Post Lasso Stability Selection (PLSS). At the first step, the Lasso screening is applied with a small regularization parameter to generate a candidate subset of active features. At the second step, Stability Selection using weighted Lasso is applied to recover the most stable features from the candidate subset. We show that under mild (generalized irrepresentable) condition, this approach yields a consistent variable selection method that is computationally fast even for a very large number of variables. Promising performance properties of the proposed PLSS technique are also demonstrated numerically using both simulated and real data examples.	active set method;computation;computational resource;feature selection;lasso;least squares;linear model;matrix regularization;numerical analysis;programme delivery control;sampling (signal processing);selection (genetic algorithm);sensor;sparse matrix;thresholding (image processing);time complexity	Niharika Gauraha;Tatyana Pavlenko;Swapan K. Parui	2017		10.5220/0006244306380646	elastic net regularization	ML	31.28488707760557	-37.155939559222524	150335
7dfe8718bc35a5010cd7d355083ac115b95a550d	application of principal component pursuit to process fault detection and diagnosis	process monitoring;data-driven process monitoring;fault detection;training data preprocessing;pcp-based process monitoring;signal processing;residual generator;scaling preprocessing step;pcp generated process model;computerised monitoring;principal component pursuit;pcp-based process modeling;data matrix quality;fault signal;fault diagnosis;data handling;filtering theory;fault isolation;fault reconstruction;pcp technique;principal component analysis;post-filtered residual;residual signal post-filtering;vectors;coherence;sparse matrices;optimization	Data-driven process monitoring has been extensively discussed in both academia and industry because of its applicability and effectiveness. One of the most applied techniques is the principal component analysis (PCA). Recently a new technique called principal component pursuit (PCP) is introduced. Compared to PCA, PCP is more robust to outliers. In this paper, the application of the PCP technique to process monitoring is thoroughly discussed from training data preprocessing to residual signal post-filtering. A new scaling preprocessing step is proposed to improve quality of data matrices in the sense of low coherence. A residual generator and a post-filter suitable for PCP generated process models are also provided. The post-filtered residual represents the fault signal, which makes the fault detection, isolation and reconstruction procedures simple and straightforward. A numerical example is provided to describe and illustrate the PCP-based process modeling and monitoring procedures.	data pre-processing;fault detection and isolation;image scaling;isolation (database systems);numerical analysis;preprocessor;principal component analysis;process modeling	Yue Cheng;Tongwen Chen	2013	2013 American Control Conference		real-time computing;computer science;engineering;machine learning;group method of data handling;signal processing;data mining;principal component analysis	Robotics	36.0087774772587	-28.693824884692546	150419
9c10188e0d0d0a7b348b6f20a19a7e673bad4a7e	multiple mutual informations and multiple interactions in frequency data	mutual information	McGill's multiple mutual informations are useful to systematically describe multiple interactions of frequency data with general  n -way. The asymptotic behaviour of the maximum likelihood estimators of them is analysed in terms of mutually independent  X  2 -distributions. On the basis of the results, the concept of  semi-independence  is introduced as a finer one of the concept of independence in the ordinary sense, and is used to interpret various multiple interactions.	interaction	Te Sun Han	1980	Information and Control	10.1016/S0019-9958(80)90478-7	econometrics;computer science;pattern recognition;mathematics;mutual information;statistics;pointwise mutual information	Robotics	33.131854308735775	-26.799187567362466	150501
18bacd9628e99d7e1ee6909114760bcd9bd0f215	an em algorithm for the student-t cluster-weighted modeling		Cluster-Weighted Modeling is a flexible statistical framework for modeling local relationships in heterogeneous populations on the basis of weighted combinations of local models. Besides the traditional approach based on Gaussian assumptions, here we consider Cluster Weighted Modeling based on Student-t distributions. In this paper we present an EM algorithm for parameter estimation in Cluster-Weighted models according to the maximum likelihood approach.	cluster-weighted modeling;estimation theory;expectation–maximization algorithm;population	Salvatore Ingrassia;Simona C. Minotti;Giuseppe Incarbone	2010		10.1007/978-3-642-24466-7_2	statistics;maximum likelihood;cluster-weighted modeling;expectation–maximization algorithm;conditional probability distribution;mathematics;gaussian;estimation theory	AI	31.032185912199836	-26.53286572030148	150604
8ba38495f61245ebd54d5eab2562632313e4a50a	doa estimation of excavation devices with elm and music-based hybrid algorithm	focusing matrix;music;direction-of-arrival;decision matrix;elm classification;excavation devices	Underground pipelines suffered severe external breakage caused by excavation devices due to arbitral road excavation. Acoustic signal-based recognition has recently shown effectiveness in underground pipeline network surveillance. However, merely relying on recognition may lead to a high false alarm rate. The reason is that underground pipelines are generally paved along a fixed direction and excavations out of the region also trigger the surveillance system. To enhance the reliability of the surveillance system, the direction-of-arrival (DOA) estimation of target sources is combined into the recognition algorithm to reduce false detections in this paper. Two hybrid recognition algorithms are developed. The first one employs extreme learning machine (ELM) for acoustic recognition followed by a focusing matrix-based multiple signal classification algorithm (ELM-MUSIC) for DOA estimation. The second introduces a decision matrix (DM) to characterize the statistic distribution of results obtained by ELM-MUSIC. Real acoustic signals collected by a cross-layer sensor array are conducted for performance comparison. Four representative excavation devices working in a metro construction site are used to generate the signal. Multiple scenarios of the experiments are designed. Comparisons show that the proposed ELM-MUSIC and DM algorithms outperform the conventional focusing matrix based MUSIC (F-MUSIC). In addition, the improved DM method is capable of localizing multiple devices working in order. Two hybrid acoustic signal recognition and source direction estimation algorithms are developed for excavation device classification in this paper. The novel recognition combining DOA estimation scheme can work efficiently for underground pipeline network protection in the real-world complex environment.	acoustic cryptanalysis;acoustic fingerprint;computer and network surveillance;direction of arrival;elm;experiment;hybrid algorithm;internationalization and localization;music (algorithm);pipeline (computing);sensor;two-hybrid screening;underground	Jianzhong Wang;Kai Ye;Jiuwen Cao;Tianlei Wang;Anke Xue;Yuhua Cheng;Chun Yin	2017	Cognitive Computation	10.1007/s12559-017-9475-3	machine learning;decision matrix;pattern recognition;computer science;artificial intelligence;sensor array;extreme learning machine;hybrid algorithm;statistic;constant false alarm rate;excavation;direction of arrival	Mobile	38.876028611712144	-34.74950747355035	150675
0505f62cce4e72528931f3651f578e133f4126b6	smooth minimization of nonsmooth functions with parallel coordinate descent methods	math oc;stat ml;cs dc	We study the performance of a family of randomized parallel coordinate descent methods for minimizing the sum of a nonsmooth and separable convex functions. The problem class includes as a special case L1-regularized L1 regression and the minimization of the exponential loss (“AdaBoost problem”). We assume the input data defining the loss function is contained in a sparse m× n matrix A with at most ω nonzeros in each row. Our methods need O(nβ/τ) iterations to find an approximate solution with high probability, where τ is the number of processors and β = 1 + (ω − 1)(τ − 1)/(n − 1) for the fastest variant. The notation hides dependence on quantities such as the required accuracy and confidence levels and the distance of the starting iterate from an optimal point. Since β/τ is a decreasing function of τ , the method needs fewer iterations when more processors are used. Certain variants of our algorithms perform on average only O(nnz(A)/n) arithmetic operations during a single iteration per processor and, because β decreases when ω does, fewer iterations are needed for sparser problems.	adaboost;approximation algorithm;central processing unit;convex function;coordinate descent;fastest;iteration;loss function;randomized algorithm;sparse matrix;time complexity;with high probability	Olivier Fercoq;Peter Richtárik	2013	CoRR		mathematical optimization;algorithm	ML	25.188392706605953	-33.90830767684857	150677
0047c9f6bd13cb7b16e7f15e2fbd2066c02811e5	is ordered weighted ℓ1 regularized regression robust to adversarial perturbation? a case study on oscar		Many state-of-the-art machine learning models such as deep neural networks have recently shown to be vulnerable to adversarial perturbations, especially in classification tasks. Motivated by adversarial machine learning, in this paper we investigate the robustness of sparse regression models with strongly correlated covariates to adversarially designed measurement noises. Specifically, we consider the family of ordered weighted `1 (OWL) regularized regression methods and study the case of OSCAR (octagonal shrinkage clustering algorithm for regression) in the adversarial setting. Under a norm-bounded threat model, we formulate the process of finding a maximally disruptive noise for OWL-regularized regression as an optimization problem and illustrate the steps towards finding such a noise in the case of OSCAR. Experimental results demonstrate that the regression performance of grouping strongly correlated features can be severely degraded under our adversarial setting, even when the noise budget is significantly smaller than the ground-truth signals.	adversarial machine learning;adversary (cryptography);algorithm;artificial neural network;cluster analysis;deep learning;mathematical optimization;oscar;optimization problem;sparse matrix;threat model;web ontology language	Pin-Yu Chen;Bhanukiran Vinzamuri;Sijia Liu	2018	CoRR			ML	25.825144072937455	-37.16032059643938	150980
0a7cd3f7f600a2eee608f649275e5b35fa5694d5	nonparametric divergence estimators for independent subspace analysis	silicon;electronic mail;ucl;discovery;theses;conference proceedings;integral formulae nonparametric divergence estimators independent subspace analysis mutual information estimation;digital web resources;estimation;ucl discovery;signal processing;open access;signal processing electronic mail mutual information estimation silicon europe computer science;mutual information;ucl library;computer science;book chapters;open access repository;europe;nonparametric statistics integral equations;ucl research	In this paper we propose new nonparametric Rényi, Tsallis, and L2 divergence estimators and demonstrate their applicability to mutual information estimation and independent subspace analysis. Given two independent and identically distributed samples, a “naïve” divergence estimation approach would simply estimate the underlying densities, and plug these densities into the corresponding integral formulae. In contrast, our estimators avoid the need to consistently estimate these densities, and still they can lead to consistent estimations. Numerical experiments illustrate the efficiency of the algorithms.	algorithm;eventual consistency;experiment;mutual information;numerical analysis;numerical method	Barnabás Póczos;Zoltán Szabó;Jeff G. Schneider	2011	2011 19th European Signal Processing Conference		econometrics;computer science;data mining;statistics	Vision	30.73454213397433	-26.012516563302153	151075
4a6d7370393542f835cb99607ed94ff67d8c74b1	use of particle swarm optimization for machinery fault detection	swarm intelligence;computational intelligence;particle swarm optimizer;condition monitoring;machinery condition monitoring;feature extraction;signal processing;principal component analysis;fault detection;success rate;genetic algorithm;time domain;feature selection;support vector machine;artificial neural network	A study is presented on the application of particle swarm optimization (PSO) combined with other computational intelligence (CI) techniques for bearing fault detection in machines. The performance of two CI based classifiers, namely, artificial neural networks (ANNs) and support vector machines (SVMs) are compared. The time domain vibration signals of a rotating machine with normal and defective bearings are processed for feature extraction. The extracted features from original and preprocessed signals are used as inputs to the classifiers for detection of machine condition. The classifier parameters, e.g., the number of nodes in the hidden layer for ANNs and the kernel parameters for SVMs are selected along with input features using PSO algorithms. The classifiers are trained with a subset of the experimental data for known machine conditions and are tested using the remaining set of data. The procedure is illustrated using the experimental vibration data of a rotating machine. The roles of the number of features, PSO parameters and CI classifiers on the detection success are investigated. Results are compared with other techniques such as genetic algorithm (GA) and principal component analysis (PCA). The PSO based approach gave a test classification success rate of 98.6–100% which were comparable with GA and much better than with PCA. The results show the effectiveness of the selected features and the classifiers in the detection of the machine condition. & 2008 Elsevier Ltd. All rights reserved.	artificial neural network;computational intelligence;fault detection and isolation;feature extraction;genetic algorithm;mathematical optimization;particle swarm optimization;phase-shift oscillator;principal component analysis;software release life cycle;support vector machine	Biswanath Samanta;Chandrasekhar Nataraj	2009	Eng. Appl. of AI	10.1016/j.engappai.2008.07.006	support vector machine;genetic algorithm;feature extraction;swarm intelligence;time domain;computer science;artificial intelligence;machine learning;computational intelligence;signal processing;pattern recognition;feature selection;artificial neural network;fault detection and isolation;principal component analysis	AI	37.17036046892906	-32.04475752787303	151157
75a9fe304f702496252628609e8f2aafe3c2ed7b	process fault detection based on dynamic kernel slow feature analysis	kernel principal component analysis;nonlinear dynamic process;fault detection;slow feature analysis	A fault detection method based on dynamic kernel slow feature analysis (DKSFA) is presented in the paper. SFA is a new feature extraction technology which can find a group of slowly varying feature outputs from the high-dimensional inputs. In order to analyze the nonlinear dynamic characteristics of the process data, DKSFA is presented which applies the augmented matrix to consider the dynamic characteristic and uses kernel slow feature analysis (KSFA) to extract the nonlinear slow features hidden in the observed data. For the purpose of fault detection, the D monitoring statistic index is built based on DKSFA model and its confidence limit is computed by kernel density estimation. Simulations on a nonlinear system and Tennessee Eastman (TE) benchmark process show that the proposed method has a better fault detection performance compared with the conventional (kernel principal component analysis) KPCA-based method. 2014 Elsevier Ltd. All rights reserved.	benchmark (computing);computer simulation;experiment;fault detection and isolation;feature extraction;kernel (operating system);kernel density estimation;kernel principal component analysis;nonlinear system;numerical analysis;participatory monitoring;peter eastman (software engineer);test engineer	Ni Zhang;Xuemin Tian;Lianfang Cai;Xiaogang Deng	2015	Computers & Electrical Engineering	10.1016/j.compeleceng.2014.11.003	real-time computing;kernel principal component analysis;computer science;engineering;machine learning;pattern recognition;variable kernel density estimation;fault detection and isolation	ML	36.25850565591621	-29.10649782118939	151382
95e9c5a47ee7631f0d0e373dd0429e0d7f80b6fa	manifold regularization based semisupervised semiparametric regression	parametric model;kernel based regression;semiparametric regression;performance analysis;numerical experiment;mr technique;manifold regularization;semisupervised regression	Semiparametric regression is an attractive solution for many practical problems. A nonlinear parametric model set, which is frequently encountered in practice, brings difficulties in complexity evaluation and control. On the other hand, it is quite common in practice that a mass of unlabeled samples is available, this fact suggests the possibility of applying a semisupervised regression method. Motivated by these facts, this paper proposes the manifold regularization based semisupervised semiparametric regression (MRBS^2R) method, which is characterized by introducing the manifold regularization (MR) technique in determining the parametric model. Generalization performance analysis shows that the generalization performance of the regression will be remarkably improved by introducing MR. Numerical experiments are performed to validate the proposed method.	manifold regularization;semi-supervised learning;semiparametric model	Zhe Sun;Zengke Zhang;Huangang Wang	2010	Neurocomputing	10.1016/j.neucom.2010.01.013	parametric model;machine learning;pattern recognition;mathematics;semiparametric model;statistics;semiparametric regression	NLP	27.113714602851484	-33.014622474649656	151694
297cab0187acd47305fdfb0116c54c97d4615c1b	flexible least squares for temporal data mining and statistical arbitrage	statistical arbitrage;flexible least squares;time varying;efficient algorithm;data stream;real time;kalman filter;trading system;financial data;investment strategies;pattern detection;algorithmic trading system;indexation;ordinary least square;time varying regression;temporal data mining	A number of recent emerging applications call for studying data streams, potentially infinite flows of information updated in realtime. When multiple co-evolving data streams are observed, an important task is to determine how these streams depend on each other, accounting for dynamic dependence patterns without imposing any restrictive probabilistic law governing this dependence. In this paper we argue that flexible least squares (FLS), a penalized version of ordinary least squares that accommodates for time-varying regression coefficients, can be deployed successfully in this context. Our motivating application is statistical arbitrage, an investment strategy that exploits patterns detected in financial data streams. We demonstrate that FLS is algebraically equivalent to the well-known Kalman filter equations, and take advantage of this equivalence to gain a better understanding of FLS and suggest a more efficient algorithm. Promising experimental results obtained from a FLS-based algorithmic trading system for the S&P 500 Futures Index are reported. 2008 Elsevier Ltd. All rights reserved.	algorithm;algorithmic trading;cluster analysis;coefficient;data mining;data point;dennis shasha;dynamic time warping;euclidean distance;feature extraction;feature selection;free library of springfield township;futures studies;hidden variable theory;independent component analysis;kalman filter;nonlinear dimensionality reduction;nonlinear system;online and offline;ordinary least squares;pattern recognition;random projection;randomness;real-time computing;real-time transcription;sensor;similarity measure;simulation;singular value decomposition;smoothing;time series;turing completeness;whole earth 'lectronic link	Giovanni Montana;Kostas Triantafyllopoulos;Theodoros Tsagaris	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.01.062	kalman filter;econometrics;investment strategy;ordinary least squares;computer science;artificial intelligence;machine learning;data mining;statistical arbitrage;statistics	ML	31.273318658452094	-32.208356697830816	152114
8225b056d1f704a63945c473e28a97e4c6b5d11f	estimation of a matrix of heterogeneity parameters in multivariate meta-analysis of random-effects models	random effects;dersimonian and laird;sidik andjonkman;hybrid method;heterogeneity parameter	Multivariate meta-analysis has potential over its univariate counterpart. The most common challenge in univariate or multivariate meta-analysis is estimating heterogeneity parameters in non-negative domains under the random-effects model assumption. In this context, two new multivariate estimation methods are demonstrated; first, by extending the Sidik and Jonkman (2005) univariate estimates to a multivariate setting, and second, by considering an iterative version of the Sidik and Jonkman method, namely, a Hybrid method developed in Wouhib (2013). These two methods are compared with extended DerSimonian and Laird methods (Jackson et al. 2009; Chen et al. 2012) by using an example and simulation in random-effects multivariate meta-analysis. Finally, the benefits of the proposed estimates are evaluated in terms of precision in estimating vectors of effect sizes and associated covariance matrices via simulation. Also, some limitations and remedies resulting from negative definite matrix in estimating heterogeneity parameters will be discussed.	random effects model	Abera Wouhib	2014	JSTA	10.2991/jsta.2014.13.1.5	econometrics;statistics	Theory	29.326074374377654	-24.261111312756974	152208
115efdc65e1c960d17a6c8b547d8313810a9f53b	structure learning for optimization		We describe a family of global optimization procedures that automatically decompose optimization problems into smaller loosely coupled problems. The solutions of these are subsequently combined with message passing algorithms. We show empirically that these methods produce better solutions with fewer function evaluations than existing global optimization methods. To develop these methods, we introduce a notion of coupling between variables of optimization. This notion of coupling generalizes the notion of independence between random variables in statistics, sparseness of the Hessian in nonlinear optimization, and the generalized distributive law. Despite its generality, this notion of coupling is easier to verify empirically, making structure estimation easy, while allowing us to migrate well-established inference methods on graphical models to the setting of global optimization.	algorithm;coupling (computer programming);generalized distributive law;global optimization;graphical model;hessian;loose coupling;mathematical optimization;message passing;neural coding;nonlinear programming;nonlinear system;program optimization	Shulin Yang;Ali Rahimi	2011			probabilistic-based design optimization;optimization problem;mathematical optimization;multi-swarm optimization;computer science;stochastic optimization;multi-objective optimization;machine learning;mathematics;vector optimization;algorithm;random optimization;metaheuristic;statistics;global optimization	ML	26.47500284243171	-32.63588609994523	152505
89794bce40a15ac24ca96a456e18d718869f3b31	a machine learning approach to adaptive covariance localization		Data assimilation plays a key role in large-scale atmospheric weather forecasting, where the state of the physical system is estimated from model outputs and observations, and is then used as initial condition to produce accurate future forecasts. The Ensemble Kalman Filter (EnKF) provides a practical implementation of the statistical solution of the data assimilation problem and has gained wide popularity as. This success can be attributed to its simple formulation and ease of implementation. EnKF is a Monte-Carlo algorithm that solves the data assimilation problem by sampling the probability distributions involved in Bayes’ theorem. Because of this, all flavors of EnKF are fundamentally prone to sampling errors when the ensemble size is small. In typical weather forecasting applications, the model state space has dimension 10−10, while the ensemble size typically ranges between 30−100 members. Sampling errors manifest themselves as long-range spurious correlations and have been shown to cause filter divergence. To alleviate this effect covariance localization dampens spurious correlations between state variables located at a large distance in the physical space, via an empirical distance-dependent function. The quality of the resulting analysis and forecast is greatly influenced by the choice of the localization function parameters, e.g., the radius of influence. The localization radius is generally tuned empirically to yield desirable results. Optimal tuning of the localization function parameters is still an open problem. This work, proposes two adaptive algorithms for covariance localization in the EnKF framework, both based on a machine learning approach. The first algorithm adapts the localization radius in time, while the second algorithm tunes the localization radius in both time and space. Numerical experiments carried out with the Lorenz-96 model, and a quasi-geostrophic model, reveal the potential of the proposed machine learning approaches.	data assimilation;ensemble kalman filter;experiment;initial condition;internationalization and localization;machine learning;monte carlo algorithm;sampling (signal processing);state space	Azam S. Zavar Moosavi;Ahmed Attia;Adrian Sandu	2018	CoRR		mathematics;probability distribution;spurious relationship;sampling (statistics);data assimilation;ensemble kalman filter;state space;state variable;covariance;machine learning;artificial intelligence	ML	28.067481348924346	-27.59229230627172	152557
06fb559243301ca43a588f32880f28e7d3c0f727	estimating complex causal effects from observational data		Causal calculus is a tool to express causal effects in the terms of observational probability distributions. The application of causal calculus in the non-parametric form requires only the knowledge of the causal structure. However, some kind of explicit modeling is needed when numeric estimates of the causal effect are to be calculated. In this paper, the estimation of complicated nonlinear causal relationships from observational data is studied. It is demonstrated that the estimation of causal effects does not necessarily require the causal model to be specified parametrically but it suffices to model directly the observational probability distributions. The conditions when this approach produces valid estimates are discussed. Generalized additive models, random forests and neural networks are applied to the estimation of causal effects in examples featuring the backdoor and the frontdoor adjustment.	additive model;artificial neural network;bundle adjustment;causal filter;causal model;causal system;causality;explicit modeling;nonlinear system;random forest	Juha Karvanen	2014	CoRR		causal system;econometrics;machine learning;mathematics;principal stratification;statistics;causal model	ML	25.824232390112225	-25.338319191228248	152687
35aa0d972f392b083138aa6834688df77c3720bb	projecting ising model parameters for fast mixing	conference paper	Inference in general Ising models is difficult, due to high treewidth making treebased algorithms intractable. Moreover, when interactions are strong, Gibbs sampling may take exponential time to converge to the stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences. We find that Gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling.	algorithm;converge;gibbs sampling;interaction;ising model;sampling (signal processing);stationary process;time complexity;treewidth	Justin Domke;Xianghang Liu	2013			econometrics;mathematical optimization;computer science;machine learning;mathematics;statistics	ML	26.792905820462835	-28.73127231511418	153237
09e5dcd265b0462f2e54eb38abf7d88b1f530bda	neural-network-based motor rolling bearing fault diagnosis	energy conversion;control systems;motor rolling bearing fault diagnosis;vibration analysis;electric motors;instruments;vibrations;neural networks;neural nets;frequency domain analysis;time frequency;time domain bearing vibration analysis;motor system;rolling bearings fault diagnosis neural networks vibration measurement energy conversion instruments monitoring control systems assembly systems frequency;bearing vibration frequency;indexing terms;time domain analysis;rolling bearings;monitoring;motor bearing fault diagnosis;vibration measurement;frequency domain bearing vibration analysis;electric machine analysis computing;assembly systems;vibration simulation;time domain;machine bearings;vibrations electric motors neural nets fault diagnosis machine bearings electric machine analysis computing frequency domain analysis time domain analysis;frequency;frequency domain bearing vibration analysis neural network motor rolling bearing fault diagnosis bearing vibration frequency motor bearing fault diagnosis time domain bearing vibration analysis vibration simulation;monitoring and control;fault diagnosis;neural network	Motor systems are very important in modern society. They convert almost 60% of the electricity produced in the U.S. into other forms of energy to provide power to other equipment. In the performance of all motor systems, bearings play an important role. Many problems arising in motor operations are linked to bearing faults. In many cases, the accuracy of the instruments and devices used to monitor and control the motor system is highly dependent on the dynamic performance of the motor bearings. Thus, fault diagnosis of a motor system is inseparably related to the diagnosis of the bearing assembly. In this paper, bearing vibration frequency features are discussed for motor bearing fault diagnosis. This paper then presents an approach for motor rolling bearing fault diagnosis using neural networks and time/frequency-domain bearing vibration analysis. Vibration simulation is used to assist in the design of various motor rolling bearing fault diagnosis strategies. Both simulation and real-world testing results obtained indicate that neural networks can be effective agents in the diagnosis of various motor bearing faults through the measurement and interpretation of motor bearing vibration signatures.	artificial neural network;electronic signature;simulation	Bo Li;Mo-Yuen Chow;Yodyium Tipsuwan;James C. Hung	2000	IEEE Trans. Industrial Electronics	10.1109/41.873214	structural engineering;control engineering;computer science;engineering;machine learning;vibration;control theory;artificial neural network;physics	Robotics	36.932210963339266	-31.019792533599066	153492
4ed6688adb26026eee306c95dccb816b97b45f4a	automated variational inference in probabilistic programming		We present a new algorithm for approximate inference in prob abilistic programs, based on a stochastic gradient for variational programs. Th is method is efficient without restrictions on the probabilistic program; it is pa rticularly practical for distributions which are not analytically tractable, inclu ding highly structured distributions that arise in probabilistic programs. We show ho w t automatically derive mean-field probabilistic programs and optimize them , and demonstrate that our perspective improves inference efficiency over other al gorithms.	approximation algorithm;calculus of variations;cobham's thesis;gradient;pa-risc;randomized algorithm;variational principle	David Wingate;Theophane Weber	2013	CoRR		mathematical optimization;probabilistic analysis of algorithms;probabilistic ctl;probabilistic relevance model;theoretical computer science;machine learning;mathematics;probabilistic logic;probabilistic logic network;divergence-from-randomness model	ML	26.27916853338636	-29.590006168868424	153636
546838981c9db80897c99c15049dcf4a0145aad5	bayesian graphical models, intention-to-treat, and the rubin causal model		In clinical trials with significant noncompliance the standard intention-to-treat analyses sometimes mislead. Rubin’s causal model provides an alternative method of analysis that can shed extra light on clinical trial data. Formulating the Rubin Causal Model as a graphical model facilitates model communication and computation.	adrian ettlinger;causal filter;causality;computation;graphical model;internet relay chat;randomized algorithm;richardson number;robbins v. lower merion school district;rubin causal model	David Madigan	1999			artificial intelligence;computer science;machine learning;causal model;computation;rubin causal model;intention-to-treat analysis;bayesian probability;graphical model	ML	25.40557432991758	-25.877986899808768	153646
73a4ab6fa39ff99fa482a7fe50d2ef2b41754140	randomized value functions via multiplicative normalizing flows		Randomized value functions offer a promising approach towards the challenge of efficient exploration in complex environments with high dimensional state and action spaces. Unlike traditional point estimate methods, randomized value functions maintain a posterior distribution over action-space values. This prevents the agent’s behavior policy from prematurely exploiting early estimates and falling into local optima. In this work, we leverage recent advances in variational Bayesian neural networks and combine these with traditional Deep Q-Networks (DQN) and Deep Deterministic Policy Gradient (DDPG) to achieve randomized value functions for high-dimensional domains. In particular, we augment DQN and DDPG with multiplicative normalizing flows in order to track a rich approximate posterior distribution over the parameters of the value function. This allows the agent to perform approximate Thompson sampling in a computationally efficient manner via stochastic gradient methods. We demonstrate the benefits of our approach through an empirical comparison in high dimensional environments.	algorithmic efficiency;approximation algorithm;artificial neural network;bellman equation;calculus of variations;gradient;local optimum;randomized algorithm;sampling (signal processing);thompson sampling	Ahmed Touati;Harsh Satija;Joshua Romoff;Joelle Pineau;Pascal Vincent	2018	CoRR		thompson sampling;mathematical optimization;machine learning;local optimum;artificial neural network;point estimation;multiplicative function;mathematics;artificial intelligence;posterior probability;bayesian probability	ML	24.745337282694816	-30.700098504810576	153692
1f751da8b7653fad5950c079eaa31390062aad51	dictionary learning for high dimensional graph signals		In recent years there is a growing interest in operating on graph signals. One systematic and productive such line of work is incorporating sparsity-inspired models to this data type, offering these signals a description as sparse linear combinations of atoms from a given dictionary. In this paper, we propose a dictionary learning algorithm for this task that is capable of handling high dimensional data. We incorporate the underlying graph topology by forcing the learned dictionary atoms to be sparse combinations of graph wavelet functions. The resulting atoms thus adhere to the underlying graph structure and possess a desired multi-scale property, yet they capture the prominent features of the data of interest. This results in both adaptive representations and an efficient implementation. Experimental results on different datasets, representing both synthetic and real network data, demonstrate the effectiveness of the proposed algorithm for graph signal processing.	algorithm;dictionary;directed graph;machine learning;signal processing;sparse matrix;synthetic intelligence;topological graph theory;wavelet	Yael Yankelevsky;Michael Elad	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462609	clustering high-dimensional data;data type;wavelet;linear combination;sparse matrix;signal processing;topological graph theory;artificial intelligence;pattern recognition;forcing (mathematics);computer science	ML	29.84799844243334	-37.09470385062762	154242
5acfdf19b3ac8dd1a133ed6846e83f8127d7e35d	some inequalities between distance measures for feature evaluation	bhattacharyya coefficient;kolmogorov variational distance;kullback divergence;matusita distance;distance measures;feature evaluation;interset feature selection criteria;pattern recognition	Some inequalities are derived between the Kullback divergence, the Bhattacharyya coefficient, the Matusita distance, and the Kolmogorov variational distance.	calculus of variations;coefficient;kullback–leibler divergence	Godfried T. Toussaint	1972	IEEE Transactions on Computers	10.1109/TC.1972.5008991	system on a chip;bhattacharyya distance;probability density function;feature extraction;computer science;machine learning;pattern recognition;mathematics;distance measures;feature selection;statistics;measurement uncertainty	Visualization	32.91456926412584	-32.55846851815931	154499
ccb25c2e26682a859b55bcc7dff1656bde4eb494	a nonparametric software reliability model based on kernel estimator and optimum algorithm	software reliability		algorithm;software quality;software reliability testing	Han Fengyan;Qin Zheng;Wang Xin	2004			kernel (linear algebra);estimator;polynomial kernel;kernel (statistics);computer science;kernel regression;kernel smoother;nonparametric statistics;pattern recognition;artificial intelligence;variable kernel density estimation	Robotics	30.562507442134013	-25.39164198004517	154555
6c86a8a669fbdd6e56c01aca3d9507018c3061bc	logistic regression for single trial eeg classification	symmetric matrix;common spatial pattern;logistic regression;feature extraction;topographic map;robust statistics;brain computer interface	We propose a novel framework for the classification of single trial ElectroEncephaloGraphy (EEG), based on regularized logistic regression. Framed in this robust statistical framework no prior feature extraction or outlier removal is required. We present two variations of parameterizing the regression function: (a) with a full rank symmetric matrix coefficient and (b) as a difference of two rank=1 matrices. In the first case, the problem is convex and the logistic regression is optimal under a generative model. The latter case is shown to be related to the Common Spatial Pattern (CSP) algorithm, which is a popular technique in Brain Computer Interfacing. The regression coefficients can also be topographically mapped onto the scalp similarly to CSP projections, which allows neuro-physiological interpretation. Simulations on 162 BCI datasets demonstrate that classification accuracy and robustness compares favorably against conventional CSP based classifiers.	algorithm;brain–computer interface;coefficient;common spatial pattern;computer simulation;electroencephalography;emoticon;feature extraction;generative model;logistic regression	Ryota Tomioka;Kazuyuki Aihara;Klaus-Robert Müller	2006			pattern recognition	ML	26.618844513616462	-34.461899489775924	154559
a316d4eedfd58ac671e1432657ea55c4efe6a7f7	identification of nonlinear block-oriented systems starting from linear approximations: a survey		Block-oriented models are popular in nonlinear modeling because of their advantages to be quite simple to understand and easy to use. Many different identification approaches were developed over the years to estimate the parameters of a wide range of block-oriented models. One class of these approaches uses linear approximations to initialize the identification algorithm. The best linear approximation framework and the $\epsilon$-approximation framework, or equivalent frameworks, allow the user to extract important information about the system, guide the user in selecting good candidate model structures and orders, and they prove to be a good starting point for nonlinear system identification algorithms. This paper gives an overview of the different block-oriented models that can be modeled using linear approximations, and of the identification algorithms that have been developed in the past. A non-exhaustive overview of the most important other block-oriented system identification approaches is also provided throughout this paper.	approximation	Maarten Schoukens;Koen Tiels	2016	CoRR		econometrics;mathematical optimization;simulation;mathematics	EDA	25.656327349718705	-24.76244097097589	154760
0b49fbd595cfabfe136614d4bdcc739a005c062b	minimizing the maximal rank	publikationer;minimisation approximation theory image denoising matrix algebra;konferensbidrag;linear shape model estimation maximal rank minimization low rank matrix approximation measurement matrix l 2 norm convex nuclear norm convex envelope manifold denoising;artiklar;rapporter;matematik;manifolds linear programming optimization computer vision noise reduction shape estimation	In computer vision, many problems can be formulated as finding a low rank approximation of a given matrix. Ideally, if all elements of the measurement matrix are available, this is easily solved in the L2-norm using factorization. However, in practice this is rarely the case. Lately, this problem has been addressed using different approaches, one is to replace the rank term by the convex nuclear norm, another is to derive the convex envelope of the rank term plus a data term. In the latter case, matrices are divided into sub-matrices and the envelope is computed for each subblock individually. In this paper a new convex envelope is derived which takes all sub-matrices into account simultaneously. This leads to a simpler formulation, using only one parameter to control the trade-of between rank and data fit, for applications where one seeks low rank approximations of multiple matrices with the same rank. We show in this paper how our general framework can be used for manifold denoising of several images at once, as well as just denoising one image. Experimental comparisons show that our method achieves results similar to state-of-the-art approaches while being applicable for other problems such as linear shape model estimation.	approximation algorithm;computer vision;convex hull;heuristic (computer science);low-rank approximation;manifold regularization;maximal set;noise reduction;proximal operator	Erik Bylow;Carl Olsson;Fredrik Kahl;Mikael G. Nilsson	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.634	mathematical optimization;combinatorics;discrete mathematics;rank;linear matrix inequality;mathematics;geometry;low-rank approximation	Vision	27.836155127560293	-37.29300201618747	154846
05831a33ad21494f5f87525cf207088851a0d145	adaptive estimation for varying coefficient models	62g05;varying coefficient models;62g08;local maximum likelihood;kernel smoothing;em algorithm;adaptive estimation	In this article, a novel adaptive estimation is proposed for varying coefficient models. Unlike the traditional least squares based methods, the proposed approach can adapt to different error distributions. An efficient EM algorithm is provided to implement the proposed estimation. The asymptotic properties of the resulting estimator are established. Both simulation studies and real data examples are used to illustrate the finite sample performance of the new estimation procedure. The numerical results show that the gain of the new procedure over the least squares estimation can be quite substantial for non-Gaussian errors.	coefficient	Yixin Chen;Qin Wang;Weixin Yao	2015	J. Multivariate Analysis	10.1016/j.jmva.2015.01.017	econometrics;mathematical optimization;expectation–maximization algorithm;mathematics;variable kernel density estimation;statistics;kernel smoother	NLP	29.859045016125567	-24.433056770936478	155000
740e0d13266916b7c4f11d364896610bfc1a6076	learning preferences with hidden common cause relations	model selection;empirical analysis;bayesian approach;prior distribution;probabilistic inference;real world application;gaussian process;learning preference	Gaussian processes have successfully been used to learn preferences among entities as they provide nonparametric Bayesian approaches for model selection and probabilistic inference. For many entities encountered in real-world applications, however, there are complex relations between them. In this paper, we present a preference model which incorporates information on relations among entities. Specifically, we propose a probabilistic relational kernel model for preference learning based on Silva et al.’s mixed graph Gaussian processes: a new prior distribution, enhanced with relational graph kernels, is proposed to capture the correlations between preferences. Empirical analysis on the LETOR datasets demonstrates that relational information can improve the performance of preference learning.	algorithm;entity;gaussian process;information retrieval;machine learning;mixed graph;model selection;mutual information;preference learning;provable security;sparse matrix;web search engine	Kristian Kersting;Zhao Xu	2009		10.1007/978-3-642-04180-8_61	preference learning;prior probability;bayesian probability;machine learning;pattern recognition;data mining;gaussian process;mathematics;model selection;statistics	ML	28.164445521109638	-31.639792858056097	155224
b817625a3dfd6ecef83a59dcc666801da3af4ae8	a new probabilistic generative model of parameter inference in biochemical networks	model selection;generic model;marginal likelihood;systems biology;discrete time;maximum likelihood estimate;system biology;biochemical network;maximum likelihood methods;biological data;parameter estimation;maximum likelihood method;biochemical networks	We present a new method for estimating rate coefficients and level of noise in models of biochemical networks from noisy observations of concentration levels at discrete time points. Its probabilistic formulation, based on maximum likelihood estimation, is key to a principled handling of the noise inherent in biological data, and it allows for a number of further extensions, such as a fully Bayesian treatment of the parameter inference and automated model selection strategies based on the comparison between marginal likelihoods of different models. We developed KInfer (Knowlegde Inference), a tool implementing our inference model. KInfer is downloadable for free at http://www.cosbi.eu.	coefficient;generative model;kinetics internet protocol;marginal model;model selection;simulation;time series	Paola Lecca;Alida Palmisano;Corrado Priami;Guido Sanguinetti	2009		10.1145/1529282.1529442	variable elimination;expectation–maximization algorithm;marginal likelihood;computer science;pattern recognition;maximum likelihood;likelihood function;quasi-maximum likelihood;maximum likelihood sequence estimation;estimation theory;systems biology;statistics	ML	31.632119157431585	-27.675048988082594	155397
4092ef60ef2caa9a1ab71b6ceedf5ec5dd1c3b06	a globally convergent algorithm for lasso-penalized mixture of linear regression models		Variable selection is an old and pervasive problem in regression analysis. One solution is to impose a lasso penalty to shrink parameter estimates toward zero and perform continuous model selection. The lasso-penalized mixture of linear regressions model (L-MLR) is a class of regularization methods for the model selection problem in the fixed number of variables setting. In this article, we propose a new algorithm for the maximum penalized-likelihood estimation of the L-MLR model. This algorithm is constructed via the minorization--maximization algorithm paradigm. Such a construction allows for coordinate-wise updates of the parameter components, and produces globally convergent sequences of estimates that generate monotonic sequences of penalized log-likelihood values. These three features are missing in the previously presented approximate expectation-maximization algorithms. The previous difficulty in producing a globally convergent algorithm for the maximum penalized-likelihood estimation of the L-MLR model is due to the intractability of finding exact updates for the mixture model mixing proportions in the maximization-step. In our algorithm, we solve this issue by showing that it can be converted into a polynomial root finding problem. Our solution to this problem involves a polynomial basis conversion that is interesting in its own right. The method is tested in simulation and with an application to Major League Baseball salary data from the 1990s and the present day. We explore the concept of whether player salaries are associated with batting performance.	algorithm;lasso	Luke R. Lloyd-Jones;Hien D. Nguyen;Geoffrey J. McLachlan	2018	Computational Statistics & Data Analysis	10.1016/j.csda.2017.09.003	econometrics;mathematical optimization;mathematics;statistics	ML	29.296327796780474	-27.606995806183143	155410
c321560482de6a0bc0eafb9cbe02f7871cb77408	automatic evaluation of flaws in pipes by means of ultrasonic waveforms and neural networks	ultrasonic applications flaw detection inspection neural nets pipes structural engineering computing;neural nets;ultrasonic wave;automatic evaluation;inspection;ultrasonic applications;neural network classification flaw detection pipes ultrasonic waveforms ultrasonic inspection technique ultrasonic wave inspection;structural engineering computing;flaw detection;error rate;intelligent networks neural networks inspection testing frequency biological neural networks ultrasonic imaging shape humans artificial neural networks;pipes;defect detection;neural network	The ultrasonic inspection technique takes a relevant place in not destructive defect detection. It can be very useful to determine the state of not accessible structure. In this paper a method based on ultrasonic waves inspection to evaluate the dimensions of flaws in not accessible pipes is shown. The method performs the extraction of time and frequency features from simulated ultrasonic waves and the proper reduction of the number of these features. Then a neural network classification evaluates the dimension of the flaws in the pipe under test. The results show low error rates for all classes considered.	neural networks;simulation;software bug	Giuseppe Acciani;Gioacchino Brunetti;Ernesto Chiarantoni;Girolamo Fornarelli	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246780	computer vision;inspection;word error rate;computer science;machine learning;artificial neural network	Robotics	37.873025473944814	-32.91120972172419	155737
2050944bcb6871e7687307c38f639edacdc883d0	efficient gradient computation for conditional gaussian models	arma model;probabilistic inference;incomplete data;mixed model;graphical model		computation;gradient	Bo Thiesson;Christopher Meek	2005			probabilistic relevance model;machine learning;pattern recognition;graphical model;statistics	ML	27.99676426704683	-29.540176262931915	155803
7b7c498e346296eb937ac519fdfa568a93cfe6da	modeling similarities among multi-dimensional financial time series		Pairs trading is one of the most successful strategies for stock investment. The performance of pairs trading heavily depends on modeling how similarity of two paired financial signals. Conventional methods measure similarity based on one-way or two-way signal, ignoring multiple information sources. In this paper, we propose a tensor-based framework to capture the intrinsic relations among multiple factors. Equities data is represented by tensors in firm-time-trading modes, on which tensor decomposition method is applied to seek a set of multilinear patterns for each mode. In this process, structural information is preserved which provides supplementary information for pairs trading. Experiments on stocks data of all constituent firms of  $S \& P 500$  demonstrate the superior performance of the proposed framework when compared with some state-of-the-art methods.	algorithmic trading;know-how trading;one-way function;time series	Dawei Cheng;Ye Liu;Zhibin Niu;Liqing Zhang	2018	IEEE Access	10.1109/ACCESS.2018.2862908	multilinear map;decomposition method (constraint satisfaction);time series;tensor;correlation;matrix decomposition;stress (mechanics);computer science;finance;pairs trade	Vision	28.831433764982073	-35.95803805136705	155946
1b2a0e8af5c1f18e47e71244973ce4ace4ac6034	compressed nonparametric language modelling		Hierarchical Pitman-Yor Process priors are compelling methods for learning language models, outperforming point-estimate based methods. However, these models remain unpopular due to computational and statistical inference issues, such as memory and time usage, as well as poor mixing of sampler. In this work we propose a novel framework which represents the HPYP model compactly using compressed suffix trees. Then, we develop an efficient approximate inference scheme in this framework that has a much lower memory footprint compared to full HPYP and is fast in the inference time. The experimental results illustrate that our model can be built on significantly larger datasets compared to previous HPYP models, while being several orders of magnitudes smaller, fast for training and inference, and outperforming the perplexity of the state-of-the-art Modified Kneser-Ney countbased LM smoothing by up to 15%.	approximation algorithm;language model;memory footprint;perplexity;random-access memory;recursion;sampling (signal processing);smoothing;speedup;suffix tree	Ehsan Shareghi;Gholamreza Haffari;Trevor Cohn	2017		10.24963/ijcai.2017/376	machine learning;natural language processing;nonparametric statistics;computer science;artificial intelligence	AI	25.818249865715458	-30.623149250578738	155970
ced7945f5cf2902e01cea20e8a066e56994dd92d	multi-way pls regression: monotony convergence of tri-linear pls2 and optimality of parameters	multi way pls regression;multi way data;monotony sequences;best rank one approximation;tensors	The tri-linear PLS2 iterative procedure, an algorithm pertaining to the NIPALS framework, is considered. It was previously proposed as a first stage to estimate parameters of the multi-way PLS regression method. It is shown that the tri-linear PLS2 procedure is convergent. The procedure generates a sequence of parameters (scores and loadings), which can be described as increasing or decreasing two specific criteria. Furthermore, a hidden tensor is described allowing tri-linear PLS2 to search its best rank-one approximation. This tensor highlights the link between multi-way PLS regression and the well-known PARAFAC model. The parameters of the multi-way PLS regression method can be computed using three alternative procedures. It is shown that the tri-linear PLS2 procedure is convergent.The sequences generated by the tri-linear PLS2 can be described as increasing or decreasing two specific criteria.A hidden tensor is described allowing tri-linear PLS2 to search its best rank one approximation.A link between multi-way PLS regression and the well-known PARAFAC model is highlighted.	triangular function	Mohamed Hanafi;Samia Samar Ouertani;Julien Boccard;Gérard Mazerolles;Serge Rudaz	2015	Computational Statistics & Data Analysis	10.1016/j.csda.2014.10.003	econometrics;tensor;machine learning;mathematics;statistics	ML	28.350425901629656	-35.430713649735885	156027
a33182e04fb0c448c4c3c4a2316879d9c8c04ce5	discrete mixtures of kernels for kriging-based optimization	likelihood ratio;mixtures of experts;maximum likelihood;gaussian processes;mixture of experts;kernel selection;gaussian kernel;ordinary kriging;global optimization;gaussian process;numerical simulation;model misspecification	Kriging-based exploration strategies often rely on a single Ordinary Kriging model which parametric covariance kernel is selected a priori or on the basis of an initial data set. Since choosing an unadapted kernel can radically harm the results, we wish to reduce the risk of model misspecification. Here we consider the simultaneous use of multiple kernels within Kriging. We give the equations of discrete mixtures of Ordinary Krigings, and derive a multikernel version of the expected improvement optimization criterion. We finally provide an illustration of the Efficient Global Optimization algorithm with mixed exponential and Gaussian kernels, where the parameters are estimated by Maximum Likelihood and the mixing weights are likelihood ratios.	akaike information criterion;algorithm;consortium;ego;earliest deadline first scheduling;estimation theory;global optimization;iteration;kernel (operating system);kriging;mathematical optimization;metamodeling;model selection;multikernel;onera;occam's razor;r language;time complexity;victor allis;victor animatograph corporation	David Ginsbourger;Céline Helbert;Laurent Carraro	2008	Quality and Reliability Eng. Int.	10.1002/qre.945	econometrics;mathematical optimization;variogram;gaussian process;mathematics;statistics;global optimization	ML	30.645667611282345	-26.43493187989082	156134
10f2ab4d63b3009db651539ecd6a3cd72e1f6f42	coherence pursuit: fast, simple, and robust principal component analysis		This paper presents a remarkably simple, yet powerful, algorithm termed coherence pursuit (CoP) to robust principal component analysis (PCA). As inliers lie in a low-dimensional subspace and are mostly correlated, an inlier is likely to have strong mutual coherence with a large number of data points. By contrast, outliers either do not admit low-dimensional structures or form small clusters. In either case, an outlier is unlikely to bear strong resemblance to a large number of data points. Given that, CoP sets an outlier apart from an inlier by comparing their coherence with the rest of the data points. The mutual coherences are computed by forming the Gram matrix of the normalized data points. Subsequently, the sought subspace is recovered from the span of the subset of the data points that exhibit strong coherence with the rest of the data. As CoP only involves one simple matrix multiplication, it is significantly faster than the state-of-the-art robust PCA algorithms. We derive analytical performance guarantees for CoP under different models for the distributions of inliers and outliers in both noise-free and noisy settings. CoP is the first robust PCA algorithm that is simultaneously non-iterative, provably robust to both unstructured and structured outliers, and can tolerate a large number of unstructured outliers.	algorithm;cache coherence;data point;gramian matrix;iterative method;matrix multiplication;mutual coherence (linear algebra);robust principal component analysis	Mostafa Rahmani;George Atia	2017	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2749215	machine learning;pattern recognition;mathematics;statistics	ML	26.731933573444056	-36.41381309336917	156176
753b15486fa1f4aaeb30d52308937723d9ae2520	pattern recognition based on time-frequency distributions of radar micro-doppler dynamics	object recognition;performance evaluation;vibrations;information extraction;pattern recognition time frequency analysis mathematical model radar scattering data mining feature extraction vibrations performance evaluation testing information analysis;target identification;radar micro doppler signatures;testing;data mining;discriminative feature set pattern recognition time frequency distribution radar micro doppler dynamics radar micro doppler signatures information extraction time frequency analysis target identification object recognition feature extraction feature selection;radar scattering;radar micro doppler dynamics;feature extraction;doppler radar;pattern classification;mathematical model;pattern recognition;pattern classification doppler radar time frequency analysis radar signal processing object recognition feature extraction target tracking;feature selection;target tracking;information analysis;time frequency analysis;radar signal processing;time frequency distribution;discriminative feature set	Radar micro-Doppler signatures are of great potential for identifying properties of unknown targets. An effective tool to extract information from the signatures is time-frequency analysis, based on which target identification and object recognition can be extended. In this paper, a method has been proposed for feature extraction and selection from simulated time-frequency distribution of micro-Doppler dynamics. Experimental results have shown that a highly discriminative feature set can be established by using this method. With this feature set, high classification performances both in training and testing stages for different classifiers have been achieved.	antivirus software;feature extraction;frequency analysis;outline of object recognition;pattern recognition;performance;radar;time–frequency analysis;time–frequency representation	Jiajin Lei	2005	Sixth International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing and First ACIS International Workshop on Self-Assembling Wireless Network	10.1109/SNPD-SAWN.2005.59	computer vision;speech recognition;time–frequency analysis;feature;feature extraction;computer science;cognitive neuroscience of visual object recognition;vibration;pattern recognition;mathematical model;software testing;data analysis;feature selection;information extraction;feature	SE	37.49001062583145	-34.53592770493138	156187
9aacc5a5424d23964f0bd46904db8a75d8fedb0c	marginalized continuous time bayesian networks for network reconstruction from incomplete observations	continuous time bayesian network;graph reconstruction;sequential monte carlo	Continuous Time Bayesian Networks (CTBNs) provide a powerful means to model complex network dynamics. However, their inference is computationally demanding — especially if one considers incomplete and noisy time-series data. The latter gives rise to a joint state- and parameter estimation problem, which can only be solved numerically. Yet, finding the exact parameterization of the CTBN has often only secondary importance in practical scenarios. We therefore focus on the structure learning problem and present a way to analytically marginalize the Markov chain underlying the CTBN model with respect its parameters. Since the resulting stochastic process is parameter-free, its inference reduces to an optimal filtering problem. We solve the latter using an efficient parallel implementation of a sequential Monte Carlo scheme. Our framework enables CTBN inference to be applied to incomplete noisy time-series data frequently found in molecular biology and other disciplines.	bayesian network	Lukas Studer;Loïc Paulevé;Christoph Zechner;Matthias Reumann;María Rodríguez Martínez;Heinz Koeppl	2016			particle filter;computer science;machine learning;statistics	ML	32.14380689125476	-28.09883191130694	156191
4d7c759c11e5127fa8b80353f354c41b1a1f35f3	em algorithm for symmetric stable mixture model		ABSTRACTMixture models are frequently used for modeling complex data. An extension of the EM algorithm, here called ECME, is proposed to compute the maximum likelihood estimate of parameters of symmetric -stable mixture model (SSMM). Comprehensive simulation studies are performed to show the performance of the proposed ECME algorithm. The robustness of the SSMM is investigated by simulations when it is used to model data generated from mixture of exponential power and t distributions. Both proposed ECME and Bayesian approaches are applied to three sets of real data, which shows that the proposed ECME algorithm outperforms the Bayesian paradigm for all three sets. Also, the SSMM is compared with the mixture of normal, skew normal, t, and skew t distributions for modeling four sets of real data. It turns out that the SSMM works as well as or better than above models. This can be considered as SSMM capability in robust mixture modeling.	expectation–maximization algorithm;mixture model	Mahdi Teimouri;Saeid Rezakhah;Adel Mohammadpour	2018	Communications in Statistics - Simulation and Computation	10.1080/03610918.2017.1288244	mixture model;econometrics;robustness (computer science);statistics;mathematics;skew;expectation–maximization algorithm;markov chain monte carlo;mathematical optimization;exponential function;stable distribution;bayesian probability	ML	30.544809753601267	-25.98354949177555	156272
ca44b160508c39d5692c64b411342040a860e787	piecewise latent variables for neural variational text processing		Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables — such as the uni-modal Gaussian distribution — which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.	calculus of variations;cobham's thesis;graphical model;latent variable;modal logic;natural language generation;natural language processing;time complexity;variational inequality;variational principle	Iulian Serban;Alexander Ororbia;Joelle Pineau;Aaron C. Courville	2017			piecewise;artificial intelligence;computer science;machine learning;natural language;prior probability;natural language generation;probabilistic latent semantic analysis;inference;graphical model;pattern recognition;latent variable	NLP	25.32822280696604	-30.592543047567	156315
fd354b37ea97576a31a2a56d27b242a1fd7c98ba	hierarchical correlation reconstruction with missing data		Machine learning often needs to model density from a multidimensional data sample, including correlations between coordinates. Additionally, we often have missing data case: that data points can miss values for some of coordinates. This article adapts rapid parametric density estimation approach for this purpose: modelling density as a linear combination of orthonormal functions, for which L optimization says that (independently) estimated coefficient for a given function is just average over the sample of value of this function. Hierarchical correlation reconstruction first models probability density for each separate coordinate using all its appearances in data sample, then adds corrections from independently modelled pairwise correlations using all samples having both coordinates, and so on independently adding correlations for growing numbers of variables using often decreasing evidence in data sample. A basic application of such modelled multidimensional density can be imputation of missing coordinates: by inserting known coordinates to the density, and taking expected values for the missing coordinates, or even their entire joint probability distribution. Presented method can be compared with cascade correlations approach, offering several advantages in flexibility and accuracy. It can be also used as artificial neuron: maximizing prediction capabilities for only local behavior modelling and predicting local connections.	artificial neuron;behavior model;coefficient;data point;geo-imputation;machine learning;mathematical optimization;missing data	Jarek Duda	2018	CoRR		algorithm;imputation (statistics);mathematical optimization;probability density function;expected value;missing data;density estimation;data point;linear combination;mathematics;parametric statistics	ML	31.060577192396128	-30.760833598161117	156420
8eeb452617beb8024e40120b36e12055122c87cb	comparison of dynamic model selection with infinite hmm for statistical model change detection	hidden markov models maximum likelihood estimation data models encoding maximum likelihood decoding maximum likelihood detection accuracy;maximum likelihood estimation;change point detection accuracy dynamic model selection infinite hmm hidden markov model statistical model change detection dms method minimum description length principle mdl principle nonparametric learning method code length sequential normalized maximum likelihood model change detection performance;hidden markov models;maximum likelihood estimation hidden markov models	In this study, we address the issue of tracking changes in statistical models under the assumption that the statistical models used for generating data may change over time. This issue is of great importance for learning from non-stationary data. One of the promising approaches for resolving this issue is the use of the dynamic model selection (DMS) method, in which a model sequence is estimated on the basis of the minimum description length (MDL) principle. Another approach is the use of the infinite hidden Markov model (HMM), which is a non-parametric learning method for the case with an infinite number of states. In this study, we propose a few new variants of DMS and propose efficient algorithms to minimize the total code-length by using the sequential normalized maximum likelihood. We compare these algorithms with infinite HMM to investigate their statistical model change detection performance, and we empirically demonstrate that one of our variants of DMS significantly outperforms infinite HMM in terms of change-point detection accuracy.	algorithm;hidden markov model;markov chain;mathematical model;minimum description length;model selection;stationary process;statistical model	Eiichi Sakurai;Kenji Yamanishi	2012	2012 IEEE Information Theory Workshop	10.1109/ITW.2012.6404680	maximum-entropy markov model;expectation–maximization algorithm;machine learning;hidden semi-markov model;pattern recognition;mathematics;quasi-maximum likelihood;maximum likelihood sequence estimation;statistics	ML	30.871612825598692	-31.138864382102522	156431
8abc2711863298f35349243be438a6b7c5074bb0	robust growth mixture models with non-ignorable missingness: models, estimation, selection, and application	growth mixture models;bayesian method;non ignorable missing data;model selecting criteria;robust methods	Challenges in the analyses of growth mixture models include missing data, outliers, estimation, and model selection. Four non-ignorable missingness models to recover the information due to missing data, and three robust models to reduce the effect of non-normality are proposed. A full Bayesian method is implemented by means of data augmentation algorithm and Gibbs sampling procedure. Model selection criteria are also proposed in the Bayesian context. Simulation studies are then conducted to evaluate the performances of the models, the Bayesian estimation method, and selection criteria under different situations. The application of the models is demonstrated through the analysis of education data on children’s mathematical ability development. The models can be widely applied to longitudinal analyses in medical, psychological, educational, and social research. © 2013 Elsevier B.V. All rights reserved.	algorithm;convolutional neural network;gibbs sampling;missing data;mixture model;model selection;performance;sampling (signal processing);simulation	Zhenqiu Lu;Zhiyong Zhang	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2013.07.036	bayesian average;econometrics;variable-order bayesian network;bayesian probability;multilevel model;data mining;mathematics;statistics	ML	29.880083172962912	-26.337038032047847	156476
9ba4bede38fe959c707fe4f7cbc8c1929e1e7f47	sequential nonparametric regression		We present algorithms for nonparametric regression in settings where the data are obtained sequentially. While traditional estimators select bandwidths that depend upon the sample size, for sequential data the effective sample size is dynamically changing. We propose a linear time algorithm that adjusts the bandwidth for each new data point, and show that the estimator achieves the optimal minimax rate of convergence. We also propose the use of online expert mixing algorithms to adapt to unknown smoothness of the regression function. We provide simulations that confirm the theoretical results, and demonstrate the effectiveness of the methods.	algorithm;data point;minimax;rate of convergence;simulation;time complexity	Haijie Gu;John D. Lafferty	2012	CoRR		econometrics;mathematical optimization;mathematics;nonparametric regression;statistics	ML	29.277891056670015	-27.146420667513883	156583
cccbe0db1887effb522e30676cfeb7acb0028b6e	learning gaussian mixtures with arbitrary separation	gaussian mixture;fourier transform;1 dimensional;gaussian mixture model;function approximation;science learning;theoretical analysis;vandermonde matrix;mixture of gaussians;high dimension;data structure;lower bound	In this paper we present a method for learning the parameters of a mixture of $k$ identical spherical Gaussians in $n$-dimensional space with an arbitrarily small separation between the components. Our algorithm is polynomial in all parameters other than $k$. The algorithm is based on an appropriate grid search over the space of parameters. The theoretical analysis of the algorithm hinges on a reduction of the problem to 1 dimension and showing that two 1-dimensional mixtures whose densities are close in the $L^2$ norm must have similar means and mixing coefficients. To produce such a lower bound for the $L^2$ norm in terms of the distances between the corresponding means, we analyze the behavior of the Fourier transform of a mixture of Gaussians in 1 dimension around the origin, which turns out to be closely related to the properties of the Vandermonde matrix obtained from the component means. Analysis of this matrix together with basic function approximation results allows us to provide a lower bound for the norm of the mixture in the Fourier domain. #R##N#In recent years much research has been aimed at understanding the computational aspects of learning parameters of Gaussians mixture distributions in high dimension. To the best of our knowledge all existing work on learning parameters of Gaussian mixtures assumes minimum separation between components of the mixture which is an increasing function of either the dimension of the space $n$ or the number of components $k$. In our paper we prove the first result showing that parameters of a $n$-dimensional Gaussian mixture model with arbitrarily small component separation can be learned in time polynomial in $n$.		Mikhail Belkin;Kaushik Sinha	2009	CoRR		mathematical optimization;combinatorics;data structure;computer science;machine learning;mixture model;mathematics;statistics	Crypto	30.710482630410137	-37.948812404095904	156918
815cc46e7e9f2a58f743e787df3471984f1e757a	a differentially private estimator for the stochastic kronecker graph model	satisfiability;social network;probability distribution;graph model;privacy;information theory;knowledge discovery	"""We consider the problem of making graph databases such as social networks available to researchers for knowledge discovery while providing privacy to the participating entities. We use a parametric graph model, the stochastic Kronecker graph model, to model the observed graph and construct an estimator of the """"true parameter"""" in a way that both satisfies the rigorous requirements of differential privacy and demonstrates experimental utility on several important graph statistics. The estimator, which may then be published, defines a probability distribution on graphs. Sampling such a distribution yields a synthetic graph that mimics important properties of the original sensitive graph and consequently, could be useful for knowledge discovery."""	differential privacy;entity;gibbs sampling;graph database;graph theory;requirement;social network;synthetic intelligence	Darakhshan J. Mir;Rebecca N. Wright	2012		10.1145/2320765.2320818	combinatorics;discrete mathematics;null graph;graph property;clique-width;machine learning;mathematics;voltage graph;graph	ML	26.275102957003327	-28.030965118382145	157263
aa35e4cd66777fabe1f66945b228b65dc9bb834c	"""discussion of the paper: """"sampling schemes for generalized linear dirichlet process random effects models"""" by m. kyung, j. gill, and g. casella"""	dirichlet process;random effects model	We congratulate the Authors for an interesting and thought provoking paper that compares different MCMC strategies for the class of generalized linear Dirichlet process random effects models. In our discussion we focus on the logistic regression model presented in Section 3.2 and would like to propose two alternative sampling schemes related to manifold methods that are quite general and could be employed every time a Metropolis-Hastings step is used within an MCMC simulation and the target distribution has closed form Fisher Information (FI). In particular we have implemented the same sampling scheme as presented by the Authors, but have sampled the parameters β using Simplified Manifold Metropolis Adjusted Langevin Algorithm (S-MMALA) and Riemann Manifold Hamiltonian Monte Carlo (RM-HMC), both presented in Girolami and Calderhead (2011). The starting point to apply these methods is the joint density of the observations and the parameters of interest, given by the likelihood of the model along with the prior distribution for β. The gradient of the logarithm of this joint density and the FI (the expectation, taken with respect to the observed variables y, of the negative Hessian of the log-likelihood) are then computed. In order to keep the notation uncluttered, we define the logistic elements	fisher information;gradient;hessian;hybrid memory cube;hybrid monte carlo;logistic regression;markov chain monte carlo;metropolis;metropolis–hastings algorithm;monte carlo method;random effects model;sampling (signal processing);simulation	Maurizio Filippone;Antonietta Mira;Mark A. Girolami	2011	Statistical Methods and Applications	10.1007/s10260-011-0170-3	econometrics;mathematical optimization;generalized dirichlet distribution;mathematics;statistics;random effects model	ML	30.92754486575675	-24.790291363719547	157317
119147279d157219eb5a88c0b4dbf2d5df0df4b2	parallel computing for efficient time-frequency feature extraction of power quality disturbances	parallel computing;stockwell transform;fourier transform;classifier model;data parallelism;shared memory architecture;distributed memory architecture;sag feature extraction;time frequency transform;time frequency feature extraction shared memory architecture distributed memory architecture sag feature extraction s transform fourier transform stockwell transform time frequency transform task parallelism data parallelism acceleration approach nonstationary pq signal multifrequency pq signal time time transform 1d discrete wavelet transform classifier model power quality disturbance detection power quality disturbance classification signal processing parallel computing;power quality disturbance detection;time frequency analysis discrete wavelet transforms distributed memory systems feature extraction fourier transforms parallel memories parallel processing power engineering computing power supply quality power system faults shared memory systems signal classification signal detection;multifrequency pq signal;nonstationary pq signal;signal processing;1d discrete wavelet transform;time frequency feature extraction;task parallelism;s transform;acceleration approach;power quality disturbance classification;time time transform	Fast signal processing implementation techniques for detection and classification of power quality (PQ) disturbances are the need of the hour. Hence in this work, a parallel computing approach has been proposed to speed up the feature extraction of PQ signals to facilitate rapid building of classifier models. Considering that the Fourier, the one-dimensional discrete wavelet, the time-time and the Stockwell transforms have been used extensively to extract pertinent time-frequency features from non-stationary and multi-frequency PQ signals, acceleration approaches using data and task parallelism have been employed for parallel implementation of the above time-frequency transforms. In the first approach, data parallelism was applied to the Stockwell transform and the time-time transform-based feature extraction methods separately to alleviate capability problems. Also, data parallelism was applied to Fourier and wavelet-based feature extraction methods independently to alleviate capacity problems. Secondly, a combination of task and data parallelism was applied to speed up S-transform based three-phase sag feature extraction. Experiments were conducted using shared-memory and distributed memory architectures to try out the effectiveness of the proposed parallel approaches. The performances of these parallel implementations were analysed in terms of computational speed and efficiency in comparison with the sequential approach.	electric power quality;feature extraction;parallel computing	Brahmadesam Krishna;Baskaran Kaliaperumal	2013	IET Signal Processing	10.1049/iet-spr.2012.0262	fourier transform;parallel computing;speech recognition;s transform;computer science;theoretical computer science;signal processing;data parallelism;shared memory architecture;task parallelism	EDA	37.99177208001057	-31.176580625584275	157462
7ac8c49a49db779283b9277f9fe5e476583d32e4	a probabilistic approach to fraud detection in telecommunications	kullback leibler divergence;latent dirichlet allocation;user profiling;fraud detection;telecommunications	In this paper, a method for telecommunications fraud detection is proposed. The method is based on the user profiling utilizing the Latent Dirichlet Allocation (LDA). Fraudulent behavior is detected with use of a threshold-type classification algorithm, allocating the telecommunication accounts into one of two classes: fraudulent account and non-fraudulent account. The paper provides also a method for automatic threshold computation. The accounts are classified with use of the Kullback–Leibler divergence (KLdivergence). Therefore, we also introduce three methods for approximating the KL-divergence between two LDAs. Finally, the results of experimental study on KL-divergence approximation and fraud detection in telecommunications are reported. 2011 Elsevier B.V. All rights reserved.	algorithm;approximation;approximation algorithm;computation;computational complexity theory;experiment;google map maker;kullback–leibler divergence;latent dirichlet allocation;mixture model;monte carlo method;multinomial logistic regression;receiver operating characteristic;simulation;statistical classification;statistical model	Dominik Olszewski	2012	Knowl.-Based Syst.	10.1016/j.knosys.2011.08.018	latent dirichlet allocation;computer science;data science;machine learning;data mining;kullback–leibler divergence;computer security;statistics	AI	33.59061101920527	-35.18218366827827	157822
0e96c4e4e809b2c57770be75297ca81e1251a8cf	motor shaft misalignment detection using multiscale entropy with wavelet denoising	spectrum analysis;multiscale entropy;induction motor;wavelet transform;fault detection;wavelet denoising	Misalignment of motor shaft (also manifesting as static eccentricity) is a common motor fault resulting from improper installation or damage of the machine components and their support structure. Spectrum analysis is generally used for online detection of such faults. This study presents a novel approach to discover features that distinguish the vibration signals of a normal motor from those of a misaligned one. These features are obtained from the difference of multiscale entropy of a signal, before and after the signal is denoised using wavelet transform. Experimental results show that classifiers based on these features obtain better and more stable accuracy rates than those based on frequency-related features.	noise reduction;wavelet	Jun-Lin Lin;Julie Yu-Chih Liu;Chih-Wen Li;Li-Feng Tsai;Hsin-Yi Chung	2010	Expert Syst. Appl.	10.1016/j.eswa.2010.04.009	spectrum analyzer;speech recognition;machine learning;pattern recognition;induction motor;mathematics;wavelet packet decomposition;fault detection and isolation;wavelet transform	Vision	37.42425447441242	-30.86104608135099	157828
0d68260ff20dbe68c439502e5058aa8e76a522c3	bispectrum entropy feature extraction and its application for fault diagnosis of gearbox	pattern recognition backpropagation entropy fault diagnosis gears mechanical engineering computing neural nets;mechanical engineering computing;vibrations;neural nets;bispectrum entropy feature extraction;backpropagation;fault characteristics bispectrum entropy feature extraction fault feature extraction gearbox fault diagnosis fault feature parameters information entropy information energy vibration signal secondary drive gearbox feature vector fault pattern recognition fault pattern diagnosis bp neural network;feature vector;gearbox fault diagnosis;secondary drive gearbox;gears;feature extraction;entropy vibrations fault diagnosis information entropy gears feature extraction teeth;fault pattern recognition;bp neural network;fault feature parameters;teeth;pattern recognition;fault characteristics;fault feature extraction;entropy;information energy;information entropy;fault diagnosis;neural network;vibration signal;fault pattern diagnosis	Fault feature extraction and application is the key technology of gearbox fault diagnosis. In this paper, a fault diagnosis method using bispectrum entropy as the fault feature parameters is put forward. Bispectrum entropy as the information entropy in bispectrum domain can reflect the complexity of information energy. When the structure is failed, the distribution of bispectrum will be changed. bispectrum entropy can reflect this change and achieve good separation of the different types of fault. In this paper, the vibration signal in different states of a secondary drive gearbox is compared and analyzed, bispectrum and bispectrum entropy are extracted. Feature vector is set up via bispectrum entropy for the fault pattern recognition and diagnosis by BP neural network. The analysis result proves that bispectrum entropy is more sensitive to fault characteristic and can separate the fault of gearbox. Via applying this method, the numerical characteristics extraction and intelligent diagnosis will be ease realized easily.	artificial neural network;bispectrum;entropy (information theory);feature extraction;feature vector;numerical analysis;pattern recognition	Jinying Huang;Hongxia Pan;Shihua Bi	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584109	entropy;speech recognition;feature vector;feature extraction;gear;computer science;backpropagation;machine learning;vibration;pattern recognition;tooth;artificial neural network;entropy	Robotics	37.1721343987393	-31.228144403645057	157946
81ad992fcced206323bf5df72c4ea24755bc9ddf	counterfactual mean embedding: a kernel method for nonparametric causal inference		This paper introduces a novel Hilbert space representation of a counterfactual distribution—called counterfactual mean embedding (CME)—with applications in nonparametric causal inference. Counterfactual prediction has become an ubiquitous tool in machine learning applications, such as online advertisement, recommendation systems, and medical diagnosis, whose performance relies on certain interventions. To infer the outcomes of such interventions, we propose to embed the associated counterfactual distribution into a reproducing kernel Hilbert space (RKHS) endowed with a positive definite kernel. Under appropriate assumptions, the CME allows us to perform causal inference over the entire landscape of the counterfactual distribution. The CME can be estimated consistently from observational data without requiring any parametric assumption about the underlying distributions. We also derive a rate of convergence which depends on the smoothness of the conditional mean and the Radon-Nikodym derivative of the underlying marginal distributions. Our framework can deal with not only real-valued outcome, but potentially also more complex and structured outcomes such as images, sequences, and graphs. Lastly, our experimental results on off-policy evaluation tasks demonstrate the advantages of the proposed estimator.	causal filter;causal inference;counterfactual conditional;counterfactual definiteness;graph (discrete mathematics);hilbert space;image;kernel method;machine learning;marginal model;online advertising;radon–nikodym theorem;rate of convergence;recommender system	Krikamol Muandet;Motonobu Kanagawa;Sorawit Saengkyongam;Sanparith Marukatat	2018	CoRR		causal inference;machine learning;mathematics;marginal distribution;counterfactual thinking;kernel method;positive-definite kernel;reproducing kernel hilbert space;parametric statistics;nonparametric statistics;artificial intelligence;pattern recognition	ML	26.261039044267946	-27.623718659251196	158113
91983e59ca50d5527cdf636f2329906d0733d72b	fast highly efficient and robust one-step m-estimators of scale based on qn	influence function;scale estimation;m estimator;m;robustness	A parametric family ofM-estimators of scale based on the Rousseeuw–Croux Qn-estimator is proposed; estimator’s bias and efficiency are studied. A low-complexity one-step M-estimator is obtained allowing a considerably faster computation with greater than 80% efficiency and the highest possible 50% breakdown point. Analytical and Monte Carlo modeling results confirm the effectiveness of the proposed approach. © 2014 Elsevier B.V. All rights reserved.	approximation algorithm;coefficient;computation;computational statistics;fixed points of isometry groups in euclidean space;fully qualified name;graphics;kaplan–meier estimator;monte carlo method;time complexity;word lists by frequency	Pavel O. Smirnov;Georgy L. Shevlyakov	2014	Computational Statistics & Data Analysis	10.1016/j.csda.2014.04.013	econometrics;mathematical optimization;haplogroup m;trimmed estimator;m-estimator;mathematics;statistics;robustness	AI	31.60781031877085	-24.699613771008163	158541
11c1a00475d4121f6bf59361ceb6e0dbc8f7701d	maximum likelihood estimation for nonlinear structural equation models with normal latent variables	monte carlo em algorithm;latent variable analysis;seemingly unrelated regression;errors in variables	Structural equation analysis has been widely used in behavioral and social sciences. In practice, for continuous observed variables, linear structural equation models have been used nearly exclusively. The use of models that are nonlinear in latent variables has been limited to simple situations with a linear measurement model and with a single polynomial structural relationship, usually containing only one quadratic or cross-product term. This paper introduces a general structural equation model with a nonlinear measurement model and a simultaneous system of nonlinear and non-polynomial structural relationships. For such a model, a parameterization useful for identification and interpretation is presented. For model fitting and parameter inferences, the maximum likelihood approach is considered. A method for obtaining parameter estimates and their asymptotic covariance matrix estimate is developed based on a new version of the Monte Carlo EM algorithm. The performance of the algorithm is examined using simulation studies.	latent variable;nonlinear system;structural equation modeling	Yan D. Zhao;Dewi Rahardja	2010	MASA	10.3233/MAS-2010-0153	latent class model;structural equation modeling;econometrics;mathematical optimization;expectation–maximization algorithm;errors-in-variables models;mathematics;latent variable model;statistics	Vision	28.36476887751369	-24.821617745307414	158783
b317c7e2a7a13cf494826c826d2d63ba3421a9f2	classification of power system disturbances using support vector machines	wavelet energy;support vector machines;wavelet decomposition;wavelet transform;power system;feature extraction;signal processing;power system disturbances;support vector machine;artificial neural network	This paper presents an effective method based on support vector machines (SVM) for identification of power system disturbances. Because of its advantages in signal processing applications, the wavelet transform (WT) is used to extract the distinctive features of the voltage signals. After the wavelet decomposition, the characteristic features of each disturbance waveforms are obtained. The wavelet energy criterion is also applied to wavelet detail coefficients to reduce the sizes of data set. After feature extraction stage SVM is used to classify the power system disturbance waveforms and the performance of SVM is compared with the artificial neural networks (ANN). 2009 Elsevier Ltd. All rights reserved.		Sami Ekici	2009	Expert Syst. Appl.	10.1016/j.eswa.2009.02.002	wavelet;support vector machine;speech recognition;second-generation wavelet transform;computer science;machine learning;signal processing;pattern recognition;cascade algorithm;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;artificial neural network	AI	36.58749456662498	-31.6906171399852	158840
4dac2be1b0501c1507f747a1a4e5f6bf37693a46	a concentrated, nonlinear information-theoretic estimator for the sample selection model	sample selection;inequalities;sample selection model;concentrated model;information theoretic;maximum entropy;information;priors	This paper develops a semi-parametric, Information-Theoretic method for estimating parameters for nonlinear data generated under a sample selection process. Considering the sample selection as a set of inequalities makes this model inherently nonlinear. This estimator (i) allows for a whole class of different priors, and (ii) is constructed as an unconstrained, concentrated model. This estimator is easy to apply and works well with small or complex data. We provide a number of explicit analytical examples for different priors’ structures and an empirical example.	estimation theory;feature selection;nonlinear programming;nonlinear system;semiconductor industry	Amos Golan;Henryk Gzyl	2010	Entropy	10.3390/e12061569	econometrics;mathematical optimization;prior probability;information;principle of maximum entropy;inequality;mathematics;statistics	ML	28.641459174486066	-29.341597190101037	158965
7c561e9ca10f1960622a3eca7cfa980f2243dd11	unsupervised classification and analysis of objects described by nonparametric probability distributions	dirichlet processes;em type algorithms;mixtures;classification;dirichlet distributions;random distributions;martingale convergence theorem	Various objects can be summarily described by probability distributions: groups of raw data, paths of stochastic processes, neighborhoods of an image pixel and so on. Dealing with nonparametric distributions, we propose a method for classifying such objects by estimating a finite mixture of Dirichlet distributions when the observed distributions are assumed to be outcomes of a finite mixture of Dirichlet processes. We prove the consistency of such a classification by using the mutual singularity of two distinct Dirichlet processes and the martingale convergence theorem. Moreover, this consistency allows us to use some standard data analysis and statistical methods for analyzing the class labels of these objects. © 2012 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 5: 388–398, 2012	data mining;john d. wiley;pixel;stochastic process;unsupervised learning	Richard Emilion	2012	Statistical Analysis and Data Mining	10.1002/sam.11160	dirichlet distribution;econometrics;concentration parameter;generalized dirichlet distribution;biological classification;statistical parameter;pattern recognition;doob's martingale convergence theorems;mathematics;mixture;statistics;hierarchical dirichlet process	ML	31.574377233380428	-25.206272444595616	159034
8d5eff8f7f081b58ca8801c918520fab8251aebb	a sequential design for estimating a nonlinear parametric function	metodo cuadrado menor;experimental design;approximation asymptotique;methode moindre carre;fully sequential design;asymptotically optimal fixed design;sequential design;optimal design statistics;least squares method;funcion no lineal;asymptotic optimality;loi probabilite;ley probabilidad;linear regression model;simulation;modele lineaire;plan experiencia;linear regression;simple linear regression model;non linear function;simulacion;regression model;informacion fisher;modelo lineal;fisher information matrix;asymptotic behavior;comportement asymptotique;second order approximation;optimalite asymptotique;comportamiento asintotico;modelo regresion;matrice information fisher;plan experience;modele regression;probability distribution;linear model;regresion lineal;estimacion parametro;fonction non lineaire;least squares estimate;plan optimal;simulation study;asymptotic approximation;least squares estimator;parameter estimation;estimation parametre;information fisher;regression lineaire;fisher information;aproximacion asintotica;plan sequentiel;plan secuencial	A fully-sequential design for estimating a nonlinear function of the parameters in the simple linear regression model is proposed and its asymptotic behavior is investigated both theoretically and by simulation. The design requires that the observations be taken at x = ±1 and specifies whether the next observation be taken x = −1 or 1. It is shown that, under this design, the mean number of observations taken at x = 1, mk, converges with probability one to an optimal value as k → ∞, where k denotes the total number of design points. The simulation study indicates that mk converges in L 2 to the optimal value with the order of o(k−2).	nonlinear system;optimization problem;simulation;stellar classification	Kamel Rekab;Mohamed Tahir	2003	Applied Mathematics and Computation	10.1016/S0096-3003(02)00113-3	econometrics;asymptotic analysis;linear regression;fisher information;calculus;mathematics;least squares;statistics	ML	33.39074734849409	-24.44124934362738	159225
e49fc57dab995ba514e1020ffec01968707d5c71	scaling submodular maximization via pruned submodularity graphs		We propose a new random pruning method (called “submodular sparsification (SS)”) to reduce the cost of submodular maximization. The pruning is applied via a “submodularity graph” over the n ground elements, where each directed edge is associated with a pairwise dependency defined by the submodular function. In each step, SS prunes a 1− 1/ √ c (for c > 1) fraction of the nodes using weights on edges computed based on only a small number (O(log n)) of randomly sampled nodes. The algorithm requires logc n steps with a small and highly parallelizable per-step computation. An accuracy-speed tradeoff parameter c, set as c = 8, leads to a fast shrink rate √ 2/4 and small iteration complexity log22 n. Analysis shows that w.h.p., the greedy algorithm on the pruned set of size O(log n) can achieve a guarantee similar to that of processing the original dataset. In news and video summarization tasks, SS is able to substantially reduce both computational costs and memory usage, while maintaining (or even slightly exceeding) the quality of the original (and much more costly) greedy algorithm.	computation;directed graph;expectation–maximization algorithm;fast fourier transform;iteration;logp machine;merge sort;randomized algorithm;randomness;submodular set function	Tianyi Zhou;Hua Ouyang;Yi Chang;Jeff A. Bilmes;Carlos Guestrin	2017			mathematical optimization;combinatorics;machine learning;mathematics	ML	25.328909653146138	-28.867124919652053	159349
67e504bc5daea4eca9ca1d5043f467e590568658	bayesian fusion of empirical distributions based on local density reconstruction	bayes methods;nearest neighbor operations empirical distributions local density reconstruction random vectors continuous probability density functions bayes law direct bayesian fusion;bayes methods approximation methods indexes standards kernel estimation probability density function	Fusing two random vectors is simple, when they are characterized by continuous probability density functions. According to Bayes' law, fusion then consists of multiplying the two densities. When only empirical distributions are given and a resulting empirical distribution is desired, Bayes' law is no longer applicable. Obviously, fusion could now be performed by reconstructing the underlying continuous densities, subsequent multiplication, and sampling of the result. As this is overly complicated, our goal is to perform a direct Bayesian fusion of the two given empirical distributions.We devise a generalized multiplication procedure that mutually reweights appropriate points of one density by local density values of the other density. The density values are efficiently estimated locally by nearest neighbor operations. The method is symmetric in the sense that it uses points from both densities.	bayesian network;sampling (signal processing)	Uwe D. Hanebeck	2015	2015 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)	10.1109/MFI.2015.7295821	econometrics;density estimation;pattern recognition;mathematics;bayes' theorem;statistics	Robotics	31.45322875705681	-29.30341158009976	159365
e38f172b3805cf608b5e5cac7d117ab99b0081e7	inferring sparse graphs from smooth signals with theoretical guarantees		We consider the problem of inferring a graph from signals which are assumed to be smooth over the graph, in the setting where the graph is also assumed to be sparse. We focus on the case where measurements are Gaussian vectors and the graph topology is encoded in the inverse of the covariance matrix. In addition, the weights of the inverse covariance are assumed to be such that the model is attractive—all partial correlations are non-negative. Unlike other approaches which seek to minimize the Laplacian quadratic form or involve solving a log-det program, we study a simple estimator based on soft thresholding. The estimator involves computing only a single eigenvalue decomposition, and so it can easily scale to networks with thousands of vertices. We provide theoretical results on the reconstruction error as a function of the number of observations and problem dimensions for the case where the underlying graph is assumed to be sparse.	directed graph;sparse matrix;thresholding (image processing);topological graph theory	Michael G. Rabbat	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953415	adjacency matrix;mathematical optimization;estimation of covariance matrices;spectral graph theory;sparse matrix;topological graph theory;covariance;graph bandwidth;level structure;mathematics	ML	28.62256789201967	-36.47507904990961	159589
8489e0b98e95cbe50aadc436dd2aa4aaef99c845	offline to online conversion	probability;combinatorics;sequential;time consistency;good turing;tractable;laplace;offline;conference paper;estimation;batch;normalization;bayes;ristad;prediction;online;regret	We consider the problem of converting offline estimators into an online predictor or estimator with small extra regret. Formally this is the problem of merging a collection of probability measures over strings of length 1,2,3,... into a single probability measure over infinite sequences. We describe various approaches and their pros and cons on various examples. As a side-result we give an elementary non-heuristic purely combinatoric derivation of Turing’s famous estimator. Our main technical contribution is to determine the computational complexity of online estimators with good guarantees in general.	computational complexity theory;heuristic;kerrison predictor;online and offline;regret (decision theory);turing	Marcus Hutter	2014		10.1007/978-3-319-11662-4_17	econometrics;estimation;prediction;computer science;machine learning;normalization;probability;mathematics;batch file;bayes' theorem;time consistency;regret;laplace transform;statistics	Theory	27.029351867950112	-27.0995137256409	159729
c26016dd1bce7048ea511413e4e01a19116ca0d7	representation and generalization in autonomous reinforcement learning		This PhD thesis investigates the role of representations that generalize values in autonomous reinforcement learning (RL). I employ the mathematical framework of function analysis to examine inductive and deductive RL approaches in continuous (or hybrid) state and action spaces. My analysis reveals the importance of the representationŠs metric for inductive generalization and the need for structural assumptions to generalize values deductively to entirely new situations. The thesis contributes to two related Ąelds of research: representation learning and deductive value estimation in large state-action spaces. I emphasize here the agentŠs autonomy by demanding little to no knowledge about (or restrictions to) possible tasks and environments. In the following I summarize my contributions in more detail. I argue that all isomorphic state spaces (and fully observable observation spaces) difer only in their metric (Böhmer et al., 2015), and show that values can be generalized optimally by a difusion metric. I prove that when the value is estimated by a linear algorithm like least squares policy iteration (LSPI), slow feature analysis (SFA) approximates an optimal representation for all tasks in the same environment (Böhmer et al., 2013). I demonstrate this claim by deriving a novel regularized sparse kernel SFA algorithm (RSK-SFA, Böhmer et al., 2012) and compare the learned representations with others, for example, in a real LSPI robot-navigation task and in extensive simulations thereof. I also extend the deĄnition of SFA to γ-SFA, which represents only a speciĄed subset of ŞanticipatedŤ tasks. Autonomous inductive learning sufers the curse of insufficient samples in many realistic tasks, as environments consisting of many variables can have exponentially many states. This precludes inductive representation learning and both inductive and deductive value estimation for these environments, all of which need training samples suiciently ŞcloseŤ to every state. I propose structural assumptions on the state dynamics to break the curse. I investigate state spaces with sparse conditional independent transitions, called Bayesian dynamic networks (DBN). In diference to value functions, sparse DBN transition models can be learned inductively without sufering the above curse. To this end, I deĄne the new class of linear factored functions (LFF, Böhmer and Obermayer, 2015), which can compute the operations in a DBN, marginalization and point-wise multiplication, for an entire function analytically. I derive compression algorithms to keep LFF compact and three the inductive LFF algorithms density estimation, regression and value estimation. As inductive value estimation sufers the curse of insuicient samples, I derive a deductive variant of LSPI (FAPI, Böhmer and Obermayer, 2013). Like LSPI, FAPI requires predeĄned basis functions and can thus not estimate values autonomously in large state-action spaces. I develop therefore a second algorithm to estimate values deductively for a DBN (represented by LFF, e.g., learned by LFF regression) directly in the function space of LFF. As most environments can not be perfectly modeled by DBN, I discuss an importance sampling technique to combine inductive and deductive value estimation. Deduction can be furthermore improved by mixture-of-expert DBN, where a set of conditions determines for each state which expert describes the dynamics best. The conditions can be constructed as LFF from grounded relational rules. This allows to generate a transition model for each conĄguration of the environment. Ultimately, the framework derived in this thesis could generalize inductively learned models to other environments with similar objects.	algorithm;autonomous robot;basis function;bayesian network;data compression;feature learning;importance sampling;iteration;least squares;markov decision process;natural deduction;observable;reinforcement learning;robotic mapping;sampling (signal processing);simulation;sparse matrix	Wendelin Böhmer	2017			reinforcement learning;machine learning;mathematics;artificial intelligence	AI	29.626470779540323	-33.39600199864193	159792
82986128f7ef3e4fe0f845dca163e2d40b597c07	a generative model and a generalized trust region newton method for noise reduction	ridge;principal manifold;density estimation;trust region;noise reduction;generative model;newton method	In practical applications related to, for instance, machine learning, data mining and pattern recognition, one is commonly dealing with noisy data lying near some lowdimensional manifold. A well-established tool for extracting the intrinsically low-dimensional structure from such data is principal component analysis (PCA). Due to the inherent limitations of this linear method, its extensions to extraction of nonlinear structures have attracted increasing research interest in recent years. Assuming a generative model for noisy data, we develop a probabilistic approach for separating the data-generating nonlinear functions from noise. We demonstrate that ridges of the marginal density induced by the model are viable estimators for the generating functions. For projecting a given point onto a ridge of its estimated marginal density, we develop a generalized trust region Newton method and prove its convergence to a ridge point. Accuracy of the model and computational efficiency of the projection method are assessed via numerical experiments where we utilize Gaussian kernels for nonparametric estimation of the underlying densities of the test datasets.	computation;data mining;experiment;generative model;machine learning;marginal model;newton's method;noise reduction;nonlinear system;numerical analysis;pattern recognition;principal component analysis;signal-to-noise ratio;trust region	Seppo Pulkkinen;Marko M. Mäkelä;Napsu Karmitsa	2014	Comp. Opt. and Appl.	10.1007/s10589-013-9581-4	econometrics;mathematical optimization;ridge;density estimation;machine learning;noise reduction;mathematics;trust region;newton's method;generative model;statistics	ML	27.192601761309938	-33.789669771311885	160190
8f18ca8abe79865db1a14df4b0fb0b87f7c397c5	lasso penalized model selection criteria for high-dimensional multivariate linear regression analysis	model selection;primary;secondary;high dimensional data;consistency;multivariate linear regression	This paper proposes two model selection criteria for identifying relevant predictors in the high-dimensional multivariate linear regression analysis. The proposed criteria are based on a Lasso type penalized likelihood function to allow the high-dimensionality. Under the asymptotic framework that the dimension of multiple responses goes to infinity while the maximum size of candidate models has smaller order of the sample size, it is shown that the proposed criteria have the model selection consistency, that is, they can asymptotically pick out the true model. Simulation studies show that the proposed criteria outperform existing criteria when the dimension of multiple responses is large.	general linear model;lasso;model selection	Shota Katayama;Shinpei Imori	2014	J. Multivariate Analysis	10.1016/j.jmva.2014.08.002	econometrics;bayesian multivariate linear regression;mathematics;consistency;model selection;statistics;general linear model;clustering high-dimensional data	ML	29.359423099678573	-25.244030059573735	160252
4ad1c5eb78be848a8fd2b1b0dade6122bcbb57f8	bayesian inference for structured spike and slab priors		Sparse signal recovery addresses the problem of solving underdetermined linear inverse problems subject to a sparsity constraint. We propose a novel prior formulation, the structured spike and slab prior, which allows to incorporate a priori knowledge of the sparsity pattern by imposing a spatial Gaussian process on the spike and slab probabilities. Thus, prior information on the structure of the sparsity pattern can be encoded using generic covariance functions. Furthermore, we provide a Bayesian inference scheme for the proposed model based on the expectation propagation framework. Using numerical experiments on synthetic data, we demonstrate the benefits of the model.	algorithm;detection theory;expectation propagation;experiment;gaussian process;microsoft outlook for mac;numerical analysis;slab allocation;software propagation;sparse matrix;synthetic data;the spike (1997)	Michael Riis Andersen;Ole Winther;Lars Kai Hansen	2014			mathematical optimization;machine learning;pattern recognition;mathematics;statistics	ML	29.529499637832636	-32.99953865073022	160537
364769f6bc17aac0408c8e486fffcab8ea2e48f4	sparse linear regression with beta process priors	linear systems;belief networks;correlated feature problem;compressed sensing;beta process priors;linear regression vectors bayesian methods linear systems polynomials approximation error predictive models gene expression;linear regression;bayesian methods;bayesian approximation;inference mechanisms;linear system;approximation theory;compressive sensing problem;vectors;compressive sensing;bayesian nonparametrics;regression analysis approximation theory belief networks inference mechanisms;nonsparse weight vector;sparse linear regression;regression analysis;approximation methods;bp lr model;correlated feature problem sparse linear regression beta process priors bayesian approximation linear system bp lr model nonsparse weight vector sparse binary vector compressive sensing problem;beta process;compressive sensing bayesian nonparametrics beta process sparse linear regression;hierarchical model;sparse binary vector;noise	A Bayesian approximation to finding the minimum ℓ0 norm solution for an underdetermined linear system is proposed that is based on the beta process prior. The beta process linear regression (BP-LR) model finds sparse solutions to the underdetermined model y = Φx + ɛ, by modeling the vector x as an element-wise product of a non-sparse weight vector, w, and a sparse binary vector, z, that is drawn from the beta process prior. The hierarchical model is fully conjugate and therefore is amenable to fast inference methods. We demonstrate the model on a compressive sensing problem and on a correlated-feature problem, where we show the ability of the BP-LR to selectively remove the irrelevant features, while preserving the relevant groups of correlated features.	approximation;bit array;compressed sensing;hierarchical database model;lr parser;linear system;relevance;sparse matrix	Bo Chen;John William Paisley;Lawrence Carin	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495400	computer science;machine learning;pattern recognition;sparse approximation;mathematics;linear system;compressed sensing;statistics	Robotics	28.6009137647086	-33.662136683804704	160562
2e38420e8bbf0cce3950ede3833f936f83beebc9	structure learning of mixed graphical models		We consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. We present a new pairwise model for graphical models with both continuous and discrete variables that is amenable to structure learning. In previous work, authors have considered structure learning of Gaussian graphical models and structure learning of discrete models. Our approach is a natural generalization of these two lines of work to the mixed case. The penalization scheme is new and follows naturally from a particular parametrization of the model.	graphical model;penalty method	Jason D. Lee;Trevor J. Hastie	2013			combinatorics;machine learning;mathematics;graphical model;statistics	ML	25.717706004662574	-29.786584430923153	160589
00c38c340aa123096873cee1db9f35d0f5dad0ec	bayesian models for large-scale hierarchical classification		A challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for improving classification performance. An even greater challenge is to do so in a manner that is computationally feasible for large scale problems. This paper proposes a set of Bayesian methods to model hierarchical dependencies among class labels using multivariate logistic regression. Specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parameters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters. We present variational algorithms for tractable posterior inference in these models, and provide a parallel implementation that can comfortably handle largescale problems with hundreds of thousands of dimensions and tens of thousands of classes. We run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach and shows improved performance over the other state-of-the-art hierarchical methods.	algorithm;benchmark (computing);calculus of variations;cobham's thesis;discriminative model;hierarchical clustering;ibm notes;internet information services;logistic regression;scalability	Siddharth Gopal;Yiming Yang;Bing Bai;Alexandru Niculescu-Mizil	2012			computer science;data science;machine learning;data mining;statistics	ML	26.28939148615125	-32.56745016074089	160630
b344a97b9293941cb3c689c8c70e2794c2f3e908	a refined composite multivariate multiscale fuzzy entropy and laplacian score-based fault diagnosis method for rolling bearings		The vibration signals of rolling bearings are often nonlinear and non-stationary. Multiscale entropy (MSE) has been widely applied to measure the complexity of nonlinear mechanical vibration signals, however, at present only the single channel vibration signals are used for fault diagnosis by many scholars. In this paper multiscale entropy in multivariate framework, i.e., multivariate multiscale entropy (MMSE) is introduced to machinery fault diagnosis to improve the efficiency of fault identification as much as possible through using multi-channel vibration information. MMSE evaluates the multivariate complexity of synchronous multi-channel data and is an effective method for measuring complexity and mutual nonlinear dynamic relationship, but its statistical stability is poor. Refined composite multivariate multiscale fuzzy entropy (RCMMFE) was developed to overcome the problems existing in MMSE and was compared with MSE, multiscale fuzzy entropy, MMSE and multivariate multiscale fuzzy entropy by analyzing simulation data. Finally, a new fault diagnosis method for rolling bearing was proposed based on RCMMFE for fault feature extraction, Laplacian score and particle swarm optimization support vector machine (PSO-SVM) for automatic fault mode identification. The proposed method was compared with the existing methods by analyzing experimental data analysis and the results indicate its effectiveness and superiority.	analysis of algorithms;effective method;electromyography;feature extraction;feature selection;mathematical optimization;nonlinear system;particle swarm optimization;rolling hash;signal processing;simulation;stationary process;support vector machine;synthetic intelligence;time series	Jinde Zheng;Deyu Tu;Haiyang Pan;Xiaolei Hu;Tao Liu;Qingyun Liu	2017	Entropy	10.3390/e19110585	mathematical optimization;fuzzy logic;mathematics;experimental data;support vector machine;feature selection;feature extraction;multivariate statistics;effective method;particle swarm optimization;artificial intelligence;pattern recognition	AI	37.46252115145864	-30.029494692390312	161355
e8551f979554b131f444b565157b7c656e4b3236	sparse bayesian linear regression using generalized normal priors		A sparse Bayesian linear regression model is proposed that generalizes the Bayesian Lasso to a class of Bayesian models with scale mixtures of normal distributions as priors for the regression coefficients. We assume a hierarchical Bayesian model with a binary indicator for whether a predictor variable is included in the model, a generalized normal prior distribution for the coefficients of the included variables, and a Student-t error model for robustness to heavy tailed noise. Our model out-performs other popular sparse regression estimators on synthetic and real data.	sparse	Hai Zhang;Puyu Wang;Qing Dong;Pu Wang	2017	IJWMIP	10.1142/S0219691317500217	bayesian multivariate linear regression;pattern recognition;generalized linear model;bayesian linear regression	AI	29.83581023359629	-24.96429012127276	161405
353e4c3d4119b45db1d8977193296aecbe50f3e4	beyond maximum likelihood: boosting the chow-liu algorithm for large alphabets	machine learning algorithms;complexity theory;maximum likelihood estimation;graphical models;mutual information;entropy	We show that in high dimensional distributions, i.e., the regime where the alphabet size of each node is comparable to the number of observations, the Chow-Liu algorithm on learning graphical models is highly sub-optimal. We propose a new approach, where the key ingredient is to replace the empirical mutual information in the Chow-Liu algorithm with a minimax rate-optimal estimator proposed recently by Jiao, Venkat, Han, and Weissman [1]. We demonstrate the improved performance of the new approach in two problems: learning tree graphical models and Bayesian network classification.	algorithm;bayesian network;boosting (machine learning);chow–liu tree;graphical model;han unification;minimax;mutual information;regular expression	Jiantao Jiao;Yanjun Han;Tsachy Weissman	2016	2016 50th Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2016.7869051	machine learning;pattern recognition;mathematics;statistics	ML	25.235502109538412	-29.54888581802116	161480
38cbbf43bb66acec2588a6ec8edbd1efeb32e16b	dictionary learning for bioacoustics monitoring with applications to species classification	dictionary learning;random matrix projection;classification	This paper deals with the application of the convolutive version of dictionary learning to analyze in-situ audio recordings for bio-acoustics monitoring. We propose an efficient approach for learning and using a sparse convolutive model to represent a collection of spectrograms. In this approach, we identify repeated bioacoustics patterns, e.g., bird syllables, as words and represent new spectrograms using these words. Moreover, we propose a supervised dictionary learning approach in the multiple-label setting to support multi-label classification of unlabeled spectrograms. Our approach relies on a random projection for reduced computational complexity. As a consequence, the non-negativity requirement on the dictionary words is relaxed. Furthermore, the proposed approach is well-suited for a collection of discontinuous spectrograms. We evaluate our approach on synthetic examples and on two real datasets consisting of multiple birds audio recordings. Bird syllable dictionary learning from a real-world dataset is demonstrated. Additionally, we successfully apply the approach to spectrogram denoising and species classification.	dictionary;machine learning	José Francisco Ruiz-Muñoz;Zeyu You;Raviv Raich;Xiaoli Z. Fern	2018	Signal Processing Systems	10.1007/s11265-016-1155-0	machine learning;artificial intelligence;computational complexity theory;k-svd;bioacoustics;syllable;computer science;spectrogram;pattern recognition;random projection	ML	31.41313258138616	-37.581042908023015	161540
a22f5b4d426694b3a5eb6d1f1788b15cb2180fcf	application of complexity and approximate entropy on fault diagnoses	fault signal analysis;complexity theory entropy time series analysis fault diagnosis feature extraction machinery vibrations;complexity theory;vibrations;vibration signals;complexity;failure analysis;approximate entropy;quantitative description;condition monitoring;time series analysis;feature extraction;rotating machinery;vibrations condition monitoring entropy failure analysis fault diagnosis feature extraction turbomachinery;nonlinear dynamics;fault information;rotating mechanical system;vibration signals entropy approximation fault diagnosis nonlinear dynamic characteristics rotating mechanical system complexity feature extraction fault signal analysis rotating machinery fault information fault feature recognition;entropy;entropy approximation;fault feature recognition;machinery;mechanical systems;fault diagnosis complexity approximate entropy quantitative description;nonlinear dynamic characteristics;fault diagnosis;turbomachinery	In connection with the nonlinear dynamic characteristics shown from the performance of fault rotating mechanical system, based on the research and analysis, complexity and approximate entropy can be used to characterize the system state of motion and non-degree rule. The authors propose to apply complexity and approximate entropy to the feature extraction of fault signal. From the analysis and calculation on simulation of different fault signals, it shows that under different rotating machinery fault conditions, its complexity and approximate entropy are significantly different, which verifies that the two quantities are effective parameters for fault information and they are excellent parameters in terms of extraction and recognition of fault feature. Studies have shown that, the complexity and the approximate entropy value can reflect the nonlinearity of the system. If combine these two parameters, it will be more conducive to recognize and analyze fault signal recognition, enhance the reliability, and thus to study the fault diagnosis of complexity rotating machinery in a more effective way.	approximate entropy;approximation algorithm;complexity;feature extraction;nonlinear system;simulation	Bingcheng Wang;Zhaohui Ren	2010	2010 Sixth International Conference on Natural Computation	10.1109/ICNC.2010.5583865	control engineering;computer science;stuck-at fault;machine learning;control theory;fault model;sample entropy	Robotics	37.255681967630416	-31.10593337192701	161544
24c7f652acd8aeb1e67ce71adbb06f3529a0dfb4	maximum entropy density estimation with generalized regularization and an application to species distribution modeling	species distribution model;performance guarantee;generalized maximum entropy;sample size;information geometry;density estimation;complete convergence;conservation biology;potential function;maximum entropy	We present a unified and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including `1, `2, `2, and `1+ ` 2 2 style regularization. We propose an algorithm solving a large and general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and unifies techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models.	algorithm;boosting (machine learning);bregman divergence;ecology;experiment;fits;feature selection;feature vector;image scaling;information geometry;iterative method;kullback–leibler divergence;logistic regression;machine learning;markov chain;markov random field;matrix regularization;mixture model;overfitting;principle of maximum entropy;test data;time complexity	Miroslav Dudík;Steven J. Phillips;Robert E. Schapire	2007	Journal of Machine Learning Research		sample size determination;mathematical optimization;maximum-entropy markov model;joint entropy;combinatorics;density estimation;binary entropy function;rényi entropy;transfer entropy;entropy maximization;maximum entropy probability distribution;principle of maximum entropy;mathematics;maximum entropy thermodynamics;joint quantum entropy;conservation biology;maximum entropy spectral estimation;entropy rate;information geometry;statistics	ML	27.68664767641469	-30.409767157273297	161664
02b8ee23d604da1271d6af0cbd08c8c0db2098bb	dimensionality reduction for supervised learning with reproducing kernel hilbert spaces	learning;supervised learning;hilbert space;contrast;mathematical models;reproducing kernel hilbert space;statistical inference;parametric analysis;dimensional reduction;functions	We propose a novel method of dimensionality reduction for supervised learning problems. Given a regression or classification problem in which we wish to predict a response variable Y from an explanatory variable X, we treat the problem of dimensionality reduction as that of finding a low-dimensional “effective subspace” for X which retains the statistical relationship between X and Y . We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem we establish a general nonparametric characterization of conditional independence using covariance operators on reproducing kernel Hilbert spaces. This characterization allows us to derive a contrast function for estimation of the effective subspace. Unlike many conventional methods for dimensionality reduction in supervised learning, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y . We present experiments that compare the performance of the method with conventional methods.	curse of dimensionality;dimensionality reduction;experiment;hilbert space;marginal model;mathematical optimization;optimization problem;parametric model;spaces;supervised learning	Kenji Fukumizu;Francis R. Bach;Michael I. Jordan	2004	Journal of Machine Learning Research		semi-supervised learning;mathematical optimization;statistical inference;kernel embedding of distributions;contrast;machine learning;pattern recognition;reproducing kernel hilbert space;mathematical model;mathematics;supervised learning;parametric statistics;function;statistics;representer theorem;hilbert space	ML	26.995679528036316	-33.306259351123614	161704
4528208945382b330ba9fc9b2ea84837ac6b7fc6	automatic functional harmonic analysis		Music scholars have been studying tonal harmony intensively for centuries, yielding numerous theories and models. Unfortunately, a large number of these theories are formulated in a rather informal fashion and lack mathematical precision. In this article we present HarmTrace, a functional model of Western tonal harmony that builds on well-known theories of tonal harmony. In contrast to other approaches that remain purely theoretical, we present an implemented system that is evaluated empirically. Given a sequence of symbolic chord labels, HarmTrace automatically derives the harmonic relations between chords. For this, we use advanced functional programming techniques that are uniquely available in the Haskell programming language. We show that our system is fast, easy to modify and maintain, robust against noisy data, and that its harmonic analyses comply with Western tonal harmony theory.	function model;functional programming;haskell;programming language;signal-to-noise ratio;theory	W. Bas de Haas;José Pedro Magalhães;Frans Wiering;Remco C. Veltkamp	2013	Computer Music Journal	10.1162/COMJ_a_00209	computer science;artificial intelligence;algorithm	ML	25.854597385717597	-25.30511290988602	161737
f50547fb5a3d11268088bfbfc451b5f43bd825f0	on the computational rationale for generative models	top down method;methode descendante;modelizacion;graphic method;vision ordenador;representation graphique;generic model;complexite calcul;computer vision;generate and test;modelisation;complejidad computacion;top down processing;methode graphique;computational complexity;metodo descendente;graphical representation;graphical model;grafo curva;metodo grafico;generative model;vision ordinateur;basis pursuit;modeling;sparse coding;discriminative model;graphics	Generative and discriminative models are best defined by the structure of their graphical representation. This paper introduces such a definition and uses it to argue that, in some practical cases, generative models need to be formulated in order to be implemented within generate-and-test algorithms. This argument is inspired mainly by the ideas of the late Donald MacKay and by considerations of computational complexity. 2006 Elsevier Inc. All rights reserved.	algorithm;computation;computational complexity theory;design rationale;discriminative model;generative model;graphical user interface	Arthur E. C. Pece	2007	Computer Vision and Image Understanding	10.1016/j.cviu.2006.10.002	systems modeling;basis pursuit;computer science;graphics;artificial intelligence;machine learning;top-down and bottom-up design;graphical model;neural coding;generative model;computational complexity theory;algorithm;discriminative model;generative design	AI	33.11278648964762	-35.934441525909236	162000
6869f6d666017f69e1a2576c1b0bfd325467ebca	feature extraction for the prognosis of electromechanical faults in electrical machines through the dwt	discrete wavelet transform;broken bars;electrical machine;wavelet transform;eccentricities;feature extraction;fault diagnosis;steady state;electric machines	Recognition of characteristic patterns is proposed in this paper in order to diagnose the presence of electromechanical faults in induction electrical machines. Two common faults in this type of machines are considered; broken rotor bars and mixed eccentricities. The presence of these faults leads to the appearance of frequency components following a very characteristic evolution during the startup transient and other transients through which the machine operates. The identification and extraction of these characteristic patterns through the Discrete Wavelet Transform (DWT) has been proven to be a reliable methodology for diagnosing the presence of these faults, showing certain advantages in comparison with the classical FFT analysis of the steady-state current. In the paper, a compilation of healthy and faulty cases are presented; they confirm the validity of the approach for the correct diagnosis of a wide range of electromechanical faults. Keywords—electric machines, fault diagnosis, wavelet tramsform, broken bars, eccentricties	algorithm;automatic identification and data capture;compiler;computer vision;discrete wavelet transform;fast fourier transform;feature extraction;r.o.t.o.r.;software bug;steady state	Jos&#x00E9; Antonino-Daviu;Martin Riera-Guasp;Manuel Pineda-Sánchez;Joan Pons-Llinares;Ruben Puche-Panadero;Juan Pérez-Cruz	2009	Int. J. Comput. Intell. Syst.	10.1080/18756891.2009.9727651	real-time computing;feature extraction;computer science;machine learning;discrete wavelet transform;steady state;wavelet transform	AI	37.48007660924277	-30.827256985966432	162128
05ad279fafb2ef9f8a32986a4cba9f40eeaedec2	isam2: incremental smoothing and mapping with fluid relinearization and incremental variable reordering	slam isam2 fluid relinearization incremental variable reordering graph based version incremental smoothing and mapping graphical model based interpretation sparse matrix factorization methods bayes tree data structure square root information matrix bayes tree mobile robots;sparse matrix factorization methods;bayes tree;smoothing method;bayes methods;slam;incremental variable reordering;mobile robots;tree data structures;bayes tree data structure;accuracy;isam2;smoothing methods;graphical models;trajectory;fluid relinearization;simultaneous localization and mapping;graphical model;simultaneous localization and mapping graphical models smoothing methods sparse matrices accuracy trajectory;post print;graph based version;square root information matrix;tree data structures bayes methods mobile robots slam robots sparse matrices;graphical model based interpretation;incremental smoothing and mapping;slam robots;sparse matrices	We present iSAM2, a fully incremental, graph-based version of incremental smoothing and mapping (iSAM). iSAM2 is based on a novel graphical model-based interpretation of incremental sparse matrix factorization methods, afforded by the recently introduced Bayes tree data structure. The original iSAM algorithm incrementally maintains the square root information matrix by applying matrix factorization updates. We analyze the matrix updates as simple editing operations on the Bayes tree and the conditional densities represented by its cliques. Based on that insight, we present a new method to incrementally change the variable ordering which has a large effect on efficiency. The efficiency and accuracy of the new method is based on fluid relinearization, the concept of selectively relinearizing variables as needed. This allows us to obtain a fully incremental algorithm without any need for periodic batch steps. We analyze the properties of the resulting algorithm in detail, and show on various real and simulated datasets that the iSAM2 algorithm compares favorably with other recent mapping algorithms in both quality and efficiency.	algorithm;central processing unit;data structure;dynamic programming;exploit (computer security);graphical model;marginal model;parallel computing;recursion;simultaneous localization and mapping;smoothing;sparse matrix;the matrix;tree (data structure);vii;web mapping	Michael Kaess;Hordur Johannsson;Richard Roberts;Viorela Ila;John J. Leonard;Frank Dellaert	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5979641	mathematical optimization;computer science;machine learning;pattern recognition;graphical model;statistics	Robotics	29.9260938342254	-30.272203760554007	162202
b5cd687bcd914d48a298440f0670fc91bb797a08	underwater acoustic targets classification using welch spectrum estimation and neural networks	target classification;operant conditioning;dynamic program;spectrum;frequency spectrum;feature vector;power spectral density;k nearest neighbor;feature selection;underwater acoustics;neural network	  In this paper we analyze the frequency spectrum signature of passive sonar target radiated noise, propose a kind of method  to extract the object feature based on Welch power spectral density estimation. Then we extract the feature of thousands of  stylebooks in many operating conditions and present a dynamic programming method for feature selection in order to get better  feature vectors. Finally, the classification experiment for these noise stylebooks is done using the BP Neural Network (BPNN)  and the K-Nearest Neighbor (KNN) methods. Results of experiment show that the methods are practical and effective. The method  can be applied to the underwater acoustic targets classification.    	acoustic cryptanalysis;neural networks;spectral density estimation;welch's method	Chunyu Kang;Xinhua Zhang;Anqing Zhang;Hongwen Lin	2004		10.1007/978-3-540-28647-9_153	spectrum;frequency spectrum;underwater acoustics;speech recognition;feature vector;computer science;machine learning;operant conditioning;pattern recognition;spectral density;feature selection;k-nearest neighbors algorithm;artificial neural network	ML	37.2850408416955	-32.55564568797184	162255
87f285ad483d955e771606a52c49ab7ce9298b06	prior selection for panel vector autoregressions	forecasting;hg finance;qa mathematics;diffusion processes;dynamic treatment effect models;hb economic theory;c32 time series models;c52 model evaluation validation and selection;shrinkage;c11 bayesian analysis general;bayesian model selection;spike and slab priors;large vector autoregression;dynamic quantile regressions;spatio temporal models;c33 panel data models;working paper;ha statistics;state space models	Bayesian shrinkage priors have been very popular in estimating vector autoregressions (VARs) of possibly large dimensions. Many of these priors are not appropriate for multi-country settings, as they cannot account for the type of restrictions typically met in panel vector autoregressions (PVARs). With this in mind, new parametric and semi-parametric priors for PVARs are proposed, which perform valuable shrinkage in large dimensions and also allow for soft clustering of variables or countries which are homogeneous. The implication of these new priors for modelling interdependencies and heterogeneities among di¤erent countries in a panel VAR setting, is discussed. Monte Carlo evidence and an empirical forecasting exercise show clear and important gains from the new priors compared to existing popular priors for VARs and PVARs.		Dimitris Korobilis	2016	Computational Statistics & Data Analysis	10.1016/j.csda.2016.02.011	econometrics;forecasting;pattern recognition;mathematics;shrinkage;statistics	ML	28.542256812096994	-24.144100669628788	162625
b8d150fac3417ace89d5c4679a87c5bae3c1da6b	scalable log determinants for gaussian process kernel learning		For applications as varied as Bayesian neural networks, determinantal point processes, elliptical graphical models, and kernel learning for Gaussian processes (GPs), one must compute a log determinant of an n× n positive definite matrix, and its derivatives – leading to prohibitive O(n) computations. We propose novel O(n) approaches to estimating these quantities from only fast matrix vector multiplications (MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and surrogate models, and converge quickly even for kernel matrices that have challenging spectra. We leverage these approximations to develop a scalable Gaussian process approach to kernel learning. We find that Lanczos is generally superior to Chebyshev for kernel learning, and that a surrogate approach can be highly efficient and accurate with popular kernels.	approximation;artificial neural network;bayesian network;computation;converge;gaussian process;graphical model;kernel (operating system);lanczos resampling;scalability;surrogate model	Kun Dong;David Eriksson;Hannes Nickisch;David Bindel;Andrew Gordon Wilson	2017			kernel embedding of distributions;kernel principal component analysis;mathematical optimization;artificial intelligence;machine learning;polynomial kernel;kernel method;lanczos resampling;gaussian process;mathematics;radial basis function kernel;variable kernel density estimation	ML	26.981819970377092	-31.266450441172513	162846
ca4d922e88f1add63f60fafa0019631acb76313e	sparse mixture conditional density estimation by superficial regularization	belief networks;nonlinear filters;kernel;gaussian mixture;closed form solution;approximation method;uncertainty;bayesian inference;sparse mixture conditional density estimation;estimation optimization density functional theory kernel probabilistic logic approximation methods uncertainty;nonlinear filter;nonlinear filters belief networks iterative methods;gaussian mixture density;local data distribution;estimation algorithm;data distribution;regularization;iterative two step optimization scheme;iterative two step optimization scheme sparse mixture conditional density estimation superficial regularization continuous random variables noisy samples heteroscedastic gaussian mixture densities closed form solution bayesian inference model uncertainty local data distribution;iterative methods;density functional theory;density estimation;estimation;model uncertainty;density function theory;conditional density estimation;random variable;superficial regularization;continuous random variables;optimization;approximation methods;noisy samples;probabilistic logic;heteroscedastic gaussian mixture densities;nonlinear filtering;regularization conditional density estimation nonlinear filtering gaussian mixture density;regularity condition	In this paper, the estimation of conditional densities of continuous random variables from noisy samples is considered. The conditional densities are modeled as heteroscedastic Gaussian mixture densities allowing for closed-form solution of Bayesian inference with full densities. The key idea is a regularization based on the curvature of the conditional density function's surface. The main contributions are the introduction of a superficial regularizer, the consideration of model uncertainty relative to the local data distribution by means of adaptive covariances, and an efficient distance-based estimation algorithm leading to an improved generalization quality of the estimates. The proposed algorithm is an iterative two-step optimization scheme for hyperparameters and the components' parameters. The obtained solutions are sparse, smooth, and generalize well as experiments, e.g., in nonlinear filtering, show.	additive white gaussian noise;approximation algorithm;collaborative filtering;computation;experiment;generative model;iterative method;level of detail;manifold regularization;mathematical optimization;mixture model;nonlinear system;sparse matrix;the superficial;utility functions on indivisible goods	Peter Krauthausen;Patrick Ruoff;Uwe D. Hanebeck	2011	14th International Conference on Information Fusion		conditional probability distribution;econometrics;mathematical optimization;mathematics;statistics	Vision	29.69738463219186	-28.14349683344147	162870
07f0aff7a06a5433129003a5b30c7bd201489d93	monte carlo methods for tempo tracking and rhythm quantization	mcmc methods;generic model;gibbs sampling;music performance;sequential monte carlo method;simulated annealing;state estimation;artificial intelligent;article letter to editor;markov chain monte carlo;particle filter;hidden variables;monte carlo method;exact computation;music information retrieval;state space model	We present a probabilistic generative model for timing deviations in expressive music performance. The structure of the proposed model is equivalent to a switching state space model. The switch variables correspond to discrete note locations as in a musical score. The continuous hidden variables denote the tempo. We formulate two well known music recognition problems, namely tempo tracking and automatic transcription (rhythm quantization) as filtering and maximum a posteriori (MAP) state estimation tasks. Exact computation of posterior features such as the MAP state is intractable in this model class, so we introduce Monte Carlo methods for integration and optimization. We compare Markov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling, simulated annealing and iterative improvement) and sequential Monte Carlo methods (particle filters). Our simulation results suggest better results with sequential methods. The methods can be applied in both online and batch scenarios such as tempo tracking and transcription and are thus potentially useful in a number of music applications such as adaptive automatic accompaniment, score typesetting and music information retrieval.	computation;generative model;gibbs sampling;hidden variable theory;information retrieval;iterative method;markov chain monte carlo;mathematical optimization;monte carlo method;particle filter;sampling (signal processing);simulated annealing;simulation;state-space representation;transcription (software)	Ali Taylan Cemgil;Hilbert J. Kappen	2003	J. Artif. Intell. Res.	10.1613/jair.1121	speech recognition;simulated annealing;gibbs sampling;hybrid monte carlo;particle filter;markov chain monte carlo;computer science;state-space representation;machine learning;monte carlo integration;hidden variable theory;statistics;monte carlo method	ML	37.882198445334204	-27.507409034987052	163196
f9789cc74301b434dc4cdadeffcad2266c3ed40e	ipf for discrete chain factor graphs	iterative proportional fitting;standard undirected model;conditional likelihood maximization;bayesian network;undirected graphical model;likelihood maximization;art method;chain graph;discrete chain factor graph;discrete variable model;factor graph;belief network;iterative algorithm	Iterative Proportional Fitting (IPF), com­ bined with EM, is commonly used as an al­ gorithm for likelihood maximization in undi­ rected graphical models. In this paper, we present two iterative algorithms that gener­ alize upon IPF. The first one is for likelihood maximization in discrete chain factor graphs, which we define as a wide class of discrete variable models including undirected graph­ ical models and Bayesian networks, but also chain graphs and sigmoid belief networks. The second one is for conditional likelihood maximization in standard undirected models and Bayesian networks. In both algorithms, the iteration steps are expressed in closed form. Numerical simulations show that the algorithms are competitive with state of the art methods.	bayesian network;computational fluid dynamics;curve fitting;expectation–maximization algorithm;factor graph;graph (discrete mathematics);graphical model;iteration;iterative proportional fitting;numerical linear algebra;sigmoid function;simulation;ical	Wim Wiegerinck;Tom Heskes	2002			mathematical optimization;combinatorics;machine learning;mathematics;statistics	ML	26.618599050384084	-29.571365546136093	163211
2631d0f64407100c0809642989b10022b24bd0c4	generalizing tree probability estimation via bayesian networks		Probability estimation is one of the fundamental tasks in statistics and machine learning. However, standard methods for probability estimation on discrete objects do not handle object structure in a satisfactory manner. In this paper, we derive a general Bayesian network formulation for probability estimation on leaf-labeled trees that enables flexible approximations which can generalize beyond observations. We show that efficient algorithms for learning Bayesian networks can be easily extended to probability estimation on this challenging structured space. Experiments on both synthetic and real data show that our methods greatly outperform the current practice of using the empirical distribution, as well as a previous effort for probability estimation on trees.	algorithm;approximation;bayesian network;clade;cobham's thesis;estimated;experiment;inference;keneth alden simons;machine learning;markov chain monte carlo;numerical analysis;physical object;population parameter;probability;synthetic intelligence;tracer;tree (data structure);trees (plant);variational principle;funding grant	Cheng Zhang;Frederick A. Matsen	2018			empirical distribution function;mathematics;artificial intelligence;machine learning;generalization;bayesian network	ML	26.32554309199234	-29.33690222209195	163310
144370a2c5876295be0b79a37276d979bdafc443	learning gaussian graphical models with observed or latent fvss	article	Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widely used in many applications, and the trade-off between the modeling capacity and the efficiency of learning and inference has been an important research problem. In this paper, we study the family of GGMs with small feedback vertex sets (FVSs), where an FVS is a set of nodes whose removal breaks all the cycles. Exact inference such as computing the marginal distributions and the partition function has complexity O(kn) using message-passing algorithms, where k is the size of the FVS, and n is the total number of nodes. We propose efficient structure learning algorithms for two cases: 1) All nodes are observed, which is useful in modeling social or flight networks where the FVS nodes often correspond to a small number of highly influential nodes, or hubs, while the rest of the networks is modeled by a tree. Regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate with complexity O(kn + n log n) if the FVS is known or in polynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes are latent variables, where structure learning is equivalent to decomposing an inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix. By incorporating efficient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank corrections with complexity O(kn + n log n) per iteration. We perform experiments using both synthetic data as well as real data of flight delays to demonstrate the modeling capacity with FVSs of various sizes.	algorithm;experiment;feedback vertex set;graphical model;iteration;latent variable;machine learning;marginal model;markov chain;markov random field;message passing;partition function (mathematics);polynomial;synthetic data;time complexity	Ying Liu;Alan S. Willsky	2013			mathematical optimization;combinatorics;computer science;machine learning;mathematics;statistics	ML	25.99808909278893	-28.81453418157391	163570
3fffed6fd9336e4fb5cff44d4195126bc8b6b835	analysis of welding defects in spot welding process u-i curves	management system;welding quality;spot welding surface resistance electrodes voltage steel signal processing data acquisition cooling signal analysis control systems;welding defect welding defects analysis spot welding process u i curves spot welding signal a d conversion welding current electrode voltage welding cycle data acquisition digit signal module wavelet filtering welding splash;welds;a d conversion;data collection;spot welding;resistance;welding current;maintenance engineering;u i curves;joints;materials;production engineering computing;welding defects analysis;welding defect welding quality data acquisition spot welding;welding defect;spot welding signal;electrodes;electrode voltage;welding splash;analogue digital conversion;welding cycle;steel;digital signal processing chips;wavelet filtering;welds analogue digital conversion data acquisition digital signal processing chips electrodes filtering theory maintenance engineering production engineering computing spot welding;data acquisition;high speed;filtering theory;spot welding process;digit signal module	High speed collecting and managing system of spot welding signal can achieve the A/D conversion and data collection. The data of welding current, electrode voltage as well as welding cycle has been collected. Subsequently, they were processed with data acquisition and memory module, wavelet filtering of digit signal module, U-I curves and energy analyzing module, respectively. Based on the collections of welding current and electrode voltage, a criterion splash occurrence and the defective of loose weld by U-I curves change and acreage were proposed. The results prove that the criterion can decipher the welding splash and defective of loose weld accurately.	analog-to-digital converter;data acquisition;double-ended queue;memory module;robot welding;wavelet	Ru-xiong Li;Xiao-chun Lei;Ping Zhou	2009	2009 Third International Conference on Genetic and Evolutionary Computing	10.1109/WGEC.2009.110	maintenance engineering;spot welding;butt welding;electrode;management system;data acquisition;resistance;statistics;data collection	EDA	38.905330309435406	-30.515320734740136	163580
110e3f33951cc4be24a6bae45b9b28f1b27f63fd	regularized multivariate von mises distribution	directional distributions;machine learning;von mises distribution;circular distance	A b s t r a c t . Regularization is necessary to avoid overfitting when the number of data samples is low compared to the number of parameters of the model. In this paper, we introduce a flexible L1 regularization for the multivariate von Mises distribution. We also propose a circular distance that can be used to estimate the Kullback-Leibler divergence between two circular distributions by means of sampling, and also serves as goodness-of-fit measure. We compare the models on synthetic data and real morphological data from human neurons and show that the regularized model achieves better results than non regularized von Mises model.	approximation algorithm;basal (phylogenetics);kullback–leibler divergence;matrix regularization;overfitting;r language;sampling (signal processing);scalability;synthetic data	Luis Rodriguez-Lujan;Concha Bielza;Pedro Larrañaga	2015		10.1007/978-3-319-24598-0_3	econometrics;mathematical optimization;von mises distribution;cramér–von mises criterion;mathematics;statistics	ML	30.435672748803796	-26.863933476668194	163667
1644018f6c1fa874ee7071f4166fb7f67f5c04dc	self-measuring similarity for multi-task gaussian process		Multi-task learning aims at transferring knowledge between similar tasks. The multi-task Gaussian process framework of Bonilla et al. models (incomplete) responses of C data points for R tasks (e.g., the responses are given by an R×C matrix) by using a Gaussian process; the covariance function takes its form as the product of a covariance function defined on input-specific features and an inter-task covariance matrix (which is empirically estimated as a model parameter). We extend this framework by incorporating a novel similarity measurement, which allows for the representation of much more complex data structures. The proposed framework also enables us to exploit additional information (e.g., the input-specific features) when constructing the covariance matrices by combining additional information with the covariance function. We also derive an efficient learning algorithm which uses an iterative method to make predictions. Finally, we apply our model to a real data set of recommender systems and show that the proposed method achieves the best prediction accuracy on the data set.	algorithm;computer multitasking;conjugate gradient method;data point;data structure;gaussian process;iterative method;movielens;multi-task learning;recommender system	Kohei Hayashi;Takashi Takenouchi;Ryota Tomioka;Hisashi Kashima	2012			matérn covariance function;estimation of covariance matrices;covariance intersection;mathematical optimization;machine learning;pattern recognition;data mining;mathematics;rational quadratic covariance function;statistics;covariance function	ML	28.236579105332698	-34.25479522918346	163748
6aaab21d79b05130d8d0e7c4b05647d89f16b1d1	algorithms for model-based gaussian hierarchical clustering	hierarchical clustering;analyse amas;hierarchy;covariancia;aglomeracion;maximum likelihood;methode cholesky;metodo cholesky;efficient algorithm;decision bayes;modele mixte;covariance;formule mise a jour;curva gauss;bayes decision;probabilistic model;hierarchical classification;agglomerative hierarchical clustering;agglomeration;cluster analysis;givens rotation;mixed model;mixture model;62h30;65f99;jerarquia;sum of squares;modele probabiliste;classification hierarchique;loi normale;recurrence relation;analisis cluster;modelo mixto;probability model;cholesky method;mixture models;65y20;hierarchie;hierarchical agglomeration;clasificacion jerarquizada;model based cluster analysis;update formula;model based clustering;gaussian distribution;rotation givens;modelo probabilista	Agglomerative hierarchical clustering methods based on Gaussian probability models have recently shown promise in a variety of applications. In this approach, a maximum-likelihood pair of clusters is chosen for merging at each stage. Unlike classical methods, model-based methods reduce to a recurrence relation only in the simplest case, which corresponds to the classical sum of squares method. We show how the structure of the Gaussian model can be exploited to yield efficient algorithms for agglomerative hierarchical clustering.	algorithm;cluster analysis;hierarchical clustering;recurrence relation	Chris Fraley	1998	SIAM J. Scientific Computing	10.1137/S1064827596311451	econometrics;determining the number of clusters in a data set;machine learning;mixture model;mathematics;hierarchical clustering;ward's method;single-linkage clustering;brown clustering;statistics;hierarchical clustering of networks	ML	30.8670433164195	-30.44885623677247	163756
63c022198cf9f084fe4a94aa6b240687f21d8b41	consensus message passing for layered graphical models	abt black	Generative models provide a powerful framework for probabilistic reasoning. However, in many domains their use has been hampered by the practical difficulties of inference. This is particularly the case in computer vision, where models of the imaging process tend to be large, loopy and layered. For this reason bottom-up conditional models have traditionally dominated in such domains. We find that widely-used, general-purpose message passing inference algorithms such as Expectation Propagation (EP) and Variational Message Passing (VMP) fail on the simplest of vision models. With these models in mind, we introduce a modification to message passing that learns to exploit their layered structure by passing consensus messages that guide inference towards good solutions. Experiments on a variety of problems show that the proposed technique leads to significantly more accurate inference results, not only when compared to standard EP and VMP, but also when compared to competitive bottom-up conditional models.	algorithm;bottom-up parsing;casio loopy;computer vision;consistency model;expectation propagation;experiment;general-purpose modeling;graphical model;variational message passing;variational principle	Varun Jampani;S. M. Ali Eslami;Daniel Tarlow;Pushmeet Kohli;John M. Winn	2015	CoRR		computer science;theoretical computer science;machine learning;algorithm;statistics	ML	32.42026444727328	-34.16802131939517	164015
09b5f0f0d26e9a9318b640493c2ab911767060bb	an efficient algorithm for weak hierarchical lasso	non convex;proximal operator;sparse learning;weak hierarchical lasso	Linear regression is a widely used tool in data mining and machine learning. In many applications, fitting a regression model with only linear effects may not be sufficient for predictive or explanatory purposes. One strategy that has recently received increasing attention in statistics is to include feature interactions to capture the nonlinearity in the regression model. Such model has been applied successfully in many biomedical applications. One major challenge in the use of such model is that the data dimensionality is significantly higher than the original data, resulting in the small sample size large dimension problem. Recently, weak hierarchical Lasso, a sparse interaction regression model, is proposed that produces a sparse and hierarchical structured estimator by exploiting the Lasso penalty and a set of hierarchical constraints. However, the hierarchical constraints make it a non-convex problem and the existing method finds the solution to its convex relaxation, which needs additional conditions to guarantee the hierarchical structure. In this article, we propose to directly solve the non-convex weak hierarchical Lasso by making use of the General Iterative Shrinkage and Thresholding (GIST) optimization framework, which has been shown to be efficient for solving non-convex sparse formulations. The key step in GIST is to compute a sequence of proximal operators. One of our key technical contributions is to show that the proximal operator associated with the non-convex weak hierarchical Lasso admits a closed-form solution. However, a naive approach for solving each subproblem of the proximal operator leads to a quadratic time complexity, which is not desirable for large-size problems. We have conducted extensive experiments on both synthetic and real datasets. Results show that our proposed algorithm is much more efficient and effective than its convex relaxation. To this end, we further develop an efficient algorithm for computing the subproblems with a linearithmic time complexity. In addition, we extend the technique to perform the optimization-based hierarchical testing of pairwise interactions for binary classification problems, which is essentially the proximal operator associated with weak hierarchical Lasso. Simulation studies show that the non-convex hierarchical testing framework outperforms the convex relaxation when a hierarchical structure exists between main effects and interactions.	algorithm;binary classification;convex optimization;data mining;experiment;interaction;iterative method;lasso;linear programming relaxation;machine learning;mathematical optimization;nonlinear system;proximal operator;simulation;sparse matrix;synthetic intelligence;thresholding (image processing);time complexity	Yashu Liu;Jie Jin Wang;Jieping Ye	2014	TKDD	10.1145/2791295	mathematical optimization;machine learning;pattern recognition;mathematics;statistics	ML	26.005097679544573	-37.0975998173995	164146
d0c496192fb34c2ac48593da7a16af1b5e980961	recommendation from intransitive pairwise comparisons		In this paper we propose a full Bayesian probabilistic method to learn preferences from non-transitive pairwise comparison data. Such lack of transitivity easily arises when the number of pairwise comparisons is large, and they are given sequentially without allowing for consistency check. We develop a Bayesian Mallows model able to handle such data through a latent layer of uncertainty which captures the generation of preference misreporting. We then construct an MCMC algorithm, and test the procedure on simulated data.	algorithm;markov chain monte carlo;pairwise summation;vertex-transitive graph	Elja Arjas;Arnoldo Frigessi;Valeria Vitelli;Marta Crispino	2016			machine learning;artificial intelligence;pairwise comparison;computer science	ML	28.20279031081917	-31.328322025565733	164471
f4420a34c67a55d3bfec76f12abc255edfdb421f	forward backward greedy algorithms for multi-task learning with faster rates		A large body of algorithms have been proposed for multi-task learning. However, the effectiveness of many multi-task learning algorithms highly depends on the structural regularization, which incurs bias in the resulting estimators and leads to slower convergence rate. In this paper, we aim at developing a multi-task learning algorithm with faster convergence rate. In particular, we propose a general estimator for multitask learning with row sparsity constraint on the parameter matrix, i.e., the number of nonzero rows in the parameter matrix being small. The proposed estimator is a nonconvex optimization problem. In order to solve it, we develop a forward backward greedy algorithm with provable guarantee. More specifically, we prove that the output of the greedy algorithm attains a sharper estimation error bound than many state-of-the-art multi-task learning methods. Moreover, our estimator enjoys model selection consistency under a mild condition. Thorough experiments on both synthetic and real-world data demonstrate the effectiveness of our method and back up our theory.	backup;computer multitasking;experiment;greedy algorithm;information engineering;iterative method;list of dreamcast homebrew games;machine learning;manifold regularization;mathematical optimization;matrix regularization;model selection;multi-task learning;optimization problem;provable prime;rate of convergence;sparse matrix;synthetic intelligence;thresholding (image processing)	Lu Tian;Pan Xu;Quanquan Gu	2016			mathematical optimization;machine learning;mathematics;statistics;generalization error	ML	25.439397225370968	-34.98100263448544	164799
470ed65dd78ec380e83dbca85c64bcbf38494ac8	translation synchronization via truncated least squares		In this paper, we introduce a robust algorithm, TranSync, for the 1D translation synchronization problem, in which the aim is to recover the global coordinates of a set of nodes from noisy measurements of relative coordinates along an observation graph. The basic idea of TranSync is to apply truncated least squares, where the solution at each step is used to gradually prune out noisy measurements. We analyze TranSync under both deterministic and randomized noisy models, demonstrating its robustness and stability. Experimental results on synthetic and real datasets show that TranSync is superior to state-of-the-art convex formulations in terms of both efficiency and accuracy.	genetic translation process;graph - visual representation;least squares;prunes;randomized algorithm;synthetic intelligence	Xiangru Huang;Zhenxiao Liang;Chandrajit L. Bajaj;Qi-Xing Huang	2017	Advances in neural information processing systems		robustness (computer science);artificial intelligence;mathematical optimization;machine learning;synchronization;regular polygon;least squares;mathematics;graph	ML	25.182878949770753	-34.09389810567415	164888
9af91d9abf67a3f4dcfe3f15561119beaf9a0cc9	a kernel-based approach to molecular conformation analysis		We present a novel machine learning approach to understand conformation dynamics of biomolecules. The approach combines kernel-based techniques that are popular in the machine learning community with transfer operator theory for analyzing dynamical systems in order to identify conformation dynamics based on molecular dynamics simulation data. We show that many of the prominent methods like Markov state models, extended dynamic mode decomposition (EDMD), and time-lagged independent component analysis (TICA) can be regarded as special cases of this approach and that new efficient algorithms can be constructed based on this derivation. The results of these new powerful methods will be illustrated with several examples, in particular, the alanine dipeptide and the protein NTL9.	algorithm;dynamical system;hidden markov model;independent component analysis;kernel (operating system);machine learning;markov chain;molecular dynamics;simulation;transfer operator	Stefan Klus;Andreas Bittracher;Ingmar Schuster;Christof Schütte	2018	The Journal of chemical physics	10.1063/1.5063533	computational chemistry;computational physics;dynamical systems theory;molecular dynamics;kernel (linear algebra);chemistry;derivation;transfer operator;markov chain	ML	32.41350717201743	-28.153708058025156	164952
6ce431249ea3e2f811d2f948c1ca24b4fa90a2ba	information matrix for hidden markov models with covariates	standard errors;forward backward recursions;oakes identity;em algorithm	For a general class of hidden Markov models that may include time-varying covariates, we illustrate how to compute the observed information matrix, which may be used to obtain standard errors for the parameter estimates and check model identifiability. The proposed method is based on the Oakes’ identity and, as such, it allows for the exact computation of the information matrix on the basis of the output of the Expectation-Maximization (EM) algorithm for maximum likelihood estimation. In addition to this output, the method requires the first derivative of the posterior probabilities computed by the forward-backward recursions introduced by Baum and Welch. Alternative methods for computing exactly the observed information matrix require, instead, to differentiate twice the forward recursion used to compute the model likelihood, with a greater additional effort with respect to the EM algorithm. The proposed method is illustrated by a series of simulations and an application based on a longitudinal dataset in Health Economics.	baum–welch algorithm;computation;expectation–maximization algorithm;formation matrix;hidden markov model;markov chain;observed information;recursion;simulation;welch's method	Francesco Bartolucci;Alessio Farcomeni	2015	Statistics and Computing	10.1007/s11222-014-9450-8	forward algorithm;econometrics;mathematical optimization;expectation–maximization algorithm;machine learning;mathematics;standard error;statistics	ML	30.30948947555608	-25.04405636955848	164981
6a972debcc5133a9d5bc355bd9a7d73fe28ca577	local bandwidth selectors for deconvolution kernel density estimation	complex setting;local bandwidth selector;kernel estimator;measurement error;contaminated data · data-driven bandwidth · ebbs · errors-in-variables · kernel smoothing · measurement errors · plug-in · smoothing parameter;smoothing parameter;global bandwidth;deconvolution kernel density estimation;significant improvement;kernel density estimation;data-driven bandwidth selector;data-driven local bandwidth selector	We consider kernel density estimation when the observations are contaminated by measurement errors. It is well known that the success of kernel estimators depends heavily on the choice of a smoothing parameter called the bandwidth. A number of data-driven bandwidth selectors exist in the literature, but they are all global. Such techniques are appropriate when the density is relatively simple, but local bandwidth selectors can be more attractive in more complex settings. We suggest several data-driven local bandwidth selectors and illustrate via simulations the significant improvement they can bring over a global bandwidth.	deconvolution;simulation;smoothing;variable kernel density estimation	Achilleas Achilleos;Aurore Delaigle	2012	Statistics and Computing	10.1007/s11222-011-9247-y	econometrics;mathematical optimization;mathematics;statistics	Arch	30.322958424708776	-29.07946733275483	165274
027ec6b50e05d5661f9f24884b2fa097e6189f63	imp: a message-passing algorithm for matrix completion	matrix factorization;recommender systems;recommender system;graph theory;psychology;matrix decomposition;groupware;factor graph;motion pictures;collaborative filtering;message passing;noise measurement;bayesian methods;data clustering	A new message-passing (MP) method is considered for the matrix completion problem associated with recommender systems. We attack the problem using a (generative) factor graph model that is related to a probabilistic low-rank matrix factorization. Based on the model, we propose a new algorithm, termed IMP, for the recovery of a data matrix from incomplete observations. The algorithm is based on a clustering followed by inference via MP (IMP). The algorithm is compared with a number of other matrix completion algorithms on real collaborative filtering (e.g., Netflix) data matrices. Our results show that, while many methods perform similarly with a large number of revealed entries, the IMP algorithm outperforms all others when the fraction of observed entries is small. This is helpful because it reduces the well-known cold-start problem associated with collaborative filtering (CF) systems in practice.	algorithm;cluster analysis;cold start;collaborative filtering;factor graph;message passing;recommender system;the matrix;whole earth 'lectronic link	Byung-Hak Kim;Arvind Yedla;Henry D. Pfister	2010	2010 6th International Symposium on Turbo Codes & Iterative Information Processing		cuthill–mckee algorithm;information theory;computer science;theoretical computer science;machine learning;mathematics;distributed computing;matrix decomposition;recommender system	ML	28.592759835514745	-33.03304314915799	165381
83a5073281f928b61e4cd6c5e2e308e01fd8e592	learning mixtures of experts with a hybrid generative-discriminative algorithm	learning process;model selection;hybrid generative discriminative algorithm;mixture of experts;hierarchical mixture of experts;inference mechanisms;hierarchical mixtures of experts model;feature space;learning artificial intelligence expectation maximisation algorithm inference mechanisms;hybrid power systems;expectation maximization;model selection problem;expectation maximization inference;learning artificial intelligence;classification accuracy;model selection problem hierarchical mixtures of experts model hybrid generative discriminative algorithm expectation maximization inference learning process;local minima;em algorithm;expectation maximisation algorithm	The hierarchical mixtures of experts (HME) model is a flexible model that stochastically partitions the feature space into sub-regions, in which simple surfaces can be fitted to data. However, there are issues with model selection during the learning of the HME model using Expectation-Maximization (EM) inference. In addition, the EM algorithm suffers from the well-known problem of local minima. In this paper, we present a hybrid generative-discriminative approach that inherits the flexibility of the HME while decomposing the learning process into a few simple steps. The proposed algorithm solves the model-selection problem and empirical experiments on public benchmark datasets show its advantages in classification accuracy and efficiency.	algorithmic efficiency;benchmark (computing);cluster analysis;emoticon;expectation–maximization algorithm;experiment;feature vector;johnson–nyquist noise;maxima and minima;model selection;openembedded;sonar (symantec);selection algorithm;whole earth 'lectronic link	Ya Xue;Xiao Hu;Weizhong Yan;Hai Qiu	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596680	expectation–maximization algorithm;computer science;artificial intelligence;machine learning;pattern recognition;mathematics	ML	29.116251225104318	-31.67551114749566	165808
d5c51d7d88c586c146d52e89e27de0a8fb7d04dc	selection and recognition of statistically defined signals in learning systems		The paper addresses a non-traditional problem of pattern recognition, when information about pattern is represented in the form of a random signal taken from the output of a corresponding physical sensor. It is supposed that there exist two types of signal to recognize, namely, specified in the statistical sense signals and totally unknown signals. Such the conditions are called conditions of increased a priory uncertainty. Developing a technique to recognize specified signals in conditions of increased a priory uncertainty is the objective of this paper. Methods for selection and recognition of a statistically defined random signal are proposed for the cases when signal description is done by various probabilistic models. Additional consideration is given to peculiarities of employing these methods for solving applied problems of pattern recognition in radar, medical diagnostics and speaker identification.		Valeriy Bezruk;Anatolii Omelchenko;Oleksii Fedorov;Paolo Mercorelli;Juan I. Nieto-Hipólito	2018	IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2018.8591321		Vision	36.20886487521649	-34.09116159388761	166068
f67ccd19962f72f9686a134242227580a3976e41	remaining useful life estimation based on nonlinear feature reduction and support vector regression	prognostics and health management;isometric feature mapping;support vector regression;remaining useful life	Prognostics and Health Management (PHM) of rotating machines is gaining importance in industry and allows increasing reliability and decreasing machines’ breakdowns. Bearings are one of the most components present in mechanical equipments and one of their most common failures. So, to assess machines’ degradations, fault prognostics of bearings is developed in this paper. The proposed method relies on two steps (an offline step and an online step) to track the health state and predict the remaining useful life (RUL) of the bearings. The offline step is used to learn the degradation models of the bearings whereas the online step uses these models to assess the current health state of the bearings and predict their RUL. During the offline step, vibration signals acquired on the bearings are processed to extract features, which are then exploited to learn models that represent the evolution of the degradations. For this purpose, the isometric feature mapping reduction technique (ISOMAP) and support vector regression (SVR) are used. The method is applied on a laboratory experimental degradations related to bearings. The obtained results show that the method can effectively model the evolution of the degradations and predict the RUL of the bearings.	degraded mode;elegant degradation;isomap;isometric projection;many-one reduction;mathematical optimization;nonlinear system;online and offline;sensor;support vector machine;time complexity	Tarak Benkedjouh;Kamal Medjaher;Noureddine Zerhouni;Saïd Rechak	2013	Eng. Appl. of AI	10.1016/j.engappai.2013.02.006	support vector machine;simulation;computer science;machine learning;prognostics	ML	36.77324011052063	-30.01556671453676	166158
0e575945aa895d955bd1587118989cb4d2c77a08	detection of electrical series arcs using time-frequency analysis and mathematical morphology	mathematical morphology;arcs electric;arc features extraction electrical series arcs detection time frequency analysis mathematical morphology time frequency representation electrical loads load perturbations real time processing;signal detection;real time processing;feature extraction arcs electric time frequency analysis mathematical morphology signal detection signal representation;time frequency representation;feature extraction;signal representation;time frequency analysis morphology welding acoustic signal detection acoustic sensors signal processing wideband wire feeds research and development;time frequency analysis	A method for detecting series arcs from time-frequency analysis of the current derivative is proposed. Mathematical morphology is used to select arc features from the time-frequency representation. The proposed method has been validated on a large set of signals from various difficult electrical loads. It proves successful in separating the series arcs from the perturbations produced by the load. Furthermore, it can easily be implemented for real-time processing.	frequency analysis;mathematical morphology;time–frequency analysis	B. Leprettre;Y. Rebiere	2001		10.1109/ISSPA.2001.950176	speech recognition;mathematical morphology;time–frequency analysis;feature extraction;computer science;machine learning;time–frequency representation;quantum mechanics;detection theory	Vision	37.73383105425485	-31.98685358430764	166269
cef4843933596f0395c597cd11d65395bbb04158	personal low-cost ultrasound training system		To meet the challenge of making realistic training opportunities in medical ultrasound readily available, a PC based low cost personal ultrasound training system has been developed. The training experience is provided by scanning a generic, curved and compliant scan surface with a sham transducer, containing position and orientation sensors, while the PC displays both a virtual subject and a virtual transducer, along with an ultrasound image.		Peder C. Pedersen;Daniel Skehan	2012	Studies in health technology and informatics	10.3233/978-1-61499-022-2-344	training system;ultrasound;data mining;medicine	HCI	38.2684314316585	-37.886920737908056	166542
c3db6cf1c59e254655f37acd5b01fffae03756c4	nonparametric classification using quadratic quantile statistics (ph.d. thesis abstr.)	nonparametric detection;pattern classification nonparametric detection;pattern classification	First Page of the Article		G. Pasternack	1975	IEEE Trans. Information Theory	10.1109/TIT.1975.1055483	econometrics;pattern recognition;nonparametric regression;statistics	Vision	30.767657683414097	-27.497846210873952	166625
51c7c8cee343f7b82183edaea2c44a70ed129d3d	adaptive penalized splines for data smoothing	data smoothing;local penalty;adaptivity;nonparametric regression;penalized splines	Data driven adaptive penalized splines are considered via the principle of constrained regression. A locally penalized vector based on the local ranges of the data is generated and added into the penalty matrix of the classical penalized splines, which remarkably improves the local adaptivity of the model for data heterogeneity. The algorithm complexity and simulations are studied. The results show that the adaptive penalized splines outperform the smoothing splines, l1 trend filtering and classical penalized splines in estimating functions with inhomogeneous smoothness. © 2016 Elsevier B.V. All rights reserved.	adaptive grammar;algorithm;b-spline;computer simulation;linear least squares (mathematics);overfitting;smoothing spline;spline (mathematics)	Lian-Qiang Yang;Yongmiao Hong	2017	Computational Statistics & Data Analysis	10.1016/j.csda.2016.10.022	econometrics;mathematical optimization;mathematics;nonparametric regression;statistics;smoothing	AI	29.14385445337321	-26.998308099804863	166633
12d8ea02fb870c391ae469e9776b6ab118116c55	hierarchal decomposition of neural data using boosted mixtures of hidden markov chains and its application to a bmi	multidimensional neural input data boosted mixtures hidden markov chains bmi brain machine interfaces;human computer interaction;neural nets;hidden markov chain;hidden markov models brain modeling animals trajectory neurons multidimensional systems boosting brain computer interfaces computer interfaces robots;mixture of experts;neural nets hidden markov models human computer interaction medical computing multidimensional systems;brain machine interfaces;medical computing;hidden markov models;boosted mixtures;brain machine interface;multidimensional neural input data;hidden markov chains;multidimensional systems;bmi	In this paper, we propose a simple algorithm that takes multidimensional neural input data and decomposes the joint likelihood into marginals using boosted mixtures of hidden Markov chains (BM-HMM). The algorithm applies techniques from boosting to create hierarchal dependencies between these marginal subspaces. Finally, borrowing ideas from mixture of experts, the local information is weighted and incorporated into an ensemble decision. Our results show that this algorithm is very simple to train and computationally efficient, while also providing the ability to reduce the input dimensionality for brain machine interfaces (BMIs).	algorithm;algorithmic efficiency;boosting (machine learning);brain–computer interface;gradient boosting;hidden markov model;marginal model;markov chain	Shalom Darmanjian;António R. C. Paiva;José Carlos Príncipe;Justin C. Sanchez	2007	2007 International Joint Conference on Neural Networks	10.1109/IJCNN.2007.4371449	brain–computer interface;speech recognition;multidimensional systems;computer science;machine learning;pattern recognition;hidden markov model	ML	30.86186735973209	-36.34497352760851	166754
f567723eef8963ec6b14487bae511cdeefb1b521	convex-constrained sparse additive modeling and its extensions		• Many functions that arise in practice tend to be convex/concave or monotonic (Groeneboom and Jongbloed 2014) • In high-dimensional setting, many covariates may not be relevant • Our contributions: • A sparse convex additive model (SCAM) to estimate convex (and monotonic) component functions in high dimensional additive modeling • A sparse difference of convex additive model (SDCAM) to address potential robustness issue of SCAM, e.g., convex functions are mistakenly believed to be concave • An efficient backfitting algorithm with linear per-iteration complexity	additive model;backfitting algorithm;concave function;convex function;iteration;sparse matrix	Junming Yin;Yaoliang Yu	2017	CoRR		econometrics;mathematical optimization;machine learning;generalized additive model;sparse approximation;mathematics;statistics	ML	26.90801588523173	-33.722285344621	167136
54bef96ba75242babd836738cc82f1db3998c674	modelling smooth paths using gaussian processes	theoretical framework;generic model;gaus sian mixture model;gaussian mixture model;expectation maximization;gaussian process;variational method;lower bound	A generative model based on the gaussian mixture model and gaussian processes is presented in this paper. Typical motion paths are learnt and then used for motion prediction using this model. The principal novel aspect of this approach is the modelling of paths using gaussian processes. It allows the representation of smooth trajectories and avoids discretization problems found in most existing methods. Gaussian processes not only provides a comprehensive and formal theoretical framework to work with, it also lends itself naturally to path clustering using gaussian mixture models. Learning is performed using expectation maximization where the E-Step uses variational methods to maximize its lower bound before optimization over parameters are performed in the M-Step.	calculus of variations;cluster analysis;discretization;expectation–maximization algorithm;gaussian process;generative model;iso 10303;mathematical optimization;mixture model;motion planning;protein structure prediction;real-time computing	Christopher Tay Meng Keat;Christian Laugier	2007		10.1007/978-3-540-75404-6_36	gaussian random field;mathematical optimization;expectation–maximization algorithm;variational method;machine learning;mixture model;gaussian process;upper and lower bounds;gaussian function;statistics	ML	27.178403605166203	-30.244762237070347	167227
61c40713b77f70fb43ee8459cbdefc392dfa30c9	beta-negative binomial process and poisson factor analysis		A beta-negative binomial (BNB) process is proposed, leading to a beta-gamma-Poisson process, which may be viewed as a “multiscoop” generalization of the beta-Bernoulli process. The BNB process is augmented into a beta-gamma-gamma-Poisson hierarchical structure, and applied as a nonparametric Bayesian prior for an infinite Poisson factor analysis model. A finite approximation for the beta process Lévy random measure is constructed for convenient implementation. Efficient MCMC computations are performed with data augmentation and marginalization techniques. Encouraging results are shown on document count matrix factorization.	approximation;computation;convolutional neural network;count data;euler–bernoulli beam theory;factor analysis;latent variable model;markov chain monte carlo;multinomial logistic regression;negative feedback;next-generation access;perplexity;predictive failure analysis;text corpus;topic model	Mingyuan Zhou;Lauren Hannah;David B. Dunson;Lawrence Carin	2012			central binomial coefficient;poisson binomial distribution;quasi-likelihood;compound poisson distribution;binomial distribution;cox process;count data;zero-inflated model;poisson regression;poisson distribution;negative binomial distribution;binomial test;compound poisson process;continuity correction;negative multinomial distribution	ML	30.84003339592314	-25.459691873835144	167457
1c21fba61415d9985e091b22218281c7d560bb39	multi-sensor data fusion using support vector machine for motor fault detection	condition based monitoring;information fusion;motor diagnosis;support vector machine;sensor fusion	Motor fault diagnosis in dynamic condition is a typical multi-sensor data fusion problem. It involves the use of information collected from multiple sensors, such as vibration, sound, current, voltage, and temperature, to detect and identify motor faults. From the viewpoint of evidence theory, information obtained from each sensor can be considered as a piece of evidence, and as such, the multi-sensor based motor fault diagnosis can be viewed as the problem of evidence fusion. In this article we propose and investigate a hybrid method for fault signal classification based on sensor data fusion by using the Support Vector Machine (SVM) and Short Term Fourier Transform (STFT) techniques. We report a practical application of this hybrid model and evaluate its performance. Finally, we compare the performance of the proposed system against some other standard fault classification techniques.	fault detection and isolation;support vector machine	Tribeni Prasad Banerjee;Swagatam Das	2012	Inf. Sci.	10.1016/j.ins.2012.06.016	support vector machine;soft sensor;computer science;machine learning;pattern recognition;data mining;sensor fusion	Robotics	36.69171540413078	-31.02768982866683	167567
bd872e32524bf6dadafc3aa5c01f4edbbe175fa7	discriminant analysis when the classes arise from a continuum	calcul matriciel;metodo estadistico;sistema experto;taux erreur;matrice covariance;statistical method;matriz covariancia;linear discriminate analysis;classification;discriminant analysis;analyse discriminante;regle decision;analisis discriminante;estimation erreur;error estimation;methode statistique;multivariate normal distribution;classification rules;estimacion error;pattern recognition;error rate;continuo;continuum;matrix calculus;reconnaissance forme;regla decision;systeme expert;reconocimiento patron;algoritmo optimo;indice error;algorithme optimal;optimal algorithm;clasificacion;calculo de matrices;decision rule;covariance matrix;expert system	Sometimes two class linear discriminant analysis is applied to situations in which the classes are formed by partitioning an underlying continuum. In such cases, a reasonable assumption is that the underlying continuous ‘‘response’’ variable forms a joint multivariate normal distribution with the predictors. We compare the error rate of linear discriminant analysis with that of the optimal classification rule under these conditions, showing that linear discriminant analysis leads to a decision surface parallel to, but shifted from, the decision surface of the optimal rule and that the two rules can lead to very different error rates. ( 1998 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved Linear discriminant analysis Classification rule Optimal decision surface Error rate	decision boundary;heart rate variability;linear discriminant analysis;pattern recognition;rule 90;sampling (signal processing);triune continuum paradigm;word error rate	David J. Hand;Jonathan J. Oliver;A. Daniel Lunn	1998	Pattern Recognition	10.1016/S0031-3203(97)00083-6	econometrics;covariance matrix;multivariate normal distribution;matrix calculus;computer science;calculus;continuum;decision rule;optimal discriminant analysis;discriminant function analysis;mathematics;linear discriminant analysis;expert system;statistics	Vision	33.094453002790296	-24.007617883765622	167575
cd1045dbd3b05dbedbf2f50d2d499fd7197561d2	efficient tracking of the dominant eigenspace of a normalized kernel matrix	large data sets;machine learning;nonlinear problem;kernel method	Various machine learning problems rely on kernel-based methods. The power of these methods resides in the ability to solve highly nonlinear problems by reformulating them in a linear context. The dominant eigenspace of a (normalized) kernel matrix is often required. Unfortunately, the computational requirements of the existing kernel methods are such that the applicability is restricted to relatively small data sets. This letter therefore focuses on a kernel-based method for large data sets. More specifically, a numerically stable tracking algorithm for the dominant eigenspace of a normalized kernel matrix is proposed, which proceeds by an updating (the addition of a new data point) followed by a downdating (the exclusion of an old data point) of the kernel matrix. Testing the algorithm on some representative case studies reveals that a very good approximation of the dominant eigenspace is obtained, while only a minimal amount of operations and memory space per iteration step is required.	abalone;algorithm;approximation;computation (action);dspace;data point;dimensions;exclusion;hl7publishingsubsection <operations>;iteration;kernel method;laplacian matrix;machine learning;nonlinear system;numerical stability;pattern recognition;requirement;signal processing;singular value decomposition;system identification	Geert Gins;Ilse Y. Smets;Jan F. M. Van Impe	2008	Neural Computation	10.1162/neco.2007.05-06-213	kernel;principal component regression;kernel method;mathematical optimization;string kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;mathematics;tree kernel;kernel;variable kernel density estimation;polynomial kernel;kernel smoother	ML	25.27679847962111	-37.54387146333904	167688
c3f3f509cb732a97cd47000064a61d5578469582	learning to be bayesian without supervision	gaussian noise;probability density;mean square;posterior distribution;least square;bayesian estimator;estimation error;likelihood function	Bayesian estimators are defined in terms of the posterior distribution. Typically, this is written as the product of the likelihood function and a prior probability density, both of which are assumed to be known. But in many situations, the prior density is not known, and is difficult to learn from data since one does not have access to uncorrupted samples of the variable being estimated. We show that for a wide variety of observation models, the Bayes least squares (BLS) estimator may be formulated without explicit reference to the prior. Specifically, we derive a direct expression for the estimator, and a related expression for the mean squared estimation error, both in terms of the density of the observed measurements. Each of these prior-free formulations allows us to approximate the estimator given a sufficient amount of observed data. We use the first form to develop practical nonparametric approximations of BLS estimators for several different observation processes, and the second form to develop a parametric family of estimators for use in the additive Gaussian noise case. We examine the empirical performance of these estimators as a function of the amount of observed data.	approximation algorithm;least squares;utility functions on indivisible goods	Martin Raphan;Eero P. Simoncelli	2006			efficient estimator;gaussian noise;minimum mean square error;minimax estimator;econometrics;probability density function;estimator;bayes estimator;maximum a posteriori estimation;pattern recognition;m-estimator;conjugate prior;mathematics;mean squared error;bayesian linear regression;likelihood function;posterior probability;bias of an estimator;estimation theory;orthogonality principle;least squares;invariant estimator;empirical probability;statistics	ML	29.7562101765731	-24.747509659740953	167823
a9f63babc99eafea1fcf0181186624da0479bae4	cross-conformal predictive distributions		Conformal predictive systems are a recent modification of conformal predictors that output, in regression problems, probability distributions for labels of test observations rather than set predictions. The extra information provided by conformal predictive systems may be useful, e.g., in decision making problems. Conformal predictive systems inherit the relative computational inefficiency of conformal predictors. In this paper we discuss two computationally efficient versions of conformal predictive systems, which we call split conformal predictive systems and cross-conformal predictive systems, and discuss their advantages and limitations.	algorithmic efficiency;benchmark (computing);conformal dimension;randomized algorithm	Vladimir Vovk;Ilia Nouretdinov;Valery Manokhin;Alexander Gammerman	2018			mathematical analysis;conformal map;mathematics	AI	28.22266167138397	-25.234585643263635	167836
025c81c281dd5f1d08159a1ae4e763487eae193a	bayesian elastic net tobit quantile regression	62j05;bayesian quantile regression;prior distribution;tobit quantile regression;elastic net;62f15	In this paper, the problem of parameter estimation and variable selection in the Tobit quantile regression model is considered. A Tobit quantile regression with the elastic net penalty from a Bayesian perspective is proposed. Independent gamma priors are put on the l1 orm penalty parameters. A novel aspect of the Bayesian elastic net Tobit quantile regression is to treat the hyperparameters of the gamma priors as unknowns and let the data estimate them along with other parameters. A Bayesian Tobit quantile regression with the adaptive elastic net penalty is also proposed. The Gibbs sampling computational technique is adapted to simulate the parameters from the posterior distributions. The proposed methods are demonstrated by both simulated and real data examples.	elastic map;elastic net regularization;estimation theory;feature selection;gibbs sampling;sampling (signal processing);simulation;tobit model	Rahim Alhamzawi	2016	Communications in Statistics - Simulation and Computation	10.1080/03610918.2014.904341	econometrics;quantile regression;prior probability;tobit model;mathematics;bayesian linear regression;elastic net regularization;statistics	ML	30.19805993032768	-26.02768658082858	167973
02810c0a71afbeb54e9afa9a42d6f620b9871c9a	variational filtering		This note presents a simple Bayesian filtering scheme, using variational calculus, for inference on the hidden states of dynamic systems. Variational filtering is a stochastic scheme that propagates particles over a changing variational energy landscape, such that their sample density approximates the conditional density of hidden and states and inputs. The key innovation, on which variational filtering rests, is a formulation in generalised coordinates of motion. This renders the scheme much simpler and more versatile than existing approaches, such as those based on particle filtering. We demonstrate variational filtering using simulated and real data from hemodynamic systems studied in neuroimaging and provide comparative evaluations using particle filtering and the fixed-form homologue of variational filtering, namely dynamic expectation maximisation.	approximation;calculi;calculus of variations;cognition disorders;computational neuroscience;deoxyhemoglobin;dynamical system;evaluation;expectation–maximization algorithm;fgfr1 gene;frontotemporal dementia, chromosome 3-linked;functional imaging;hemodynamics;histamine h1 antagonists;histamine h2 antagonists;inference;negative feedback;neuroimaging;neuroscience discipline;nonlinear system;normal statistical distribution;particle filter;rln2 gene;rendering (computer graphics);resonance;state space;variational principle;word lists by frequency;density;fmri	Karl J. Friston	2008	NeuroImage	10.1016/j.neuroimage.2008.03.017		ML	33.50243842580047	-29.531852589481808	168072
15edfdc6f713543bd652ecb440a94c33f8940878	sequential change point detection in molecular dynamics trajectories	62j12;mathematics;62h15;molecular dynamics;92c40;fractional bayes;applied mathematics;change point detection;var model	Motivated from a molecular dynamics context we propose a sequential change point detection algorithm for vector-valued autoregressive models based on Bayesian model selection. The algorithm does not rely on any sampling procedure or assumptions underlying the dynamics of the transitions, and is designed to cope with high dimensional data. We show the applicability of the algorithm on a time series obtained from numerical simulation of a penta peptide molecule.	algorithm;approximation;autoregressive model;bayes factor;bayesian network;cholesky decomposition;computational complexity theory;computer simulation;encode;hidden markov model;model selection;molecular dynamics;moment matrix;numerical analysis;observable;online and offline;sampling (signal processing);sparse matrix;time series;vector autoregression	Eike Meerbach;Juan C. Latorre;Christof Schütte	2012	Multiscale Modeling & Simulation	10.1137/110850621	econometrics;mathematical optimization;molecular dynamics;mathematics;change detection;vector autoregression;statistics	ML	32.217520085613835	-28.01633473000949	168090
07e1ed6061d226b9f96b229a58e8fcd0f2dbf314	segmentation using population based markov chain monte carlo	image segmentation;swendsen wang;pop mcmc;markov chain monte carlo;pop mcmc image segmentation markov chain monte carlo swendsen wang;markov processes image segmentation monte carlo methods convergence probabilistic logic computer vision couplings;population based markov chain monte carlo reconstructed energy function swendsen wang cuts algorithm atomic regions meanshift filter pop mcmc np hard problem simulated annealing image segmentation;simulated annealing filtering theory image segmentation markov processes monte carlo methods	Simulated Annealing is a methodology employed to solve NP-hard problem proximately. Compared with other methods, SA is able to obtain more accurate solution to the problem. However, this algorithm is too costly to be applied to the complicated problems. With this motivation, a novel algorithm Using Population based Markov chain Monte Carlo (Pop-MCMC) is proposed for segmentation. It takes less time from the initial state to the state that chains are coupling with Pop-MCMC than with the Simulated Annealing which is usually employed in traditional MCMC. The main feature Pop-MCMC owns is that multiple samples are generated at a time and information is exchanged between the Markov Chains. A graph is constructed with the atomic regions which are formed using the MeanShift filter. Secondly, the Swendsen-Wang Cuts Algorithm is employed to construct the Markov chain based on the reconstructed energy function. Thirdly, pop-MCMC is employed to speed up the convergence of the Markov chain. Experiments show our algorithm achieves better segmentation results.	algorithm;markov chain monte carlo;mathematical optimization;monte carlo method;np-hardness;population;simulated annealing;speedup	Xiangrong Wang	2013	2013 Ninth International Conference on Natural Computation (ICNC)	10.1109/ICNC.2013.6817967	monte carlo method in statistical physics;mathematical optimization;coupling from the past;hybrid monte carlo;particle filter;markov chain monte carlo;computer science;machine learning;monte carlo molecular modeling;markov chain mixing time;markov model;parallel tempering;statistics;monte carlo method;variable-order markov model	Robotics	34.33666276344327	-33.45505401190784	168884
b233c7f16c5494a3478f04ce00c8a70280c102f4	efficient online inference for infinite evolutionary cluster models with applications to latent social event discovery		The Recurrent Chinese Restaurant Process (RCRP) is a powerful statistical method for modeling evolving clusters in large scale social media data. With the RCRP, one can allow both the number of clusters and the cluster parameters in a model to change over time. However, application of the RCRP has largely been limited due to the non-conjugacy between the cluster evolutionary priors and the Multinomial likelihood. This non-conjugacy makes inference di cult and restricts the scalability of models which use the RCRP, leading to the RCRP being applied only in simple problems, such as those that can be approximated by a single Gaussian emission. In this paper, we provide a novel solution for the non-conjugacy issues for the RCRP and an example of how to leverage our solution for one speci c problem the social event discovery problem. By utilizing Sequential Monte Carlo methods in inference, our approach can be massively paralleled and is highly scalable, to the extent it can work on tens of millions of documents. We are able to generate high quality topical and location distributions of the clusters that can be directly interpreted as real social events, and our experimental results suggest that the approaches proposed achieve much better predictive performance than techniques reported in prior work. We also demonstrate how the techniques we develop can be used in a much more general ways toward similar problems.	approximation algorithm;display resolution;monte carlo method;multinomial logistic regression;parametric model;scalability;social media;stream (computing)	Wei Wei;Kenneth Joseph;Kathleen M. Carley	2017	CoRR		computer science;artificial intelligence;data mining;machine learning;prior probability;scalability;particle filter;chinese restaurant process;multinomial distribution;inference	ML	26.479099828230055	-30.91410504821664	168885
b7433239de36fcb09e9d74855122b9e63dee9491	rao-blackwellised particle filtering via data augmentation	data augmentation;latent variable;discrete observation;hybrid model;rao blackwellised particle filter	In this paper, we extend the Rao-Blackwellised particle filtering method to more complex hybrid models consisting of Gaussian latent variables and discrete observations. This is accomplished by augmenting the models with artificial variables that enable us to apply Rao-Blackwellisation. Other improvements include the design of an optimal importance proposal distribution and being able to swap the sampling an selection steps to handle outliers. We focus on sequential binary classifiers that consist of linear combinations of basis functions, whose coefficients evolve according to a Gaussian smoothness prior. Our results show significant improvements.	particle filter	Christophe Andrieu;Nando de Freitas;Arnaud Doucet	2001			latent variable;econometrics;computer science;machine learning;mathematics;statistics	Vision	29.81798258400489	-28.639426398047355	168886
a828bfce8bafbc46ed5309abb6a77cabd121b034	application of computational intelligence tools for the analysis of marine geotechnical properties in the head of zakynthos canyon, greece	zakynthos canyon;marine geotechnical properties;generic interaction matrix;greece;self organizing map;artificial neural network	This paper uses a computational approach to provide insight into the relationships among marine geotechnical properties that characterize the recent sedimentary cover at the head of Zakynthos Canyon in western Greece. Self-organizing maps (SOM) and generic interaction matrix (GIM) theory were used to investigate the tendency of the data to cluster and to examine the sediment property relationships. This analysis has also focused on the assessment of the dominance and interaction intensity between the related parameters following GIM theory definition. The principal results refer to the identification of clusters in the original multivariate data set. SOM-based analysis distinguished five clusters, with similar geotechnical characteristics, which led to the separation of the surficial (~80cm) unconsolidated sediments from the deeper normally consolidated sediments and depicted better relations between the geotechnical properties within each cluster. The combination of SOM with GIM theory also demonstrates the dominance of fine-grained sediments (especially silts) and their associated Atterberg limits. The strongest interaction intensity is observed between silt and water content, whereas the undrained shear strength of the surficial deposits appears to be least interactive. The application of computational intelligence methods in the study of marine geotechnical properties allows insight into the relationships between the various geotechnical parameters and provides a promising tool for knowledge extraction in marine geo-environments.	computational intelligence	Maria Ferentinou;Thomas Hasiotis;Michael Sakellariou	2012	Computers & Geosciences	10.1016/j.cageo.2011.06.022	mining engineering;self-organizing map;geology;computer science;machine learning;geotechnical engineering;artificial neural network	NLP	35.0242731288183	-36.48355565930499	168968
59e03eba4b9c7d848c71bbaa14bc80959e9311e7	a kernel-independent fmm in general dimensions	machine learning algorithms;kernel;complexity theory;approximation algorithms;kernel methods;data mining;skeleton;acceleration;machine learning;transforms;n body problems;kernel ridge regression;parallel algorithms	We introduce a general-dimensional, kernel-independent, algebraic fast multipole method and apply it to kernel regression. The motivation for this work is the approximation of kernel matrices, which appear in mathematical physics, approximation theory, non-parametric statistics, and machine learning. Existing fast multipole methods are asymptotically optimal, but the underlying constants scale quite badly with the ambient space dimension. We introduce a method that mitigates this shortcoming; it only requires kernel evaluations and scales well with the problem size, the number of processors, and the ambient dimension---as long as the intrinsic dimension of the dataset is small. We test the performance of our method on several synthetic datasets. As a highlight, our largest run was on an image dataset with 10 million points in 246 dimensions.	analysis of algorithms;approximation theory;asymptotically optimal algorithm;central processing unit;dhrystone;fast multipole method;intrinsic dimension;kernel (operating system);linear algebra;machine learning	William B. March;Bo Xiao;Sameer Tharakan;Chenhan D. Yu;George Biros	2015	SC15: International Conference for High Performance Computing, Networking, Storage and Analysis	10.1145/2807591.2807647	kernel;acceleration;principal component regression;kernel method;mathematical optimization;kernel;string kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;theoretical computer science;machine learning;graph kernel;parallel algorithm;tree kernel;skeleton;variable kernel density estimation;polynomial kernel;approximation algorithm;statistics	HPC	24.82690651291686	-36.71028787682095	169085
fd8c4a0f82dc51bd29068615bf7116be8d03c575	multiple health phases based remaining useful lifetime prediction on bearings		Bearings are key components for all industrial machinery systems. The health status of bearings has great impact on the performance of rotating machineries. Remaining useful lifetime (RUL) estimation on bearings can effectively improve the reliability and availability of industrial machineries. In this paper, a multiple health phases based method is proposed for RUL estimation with application to bearings. Bags of word is brought into the method to model the time-frequency domain features of bearing vibration signals. Besides that, a gaussian mixture model is utilized to model the lifetime of various bearings to build accurate lifetime prediction model. Finally, the experiments demonstrate that the proposed method achieves a good performance comparing with other existing methods.		Junjie Chen;Xiaofeng Wang;Wenjing Zhou;Fei Liu	2016		10.1007/978-3-319-41561-1_9	mixture model;control engineering;bag-of-words model;bearing (mechanical);computer science;vibration	HCI	36.82395336200378	-30.14829744152863	169095
4f81b025bfa1e8d1d672ddba7ca75207f81167a2	likelihood-free bayesian inference for α-stable models	multivariate models;likelihood free inference;approximate bayesian computation;bayesian approach;stable models;bayesian inference;heavy tail;statistical computing;signal processing;exchange rate;stable distribution;α stable distributions;α	@a-stable distributions are utilized as models for heavy-tailed noise in many areas of statistics, finance and signal processing engineering. However, in general, neither univariate nor multivariate @a-stable models admit closed form densities which can be evaluated pointwise. This complicates the inferential procedure. As a result, @a-stable models are practically limited to the univariate setting under the Bayesian paradigm, and to bivariate models under the classical framework. A novel Bayesian approach to modelling univariate and multivariate @a-stable distributions is introduced, based on recent advances in ''likelihood-free'' inference. The performance of this procedure is evaluated in 1, 2 and 3 dimensions, and through an analysis of real daily currency exchange rate data. The proposed approach provides a feasible inferential methodology at a moderate computational cost.		Gareth W. Peters;Scott A. Sisson;Yanan Fan	2012	Computational Statistics & Data Analysis	10.1016/j.csda.2010.10.004	bayesian average;econometrics;multivariate statistics;statistical inference;fiducial inference;bayesian experimental design;variable-order bayesian network;frequentist inference;heavy-tailed distribution;bayesian probability;stable distribution;machine learning;signal processing;mathematics;bayesian linear regression;bayesian statistics;bayesian inference;dynamic bayesian network;statistics	ML	28.24648598319808	-26.6611065796519	169284
f1337827f4c640efe70a926c1b09208af12ccf1c	test of simonds' method in factor analysis by digital simulation models	factor analysis;digital simulation	Abstract-Digital simulation models have been applied in order to check whether Simonds’ method in L~ctor analysis enables us to recover the shapes of vectors and their coefficients used in generating synthetic data matrixes. It has been found that the results obtained by Simonds’ method should be treated with caution because in a number of cases agreement between the initial and recovered shapes of vectors and their coefficients was rather poor. It has been demonstrated that Simonds’ method shows a tendency to overestimate one factor on account of the other.	coefficient;constructor (object-oriented programming);factor analysis;simulation;synthetic data	Ülo Haldna;A. Murshak	1983	Computers & Chemistry	10.1016/0097-8485(83)80004-7	biology;econometrics;simulation;computer science;mathematics;factor analysis;statistics	Comp.	33.03338138746266	-25.45801888670187	169311
2e2d2da025695b11053a2188809bd37d52c26341	tensor principal component analysis via convex optimization	15a18;15a69;tensor;15a03;principal component analysis;nuclear norm;62h25;90c22;low rank;semidefinite programming relaxation	This paper is concerned with the computation of the principal components for a general tensor, known as the tensor principal component analysis (PCA) problem. We show that the general tensor PCA problem is reducible to its special case where the tensor in question is supersymmetric with an even degree. In that case, the tensor can be embedded into a symmetric matrix. We prove that if the tensor is rank-one, then the embedded matrix must be rankone too, and vice versa. The tensor PCA problem can thus be solved by means of matrix optimization under a rank-one constraint, for which we propose two solution methods: (1) imposing a nuclear norm penalty in the objective to enforce a low-rank solution; (2) relaxing the rank-one constraint by Semidefinite Programming. Interestingly, our experiments show that both methods yield a rank-one solution with high probability, thereby solving the original tensor PCA problem to optimality with high probability. To further cope with the size of the resulting convex optimization models, we propose to use the alternating direction method of multipliers, which reduces significantly the computational efforts. Various extensions of the model are considered as well.	augmented lagrangian method;bioinformatics;british informatics olympiad;compressed sensing;computation;convex optimization;degree (graph theory);embedded system;experiment;first-order predicate;image processing;low-rank approximation;machine learning;mathematical optimization;principal component analysis;procedural generation;semidefinite programming;taxicab geometry;with high probability	Bo Jiang;Necdet Serhat Aybat;Shuzhong Zhang	2015	Math. Program.	10.1007/s10107-014-0774-0	tensor product;symmetric tensor;mathematical optimization;sparse pca;combinatorics;mathematical analysis;tensor field;tensor;matrix norm;cartesian tensor;tensor;tensor contraction;mathematics;tensor product of hilbert spaces;principal component analysis	ML	26.710589385845942	-37.29308082708873	169403
72cd7a791060e05b0607265141c3aadaf9e6193b	interest level estimation of items via matrix completion based on adaptive user matrix construction		This paper presents a novel method for interest level estimation of items via matrix completion based on adaptive user matrix construction. The proposed method introduces a new criterion for adaptively constructing a user matrix that consists of user behavior features and interest levels, which are evaluated by target users and similar users. In the estimation, the matrix completion via rank minimization using the truncated nuclear norm is applied to the constructed matrix. The proposed method enables both of the interest level estimation of the target users and the selection of the similar users suitable for the estimation by monitoring errors caused in the matrix completion algorithm. The caused errors indicate the minimum differences between the estimated interest levels and true ones, and they can be regarded as the criterion for both of the optimal estimation and the adaptive selection. Furthermore, the proposed method uses weight matrices for decreasing an influence of missing data on the estimation. Consequently, accurate estimation of the interest levels becomes feasible by using the adaptively constructed matrix. Experimental results obtained by applying the proposed method to users' behavior and interest data show the effectiveness of the proposed method.	algorithm;missing data;the matrix;weight function	Tetsuya Kushima;Sho Takahashi;Takahiro Ogawa;Miki Haseyama	2018	2018 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2018.8486598	artificial intelligence;skeleton (computer programming);matrix completion;computer science;pattern recognition;feature extraction;missing data;matrix (mathematics);matrix norm;optimal estimation	Robotics	31.972780196730707	-37.72625893066489	169424
b0f65bfce3f4e078a56366a5202675287a2fb78c	fast hidden markov model map-matching for sparse and noisy trajectories	hidden markov models global positioning system;hidden markov model;gps processing;data analysis;global positioning system;algorithms;optimization;global positioning system roads routing trajectory hidden markov models optimization viterbi algorithm;optimization data analysis gps processing map matching hidden markov model;markov processes;map matching;identical map matching fast hidden markov model map matching noisy trajectories sparse trajectories noisy gps trajectories road networks hmm plausible road sequence noisy data sparse data computational complexity extended search radii state transitions;vehicle trajectories	The problem of map-matching sparse and noisy GPS trajectories to road networks has gained increasing importance in recent years. A common state-of-the-art solution to this problem relies on a Hidden Markov Model (HMM) to identify the most plausible road sequence for a given trajectory. While this approach has been shown to work well on sparse and noisy data, the algorithm has a high computational complexity and becomes slow when working with large trajectories and extended search radii. We propose an optimization to the original approach which significantly reduces the number of state transitions that need to be evaluated in order to identify the correct solution. In experiments with publicly available benchmark data, the proposed optimization yields nearly identical map-matching results as the original algorithm, but reduces the algorithm runtime by up to 45%. We demonstrate that the effects of our optimization become more pronounced when dealing with larger problem spaces and indicate how our approach can be combined with other recent optimizations to further reduce the overall algorithm runtime.	algorithm;benchmark (computing);computational complexity theory;experiment;global positioning system;hidden markov model;map matching;markov chain;mathematical optimization;signal-to-noise ratio;sparse matrix	Hannes Koller;Peter Widhalm;Melitta Dragaschnig;Anita Graser	2015	2015 IEEE 18th International Conference on Intelligent Transportation Systems	10.1109/ITSC.2015.411	forward algorithm;mathematical optimization;viterbi algorithm;machine learning;hidden semi-markov model;pattern recognition;mathematics;markov model;hidden markov model;variable-order markov model	Robotics	24.661693404648496	-29.293362468597465	169443
9f24940a99b81367ca1bc9c9091d40dafb39940f	estimation of the joint probability of multisensory signals	maximum mutual information;maximum entropy principle;dimension reduction;the maximum entropy principle;canonical correlation analysis;linear transformation;information fusion;multisensory information fusion;maximum entropy;joint probability	This paper presents a novel method for estimation of the joint probability of multisensory signals by introducing dimension-reduction mapping functions based on the principle of maximum entropy. A maximum mutual information criterion is derived for selecting the desired mapping functions. An algorithm is further presented for linear transformations of Gaussian random vectors. Experimental results are shown to demonstrate the performance of the proposed method.		Hao Pan;Zhi-Pei Liang;Thomas S. Huang	2001	Pattern Recognition Letters	10.1016/S0167-8655(01)00080-0	joint entropy;binary entropy function;transfer entropy;maximum entropy probability distribution;principle of maximum entropy;machine learning;pattern recognition;mathematics;maximum entropy thermodynamics;joint quantum entropy;kullback–leibler divergence;mutual information;maximum entropy spectral estimation;total correlation;conditional entropy;statistics	Vision	32.84495218187225	-32.555829295660125	169725
74c7672f76e562f616f5d7d28fbf43d0bd1b3aa2	sustainable ℓ2-regularized actor-critic based on recursive least-squares temporal difference learning		Least-squares temporal difference learning (LSTD) has been used mainly for improving the data efficiency of the critic in actor-critic (AC). However, convergence analysis of the resulted algorithms is difficult when policy is changing. In this paper, a new AC method is proposed based on LSTD under discount criterion. The method comprises two components as the contribution: (1) LSTD works in an on-policy way to achieve a good convergence property of AC. (2) A sustainable ℓ2-regularization version of recursive LSTD, which is termed as RRLSTD, is proposed to solve the ℓ2-regularization problem of the critic in AC. To reduce the computation complexity of RRLSTD, we propose a fast version that is termed as FRRLSTD. Simulation results show that RRLSTD/FRRLSTD-based AC methods have better learning efficiency and faster convergence rate than conventional AC methods.	actor model;algorithm;computation;rate of convergence;recursion;recursive least squares filter;simulation;temporal difference learning	Luntong Li;Dazi Li;Tianheng Song	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122892	machine learning;rate of convergence;recursive least squares filter;artificial intelligence;temporal difference learning;function approximation;approximation algorithm;markov process;computer science;regularization (mathematics);computational complexity theory	Robotics	25.341051295674447	-31.674103796412208	169768
31cbc51a4a441d2ca6668e0e0614f5abfcb88194	nonparametric empirical bayes for the dirichlet process mixture model	nonparametric method;dirichlet process prior;empirical bayes;dirichlet process;point estimation;clusetring;gibbs sampling;polya urn schemes;density estimation;maximum likelihood estimate;mixture model;dirichlet process mixture;kernel density estimate;bayesian nonparametrics;dirichlet process mixture model;kernel density estimates	The Dirichlet process prior allows flexible nonparametric mixture modeling. The number of mixture components is not specified in advance and can grow as new data come in. However, the behavior of the model is sensitive to the choice of the parameters, including an infinite-dimensional distributional parameter G0. Most previous applications have either fixedG0 as a member of a parametric family or treated G0 in a Bayesian fashion, using parametric prior specifications. In contrast, we have developed an adaptive nonparametric method for constructing smooth estimates of G0. We combine this method with a technique for estimating α, the other Dirichlet process parameter, that is inspired by an existing characterization of its maximum-likelihood estimator. Together, these estimation procedures yield a flexible empirical Bayes treatment of Dirichlet process mixtures. Such a treatment is useful in situations where smooth point estimates of G0 are of intrinsic interest, or where the structure of G0 cannot be conveniently modeled with the usual parametric prior families. Analysis of simulated and real-world datasets illustrates the robustness of this approach.	mixture model	Jon D. McAuliffe;David M. Blei;Michael I. Jordan	2006	Statistics and Computing	10.1007/s11222-006-5196-2	latent dirichlet allocation;kernel density estimation;econometrics;concentration parameter;density estimation;gibbs sampling;point estimation;pattern recognition;mixture model;mathematics;maximum likelihood;statistics;hierarchical dirichlet process	ML	30.499223511761013	-25.98774634035681	169822
4afefc337c25d9758f4fcdd8288ba9478822881c	kernel-based structural equation models for topology identification of directed networks	topology;brain models;additives;network topology mathematical model numerical analysis brain models topology additives;network topology;numerical analysis;mathematical model;kernel based models structural equation models nonlinear modeling network topology inference	Structural equation models (SEMs) have been widely adopted for inference of causal interactions in complex networks. Recent examples include unveiling topologies of hidden causal networks over which processes, such as spreading diseases, or rumors propagate. The appeal of SEMs in these settings stems from their simplicity and tractability, since they typically assume linear dependencies among observable variables. Acknowledging the limitations inherent to adopting linear models, the present paper put forth nonlinear SEMs, which account for (possible) nonlinear dependencies among network nodes. The advocated approach leverages kernels as a powerful encompassing framework for nonlinear modeling, and an efficient estimator with affordable tradeoffs is put forth. Interestingly, pursuit of the novel kernel-based approach yields a convex regularized estimator that promotes edge sparsity, a property exhibited by most real world networks, and the resulting optimization problem is amenable to proximal-splitting optimization methods. To this end, solvers with complementary merits are developed by leveraging the alternating direction method of multipliers, and proximal gradient iterations. Experiments conducted on simulated data demonstrate that the novel approach outperforms linear SEMs with respect to edge detection errors. Furthermore, tests on a real gene expression dataset unveil interesting new edges that were not revealed by linear SEMs, which could shed more light on regulatory behavior of human genes.	augmented lagrangian method;causal filter;complex network;edge detection;interaction;iteration;kernel (operating system);linear logic;linear model;mathematical optimization;nonlinear system;observable;optimization problem;proximal gradient methods for learning;sparse matrix;structural equation modeling	Yanning Shen;Brian Baingana;Georgios B. Giannakis	2017	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2664039	mathematical optimization;combinatorics;numerical analysis;food additive;computer science;machine learning;mathematical model;mathematics;network topology;statistics	Vision	27.660296526228127	-33.31575969569407	169872
319db1747241e49112d7d533077f73589c50ca8d	an svm-like approach for expectile regression	expectile regression;support vector machines;asymmetric least square loss	Expectile regression is a nice tool for investigating conditional distributions beyond the conditional mean. It is well-known that expectiles can be described with the help of the asymmetric least square loss function, and this link makes it possible to estimate expectiles in a non-parametric framework by a support vector machine like approach. In this work we develop an efficient sequential-minimal-optimization-based solver for the underlying optimization problem. The behavior of the solver is investigated by conducting various experiments and the results are compared with the recent R-package ER-Boost.	experiment;loss function;mathematical optimization;optimization problem;sequential minimal optimization;solver;support vector machine	Muhammad Farooq;Ingo Steinwart	2017	Computational Statistics & Data Analysis	10.1016/j.csda.2016.11.010	support vector machine;econometrics;machine learning;mathematics;statistics	ML	26.19442233042346	-32.64871876538709	170039
0b7655a02e6792eabccd5d7726bc0993179f05df	a modified support vector data description based novelty detection approach for machinery components	one class classification;support vector data description;novelty detection;parameter selection;bearing	Novelty detection is an important issue for practical industrial application, in which there is only normal operating data available in most cases. This paper proposes a systematic approach for novelty detection of mechanical components, using support vector data description (SVDD), a kernel approach for modeling the support of a distribution. To reduce the false alarm rate and increase the detection accuracy, a parameter optimization estimation scheme is proposed based on a grid search method that relies on the performance trade-off between the minimum fraction of support vectors and the maximum dual eywords: ovelty detection upport vector data description ne-class classification arameter selection earing problem objective value. An evaluation value (E-value) chart based on the kernel distance for detection result is also designed to facilitate the decision visualization. To illustrate the effectiveness of the proposed method, novelty detection was applied to a particular kind of tapered roller bearing used in an industrial robot, which is investigated as a case study. The experimental results, in comparison to other methods, demonstrate that the proposed SVDD can conduct novelty detection of the monitored mechanical component effectively with higher accuracy. © 2012 Elsevier B.V. All rights reserved.	industrial robot;kernel (operating system);mathematical optimization;ne (complexity);novelty detection	Shijin Wang;Jianbo Yu;Edzel Lapira;Jay Lee	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.11.005	bearing;machine learning;pattern recognition;data mining;mathematics;one-class classification	AI	37.2977046899225	-30.337976513799596	170333
b8c08706a5a48664f1ace4db980fa2d9f1d97b51	poisson multi-bernoulli mapping using gibbs sampling	clutter;standards;time measurement;uncertainty;radio frequency;simultaneous localization and mapping	This paper addresses the mapping problem. Using a conjugate prior form, we derive the exact theoretical batch multiobject posterior density of the map given a set of measurements. The landmarks in the map are modeled as extended objects, and the measurements are described as a Poisson process, conditioned on the map. We use a Poisson process prior on the map and prove that the posterior distribution is a hybrid Poisson, multi-Bernoulli mixture distribution. We devise a Gibbs sampling algorithm to sample from the batch multiobject posterior. The proposed method can handle uncertainties in the data associations and the cardinality of the set of landmarks, and is parallelizable, making it suitable for large-scale problems. The performance of the proposed method is evaluated on synthetic data and is shown to outperform a state-of-the-art method.	algorithm;bernoulli polynomials;gibbs sampling;sampling (signal processing);synthetic data	Maryam Fatemi;Karl Granström;Lennart Svensson;Francisco J. R. Ruiz;Lars Hammarstrand	2017	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2675866	econometrics;uncertainty;computer science;machine learning;mathematics;clutter;radio frequency;statistics;time;simultaneous localization and mapping	ML	38.884068770807446	-26.723409379333802	170420
aa57647a3f6b876e7c88e28a139152f6b3210f2c	bayesian inference via rejection filtering		We provide a method for approximating Bayesian inference using rejection sampling. We not only make the process efficient, but also dramatically reduce the memory required relative to conventional methods by combining rejection sampling with particle filtering. We also provide an approximate form of rejection sampling that makes rejection filtering tractable in cases where exact rejection sampling is not efficient. Finally, we present several numerical examples of rejection filtering that show its ability to track time dependent parameters in online settings and also benchmark its performance on MNIST classification problems.	approximation algorithm;benchmark (computing);cobham's thesis;collaborative filtering;mnist database;numerical analysis;particle filter;rejection sampling;sampling (signal processing)	Nathan Wiebe;Christopher E. Granade;Ashish Kapoor;Krysta Marie Svore	2015	CoRR		econometrics;computer science;machine learning;statistics	ML	25.30047664503249	-31.36878735441081	170580
c6b13fea1644ec540a22fd57e85e3d2fbdcc059e	data visualization in engineering pedagogy through determination of colour variance in contaminated grass samples		Big Data and Data Analytics have in recent times become important areas of focus in academia, in business and in society. This paper utilises experiments involving data visualisation of oil pollution studies and their effects on environment for enhanced learning in engineering education. Tracking and analysis of images and the use of accessible applications for the analysis of acquired data revealed the level of impact of the different types of oil pollution on grass vegetation. In accounting for these changes the primary RGB colours and corresponding values are used. The use of spectral analysis applications available in spectroscopy and comparison of results would in future prove useful in assessing some aspects of these changes in relation to wavelength and colours changes. The results of these studies would contribute in no small measure to the determination of best cleaning strategies for oil spills.	data visualization	Conor White;James Uhomoibhi	2018	iJEP		engineering;vegetation;pedagogy;contamination;data visualization;big data;data analysis	Visualization	36.63957256200329	-37.32145898227212	171097
522b920b9797244209d949f58f42a0368fef566f	dynamic structure embedded online multiple-output regression for streaming data	data models;linear programming;correlation;current measurement;prediction algorithms;load modeling	Online multiple-output regression is an important machine learning technique for modeling, predicting, and compressing multi-dimensional correlated data streams. In this paper, we propose a novel online multiple-output regression method, called MORES, for streaming data. MORES can dynamically learn the structure of the regression coefficients to facilitate the model’s continuous refinement. Considering that limited expressive ability of regression models often leading to residual errors being dependent, MORES intends to dynamically learn and leverage the structure of the residual errors to improve the prediction accuracy. Moreover, we introduce three modified covariance matrices to extract necessary information from all the seen data for training, and set different weights on samples so as to track the data streams’ evolving characteristics. Furthermore, an efficient algorithm is designed to optimize the proposed objective function, and an efficient online eigenvalue decomposition algorithm is developed for the modified covariance matrix. Finally, we analyze the convergence of MORES in certain ideal condition. Experiments on two synthetic datasets and three real-world datasets validate the effectiveness and efficiency of MORES. In addition, MORES can process at least 2,000 instances per second (including training and testing) on the three real-world datasets, more than 12 times faster than the state-of-the-art online learning algorithm.	convergence (action);eigenvalue;embedding;increment;numerous;regression analysis;sensorineural hearing loss (disorder);silo (dataset);weight;algorithm	Changsheng Li;Fan Wei;Weishan Dong;Xiangfeng Wang;Qingshan Liu;Xin Zhang	2018	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2018.2794446	regression analysis;residual;computer science;data stream mining;covariance matrix;machine learning;data modeling;data mining;linear regression;covariance;artificial intelligence;linear programming	ML	27.83139153467044	-35.61665818012155	171103
2e0b455e8a3818a8c6f9398b8ba5b7c975c0fdb9	a real-time power quality disturbances classification using hybrid method based on s-transform and dynamics	dynamics dyn;s transform st;power quality;field signal tests real time power quality disturbances classification pqd hybrid method s transform classification accuracy signal components inverse fourier transform heisenberg uncertainty gaussian windows dsp fpga based hardware platform;pattern classfication;s transform st dynamics dyn pattern classification power quality real time analysis;power supply quality;real time analysis;journal;期刊论文;fourier transforms;pattern classification;power supply quality field programmable gate arrays fourier transforms;s transform;field programmable gate arrays	This paper proposes a real-time power quality disturbances (PQDs) classification by using a hybrid method (HM) based on S-transform (ST) and dynamics (Dyn). Classification accuracy and run time are mainly considered in our work. The HM firstly uses Dyn to identify the location of the signal components in the frequency spectrum yielded by Fourier transform, and uses inverse Fourier transform to only some of the signal components. Then features from Fourier transform, ST, and Dyn are selected, and a decision tree is used to classify the types of PQD. In order to reduce the influence of Heisenberg' s uncertainty, we proposed that different signal components are windowed by different Gaussian windows, which brings better adaption and flexibility. By the HM, run time of the application has been greatly reduced with satisfactory classification accuracy. Finally, a DSP-FPGA based hardware platform is adopted to test the run time and correctness of the proposed method under real standard signals. Field signal tests have also presented. Both simulations and experiments validate the feasibility of the new method.	correctness (computer science);decision tree;digital signal processor;electric power quality;embedded system;experiment;field-programmable gate array;microsoft windows;potential energy surface;real-time clock;real-time computing;real-time transcription;run time (program lifecycle phase);s transform;sampling (signal processing);scalability;simulation;spectral density;window function	Shunfan He;Kaicheng Li;Ming Zhang	2013	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2013.2258761	embedded system;fourier transform;constant q transform;electronic engineering;speech recognition;s transform;harmonic wavelet transform;short-time fourier transform;engineering;discrete fourier transform;mathematics;spectral density estimation;field-programmable gate array	Robotics	37.955028510976945	-31.03443877994233	171247
d75166cec74e861c8c5063eac9a6c20d4f759d9c	incremental relevance sample-feature machine: a fast marginal likelihood maximization approach for joint feature selection and classification	sparse bayesian learning;bayesian inference;embedded feature selection methods;fast marginal likelihood maximization;relevance sample feature machine rsfm	The recently proposed Relevance Sample-Feature Machine (RSFM) performs joint feature selection and classification with state-of-the-art performance in terms of accuracy and sparsity. However, it suffers from high computational cost for large training sets. To accelerate its training procedure, we introduce a new variant of this algorithm named Incremental Relevance Sample-Feature Machine (IRSFM). In IRSFM, the marginal likelihood maximization approach is changed such that the model learning follows a constructive procedure (starting with an empty model, it iteratively adds or omits basis functions to construct the learned model). Our extensive experiments on various data sets and comparison with various competing algorithms demonstrate the effectiveness of the proposed IRSFM in terms of accuracy, sparsity and run-time. While the IRSFM achieves almost the same classification accuracy as the RSFM, it benefits from sparser learned model both in sample and feature domains and much less training time than RSFM especially for large data sets.	algorithmic efficiency;basis function;expectation–maximization algorithm;experiment;feature selection;marginal model;relevance;sparse matrix;statistical classification	Yalda Mohsenzadeh;Hamid Sheikhzadeh;Sobhan Nazari	2016	Pattern Recognition	10.1016/j.patcog.2016.06.028	computer science;machine learning;pattern recognition;data mining;mathematics;bayesian inference;statistics	ML	25.166921769288866	-35.20166202744278	171256
89bd8213bc81c15fc265af3c9e79c597f3096ae4	adaptive conditional feature screening	adaptability;marginal utility;期刊论文;high dimensional data;conditional feature screening;model free	When the correlation among the predictors is relatively strong and/or themodel structures cannot be specified, the construction of adaptive feature screening remains a challenging issue. A general technique of conditional feature screening is proposed via combining a model-free feature screening with a predetermined set of predictors. The proposed centralization technique can remove the irrelevant part from the criterion of the modelfree feature screening. Consequently, the new criterion can measure the marginal utilities of predictors conditional on the predetermined set of predictors. The conditional information about these predetermined predictors helps reducing the correlation among covariates and as a result the resulting method can reduce the false positive and the false negative rates in the variable selection procedure. Thus, our method is adaptive to both the correlation among the covariates and the model misspecification. The new procedures are computationally efficient and simple, and can be extended to other relevant methods. © 2015 Elsevier B.V. All rights reserved.	algorithmic efficiency;conditional entropy;feature selection;marginal model;relevance	Lu Lin;Jing Sun	2016	Computational Statistics & Data Analysis	10.1016/j.csda.2015.09.002	adaptability;marginal utility;machine learning;pattern recognition;mathematics;statistics;clustering high-dimensional data	AI	25.086517473310636	-25.846971261423704	171264
73bcfdfdd030743d51dc5817fc1f8624fee65249	sequential independent component analysis density estimation		A problem of multivariate probability density function estimation by exploiting linear independent components analysis (ICA) is addressed. Historically, ICA density estimation was initially proposed under the name projection pursuit density estimation (PPDE) and two basic methods, named forward and backward, were published. We derive a modification of the forward PPDE method, which avoids a computationally demanding optimization involving Monte Carlo sampling of the original method. The results of the experiments show that the proposed method presents an attractive choice for density estimation, which is pronounced for a small number of training observations. Under such conditions, our method usually outperforms model-based Gaussian mixture model. We also found that our method obtained better results than the backward PPDE methods in the situation of nonfactorizable underlying density functions. The proposed method has demonstrated a competitive performance compared with the support vector machine and the extreme learning machine in some real classification tasks.	estimation theory;experiment;independent computing architecture;independent component analysis;kernel density estimation;leucaena pulverulenta;mathematical optimization;mixture model;monte carlo method;name;normal statistical distribution;projections and predictions;sampling (signal processing);scientific publication;support vector machine	Mayer Aladjem;Itamar Israeli-Ran;Maria Bortman	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2018.2791358	mixture model;artificial intelligence;independent component analysis;monte carlo method;extreme learning machine;density estimation;support vector machine;projection pursuit;computer science;pattern recognition;small number	ML	29.770928380685625	-28.545757222580267	171384
5581edfa2222b684da98e0ac73db4e60a09e7480	a comparison of error metrics for learning model parameters in bayesian knowledge tracing		In the knowledge-tracing model, error metrics are used to guide parameter estimation towards values that accurately represent students’ dynamic cognitive state. We compare several metrics, including log likelihood (LL), root mean squared error (RMSE), and area under the receiver operating characteristic curve (AUC), to evaluate which metric is most suited for this purpose. LL is commonly used as an error metric in Expectation Maximization (EM) to perform parameter estimation. RMSE and AUC have been suggested but have not been explored in depth. In order to examine the effectiveness of using each metric, we measure the correlations between the values calculated by each and the distances from the corresponding points to the ground truth. Additionally, we examine how each metric compares to the others. Our findings show that RMSE is significantly better than LL and AUC. With more knowledge of effective error metrics for estimating parameters in the knowledge-tracing model, we hope that better parameter searching algorithms can be created.	estimation theory;expectation–maximization algorithm;ground truth;ll parser;mean squared error;receiver operating characteristic;search algorithm	Asif Dhanani;Seung Yeon Lee;Phitchaya Phothilimthana;Zachary A. Pardos	2014			machine learning;pattern recognition;data mining	HCI	26.11092369281878	-25.2872898550609	171497
3a6f217c53c6e25cb2527853dd4b4714a6cd2225	nonconvex statistical optimization: minimax-optimal sparse pca in polynomial time		Sparse principal component analysis (PCA) involves nonconvex optimization for which the global solution is hard to obtain. To address this issue, one popular approach is convex relaxation. However, such an approach may produce suboptimal estimators due to the relaxation effect. To optimally estimate sparse principal subspaces, we propose a two-stage computational framework named “tighten after relax”: Within the “relax” stage, we approximately solve a convex relaxation of sparse PCA with early stopping to obtain a desired initial estimator; For the “tighten” stage, we propose a novel algorithm called sparse orthogonal iteration pursuit (SOAP), which iteratively refines the initial estimator by directly solving the underlying nonconvex problem. A key concept of this two-stage framework is the basin of attraction. It represents a local region within which the “tighten” stage has desired computational and statistical guarantees. We prove that, the initial estimator obtained from the “relax” stage falls into such a region, and hence SOAP geometrically converges to a principal subspace estimator which is minimax-optimal within a certain model class. Unlike most existing sparse PCA estimators, our approach applies to the non-spiked covariance models, and adapts to non-Gaussianity as well as dependent data settings. Moreover, through analyzing the computational complexity of the two stages, we illustrate an interesting phenomenon: Larger sample size can reduce the total iteration complexity. Our framework motivates a general paradigm for solving many complex statistical problems which involve nonconvex optimization with provable guarantees.	algorithm;computation;computational complexity theory;early stopping;feedback;handel;iteration;linear programming relaxation;mathematical optimization;minimax;p (complexity);philippe kruchten;principal component analysis;programming paradigm;provable security;quadratic programming;rate of convergence;regular language description for xml;soap;soft computing;sparse pca;sparse matrix;thresholding (image processing);time complexity	Zhaoran Wang;Huanran Lu;Han Liu	2014	CoRR		mathematical optimization;sparse pca;machine learning;sparse approximation;mathematics;statistics	ML	26.043231297162507	-35.15022488207829	171854
cd5a5f2d9e4c2bbe39df359831f0e5a330f81af0	weighted euclidean biplots	weighted least squares;info eu repo semantics workingpaper;singular value decomposition;correspondence analysis;majorization;multidimensional scaling;biplot;distance	We construct a weighted Euclidean distance that approximates any distance or dissimilarity measure between individuals that is based on a rectangular cases-by-variables data matrix. In contrast to regular multidimensional scaling methods for dissimilarity data, the method leads to biplots of individuals and variables while preserving all the good properties of dimension-reduction methods that are based on the singular-value decomposition. The main benefits are the decomposition of variance into components along principal axes, which provide the numerical diagnostics known as contributions, and the estimation of nonnegative weights for each variable. The idea is inspired by the distance functions used in correspondence analysis and in principal component analysis of standardized data, where the normalizations inherent in the distances can be considered as differential weighting of the variables. In weighted Euclidean biplots we allow these weights to be unknown parameters, which are estimated from the data to maximize the fit to the chosen distances or dissimilarities. These weights are estimated using a majorization algorithm. Once this extra weight-estimation step is accomplished, the procedure follows the classical path in decomposing the matrix and displaying its rows and columns in biplots.	algorithm;column (database);correspondence analysis;dimensionality reduction;euclidean distance;image scaling;multidimensional scaling;numerical analysis;principal component analysis;singular value decomposition;the matrix	Michael Greenacre;Patrick J. F. Groenen	2016	J. Classification	10.1007/s00357-016-9213-7	mathematical optimization;biplot;combinatorics;stress majorization;multidimensional scaling;machine learning;mathematics;correspondence analysis;singular value decomposition;distance;least squares;statistics	ML	32.97899943746368	-30.401117139568584	171997
158f75abd8d094a3246628ca90eeb83c093ba089	tree-structured gaussian process approximations		Gaussian process regression can be accelerated by constructing a small pseudodataset to summarize the observed data. This idea sits at the heart of many approximation schemes, but such an approach requires the number of pseudo-datapoints to be scaled with the range of the input space if the accuracy of the approximation is to be maintained. This presents problems in time-series settings or in spatial datasets where large numbers of pseudo-datapoints are required since computation typically scales quadratically with the pseudo-dataset size. In this paper we devise an approximation whose complexity grows linearly with the number of pseudo-datapoints. This is achieved by imposing a tree or chain structure on the pseudo-datapoints and calibrating the approximation using a Kullback-Leibler (KL) minimization. Inference and learning can then be performed efficiently using the Gaussian belief propagation algorithm. We demonstrate the validity of our approach on a set of challenging regression tasks including missing data imputation for audio and spatial datasets. We trace out the speed-accuracy trade-off for the new method and show that the frontier dominates those obtained from a large number of existing approximation techniques.	algorithm;approximation;approximation algorithm;belief propagation;computation;computational complexity theory;expectation propagation;gaussian process;geo-imputation;gradient;image scaling;kriging;kullback–leibler divergence;missing data;requirement;sampling (signal processing);software propagation;stationary process;time series;tree structure;while	Thang D. Bui;Richard E. Turner	2014			mathematical optimization;machine learning;mathematics;statistics	ML	27.12434789727611	-31.117446563193003	172044
0c4445ef67c4868739cee7742d9a509c64f34fa8	time varying undirected graphs	distributed data;metodo regularizacion;graphe non oriente;time varying;regularisation;non directed graph;covariancia;high dimensionality;ley n variables;high dimensional asymptotics;regularization method;matrice covariance;methode regularisation;covariance;multidimensional analysis;matriz covariancia;curva gauss;time varying system;regularization;l1 regularization;matrice creuse;analyse n dimensionnelle;grafo no orientado;systeme parametre variable;analisis n dimensional;loi normale;representacion parsimoniosa;multivariate distribution;regularizacion;sistema parametro variable;sparse matrix;sparse representation;loi n variables;graph selection;gaussian distribution;covariance matrix;matriz dispersa;representation parcimonieuse;risk consistency	Undirected graphs are often used to describe high dimensional distributions. Under sparsity conditions, the graph can be estimated using ℓ 1 penalization methods. However, current methods assume that the data are independent and identically distributed. If the distribution, and hence the graph, evolves over time then the data are not longer identically distributed. In this paper we develop a nonparametric method for estimating time varying graphical structure for multivariate Gaussian distributions using an ℓ 1 regularization method, and show that, as long as the covariances change smoothly over time, we can estimate the covariance matrix well (in predictive risk) even when p is large.	graph (discrete mathematics);matrix regularization;penalty method;smoothing;sparse matrix	Shuheng Zhou;John D. Lafferty;Larry A. Wasserman	2008	Machine Learning	10.1007/s10994-010-5180-0	regularization;mathematical optimization;combinatorics;mathematics;statistics	ML	27.003370805235967	-27.84585620151867	172197
2b30c8cd9663f71f6d55c9efb315ffa24b374362	novelty detection employing an l2 optimal non-parametric density estimator	technology;novelty detection;reduced set density estimator rsde;density estimation;science technology;support;artificial intelligence;binary classification;computer science;support vector machine;non parametric density estimation	This paper considers the application of a recently proposed L2 optimal nonparametric Reduced Set Density Estimator to novelty detection and binary classification and provides empirical comparisons with other forms of density estimation as well as Support Vector Machines.	binary classification;kernel density estimation;matlab;novelty detection;support vector machine;window function	Chao He;Mark A. Girolami	2004	Pattern Recognition Letters	10.1016/j.patrec.2004.05.004	binary classification;support vector machine;density estimation;support;computer science;machine learning;pattern recognition;statistics;technology	ML	30.12082721735292	-27.046367554609596	172208
456d1445aeba8c607bfe3c446a159b2b5949cea8	addressing overfitting and underfitting in gaussian model-based clustering		The expectation–maximization (EM) algorithm is a common approach for parameter estimation in the context of cluster analysis using finite mixture models. This approach suffers from the well-known issue of convergence to local maxima, but also the less obvious problem of overfitting. These combined, and competing, concerns are illustrated through simulation and then addressed by introducing an algorithm that augments the traditional EM with the nonparametric bootstrap. Further simulations and applications to real data lend support for the usage of this bootstrap augmented EM-style algorithm to avoid both overfitting and local maxima.	cluster analysis;overfitting	Jeffrey L. Andrews	2018	Computational Statistics & Data Analysis	10.1016/j.csda.2018.05.015	statistics;overfitting;mixture model;expectation–maximization algorithm;cluster analysis;bootstrapping (electronics);maxima and minima;mathematics;algorithm;estimation theory;nonparametric statistics	ML	29.805876861998648	-26.810415768397768	172312
98657f1cabb0ad52dd9bf6f8a338f91e453f88b7	a bayesian model for collaborative filtering		Consider the general setup where a set of items have been partially rated by a set of judges, in the sense that not every item has been rated by every judge. For this setup, we propose a Bayesian approach for the problem of predicting the missing ratings from the observed ratings. This approach incorporates similarity by assuming the set of judges can be partitioned into groups which share the same ratings probability distribution. This leads to a predictive distribution of missing ratings based on the posterior distribution of the groupings and associated ratings probabilities. Markov chain Monte Carlo methods and a hybrid search algorithm are then used to obtain predictions of the missing ratings.	bayesian network;collaborative filtering;markov chain monte carlo;monte carlo method;search algorithm	Yung-Hsin Chien;Edward I. George	1999			machine learning;collaborative filtering;artificial intelligence;bayesian programming;computer science;recursive bayesian estimation;variable-order bayesian network;bayesian linear regression;bayesian statistics;bayesian hierarchical modeling;pattern recognition;graphical model	Web+IR	28.314204131432085	-31.356604458293738	172425
c966a33edab9220208e992108c302404a2deed1b	two time-efficient gibbs sampling inference algorithms for biterm topic model	biterm topic model;topic model;latent dirichlet allocation;gibbs sampling	Biterm Topic Model (BTM) is an effective topic model proposed to handle short texts. However, its standard gibbs sampling inference method (StdBTM) costs much more time than that (StdLDA) of Latent Dirichlet Allocation (LDA). To solve this problem we propose two time-efficient gibbs sampling inference methods, SparseBTM and ESparseBTM, for BTM by making a tradeoff between space and time consumption in this paper. The idea of SparseBTM is to reduce the computation in StdBTM by both recycling intermediate results and utilizing the sparsity of count matrix NWT $\mathbf {N}^{\mathbf {T}}_{\mathbf {W}}$. Theoretically, SparseBTM reduces the time complexity of StdBTM from O(|B| K) to O(|B| K w ) which scales linearly with the sparsity of count matrix NWT $\mathbf {N}^{\mathbf {T}}_{\mathbf {W}}$ (K w ) instead of the number of topics (K) (K w < K, K w is the average number of non-zero topics per word type in count matrix NWT $\mathbf {N}^{\mathbf {T}}_{\mathbf {W}}$). Experimental results have shown that in good conditions SparseBTM is approximately 18 times faster than StdBTM. Compared with SparseBTM, ESparseBTM is a more time-efficient gibbs sampling inference method proposed based on SparseBTM. The idea of ESparseBTM is to reduce more computation by recycling more intermediate results through rearranging biterm sequence. In theory, ESparseBTM reduces the time complexity of SparseBTM from O(|B|K w ) to O(R|B|K w ) (0 < R < 1, R is the ratio of the number of biterm types to the number of biterms). Experimental results have shown that the percentage of the time efficiency improved by ESparseBTM on SparseBTM is between 6.4% and 39.5% according to different datasets.	algorithm;computation;gibbs sampling;latent dirichlet allocation;sampling (signal processing);sparse matrix;time complexity;topic model	Xiaotang Zhou;Jihong OuYang;Ximing Li	2017	Applied Intelligence	10.1007/s10489-017-1004-2	artificial intelligence;time complexity;computer science;latent dirichlet allocation;machine learning;topic model;spacetime;inference;matrix (mathematics);gibbs sampling	AI	25.8537974362784	-32.896156920041406	172426
2283bf8c4ecee52c2ae462f383527780953bb67a	approximate inference a lgorithms for two-layer bayesian networks	bayesian network;convergence rate;graphical model;approximate inference	We present a class of approximate inference algorithms for graphical models of the QMR-DT type. We give convergence rates for these algorithms and for the Jaakkola and Jordan (1999) algorithm, and verify these theoretical predictions empirically. We also present empirical results on the difficult QMR-DT network problem, obtaining performance of the new algorithms roughly comparable to the Jaakkola and Jordan algorithm.	approximation algorithm;bayesian network;graphical model	Andrew Y. Ng;Michael I. Jordan	1999			econometrics;computer science;machine learning;bayesian network;mathematics;graphical model;rate of convergence;statistics	ML	24.89594163238544	-30.375543859084424	172698
f20dab597d45a16b60a44ddf66c5666ac086ce39	multiscale pmu data compression based on wide-area event detection		In this paper, a multiscale compression process for phasor a measurement unit (PMU) is proposed using a wide-area event detection method. For the first step, the data compression intervals are adaptively selected by monitoring the average of modified wavelet energy (AMWE) in order to reflect two different operating conditions of power system; i.e., ambient and event. In the next step, the interval-selected dataset is compressed by a multiscale dimensionality reduction process. The dimensionality reduction step uses wavelet decomposition to reflect non-stationary characteristics and extract time-varying features from the PMU signals. The principal component analysis is then applied to the wavelet-decomposed matrices for data compression. The effectiveness of the proposed method was confirmed by application to the real-world PMU voltage and frequency data, and comparisons are made with the conventional wavelet compression technique.	cluster analysis;data compression;dimensionality reduction;discrete wavelet transform;multiscale modeling;phasor;power management unit;principal component analysis;robustness (computer science);stationary process;wavelet transform	Gyul Lee;Yong-June Shin	2017	2017 IEEE International Conference on Smart Grid Communications (SmartGridComm)	10.1109/SmartGridComm.2017.8340705	wavelet transform;real-time computing;wavelet;principal component analysis;dimensionality reduction;compression (physics);data compression;engineering;phasor;matrix decomposition;artificial intelligence;pattern recognition	Robotics	35.639036511078395	-31.383750909199176	172785
2d01567a9e8f8f0c53ad22df105cbefa6b7bd22b	evolutionary spectral based classification of machine vibrations	equipo electrico;detection panne;failure detection;signal analysis;vibracion;analisis de senal;electrical equipment;analyse frequence temps;vibration;classification automatique;reseau neuronal;automatic classification;equipement electrique;deteccion falla;clasificacion automatica;time frequency analysis;red neuronal;analyse signal;neural network	In this paper, we present a method for time-frequency analysis and classification of electrical machine vibrations based on evolutionary spectrum. Time-frequency analysis provides a means for identifying changes in the vibrations produced by machines. In this work, vibrations generated by different electrical machines are recorded by using accelerometers during operation. Three categories of these vibrations are then defined based on their evolutionary spectra: 1) the normal operation, 2) small level of failure, and 3) high level of failure. A method is presented for automatic detection and classification of abnormality in these machine vibrations using their joint time-frequency moments and neural networks. Simulation results are given to illustrate our algorithm.	algorithm;artificial neural network;frequency analysis;high-level programming language;simulation;time–frequency analysis	Nihat Kabaoglu;Aydin Akan	2000	2000 10th European Signal Processing Conference		speech recognition;engineering;artificial intelligence;algorithm	HPC	36.40536908932381	-31.4443543629637	173411
ec0aad2b7b4796dd9e3b3798e0f9a2346f1d43b1	virel: a variational inference framework for reinforcement learning		Applying probabilistic models to reinforcement learning (RL) has become an exciting direction of research owing to powerful optimisation tools such as variational inference becoming applicable to RL. However, due to their formulation, existing inference frameworks and their algorithms pose significant challenges for learning optimal policies, for example, the absence of mode capturing behaviour in pseudo-likelihood methods and difficulties in optimisation of learning objective in maximum entropy RL based approaches. We propose VIREL, a novel, theoretically grounded probabilistic inference framework for RL that utilises the action-value function in a parametrised form to capture future dynamics of the underlying Markov decision process. Owing to its generality, our framework lends itself to current advances in variational inference. Applying the variational expectation-maximisation algorithm to our framework, we show that the actor-critic algorithm can be reduced to expectation-maximisation. We derive a family of methods from our framework, including state-of-the-art methods based on soft value functions. We evaluate two actor-critic algorithms derived from this family, which perform on par with soft actor critic, demonstrating that our framework offers a promising perspective on RL as inference.		Matthew Fellows;Anuj Mahajan;Tim G. J. Rudner;Shimon Whiteson	2018	CoRR			ML	25.99021015185452	-31.804523732678987	173843
198769add6fa204c18e80185e5ec8511367200f7	an information geometry of statistical manifold learning	serveur institutionnel;archive institutionnelle;open access;archive ouverte unige;cybertheses;institutional repository	Manifold learning seeks low-dimensional representations of high-dimensional data. The main tactics have been exploring the geometry in an input data space and an output embedding space. We develop a manifold learning theory in a hypothesis space consisting of models. A model means a specific instance of a collection of points, e.g., the input data collectively or the output embedding collectively. The semi-Riemannian metric of this hypothesis space is uniquely derived in closed form based on the information geometry of probability distributions. There, manifold learning is interpreted as a trajectory of intermediate models. The volume of a continuous region reveals an amount of information. It can be measured to define model complexity and embedding quality. This provides deep unified perspectives of manifold	akaike information criterion;binary prefix;convex function;convex optimization;data visualization;dataspaces;encode;fisher information;gradient;information geometry;input/output;international conference on machine learning;journal of machine learning research;manifold regularization;mathematical optimization;nonlinear dimensionality reduction;nonlinear system;optimization problem;semiconductor industry;spectral method;statistical classification;statistical learning theory;statistical manifold;word lists by frequency;yang	Ke Sun;Stéphane Marchand-Maillet	2014			local tangent space alignment;data science;machine learning;data mining;mathematics;manifold alignment	ML	29.820686287258326	-35.48686417839989	174066
7d9968c984038348a909cc9b78f05cafb023e401	a new approach to data driven clustering	statistical model;random walk;number of clusters;spectral properties	We consider the problem of clustering in its most basic form where only a local metric on the data space is given. No parametric statistical model is assumed, and the number of clusters is learned from the data. We introduce, analyze and demonstrate a novel approach to clustering where data points are viewed as nodes of a graph, and pairwise similarities are used to derive a transition probability matrix P for a Markov random walk between them. The algorithm automatically reveals structure at increasing scales by varying the number of steps taken by this random walk. Points are represented as rows of Pt, which are the t-step distributions of the walk starting at that point; these distributions are then clustered using a KL-minimizing iterative algorithm. Both the number of clusters, and the number of steps that 'best reveal' it, are found by optimizing spectral properties of P.	algorithm;cluster analysis;data point;dataspaces;iterative method;markov chain;parametric model;statistical model;stochastic matrix	Arik Azran;Zoubin Ghahramani	2006		10.1145/1143844.1143852	correlation clustering;statistical model;determining the number of clusters in a data set;combinatorics;discrete mathematics;heterogeneous random walk in one dimension;mathematics;random walk;statistics	ML	31.25457177605275	-30.695759890109777	174560
71b7a55ea3bc35537e28c5f58bfc9d707b66049e	document marking and identification using both line and word shifting	centroid method;line centroid;correlation method;unmarked control line;line shift;illicit dissemination;word shift;maximum likelihood detector;text line;horizontal direction;waveform;vertical direction;correlation;fingerprint recognition;cryptographic protocols;distributed computing;watermarking;detectors;identification;image processing;error correction;distortion	We continue our study of document marking t o deter illicit dissemination. A n experiment we performed reveals that the distortion on the photocopy of a document is very different in the vertical and horizontal directions. This leads to the strategy that marks a text line both vertically using line shifting and horizontally using word shifting. A line that is marked is always accompanied by two unmarked control lines one above and one below. They are used t o measure distortions in the vertical and horizontal directions in order to decide whether line or word shift should be detected. Line shifts are detected using a centroid method that bases its decision on the relative distance of line centroids. Word shifts are detected using a correlation method that treats a profile as a waveform and decides whether it originated from a waveform whose middle block has been shifted left or right. The maximum likelihood detectors for both methods are given.	distortion;item unique identification;photocopier;sensor;waveform	Steven H. Low;Nicholas F. Maxemchuk;Jack Brassil;Lawrence O'Gorman	1995			identification;computer vision;detector;error detection and correction;speech recognition;waveform;distortion;telecommunications;image processing;digital watermarking;computer science;pattern recognition;cryptographic protocol;vertical direction;correlation;fingerprint recognition	AI	38.337947195082926	-35.84714873371324	174602
3def541e01e003a0a00cdad864986f70889781ef	independent vector analysis with a generalized multivariate gaussian source prior for frequency domain blind source separation	blind source separation;multivariate generalized gaussian distribution;source prior;independent vector analysis	Independent vector analysis (IVA) is designed to retain the dependency within individual source vectors, while removing the dependency between different source vectors. It can theoretically avoid the permutation problem inherent to independent component analysis (ICA). The dependency in each source vector is retained by adopting a multivariate source prior instead of a univariate source prior. In this paper, a multivariate generalized Gaussian distribution is adopted as the source prior which can exploit frequency domain energy correlation within each source vector. As such, it can utilize more information describing the dependency structure and provide improved source separation performance. This proposed source prior is suitable for the whole family of IVA algorithms and found to be more robust in applications where non-stationary signals are separated than the one preferred by Lee. Experimental results on real speech signals confirm the advantage of adopting the proposed source prior on three types of IVA algorithm. & 2014 Elsevier B.V. All rights reserved.	approximation algorithm;blind signal separation;constant term;independent computing architecture;independent component analysis;loss function;major stationary source;ng-pon2;source separation;stationary process	Yanfeng Liang;Jack Harris;Syed Mohsen Naqvi;Gaojie Chen;Jonathon A. Chambers	2014	Signal Processing	10.1016/j.sigpro.2014.05.022	speech recognition;computer science;machine learning;pattern recognition;blind signal separation;statistics	AI	30.96368795151911	-33.53737328144512	174754
08ddd7eca53336f4a55e5a3532e2afd96fa92b0a	language distribution prediction based on batch markov monte carlo simulation with migration.		Language spreading is a complex mechanism that involves issues like culture, economics, migration, population etc. In this paper, we propose a set of methods to model the dynamics of the spreading system. To model the randomness of language spreading, we propose the Batch Markov Monte Carlo Simulation with Migration(BMMCSM) algorithm, in which each agent is treated as a language stack. The agent learns languages and migrates based on the proposed Batch Markov Property according to the transition matrix T and migration matrix M. Since population plays a crucial role in language spreading, we also introduce the Mortality and Fertility Mechanism, which controls the birth and death of the simulated agents, into the BMMCSM algorithm. The simulation results of BMMCSM show that the numerical and geographic distribution of languages varies across the time. The change of distribution fits the world’s cultural and economic development trend. Next, when we construct Matrix T, there are some entries of T can be directly calculated from historical statistics while some entries of T is unknown. Thus, the key to the success of the BMMCSM lies in the accurate estimation of transition matrix T by estimating the unknown entries of T under the supervision of the known entries. To achieve this, we first construct a 20 × 20 × 5 factor tensor ?⃑? to characterize each entry of T. Then we train a Random Forest Regressor on the known entries of T and use the trained regressor to predict the unknown entries. The reason why we choose Random Forest (RF) is that, compared to Single Decision Tree, it conquers the problem of over-fitting and the Shapiro test also suggests that the residual of RF subjects to the Normal distribution.	algorithm;decision tree;fits;markov chain monte carlo;markov property;monte carlo method;numerical analysis;overfitting;population;radio frequency;random forest;randomness;simulation;stochastic matrix;system migration	XingYu Fu;ZiYi Yang;XiuWen Duan	2018	CoRR		randomness;artificial intelligence;machine learning;markov property;normal distribution;monte carlo method;stochastic matrix;computer science;birth–death process;population;algorithm;markov chain	ML	31.07037944079429	-26.05811306521793	174798
969847301283e5fdba35314ca63d646400404588	fake finger detection using an electrotactile display system	vibrations;touch sensation;skin;biometrics;fake finger;display instrumentation;tactile sensors display instrumentation fingerprint identification;electrodes;fingerprint recognition;touch sensation fake finger electrotactile;tactile perception;fingers;tactile sensors;tactile pattern fake finger detection electrotactile display system fingerprint recognition biometrics fingerprint authentication system tactile sensation tactile perception;fingers displays fingerprint recognition skin distortion measurement robotics and automation biometrics electric variables measurement blood temperature measurement;temperature measurement;electrotactile;fingerprint identification	Fingerprint recognition is the most popular biometrics but the current fingerprint authentication system can still be easily circumvented by fake fingers. In this paper, we propose a novel method to detect fake finger using tactile sensation. It relies on an additional ability of a live finger, the sense of touch. Such tactile perception capability is absent in the dead or a totally fake finger. Based on this concept, an electrotactile display system capable of presenting tactile pattern is developed. This is then correlated with the pattern shown visually to the user. The system does not produce any visible, auditory or other clues and as such, its strength is only dependent on the number of visible patterns available for the user to select. Preliminary result obtained from the prototype system shows that the proposed approach is indeed able to detect gelatin fake finger worn over live fingers, even when the gelatin is only 1mm thick.	authentication;biometrics;experiment;fingerprint recognition;machine perception;prototype;sensor;tactile imaging	Wei-Yun Yau;Hai-Linh Tran;Eam Khwang Teoh	2008	2008 10th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2008.4795648	fingerprint;computer vision;speech recognition;temperature measurement;computer science;engineering;electrode;vibration;skin;fingerprint recognition;tactile sensor;biometrics	Robotics	39.019543596280286	-36.127115307911666	175029
add4401abfc8ed115b2f049ecf34388eb679ae49	on the consistency of coordinate-independent sparse estimation with bic	central;62h20;reduction;selection;dimension;journal;variable selection;subspace;central subspace;bic;sufficient dimension reduction;coordinate independent;62j07;sufficient;consistency;variable	Chen et al. (2010) propose a unified method – coordinate-independent sparse estimation (CISE) – that is able to simultaneously achieve sparse sufficient dimension reduction and screen out irrelevant and redundant variables efficiently. However, its attractive features depend on appropriate choice of the tuning parameter. In this note, we re-examine the Bayesian information criterion (BIC) in sufficient dimension reduction and provide a heuristic derivation. Furthermore, the CISE with BIC is shown to be able to identify the true model consistently.	bayesian information criterion;dimensionality reduction;entity–relationship model;heuristic;relevance;sparse matrix;sufficient dimension reduction	Changliang Zou;Xin Chen	2012	J. Multivariate Analysis	10.1016/j.jmva.2012.04.014	sufficient dimension reduction;selection;econometrics;reduction;variable;mathematics;consistency;dimension;feature selection;statistics	AI	29.136354706013698	-25.195892911258493	175041
9734f241b9a3bc1b2ecaf644d34e787cf987b7cc	learning trans-dimensional random fields with applications to language modeling		To describe trans-dimensional observations in sample spaces of different dimensions, we propose a probabilistic model, called the trans-dimensional random field (TRF) by explicitly mixing a collection of random fields. In the framework of stochastic approximation (SA), we develop an effective training algorithm, called augmented SA, which jointly estimates the model parameters and normalizing constants while using trans-dimensional mixture sampling to generate observations of different dimensions. Furthermore, we introduce several statistical and computational techniques to improve the convergence of the training algorithm and reduce computational cost, which together enable us to successfully train TRF models on large datasets. The new model and training algorithm are thoroughly evaluated in a number of experiments. The word morphology experiment provides a benchmark test to study the convergence of the training algorithm and to compare with other algorithms, because log-likelihoods and gradients can be exactly calculated in this experiment. For language modeling, our experiments demonstrate the superiority of the TRF approach in being computationally more efficient in computing data probabilities by avoiding local normalization and being able to flexibly integrate a richer set of features, when compared with n-gram models and neural network models.	algorithm;algorithmic efficiency;artificial neural network;benchmark (computing);computation (action);convergence (action);data modeling;dimensional modeling;dimensions;estimated;experiment;futures studies;galaxy morphological classification;generative modelling language;gradient;graph (discrete mathematics);hidden variable theory;interaction;language model;log-linear model;neural network simulation;nonlinear system;normalize;open-source software;probability;radio frequency;representation (action);sampling (signal processing);sampling - surgical action;source-to-source compiler;statistical model;stochastic approximation;vocabulary;gram	Bin Wang;Zhijian Ou;Zhiqiang Tan	2018	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2017.2696536	random field;computer science;approximation algorithm;language model;artificial neural network;normalization (statistics);hidden markov model;artificial intelligence;machine learning;statistical model;stochastic approximation	ML	25.49462485815136	-30.31344477326813	175071
ad126f49182fc2519333efa428db2c76c22cf220	robustness of marginal likelihood based tests for random regression coefficients			coefficient;marginal model	Maxwell L. King;Shahidur Rahman	2015	MASA	10.3233/MAS-150325	econometrics;marginal model;marginal likelihood;pattern recognition;statistics	ML	30.49521165033743	-25.027784287878426	175196
b7c4b53e4e27147a76c619a658faf7055c44ee47	combined intelligent methods based on wireless sensor networks for condition monitoring and fault diagnosis	fuzzy logic;induction motors;gaussian mixture models;bayesian classification method;wireless sensor networks;fault diagnosis	This study presents new combined methods based on multiple wireless sensor system for real-time condition monitoring of electric machines. The established experimental setup measures multiple signals such as current and vibration on a common wireless node. The proposed methods are low-cost, intelligent, and non-intrusive. The proposed wireless network based framework is useful for analyzing and monitoring of signals from multiple induction motors. Motor current and vibration signals are simultaneously read from multiple motors through wireless nodes and the faults are estimated using two combined methods. Phase space analysis of vibration data and amplitudes of three phase current signals are used as features in combined intelligent classifiers. Stator related faults are diagnosed by analyzing the magnitudes of read current signals with fuzz logic. The vibration signal taken from the two-axis acceleration meter is normalized and phase space of this signal is constructed. The change in phase spaces are analyzed with machine learning techniques based on Gaussian Mixture Models and Bayesian classification to detect bearing faults. The phase space of vibration signals is constructed by using non-linear time series analysis and Gaussian mixtures are obtained for healthy and each faulty conditions. The constructed mixture models are classified according to their distribution on phase space by using Bayesian classification method. Four motor operating conditions- stator open phase fault, one and two bearing imbalance faults, and healthy condition are considered and related signals are obtained to evaluate the proposed system. The accuracy of the proposed system is confirmed by experimental data.		Ilhan Aydin;Mehmet Karaköse;Erhan Akin	2015	J. Intelligent Manufacturing	10.1007/s10845-013-0829-8	fuzzy logic;control engineering;electronic engineering;wireless sensor network;computer science;engineering;artificial intelligence;machine learning;mixture model;induction motor	Robotics	36.37148738026573	-31.0189267457977	175282
97de0e8d1df1f41448b3d439c8b19afc479d2dbe	detecting structural changes in dependent data		In the era of big data, a frequently encountered task is to model and identify structural changes in the data generating process. It is quite challenging especially when data are dependent and massive, requiring computationally efficient analysis. To address the challenge, we model the data generating process as a segment-wise autoregression, and propose a multi-window method that is both effective and efficient for discovering the structural changes. The proposed approach was motivated by transforming a segment-wise autoregression into a multivariate time series that is asymptotically segment-wise independent and identically distributed. We then derive theoretical guarantees for (almost surely) selecting the true number of change points of segment-wise independent multivariate time series. In particular, we prove that a wide variety of penalized selection procedure produces a strongly consistent selection of the optimal number of change points, under mild assumptions. We demonstrate the theory and strength of the proposed algorithms by experiments on both synthetic and real-world data.	algorithm;algorithmic efficiency;autoregressive model;big data;experiment;massive (software);sensor;synthetic data;time series;window function	Jie Ding;Yu Xiang;Lu Shen;Vahid Tarokh	2017	2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2017.8309060	strong consistency;big data;almost surely;independent and identically distributed random variables;multivariate statistics;autoregressive model;mathematics;artificial intelligence;pattern recognition	DB	28.18481285027051	-28.12552330317243	175335
58c74d6dde0919313d1004d807aff7ba701f397f	separability assessment of selected types of vehicle-associated noise		Music Information Retrieval (MIR) area as well as development of speech and environmental information recognition techniques brought various tools intended for recognizing low-level features of acoustic signals based on a set of calculated parameters. In this study, the MIRtoolbox MATLAB tool, designed for music parameter extraction, is used to obtain a vector of parameters to check whether they are suitable for separation of selected types of vehicle-associated noise, i.e.: car, truck and motorcycle. Then, cross-correlation between pairs of parameters is calculated. Parameters for which absolute value of cross-correlation factor is below a selected threshold, are chosen for further analysis. Subsequently, pairs of parameters found in the previous step are analyzed as a graph of low-correlated parameters with the use of the Bron-Kerbosch algorithm. Graph is checked for existence of cliques of parameters linked in all-to-all manner related to their low correlation. The largest clique of low-correlated parameters is then tested for suitability for separation into three vehicle noise classes. Behrens-Fisher statistic is used for this purpose. Results are visualized in the form of 2D and 3D scatter plots.	linear separability	Adam Kurowski;Karolina Marciniuk;Bozena Kostek	2016		10.1007/978-3-319-43982-2_10	music information retrieval;scatter plot;clique;bron–kerbosch algorithm;statistic;mathematics;absolute value;artificial intelligence;graph;pattern recognition;correlation	HCI	36.18441012704786	-34.250186869405766	175386
5597987bdda355d60da53e26939c7fab24dfb6bf	on robustness of kernel clustering		Clustering is an important unsupervised learning problem in machine learning and statistics. Among many existing algorithms, kernel k-means has drawn much research attention due to its ability to find non-linear cluster boundaries and its inherent simplicity. There are two main approaches for kernel k-means: SVD of the kernel matrix and convex relaxations. Despite the attention kernel clustering has received both from theoretical and applied quarters, not much is known about robustness of the methods. In this paper we first introduce a semidefinite programming relaxation for the kernel clustering problem, then prove that under a suitable model specification, both K-SVD and SDP approaches are consistent in the limit, albeit SDP is strongly consistent, i.e. achieves exact recovery, whereas K-SVD is weakly consistent, i.e. the fraction of misclassified nodes vanish. Also the error bounds suggest that SDP is more resilient towards outliers, which we also demonstrate with experiments.	algorithm;cluster analysis;experiment;k-svd;k-means clustering;kernel (operating system);linear programming relaxation;machine learning;nonlinear system;robustness (computer science);semidefinite programming;singular value decomposition;unsupervised learning	Bowei Yan;Purnamrita Sarkar	2016			kernel method;mathematical optimization;kernel embedding of distributions;radial basis function kernel;machine learning;mathematics;variable kernel density estimation;polynomial kernel;statistics	ML	24.835155167234284	-36.78460848984915	175462
1b1c94341820ff62b287f30c50fc1a12ca0dbb8f	san lite-solver: a user-friendly software tool to solve san models	software tool;numerical solution;stochastic automata networks;structured markovian models;mdd	Structured Markovian models are widely used to map and analyze the behavior of complex systems. However, the modeler must frequently need a deeply knowledge about specialized tools or limitations imposed on the solution of their models. This paper presents an easy and practical software tool, called SAN LITE-SOLVER, that applies the Power method to solve a Stochastic Automata Network (SAN) model, using a standard Multi-valued Decision Diagram (MDD) structure to compute and to store the model’s reachable state space (RSS) and a Harwell-Boeing format (HBF) matrix which represents the underlying Markov chain (MC). The performance analysis of this new tool (in terms of memory used and CPU time to solve models) is presented and compared to the current approach used to solve SAN models.	automaton;central processing unit;complex systems;diagram;markov chain;model-driven engineering;power iteration;programming tool;rss;solver;state space;usability	Afonso Sales	2012			simulation;computer science;artificial intelligence;theoretical computer science	Metrics	33.75737412426246	-28.939866527325556	175765
40687aabf19edc28a93a162ab13964d179ccf0bd	the multiple hypothesis tracker derived from finite set statistics		The multiple hypothesis tracker (MHT) has historically been considered a gold standard for multi-target tracking. In this paper we show that the key formula for hypothesis probabilities in Reid's MHT can be derived from the modern theory of finite set statistics (FISST) insofar as appropriate assumptions (Poisson models for clutter and undetected targets, no target-death, linear-Gaussian Markov target kinematics) are adhered to.	clutter;euler–bernoulli beam theory;mhtml;markov chain;recursion;remote file sharing	Edmund Førland Brekke;Mandar Chitre	2017	2017 20th International Conference on Information Fusion (Fusion)	10.23919/ICIF.2017.8009708	probability density function;clutter;statistics;kinematics;poisson distribution;finite set;mathematics;markov chain	Robotics	39.16436044629219	-26.85325278701951	175792
03f4fe82376a17bee8cd70a64878fed40c5dde6b	radar pattern classification based on class probability output networks		Modern aircraft and ships are equipped with radars emitting specific patterns of electromagnetic signals. The radar antennas are detecting these patterns which are required to identify the types of emitters. A conventional way of emitter identification is to categorize the radar patterns according to the sequences of frequencies, time of arrivals, and pulse widths of emitting signals by human experts. In this respect, this paper presents a method of classifying the radar patterns automatically using the network of calculating the p-values of testing the hypotheses of the types of emitters referred to as the class probability output network (CPON). Through the simulation for radar pattern classification, the effectiveness of the proposed approach has been demonstrated.	radar	Lee Suk Kim;Rhee Man Kil;Churl Hee Jo	2015		10.1007/978-3-319-26532-2_53	machine learning;pattern recognition;data mining	ML	36.38610221566179	-34.00614316412046	175928
03bbbd34377e41cc188fb8257399530f0f9e3f93	binary hypothesis testing via measure transformed quasi-likelihood ratio test		In this paper, the Gaussian quasi-likelihood ratio test (GQLRT) for non-Bayesian binary hypothesis testing is generalized by applying a transform to the probability distribution of the data. The proposed generalization, called measure-transformed GQLRT (MT-GQLRT), selects a Gaussian probability model that best empirically fits a transformed probability measure of the data. By judicious choice of the transform, we show that, unlike the GQLRT, the proposed test is resilient to outliers and involves higher order statistical moments leading to significant mitigation of the model mismatch effect on the decision performance. A Bayesian extension of the proposed MT-GQLRT is also developed that is based on selection of a Gaussian probability model that best empirically fits a transformed conditional probability distribution of the data. The non-Bayesian and Bayesian MT-GQLRTs are applied to signal detection and classification, in simulation examples that illustrate their advantages over the standard GQLRT and other robust alternatives.	detection theory;fits;simulation	Nir Halay;Koby Todros;Alfred O. Hero	2017	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2752692	inverse-chi-squared distribution;probability distribution;empirical probability;mathematics;statistics;probability measure;conditional probability distribution;regular conditional probability;joint probability distribution;posterior probability;pattern recognition;artificial intelligence	ML	27.864828075886503	-26.4368366123103	175961
3bf4e48c47fde835043c3baffa469f1b7c52e3b1	nonparametric regression estimation using penalized least squares	i i d random vectors nonparametric regression estimation penalized least squares multivariate penalized least squares regression estimates vapnik chervonenkis theory covering numbers bound estimates convergence strong consistency truncated estimates independent identically distributed random vectors;nearest neighbor searches;histograms;estimation theory;least squares approximations;covering number;kernel;convergence;neural networks;nonparametric statistics;vapnik chervonenkis theory;least squares approximation;multivariate penalized least squares regression estimates;penalized least square;independent identically distributed random vectors;smoothing methods;statistical analysis;strong consistency;statistical learning theory;estimation theory nonparametric statistics statistical analysis least squares approximations random processes;penalized least squares;regression estimator;random processes;statistics;nonparametric regression;covering numbers bound;regression analysis;nonparametric regression estimation;computer science;estimates convergence;vapnik chervonenkis;truncated estimates;smoothing spline;i i d random vectors	We present multivariate penalized least squares regression estimates. We use Vapnik{ Chervonenkis theory and bounds on the covering numbers to analyze convergence of the estimates. We show strong consistency of the truncated versions of the estimates without any conditions on the underlying distribution.	alexey chervonenkis;least squares;smoothing spline;strong consistency	Michael Kohler;Adam Krzyzak	2001	IEEE Trans. Information Theory	10.1109/18.998089	generalized least squares;nonparametric statistics;total least squares;stochastic process;econometrics;kernel;convergence;smoothing spline;pattern recognition;histogram;mathematics;non-linear least squares;estimation theory;least squares;nonparametric regression;artificial neural network;regression analysis;statistics;strong consistency	ML	29.49085409734622	-27.609350139049802	176161
1ae51405bcb1d1baa283615daf0258f836d5aa33	fast inference and learning in large-state-space hmms	time complexity;hidden markov model;state space	For Hidden Markov Models (HMMs) with fully connected transition models, the three fundamental problems of evaluating the likelihood of an observation sequence, estimating an optimal state sequence for the observations, and learning the model parameters, all have quadratic time complexity in the number of states. We introduce a novel class of non-sparse Markov transition matrices called Dense-Mostly-Constant (DMC) transition matrices that allow us to derive new algorithms for solving the basic HMM problems in sub-quadratic time. We describe the DMC HMM model and algorithms and attempt to convey some intuition for their usage. Empirical results for these algorithms show dramatic speedups for all three problems. In terms of accuracy, the DMC model yields strong results and outperforms the baseline algorithms even in domains known to violate the DMC assumption.	algorithm;baseline (configuration management);dynamic markov compression;hidden markov model;markov chain;sparse matrix;state space;time complexity	Sajid M. Siddiqi;Andrew W. Moore	2005		10.1145/1102351.1102452	time complexity;computer science;state space;machine learning;pattern recognition;mathematics;hidden markov model;statistics	ML	24.76809405339639	-29.37441968196696	176347
0daad930a07d7dc0d1368993cc0925363ce383bf	re-imaginig the networks: detecting local communities in networks by approximating derivatives in graph space		Finding communities in networks has become very important because various social and physical complex systems are represented as networks. Because of their ability to address large networks, local community detection methods that start from a seed node have become the focus of many research endeavours in recent times. In this paper, a derivative-based local community detection method, inspired by active contours, is proposed for finding a community surrounding an initial seed. The method is based on the concepts of curvature and gradient of the current communitys boundary. Curvature and gradient comprise the velocity function used to determine whether the boundary should move to include a candidate node in its vicinity. Unlike Euclidean space, networks have non-uniform space with fluctuations of dimensionality, given by the fluctuation in degrees of nodes, which complicates the calculation of derivatives used to define the curvature and gradient of boundary nodes. In this research, with some intuitive imagination, a framework for approximating derivatives in network space is proposed. Benchmarking this community detection method against three contemporary methods shows that it is capable of building communities with equal or better conductance.	algorithm;complex systems;computational complexity theory;conductance (graph);gradient;lightweight portable security;quantum fluctuation;sensor;social network;velocity (software development)	Mohammed Rigi;Irene Moser;Seddigh Rigi;Chengfei Liu	2017		10.1145/3110025.3120988	complex system;power graph analysis;euclidean space;complex network;computer science;curvature;curse of dimensionality;topology;spatial network;geometric networks	ML	33.89587938592658	-33.727746886386825	176366
0e0cdf169092b84462fa80c990d08d4152d4cddd	pg-means: learning the number of clusters in data	yu;variational bayes;computer network architecture;statistical test;statistical hypothesis testing;gaussian mixture model;thesis;computer science pg means learning the number of clusters in data baylor university gregory j hamerly feng;mixture model;number of clusters;artificial intelligence;algorithms;high dimension	We present a novel algorithm called PG-means which is able to learn the number of clusters in a classical Gaussian mixture model. Our metho d is robust and efficient; it uses statistical hypothesis tests on one-dimensi onal projections of the data and model to determine if the examples are well represented b y the model. In so doing, we are applying a statistical test for the entire mode l at once, not just on a per-cluster basis. We show that our method works well in diffi cult cases such as non-Gaussian data, overlapping clusters, eccentric clust ers, high dimension, and many true clusters. Further, our new method provides a much m ore stable estimate of the number of clusters than existing methods.	algorithm;map projection;mixture model	Yu Feng;Greg Hamerly	2006			econometrics;statistical hypothesis testing;determining the number of clusters in a data set;computer science;machine learning;mixture model;mathematics;statistics	ML	30.27244138821381	-29.18820078179946	176381
2ab613ed0f576183d7e399c7a2f4149b44128adb	multi-task sparse structure learning	structure learning;global climate model combination;sparse models;algorithms;multitask learning	Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously. While sometimes the underlying task relationship structure is known, often the structure needs to be estimated from data at hand. In this paper, we present a novel family of models for MTL, applicable to regression and classification problems, capable of learning the structure of task relationships. In particular, we consider a joint estimation problem of the task relationship structure and the individual task parameters, which is solved using alternating minimization. The task relationship structure learning component builds on recent advances in structure learning of Gaussian graphical models based on sparse estimators of the precision (inverse covariance) matrix. We illustrate the effectiveness of the proposed model on a variety of synthetic and benchmark datasets for regression and classification. We also consider the problem of combining climate model outputs for better projections of future climate, with focus on temperature in South America, and show that the proposed model outperforms several existing methods for the problem.	benchmark (computing);climate model;dhrystone;graphical model;multi-task learning;sparse matrix	André Ricardo Gonçalves;Puja Das;Soumyadeep Chatterjee;Vidyashankar Sivakumar;Fernando José Von Zuben;Arindam Banerjee	2014		10.1145/2661829.2662091	semi-supervised learning;unsupervised learning;multi-task learning;computer science;artificial intelligence;machine learning;pattern recognition;active learning;statistics;generalization error	ML	28.093404137223278	-33.35599810704417	176405
42580e894bf80b806a0ca2d89da23c8eee07c2ee	learning probabilistic programs		We develop a technique for generalising from data in which models are samplers represented as program text. We establish encouraging empirical results that suggest that Markov chain Monte Carlo probabilistic programming inference techniques coupled with higher-order probabilistic programming languages are now sufficiently powerful to enable successful inference of this kind in nontrivial domains. We also introduce a new notion of probabilistic program compilation and show how the same machinery might be used in the future to compile probabilistic programs for efficient reusable predictive inference.	compiler;markov chain monte carlo;monte carlo method;programming language;randomized algorithm	Yura N. Perov;Frank D. Wood	2014	CoRR		probabilistic analysis of algorithms;probabilistic ctl;probabilistic relevance model;computer science;artificial intelligence;theoretical computer science;machine learning;data mining;probabilistic logic;probabilistic argumentation;probabilistic logic network;divergence-from-randomness model	ML	25.94109803264808	-28.86289009829474	176427
8f21735c7f89eee9c6ad2aa01c42918a5aa9be1b	a 95% accurate eeg-connectome processor for a mental health monitoring system	electroencephalography table lookup alzheimer s disease support vector machines computational efficiency monitoring;support vector machines;conference;support vector machines cmos integrated circuits electroencephalography health care neural chips patient monitoring radial basis function networks;monitoring;alzheimer s disease;wearable connectome eeg processor synchronization likelihood;electroencephalography;computational efficiency;table lookup;power 1 71 mw eeg connectome processor mental health monitoring system electroencephalogram synchronization likelihood connectome feature sparse matrix inscription scheme smi scheme memory size reduction radial basis function kernel based support vector machine rbf svm rbf kernels lookup tables floating point operations alzheimer disease cmos technology size 0 18 mum	An electroencephalogram (EEG)-connectome processor is proposed for a mental health monitoring system. The proposed processor computes synchronization likelihood (SL) as the connectome feature. A sparse matrix inscription (SMI) scheme is proposed to reduce the memory size to 1/24 during SL calculation. From the calculated SL information, radial basis function (RBF) kernel-based support vector machine (SVM) diagnoses user's mental health condition. For RBF kernels, lookup-tables (LUTs) are used to replace the floating-point operations, decreasing the required operation by 54%. The EEG-connectome processor improves the diagnosis accuracy from 89% to 95% in Alzheimer's disease case. The proposed processor occupies 3.8 mm2 and consumes 1.71mW with 0.18μm CMOS technology.	cmos;central processing unit;connectome;electroencephalography;lookup table;radial (radio);radial basis function kernel;sl (complexity);sparse matrix;support vector machine	Hyunki Kim;Kiseok Song;Taehwan Roh;Hoi-Jun Yoo	2015	2015 IEEE Asian Solid-State Circuits Conference (A-SSCC)	10.1109/ASSCC.2015.7387479	computer science;artificial intelligence;theoretical computer science;machine learning	EDA	34.37783279425452	-35.66688143413731	176599
30a2131ed93b90e86bdbb27988c42e4c9672a578	asymptotic analysis of generative semi-supervised learning	naive bayes;asymptotic analysis;semi supervised learning;text classification;science learning;structure prediction;simulation study;composite likelihood;semisupervised learning	Semi-supervised learning has emerged as a popular framework for improving modeling accuracy while controlling labeling cost. Based on an extension of stochastic composite likelihood we quantify the asymptotic accuracy of generative semi-supervised learning. In doing so, we complement distributionfree analysis by providing an alternative framework to measure the value associated with different labeling policies and resolve the fundamental question of how much data to label and in what manner. We demonstrate our approach with both simulation studies and real world experiments using naive Bayes for text classification and MRFs and CRFs for structured prediction in NLP.	document classification;experiment;naive bayes classifier;natural language processing;reference frame (video);semi-supervised learning;semiconductor industry;simulation;structured prediction;supervised learning	Joshua V. Dillon;Krishnakumar Balasubramanian;Guy Lebanon	2010			semi-supervised learning;naive bayes classifier;asymptotic analysis;computer science;machine learning;pattern recognition;data mining;quasi-maximum likelihood;supervised learning;statistics	ML	26.187908547396507	-31.471593732382484	176757
ad207725b668cfba07e6147676c20c9ab9e04e7c	random vector generation from mixed-attribute datasets using random walk		Given data in a matrix X in which rows represent vectors and columns comprise a mix of discrete and continuous variables, the method presented in this paper can be used to generate random vectors whose elements display the same marginal distributions and correlations as the variables in X. The data is represented as a bipartite graph consisting of object nodes (representing vectors) and attribute value nodes. Random walk can be used to estimate the distribution of a target variable conditioned on the remaining variables, allowing a random value to be drawn for that variable. This leads to the use of Gibbs sampling to generate entire vectors. Unlike conventional methods, the proposed method requires neither the joint distribution nor the correlations to be specified, learned, or modeled explicitly in any way. Application to the Australian Credit dataset demonstrates the feasibility of the approach in generating random vectors on challenging real-world datasets.	column (database);gibbs sampling;marginal model;random access;sampling (signal processing);the australian	Andrew Skabar	2016	2016 Winter Simulation Conference (WSC)		random variate;marginal distribution;random graph;probability density function;combinatorics;discrete mathematics;random field;multivariate random variable;bipartite graph;random element;convergence of random variables;mixture distribution;sum of normally distributed random variables;random function;humidity;mathematics;graphical model;convolution random number generator;probabilistic logic;algebra of random variables;correlation;statistics;multivariate t-distribution	ML	32.01892175617756	-27.109863563140934	177009
939ebb29ab03eab5d9f268c8d52b31d518984528	iterative and active graph clustering using trace norm minimization without cluster size constraints	community detection;planted partition model;graph clustering;convex optimization;stochastic block model;active clustering	This paper investigates graph clustering under the planted partition model in the presence of small clusters. Traditional results dictate that for an algorithm to provably correctly recover the underlying clusters, all clusters must be sufficiently large—in particular, the cluster sizes need to be Ω̃( √ n), where n is the number of nodes of the graph. We show that this is not really a restriction: by a refined analysis of a convex-optimization-based recovery approach, we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones. Based on this result, we further devise an iterative algorithm to provably recover almost all clusters via a “peeling strategy”: we recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the partial observation setting, in which only a (chosen) part of the graph is observed. The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often after large clusters are learned (and removed). We expect that the idea of iterative peeling—that is, sequentially identifying a subset of the clusters and reducing the problem to a smaller one—is useful more broadly beyond the specific implementations (based on convex optimization) used in this paper.	active learning (machine learning);algorithm;cluster analysis;convex optimization;iterative method;mathematical optimization	Nir Ailon;Yudong Chen;Huan Xu	2015	Journal of Machine Learning Research		correlation clustering;mathematical optimization;combinatorics;discrete mathematics;convex optimization;graph partition;machine learning;clustering coefficient;mathematics;affinity propagation	ML	28.774231683957165	-36.70878980385233	177246
42464af265aee2c2d05f057ca94289ab2bf1a2d8	on nonparametric residual variance estimation	continuous function;regression function;convergence;fonction regression;funcion regresion;echantillonnage;estimation non parametrique;residual variance estimation;nonparametric;additive noise;ruido aditivo;bruit additif;fonction continue;prior knowledge;sampling;non parametric estimation;vecino mas cercano;convergencia;analisis regresion;heteroscedasticidad;funcion continua;heteroscedasticite;nearest neighbor;analyse regression;plus proche voisin;nearest neighbour;regression analysis;variance estimation;estimacion no parametrica;heteroscedasticity;muestreo;noise variance;variance;variancia;nearest neighbor graph	In this paper, the problem of residual variance estimation is examined. The problem is analyzed in a general setting which covers non-additive heteroscedastic noise under non-iid sampling. To address the estimation problem, we suggest a method based on nearest neighbor graphs and we discuss its convergence properties under the assumption of a Hölder continuous regression function. The universality of the estimator makes it an ideal tool in problems with only little prior knowledge available.	estimation theory;experiment;machine learning;model selection;nonlinear system;random effects model;sampling (signal processing);sparse matrix;supervised learning;universality probability;utility functions on indivisible goods	Elia Liitiäinen;Francesco Corona;Amaury Lendasse	2008	Neural Processing Letters	10.1007/s11063-008-9087-8	nonparametric statistics;continuous function;sampling;econometrics;nearest neighbor graph;convergence;pattern recognition;mathematics;variance;heteroscedasticity;k-nearest neighbors algorithm;regression analysis;statistics	ML	31.88313993725943	-24.713386619379236	177389
0d18e5dfa033b1f046b36b84c854b4a8e32a1a4f	exploiting functional dependence in bayesian network inference	adaptive testing;bayesian network;functional dependency;factorization method;hidden variables	In this paper we propose an efficient method for Bayesian network inference in models with functional dependence. We generalize the multiplicative factorization method originally designed by Takikawa and D’Ambrosio (1999) for models with independence of causal influence. Using a hidden variable, we transform a probability potential into a product of two-dimensional potentials. The multiplicative factorization yields more efficient inference. For example, in junction tree propagation it helps to avoid large cliques. In order to keep potentials small, the number of states of the hidden variable should be minimized. We transform this problem into a combinatorial problem of minimal base in a particular space. We present an example of a computerized adaptive test, in which the factorization method is significantly more efficient than previous inference methods.	bayesian network;causal filter;hidden variable theory;software propagation;tree decomposition	Jirí Vomlel	2002			computer science;machine learning;pattern recognition;bayesian network;mathematics;functional dependency;computerized adaptive testing;hidden variable theory;statistics	ML	24.858478833131976	-27.644138794388407	177571
bc6162899876b2e58e322609ec84cda335a4c055	information source detection via maximum a posteriori estimation	greedy search;data mining;conferences data mining;gsba information source detection maximum a posteriori estimation map estimator brute force search approximation bfsa greedy search bound approximation;greedy search information source detection maximum a posteriori likelihood approximation;search problems greedy algorithms information networks maximum likelihood estimation;maximum a posteriori;information source detection;conferences;likelihood approximation	The problem of information source detection, whose goal is to identify the source of a piece of information from a diffusion process (e.g., computer virus, rumor, epidemic, and so on), has attracted ever-increasing attention from research community in recent years. Although various methods have been proposed, such as those based on centrality, spectral and belief propagation, the existing solutions still suffer from high time complexity and inadequate effectiveness. To this end, we revisit this problem in the paper and present a comprehensive study from the perspective of likelihood approximation. Different from many previous works, we consider both infected and uninfected nodes to estimate the likelihood for the detection. Specifically, we propose a Maximum A Posteriori (MAP) estimator to detect the information source for general graphs with rumor centrality as the prior. To further improve the efficiency, we design two approximate estimators, namely Brute Force Search Approximation (BFSA) and Greedy Search Bound Approximation (GSBA). BFSA tries to traverse the permitted permutations and directly computes the likelihood, while GSBA exploits a strategy of greedy search to find a surrogate upper bound of the probabilities of permitted permutations for a given node, and derives an approximate MAP estimator. Extensive experiments on several network data sets clearly demonstrate the effectiveness of our methods in detecting the single information source.	approximation algorithm;belief propagation;brute force;brute-force search;centrality;computer virus;experiment;greedy algorithm;information source;sensor;software propagation;traverse;time complexity	Biao Chang;Feida Zhu;Enhong Chen;Qi Feng Liu	2015	2015 IEEE International Conference on Data Mining	10.1109/ICDM.2015.116	greedy randomized adaptive search procedure;mathematical optimization;greedy algorithm;computer science;maximum a posteriori estimation;machine learning;pattern recognition;data mining;mathematics;statistics	AI	25.06225715426532	-29.30441630564273	177679
30b3518cc5a5fefc5c40a129e5d1c1c1c5264539	tensor factorization for missing data imputation in medical questionnaires	health information management;tensile stress;standards;medical administrative data processing;training;data handling medical information systems health information management public healthcare;tensors mean square error methods medical administrative data processing regression analysis;drntu engineering electrical and electronic engineering;conference paper;vectors;estimation;medical information systems;mean square error methods;regression analysis;approximation methods;root mean square;data handling;tensile stress standards root mean square estimation vectors approximation methods training;k nearest neighbor estimation tensor factorization missing data imputation medical questionnaires innovative collaborative filtering techniques canonical polyadic decomposition parafac normalized decomposition systemic lupus erythematosus specific quality of life questionnaire normalized root mean square error regression imputations;public healthcare;tensors	This paper presents innovative collaborative filtering techniques to complete missing data in repeated medical questionnaires. The proposed techniques are based on the canonical polyadic (CP) decomposition (a.k.a. PARAFAC). Besides the standard CP decomposition, also a normalized decomposition is utilized. As an illustration, systemic lupus erythematosus-specific quality-of-life questionnaire is considered. Measures such as normalized root mean square error, bias and variance are used to assess the performance of the proposed tensor-based methods in comparison with other widely used approaches, such as mean substitution, regression imputations and k-nearest neighbor estimation. The numerical results demonstrate that the proposed methods provide significant improvement in comparison to popular methods. The best results are obtained for the normalized decomposition.	collaborative filtering;geo-imputation;k-nearest neighbors algorithm;mean squared error;missing data;nearest-neighbor interpolation;numerical analysis	Justin Dauwels;Lalit Garg;Arul Earnest;Khai Pang Leong	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288327	econometrics;estimation;speech recognition;tensor;root mean square;group method of data handling;data mining;mathematics;stress;regression analysis;statistics	HPC	28.1894231028946	-25.4445625566619	177691
8cda3333e5acf0d87c18ca62ce97ca96acaf75bc	visual nonlinear discriminant analysis for classifier design	posterior probability;discriminant analysis	We present a new method for analyzing classifiers by visualization, which we call visual nonlinear discriminant analysis. Classifiers that output posterior probabilities are visualized by embedding samples and classes so as to approximate posterior probabilities using parametric embedding. The visualization provides a better intuitive understanding of such classifier characteristics as separability and generalization ability than conventional methods. We evaluate our method by visualizing classifiers for an artificial data set.	approximation algorithm;linear discriminant analysis;linear separability;nonlinear system	Tomoharu Iwata;Kazumi Saito;Naonori Ueda	2006			random subspace method;computer science;machine learning;pattern recognition;mathematics;posterior probability;statistics	ML	30.66008511799528	-36.41325656682259	177716
192941c463e1be6620b82baf220ba98e90400d36	multi-resolution tensor learning for large-scale spatial data		"""High-dimensional tensor models are notoriously computationally expensive to train. We present a meta-learning algorithm, MRTL, that can significantly speed up the process for spatial tensor models. MRTL leverages the property that spatial data can be viewed at multiple resolutions, which are related by coarsening and finegraining from one resolution to another. Using this property, MRTL learns a tensor model by starting from a coarse resolution and iteratively increasing the model complexity. In order to not """"over-train"""" on coarse resolution models, we investigate an information-theoretic fine-graining criterion to decide when to transition into higherresolution models. We provide both theoretical and empirical evidence for the advantages of this approach. When applied to two real-world large-scale spatial datasets for basketball player and animal behavior modeling, our approach demonstrate 3 key benefits: 1) it efficiently captures higher-order interactions (i.e., tensor latent factors), 2) it is orders of magnitude faster than fixed resolution learning and scales to very fine-grained spatial resolutions, and 3) it reliably yields accurate and interpretable models."""		Stephan Zheng;Rose Yu;Yisong Yue	2018	CoRR		tensor;mathematical optimization;machine learning;artificial intelligence;spatial analysis;speedup;orders of magnitude (numbers);mathematics	ML	26.82101449401127	-31.41324376547509	177785
11b28dac435449c7abd74a7c97ff29f5a9b5bd7a	bayesian joint spike-and-slab graphical lasso		In this article, we propose a new class of priors for Bayesian inference with multiple Gaussian graphical models. We introduce fully Bayesian treatments of two popular procedures, the group graphical lasso and the fused graphical lasso, and extend them to a continuous spike-and-slab framework to allow self-adaptive shrinkage and model selection simultaneously. We develop an EM algorithm that performs fast and dynamic explorations of posterior modes. Our approach selects sparse models efficiently with substantially smaller bias than would be induced by alternative regularization procedures. The performance of the proposed methods are demonstrated through simulation and two real data examples.	bayesian approaches to brain function;expectation–maximization algorithm;graphical model;graphical user interface;lasso;model selection;simulation;slab allocation;sparse matrix	Zehang Richard Li;Tyler H. McCormick;Samuel J. Clark	2018	CoRR		lasso (statistics);artificial intelligence;prior probability;model selection;expectation–maximization algorithm;regularization (mathematics);computer science;pattern recognition;bayesian inference;bayesian probability;graphical model	ML	28.531230635168882	-32.23515562202821	177994
532af7e7199106827a1edde624752053fe6e5d66	community detection from low-rank excitations of a graph filter		This paper considers the problem of inferring the topology of a graph from noisy outputs of an unknown graph filter excited by low-rank signals. Limited by this low-rank structure, we focus on solving the community detection problem, whose aim is to partition the node set of the unknown graph into subsets with high edge densities. We propose to detect the communities by applying spectral clustering on the low-rank output covariance matrix. To analyze the performance, we show that the low-rank covariance yields a sketch of the eigenvectors of the unknown graph. Importantly, we provide theoretical bounds on the error introduced by this sketching procedure based on spectral features of the graph filter involved. Finally, our theoretical findings are validated via numerical experiments.	cluster analysis;experiment;low-rank approximation;numerical analysis;spectral clustering	Hoi-To Wai;Santiago Segarra;Asuman E. Ozdaglar;Anna Scaglione;Ali Jadbabaie	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462239	mathematical optimization;spectral clustering;signal processing;eigenvalues and eigenvectors;covariance matrix;cluster analysis;sketch;symmetric matrix;covariance;computer science	Robotics	28.645469810708708	-36.75309746052433	178006
49c5aedd856a3976f7b48be4f6922fc8057523af	factorization with uncertainty	mahalanobis distance;global solution;factor structure;least square error;directional uncertainty;singular value decomposition;data uncertainty;multiple views;objective function;factorization;structure and motion;structure from motion	Factorization using Singular Value Decomposition (SVD) is often used for recovering 3D shape and motion from feature correspondences across multiple views. SVD is powerful at finding the global solution to the associated least-square-error minimization problem. However, this is the correct error to minimize only when the x and y positional errors in the features are uncorrelated and identically distributed. But this is rarely the case in real data. Uncertainty in feature position depends on the underlying spatial intensity structure in the image, which has strong directionality to it. Hence, the proper measure to minimize is covariance-weighted squared-error (or the Mahalanobis distance). In this paper, we describe a new approach to covariance-weighted factorization, which can factor noisy feature correspondences with high degree of directional uncertainty into structure and motion. Our approach is based on transforming the raw-data into a covariance-weighted data space, where the components of noise in the different directions are uncorrelated and identically distributed. Applying SVD to the transformed data now minimizes a meaningful objective function in this new data space. This is followed by a linear but suboptimal second step to recover the shape and motion in the original data space. We empirically show that our algorithm gives very good results for varying degrees of directional uncertainty. In particular, we show that unlike other SVD-based factorization algorithms, our method does not degrade with increase in directionality of uncertainty, even in the extreme when only normal-flow data is available. It thus provides a unified approach for treating corner-like points together with points along linear structures in the image.	algorithm;dataspaces;emoticon;linear combination of atomic orbitals;loss function;matrix multiplication;ne (complexity);optimization problem;singular value decomposition	Michal Irani;P. Anandan	2000	International Journal of Computer Vision	10.1023/A:1020137420717	mathematical optimization;structure from motion;mahalanobis distance;machine learning;mathematics;singular value decomposition;factorization;statistics	Vision	28.362073762264426	-37.980506021317865	178171
631b6fbd3a6686cc95036a561429533ebfb58096	using multivariate regression methods to resolve overlapped electrochemical signals	multivariate regression	This paper proposes the application of Gaussian process regression (GPR) as an alternative regression model to resolve the hard overlapped electrochemical signals belonging to the 2,4,6trichlorophenol/2,6-dichlorophenol (TCP/DCP) system. Gaussian process derives from the perspective of Bayesian non-parametric regression methods, in terms of the parameterization of the covariance function, results in its good performance for the development of a calibration model for both linear and non-linear data sets. The multivariate regression model developed by GPR was compared with some traditional regression methods such as partial least squares regression (PLSR), and support vector regression (SVR). The comparative results were satisfied. The satisfactory results obtained through GPR method suggest that it can be used as a more effective and promising tool for multivariate regression tasks than the others.	gaussian process;general linear model;kriging;nonlinear system;partial least squares regression;support vector machine	Xin-feng Zhu;Jiandong Wang;Bin Li	2010	JDCTA	10.4156/jdcta.vol4.issue9.15	segmented regression;multivariate statistics;computer science;bayesian multivariate linear regression;statistics	ML	27.029006921342848	-24.68855035076328	178308
157f96655484cace389888c603616be31c5af34a	joint causal inference on observational and experimental datasets		We introduce Joint Causal Inference (JCI), a powerful formulation of causal discovery over multiple datasets in which we jointly learn both the causal structure and targets of interventions from independence test results. While offering many advantages, JCI induces faithfulness violations due to deterministic relations, so we extend a recently proposed constraint-based method to deal with this type of violations. A preliminary evaluation shows the benefits of JCI.	acid;algorithm;causal filter;causal inference;scalability;storage violation;synthetic intelligence	Sara Magliacane;Tom Claassen;Joris M. Mooij	2016	CoRR		econometrics;machine learning;data mining;mathematics;statistics	ML	28.39651632151113	-32.38924686905676	178562
799448f7c31d69df7ec062bb9fdb034550b7ebf3	probabilistic boolean tensor decomposition		Boolean tensor decomposition approximates data of multi-way binary relationships as product of interpretable low-rank binary factors, following the rules of Boolean algebra. Here, we present its first probabilistic treatment. We facilitate scalable sampling-based posterior inference by exploitation of the combinatorial structure of the factor conditionals. Maximum a posteriori decompositions feature higher accuracies than existing techniques throughout a wide range of simulated conditions. Moreover, the probabilistic approach facilitates the treatment of missing data and enables model selection with much greater accuracy. We investigate three real-world data-sets. First, temporal interaction networks in a hospital ward and behavioural data of university students demonstrate the inference of instructive latent patterns. Next, we decompose a tensor with more than 10 billion data points, indicating relations of gene expression in cancer patients. Not only does this demonstrate scalability, it also provides an entirely novel perspective on relational properties of continuous data and, in the present example, on the molecular heterogeneity of cancer. Our implementation is available on GitHub2.	boolean algebra;cylinder-head-sector;data point;expectation propagation;ka band;missing data;model selection;sampling (signal processing);scalability;turing institute;uncertainty quantification	Tammo Rukat;Christopher C. Holmes;Christopher Yau	2018			tensor;machine learning;artificial intelligence;probabilistic logic;computer science	ML	27.584657860877613	-32.0580486758955	178578
46fd3b723678cd849c053192e63c8ee414780afa	directional statistics and filtering using libdirectional		In this paper, we present libDirectional, a MATLAB library for directional statistics and directional estimation. It supports a variety of commonly used distributions on the unit circle, such as the von Mises, wrapped normal, and wrapped Cauchy distributions. Furthermore, various distributions on higher-dimensional manifolds such as the unit hypersphere and the hypertorus are available. Based on these distributions, several recursive filtering algorithms in libDirectional allow estimation on these manifolds. The functionality is implemented in a clear, well-documented, and object-oriented structure that is both easy to use and easy to extend.	algorithm;directional statistics;matlab;recursion	Gerhard Kurz;Igor Gilitschenski;Florian Pfaff;Lukas Drude;Uwe D. Hanebeck;Reinhold Häb-Umbach;Roland Siegwart	2017	CoRR		cauchy distribution;manifold;filter (signal processing);statistics;computer science;directional statistics;matlab;von mises yield criterion;algorithm;unit circle;hypersphere	ML	31.206685604737803	-27.316761412127093	178634
268205b87ea75e2268a93d4a30dc4bba4788f4e4	synchronization over z2 and community detection in signed multiplex networks with constraints		Finding group elements from noisy measurements of their pairwise ratios is also known as the group synchronization problem, first introduced in the context of the group SO(2) of planar rotations. The usefulness of synchronization over the group Z2 has been demonstrated in recent algorithms for localization of sensor networks and three-dimensional structuring of molecules. In this paper, we focus on synchronization over Z2, and consider the problem of identifying communities in a multiplex network when the interaction between the nodes is described by a signed (and possibly weighted) measure of similarity, and when the multiplex network has a natural partition into two communities, of possibly different sizes. In the setting where one has the additional information that certain subsets of nodes represent the same (unknown) group element, we consider and compare several algorithms for synchronization over Z2, based on spectral and semidefinite programming relaxations (SDP), and message passing algorithms. In other words, all nodes within such a subset represent the same unknown group element, and one has available noisy pairwise measurements between pairs of nodes that belong to different non-overlapping subsets. Following a recent analysis of the eigenvector method for synchronization over SO(2), we analyze the robustness to noise of the eigenvector method for synchronization over Z2, when the underlying graph of pairwise measurements is the Erdős-Rényi random graph, using results from the random matrix theory literature on the largest eigenvalue of rank-1 deformation of large random matrices. We also propose a message passing synchronization algorithm, inspired by the standard belief propagation algorithm, that outperforms the existing eigenvector synchronization algorithm only for certain classes of graphs and noise models, and enjoys the flexibility of incorporating additional constraints that may not be easily accommodated by any of the other spectral or SDP-based methods. We apply the synchronization methods both to several synthetic models and a real data set of roll call voting patterns in the U.S. Congress across time, to identify the two existing communities, i.e., the Democratic and Republican parties. Finally, we discuss a number of related open problems and future research directions.	algorithm;belief propagation;digimon;directed graph;erdős number;erdős–rényi model;graph coloring;message passing;multiplexing;random graph;semidefinite programming;software propagation;synthetic data	Mihai Cucuringu	2015	J. Complex Networks	10.1093/comnet/cnu050	combinatorics;discrete mathematics;theoretical computer science;data mining;mathematics;statistics	ML	27.1304115790643	-36.07878404533661	178681
db51b2a45baa35f68af1a470ff1abd09da1bfbbc	high dimension time series mining based on state transition chain analysis method	control systems;state transitionl;time series data mining;time series;data mining;temporal conversion;spatial conversion;synchronisation;control system;temporal and spatial conversion time series data mining state space state transitionl;time series mining;time series data mining synchronisation;temporal and spatial conversion;time series analysis;synchronization;state space;time series analysis data mining strips synchronization control systems algorithm design and analysis;state transition chain analysis method;strips;control state;association analysis;high dimension;algorithm design;algorithm design and analysis;association analysis time series mining state transition chain analysis method temporal conversion spatial conversion control state;state transition	In concerned with the high dimensions time series, which have the characteristic of multivariable, time-varying and time-lagging, were collected from multi-stage industrial processes, a method which is used to synchronize high-dimensions time series with temporal and spatial conversion is introduced in this paper. The high-dimensions time series is synchronized in spatial sampling by the conversion method. After the discretization for high-dimensions time series which is preprocessed by synchronization, first the control state is classified into normal state and high risk state by a simple association analysis; then using the method of state transition chain analysis, we successfully find the transition condition when the control system transform normal state into high risk state. This condition can be applied to reduce the quality defects of product and be used to guide the control strategy design of control system.	algorithm;control system;control theory;data mining;data pre-processing;discretization;preprocessor;sampling (signal processing);software bug;state transition table;time series	Zhimin Lv;Kai Zhang;Xiangwei Zhang;Shengyue Zong	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019814	algorithm design;synchronization;computer science;control system;artificial intelligence;time series;data mining;control theory;statistics	SE	38.314503159225474	-30.172499479845886	179627
d4091c770d5e69e04c27e861a57084a9b9e41aab	identification of continuous-time ar processes from unevenly sampled data	continuous time;linear regression model;uneven sampling;discrete time;autoregressive process;least square method;nonuniform sampling;continuous time models;system identification;computational complexity;least square;prediction error method	When identifying a continuous-time AR process from discrete-time data, an obvious approach is to replace the derivative operator in the continuous-time model by an approximation. In some cases, a linear regression model can then be formulated. The well-known least-squares method would be very desirable to apply, since it enjoy good numerical properties and low computational complexity, in particular for fast or nonuniform sampling. The focus of this paper is the latter, i.e., nonuniform sampling. Two consistent least-squares schemes for the case of unevenly sampled data are presented. The precise choice of derivative approximation turns out to be crucial. The obtained results are compared to a prediction error method.	unevenly spaced time series	Erik K. Larsson;Torsten Söderström	2002	Automatica	10.1016/S0005-1098(01)00244-8	econometrics;mathematical optimization;nonuniform sampling;mathematics;least squares;statistics	Logic	28.965854247128227	-25.31688754290305	179879
ba29802bf361c437e3a1b16c5da0662c33786e61	fast smoothing in switching approximations of non-linear and non-gaussian models	conditionally markov switching hidden linear models;conditionally gaussian linear state space models;stochastic volatility;optimal statistical smoother;smoothing in non linear systems	Statistical smoothing in general non-linear non-Gaussian systems is a challenging problem. A new smoothing method based on approximating the original system by a recent switching model has been introduced. Such switching model allows fast and optimal smoothing. The new algorithm is validated through an application on stochastic volatility and dynamic beta models. Simulation experiments indicate its remarkable performances and low processing cost. In practice, the proposed approach can overcome the limitations of particle smoothing methods and may apply where their usage is discarded.	algorithm;approximation;experiment;jsp model 2 architecture;kalman filter;latent variable;nonlinear system;performance;simulation;smoothing;statistical model;stochastic process;volatility	Ivan Gorynin;Stéphane Derrode;Emmanuel Monfrini;Wojciech Pieczynski	2017	Computational Statistics & Data Analysis	10.1016/j.csda.2017.04.007	additive smoothing;econometrics;mathematical optimization;mathematics;stochastic volatility;statistics;smoothing	ML	29.369921644684023	-24.10717536732898	180220
9f6e5932fc9de0449f5cdd8e635a40720838fef1	development of neural network committee machines for automatic forest fire detection using lidar	detection efficiency;signal analysis;committee machine;neyman pearson;backpropagation;random noise;automatic detection;forest fire;early stopping;roc curve;infrared;hold out;line of sight;neural network;lidar	"""Lidar has considerable potential as an early forest """"re detection technique, presenting considerable advantages when compared to the passive detection methods based on infrared cameras currently in common use, due to its higher sensitivity, ability to accurately locate the """"re and the fact that it does not need line of sight to the 3ames. The method has recently been demonstrated by the authors, but its automation requires the availability of a rapid signal analysis technique, for prompt alarm emission whenever required. In the present paper a novel method of classifying lidar signals using committee machines composed of neural networks is proposed. A new method based on ROC curves and the Neyman-Pearson criterion is used to choose the optimal number of training epochs for each neural network in order to avoid over""""tting. The best committee machine, obtained on the basis of these principles and selected to lead to the lowest percentage of false alarms for a true detection percentage of 90% for a test set created by adding random noise to patterns obtained experimentally, was composed of three single-layer perceptrons and presented a true detection e:ciency of 94.4% and 0.553% of false alarms in the validation set. ? 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved."""	artificial neural network;committee machine;experiment;noise (electronics);pattern recognition;perceptron;receiver operating characteristic;signal processing;test set	Armando M. Fernandes;Andrei B. Utkin;Alexander V. Lavrov;Rui M. Vilar	2004	Pattern Recognition	10.1016/j.patcog.2004.04.002	lidar;infrared;early stopping;computer science;artificial intelligence;backpropagation;machine learning;signal processing;pattern recognition;data mining;receiver operating characteristic;artificial neural network;statistics	Vision	39.06343585757262	-33.132790342838106	180349
9ce6ad931d2c28d4e2a7247e20a154be7ec860f4	lfda model for the assessment of water quality through microtox® using excitation-emission matrices	nonparametric estimation;latent dirichlet allocation;fluorescence spectroscopy			Oscar Martinez;Ranga Dabarera;Kamal Premaratne;Miroslav Kubat;James D. Englehardt	2017	Intell. Data Anal.	10.3233/IDA-150291	latent dirichlet allocation;econometrics;fluorescence spectroscopy;computer science;machine learning;pattern recognition;statistics	AI	30.758337458442693	-27.072502997154754	180383
a2d4482017b0f4d6bb10ec985cde3c04537bfe06	neural separation of observed and unobserved distributions		Separating mixed distributions is a long standing challenge for machine learning and signal processing. Applications include: single-channel multi-speaker separation (cocktail party problem), singing voice separation and separating reflections from images. Most current methods either rely on making strong assumptions on the source distributions (e.g. sparsity, low rank, repetitiveness) or rely on having training samples of each source in the mixture. In this work, we tackle the scenario of extracting an unobserved distribution additively mixed with a signal from an observed (arbitrary) distribution. We introduce a new method: Neural Egg Separation an iterative method that learns to separate the known distribution from progressively finer estimates of the unknown distribution. In some settings, Neural Egg Separation is initialization sensitive, we therefore introduce GLO Masking which ensures a good initialization. Extensive experiments show that our method outperforms current methods that use the same level of supervision and often achieves similar performance to full supervision.		Tavi Halperin;Ariel Ephrat;Yedid Hoshen	2018	CoRR		signal processing;initialization;artificial intelligence;pattern recognition;masking (art);computer science	ML	32.92428998979392	-36.818701377366665	180833
127d2fc15a43145033dd538c24e56958e17bc79e	online kernel pca with entropic matrix updates	relative entropy;feature vector;kernel pca	A number of updates for density matrices have been developed recently that are motivated by relative entropy minimization problems. The updates involve a softmin calculation based on matrix logs and matrix exponentials. We show that these updates can be kernelized. This is important because the bounds provable for these algorithms are logarithmic in the feature dimension (provided that the 2-norm of feature vectors is bounded by a constant). The main problem we focus on is the kernelization of an online PCA algorithm which belongs to this family of updates.	algorithm;density matrix;kernel method;kernel principal component analysis;kernelization;kullback–leibler divergence;linux;provable security	Dima Kuzmin;Manfred K. Warmuth	2007		10.1145/1273496.1273555	mathematical optimization;feature vector;kernel principal component analysis;computer science;machine learning;pattern recognition;mathematics;kullback–leibler divergence	ML	24.98832267254458	-36.32542342803114	180847
058c7238732c3564a0ced75538e811d34007aa95	evaluating bayesian and l1 approaches for sparse unsupervised learning		The use of L1 regularisation for sparse learning has generated immense research interest, with many successful applications in diverse areas such as signal acquisition, image coding, genomics and collaborative filtering. While existing work highlights the many advantages of L1 methods, in this paper we find that L1 regularisation often dramatically under-performs in terms of predictive performance when compared to other methods for inferring sparsity. We focus on unsupervised latent variable models, and develop L1 minimising factor models, Bayesian variants of “L1”, and Bayesian models with a stronger L0-like sparsity induced through spike-and-slab distributions. These spikeand-slab Bayesian factor models encourage sparsity while accounting for uncertainty in a principled manner, and avoid unnecessary shrinkage of non-zero values. We demonstrate on a number of data sets that in practice spike-and-slab Bayesian methods outperform L1 minimisation, even on a computational budget. We thus highlight the need to re-assess the wide use of L1 methods in sparsity-reliant applications, particularly when we care about generalising to previously unseen data, and provide an alternative that, over many varying conditions, provides improved generalisation performance. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).	bayesian network;collaborative filtering;international conference on machine learning;latent variable;slab allocation;sparse matrix;unsupervised learning	Shakir Mohamed;Katherine A. Heller;Zoubin Ghahramani	2012				ML	24.95918856065373	-34.86113032123142	181005
87e1742a9c7ca3383151da211d864f24affcc188	homogeneous superpixels from random walks		This paper presents a novel algorithm to generate homogeneous superpixels from the process of Markov random walks. We exploit Markov clustering (MCL) as the methodology, a generic graph clustering method based on stochastic flow circulation. In particular, we introduce a new graph pruning strategy called compact pruning in order to capture intrinsic local image structure, and thereby keep the superpixels homogeneous, i.e. uniform in size and compact in shape. Further, this new pruning scheme comes with three advantages: faster computation, smaller memory footprint, and straightforward parallel implementation. Through comparisons with other recent standard techniques, we show that the proposed algorithm achieves an optimal performance in terms of qualitative measure at a decent computational speed.	algorithm;cluster analysis;computation;isoperimetric inequality;markov chain monte carlo;memory footprint;monte carlo localization;perimeter;stochastic matrix;topological graph theory	Frank Perbet;Atsuto Maki	2011			mathematical optimization;combinatorics;discrete mathematics;machine learning;mathematics	AI	33.735154661733205	-33.708528763212485	181645
2b67dbfadb5812501780d700d20c9dbfb1d1727b	from a set of shapes to object discovery	metropolis hastings;reversible jump;local minima;spatial configuration;lower bound	This paper presents an approach to object discovery in a give n unlabeled image set, based on mining repetitive spatial config urations of image contours. Contours that similarly deform from one image to a n ther are viewed as collaborating, or, otherwise, conflicting. This is captu red by a graph over all pairs of matching contours, whose maximum a posteriori mult icoloring assignment is taken to represent the shapes of discovered objects. Multicoloring is conducted by our new Coordinate Ascent Swendsen-Wang cut (CASW ). CASW uses the Metropolis-Hastings (MH) reversible jumps to probabil istically sample graph edges, and color nodes. CASW extends SW cut by introducing a r e ularization in the posterior of multicoloring assignments that prevent s the MH jumps to arrive at trivial solutions. Also, CASW seeks to learn paramet ers of the posterior via maximizing a lower bound of the MH acceptance rate. This s peeds up multicoloring iterations, and facilitates MH jumps from local mi nima. On benchmark datasets, we outperform all existing approaches to unsuper vised object discovery.	benchmark (computing);cluster analysis;emoticon;iteration;metropolis;metropolis–hastings algorithm;modified huffman coding;online and offline;online machine learning;portable document format;shattered world;whole earth 'lectronic link	Nadia Payet;Sinisa Todorovic	2010		10.1007/978-3-642-15555-0_5	metropolis–hastings algorithm;mathematical optimization;machine learning;maxima and minima;mathematics;upper and lower bounds;algorithm;statistics	Vision	34.22135892640413	-37.8372706977225	181781
120adede81dc6b3bfcb4cd276f0b73cc58a96c1a	pixelcnn models with auxiliary variables for natural image modeling		We study probabilistic models of natural images and extend the autoregressive family of PixelCNN architectures by incorporating auxiliary variables. Subsequently, we describe two new generative image models that exploit different image transformations as auxiliary variables: a quantized grayscale view of the image or a multi-resolution image pyramid. The proposed models tackle two known shortcomings of existing PixelCNN models: 1) their tendency to focus on low-level image details, while largely ignoring high-level image information, such as object shapes, and 2) their computationally costly procedure for image sampling. We experimentally demonstrate benefits of the proposed models, in particular showing that they produce much more realistically looking image samples than previous state-of-the-art probabilistic models.		Alexander Kolesnikov;Christoph H. Lampert	2017			generative grammar;machine learning;grayscale;pyramid (image processing);artificial intelligence;autoregressive model;pattern recognition;probabilistic logic;sampling (statistics);computer science;exploit	ML	31.584180990711992	-34.12551322877104	181979
fb18e7c80ab5cc11df6671678474dabe3e7df3e4	hopfield neural network approach for supervised nonlinear spectral unmixing	neurons hyperspectral imaging optimization estimation biological neural networks mathematical model mixture models;hyperspectral data hopfield neural network approach supervised nonlinear spectral unmixing real world hyperspectral imaging scenarios linear mixture model hnn machine learning approach generalized bilinear model optimization problem seminonnegative matrix factorization problems nonlinear coefficient estimation hnn based gbm unmixing method gbm optimization algorithms endmember spectra;mixture models geophysical image processing hopfield neural nets hyperspectral imaging iterative methods learning artificial intelligence matrix decomposition;estimation;mathematical model;optimization;neurons;hyperspectral imaging;mixture models;nonlinear unmixing generalized bilinear model gbm hopfield neural network hnn;biological neural networks	Nonlinear unmixing, which has attracted considerable interest from researchers and developers, has been successfully applied in many real-world hyperspectral imaging scenarios. Hopfield neural network (HNN) machine learning has already proven successful in solving the linear mixture model; this study utilized an HNN machine learning approach to solve the generalized bilinear model (GBM) optimization problem. Two HNNs were constructed in a successive manner to solve respective seminonnegative matrix factorization problems intended for abundance and nonlinear coefficient estimation. In the proposed HNN-based GBM unmixing method, both HNNs evolve to stable states after a number of iterations to obtain unmixing results related to the states of neurons. In experiments on synthetic data, the proposed method showed more efficient performance in regard to abundance estimation accuracy than other GBM optimization algorithms, especially when given reliable endmember spectra. The proposed method was also applied to real hyperspectral data and still demonstrated notable advantages despite the obvious increase in unmixing difficulty.	algorithm;artificial neural network;bilinear filtering;coefficient;experiment;gnome-db;hopfield network;information extraction;iteration;machine learning;mathematical optimization;mesa;mixture model;nonlinear acoustics;nonlinear programming;nonlinear system;optimization problem;supervised learning;synthetic data;unsupervised learning	Jing Li;Xiaorun Li;Bormin Huang;Liaoying Zhao	2016	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2016.2560222	mathematical optimization;estimation;theoretical computer science;hyperspectral imaging;machine learning;mixture model;mathematical model;mathematics;statistics	ML	26.430201148610593	-36.597631883291434	182142
0469f010045afe5fca3c0ec095b4f677f0489cff	spectral clustering for multiclass erdös-rényi graphs	erbium;graph theory;community detection;machine learning algorithms;pattern clustering;spectral graph theory community detection non euclidean datasets random graph models;non euclidean datasets;biological system modeling;biology;spectral clustering;geometric property;laplace equations;multiclass erdos renyi graph;computational modeling;phase transition;stochastic processes;guidelines;laplace equations graph theory biological system modeling machine learning algorithms spectral analysis sampling methods computational modeling context modeling guidelines biology;sampling graph spectral clustering multiclass erdos renyi graph spectral analysis graph laplacian decomposition geometric property interclass intersection phase transition diagonal concentration;diagonal concentration;sampling graph;mathematical model;interclass intersection;spectral graph theory;spectral analysis graph theory pattern clustering;spectral analysis;sampling methods;random graph models;context modeling;graph laplacian decomposition	In this article, we study the properties of the spectral analysis of multiclass Erdös-Rényi graphs. With a view towards using the embedding afforded by the decomposition of the graph Laplacian for subsequent processing, we analyze two basic geometric properties, namely interclass intersection and interclass distance. We will first study the dyadic two-class case in details and observe the existence of a phase transition for the interclass intersection. We then focus on the general multiclass case, where we introduce an appropriate notion of diagonal concentration and derive a statistical model that allows sampling graphs whose expected diagonal concentration is fixed. The simulations provided yield useful guidelines for practitioners to choose appropriately parameters in the context of spectral clustering.	cluster analysis;dyadic transformation;laplacian matrix;norm (social);sampling (signal processing);simulation;spectral clustering;spectral density estimation;statistical model	Mohamed-Ali Belabbas	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5494932	phase transition;sampling;mathematical optimization;combinatorics;discrete mathematics;erbium;graph theory;machine learning;mathematical model;mathematics;context model;computational model;spectral graph theory;spectral clustering;statistics	Robotics	28.875477360048997	-37.690816706790535	182199
191e17b5b22514ddbea501a12d50f7f8d13359c4	computation of variances in causal networks		The causal (beli� network is a well-known graphical structure for representing independencies in a joint probability distribution. The exact methods and the approximation methods, which perform probabilistic inference in causal networks, often treat the conditional probabilities which are stored in the network as certain values. However, if one takes either a subjectivistic or a limiting frequency approach to probability, one can never be certain of probability values. An algorithm for probabilistic inference should not only be capable of reporting the inferred probabilities; it should also be capable of reporting the uncertainty in these probabilities relative to the uncertainty in the probabilities which are stored in the network. In section 2 of this paper a method is given for determining the prior variances of the probabilities of all the nodes. Section 3 contains an approximation method for determining the variances in inferred probabilities.	algorithm;approximation;causal filter;computation;monte carlo method;newton's method	Richard E. Neapolitan;James R. Kenevan	2011	CoRR		econometrics;machine learning;chain rule;mathematics;law of total probability;statistics	AI	26.202226688506933	-26.62562714722937	182261
b4fd5791b224a5ea0f630923eae7f1e0d1f687d7	an independent component ordering and selection procedure based on the mse criterion	simulation ordinateur;traitement signal;analisis componente principal;validacion cruzada;separacion ciega;conference_paper;separation aveugle source;dimension reduction;blind source separation;relacion orden;erreur quadratique moyenne;ordering;reduction dimension;pattern discovery;blind separation;simulation experiment;relation ordre;mean square error;signal processing;principal component analysis;decouverte connaissance;validation croisee;analyse composante principale;separation aveugle;reduccion dimension;descubrimiento conocimiento;cross validation;simulacion computadora;error medio cuadratico;cumulant;procesamiento senal;computer simulation;independent component;variance;variancia;principal component;knowledge discovery	Personal Introduction Dr. Edmond Wu has working and teaching experience in both industry and academia. Prior to joining The Hong Kong Polytechnic University in 2009, he worked in financial institutes as business consultant and IT training lecturer. Dr. Wu is interested in multidisciplinary research. His research interests focuses on applying information technology, statistical knowledge and mathematical modeling techniques for business applications in areas of hospitality, tourism and financial management. He has authored/co-authored over 20 referred journals articles and conference papers in these research areas.	mathematical model	Edmond HaoCun Wu;Philip L. H. Yu;Wai Keung Li	2006		10.1007/11679363_36	computer simulation;econometrics;computer science;machine learning;signal processing;mathematics;algorithm;statistics;principal component analysis	DB	26.63517211070345	-24.266164719475874	182281
7550b1c4abfd8fb046847f20cac360217057c47a	nonparametric density estimation: toward computational tractability	space-partitioning trees.;divide-and-conquer;nonparametric statistics;algorithms;kernel density estimation;exploratory data analysis;higher order;space partitioning;divide and conquer;density estimation;bayesian network;kernel density estimate;algorithm design	Densityestimationis a coreoperationof virtually all probabilistic learningmethods(asopposedto discriminati ve methods).Approachesto densityestimationcan be divided into two principal classes, parametricmethods,suchasBayesiannetworks,andnonparametricmethodssuchaskerneldensityestimationandsmoothing splines. While neitherchoiceshouldbe universallypreferred for all situations,a well-known benefitof nonparametricmethods is theirability to achieveestimationoptimality for ANY input distribution asmoredataareobserved,a propertythatno modelwith a parametricassumptioncanhave, andoneof greatimportancein exploratorydataanalysisandmining wherethe underlyingdistribution is decidedlyunknown. To date,however, despitea wealth of advancedunderlyingstatisticaltheory, theuseof nonparametric methodshasbeenlimited by their computationalintractibility for all but thesmallestdatasets. In this paper , we presentanalgorithm for kernel densityestimation,the chief nonparametricapproach, which is dramaticallyfasterthanprevious algorithmicapproaches in termsof bothdatasetsizeanddimensionality. Furthermore,the algorithmprovidesarbitrarily tight accuracy guarantees, provides anytime convergence,works for all commonkernelchoices,and requiresno parametertuning. The algorithmis an instanceof a new principleof algorithmdesign:multi-recursion,or higher-order divide-and-conquer .	anytime algorithm;computational complexity theory;recursion;typeof	Alexander G. Gray;Andrew W. Moore	2003		10.1137/1.9781611972733.19	econometrics;pattern recognition;artificial intelligence;kernel density estimation;density estimation;nonparametric statistics;multivariate kernel density estimation;bayesian network;nonparametric regression;mathematics;semiparametric regression;variable kernel density estimation	ML	28.743056814492448	-27.18536107084987	182449
5f9f7c5cfed003200d2efbcb42838f896aabe64f	generalized gaussian mixture models as a nonparametric bayesian approach for clustering using class-specific visual features	painting;nonparametric bayes;gibbs sampling;infrared images;segmentation;photographic;generalized gaussian;mcmc;feature selection;mixture models	Recently, there has been a growing interest in the problem of learning mixture models from data. The reasons and motivations behind this interest are clear, since finite mixture models offer a formal approach to the important problems of clustering and data modeling. In this paper, we address the problem of modeling non-Gaussian data which are largely present, and occur naturally, in several computer vision and image processing applications via the learning of a generative infinite generalized Gaussian mixture model. The proposed model, which can be viewed as a Dirichlet process mixture of generalized Gaussian distributions, takes into account the feature selection problem, also, by determining a set of relevant features for each data cluster which provides better interpretability and generalization capabilities. We propose then an efficient algorithm to learn this infinite model parameters by estimating its posterior distributions using Markov Chain Monte Carlo (MCMC) simulations. We show how the model can be used, while comparing it with other models popular in the literature, in several challenging applications involving photographic and painting images categorization, image and video segmentation, and infrared facial expression recognition.	cluster analysis;mixture model	Tarek Elguebaly;Nizar Bouguila	2012	J. Visual Communication and Image Representation	10.1016/j.jvcir.2012.08.003	gibbs sampling;painting;computer science;machine learning;pattern recognition;mixture model;mathematics;feature selection;segmentation;statistics	ML	32.15995304582037	-36.48920606899965	182563
938afb2af195561889e1f20dcd917015fc3527ec	a machine learning approach for statistical software testing	software testing;statistical software;machine learning;long range dependent;adaptive sampling;experimental validation	Some Statistical Software Testing approaches rely on sampling the feasible paths in the control flow graph of the program; the difficulty comes from the tiny ratio of feasible paths. This paper presents an adaptive sampling mechanism called EXIST for Exploration/eXploitation Inference for Software Testing, able to retrieve distinct feasible paths with high probability. EXIST proceeds by alternatively exploiting and updating a distribution on the set of program paths. An original representation of paths, accommodating long-range dependencies and data sparsity and based on extended Parikh maps, is proposed. Experimental validation on real-world and artificial problems demonstrates dramatic improvements compared to the state of the art.	adaptive sampling;algorithmic inference;automata theory;control flow graph;heuristic (computer science);list of statistical packages;machine learning;map;original chip set;reachability;sampling (signal processing);software testing;sparse matrix;turing completeness;with high probability	Nicolas Baskiotis;Michèle Sebag;Marie-Claude Gaudel;Sandrine-Dominique Gouraud	2007			white-box testing;computer science;artificial intelligence;theoretical computer science;machine learning;data mining;software testing;statistics	SE	25.720270928480037	-28.422020134600732	182700
5fe9451593780ebdd9261f9dcdaa3aa673d2bceb	exact and efficient bayesian inference for multiple changepoint problems	mcmc algorithm;well log data;qa mathematics;bayesian inference;bayes factor;model choice;posterior distribution;perfect simulation;computational complexity;forward backward algorithm;reversible jump mcmc;coal mining	We demonstrate how to perform direct simulation from the posterior distribution of a class of multiple changepoint models where the number of changepoints is unknown. The class of models assumes independence between the posterior distribution of the parameters associated with segments of data between successive changepoints. This approach is based on the use of recursions, and is related to work on product partition models. The computational complexity of the approach is quadratic in the number of observations, but an approximate version, which introduces negligible error, and whose computational cost is roughly linear in the number of observations, is also possible. Our approach can be useful, for example within an MCMC algorithm, even when the independence assumptions do not hold. We demonstrate our approach on well-log data. Our method can cope with a range of models for this data, and exact simulation from the posterior distribution is possible in a matter of minutes.	algorithmic efficiency;approximation algorithm;computation;computational complexity theory;markov chain monte carlo;recursion;simulation	Paul Fearnhead	2006	Statistics and Computing	10.1007/s11222-006-8450-8	econometrics;bayes factor;machine learning;forward–backward algorithm;mathematics;coal mining;posterior probability;computational complexity theory;bayesian inference;statistics	ML	27.070270436789336	-27.290346309433424	182759
5faf6ffcc10506351b74e99bf51bc222be8f0ca7	the alpha-em algorithm and its basic properties	em algorithm		expectation–maximization algorithm	Yasuo Matsuyama	2000	Systems and Computers in Japan	10.1002/1520-684X(200010)31:11%3C12::AID-SCJ2%3E3.0.CO;2-O		NLP	28.502364361321096	-29.242334152803313	182807
1957c52e564a1a62adb3d81e1ded0604636b32f9	efficient monte carlo optimization for multi-label classifier chains	classifier chains;multilabel classifier chain;optimisation;bayes optimal method;multi label classification;mlc method;training;cc algorithm;high dimensional data set monte carlo optimization multilabel classifier chain mlc method cc algorithm greedy approximation bayes optimal method double monte carlo scheme m2cc algorithm;computational modeling;vectors;double monte carlo scheme;matematicas;pattern classification;m2cc algorithm;inference algorithms;high dimensional data set;probabilistic logic;learning artificial intelligence;computational efficiency;pattern classification learning artificial intelligence monte carlo methods optimisation;monte carlo optimization;monte carlo methods;training monte carlo methods probabilistic logic computational modeling vectors computational efficiency inference algorithms;classifier chains multi label classification monte carlo methods;greedy approximation	Multi-label classification (MLC) is the supervised learning problem where an instance may be associated with multiple labels. Modeling dependencies between labels allows MLC methods to improve their performance at the expense of an increased computational cost. In this paper we focus on the classifier chains (CC) approach for modeling dependencies. On the one hand, the original CC algorithm makes a greedy approximation, and is fast but tends to propagate errors down the chain. On the other hand, a recent Bayes-optimal method improves the performance, but is computationally intractable in practice. Here we present a novel double-Monte Carlo scheme (M2CC), both for finding a good chain sequence and performing efficient inference. The M2CC algorithm remains tractable for high-dimensional data sets and obtains the best overall accuracy, as shown on several real data sets with input dimension as high as 1449 and up to 103 labels.	algorithmic efficiency;approximation;classifier chains;cobham's thesis;computation;computational complexity theory;greedy algorithm;mathematical optimization;monte carlo method;multi-label classification;multi-level cell;supervised learning	Jesse Read;Luca Martino;David Luengo	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638300	mathematical optimization;computer science;machine learning;pattern recognition;mathematics;probabilistic logic;computational model;statistics;monte carlo method	Robotics	26.145516399831614	-30.573792755860097	182999
8b56ebf36c692ffbdd6a6dd7c623664b944438a7	on discrete epanechnikov kernel functions		Least-squares cross-validation is commonly used for selection of smoothing parameters in the discrete data setting; however, in many applied situations, it tends to select relatively small bandwidths. This tendency to undersmooth is due in part to the geometric weighting scheme that many discrete kernels possess. This problem may be avoided by using alternative kernel functions. Specifically, discrete versions (both unordered and ordered) of the popular Epanechnikov kernel do not have rapidly decaying weights. The analytic properties of these kernels are contrasted with commonly used discrete kernel functions and their relative performance is compared using both simulated and real data. The simulation and empirical results show that these kernel functions generally perform well and in some cases demonstrate substantial gains in terms of mean squared error.		Chi-Yang Chu;Daniel J. Henderson;Christopher F. Parmeter	2017	Computational Statistics & Data Analysis	10.1016/j.csda.2017.07.003	statistics;kernel principal component analysis;kernel embedding of distributions;econometrics;kernel (statistics);mathematics;polynomial kernel;kernel density estimation;kernel smoother;mathematical optimization;radial basis function kernel;variable kernel density estimation	ML	29.301763810020937	-25.177678812845514	183145
f59724ebad79f676e08b208bd93699119557d374	deep residual neural network for emi event classification using bispectrum representations		This paper presents a novel method for condition monitoring of High Voltage (HV) power plant equipment through analysis of discharge signals. These discharge signals are measured using the Electromagnetic Interference (EMI) method and processed using third order Higher-Order Statistics (HOS) to obtain a Bispectrum representation. By mapping the time-domain signal to a Bispectrum image representations the problem can be approached as an image classification task. This allows for the novel application of a Deep Residual Neural Network (ResNet) to the classification of HV discharge signals. The network is trained on signals into 9 classes and achieves high classification accuracy in each category, improving upon our previous work on this task.		Imene Mitiche;Mark David Jenkins;Philip Boreham;Alan Nesbitt;Brian G. Stewart;Gordon Morison	2018	2018 26th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2018.8553177	signal processing;condition monitoring;feature extraction;bispectrum;emi;electromagnetic interference;contextual image classification;third order;artificial intelligence;pattern recognition;computer science	ML	36.751739721973664	-31.675294521483146	183164
7827c2d485b77ca5e1d801b523938e71568ed7ef	spectral learning of hidden markov models from dynamic and static data		We develop spectral learning algorithms for Hidden Markov Models that learn not only from time series, or dynamic data but also static data drawn independently from the HMM’s stationary distribution. This is motivated by the fact that static, orderless snapshots are usually easier to obtain than time series in quite a few dynamic modeling tasks. Building on existing spectral learning algorithms, our methods solve convex optimization problems minimizing squared loss on the dynamic data plus a regularization term on the static data. Experiments on synthetic and real human activities data demonstrate better prediction by the proposed method than existing spectral algorithms.	approximation algorithm;concatenation;convex optimization;dynamic data;experiment;hidden markov model;kernel method;machine learning;markov chain;mathematical optimization;matrix regularization;mean squared error;microsoft windows;rewrite (programming);stationary process;synthetic data;the matrix;time series	Tzu-Kuo Huang;Jeff G. Schneider	2013			mathematical optimization;computer science;machine learning;pattern recognition	ML	26.69196322918078	-32.6537587031733	183212
3527b40d99c3871d73f7fa142dc6154bbfed06cf	generalizing matrix factorization through flexible regression priors	matrix factorization;decision tree;linear regression;regression model;variable selection;recommender system;tree models;latent factor;smooth transition;regression priors;model fitting;low rank approximation;recommender systems	"""Predicting user """"ratings"""" on items is a crucial task in recommender systems. Matrix factorization methods that computes a low-rank approximation of the incomplete user-item rating matrix provide state-of-the-art performance, especially for users and items with several past ratings (warm starts). However, it is a challenge to generalize such methods to users and items with few or no past ratings (cold starts). Prior work [4][32] have generalized matrix factorization to include both user and item features for performing better regularization of factors as well as provide a model for smooth transition from cold starts to warm starts. However, the features were incorporated via linear regression on factor estimates. In this paper, we generalize this process to allow for arbitrary regression models like decision trees, boosting, LASSO, etc. The key advantage of our approach is the ease of computing --- any new regression procedure can be incorporated by """"plugging"""" in a standard regression routine into a few intermediate steps of our model fitting procedure. With this flexibility, one can leverage a large body of work on regression modeling, variable selection, and model interpretation. We demonstrate the usefulness of this generalization using the MovieLens and Yahoo! Buzz datasets."""	boosting (machine learning);curve fitting;decision tree;feature selection;lasso;low-rank approximation;movielens;recommender system	Liang Zhang;Deepak Agarwal;Bee-Chung Chen	2011		10.1145/2043932.2043940	proper linear model;computer science;linear regression;machine learning;decision tree;pattern recognition;matrix decomposition;regression analysis;recommender system;low-rank approximation	ML	28.029183766710393	-35.20860627525298	183235
549add93d3e476102869f8d32e278b46e41390d7	guest editorial: semiparametric function estimation and testing	hazard regression;bayesian estimation;test procedures;bootstrap;profile likelihood;generalized partial linear models;markov chain monte carlo;smoothing;semiparametric regression;nonparametric regression;curve estimation		semiparametric model	Michael G. Schimek	2001	Statistics and Computing	10.1023/A:1011973117955	econometrics;bayes estimator;markov chain monte carlo;bayesian multivariate linear regression;polynomial regression;pattern recognition;mathematics;bayesian linear regression;variance function;nonparametric regression;semiparametric model;statistics;smoothing;semiparametric regression	ML	30.375198782073326	-24.43118116281914	183256
99d3797868c360ff0119f8e08d0b38fb5c09a5f2	the discrete acyclic digraph markov model in data mining	structure learning;graph theory;learning process;bayesian network;learning algorithm;labeled tree;graphical markov model;graphical markov model inclusion;data mining;artificial intelligent;heuristic search;markov model;tci markov model;markov chain monte carlo;markov chain monte carlo methods;structural learning;expert system	The research reported in this thesis has been partially carried out at CWI, the Dutch national research laboratory for mathematics and computer science, within the theme Data Mining and Knowledge Discovery, a subdivision of the research cluster Information Systems. To the memory of my mother A la memòria de ma mare	computer science;data mining and knowledge discovery;directed acyclic graph;directed graph;information system;linear algebra;markov chain;markov model;subdivision surface	Juan Roberto Castelo Valdueza	2002			markov decision process;markov chain;maximum-entropy markov model;markov kernel;variable-order bayesian network;partially observable markov decision process;computer science;machine learning;pattern recognition;graphical model;markov algorithm;markov process;markov model;hidden markov model;statistics;variable-order markov model	ML	24.9623244714237	-24.174185433764414	183292
58885a5c06743a751c1893452c9efc0353b3c1a0	online bayesian max-margin subspace multi-view learning		Last decades have witnessed a number of studies devoted to multi-view learning algorithms, however, few efforts have been made to handle online multi-view learning scenarios. In this paper, we propose an online Bayesian multi-view learning algorithm to learn predictive subspace with max-margin principle. Specifically, we first define the latent margin loss for classification in the subspace, and then cast the learning problem into a variational Bayesian framework by exploiting the pseudo-likelihood and data augmentation idea. With the variational approximate posterior inferred from the past samples, we can naturally combine historical knowledge with new arrival data, in a Bayesian Passive-Aggressive style. Experiments on various classification tasks show that our model have superior performance.	approximation algorithm;convolutional neural network;experiment;machine learning;variational principle	Jia He;Changying Du;Fuzhen Zhuang;Xin Yin;Qing He;Guoping Long	2016			wake-sleep algorithm;computer science;artificial intelligence;machine learning;pattern recognition;data mining;statistics	AI	26.492781297928076	-32.19441899366046	183364
036824e1b308df6ad04638b032015d28609c625b	probabilistic models for joint clustering and time-warping of multidimensional curves	time warp;bayesian network;learning algorithm;discrete time;probabilistic model;expectation maximization;mixture model;linear time;em algorithm	In this paper we present a family of mod­ els and learning algorithms that can simul­ taneously align and cluster sets of multidi­ mensional curves measured on a discrete time grid. Our approach is based on a generative mixture model that allows both local non­ linear time warping and global linear shifts of the observed curves in both time and mea­ surement spaces relative to the mean curves within the clusters. The resulting model can be viewed as a form of Bayesian net­ work with a special temporal structure. The Expectation-Maximization (EM) algorithm is used to simultaneously recover both the curve models for each cluster, and the most likely alignments and cluster membership for each curve. We evaluate the methodology on two real-world data sets, and show that the Bayesian network models provide systematic improvements in predictive power over more conventional clustering approaches.	align (company);bayesian network;cluster analysis;expectation–maximization algorithm;machine learning;mixture model;time complexity	Darya Chudova;Scott Gaffney;Padhraic Smyth	2003			econometrics;variable-order bayesian network;expectation–maximization algorithm;computer science;machine learning;mathematics;statistics	ML	31.123501295814027	-30.74605807524177	183390
1028e38dc24e2a9dc5aa12e99df6cf1ec28375db	group lasso with overlaps: the latent group lasso approach	statistical machine learning;latent variable;support recovery;group lasso;gene expression data;regularization;block norm;sparsity;science learning;structured sparsity;graph;feature selection;breast cancer	We study a norm for structured sparsity which leads to sparse linear predictors whose supports are unions of predefined overlapping groups of variables. We call the obtained formulation latent group Lasso, since it is based on applying the usual group Lasso penalty on a set of latent variables. A detailed analysis of the norm and its properties is presented and we characterize conditions under which the set of groups associated with latent variables are correctly identified. We motivate and discuss the delicate choice of weights associated to each group, and illustrate this approach on simulated data and on the problem of breast cancer prognosis from gene expression data.	lasso;latent variable;sparse matrix	Guillaume Obozinski;Laurent Jacob;Jean-Philippe Vert	2011	CoRR		latent class model;latent variable;regularization;computer science;breast cancer;machine learning;pattern recognition;mathematics;graph;sparsity-of-effects principle;feature selection;statistics	ML	28.25497416444741	-35.2128038306557	183428
8be1a4f42eaa4c69a96724dcc574e30366f14310	workload-driven antijoin cardinality estimation	computacion informatica;bayesian statistics;query optimization;sampling;ciencias basicas y experimentales;monte carlo;grupo a;antijoin operator	Antijoin cardinality estimation is among a handful of problems that has eluded accurate efficient solutions amenable to implementation in relational query optimizers. Given the widespread use of antijoin and subset-based queries in analytical workloads and the extensive research targeted at join cardinality estimation—a seemingly related problem—the lack of adequate solutions for antijoin cardinality estimation is intriguing. In this article, we introduce a novel sampling-based estimator for antijoin cardinality that (unlike existent estimators) provides sufficient accuracy and efficiency to be implemented in a query optimizer. The proposed estimator incorporates three novel ideas. First, we use prior workload information when learning a mixture superpopulation model of the data offline. Second, we design a Bayesian statistics framework that updates the superpopulation model according to the live queries, thus allowing the estimator to adapt dynamically to the online workload. Third, we develop an efficient algorithm for sampling from a hypergeometric distribution in order to generate Monte Carlo trials, without explicitly instantiating either the population or the sample. When put together, these ideas form the basis of an efficient antijoin cardinality estimator satisfying the strict requirements of a query optimizer, as shown by the extensive experimental results over synthetically-generated as well as massive TPC-H data.	algorithm;ibm tivoli storage productivity center;instance (computer science);mathematical optimization;monte carlo method;online and offline;query optimization;relational algebra;relational database;requirement;sampling (signal processing)	Florin Rusu;Zixuan Zhuang;Mingxi Wu;Chris Jermaine	2015	ACM Trans. Database Syst.	10.1145/2818178	sampling;mathematical optimization;query optimization;data mining;database;bayesian statistics;monte carlo method	DB	27.60361533022434	-27.84848418313304	183695
237316762470d72a02795a7f57de9279e9cda16a	dimensionality-reduced subspace clustering		Subspace clustering refers to the problem of clustering unlabeled high-dimensional data points into a union of low-dimensional linear subspaces, whose number, orientations, and dimensions are all unknown. In practice one may have access to dimensionality-reduced observations of the data only, resulting, e.g., from undersampling due to complexity and speed constraints on the acquisition device or mechanism. More pertinently, even if the high-dimensional data set is available it is often desirable to first project the data points into a lower-dimensional space and to perform clustering there; this reduces storage requirements and computational cost. The purpose of this paper is to quantify the impact of dimensionality reduction through random projection on the performance of three subspace clustering algorithms, all of which are based on principles from sparse signal recovery. Specifically, we analyze the thresholding based subspace clustering (TSC) algorithm, the sparse subspace clustering (SSC) algorithm, and an orthogonal matching pursuit variant thereof (SSC-OMP). We find, for all three algorithms, that dimensionality reduction down to the order of the subspace dimensions is possible without incurring significant performance degradation. Moreover, these results are order-wise optimal in the sense that reducing the dimensionality further leads to a fundamentally ill-posed clustering problem. Our findings carry over to the noisy case as illustrated through analytical results for TSC and simulations for SSC and SSC-OMP. Extensive experiments on synthetic and real data complement our theoretical findings.	algorithm;algorithmic efficiency;cluster analysis;clustering high-dimensional data;computation;data point;detection theory;dimensionality reduction;elegant degradation;experiment;matching pursuit;online advertising;openmp;random projection;remote desktop services;requirement;sql server compact;simulation;sparse matrix;synthetic intelligence;thresholding (image processing);undersampling;well-posed problem	Reinhard Heckel;Michael Tschannen;Helmut Bölcskei	2015	CoRR		correlation clustering;constrained clustering;mathematical optimization;data stream clustering;subclu;k-medians clustering;fuzzy clustering;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;statistics;clustering high-dimensional data	ML	25.462077411175482	-36.87475624965919	184005
25e5fb9c1174994281a28ceefdf17c970af2e8e9	focused inference		We develop a method similar to variable elimination for computing approximate marginals in graphical models. An underlying notion in this method is that it is not always necessary to compute marginals over all the variables in the graph, but focus on a few variables of interest. The Focused Inference (FI) algorithm introduced reduces the original distribution to a simpler one over the variables of interest. This is done in an iterative manner where in each step the operations are guided by (local) optimality properties. We exemplify various properties of the focused inference algorithm and compare it with other methods. Numerical simulation indicates that FI outperform competing methods.	approximation algorithm;cobham's thesis;exemplification;graphical model;iterative method;local optimum;numerical method;planar graph;simulation;tree (data structure);variable elimination	Rómer Rosales;Tommi S. Jaakkola	2005				AI	26.326186969144597	-29.32264599708298	184211
8539fd580d1cd5d8b42c6fda89ccd47a74a82031	recursive bayesian estimation under memory limitation			recursion (computer science)	Rudolf Kulhavý	1990	Kybernetika		mathematics;recursive partitioning;probability theory;recursive bayesian estimation;machine learning;turing machine;artificial intelligence;estimation theory	ML	28.221003138068202	-29.323573207076244	184464
47bd308b5ee2b5182bcff5a424f969976909b419	chain graphs for learning	feed forward;chain graph;bayes classifier	Chain graphs combine directed and undi­ rected graphs and their underlying mathe­ matics combines properties of the two. This paper gives a simplified definition of chain graphs based on a hierarchical combination of Bayesian (directed) and Markov (undirected) networks. Examples of a chain graph are multivariate feed-forward networks, cluster­ ing with conditional interaction between vari­ ables, and forms of Bayes classifiers. Chain graphs are then extended using the notation of plates so that samples and data analysis problems can be represented in a graphical model as well. Implications for learning are discussed in the conclusion.	graph (discrete mathematics);graphical model;hidden markov model;markov chain;mixed graph	Wray L. Buntine	1995			bayes classifier;combinatorics;graph product;computer science;graph theory;machine learning;comparability graph;pattern recognition;mathematics;tree-depth;graph;maximal independent set;graphical model;modular decomposition;partial k-tree;indifference graph;feed forward;line graph;statistics	ML	25.12900228470429	-27.87426448290564	184645
1935cdb485247a1f7cb867320aa9b5592ea1253d	a non-convex one-pass framework for generalized factorization machines and rank-one matrix sensing		We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from d dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank k, our algorithm converges linearly, achieves O(ǫ) recovery error after retrieving O(kd log(1/ǫ)) training instances, consumes O(kd) memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval.	algorithm;coefficient;iteration;matrix multiplication;phase retrieval;provable security;rate of convergence	Ming Lin;Jieping Ye	2016				ML	25.863549084347095	-35.79865329144	184737
4f8bd116271408e80f9d14c5458f61962ac43257	robust m-estimation for array processing: a random matrix approach		This article studies the limiting behavior of a robust M-estimator of population covariance matrices as both the number of available samples and the population size are large. Using tools from random matrix theory, we prove that the difference between the sample covariance matrix and (a scaled version of) the robust M-estimator tends to zero in spectral norm, almost surely. This result is applied to prove that recent subspace methods arising from random matrix theory can be made robust without altering their first order behavior.	array processing	Romain Couillet;Frédéric Pascal;Jack W. Silverstein	2012	CoRR		applied mathematics;random matrix;matrix norm;almost surely;subspace topology;population;mathematics;covariance;matrix (mathematics);array processing	ML	31.579545998002104	-27.47390303814771	184891
2744f77d013c9e33a13e512f11ab6c0cdb2f75da	towards understanding sparse filtering: a theoretical perspective	cosine metric;feature distribution learning;information preservation;intrinsic structure;soft clustering;sparse filtering	Abstract In this paper we present a theoretical analysis to understand sparse filtering, a recent and effective algorithm for unsupervised learning. The aim of this research is not to show whether or how well sparse filtering works, but to understand why and when sparse filtering does work. We provide a thorough theoretical analysis of sparse filtering and its properties, and further offer an experimental validation of the main outcomes of our theoretical analysis. We show that sparse filtering works by explicitly maximizing the entropy of the learned representations through the maximization of the proxy of sparsity, and by implicitly preserving mutual information between original and learned representations through the constraint of preserving a structure of the data. Specifically, we show that the sparse filtering algorithm implemented using an absolute-value non-linearity determines the preservation of a data structure defined by relations of neighborhoodness under the cosine distance . Furthermore, we empirically validate our theoretical results with artificial and real data sets, and we apply our theoretical understanding to explain the success of sparse filtering on real-world problems. Our work provides a strong theoretical basis for understanding sparse filtering: it highlights assumptions and conditions for success behind this feature distribution learning algorithm, and provides insights for developing new feature distribution learning algorithms.	advance directive - proxy;algorithm;biologic preservation;collaborative filtering;cosine distance method;cosine similarity;data structure;entropy maximization;machine learning;mutual information;nonlinear system;sparse matrix;unsupervised learning	Fabio Massimo Zennaro;K X Chen	2018	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2017.11.010	machine learning;artificial intelligence;collaborative filtering;unsupervised learning;k-svd;mutual information;filter (signal processing);data structure;sparse approximation;computer science;maximization	ML	26.83057757369677	-37.116031399633904	184914
72e6124e5e154b06207ecae564e9e6094d9cc761	performance analysis of joint-sparse recovery from multiple measurement vectors via convex optimization: which prior information is better?		"""In sparse signal recovery of compressive sensing, the phase transition determines the edge, which separates successful recovery and failed recovery. The phase transition can be seen as an indicator and an intuitive way to judge, which recovery performance is better. Traditionally, the multiple measurement vectors (MMVs) problem is usually solved via <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1}$ </tex-math></inline-formula>-norm minimization, which is our first investigation via conic geometry in this paper. Then, we are interested in the same problem but with two common constraints (or prior information): prior information relevant to the ground truth and the inherent low rank within the original signal. To figure out which constraint is most helpful, the MMVs problems are solved via <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1}$ </tex-math></inline-formula>-<inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1}$ </tex-math></inline-formula> minimization and <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1}$ </tex-math></inline-formula>-low rank minimization, respectively. By theoretically presenting the necessary and sufficient condition of successful recovery from MMVs, we can have a precise prediction of phase transition to judge, which constraint or prior information is better. All our findings are verified via simulations and show that, under certain conditions, <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1}$ </tex-math></inline-formula>-<inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1}$ </tex-math></inline-formula> minimization outperforms <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1}$ </tex-math></inline-formula>-low rank minimization. Surprisingly, <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1}$ </tex-math></inline-formula>-low rank minimization performs even worse than <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{2,1}$ </tex-math></inline-formula>-norm minimization. To the best of our knowledge, we are the first to study the MMVs problem under different prior information in the context of compressive sensing."""	compressed sensing;convex optimization;detection theory;enigma machine;ground truth;profiling (computer programming);simulation;sparse matrix	Shih-Wei Hu;Gang-Xuan Lin;Sung-Hsien Hsieh;Chun-Shien Lu	2018	IEEE Access	10.1109/ACCESS.2018.2791580	compressed sensing;mathematical optimization;conic section;computer science;sparse matrix;distributed computing;convex optimization;decoding methods;convex function	ML	27.884021157276504	-36.77752677009739	185000
d02d1781e345e08a21d1dcff4f1b345ed4c3b023	tool wear monitoring based on novel evolutionary artificial neural networks	signal feature extraction;evolutionary artificial neural network;machining;tool wear monitoring multi model neural networks genetic algorithm;vibrations;wear;neural networks;neural nets;cutting tools;evolutionary neural network;tool wear monitoring;backpropagation;force;production engineering computing;wear acoustic emission backpropagation condition monitoring cutting tools feature extraction genetic algorithms neural nets production engineering computing signal classification tools vibrations;variable string genetic algorithm;artificial neural networks;monitoring system;online tool wear monitoring system;condition monitoring;monitoring;vibration;feature extraction;signal processing;classification precision;backpropagation algorithm;multi model;tool wear;signal classification;classification precision evolutionary artificial neural network online tool wear monitoring system variable string genetic algorithm signal feature extraction cutting force vibration acoustic emission signal processing conformable connection weight backpropagation algorithm;genetic algorithm;genetic algorithms;tools;acoustic emission;cutting force;back propagation;feature extraction artificial neural networks monitoring machining vibrations equations force;artificial neural network;conformable connection weight;neural network	In order to improve the accuracy and speed of on-line tool wear monitoring system, an evolutionary neural network using variable string genetic algorithm (VGA) was developed to construct the relations between tool wear and signal features extracted from cutting forces, vibrations, and acoustic emission by different signal processing methods. The system could automatically evolve the appropriate architecture of neural network and find a near-optimal set of connection weights globally. Then the conformable connection weights for model could be found with back-propagation (BP) algorithm, the multi-model finally completed calculation of tool wear. The experimental results show that the system proposed in the paper has higher classification precision and calculating speed.	acoustic cryptanalysis;artificial neural network;backpropagation;coefficient;genetic algorithm;mathematical optimization;network packet;online and offline;signal processing;software propagation;video graphics array;wavelet	Hongli Gao;Dengwan Li;Mingheng Xu;Min Zhao;Xiaohui Shi;HaiFeng Huang	2010	2010 Sixth International Conference on Natural Computation	10.1109/ICNC.2010.5583585	engineering;artificial intelligence;machine learning;engineering drawing	Robotics	37.7456521631617	-31.683703517042368	185315
13db14344438f1e65890fb9807feddb28fe475be	weakly supervised learning using proportion-based information: an application to fisheries acoustics	signal classification acoustic signal processing aquaculture learning artificial intelligence;marine animals;nonlinear classification models;supervised learning;fisheries acoustics;acoustics;fish school classification;training;acoustic signal processing;aquaculture;fish school classification weakly supervised learning proportion based information fisheries acoustics probabilistic classification models proportion based training data nonlinear classification models;computational modeling;signal classification;probabilistic classification models;supervised learning aquaculture acoustic applications marine animals educational institutions sonar training data physics computing acoustic devices marine vegetation;weakly supervised learning;learning artificial intelligence;proportion based training data;proportion based information	This paper addresses the inference of probabilistic classification models using weakly supervised learning. In contrast to previous work, the use of proportion-based training data is investigated in combination to non-linear classification models. An application to fisheries acoustics and fish school classification is considered and experiments are reported for synthetic and real datasets.	experiment;linear classifier;nonlinear system;statistical classification;supervised learning;synthetic intelligence	Ronan Fablet;Riwal Lefort;Carla Scalarin;Jacques Masse;Paul Cauchy;Jean-Marc Boucher	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761065	aquaculture;speech recognition;computer science;machine learning;pattern recognition;supervised learning;computational model	Vision	33.317195195223114	-36.5991604057678	185319
95ed213d44adc6a9899ced2af6a75e30a6d9abe8	multi-fault classification based on wavelet svm with pso algorithm to analyze vibration signals from rolling element bearings	distance evaluation technique;rolling element bearings;particle swarm optimization;wavelet support vector machine;empirical model decomposition;fault diagnosis	Condition monitoring and fault diagnosis of rolling element bearings timely and accurately is very important to ensure the reliable operation of rotating machinery. In this paper, a multi-fault classification model based on the kernel method of support vector machines (SVM) and wavelet frame, wavelet basis were introduced to construct the kernel function of SVM, and wavelet support vector (PSO) is applied to optimize unknown parameters of WSVM. In this work, the vibration signals measured from rolling element bearings are preprocessed using empirical model decomposition (EMD). Moreover, a distance evaluation technique is performed to remove the redundant and irrelevant information and select the salient features for the classification process. Hence, a relatively new hybrid intelligent fault detection and classification method based on EMD, distance evaluation technique and WSVM with PSO is proposed. This method is validated on a rolling element bearing test bench and then applied to the bearing fault diagnosis for electric locomotives. Compared with the commonly used SVM, the WSVM can achieve a greater accuracy. The results indicated that the proposed method can reliably identify different fault patterns of rolling element bearings based on the vibration signals. & 2012 Elsevier B.V. All rights reserved.	algorithm;fault detection and isolation;kernel method;particle swarm optimization;phase-shift oscillator;relevance;support vector machine;test bench;wavelet	Zhiwen Liu;Hongrui Cao;Xuefeng Chen;Zhengjia He;Zhongjie Shen	2013	Neurocomputing	10.1016/j.neucom.2012.07.019	computer science;machine learning;pattern recognition;particle swarm optimization	AI	37.18733433889202	-30.811162222736122	185446
02f34f9d891ec0561439008028a4059db52f3aac	distributed coverage maximization via sketching		Coverage problems are central in optimization and have a wide range of applications in data mining and machine learning. While several distributed algorithms have been developed for coverage problems, the existing methods suffer from several limitations, e.g., they all achieve either suboptimal approximation guarantees or suboptimal space and memory complexities. In addition, previous algorithms developed for submodular maximization assume oracle access, and ignore the computational complexity of communicating large subsets or computing the size of the union of the subsets in this subfamily. In this paper, we develop an improved distributed algorithm for the k-cover and the set cover with λ outliers problems, with almost optimal approximation guarantees, almost optimal memory complexity, and linear communication complexity running in only four rounds of computation. Finally, we perform an extensive empirical study of our algorithms on a number of publicly available real data sets, and show that using sketches of size 30 to 600 times smaller than the input, one can solve the coverage maximization problem with quality very close to that of the state-of-the-art single-machine algorithm.	algorithmic efficiency;approximation;communication complexity;computation;computational complexity theory;covering problems;dspace;data mining;distributed algorithm;expectation–maximization algorithm;machine learning;mathematical optimization;set cover problem;streaming media;submodular set function	MohammadHossein Bateni;Hossein Esfandiari;Vahab S. Mirrokni	2016	CoRR		mathematical optimization;combinatorics;maximum coverage problem;theoretical computer science;machine learning;mathematics;algorithm	ML	25.278776191248177	-33.41654415804367	185771
97cbaa337b88fd81367177a979de9f759c2fa715	blind signal separation using fixed overcomplete basis function dictionaries	libraries;learning process;sensor systems;correlation methods blind source separation dictionaries;correlation method;blind source separation;speech analysis;independent component analysis;correlation methods;fixed dictionary;overcomplete basis function set;vector quantization;probability distribution;dictionaries;correlation method blind signal separation underdetermined system overcomplete basis function set fixed dictionary learning process;blind signal separation;underdetermined system;data models;blind source separation dictionaries independent component analysis equations libraries data models speech analysis sensor systems vector quantization probability distribution	A solution for achieving blind separation for underdetermined systems is to use an overcomplete basis function set that has the ability to span all possible inputs. Ideally, such a basis would be learned for each set of inputs but this is computationally expensive. A less processor intensive system is shown using a fixed dictionary of basis functions learned from existing sources and reduced using a correlation-based method. The relation between dictionary size and separation performance for underdetermined scenarios is examined and we demonstrate that a reduced dictionary can produce comparable results using less computational power.	analysis of algorithms;authorization;basis function;basis set (chemistry);blind signal separation;compiler;computation;data dictionary;ieee xplore	Paul Sugden;Cedric Nishan Canagarajah	2003		10.1109/ISCAS.2003.1204951	speech recognition;computer science;machine learning;pattern recognition;mathematics;blind signal separation	PL	33.50565250727556	-32.345170444795166	186167
8c4309e94680d65ccd51e6ff685e7d8893116674	parallel markov chain monte carlo via spectral clustering		As it has become common to use many computer cores in routine applications, finding good ways to parallelize popular algorithms has become increasingly important. In this paper, we present a parallelization scheme for Markov chain Monte Carlo (MCMC) methods based on spectral clustering of the underlying state space, generalizing earlier work on parallelization of MCMC methods by state space partitioning. We show empirically that this approach speeds up MCMC sampling for multimodal distributions and that it can be usefully applied in greater generality than several related algorithms. Our algorithm converges under reasonable conditions to an ‘optimal’ MCMC algorithm. We also show that our approach can be asymptotically far more efficient than naive parallelization, even in situations such as completely flat target distributions where no unique optimal algorithm exists. Finally, we combine theoretical and empirical bounds to provide practical guidance on the choice of tuning parameters.	algorithm;cluster analysis;markov chain monte carlo;monte carlo method;multimodal interaction;parallel computing;sampling (signal processing);space partitioning;spectral clustering;state space	Guillaume W. Basse;Aaron Smith;Natesh S. Pillai	2016			econometrics;mathematical optimization;mathematics;statistics	ML	27.369683142009393	-28.352197850025817	186233
32e443fff3f67ca8325b5b657001122871829e5f	stress functions for nonlinear dimension reduction, proximity analysis, and graph drawing	unsupervised learning;box cox transformations;force directed layout;cluster analysis;multidimensional scaling;clustering strength	Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called “stress functions”. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following benefits and problem solutions: (a) It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs.	emoticon;graph drawing;hoc (programming language);image scaling;mathematical optimization;multidimensional scaling;nonlinear dimensionality reduction;nonlinear system	Lisha Chen;Andreas Buja	2013	Journal of Machine Learning Research		unsupervised learning;combinatorics;discrete mathematics;multidimensional scaling;computer science;machine learning;mathematics;cluster analysis;statistics	ML	30.91156033056413	-35.28420086979657	186371
882e47ecab26ab9227a21131ae22e15c62af68ed	convex regression with interpretable sharp partitions	non additivity;convex optimization;interpretability;non parametric regression;prediction	We consider the problem of predicting an outcome variable on the basis of a small number of covariates, using an interpretable yet non-additive model. We propose convex regression with interpretable sharp partitions (CRISP) for this task. CRISP partitions the covariate space into blocks in a data-adaptive way, and fits a mean model within each block. Unlike other partitioning methods, CRISP is fit using a non-greedy approach by solving a convex optimization problem, resulting in low-variance fits. We explore the properties of CRISP, and evaluate its performance in a simulation study and on a housing price data set.	additive model;computer retrieval of information on scientific projects database;convex function;convex optimization;cross industry standard process for data mining;fits;greedy algorithm;mathematical optimization;optimization problem;sample variance;simulation	Ashley Petersen;Noah Simon;David M. Witten	2016	Journal of machine learning research : JMLR		econometrics;mathematical optimization;convex optimization;prediction;mathematics;nonparametric regression;statistics	ML	28.98050960898316	-25.036773713197338	186696
00962655fc63ee46cf44c9a3ad150fb1b0ab7bd5	an inequality with applications to structured sparsity and multitask dictionary learning		Abstract From concentration inequalities for the suprema of Gaussian or Rademacher processes an inequality is derived. It is applied to sharpen existing and to derive novel bounds on the empirical Rademacher complexities of unit balls in various norms appearing in the context of structured sparsity and multitask dictionary learning or matrix factorization. A key role is played by the largest eigenvalue of the data covariance matrix.	computer multitasking;dictionary;machine learning;rademacher complexity;social inequality;sparse matrix	Andreas Maurer;Massimiliano Pontil;Bernardino Romera-Paredes	2014			mathematical optimization;machine learning;pattern recognition;mathematics;rademacher complexity	ML	25.0965047298524	-35.90469501149875	186862
f5d1d92caeec29ee0320ace86e9cbf33874800de	regularized k-order markov models in edas	optimization problem;probabilistic model;estimation of distribution algorithm;markov model;markov models;protein folding;probabilistic modeling;hp protein model;estimation of distribution algorithms;hydrophobic polar	k-order Markov models have been introduced to estimation of distribution algorithms (EDAs) to solve a particular class of optimization problems in which each variable depends on its previous k variables in a given, fixed order. In this paper we investigate the use of regularization as a way to approximate k-order Markov models when $k$ is increased. The introduced regularized models are used to balance the complexity and accuracy of the k-order Markov models. We investigate the behavior of the EDAs in several instances of the hydrophobic-polar (HP) protein problem, a simplified protein folding model. Our preliminary results show that EDAs that use regularized approximations of the k-order Markov models offer a good compromise between complexity and efficiency, and could be an appropriate choice when the number of variables is increased.	approximation algorithm;estimation of distribution algorithm;markov chain;markov model;mathematical optimization;optimization problem	Roberto Santana;Hossein Karshenas;Concha Bielza;Pedro Larrañaga	2011		10.1145/2001576.2001658	markov chain;mathematical optimization;maximum-entropy markov model;markov kernel;variable-order bayesian network;markov property;estimation of distribution algorithm;computer science;machine learning;mathematics;additive markov chain;markov process;markov model;hidden markov model;statistics;variable-order markov model	AI	26.614411305656894	-29.86626733141707	187167
4ac5a81e1a5f692b48d6dd442861ad7599193e82	odor change of citrus juice during storage based on electronic nose technology		In order to master the law of citrus juice odor components changes during the storing process, electronic nose composed of metal-oxide semiconductor (MOS) sensors array is used to monitor the odor during valencia oranges juice storing process. A self-made electronic nose system and experiment are described in detail, after data preprocessing, extreme learning machine (ELM) is used for analysis on samples. Analysis result indicates that the odor synthesized curve derived from the electronic nose technology can reflect overall trend of odor during valencia oranges juice storing process truly and effectively, and the experimental results prove that the E-nose can correctly distinguish the current stage of the stored valencia oranges juice and the classification accuracy of test data set is 96.29% when ELM is used as the classifier, which shows that the E-nose can be successfully applied to the qualitative analysis of citrus.		Xue Jiang;Pengfei Jia;Siqi Qiao;Shukai Duan	2017		10.1007/978-3-319-70139-4_32	citrus juice;machine learning;artificial intelligence;odor;computer science;extreme learning machine;pattern recognition;data pre-processing;electronic nose;test data;classifier (linguistics)	EDA	36.7228477125929	-34.223151850042534	187301
4f6fa2b6d0e43c3ee94053e48075c5538db0b3dd	nonparametric estimation of conditional information and divergences		In this paper we propose new nonparametric estimators for a family of conditional mutual information and divergences. Our estimators are easy to compute; they only use simple k nearest neighbor based statistics. We prove that the proposed conditional information and divergence estimators are consistent under certain conditions, and demonstrate their consistency and applicability by numerical experiments on simulated and on real data as well.	conditional entropy;conditional mutual information;experiment;numerical analysis	Barnabás Póczos;Jeff G. Schneider	2012			econometrics;conditional variance;pattern recognition;mathematics;statistics	ML	30.02112237691615	-24.723745711713168	187616
7405b59081eb5b4c8e5fbbbf0f804510fe744eb5	extended bayesian information criteria for gaussian graphical models	sample size;information criteria;connected graph;cross validation;gaussian graphical model;bayesian information criterion;covariance matrix	Gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n.	algorithm;bayesian information criterion;cross-validation (statistics);graphical model;graphical user interface;lasso;mathematical optimization;penalty method;sparse matrix	Rina Foygel;Mathias Drton	2010			sample size determination;econometrics;covariance matrix;connectivity;mathematics;graphical model;bayesian information criterion;cross-validation;statistics	ML	29.61681475456183	-26.49145615028724	187656
aedfbb569eb86669dffd658208f24c8b8f72abbb	dynamic k-nearest neighbors for the monitoring of evolving systems	nonlinear filters;industrial system;welding quality monitoring;detection phase;dynamic k nearest neighbors;evolving systems monitoring;acoustics;gravity;welding;dynamic system;dynamic knn;functioning modes evolutions;monitoring;acoustic noise;maximum likelihood detection;pattern recognition;acoustic noise dynamic k nearest neighbors evolving systems monitoring pattern recognition functioning modes evolutions dynamic system functioning mode dynamic knn detection phase adaptation phase welding quality monitoring industrial system;welding noise monitoring maximum likelihood detection acoustics nonlinear filters gravity;k nearest neighbor;dynamic system functioning mode;welding pattern recognition;adaptation phase;noise	In this article, a new Pattern Recognition (PR) approach is proposed to monitor the functioning modes evolutions in dynamic systems. When a functioning mode evolves, the system characteristics change and the observations, i.e. the patterns, obtained on the system change too. In this case, classes representing the system functioning modes have to be updated by keeping representative patterns only. The developed PR approach is based on the K-Nearest Neighbors (KNN) method. It is named Dynamic KNN (DKNN) and comprises two phases: a detection phase to detect and confirm classes evolutions and an adaptation phase realized incrementally to update the evolved classes parameters and reduce the dataset. To illustrate this approach, the monitoring of weldings quality (good or bad) is realized on an industrial system, based on acoustic noises issued of weldings operations.	acoustic cryptanalysis;dynamical system;emergence;k-nearest neighbors algorithm;pattern recognition	Laurent Hartert;Moamar Sayed Mouchaweh;Patrice Billaudel	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584331	speech recognition;gravity;computer science;noise;artificial intelligence;dynamical system;machine learning;noise;k-nearest neighbors algorithm;welding	Robotics	37.371518263672414	-32.06501776936383	187708
00b28a7d380ec7f04850fc299f47c3a80028b541	gaussian process latent variable models for fault detection	principal component analysis data analysis data reduction gaussian processes;gaussian processes;dimension reduction;dimensionality reduction gaussian process latent variable models fault detection unsupervised approach linear dimension reduction principal component analysis data visualisation data analysis;latent variable model;data analysis;dimensionality reduction;principal component analysis fault detection dimensionality reduction;principal component analysis;fault detection;gaussian processes fault detection principal component analysis testing data analysis condition monitoring automatic control information analysis failure analysis computational intelligence;data reduction;gaussian process;dimensional reduction	The Gaussian process latent variable model (GPLVM) is a novel unsupervised approach to nonlinear low dimensional embedding proposed by Lawrence (2005). This paper presents the development of a framework for the implementation of the GPLVM for fault detection. A series of experiments have been carried out comparing and combining the GPLVM to the conventional and widely used linear dimension reduction technique of principal component analysis (PCA). The inclusion of the GPLVM for the visualisation and data analysis, led to a considerable improvement in the classification results	dimensionality reduction;experiment;fault detection and isolation;gaussian process;latent variable model;nonlinear system;principal component analysis;unsupervised learning	Luka Eciolaza;Muhammad Alkarouri;Neil D. Lawrence;Visakan Kadirkamanathan;Peter J. Fleming	2007	2007 IEEE Symposium on Computational Intelligence and Data Mining	10.1109/CIDM.2007.368886	computer science;machine learning;pattern recognition;gaussian process;mathematics;statistics;dimensionality reduction	SE	30.497699461649958	-28.387242858262354	187761
2e468aa380a0d4883476a20a311d6fe90e79777c	information-space partitioning and symbolization of multi-dimensional time-series data using density estimation	kernel;density measurement;training;time series multidimensional systems nonparametric statistics;estimation;vector valued measurement information space partitioning multidimensional time series nonparametric density estimation symbolization technique statistical behavior dynamic data driven application system;aerospace electronics;optimization;entropy;time series analysis data driven models kernel based density estimation linear programming;estimation optimization training kernel aerospace electronics entropy density measurement	This paper proposes a nonparametric density estimation-based information-space partitioning and symbolization technique for capturing and representing the underlying statistical behavior in dynamic data-driven application systems (DDDAS). In contrast with existing tools that address alphabet-size selection and partitioning in two separate steps, the proposed technique jointly determines both the number of symbols (i.e., the alphabet size) and the regions associated with these symbols (i.e., the information-space partition) in a single step. In order to validate the technique, dynamic data-driven models have been developed from time series of vector-valued measurements extracted from a simulation testbed, and are compared with models that rely on Gaussian mixture modeling for information-space partitioning.	cross-validation (statistics);dynamic data driven applications systems;error detection and correction;experiment;google map maker;high- and low-level;linear programming;linear separability;mathematical optimization;nonlinear system;optimization problem;partition problem;scalability;simulation;space partitioning;sparse matrix;testbed;time series;voronoi diagram	Nurali Virani;Ji-Woong Lee;Shashi Phoha;Asok Ray	2016	2016 American Control Conference (ACC)	10.1109/ACC.2016.7525431	econometrics;entropy;estimation;kernel;computer science;machine learning;multivariate kernel density estimation;mathematics;variable kernel density estimation;statistics	EDA	30.496539507131853	-30.93871441391993	187807
732026582f85ecb5d3a0d8e711fdda59cefbe293	the analysis of multivariate data using semi-definite programming	kernel methods;sign pca;kernel trick;sdp;positive semi definite;pca;fa;multivariate statistical analysis	A model is presented for analyzing general multivariate data. The model puts as its prime objective the dimensionality reduction of the multivariate problem. The only requirement of the model is that the input data to the statistical analysis be a covariance matrix, a correlation matrix, or more generally a positive semi-definite matrix. The model is parameterized by a scale parameter and a shape parameter both of which take on non-negative values smaller than unity. We first prove a wellknown heuristic for minimizing rank and establish the conditions under which rank can be replaced with trace. This result allows us to solve our rank minimization problem as a Semi-Definite Programming (SDP) problem by a number of available solvers. We then apply the model to four case studies dealing with four well-known problems in multivariate analysis. The first problem is to determine the number of underlying factors in factor analysis (FA) or the number of retained components in principal component analysis (PCA). It is shown that our model determines the number of factors or components more efficiently than the commonly used methods. The second example deals with a problem that has received much attention in recent years due to its wide applications, and it concerns sparse principal components and variable selection in PCA. When applied to a data set known in the literature as the pitprop data, we see that our approach yields PCs with larger variances than PCs derived from other approaches. The third problem concerns sensitivity analysis of the multivariate models, a topic not widely researched in the sequel due to its difficulty. Finally, we apply the model to a difficult problem in PCA known as lack of scale invariance in the solutions of PCA. This is the problem that the solutions derived from analyzing the covariance matrix in PCA are generally different (and not linearly related to) the solutions derived from analyzing the correlation matrix. Using our model, we obtain the same solution whether we analyze the correlation matrix or the covariance matrix since the analysis utilizes only the signs of the correlations/covariances but not their values. This is where we introduce a new type of PCA, called Sign PCA, which we speculate on its applications in social sciences and other fields of science.	semiconductor industry;semidefinite programming	A. H. Al-Ibrahim	2015	J. Classification	10.1007/s00357-015-9184-0	kernel method;econometrics;sparse pca;computer science;machine learning;mathematics;statistics;principal component analysis	ML	26.810167612298656	-36.94563455115976	188152
c4e539d70155e5dc6d65416aaa59fb0ec9ea34dc	asymptotic properties of multivariate tapering for estimation and prediction	one taper likelihood;gaussian random field;primary62m30;domain increasing;sparse matrix;secondary62f12	Parameter estimation for and prediction of spatially or spatio–temporally correlated random processes are used in many areas and often require the solution of a large linear system based on the covariance matrix of the observations. In recent years, the dataset sizes to which these methods are applied have steadily increased such that straightforward statistical tools are computationally too expensive to be used. In the univariate context, tapering, i.e., creating sparse approximate linear systems, has been shown to be an efficient tool in both the estimation and prediction settings. The asymptotic properties are derived under an infill asymptotic setting. In this paper we use a domain increasing framework for estimation and prediction using multivariate tapering. Under this asymptotic regime we prove that tapering (one-tapered form) preserves the consistency of the untapered maximum likelihood estimator and show that tapering has asymptotically the same mean squared prediction error as using the corresponding untapered predictor. The theoretical results are illustrated with simulations.	approximation algorithm;asymptote;estimation theory;kerrison predictor;linear system;mean squared prediction error;simulation;sparse matrix;stochastic process	Reinhard Furrer;François Bachoc;Juan Du	2016	J. Multivariate Analysis	10.1016/j.jmva.2016.04.006	gaussian random field;econometrics;mathematical optimization;sparse matrix;mathematics;statistics	ML	28.289800380167947	-27.766882842437855	188237
fa1290ed4dea6dbc21337d7c82140d62bbd7383f	probabilistic programming in python using pymc3		Probabilistic Programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source Probabilistic Programming framework written in Python that uses Theano to compute gradients via automatic dierentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other Probabilistic Programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.	python	John Salvatier;Thomas V. Wiecki;Christopher Fonnesbeck	2016	PeerJ PrePrints	10.7287/peerj.preprints.1686v1	software framework;probabilistic relevance model;python (programming language);probabilistic logic;theoretical computer science;probabilistic ctl;machine learning;hybrid monte carlo;theano;computer science;markov chain monte carlo;artificial intelligence	HCI	26.044744917497752	-28.876986299851975	188275
c032ab759b8586bdf0e314e20e89c98d01383a33	efficient l1-norm principal-component analysis via bit flipping		"""It was shown recently that the <inline-formula><tex-math notation=""""LaTeX"""">$K$</tex-math></inline-formula> L1-norm principal components (L1-PCs) of a real-valued data matrix <inline-formula><tex-math notation=""""LaTeX"""">$\mathbf X \in \mathbb {R}^{D \times N}$</tex-math></inline-formula> (<inline-formula><tex-math notation=""""LaTeX"""">$N$</tex-math> </inline-formula> data samples of <inline-formula><tex-math notation=""""LaTeX"""">$D$</tex-math></inline-formula> dimensions) can be exactly calculated with cost <inline-formula><tex-math notation=""""LaTeX"""">$\mathcal {O}(2^{NK})$ </tex-math></inline-formula> or, when advantageous, <inline-formula><tex-math notation=""""LaTeX"""">$\mathcal {O}(N^{dK - K + 1})$</tex-math></inline-formula> where <inline-formula><tex-math notation=""""LaTeX"""">$d=\mathrm{rank}(\mathbf X)$ </tex-math></inline-formula>, <inline-formula><tex-math notation=""""LaTeX"""">$K<d$</tex-math></inline-formula>. In applications where <inline-formula><tex-math notation=""""LaTeX"""">$\mathbf X$</tex-math></inline-formula> is large (e.g., “big” data of large <inline-formula><tex-math notation=""""LaTeX"""">$N$</tex-math></inline-formula> and/or “heavy” data of large <inline-formula><tex-math notation=""""LaTeX"""">$d$</tex-math></inline-formula>), these costs are prohibitive. In this paper, we present a novel suboptimal algorithm for the calculation of the <inline-formula><tex-math notation=""""LaTeX"""">$K < d$</tex-math></inline-formula> L1-PCs of <inline-formula> <tex-math notation=""""LaTeX"""">$\mathbf X$</tex-math></inline-formula> of cost <inline-formula><tex-math notation=""""LaTeX""""> $\mathcal O (ND \mathrm{min} \lbrace N,D\rbrace + N^2K^2(K^2 + d))$</tex-math></inline-formula>, which is comparable to that of standard L2-norm PC analysis. Our theoretical and experimental studies show that the proposed algorithm calculates the exact optimal L1-PCs with high frequency and achieves higher value in the L1-PC optimization metric than any known alternative algorithm of comparable computational cost. The superiority of the calculated L1-PCs over standard L2-PCs (singular vectors) in characterizing potentially faulty data/measurements is demonstrated with experiments in data dimensionality reduction and disease diagnosis from genomic data."""	algorithm;algorithmic efficiency;big data;computation;dimensionality reduction;experiment;mathematical optimization;norsk data;principal component analysis;taxicab geometry	Panos P. Markopoulos;Sandipan Kundu;Shubham Chamadia;Dimitris A. Pados	2017	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2708023	mathematical optimization;combinatorics;mathematics;statistics	ML	25.56485690366783	-36.22483775295034	188278
2c9f60b324a412da1e351a47c3fdc22d84f67f35	algorithms for variable length markov chain modeling	modelizacion;markov chain model;chaine markov;cadena markov;modelo markov;bioinformatique;result;algorithme;modelisation;algorithm;markov model;resultado;resultat;bioinformatica;modele markov;modeling;bioinformatics;algoritmo;markov chain	UNLABELLED We present a general purpose implementation of variable length Markov models. Contrary to fixed order Markov models, these models are not restricted to a predefined uniform depth. Rather, by examining the training data, a model is constructed that fits higher order Markov dependencies where such contexts exist, while using lower order Markov dependencies elsewhere. As both theoretical and experimental results show, these models are capable of capturing rich signals from a modest amount of training data, without the use of hidden states.   AVAILABILITY The source code is freely available at http://www.soe.ucsc.edu/~jill/src/	algorithm;fits;markov chain;markov model;source code	Gill Bejerano	2004	Bioinformatics	10.1093/bioinformatics/btg489	econometrics;markov chain;maximum-entropy markov model;markov kernel;variable-order bayesian network;partially observable markov decision process;continuous-time markov chain;hidden semi-markov model;mathematics;additive markov chain;markov algorithm;markov process;markov model;algorithm;hidden markov model;statistics;variable-order markov model	Comp.	25.711335973749236	-29.805139633226773	188579
7afb6be709be11a65f0d9f445378b785e1f0e984	the effect of ambiguous prior knowledge on bayesian model parameter inference and prediction	density ratio class;bayesian inference;intersubjective knowledge;robust bayesian analysis;imprecise probabilities;marginalization and prediction;interval probabilities	Environmental modeling often requires combining prior knowledge with information obtained from data. The robust Bayesian approach makes it possible to consider ambiguity in this prior knowledge. Describing such ambiguity using sets of probability distributions defined by the Density Ratio Class has important conceptual advantages over alternative robust formulations. Earlier studies showed that the Density Ratio Class is invariant under Bayesian inference and marginalization. We prove that (i) the Density Ratio Class is also invariant under propagation through deterministic models, whereas (ii)?predictions of a stochastic model with parameters defined by a Density Ratio Class are embedded in a Density Ratio Class. These invariance properties make it possible to describe sequential learning and prediction under a unified framework. We developed numerical algorithms to minimize the additional computational burden relative to the use of single priors. Practical feasibility of these methods is demonstrated by their application to a simple ecological model. Display Omitted There is often ambiguity about the choice of Bayesian prior probability distribution.The Density Ratio Class represents such ambiguity using sets of densities.We show that this class is invariant under inference, marginalization, and prediction.Such properties are conceptually satisfying and enable computational efficiency.We demonstrate concepts and new algorithms using a simple ecological model.	bayesian network	Simon L. Rinderknecht;Carlo Albert;Mark E. Borsuk;Nele Schuwirth;Hans R. Künsch;Peter Reichert	2014	Environmental Modelling and Software	10.1016/j.envsoft.2014.08.020	econometrics;machine learning;mathematics;bayesian statistics;bayesian inference;statistics	ML	27.380019158057657	-25.686900582136307	188737
348f23af3d4005c7b497675e11afd58096880167	quantifying and reducing model-form uncertainties in reynolds-averaged navier-stokes simulations: a data-driven, physics-informed bayesian approach		Despite their well-known limitations, Reynolds-Averaged Navier-Stokes (RANS) models are still the workhorse tools for turbulent flow simulations in today's engineering analysis, design and optimization. While the predictive capability of RANS models depends on many factors, for many practical flows the turbulence models are by far the largest source of uncertainty. As RANS models are used in the design and safety evaluation of many mission-critical systems such as airplanes and nuclear power plants, quantifying their model-form uncertainties has significant implications in enabling risk-informed decision-making. In this work we develop a data-driven, physics-informed Bayesian framework for quantifying model-form uncertainties in RANS simulations. Uncertainties are introduced directly to the Reynolds stresses and are represented with compact parameterization accounting for empirical prior knowledge and physical constraints (e.g., realizability, smoothness, and symmetry). An iterative ensemble Kalman method is used to assimilate the prior knowledge and observation data in a Bayesian framework, and to propagate them to posterior distributions of velocities and other Quantities of Interest (QoIs). We use two representative cases, the flow over periodic hills and the flow in a square duct, to evaluate the performance of the proposed framework. Both cases are challenging for standard RANS turbulence models. Simulation results suggest that, even with very sparse observations, the obtained posterior mean velocities and other QoIs have significantly better agreement with the benchmark data compared to the baseline results. At most locations the posterior distribution adequately captures the true model error within the developed model form uncertainty bounds. The framework is a major improvement over existing black-box, physics-neutral methods for model-form uncertainty quantification, where prior knowledge and details of the models are not exploited. This approach has potential implications in many fields in which the governing equations are well understood but the model uncertainty comes from unresolved physical processes. Proposed a physics-informed framework to quantify uncertainty in RANS simulations.Framework incorporates physical prior knowledge and observation data.Based on a rigorous Bayesian framework yet fully utilizes physical model.Applicable for many complex physical systems beyond turbulent flows.	navier–stokes equations;simulation	Heng Xiao;J.-L. Wu;Jian-Xun Wang;Rui Sun;C. J. Roy	2016	J. Comput. Physics	10.1016/j.jcp.2016.07.038	econometrics;simulation;statistics	Theory	37.80120457427318	-24.25237092356551	188896
2cfd675abda333f2c265d5c5301ec9e671ff9cb5	learning task relatedness via dirichlet process priors for linear regression models		In this paper we present a hierarchical model of linear regression functions in the context of multi–task learning. The parameters of the linear model are coupled by a Dirichlet Process (DP) prior, which implies a clustering of related functions for different tasks. To make approximate Bayesian inference under this model we apply the Bayesian Hierarchical Clustering (BHC) algorithm. The experiments are conducted on two real world problems: (i) school exam score prediction and (ii) prediction of ground–motion parameters. In comparison to baseline methods with no shared prior the results show an improved prediction performance when using the hierarchical model.	approximation algorithm;baseline (configuration management);bayesian network;cluster analysis;experiment;gaussian process;hierarchical clustering;hierarchical database model;linear function;linear model	Marcel Hermkes;Nicolas Kühn;Carsten Riggelsen	2012			computer science;machine learning;data mining;statistics	ML	28.59886153799565	-33.592383756649795	188938
0b7bd5e3ce483adbb481fbf0ae13a5074f9af296	simultaneously structured models with application to sparse and low-rank matrices	convex recovery problem;optimisation;order wise optimal number;compressed sensing;complexity theory;tensile stress;nuclear measurements;simultaneously structured model;matrix algebra;low rank matrices;low rank tensor completion;regularization;symmetric matrices;sparse phase retrieval;optimisation compressed sensing matrix algebra;sample complexity compressed sensing convex relaxation regularization;vectors;matrix decomposition;nonconvex recovery problem;sparse matrices vectors complexity theory tensile stress symmetric matrices nuclear measurements matrix decomposition;multiobjective optimization;convex relaxation;low rank tensor completion simultaneously structured model low rank matrices sparse matrices recovering structured models multiobjective optimization order wise optimal number sample complexity nonconvex recovery problem convex recovery problem sparse phase retrieval;sample complexity;recovering structured models;sparse matrices	Recovering structured models (e.g., sparse or group-sparse vectors, low-rank matrices) given a few linear observations have been well-studied recently. In various applications in signal processing and machine learning, the model of interest is structured in several ways, for example, a matrix that is simultaneously sparse and low rank. Often norms that promote the individual structures are known, and allow for recovery using an order-wise optimal number of measurements (e.g., 11 norm for sparsity, nuclear norm for matrix rank). Hence, it is reasonable to minimize a combination of such norms. We show that, surprisingly, using multiobjective optimization with these norms can do no better, orderwise, than exploiting only one of the structures, thus revealing a fundamental limitation in sample complexity. This result suggests that to fully exploit the multiple structures, we need an entirely new convex relaxation. Further, specializing our results to the case of sparse and low-rank matrices, we show that a nonconvex formulation recovers the model from very few measurements (on the order of the degrees of freedom), whereas the convex problem combining the 11 and nuclear norms requires many more measurements, illustrating a gap between the performance of the convex and nonconvex recovery problems. Our framework applies to arbitrary structure-inducing norms as well as to a wide range of measurement ensembles. This allows us to give sample complexity bounds for problems such as sparse phase retrieval and low-rank tensor completion.	convex optimization;linear programming relaxation;low-rank approximation;machine learning;mathematical optimization;multi-objective optimization;phase retrieval;sample complexity;signal processing;sparse matrix	Samet Oymak;Amin Jalali;Maryam Fazel;Yonina C. Eldar;Babak Hassibi	2015	IEEE Transactions on Information Theory	10.1109/TIT.2015.2401574	regularization;mathematical optimization;combinatorics;discrete mathematics;sparse matrix;multi-objective optimization;sparse approximation;mathematics;stress;matrix decomposition;compressed sensing;symmetric matrix	ML	27.256650005917983	-36.330058879678965	189174
ea610bf67fbca225351e7fe4f86e1ef70c03c098	generalized em algorithms for minimum divergence estimation		Minimum divergence estimators are derived through the dual form of the divergence in parametric models. These estimators generalize the classical maximum likelihood ones. Models with unobserved data, as mixture models, can be estimated with EM algorithms, which are proved to converge to stationary points of the likelihood function under general assumptions. This paper presents an extension of the EM algorithm based on minimization of the dual approximation of the divergence between the empirical measure and the model using a proximaltype algorithm. The algorithm converges to the stationary points of the empirical criterion under general conditions pertaining to the divergence and the model. Robustness properties of this algorithm are also presented. We provide another proof of convergence of the EM algorithm in a two-component gaussian mixture. Simulations on Gaussian andWeibull mixtures are performed to compare the results with the MLE.	algorithm	Diaa Al Mohamad;Michel Broniatowski	2015		10.1007/978-3-319-25040-3_45	econometrics;mathematical optimization;mathematics;statistics	ML	30.708618732310846	-26.094649488031333	189199
213779a105959a93e675c93b8abc735f0d0c60f7	numerical analysis of em estimation of mixture model parameters	em estimation;mixture model parameter;gaussian mixture model;numerical analysis;graphical data;common problem;optimal input parameters selection;present algorithm performance observation;iterative algorithm;mixture model;em algorithm;common method	Optimisation of distribution parameters is a very common problem. There are many sorts of distributions which can be used to model environment processes, biological functions or graphical data. However, it is common that parameters of those distribution may be, partially or completely unknown. Mixture models composed of a few distributions are easier to solve. In such a case simple estimation methods may be used to obtain results. Usually models are composed of several distributions. Those distributions may be of the same or different type. Such models are called mixture models. Finding their parameters may be complicated. Usually in such cases iterative methods need to be used. The paper gives a brief survey of algorithms designed for solving mixtures of distributions and problems connected with their usage. One of the most common method used to obtain mixture model parameters is ExpectationMaximization (EM) algorithm. EM is the iterative algorithm performing maximum likelihood estimation. The authors present the results of adjusting the Gaussian mixture models to the data. It is done with the usage of EM algorithm. The article gives advantages and disadvantages of EM algorithm. Improvements of EM applied in the case of large data are also presented. They help increase efficiency and decrease operation time of the algorithm. Another considered issue is the problem of optimal input parameters selection and its influence on the adjustment results. The authors also present algorithm performance observations. ∗E-mail address: gosiap@pluton.pol.lublin.pl Pobrane z czasopisma Annales AIInformatica http://ai.annales.umcs.pl Data: 22/11/2018 04:39:34	expectation–maximization algorithm;graphical user interface;iterative method;mathematical optimization;mixture model;numerical analysis;operation time	Malgorzata Plechawska-Wójcik;Lukasz Wójcik;Andrzej Polanski	2009	Annales UMCS, Informatica	10.2478/v10065-009-0009-9	mixture model;computer science;iterative method;maximum likelihood;mathematical optimization;numerical analysis;expectation–maximization algorithm	ML	32.1346267769927	-24.460846959104533	189252
7e3b8a510f0509779df5ddcb32253c9f492b5619	fast algorithms for the all nearest neighbors problem	nearest neighbor searches;linearity;cost function;iso;sorting;probability density function;barium;computational geometry;size measurement;length measurement;data analysis;artificial neural networks;rail to rail outputs;data structures;fast algorithm;nearest neighbor;pattern recognition;joining processes;nearest neighbor searches partitioning algorithms computer science computational geometry data analysis pattern recognition probability density function cost function data structures;refining;computer science;construction;algorithm design and analysis;buildings;floors;partitioning algorithms;radio access networks	The basic approach to algorithm ANNs, as well as to all of the algorithms presented here, is to use a cell technique ([BWY], [BFP]). The region containing the input points is partitioned into equal-sized cubic cells. 'rhe distance from a point to its nearest neighbors is bounded using the distance of the cell cont.aining the point to the nearest cell also occupied by a point. This bound is improved by refining the. cellular partition into smaller and smaBer cubic cells. Computational effort at each step is saved by storing, for each cell occupied by at least one point, a set of nearest neigh.bor occupied cells: those cells that contain points that might be nearest neighbors to the points in the cell.	algorithm;computation;cubic function	Kenneth L. Clarkson	1983	24th Annual Symposium on Foundations of Computer Science (sfcs 1983)	10.1109/SFCS.1983.16	nearest neighbour algorithm;algorithm design;probability density function;iso image;refining;construction;data structure;computational geometry;length measurement;computer science;sorting;theoretical computer science;machine learning;data mining;linearity;barium;data analysis;k-nearest neighbors algorithm;algorithm	Theory	36.13410922898103	-36.692383890309884	189385
120ea53e70cdfae8f1dac6f81ed428ef7393a1d9	incorporating qualitative information into quantitative estimation via sequentially constrained hamiltonian monte carlo sampling		In human-robot collaborative tasks, incorporating qualitative information provided by humans can greatly enhance the robustness and efficacy of robot state estimation. We introduce an algorithmic framework to model qualitative information as quantitative constraints on and between states. Our approach, named Sequentially Constrained Hamiltonian Monte Carlo, integrates Hamiltonian dynamics into Sequentially Constrained Monte Carlo sampling. We are able to generate samples that satisfy arbitrarily complex, non-smooth and discontinuous constraints, which in turn allows us to support a wide range of qualitative information. We evaluate our approach for constrained sampling qualitatively and quantitatively with several classes of constraints. SCHMC significantly outperforms the Metropolis-Hastings algorithm (a standard Markov Chain Monte Carlo (MCMC) method) and the Hamiltonian Monte Carlo (HMC) method, in terms of both the accuracy of the sampling (for satisfying constraints) and the quality of approximation. Compared to Sequentially Constrained Monte Carlo (SCMC), which supports similar kinds of constraints, our SCHMC approach has faster convergence rates and lower parameter sensitivity.	approximation;bridging (networking);burn-in;constraint (mathematics);experiment;hybrid memory cube;hybrid monte carlo;markov chain monte carlo;metropolis;metropolis–hastings algorithm;mind;monte carlo method;primary direction;rate of convergence;sampling (signal processing);series acceleration;simulation	Daqing Yi;Shushman Choudhury;Siddhartha S. Srinivasa	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206336	computer science;robustness (computer science);mathematical optimization;control engineering;monte carlo method;sampling (statistics);markov chain monte carlo;hybrid monte carlo;hamiltonian mechanics;convergence (routing)	Robotics	27.16009280290728	-28.635272236276162	189389
c9274dd0de90bea579725025b73329f59e5b3a99	orthogonal tensor decompositions via two-mode higher-order svd (hosvd)		Tensor decompositions have rich applications in statistics and machine learning, and developing efficient, accurate algorithms for the problem has received much attention recently. Here, we present a new method built on Kruskal’s uniqueness theorem to decompose symmetric, nearly orthogonally decomposable tensors. Unlike the classical higher-order singular value decomposition which unfolds a tensor along a single mode, we consider unfoldings along two modes and use rank-1 constraints to characterize the underlying components. This tensor decomposition method provably handles a greater level of noise compared to previous methods and achieves a high estimation accuracy. Numerical results demonstrate that our algorithm is robust to various noise distributions and that it performs especially favorably as the order increases.	kruskal's algorithm;machine learning;numerical method;singular value decomposition	Miaoyan Wang;Yun S. Song	2017			mathematical optimization;combinatorics;discrete mathematics;mathematics;statistics	ML	26.93854858870203	-35.93384678505013	189572
648990ef20335c4bce1d659c9d8917f8cfef2d3e	computer controlled dynamic wedge collimator for cancer treatment machine	biomedical monitoring;software;beam shaping;linac;cancer;table lookup cancer collimators graphical user interfaces medical computing radiation therapy;medical computing;cancer treatment;graphical user interfaces;monitoring;dynamic wedge;gui computer controlled dynamic wedge collimator cancer treatment machine dynamic wedge system beam shaping device secondary collimator radiation beam computer selected lookup table conformal treatment dynamic wedge profile xilinx labview software;graphic user interface;vhdl lab view tumor treatment linac dynamic wedge;lookup table;tumor treatment;radiation therapy;vhdl lab view;table lookup;collimators;software graphical user interfaces monitoring biomedical monitoring	In this paper design of Dynamic wedge system we discuss the controlling of beam shaping device namely secondary collimator, which creates varying field intensity while the radiation beam is on. Due to the motion of collimator, different segments of the treatment field will be exposed to the primary beam for different intervals of time. The amount of dose that will be delivered to the patient as the collimator moves across the treatment field is controlled by pre-calculated computer selected lookup table. It leads toward the conformal treatment and there by sparing good tissue, this increases quality of treatment. This dynamic wedge profile will be achieved by the logical and mathematical algorithm which will be implemented using Xilinx and Lab VIEW software. National semiconductors LabVIEW software is use to design GUI and obtain prerequisites for Xilinx to do mathematical calculation.	algorithm;graphical user interface;labview;lookup table;noise shaping;redundancy (engineering);requirement;semiconductor;simulation	Mayur U. Yelpale;S. N. Pethe;Mandar Vidwans;Paresh Jadhav;M. M. Jadhav	2012	Proceedings of 2012 IEEE-EMBS International Conference on Biomedical and Health Informatics	10.1109/BHI.2012.6211755	simulation;computer hardware;computer science;computer graphics (images)	Visualization	38.72188161971816	-37.21552340399434	189618
3ac11cbe0321c0a54ad8e8d9e5a2e6cdec140e4f	a non-parametric approach to approximate dynamic programming	dynamic programming;approximate algorithm;measurement;stochastic systems approximation theory dynamic programming learning artificial intelligence optimal control regression analysis;approximation algorithms;reinforcement learning;training;noise measurement;scalability problem nonparametric approach approximate dynamic programming machine learning method optimal control policy learning dynamic system stochastic system reinforcement learning algorithm state space system approximation scheme multidimensional continuous state space locally weighted projection regression method;stochastic system;optimal control;approximation theory;trajectory;machine learning;function approximation;locally weighted projection regression;state space;approximation scheme;training trajectory approximation algorithms function approximation noise measurement;approximate dynamic programming;regression analysis;stochastic systems;learning artificial intelligence;noise	Approximate Dynamic Programming (ADP) is a machine learning method aiming at learning an optimal control policy for a dynamic and stochastic system from a logged set of observed interactions between the system and one or several non-optimal controlers. It defines a class of particular Reinforcement Learning (RL) algorithms which is a general paradigm for learning such a control policy from interactions. ADP addresses the problem of systems exhibiting a state space which is too large to be enumerated in the memory of a computer. Because of this, approximation schemes are used to generalize estimates over continuous state spaces. Nevertheless, RL still suffers from a lack of scalability to multidimensional continuous state spaces. In this paper, we propose the use of the Locally Weighted Projection Regression (LWPR) method to handle this scalability problem. We prove the efficacy of our approach on two standard benchmarks modified to exhibit larger state spaces.	algorithm;approximation;benchmark (computing);dynamic programming;interaction;machine learning;optimal control;programming paradigm;reinforcement learning;scalability;state space;stochastic process;usb on-the-go	Hadrien Glaude;Fadi Akrimi;Matthieu Geist;Olivier Pietquin	2011	2011 10th International Conference on Machine Learning and Applications and Workshops	10.1109/ICMLA.2011.19	mathematical optimization;optimal control;function approximation;computer science;state space;noise measurement;noise;trajectory;theoretical computer science;machine learning;dynamic programming;mathematics;reinforcement learning;regression analysis;measurement;approximation theory	ML	25.00036770767441	-30.015142031602288	190158
27da8cfe20db9de76c38fb12dd70b6c1213d82df	a fast algorithm for robust mixtures in the presence of measurement errors	measurement error;data mining;outlier detection;machine learning;tree structure;multivariate data;high redshift quasars;markov chain	We develop a mixture-based approach to robust density modeling and outlier detection for experimental multivariate data that includes measurement error information. Our model is designed to infer atypical measurements that are not due to errors, aiming to retrieve potentially interesting peculiar objects. Since exact inference is not possible in this model, we develop a tree-structured variational EM solution. This compares favorably against a fully factorial approximation scheme, approaching the accuracy of a Markov-Chain-EM, while maintaining computational simplicity. We demonstrate the benefits of including measurement errors in the model, in terms of improved outlier detection rates in varying measurement uncertainty conditions. We then use this approach for detecting peculiar quasars from an astrophysical survey, given photometric measurements with errors.	algorithm;anomaly detection;approximation;calculus of variations;estimation theory;expectation–maximization algorithm;markov chain;mixture model;redshift;sensor;spatial decision support system	Jianyong Sun;Ata Kabán;Somak Raychaudhury	2007	IEEE Transactions on Neural Networks	10.1145/1273496.1273603	multivariate statistics;markov chain;anomaly detection;computer science;machine learning;mathematics;tree structure;statistics;observational error	ML	29.39066903679798	-28.25983933047526	190477
029d90b0a45393b0761204a1fbc63f5679c5ced9	a factorization criterion for acyclic directed mixed graphs	mixed graph;factorization criterion;global markov property;latent variable;dag model;conditional independence structure;semi-markov model;natural extension;observed margin	Acyclic directed mixed graphs, also known as semi-Markov models represent the conditional independence structure induced on an observed margin by a DAG model with latent variables. In this paper we present a factorization criterion for these models that is equivalent to the global Markov property given by (the natural extension of) dseparation.	directed acyclic graph;latent variable;markov chain;markov model;markov property;mixed graph;semiconductor industry	Thomas S. Richardson	2009			mathematical optimization;combinatorics;discrete mathematics;mathematics	ML	25.636836735737674	-28.426097604228048	190643
4068c837a6a5175157118b095d437f5ae8e6f48c	new method for target identification in a foliage environment using selected bispectra and chaos particle swarm optimisation-based support vector machine	target classifier target identification foliage environment chaos particle swarm optimisation based support vector machine ultra wideband wireless sensor network uwb received signal waveform transceivers wsn bispectra algorithm;target identification;wsn;target classifier;ultra wideband wireless sensor network;wireless sensor networks chaos feature extraction geophysical signal processing particle swarm optimisation radio transceivers signal classification support vector machines vegetation;chaos particle swarm optimisation based support vector machine;transceivers;received signal waveform;bispectra algorithm;uwb;foliage environment	In this study, a novel method for target identification in a foliage environment is presented. This method is based on the ultra wideband (UWB) wireless sensor networks (WSNs) model, and the foliage environment is specially considered. The data used to identify the targets are derived from the received signal waveform, so most existing transceivers can be exploited as detecting sensors, which leads to a potential low-cost way to identify targets during the normal communications within the WSNs under foliage environment. The selected bispectra algorithm is applied to extract the feature vector, and chaos particle swarm optimisation-based support vector machine is used as the target classifier. Experiments with real-world data samples indicate that this method has an excellent classification performance in a foliage environment. Moreover, this method shows potential for online training.	mathematical optimization;particle swarm optimization;support vector machine	Minglei You;Ting Jiang	2014	IET Signal Processing	10.1049/iet-spr.2012.0389	electronic engineering;telecommunications;computer science;engineering;ultra-wideband;transceiver	ML	38.50498755738399	-33.825441525106875	191012
1fa1b112b75b7af2582d8d17d45311e32f15265b	sparse estimation with structured dictionaries	model selection;compressed sensing;source localization;estimation algorithm;penalized regression;bayesian estimator	In the vast majority of recent work on sparse estimation algo rithms, performance has been evaluated using ideal or quasi-ideal dictionaries (e.g., random Gaussian or Fourier) characterized by unit l2 norm, incoherent columns or features. But in reality, these types of dictionaries represent only a subse t of the dictionaries that are actually used in practice (largely restricted to ideali zed compressive sensing applications). In contrast, herein sparse estimation is co nsidered in the context of structured dictionaries possibly exhibiting high coher ence between arbitrary groups of columns and/or rows. Sparse penalized regression models are analyzed with the purpose of finding, to the extent possible, regimes o f dictionary invariant performance. In particular, a Type II Bayesian estimato r with a dictionarydependent sparsity penalty is shown to have a number of desir able invariance properties leading to provable advantages over more conven tional penalties such as thel1 norm, especially in areas where existing theoretical recov ry guarantees no longer hold. This can translate into improved performanc e i applications such as model selection with correlated features, source locali zation, and compressive sensing with constrained measurement directions.	column (database);compressed sensing;dictionary;model selection;provable security;sparse matrix	David P. Wipf	2011			machine learning;pattern recognition;mathematics;compressed sensing;model selection;statistics	ML	27.635470964561527	-26.420975620785278	191190
7d5856aa1373aa1a8d884137fa640b68b8993c67	wavelet-based scalar-on-function finite mixture regression models	biological patents;biomedical journals;functional data analysis;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;em algorithm;lasso;wavelets;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Classical finite mixture regression is useful for modeling the relationship between scalar predictors and scalar responses arising from subpopulations defined by the di ering associations between those predictors and responses. The classical finite mixture regression model is extended to incorporate functional predictors by taking a wavelet-based approach in which both the functional predictors and the component-specific coefficient functions are represented in terms of an appropriate wavelet basis. By using the wavelet representation of the model, the coefficients corresponding to the functional covariates become the predictors. In this setting, there are typically many more predictors than observations. Hence a lasso-type penalization is employed to simultaneously perform feature selection and estimation. Specification of the model is discussed and a fitting algorithm is provided. The wavelet-based approach is evaluated on synthetic data as well as applied to a real data set from a study of the relationship between cognitive ability and di usion tensor imaging measures in subjects with multiple sclerosis.	algorithm;coefficient;cognition;feature selection;lasso;mental association;multiple sclerosis;penalty method;synthetic data;wavelet	Adam J Ciarleglio;R. Todd Ogden	2016	Computational statistics & data analysis	10.1016/j.csda.2014.11.017	wavelet;functional data analysis;text mining;multicollinearity;expectation–maximization algorithm;computer science;data science;lasso;data mining;mathematics;statistics	ML	29.526585368166135	-25.692226589786895	191455
b4d2aa70a84e6f72a7733af4445342ebb6e27e7f	bss algorithm by diffusion mixing non-parametric density estimator	non bayesian framework;diffusion mixing estimator;blind source separation;independent component analysis;fixed width kernel density estimator;bss;dme;ica;fkde	Diffusion mixing estimator (DME)-based non-parametric blind source separation (BSS) algorithm is proposed under the framework of non-Bayesian framework and natural gradient optimisation method. In order to improve the performance of signal separation by BSS, the probability distribution of source signals must be described as accurately as possible. Compared to the non-parametric fixed-width kernel density estimator (FKDE) method, the DME with a new data-driven bandwidth selection method can improve the performance of FKDE, which is inspired via a Langevin diffusion process. Moreover, the direct estimation of the score functions can separate the hybrid mixtures of sources that contain both symmetric and asymmetric distribution source signals and do not need to assume the parametric non-linear functions as them. The effectiveness of the proposed algorithm has been confirmed by simulation experiments.	algorithm	Fasong Wang;Rui Li	2012	IJMIC	10.1504/IJMIC.2012.043936	independent component analysis;econometrics;mathematical optimization;distance measuring equipment;computer science;.bss;machine learning;mathematics;blind signal separation;statistics	ML	32.25596866330315	-26.607638299018312	191616
5ba578d8f82ec99459df675537d393778a7b1992	bounding the test log-likelihood of generative models		Several interesting generative learning algorithms involve a complex probability distribution over many random variables, involving intractable normalization constants or latent variable marginalization. Some of them may not have even an analytic expression for the unnormalized probability function and no tractable approximation. This makes it difficult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model’s probability function from samples generated by the model. We revisit this idea, propose a more efficient estimator, and prove that it provides a lower bound on the true test log-likelihood and an unbiased estimator as the number of generated samples goes to infinity, although one that incorporates the effect of poor mixing. We further propose a biased variant of the estimator that can be used reliably with a finite number of samples for the purpose of model comparison. 1 Motivating Sampling-Based Estimators of Generative Models’ Quality Since researchers have begun considering more and more powerful models of data distributions, they have been facing the difficulty of estimating the quality of these models. ∗yoshua.bengio@umontreal.ca †li.yao@umontreal.ca ‡kyunghyun.cho@aalto.fi 1 ar X iv :1 31 1. 61 84 v4 [ cs .L G ] 9 M ay 2 01 4 In some cases, the probability distribution of a model involves many latent variables, and it is intractable to marginalize over those latent variables or to compute the normalization constant (partition function). There exist approximation algorithms that were proposed to overcome these intractabilities. One such example is Annealed Importance Sampling (AIS, Neal, 2001; Salakhutdinov and Murray, 2008; Murray and Salakhutdinov, 2009; Salakhutdinov and Hinton, 2009). AIS, however, tends to provide optimistic estimates most of the time, just like any estimator based on importance sampling. This optimistic estimation happens when the samples from the AIS proposal distribution miss many important modes of the distribution. This is problematic because when we want to compare learning algorithms, we often prefer a conservative estimate of performance than an optimistic estimate that tends to over-estimate the value of the model. This issue can be particularly troubling when the amount of over-estimation depends on the model, which makes model comparisons based on an optimistic estimator dangerous. In other cases, one has a generative model but there is no explicit formulat corresponding to the probability function estimated by the model. That includes Herding Welling (2009), the non-zero temperature version of Herding (Breuleux et al., 2011), and the recently proposed generative procedures for contractive auto-encoders (CAE, Rifai et al., 2012a), denoising auto-encoders (DAE, Bengio et al., 2013b), and generative stochastic networks (GSN, Bengio et al., 2014). For this reason, in this paper, we discuss a way to assess the quality of a generative model simply by considering the samples it generates. In the next section, we discuss the general idea of estimating a probability function (or a density function) from the samples generated by a generative model. We first review a previously proposed estimator that aimed to solve this goal. We show that the estimate by this estimator, in expectation over the generated samples from a generative model, is a lower bound on the true test log-likelihood and unbiased asymptotically. We then propose a more efficient variant of this estimator that has lower variance.	approximation algorithm;cobham's thesis;differential algebraic equation;early stopping;encoder;generative model;hippi;importance sampling;latent variable;machine learning;model selection;noise reduction;partition function (mathematics);sampling (signal processing)	Yoshua Bengio;Li Yao	2013	CoRR		minimax estimator;econometrics;minimum-variance unbiased estimator;stein's unbiased risk estimate;machine learning;mathematics;generative model;consistent estimator;invariant estimator;statistics	ML	24.658758109821218	-29.453220931911677	191637
e028d95530a974ccc69e9abc881e855915d0b63e	qr decomposition based orthogonality estimation for partially linear models with longitudinal data	62g05;partially linear model;62g20;62g30;qr decomposition;longitudinal data;orthogonality estimation	This paper studies the estimation for a class of partially linear models with longitudinal data. By combining quadratic inference functions with QR decomposition technology, we propose a new estimation method for the parametric and nonparametric components. The resulting estimators for parametric and nonparametric components do not affect each other, and then it is easy for application in practice. Under some mild conditions, we establish some asymptotic properties of the resulting estimators. Some simulation studies are undertaken to assess the finite sample performance of the proposed estimation procedure.	linear model;qr decomposition	Jiting Huang;Peixin Zhao	2017	J. Computational Applied Mathematics	10.1016/j.cam.2017.02.024	econometrics;mathematical optimization;mathematics;qr decomposition;algorithm;statistics;algebra	ML	29.930980036252144	-24.512087084243213	191913
487ef6beb5370f83fb2cfab474fefdb0c3f09baa	statistical weight kinetics modeling and estimation for silica nanowire growth catalyzed by pd thin film	nanostructure growth;silicon;belief networks;diffusion induced model;model selection;bayesian hierarchical model;thin film;bayesian approach;nonlinear growth models;kinetic model;computer model;silica;computational modeling silicon silicon compounds kinetic theory substrates maximum likelihood estimation;pd statistical weight kinetics modeling statistical weight kinetics estimation silica nanowire growth catalysis thin film adsorption induced model diffusion induced model unified vapor liquid solid growth model substrate weight maximum likelihood estimation bayesian approaches nonlinear growth models bayesian hierarchical model sio 2;mass production;statistical weight kinetics estimation;maximum likelihood estimation;process modeling model selection nanomanufacturing nanostructure growth;statistical weight kinetics modeling;sio 2;computational modeling;vapor liquid solid;kinetic theory;maximum likelihood estimate;silica nanowire growth;silicon compounds;process modeling;catalysis;growth kinetics;substrate weight;adsorption induced model;bayesian approaches;nanomanufacturing;silicon compounds adsorption belief networks catalysis diffusion magnetic thin films nanowires palladium;nanowires;pd;magnetic thin films;prediction model;process model;substrates;weight change;unified vapor liquid solid growth model;palladium;kinetics;growth process;diffusion;control method;growth model;adsorption	This work intends to understand and model the kinetic aspect or the change of substrate weight over time in the selective growth of silica nanowires (NWs) catalyzed through Pd thin film. Various adsorption-induced, diffusion-induced, or unified vapor-liquid-solid (VLS) growth models have been developed to describe the NW length varying with time. Since NW length has been difficult to be measured, substrate weight change is therefore used as an alternative in this study to investigate growth kinetics of NWs. We investigate six different weight kinetics models in predicting weight changes during growth. Model estimation and comparison are conducted using both maximum-likelihood estimation (MLE) and Bayesian approaches. Owing to the embedded kinetics information in the nonlinear growth models, the Bayesian hierarchical model is shown to be more desirable when process data is limited.	embedded system;experiment;fits;hierarchical database model;kinesiology;kinetics internet protocol;linear model;netware;nonlinear system;time complexity;transition system;von luschan's chromatic scale	Qiang Huang;Li Wang;Tirthankar Dasgupta;Li Zhu;P. K. Sekhar;S. Bhansali;Yu An	2011	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2010.2070493	computer simulation;econometrics;process modeling;maximum likelihood;statistics	Vision	33.17786633340845	-27.607347219508288	192084
52eee7c41edf73dd9d2f7008b98ab7dd0dd3beb2	an engine fault diagnosis system using intake manifold pressure signal and wigner-ville distribution technique	operant conditioning;energy spectrum;general regression neural network;feature extraction;radial basis function neural network;fault diagnosis system;intake manifold pressure;wigner ville distribution;frequency domain;artificial neural network;neural network;diagnostic method	This paper proposed an engine fault diagnosis system based on intake manifold pressure signal and artificial neural network with the Wigner–Ville distribution technique. Traditionally, the engine diagnostic method depends on the experience of the technician, but some faults might be inaccurately judged by the technician’s experience when the engine is operating. In the present study, an engine platform diagnosis system using intake manifold pressure was developed. The algorithm of the proposed system consisted of Wigner–Ville distribution (WVD) for feature extraction and the neural network technique for fault classification. In previous work, the Wigner–Ville distribution was often used to analyze the nonstationary signal, because it provides a simple and clear energy spectrum diagram both in the time and frequency domains. This instantaneous energy diagram presented the magnitude of each engine fault under various operating conditions. The Wigner–Ville distribution extracts these features as database input to a neural network and the neural network is used to develop the training and testing modules. To prove the efficiency of the neural network, both the radial basis function neural network and generalized regression neural network are used and compared. The experimental results demonstrated the proposed system is effective and the performance is satisfactory. 2010 Elsevier Ltd. All rights reserved.	algorithm;artificial neural network;diagram;expert system;feature extraction;radial (radio);radial basis function;wigner distribution function;wigner quasiprobability distribution	Jian-Da Wu;Cheng-Kai Huang	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.06.099	probabilistic neural network;feature extraction;computer science;artificial intelligence;machine learning;operant conditioning;time delay neural network;frequency domain;artificial neural network	AI	36.99673749109929	-31.219568836630263	192242
966261b77a6517885dd608c31eb8ec7564ae2748	two manifold problems with applications to nonlinear system identification		Recently, there has been much interest in spectral approaches to learning manifolds— so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space, and discuss when two-manifold problems are useful. Finally, we demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.	algorithm;cross-covariance;data point;dynamical system;hilbert space;ibm notes;machine learning;nonlinear dimensionality reduction;nonlinear system identification;predictive modelling;state space;time series	Byron Boots;Geoffrey J. Gordon	2012	CoRR			ML	27.222724625338845	-37.266895765321024	192465
7b3a98392104826496cbd7388496b4f27abfda64	on von-mises fisher mixture model in text-independent speaker identification		This paper addresses text-independent speaker identification (SI) based on line spectral frequencies (LSFs). The LSFs are transformed to differential LSFs (MLSF) in order to exploit their boundary and ordering properties. We show that the square root of MLSF has interesting directional characteristics implying that their distribution can be modeled by a mixture of von-Mises Fisher (vMF) distributions. We analytically estimate the mixture model parameters in a fully Bayesian treatment by using variational inference. In the Bayesian inference, we can potentially determine the model complexity and avoid overfitting problem associated with conventional approaches based on the expectation maximization. The experimental results confirm the effectiveness of the proposed SI system.	bayesian network;entropy maximization;expectation–maximization algorithm;line spectral pairs;mixture model;overfitting;speaker recognition;variational principle	Jalil Taghia;Zhanyu Ma;Arne Leijon	2013			computer science;overfitting;mixture model;pattern recognition;square root;artificial intelligence;speech recognition;von mises yield criterion;statistics;expectation–maximization algorithm;bayesian probability;inference;bayesian inference	ML	30.39424962421187	-27.61131001862274	192723
3463db73283fd5aa27f8073080784f2bf3bcd23f	wavelet-based classification of transient signals for gravitational wave detectors		The detection of gravitational waves opened a new window on the cosmos. The Advanced LIGO and Advanced Virgo interferometers will probe a larger volume of Universe and discover new gravitational wave emitters. Characterizing these detectors is of primary importance in order to recognize the main sources of noise and optimize the sensitivity of the searches. Glitches are transient noise events that can impact the data quality of the interferometers and their classification is an important task for detector characterization. In this paper we present a classification method for short transient signals based on a Wavelet decomposition and de-noising and a classification of the extracted features based on XGBoost algorithm. Although the results show the accuracy is lower than that obtained with the use of deep learning, this method which extracts features while detecting signals in real time, can be configured as a fast classification system.		Elena Cuoco;Massimiliano Razzano;Andrei Utina	2018	2018 26th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2018.8553393		ML	36.487568085189785	-32.17349655603902	192734
34bb182e4f27b9081f041b5d483a98f052a7fc06	tensor regression based on linked multiway parameter analysis	vectors data acquisition parameter estimation regression analysis tensors;tensile stress;nickel;training;endnotes;tensor regression;tensile stress vectors training data models nickel educational institutions;vectors;sparse coding tensor regression linked multiway parameter analysis;linked multiway parameter analysis;pubications;sparse coding;memory cost linked multiway parameter analysis multidimensional arrays tensor regression model tensor decomposition multidimensional data tucker decomposition sparsity preserving regulariser sparsity modeling;data models	Classical regression methods take vectors as covariates and estimate the corresponding vectors of regression parameters. When addressing regression problems on covariates of more complex form such as multi-dimensional arrays (i.e. Tensors), traditional computational models can be severely compromised by ultrahigh dimensionality as well as complex structure. By exploiting the special structure of tensor covariates, the tensor regression model provides a promising solution to reduce the model's dimensionality to a manageable level, thus leading to efficient estimation. Most of the existing tensor-based methods independently estimate each individual regression problem based on tensor decomposition which allows the simultaneous projections of an input tensor to more than one direction along each mode. As a matter of fact, multi-dimensional data are collected under the same or very similar conditions, so that data share some common latent components but can also have their own independent parameters for each regression task. Therefore, it is beneficial to analyse regression parameters among all the regressions in a linked way. In this paper, we propose a tensor regression model based on Tucker Decomposition, which identifies not only the common components of parameters across all the regression tasks, but also independent factors contributing to each particular regression task simultaneously. Under this paradigm, the number of independent parameters along each mode is constrained by a sparsity-preserving regulariser. Linked multiway parameter analysis and sparsity modeling further reduce the total number of parameters, with lower memory cost than their tensor-based counterparts. The effectiveness of the new method is demonstrated on real data sets.	computation;computational model;programming paradigm;sparse matrix;tucker decomposition	Yifan Fu;Junbin Gao;Xia Hong;David Tien	2014	2014 IEEE International Conference on Data Mining	10.1109/ICDM.2014.37	nickel;data modeling;econometrics;computer science;machine learning;polynomial regression;mathematics;stress;tensor product network;neural coding;multilinear subspace learning;regression analysis;statistics	ML	27.987066951709973	-35.52363780563644	192841
36bca9d41de386fce5dce06999a45a802a7c4f41	generating cp-nets uniformly at random		Conditional preference networks (CP-nets) are a commonly studied compact formalism for modeling preferences. To study the properties of CP-nets or the performance of CP-net algorithms on average, one needs to generate CP-nets in an equiprobable manner. We discuss common problems with naı̈ve generation, including sampling bias, which invalidates the base assumptions of many statistical tests and can undermine the results of an experimental study. We provide a novel algorithm for provably generating acyclic CP-nets uniformly at random. Our method is computationally efficient and allows for multi-valued domains and arbitrary bounds on the indegree in the dependency graph.	algorithm;algorithmic efficiency;c++;cp/m;cpt (file format);directed acyclic graph;directed graph;experiment;formal system;sampling (signal processing);the australian	Thomas E. Allen;Judy Goldsmith;Hayden Elizabeth Justice;Nicholas Mattei;Kayla Raines	2016			mathematical optimization;statistics	AI	26.355715044514596	-28.072641519262255	193435
25f0b0706ac0e747a66e29af39180e289d39c6be	a digital-signal-processing technique for ultrasonic signal modeling and classification	intelligent sensor;digital signal processing technique;hilbert transforms;digital signal processing;damping;polynomial matrices;least squares approximations;interpolation;nonlinear least squares;sensor phenomena and characterization;sampling rate;tof estimation;robust feature space;class clustering;signal sampling;convolution;signal sampling sonar signal processing sonar detection signal classification pattern classification robot vision least squares approximations acoustic convolution hilbert transforms covariance matrices polynomial matrices laplace transforms analogue digital conversion demodulation computational complexity;sensor phenomena and characterization shape signal synthesis robustness narrowband bandwidth polynomials piezoelectric transducers damping signal to noise ratio;adc digital signal processing technique ultrasonic signal modeling signal classification robust feature space automatic classification narrow band in air ultrasonic sensors sensor bandwidth restrictions signal descriptor echo envelopes orthonormal laguerre polynomials high frequency piezoelectric transducer electrical damping return signal snr sampling rate intelligent sensors sonar sensing robotic eyes class clustering computational complexity demodulation envelope extraction tof estimation convolution laplace transforms interpolation hilbert transform nonlinear least squares covariance;covariance;electrical damping;indexing terms;feature space;polynomials;sonar detection;hilbert transform;orthonormal laguerre polynomials;robot vision;demodulation;acoustic convolution;shape;sensor bandwidth restrictions;laplace transforms;computational complexity;covariance matrices;visual inspection;envelope extraction;high frequency piezoelectric transducer;analogue digital conversion;signal classification;adc;pattern classification;echo envelopes;bandwidth;piezoelectric transducers;narrow band in air ultrasonic sensors;robustness;return signal snr;signal descriptor;laguerre polynomial	In this paper, we face the problem of constructing a robust feature space for automatic classification of signals from narrow-band in-air ultrasonic sensors. In consideration of the existing sensor bandwidth restrictions, the importance of selecting a suitable signal descriptor is highlighted. We assume that the characteristics of the ultrasonic sources which produce the signals are impressed in the shape of their echo envelopes. A technique based on orthonormal Laguerre polynomials is applied to the echo envelopes for constructing the feature space. Different methods for computing the Laguerre coefficients are discussed, and the properties of the resulting feature space are investigated. For the experimental verification of the method, a set of acoustic sources is synthesized by submitting a high-frequency piezoelectric transducer to varying levels of electrical damping. How some factors, i.e., the signal-to-noise ratio (SNR) of the return signals, and the sampling rate to digitize them, affect the achievable recognition rate is discussed. High recognition rates are obtained in our experiments, in spite of the fact that, by visual inspection, the shapes of the signals from the synthesized sources are very similar to one another.	acoustic cryptanalysis;coefficient;digital signal processing;experiment;feature vector;laguerre polynomials;piezoelectricity;polynomial;sampling (signal processing);sensor;signal-to-noise ratio;statistical classification;transducer;visual inspection	Angelo M. Sabatini	2001	IEEE Trans. Instrumentation and Measurement	10.1109/19.903873	control engineering;electronic engineering;speech recognition;mathematics;statistics;intelligent sensor	Vision	37.597253924452644	-34.550931162907276	193459
66cbca847cdc9fe4e1030d6ad94987be526ef451	integrating local information for inference and optimization in machine learning	large scale bayesian sampling;thesis or dissertation;machine learning;large scale optimization	In practice, machine learners often care about two key issues: one is how to obtain a more accurate answer with limited data, and the other is how to handle large-scale data (often referred to as “Big Data” in industry) for efficient inference and optimization. One solution to the first issue might be aggregating learned predictions from diverse local models. For the second issue, integrating the information from subsets of the large-scale data is a proven way of achieving computation reduction. In this thesis, we have developed some novel frameworks and schemes to handle several scenarios in each of the two salient issues. For aggregating diverse models – in particular, aggregating probabilistic predictions from different models – we introduce a spectrum of compositional methods, Rényi divergence aggregators, which are maximum entropy distributions subject to biases from individual models, with the Rényi divergence parameter dependent on the bias. Experiments are implemented on various simulated and real-world datasets to verify the findings. We also show the theoretical connections between Rényi divergence aggregators and machine learning markets with isoelastic utilities. The second issue involves inference and optimization with large-scale data. We consider two important scenarios: one is optimizing large-scale Convex-Concave Saddle Point problem with a Separable structure, referred as Sep-CCSP; and the other is largescale Bayesian posterior sampling. Two different settings of Sep-CCSP problem are considered, Sep-CCSP with strongly convex functions and non-strongly convex functions. We develop efficient stochastic coordinate descent methods for both of the two cases, which allow fast parallel processing for large-scale data. Both theoretically and empirically, it is demonstrated that the developed methods perform comparably, or more often, better than state-of-the-art methods. To handle the scalability issue in Bayesian posterior sampling, the stochastic approximation technique is employed, i.e., only touching a small mini batch of data items to approximate the full likelihood or its gradient. In order to deal with subsampling error introduced by stochastic approximation, we propose a covariance-controlled adaptive Langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution. This method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications.	approximation algorithm;big data;chroma subsampling;computation;concave function;convex function;coordinate descent;experiment;gradient;kullback–leibler divergence;machine learning;mathematical optimization;parallel computing;rényi entropy;sampling (signal processing);scalability;speedup;stochastic approximation	Zhanxing Zhu	2016			algorithmic learning theory;wake-sleep algorithm;computer science;data science;online machine learning;machine learning;pattern recognition;relevance vector machine;computational learning theory;active learning	ML	25.287079768796893	-32.587741083827055	193519
bcc0bf4525526cf6b2855e7bb1d5edbd92f368a2	approximation methods for efficient learning of bayesian networks	statistical approach;mcmc algorithm;bayesian network;structural model;mcmc methods;approximate algorithm;bayesian statistics;learning;approximation method;data mining;incomplete data;markov chain monte carlo;monte carlo method;fast algorithm;probability distribution;bayesian analysis;monte carlo methods;data generation process;approaches to learning;bayesian networks	This publication offers and investigates efficient Monte Carlo simulation methods in order to realize a Bayesian approach to approximate learning of Bayesian networks from both complete and incomplete data. For large amounts of incomplete data when Monte Carlo methods are inefficient, approximations are implemented, such that learning remains feasible, albeit non-Bayesian. Topics discussed are; basic concepts about probabilities, graph theory and conditional independence; Bayesian network learning from data; Monte Carlo simulation techniques; and the concept of incomplete data. In order to provide a coherent treatment of matters, thereby helping the reader to gain a thorough understanding of the whole concept of learning Bayesian networks from (in)complete data, this publication combines in a clarifying way all the issues presented in the papers with previously unpublished work.IOS Press is an international science, technical and medical publisher of high-quality books for academics, scientists, and professionals in all fields. Some of the areas we publish in: -Biomedicine -Oncology -Artificial intelligence -Databases and information systems -Maritime engineering -Nanotechnology -Geoengineering -All aspects of physics -E-governance -E-commerce -The knowledge economy -Urban studies -Arms control -Understanding and responding to terrorism -Medical informatics -Computer Sciences	approximation;bayesian network	Carsten Riggelsen	2006			econometrics;computer science;machine learning;statistics	ML	25.041876653546463	-24.255850364068696	193916
ea8519443f8297b45d7fadb92b613ade1ba31043	least squares parameter estimation for sparse functional varying coefficient model		In the present paper, we study functional varying coefficient model in which both the response and the predictor are functions. We give estimates of the intercept and the slope functions in the case that the observations are sparse and noise-contaminated longitudinal data by using least squares representation of the model parameters. To estimate the parameter functions involved in the representation, we use a regularization method in some reproducing kernel Hilbert spaces. As we will see, our procedure is easy to implement. Also, we obtain the convergence rates of the estimators in the L2-sense. These convergence rates establish that the procedure performs well, especially, when sampling frequency or sample size increases.	coefficient;estimation theory;extension (mac os);hilbert space;kerrison predictor;least squares;matrix regularization;sampling (signal processing);sparse matrix;stochastic process;visual intercept	Behdad Mostafaiy;Mohammad Reza Faridrohani	2017	JSTA	10.2991/jsta.2017.16.3.5	coefficient of determination;non-linear least squares;statistics;total least squares;explained sum of squares;generalized least squares;least squares;mathematical optimization;residual sum of squares;mathematics;simple linear regression	ML	29.98629021152091	-25.06835623246233	193960
b996cba727e6a3b8ccfbbe1fe3d45bc95e1d3757	smem algorithm is not fully compatible with maximum-likelihood framework	smem algorithm;maximum likelihood;maximum vraisemblance;local optimality;algorithme em;algoritmo em;split and merge expectation maximization algorithm;em algorithm;maxima verosimilitud;optimalite locale	The expectation-maximization (EM) algorithm with split-and-merge operations (SMEM algorithm) proposed by Ueda, Nakano, Ghahramani, and Hinton (2000) is a nonlocal searching method, applicable to mixture models, for relaxing the local optimum property of the EM algorithm. In this article, we point out that the SMEM algorithm uses the acceptance-rejection evaluation method, which may pick up a distribution with smaller likelihood, and demonstrate that an increase in likelihood can then be guaranteed only by comparing log likelihoods.	blast e-value;deny (action);expectation–maximization algorithm;hl7publishingsubsection <operations>;local optimum;mixture model;nonlocal lagrangian;rejection sampling;small	Akihiro Minagawa;Norio Tagawa;Toshiyuki Tanaka	2002	Neural Computation	10.1162/089976602753712927	econometrics;mathematical optimization;expectation–maximization algorithm;mathematics;maximum likelihood;statistics	ML	31.62583906969575	-27.986809361573428	193972
084411072e7e8fc864d0e1065a6a99521ff13065	partial least squares (pls) applied to medical bioinformatics	shrinkage estimation;latent variable;partial least square;complex adaptive system;regression model;colon cancer;ordinary least squares regression;statistical learning theory;sensitivity analysis;principal component analysis;least square;kernel ridge regression;variance estimation	PLS initially creates uncorrelated latent variables which are linear combinations of the original input vectors Xi, where weights are used to determine linear combinations, which are proportional to the covariance. Secondly, a least squares regression is then performed on the subset of extracted latent variables that lead to a lower and biased variance on transformed data. This process, leads to a lower variance estimate of the regression coefficients when compared to the Ordinary Least Squares regression approach. Classical Principal Component Analysis (PCA), linear PLS and kernel ridge regression (KRR) techniques are well known shrinkage estimators designed to deal with multicollinearity, which can be a serious problem. That is, multi-collinearity can dramatically influence the effectiveness of a regression model by changing the values and signs of estimated regression coefficients given different but similar data samples, thereby leading to a regression model which represents training data reasonably well, but generalizes poorly to validation and test data. We explain how to address these problems, which is followed by performing a PLS hypotheses driven preliminary research study and sensitivities analysis by not doing a combinatorial analysis as PLS will eliminate the unnecessary variables using a microarray colon cancer data set. Research studies as well as preliminary results are described in the results section. © 2010 Published by Elsevier B.V. Partial Least Squares; biomarker research; statistical learning theory;complex adaptive systems;colon cancer;microarrays	bioinformatics;coefficient;colon classification;latent variable;machine learning;microarray;ordinary least squares;partial least squares regression;principal component analysis;software release life cycle;statistical learning theory;support vector machine;test data	Walker H. Land;William S. Ford;Jin-Woo Park;Ravi Mathur;Nathan Hotchkiss;John J. Heine;Steven Eschrich;Xingye Qiao;Timothy Yeatman	2011		10.1016/j.procs.2011.08.051	latent variable;generalized least squares;segmented regression;principal component regression;complex adaptive system;total least squares;simple linear regression;econometrics;proper linear model;local regression;ordinary least squares;computer science;linear regression;linear predictor function;machine learning;bayesian multivariate linear regression;linear model;polynomial regression;regression diagnostic;mathematics;variance function;partial least squares regression;path coefficient;least squares;sensitivity analysis;robust regression;regression analysis;nonlinear regression;statistics;cross-sectional regression;principal component analysis	ML	27.222331548619003	-24.50356398240914	194190
c8b899b1cf87bda71a13c00fdeb72050d65e7b56	split-merge augmented gibbs sampling for hierarchical dirichlet processes	080109 pattern recognition and data mining	The Hierarchical Dirichlet Process (HDP) model is an important tool for topic analysis. Inference can be performed through a Gibbs sampler using the auxiliary variable method. We propose a splitmerge procedure to augment this method of inference, facilitating faster convergence. Whilst the incremental Gibbs sampler changes topic assignments of each word conditioned on the previous observations and model hyper-parameters, the split-merge sampler changes the topic assignments over a group of words in a single move. This allows efficient exploration of state space. We evaluate the proposed sampler on a synthetic test set and two benchmark document corpus and show that the proposed sampler enables the MCMC chain to converge faster to the desired stationary distribution.	benchmark (computing);converge;dhrystone;gibbs sampling;markov chain monte carlo;merge algorithm;sampling (signal processing);speedup;state space;stationary process;test set;text corpus;while	Santu Rana;Dinh Q. Phung;Svetha Venkatesh	2013		10.1007/978-3-642-37456-2_46	latent dirichlet allocation;computer science;machine learning;pattern recognition;statistics;hierarchical dirichlet process	ML	26.20014216855424	-30.71869873383453	194208
9aee738ffc01c391375a858d996f263b2bef25d1	a bayesian framework for regularized svm parameter estimation	bayesian framework;bayesian methods support vector machines parameter estimation support vector machine classification pattern classification computer science costs least squares methods constraint optimization error analysis;support vector machines;bayes methods;parameter estimation support vector machines bayes methods pattern classification;public domain;support set bayesian framework regularized svm parameter estimation support vector machine pattern classification soft margin classifier nonseparable learning samples;pattern classification;support vector machine;parameter estimation	The support vector machine (SVM) is considered here in the context of pattern classification. The emphasis is on the soft margin classifier which uses regularization to handle non-separable learning samples. We present an SVM parameter estimation algorithm that first identifies a subset of the learning samples that we call the support set and then determines not only the weights of the classifier but also the hyperparameter that controls the influence of the regularizing penalty term on basis thereof. We provide numerical results using several data sets from the public domain.	algorithm;estimation theory;least squares;margin classifier;numerical analysis;support vector machine	Jens Gregor;Zhenqiu Liu	2004	Fourth IEEE International Conference on Data Mining (ICDM'04)	10.1109/ICDM.2004.10094	margin classifier;support vector machine;least squares support vector machine;computer science;machine learning;linear classifier;pattern recognition;data mining;mathematics;relevance vector machine;structured support vector machine	ML	29.556937421300876	-31.959391716412767	194334
d4fc53f84f1d577913566ed98d8e05bf2ef76ca9	a diversified generative latent variable model for wifi-slam	slam;diversity encouraging prior;latent variable model	WiFi-SLAM aims to map WiFi signals within an unknown environment while simultaneously determining the location of a mobile device. This localization method has been extensively used in indoor, space, undersea, and underground environments. For the sake of accuracy, most methods label the signal readings against ground truth locations. However, this is impractical in large environments, where it is hard to collect and maintain the data. Some methods use latent variable models to generate latent-space locations of signal strength data, an advantage being that no prior labeling of signal strength readings and their physical locations is required. However, the generated latent variables cannot cover all wireless signal locations and WiFi-SLAM performance is significantly degraded. Here we propose the diversified generative latent variable model (DGLVM) to overcome these limitations. By building a positive-definite kernel function, a diversity-encouraging prior is introduced to render the generated latent variables non-overlapping, thus capturing more wireless signal measurements characteristics. The defined objective function is then solved by variational inference. Our experiments illustrate that the method performs WiFi localization more accurately than other label-free methods.	experiment;ground truth;internationalization and localization;kinect;latent variable model;loss function;mobile device;optimization problem;sensor;simultaneous localization and mapping;variational principle	Hao Xiong;Dacheng Tao	2017			simulation;computer science;machine learning;data mining;latent variable model	AI	37.12700750434849	-37.417038824610785	194342
b5c3f03c96f5d48c24840fc636ba742daf7f6ddf	bayesian analysis of hierarchical heteroscedastic linear models using dirichlet-laplace priors		From practical point of view, in a two-level hierarchical model, the variance of second-level usually has a tendency to change through sub-populations. The existence of this kind of local (or intrinsic ) heteroscedasticity is a major concern in the application of statistical modeling. The main purpose of this study is to construct a Bayesian methodology via shrinkage priors in order to estimate the interesting parameters under local heteroscedasticity. The suggested methodology for this issue is to use of a class of the local-global shrinkage priors, called Dirichlet-Laplace priors. The optimal posterior concentration and straightforward posterior computation are the appealing properties of these priors. Two real data sets are analyzed to illustrate the proposed methodology.	algorithm;computation;hierarchical database model;linear model;multilevel model;population;statistical model	S. K. Ghoreishi	2017	JSTA	10.2991/jsta.2017.16.1.5	variable-order bayesian network	Vision	29.233731172402386	-24.555650957369195	194566
bea8f3dd369682b71e5c69d32726cf0225e21b85	conjugate relation between loss functions and uncertainty sets in classification problems	convex conjugate;uncertainty set;loss function;consistency	There are two main approaches to binary classification problems: the loss function approach and the uncertainty set approach. The loss function approach is widely used in real-world data analysis. Statistical decision theory has been used to elucidate its properties such as statistical consistency. Conditional probabilities can also be estimated by using the minimum solution of the loss function. In the uncertainty set approach, an uncertainty set is defined for each binary label from training samples. The best separating hyperplane between the two uncertainty sets is used as the decision function. Although the uncertainty set approach provides an intuitive understanding of learning algorithms, its statistical properties have not been sufficiently studied. In this paper, we show that the uncertainty set is deeply connected with the convex conjugate of a loss function. On the basis of the conjugate relation, we propose a way of revising the uncertainty set approach so that it will have good statistical properties such as statistical consistency. We also introduce statistical models corresponding to uncertainty sets in order to estimate conditional probabilities. Finally, we present numerical experiments, verifying that the learning with revised uncertainty sets improves the prediction accuracy.	algorithm;binary classification;convex conjugate;decision theory;experiment;loss function;machine learning;numerical analysis;statistical model;verification and validation	Takafumi Kanamori;Akiko Takeda;Taiji Suzuki	2013	Journal of Machine Learning Research		mathematical optimization;discrete mathematics;uncertainty analysis;convex conjugate;computer science;machine learning;mathematics;consistency;sensitivity analysis;expected value of including uncertainty;statistics;loss function	ML	25.512957803716912	-27.209267737808464	194659
0084009de57409856694d857a261d69ad29b9eb2	stable and informative spectral signatures for graph matching	heating stability analysis kernel laplace equations eigenvalues and eigenfunctions optimization upper bound;spectral analysis graph theory image matching integer programming matrix algebra quadratic programming realistic images;real images informative spectral signatures weighted graph matching problem first order compatibility integer quadratic program formulation graph laplacian objective function spectral signature spectral node signatures pairwise heat kernel distance second order compatibility term adjacency matrix based second order compatibility function synthetic graphs cmu house sequence	In this paper, we consider the approximate weighted graph matching problem and introduce stable and informative first and second order compatibility terms suitable for inclusion into the popular integer quadratic program formulation. Our approach relies on a rigorous analysis of stability of spectral signatures based on the graph Laplacian. In the case of the first order term, we derive an objective function that measures both the stability and informativeness of a given spectral signature. By optimizing this objective, we design new spectral node signatures tuned to a specific graph to be matched. We also introduce the pairwise heat kernel distance as a stable second order compatibility term, we justify its plausibility by showing that in a certain limiting case it converges to the classical adjacency matrix-based second order compatibility function. We have tested our approach on a set of synthetic graphs, the widely-used CMU house sequence, and a set of real images. These experiments show the superior performance of our first and second order compatibility terms as compared with the commonly used ones.	adjacency matrix;approximation algorithm;electronic signature;experiment;information;laplacian matrix;loss function;matching (graph theory);optimization problem;plausibility structure;quadratic programming;synthetic intelligence;type signature	Nan Hu;Raif M. Rustamov;Leonidas J. Guibas	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.296	graph energy;mathematical optimization;combinatorics;discrete mathematics;graph bandwidth;laplacian matrix;machine learning;comparability graph;mathematics;geometry;spectral graph theory;adjacency matrix;statistics	Vision	35.95136726768338	-37.26312153493715	194883
140038e3b17b02ab7945c5447e8be6352d8d7034	bayes estimators for phylogenetic reconstruction	software;animals;phylogeny;maximum likelihood;phylogenetic reconstruction;bayes theorem;robinson foulds;euclidean distance;urodela;bayes estimator;models genetic;science learning;consensus tree;hill climbing;neighbor joining;quantitative method;path difference metric;computer simulation;phylogenetic inference	"""Tree reconstruction methods are often judged by their accuracy, measured by how close they get to the true tree. Yet, most reconstruction methods like maximum likelihood (ML) do not explicitly maximize this accuracy. To address this problem, we propose a Bayesian solution. Given tree samples, we propose finding the tree estimate that is closest on average to the samples. This """"median"""" tree is known as the Bayes estimator (BE). The BE literally maximizes posterior expected accuracy, measured in terms of closeness (distance) to the true tree. We discuss a unified framework of BE trees, focusing especially on tree distances that are expressible as squared euclidean distances. Notable examples include Robinson-Foulds (RF) distance, quartet distance, and squared path difference. Using both simulated and real data, we show that BEs can be estimated in practice by hill-climbing. In our simulation, we find that BEs tend to be closer to the true tree, compared with ML and neighbor joining. In particular, the BE under squared path difference tends to perform well in terms of both path difference and RF distances."""	centrality;hill climbing;mental retardation, x-linked, snyder-robinson type;naive bayes classifier;neighbor joining;phylogenetics;quartet distance;radio frequency;robinson–foulds metric;simulation;trees (plant);unified framework	Peter Huggins;Wenbin Li;David Haws;Thomas Friedrich;Jinze Liu;Ruriko Yoshida	2011	Systematic biology	10.1093/sysbio/syr021	computer simulation;mathematical optimization;combinatorics;vantage-point tree;bayes estimator;exponential tree;quantitative research;hill climbing;order statistic tree;euclidean distance;neighbor joining;interval tree;mathematics;maximum likelihood;bayes' theorem;statistics	ML	26.152437518645815	-25.333163293361366	195035
0b31db39d4e07fbe275985299be6b7a73e35a14f	efficient approximate linear programming for factored mdps	cluster constraints;junction graph;期刊论文;approximate linear programming;factored mdps	Factored Markov Decision Processes (MDPs) provide a compact representation for modeling sequential decision making problems with many variables. Approximate linear programming (LP) is a prominent method for solving factored MDPs. However, it cannot be applied to models with large treewidth due to the exponential number of constraints. This paper proposes a novel and efficient approximate method to represent the exponentially many constraints. We construct an augmented junction graph from the factored MDP, and represent the constraints using a set of cluster constraints and separator constraints, where the cluster constraints play the role of reducing the number of constraints, and the separator constraints enforce the consistency of neighboring clusters so as to improve the accuracy. In the case where the junction graph is tree-structured, our method provides an equivalent representation to the original constraints. In other cases, our method provides a good trade-off between computation and accuracy. Experimental results on different models show that our algorithm performs better than other approximate linear programming algorithms on computational cost or expected reward. Propose a new framework to approximate the exponentially many constraints in the linear program for factored MDPs.Prove that our approximation turns to be exact when the constructed junction graph is tree-structured.Show the good performance of our method by experimental results.	approximation algorithm;linear programming	Feng Chen;Qiang Cheng;Jianwu Dong;Zhaofei Yu;Guojun Wang;Wenli Xu	2015	Int. J. Approx. Reasoning	10.1016/j.ijar.2015.06.002	mathematical optimization;combinatorics;machine learning;mathematics	AI	25.05219122584643	-28.38527459875942	195207
c0fa5e629316b875a839331d4ef942f220ebdf61	estimation of time-varying mixture models: an application to traffic estimation	traffic flow estimation scenario time varying mixture models complex data collections mixture components distribution parameters automatic hard em algorithm;mixture models vehicles clustering algorithms signal processing algorithms sensors estimation roads;traffic engineering computing expectation maximisation algorithm mixture models time varying channels	Time varying mixture models can be a useful tool for modelling complex data collections. However the additional complexity of letting the number of mixture components vary over time adds even more difficulty in inference of the distribution parameters. We propose the automatic hard EM algorithm to infer the parameters of these complex, time-varying mixture models. We demonstrate its performance using simulated and real data in a traffic flow estimation scenario.	data flow diagram;estimation theory;expectation–maximization algorithm;mixture model	Sean F. Lawlor;Michael G. Rabbat	2016	2016 IEEE Statistical Signal Processing Workshop (SSP)	10.1109/SSP.2016.7551777	econometrics;computer science;machine learning;statistics	Vision	30.13297828611231	-30.80700135191842	195221
461fa46a23fb057d807742d17af64f663d2e4229	global optimization of factor models using alternating minimization		Learning new representations in machine learning is often tackled using a factorization of the data. For many such problems, including sparse coding and matrix completion, learning these factorizations can be difficult, in terms of efficiency and to guarantee that the solution is a global minimum. Recently, a general class of objectives have been introduced, called induced regularized factor models (RFMs), which have an induced convex form that enables global optimization. Though attractive theoretically, this induced form is impractical, particularly for large or growing datasets. In this work, we investigate the use of a practical alternating minimization algorithms for induced RFMs, that ensure convergence to global optima. We characterize the stationary points of these models, and, using these insights, highlight practical choices for the objectives. We then provide theoretical and empirical evidence that alternating minimization, from a random initialization, converges to global minima for a large subclass of induced RFMs. In particular, we prove that induced RFMs do not have degenerate saddlepoints and that local minima are actually global minima. Finally, we provide an extensive investigation into practical optimization choices for using alternating minimization for induced RFMs, for both batch and stochastic gradient descent.	algorithm;global optimization;machine learning;mathematical optimization;maxima and minima;neural coding;sparse matrix;stationary process;stochastic gradient descent	Lei Le;Martha White	2016	CoRR		mathematical optimization;combinatorics;machine learning;mathematics;statistics	ML	25.25406217543516	-34.38621020620588	195227
6476e617089c5c3cd01652dfa5a0226f014232b4	a novel method for reducing the dimensionality in a sensor array	eigenvalues and eigenfunctions;data compression;pattern classifier sensor array dimensionality reduction gas sensors response spectrum volatile compounds response sensor selection reduction electronic nose polymer gas sensors eigenvectors karhunen loeve expansion feature selection problems aroma sensor data compression odour recognition ann calibration truncated expansion eigenvalues covariance matrix space projections;feature extraction gas sensors intelligent sensors pattern classification sensor fusion eigenvalues and eigenfunctions feedforward neural nets karhunen loeve transforms data compression covariance matrices array signal processing;array signal processing;spectrum;indexing terms;gas sensor;sensor arrays gas detectors sensor systems acoustic sensors costs polymers electronic noses sensor systems and applications conducting materials organic materials;karhunen loeve transforms;covariance matrices;feature extraction;pattern classification;sensor array;volatile compounds;feature selection;feedforward neural nets;sensor fusion;electronic nose;gas sensors;karhunen loeve expansion;intelligent sensors;artificial neural network;eigenvectors	Specific types of gas sensors are normally produced by adding different dopants to a common substrate. The advancement of technology has made the fabrication of many dopants and consequently various sensors possible. As a result, in each family of gas sensors, one can find tens of different sensors which are only slightly different in the spectrum of response to various volatile compounds. The wide variety of available gas sensors creates a selection problem for any specific application. Sensor selection/reduction becomes even more important when cost and technology limitations are issues of concern. Accordingly, a methodology by which one can tailor a sensor array to a specific need is highly desirable. In this paper, a novel method is introduced to address this task using data from an electronic nose that uses polymer gas sensors. This method has been delineated based on the geometry of eigenvectors in Karhunen-Loeve expansion. The methodology is general and therefore suitable for many other feature selection problems.		Bahram Ghaffarzadeh Kermani;Susan S. Schiffman;H. Troy Nagle	1998	IEEE Trans. Instrumentation and Measurement	10.1109/19.744338	data compression;electronic nose;spectrum;electronic engineering;speech recognition;index term;feature extraction;eigenvalues and eigenvectors;computer science;engineering;machine learning;sensor fusion;feature selection;sensor array;artificial neural network;intelligent sensor	Embedded	37.48447554617175	-34.55523252063878	195317
5a1b1c33d8794d4cf24495b2355a88bb9929601a	spatio-temporal pattern of viral meningitis in michigan, 1993-2001	engineering;maps;distribucion espacial;variation spatiotemporelle;civil and environmental engineering;population;densite;information systems;systeme information geographique;mapa;coeficiente correlacion;medical geology;north america;america del norte;amerique du nord;viral meningitis jel classification i10;variation saisonniere;salud publica;social sciences;communicable diseases;preventive measures;time variations;viral meningitis;michigan;economics management science;communicable disease;meningitis;densidad;seasonal variations;variation temporelle;detection;space time;cumulative incidence;etats unis;classification;estados unidos;carte;epidemiology;spatial variations;natural resources and environment;statistical analysis;population density;humanities;echantillon reference;spatial distribution;geographic information systems;epidemiologia;seasonality;atmospheric oceanic and space sciences;interannual variability;regional science;variacion espacial;analyse statistique;temporal pattern;poblacion;philosophy;incidence rate;variation spatiale;sante publique;meningite;variacion estacional;density;information system;distribution spatiale;spatial analysis;communities;geologie et sante;geography general;environmental exposure;correlation coefficient;space time clustering;action preventive;accion preventiva;geography and maps;variacion temporal;coefficient correlation;clasificacion;standard samples;systeme information;roca patron;public health;geographic distribution;epidemiologie;spatiotemporal variations;high risk	To characterize Michigan’s high viral meningitis incidence rates, 8,803 cases from 1993–2001 were analyzed for standard epidemiological indices, geographic distribution, and spatio-temporal clusters. Blacks and infants were found to be high-risk groups. Annual seasonality and interannual variability in epidemic magnitude were apparent. Cases were concentrated in southern Michigan, and cumulative incidence was correlated with population density at the county level (r 1⁄4 0.45, p<0.001). Kulldorff’s Scan test identified the occurrence of spatio-temporal clusters in Lower Michigan during July–October 1998 and 2001 (p 1⁄4 0.01). More extensive data on cases, laboratory isolates, sociodemographics, and environmental exposures should improve detection and enhance the effectiveness of a SpaceTime Information System aimed at prevention.	computer cluster;incidence matrix;information system;seasonality;spatial variability	Sharon K. Greene;Mark A. Schmidt;Mary Grace Stobierski;Mark L. Wilson	2005	Journal of Geographical Systems	10.1007/s10109-005-0151-x	demography;epidemiology;public health;geography;mathematics;information system;cartography	DB	38.72303884496702	-24.182858337590417	195525
bc99b608c495b42685b02511c04318fac244318d	graphical model selection and estimation for high dimensional tensor data	62f12;l 1 penalized likelihood;oracle property;62f10;gene networks;l 1;期刊论文;l1 penalized likelihood;tensor normal distribution;gaussian graphical model	Multi-way tensor data are prevalent in many scientific areas such as genomics and biomedical imaging. We consider a K-way tensor-normal distribution, where the precision matrix for each way has a graphical interpretation. We develop an l1 penalized maximum likelihood estimation and an efficient coordinate descent-based algorithm for model selection and estimation in such tensor normal graphical models. When the dimensions of the tensor are fixed, we drive the asymptotic distributions and oracle property for the proposed estimates of the precision matrices. When the dimensions diverge as the sample size goes to infinity, we present the rates of convergence of the estimates and sparsistency results. Simulation results demonstrate that the proposed estimation procedure can lead to better estimates of the precision matrices and better identifications of the graph structures defined by the precision matrices than the standard Gaussian graphical models. We illustrate the methods with an analysis of yeast gene expression data measured over different time points and under different experimental conditions.	graphical model;model selection	Shiyuan He;Jianxin Yin;Hongzhe Li;Xing Wang	2014	J. Multivariate Analysis	10.1016/j.jmva.2014.03.007	econometrics;gene regulatory network;mathematical optimization;mathematics;statistics	ML	29.267863950153806	-25.944393739413826	195534
0238bd34c91bbf18db1af9dcdb656906fb19d0c7	pushing for the extreme: estimation of poisson distribution from low count unreplicated data - how close can we get?	bayesian learning;unreplicated data;expected kullback leibler divergence;poisson distribution	Studies of learning algorithms typically concentrate on situations where potentially ever growing training sample is available. Yet, there can be situations (e.g., detection of differentially expressed genes on unreplicated data or estimation of time delay in non-stationary gravitationally lensed photon streams) where only extremely small samples can be used in order to perform an inference. On unreplicated data, the inference has to be performed on the smallest sample possible—sample of size 1. We study whether anything useful can be learnt in such extreme situations by concentrating on a Bayesian approach that can account for possible prior information on expected counts. We perform a detailed information theoretic study of such Bayesian estimation and quantify the effect of Bayesian averaging on its first two moments. Finally, to analyze potential benefits of the Bayesian approach, we also consider Maximum Likelihood (ML) estimation as a baseline approach. We show both theoretically and empirically that the Bayesian model averaging can be potentially beneficial.	algorithm;baseline (configuration management);broadcast delay;ensemble learning;information theory;machine learning;stationary process	Peter Tiño	2013	Entropy	10.3390/e15041202	bayesian average;econometrics;pattern recognition;mathematics;poisson distribution;bayesian statistics;bayesian inference;statistics	ML	28.132037429545257	-27.62906228603511	195645
54baff9a21b707aa9aa117cd5c1fb75da9a9495e	visual learning given sparse data of unknown complexity	unsupervised learning;sample size;image processing;information criterion;bayes methods;layout bayesian methods kernel computer science humans prototypes face recognition codes predictive models computer vision;dynamic scene structure unsupervised visual learning model order selection criteria sparse data model complexity bayesian information criterion completed likelihood akaike information criterion optimal model order data sample size;model order selection;model complexity;visual learning;sparse data;bayesian information criterion;natural scenes;unsupervised learning image processing natural scenes bayes methods;optimization model;dynamic scenes	This study addresses the problem of unsupervised visual learning. It examines existing popular model order selection criteria before proposes two novel criteria for improving visual learning given sparse data and without any knowledge about model complexity. In particular, a rectified Bayesian information criterion (BICr) and a completed likelihood Akaike's information criterion (CL-AIC) are formulated to estimate the optimal model order (complexity) for learning the dynamic structure of a visual scene. Both criteria are designed to overcome poor model selection by existing popular criteria when the data sample size varies from very small to large. Extensive experiments on learning a dynamic scene structure are carried out to demonstrate the effectiveness of BICr and CL-AIC, compared to that of BIC (Schwarz, 1978), AIC (Akaike, 1973), ICL (Biernacki, 2000) and a MML (Figueiredo and Jain, 2002) based criterion.	akaike information criterion;bayesian information criterion;complexity;experiment;icl;langrisser schwarz;mixture model;model selection;sparse matrix;synthetic intelligence;unsupervised learning;visual learning	Tao Xiang;Shaogang Gong	2005	Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1	10.1109/ICCV.2005.250	unsupervised learning;sample size determination;computer vision;sparse matrix;image processing;computer science;machine learning;pattern recognition;mathematics;bayesian information criterion;statistics	Vision	29.25932227526071	-32.32200086177367	195680
62d00e5bf748a797fb6cea56addc39ae0dea0fcd	subspace penalized sparse learning for joint sparse recovery	joint sparse recovery;compressed sensing;bayes methods;vectors bayes methods compressed sensing greedy algorithms learning artificial intelligence;greedy algorithms;subspace based hybrid greedy algorithms subspace penalized sparse learning multiple measurement vector problem mmv compressed sensing problem sparse signal vectors joint sparse recovery algorithms m sbl multiple sparse bayesian learning spark reduction property;multiple measurement vector problem compresse sensing joint sparse recovery;vectors;learning artificial intelligence;compresse sensing;multiple measurement vector problem;joints multiple signal classification bayes methods algorithm design and analysis sensors signal to noise ratio signal processing algorithms	The multiple measurement vector problem (MMV) is a generalization of the compressed sensing problem that addresses the recovery of a set of jointly sparse signal vectors. One of the important contributions of this paper is to reveal that the seemingly least related state-of-art MMV joint sparse recovery algorithms - M-SBL (multiple sparse Bayesian learning) and subspace-based hybrid greedy algorithms - have a very important link. More specifically, we show that replacing the log det(·) term in M-SBL by a log det(·) rank proxy that exploits the spark reduction property discovered in subspace-based joint sparse recovery algorithms, provides significant improvements. Theoretical analysis demonstrates that even thoughM-SBL is often unable to remove all localminimizers, the proposed method can do so under fairly mild conditions, without affecting the global minimizer.	compressed sensing;greedy algorithm;multiuse model view;sparse matrix	Jong Chul Ye;Jongmin Kim;Yoram Bresler	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638824	mathematical optimization;greedy algorithm;computer science;machine learning;pattern recognition;sparse approximation;mathematics;compressed sensing	Vision	26.562466115610555	-37.5421792282495	195760
9076cdd67d4f2bbe2708ec8196f5caed13341a25	the influence of operational cost on estimation	article	This work concerns the way that statistical models are used to make decisions. In particular, we aim to merge the way estimation algorithms are designed with how they are used for a subsequent task. Our methodology considers the operational cost of carrying out a policy, based on a predictive model. The operational cost becomes a regularization term in the learning algorithm’s objective function, allowing either an optimistic or pessimistic view of possible costs. Limiting the operational cost reduces the hypothesis space for the predictive model, and can thus improve generalization. We show that different types of operational problems can lead to the same type of restriction on the hypothesis space, namely the restriction to an intersection of an `q ball with a halfspace. We bound the complexity of such hypothesis spaces by proposing a technique that involves counting integer points in polyhedrons.	approximation algorithm;gradient;ibm notes;karp's 21 np-complete problems;loss function;machine learning;mathematical optimization;matrix regularization;optimization problem;polyhedron;predictive modelling;statistical model	Cynthia Rudin;Theja Tulabandhula	2012			political science;public administration;management;operations research	ML	32.36672457968328	-31.45836392176211	195990
fb274bb4e717fc2396d4378a77c70a6efba272df	an exact penalty method for locally convergent maximum consensus		Maximum consensus estimation plays a critically important role in computer vision. Currently, the most prevalent approach draws from the class of non-deterministic hypothesize-and-verify algorithms, which are cheap but do not guarantee solution quality. On the other extreme, there are global algorithms which are exhaustive search in nature and can be costly for practical-sized inputs. This paper aims to fill the gap between the two extremes by proposing a locally convergent maximum consensus algorithm. Our method is based on a formulating the problem with linear complementarity constraints, then defining a penalized version which is provably equivalent to the original problem. Based on the penalty problem, we develop a Frank-Wolfe algorithm that can deterministically solve the maximum consensus problem. Compared to the randomized techniques, our method is deterministic and locally convergent, relative to the global algorithms, our method is much more practical on realistic input sizes. Further, our approach is naturally applicable to problems with geometric residuals.	approximation algorithm;brute-force search;chandra–toueg consensus algorithm;complementarity theory;computer vision;consensus (computer science);frank–wolfe algorithm;genetic algorithm;heuristic (computer science);least squares;local convergence;local optimum;penalty method;random sample consensus;randomized algorithm;unbalanced circuit	Huu Le;Tat-Jun Chin;David Suter	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.48	robustness (computer science);penalty method;local convergence;brute-force search;mathematical optimization;machine learning;artificial intelligence;mathematics;consensus	Vision	24.713142719890456	-33.55964005771743	196238
f4d939f8fe3e35db014828d9746a797583466448	arma modeling in multichannel filter banks with applications to radar signal analysis	multichannel filter banks;filtering;filter bank;channel bank filters radar applications signal analysis polynomials stochastic processes filtering pattern classification filter bank application software predictive models;radar signal analysis;autoregressive moving average processes;application software;signal analysis;coarse scales;arma model;classification;polynomials;moving average;stochastic processes;minimum distance;arma modeling;channel bank filters;feature extraction;feature extraction autoregressive moving average processes radar signal processing pattern classification;minimum distance classifier;pattern classification;radar applications;predictive models;real aperture radar signals;stochastic model;hierarchical autoregressive moving average model;radar signal processing;hierarchical model;stochastic models;real aperture radar signals multichannel filter banks radar signal analysis arma modeling classification stochastic models hierarchical autoregressive moving average model coarse scales minimum distance classifier	In this paper, we consider the classification of radar signals by using stochastic models a t different scales. The signal at a different scale is modeled by a hierarchical Autoregressive Moving Average (ARMA) model, and the features at coarse scales are extracted from the model without performing expensive filtering operation. The hierarchical modeling can increase the accuracy of radar signal classification by exploiting features at different scales. For radar signal classification, model parameters at five different scales obtained by hierarchical modeling are used as features. A minimum distance classifier is implemented, and is tested on real aperture radar signals.	autoregressive model;filter bank;radar;signal processing;stochastic process	Kie B. Eom	1995		10.1109/ICASSP.1995.479777	stochastic process;speech recognition;computer science;stochastic modelling;signal processing;pattern recognition;mathematics;statistics	Robotics	37.56879400261574	-34.40345401200455	196528
44f9f347017c81b4262b7a8f5cd1a4b99c761858	winnowing subspaces	symmetric projection matrix;winnowing subspaces;matrix exponential;arbitrary matrix;online algorithm;matrix exponentiated gradient algorithm;hidden low rank projection;matrix logarithm;symmetric positive definite matrix;low rank;winnow algorithm	We generalize the Winnow algorithm for learning disjunctions to learning subspaces of low rank. Subspaces are represented by symmetric projection matrices. The online algorithm maintains its uncertainty about the hidden low rank projection matrix as a symmetric positive definite matrix. This matrix is updated using a version of the Matrix Exponentiated Gradient algorithm that is based on matrix exponentials and matrix logarithms. As in the case of the Winnow algorithm, the bounds are logarithmic in the dimension n of the problem, but linear in the rank r of the hidden subspace. We show that the algorithm can be adapted to handle arbitrary matrices of any dimension via a reduction.	gradient;online algorithm;reduction (complexity);the matrix;winnow (algorithm)	Manfred K. Warmuth	2007		10.1145/1273496.1273622	winnow;cuthill–mckee algorithm;mathematical optimization;gaussian elimination;combinatorics;discrete mathematics;eigendecomposition of a matrix;eight-point algorithm;nonnegative matrix;single-entry matrix;band matrix;centering matrix;mathematics;pascal matrix;state-transition matrix;augmented matrix;block matrix;matrix;symmetric matrix	ML	25.299963678800328	-35.82424810379372	196605
0f2562f80ff868da91a718c55dd96618b0af5fc9	graphical markov models		Graphical Markov models are multivariate statistical models which are currently under vigorous development and which combine two simple but most powerful notions, generating processes in single and joint response variables and conditional independences captured by graphs. The development of graphical Markov started with work by Wermuth (1976, 1980) and Darroch, Lauritzen and Speed (1980) which built on early results in 1920 to 1930 by geneticist Sewall Wright and probabilist Andrej Markov as well as on results for log-linear models by Birch (1963), Goodman (1970), Bishop, Fienberg and Holland (1973) and for covariance selection by Dempster (1972). Wright used graphs, in which nodes represent variables and arrows indicate linear dependence, to describe hypotheses about stepwise processes in single responses that could have generated his data. He developed a method, called path analysis, to estimate linear dependences and to judge whether the hypotheses are well compatible with his data which he summarized in terms of simple and partial correlations. With this approach he was far ahead of his time, since corresponding formal statistical methods for estimation and tests of goodness of fit were developed much later and graphs that capture independences even much later than tests of goodness of fit. It remains a primary objective of graphical Markov models to uncover graphical representations that lead to an understanding of data generating processes. Such processes are no longer restricted to linear relations but contain linear dependences as special cases. A probabilistic data generating process is a recursive sequence of conditional distributions in which response variables may be vector variables that contain discrete or continuous components. Thereby, each conditional distribution specifies both the dependence of response Ya, say, on an explanatory variable vector Yb and the undirected associations of the components of Ya. Graphical Markov models also generalize sequences in single responses and single explanatory variables that have been named Markov chains, after probabilist Markov. He recognized at the beginning of the 29th century that seemingly complex joint probability distributions may be radically simplified by using the notion of conditional independence and defined what are now called Markov chains. In a Markov chain of random variables Y1, . . . , Yi, . . . , Yd, the joint distribution is built up by starting with the density of fd of Ydand generating next fd−1|d. Then, conditional independence of Yd−2 from Yd given Yd−1 is taken into account with fd−2|d−1,d = fd−2|d−1. One continues such that, with fi|i+1,...d = fi|i+1, response Yi is conditionally independent of Yi+2, . . . , Yd given Yi+1, written compactly	graph (discrete mathematics);graphical user interface;linear logic;linear model;log-linear model;markov chain;markov model;path analysis (statistics);recurrence relation;recursion;statistical model;stepwise regression	Nanny Wermuth	2011		10.1007/978-3-642-04898-2_282	machine learning;markov model;artificial intelligence;mathematics	ML	26.49905518712021	-26.045299408136636	196756
899d55dbdb3ab147d172f19082a8d44408c7ffa0	high dimensional robust sparse regression		We provide a novel – and to the best of our knowledge, the first – algorithm for high dimensional sparse regression with corruptions in explanatory and/or response variables. Our algorithm recovers the true sparse parameters in the presence of a constant fraction of arbitrary corruptions. Our main contribution is a robust variant of Iterative Hard Thresholding. Using this, we provide accurate estimators with sub-linear sample complexity. Our algorithm consists of a novel randomized outlier removal technique for robust sparse mean estimation that may be of interest in its own right: it is orderwise more efficient computationally than existing algorithms, and succeeds with high probability, thus making it suitable for general use in iterative algorithms. We demonstrate the effectiveness on large-scale sparse regression problems with arbitrary corruptions.	iterative method;randomized algorithm;sample complexity;sparse matrix;thresholding (image processing);with high probability	Liu Liu;Yanyao Shen;Tianyang Li;Constantine Caramanis	2018	CoRR		time complexity;estimator;mathematical optimization;outlier;covariance matrix;filter (signal processing);thresholding;mathematics;matrix (mathematics);algorithm;covariance	ML	30.30048476007542	-29.51796675768997	196856
fde22484d61a0c9c4170746f1c8d88320dd10887	pairwise meta-modeling of multivariate output computer models using nonseparable covariance function	software and technology for statistics measurement analysis statistics;pseudolikelihood;pairwise modeling;grupo de excelencia;computer models multivariate covariance output parameters function;computer experiment;ciencias basicas y experimentales;matematicas;multivariate gaussian process;meta models	The Gaussian process (GP) model is a popular method for emulating deterministic computer simulation models. Its natural extension to computer models with multivariate outputs employs a multivariate Gaussian process (MGP) framework. Nevertheless, with significant increase in the number of design points and the number of model parameters, building a MGP model is a very challenging task. Under a general MGP model framework with nonseparable covariance functions, we propose an efficient meta-modeling approach featuring a pairwise model building scheme. The proposed method has excellent scalability even for a large number of output levels. Some properties of the proposed method have been investigated and its performance has been demonstrated through several numerical examples. Supplementary materials of this paper are available online.	algorithm;bivariate data;computer simulation;data general eclipse mv/8000;emulator;gaussian process;mathematical model;mathematical optimization;metamodeling;mouse genetics project;numerical analysis;numerical method;scalability	Yongxiang Li;Qiang Zhou	2016	Technometrics	10.1080/00401706.2015.1079244	econometrics;computer experiment;mathematics;statistics	ML	31.596844070953516	-26.579869705652516	197138
0dad096cda3c1eea44e6a149e44d08164037f1ea	tensor decomposition for fast parsing with latent-variable pcfgs		We describe an approach to speed-up inference with latent-variable PCFGs, which have been shown to be highly effective for natural language parsing. Our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable PCFGs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature. We also describe an error bound for this approximation, which gives guarantees showing that if the underlying tensors are well approximated, then the probability distribution over trees will also be well approximated. Empirical evaluation on real-world natural language parsing data demonstrates a significant speed-up at minimal cost for parsing performance.	approximation algorithm;dynamic programming;graphical model;inside–outside algorithm;latent variable;natural language;parsing;spectral density estimation;statistical model;stochastic context-free grammar	Shay B. Cohen;Michael Collins	2012			natural language processing;theoretical computer science;machine learning;mathematics	ML	26.10134887073506	-30.340585647965725	197423
80ffc8098c6d2edabdb2f7b798cb6ef157d61d42	bivariate copula additive models for location, scale and shape	additive predictor;mathematics and statistics;copula;simultaneous parameter estimation;marginal distribution;economics	In generalized additive models for location, scale and shape (GAMLSS), the response distribution is not restricted to belong to the exponential family and all the model’s parameters can be made dependent on additive predictors that allow for several types of covariate effects (such as linear, non-linear, random and spatial effects). In many empirical situations, however, modeling simultaneously two or more responses conditional on some covariates can be of considerable relevance. The scope of GAMLSS is extended by introducing bivariate copula models with continuous margins for the GAMLSS class. The proposed computational tool permits the copula dependence and marginal distribution parameters to be estimated simultaneously, and each parameter to be modeled using an additive predictor. Simultaneous parameter estimation is achieved within a penalized likelihood framework using a trust region algorithm with integrated automatic multiple smoothing parameter selection. The introduced approach allows for straightforward inclusion of potentially any parametric marginal distribution and copula function. The models can be easily used via the copulaReg() function in the R package SemiParBIVProbit. The proposal is illustrated through two case studies and simulated data.		Giampiero Marra;Rosalba Radice	2017	Computational Statistics & Data Analysis	10.1016/j.csda.2017.03.004	marginal distribution;econometrics;copula;pattern recognition;mathematics;statistics;generalized additive model for location, scale and shape	ML	28.961278258796124	-23.99516683093506	197528
1862bc66f8b700e81bbe9850ee68e7abf54f4c54	belief function robustness in estimation		We consider the case in which the available knowledge does not allow to specify a precise probabilistic model for the prior and/or likelihood in statistical estimation. We assume that this imprecision can be represented by belief functions. Thus, we exploit the mathematical structure of belief functions and their equivalent representation in terms of closed convex sets of probability measures to derive robust posterior inferences.	estimation theory;ibm notes;mathematical structure;statistical model;switzerland	Alessio Benavoli	2012		10.1007/978-3-642-29461-7_44	machine learning;pattern recognition;mathematics;statistics	ML	29.717415683323868	-27.478863285592944	197619
81c279112c6e325f17ef8c37031d1b842aa84a4c	machine learning of space-fractional differential equations		Data-driven discovery of “hidden physics” – i.e., machine learning of differential equation models underlying observed data – has recently been approached by embedding the discovery problem into a Gaussian Process regression of spatial data, treating and discovering unknown equation parameters as hyperparameters of a “physics informed” Gaussian Process kernel. This kernel includes the parametrized differential operators applied to a prior covariance kernel. We extend this framework to the data-driven discovery of linear space-fractional differential equations. The methodology is compatible with a wide variety of space-fractional operators in Rd and stationary covariance kernels, including the Matérn class, and allows for optimizing the Matérn parameter during training. The main challenges to be addressed are a user-friendly way to perform fractional-order derivatives of covariance kernels, together with numerical methods that render such implementations feasible. Making use of the simple Fourier-space representation of space-fractional derivatives in Rd, we provide a unified set of integral formulas for the resulting Gaussian Process kernels. The shift property of the Fourier transform results in formulas involving d-dimensional integrals that can be efficiently treated using generalized Gauss-Laguerre quadrature. The implementation of fractional derivatives has several benefits. First, the method allows for discovering models involving fractional-order PDEs for systems characterized by heavy tails or anomalous diffusion, while bypassing the analytical difficulty of fractional calculus. Data sets exhibiting such features are of increasing prevalence in physical and financial domains. Second, a single fractional-order archetype allows for a derivative term of arbitrary order to be learned, with the order itself being a parameter in the regression. As a result, even when used for discovering integer-order equations, the proposed method has several benefits compared to previous works on data-driven discovery of differential equations; the user is not required to assume a “dictionary” of derivatives of various orders, and directly controls the parsimony of the models being discovered. We illustrate our method on several examples, including fractional-order interpolation of advection-diffusion and modeling relative stock performance in the S&P 500 with α-stable motion via a fractional diffusion equation.	artificial neural network;automatic differentiation;black–scholes model;computation;constant term;dictionary;dynamical system;emoticon;gaussian process;gauss–laguerre quadrature;interpolation;kriging;machine learning;maximum parsimony (phylogenetics);nonlinear system;numerical analysis;numerical method;occam's razor;risk management;robustness (computer science);stationary process;tails;time complexity;time series;usability	Mamikon Gulian;Maziar Raissi;Paris Perdikaris;George Em Karniadakis	2018	CoRR		diffusion equation;fourier transform;mathematical optimization;machine learning;artificial intelligence;fractional calculus;anomalous diffusion;differential equation;kriging;gaussian process;mathematics;covariance	ML	32.70459451675015	-27.893120874454702	197753
12b28ab9e7f3657616843b3105fcc4b014e334f7	bayesian switching interaction analysis under uncertainty		We introduce a Bayesian discrete-time framework for switching-interaction analysis under uncertainty, in which latent interactions, switching pattern and object states and dynamics are inferred from noisy (and possibly missing) observations of these objects. We propose reasoning over full posterior distribution of these latent variables as a means of combating and characterizing uncertainty. This approach also allows for answering a variety of questions probabilistically, which is suitable for exploratory pattern discovery and post-analysis by human experts. This framework is based on a fully-Bayesian learning of the structure of a switching dynamic Bayesian network (DBN) and utilizes a statespace approach to allow for noisy observations and missing data. It generalizes the autoregressive switching interaction model of Siracusa et al. [1], which does not allow observation noise, and the switching linear dynamic system model of Fox et al. [2], which does not infer interactions among objects. Posterior samples are obtained via a Gibbs sampling procedure, which is particularly efficient in the case of linear Gaussian dynamics and observation models. We demonstrate the utility of our framework on a controlled human-generated data, and climate data.	autoregressive model;dynamic bayesian network;dynamical system;experiment;gibbs sampling;image noise;interaction;jetflash;latent variable;missing data;sampling (signal processing);state space;structural similarity;time series	Zoran Dzunic;John W. Fisher	2014			econometrics;computer science;data mining;statistics	ML	27.80393052590366	-26.124055587971363	197777
32957c7ded8b23bc58a8cf025012898d9bd6363c	machine olfaction: pattern recognition for the identification of aromas	pattern recognition instruments machine intelligence electronic noses signal design digital signal processing digital communication hardware oils petroleum;digital signal processing;instruments;advanced signal processing;extra virgin olive oils;signal design;pattern recognition equipment;aromas identification;petroleum;machine olfaction;digital communication;oils;digital communications services;spanish olive oil machine olfaction pattern recognition aromas identification intelligent volatile detection embedded technology electronic nose advanced signal processing digital communications services extra virgin olive oils;embedded technology;feature extraction;signal processing;machine intelligence;intelligent sensors pattern recognition equipment feature extraction sensor fusion gas sensors;pattern recognition;intelligent volatile detection;extra virgin olive oil;electronic noses;olive oil;sensor fusion;electronic nose;gas sensors;spanish olive oil;intelligent sensors;hardware	This paper describes a portable Pattern Recognition System (PRS) based on embedded technology for intelligent volatile detection (Electronic Nose). This instrument is designed to hold advanced signal processing and digital communications services in a contained size. A summary of the hardware is presented followed by an application to the identification of extra virgin olive oils. The instrument of the example is able to classify eleven different classes of Spanish olive oil with a 79% of accuracy and relatively simple pattern recognition techniques.	dimensionality reduction;embedded system;feature extraction;feature selection;mathematical optimization;modulation;pattern recognition;plasma cleaning;procedural reasoning system;sensor;signal processing	Alexandre Perera-Lluna;A. Gomez-Baena;Teodor Sundic;T. Pardo;Santiago Marco	2002		10.1109/ICPR.2002.1048326	electronic nose;computer vision;speech recognition;feature extraction;computer science;digital signal processing;signal processing;sensor fusion;petroleum;intelligent sensor	Vision	37.167635504609514	-33.711462153161115	197882
dccf47646bea3dae2b062c77d0770e1713910526	kernel clustering: density biases and solutions		Kernel methods are popular in clustering due to their generality and discriminating power. However, we show that many kernel clustering criteria have density biases theoretically explaining some practically significant artifacts empirically observed in the past. For example, we provide conditions and formally prove the density mode isolation bias in kernel K-means for a common class of kernels. We call it Breiman’s bias due to its similarity to the histogram mode isolation previously discovered by Breiman in decision tree learning with Gini impurity. We also extend our analysis to other popular kernel clustering methods, e.g., average/normalized cut or dominant sets, where density biases can take different forms. For example, splitting isolated points by cut-based criteria is essentially the sparsest subset bias, which is the opposite of the density mode bias. Our findings suggest that a principled solution for density biases in kernel clustering should directly address data inhomogeneity. We show that density equalization can be implicitly achieved using either locally adaptive weights or locally adaptive kernels. Moreover, density equalization makes many popular kernel clustering objectives equivalent. Our synthetic and real data experiments illustrate density biases and proposed solutions. We anticipate that theoretical understanding of kernel clustering limitations and their principled solutions will be important for a broad spectrum of data analysis applications across the disciplines.		Dmitrii Marin;Meng Tang;Ismail Ben Ayed;Yuri Boykov	2019	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2017.2780166	artificial intelligence;kernel (linear algebra);computer science;kernel (statistics);correlation clustering;kernel method;pattern recognition;decision tree learning;cluster analysis;histogram;variable kernel density estimation	ML	27.40921239523526	-37.62166388112682	198075
214f1a4d58221e1668f1ea4deb0c02ac99315a9b	perfect dimensionality recovery by variational bayesian pca		The variational Bayesian (VB) approach is one of the best tractable approximations to the Bayesian estimation, and it was demonstrated to perform well in many applications. However, its good performance was not fully understood theoretically. For example, VB sometimes produces a sparse solution, which is regarded as a practical advantage of VB, but such sparsity is hardly observed in the rigorous Bayesian estimation. In this paper, we focus on probabilistic PCA and give more theoretical insight into the empirical success of VB. More specifically, for the situation where the noise variance is unknown, we derive a sufficient condition for perfect recovery of the true PCA dimensionality in the large-scale limit when the size of an observed matrix goes to infinity. In our analysis, we obtain bounds for a noise variance estimator and simple closed-form solutions for other parameters, which themselves are actually very useful for better implementation of VB-PCA.	approximation;calculus of variations;cobham's thesis;sparse matrix;variational principle	Shinichi Nakajima;Ryota Tomioka;Masashi Sugiyama;S. Derin Babacan	2012			econometrics;mathematical optimization;machine learning;mathematics;statistics	ML	27.09077256660752	-32.88279035385428	198187
6154ce8c02375184f7928e41c4fae532500f7175	stochastic gradient descent with differentially private updates	privacy signal processing algorithms data privacy noise algorithm design and analysis logistics linear programming;stochastic processes;data privacy;gradient methods;scalable data stochastic gradient descent method learning sgd differential privacy batch size tractable data;stochastic processes data privacy gradient methods	Differential privacy is a recent framework for computation on sensitive data, which has shown considerable promise in the regime of large datasets. Stochastic gradient methods are a popular approach for learning in the data-rich regime because they are computationally tractable and scalable. In this paper, we derive differentially private versions of stochastic gradient descent, and test them empirically. Our results show that standard SGD experiences high variability due to differential privacy, but a moderate increase in the batch size can improve performance significantly.	algorithm;algorithmic efficiency;cobham's thesis;computation;differential privacy;experiment;heart rate variability;ibm notes;internet information services;iteration;mathematical optimization;scalability;spatial variability;stochastic gradient descent;stochastic optimization	Shuang Song;Kamalika Chaudhuri;Anand D. Sarwate	2013	2013 IEEE Global Conference on Signal and Information Processing	10.1109/GlobalSIP.2013.6736861	computer science;theoretical computer science;machine learning;data mining	ML	26.316329566621974	-31.562377981313666	198214
3264f6335b680e7cf25ce9cca2b89e54c3bf8ddd	an update on statistical boosting in biomedicine		Statistical boosting algorithms have triggered a lot of research during the last decade. They combine a powerful machine learning approach with classical statistical modelling, offering various practical advantages like automated variable selection and implicit regularization of effect estimates. They are extremely flexible, as the underlying base-learners (regression functions defining the type of effect for the explanatory variables) can be combined with any kind of loss function (target function to be optimized, defining the type of regression setting). In this review article, we highlight the most recent methodological developments on statistical boosting regarding variable selection, functional regression, and advanced time-to-event modelling. Additionally, we provide a short overview on relevant applications of statistical boosting in biomedicine.	algorithm;biomedicine;estimated;feature selection;loss function;machine learning;statistical model;explanation	Andreas Mayr;Benjamin Hofner;Elisabeth Waldmann;Tobias Hepp;Sebastian Meyer;Olaf Gefeller	2017		10.1155/2017/6083072	econometrics;computer science;machine learning;statistics	ML	26.422890238067694	-25.379385703034632	198241
bdaeb4397d11fe370bfea6130dce3490c7961c9d	real-time algorithms for sparse neuronal system identification	greedy algorithms;estimation;signal processing algorithms;adaptation models;algorithm design and analysis;real time systems;modulation	We consider the problem of sparse adaptive neuronal system identification, where the goal is to estimate the sparse time-varying neuronal model parameters in an online fashion from neural spiking observations. We develop two adaptive filters based on greedy estimation techniques and regularized log-likelihood maximization. We apply the proposed algorithms to simulated spiking data as well as experimentally recorded data from the ferret's primary auditory cortex during performance of auditory tasks. Our results reveal significant performance gains achieved by the proposed algorithms in terms of sparse identification and trackability, compared to existing algorithms.	adaptive filter;auditory area;convex optimization;expectation–maximization algorithm;experiment;ferrets;greedy algorithm;mathematical optimization;real-time clock;real-time computing;real-time transcription;recursion;sparse matrix;system identification	Alireza Sheikhattar;Behtash Babadi	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7591460	algorithm design;estimation;greedy algorithm;computer science;theoretical computer science;machine learning;pattern recognition;sparse approximation;statistics;modulation	DB	30.2459332029998	-33.117226459548206	198489
127e7ac22f361227eb1a9c6d56fd1b8f35b0a9f1	global parameter optimization for biokinetic modeling of simple batch experiments	nob;global optimality;nitrification;do;ode;wrmsr;qp;deterministic search;our;tnn;kinetic modeling;parameter estimation;wls;biotechnology;aob	Environmental process modeling is challenged by the lack of high quality data, stochastic variations, and nonlinear behavior. Conventionally, parameter optimization is based on stochastic sampling techniques to deal with the nonlinear behavior of the proposed models. Despite widespread use, such tools cannot guarantee globally optimal parameter estimates. It can be especially difficult in practice to differentiate between lack of algorithm convergence, convergence to a non-global local optimum, and model structure deficits. For this reason, we use a deterministic global optimization algorithm for kinetic model identification and demonstrate it with a model describing a typical batch experiment. A combination of interval arithmetic, reformulations, and relaxations allows globally optimal identification of all (six) model parameters. In addition, the results suggest that further improvements may be obtained by modification of the optimization problem or by proof of the hypothesized pseudo-convex nature of the problem suggested by our results. © 2016 Elsevier Ltd. All rights reserved.	algorithm;convex optimization;deterministic global optimization;display resolution;experiment;interval arithmetic;local optimum;mathematical optimization;maxima and minima;nonlinear system;optimization problem;process modeling;sampling (signal processing);system identification	Alma Masic;Kai M. Udert;Kris Villez	2016	Environmental Modelling and Software	10.1016/j.envsoft.2016.06.015	biology;econometrics;mathematical optimization;mathematics;estimation theory;ecology;ode;statistics;nitrification	AI	26.63175290059095	-30.54942093398776	198620
156bcc21e9287e578b565b367f554d0d9f08f1de	inductive framework for multi-aspect streaming tensor completion with side information.		Low rank tensor completion is a well studied problem and has applications in various fields. However, in many real world applications the data is dynamic, i.e., new data arrives at different time intervals. As a result, the tensors used to represent the data grow in size. Besides the tensors, in many real world scenarios, side information is also available in the form of matrices which also grow in size with time. The problem of predicting missing values in the dynamically growing tensor is called dynamic tensor completion. Most of the previous work in dynamic tensor completion make an assumption that the tensor grows only in one mode. To the best of our Knowledge, there is no previous work which incorporates side information with dynamic tensor completion. We bridge this gap in this paper by proposing a dynamic tensor completion framework called Side Information infused Incremental Tensor Analysis (SIITA), which incorporates side information and works for general incremental tensors. We also show how non-negative constraints can be incorporated with SIITA, which is essential for mining interpretable latent clusters. We carry out extensive experiments on multiple real world datasets to demonstrate the effectiveness of SIITA in various different settings.	algorithm;experiment;inductive reasoning;missing data	Madhav Nimishakavi;Bamdev Mishra;Manish Gupta;Partha Talukdar	2018		10.1145/3269206.3271713	tensor;data mining;theoretical computer science;missing data;cluster (physics);tucker decomposition;matrix (mathematics);computer science	ML	27.904152704679834	-35.948028530086404	198679
0e80849244270a232894786048032d0ccaf8a373	a robust regression method based on exponential-type kernel functions	regression models;width hyper parameter estimators;exponential type kernel functions;robust methods	Robust regression methods appear commonly in practical situations due the presence of outliers. In this paper we propose a robust regression method that penalize bad fitted observations (outliers) through the use of exponential-type kernel functions in the parameter estimator iterative process. Thus, the weights given to each observation are updated at each iteration in order to optimize a suitable objective function. The convergence of the parameter estimation algorithm is guaranteed with a low computational cost. Its performance is sensitive to the choice of the initial values for the vector of parameters of the regression model as well as to the width hyper-parameter estimator of the kernel functions. A simulation study with synthetic data sets revealed that some width hyper-parameter estimators can improve the performance of the proposed approach and that the ordinary least squares (OLS) method is a suitable choice for the initial values for the vector of parameters of the proposed regression method. A comparative study between the proposed method against some classical robust approaches (WLS, M-Estimator, MM-Estimator and L1 regression) and the OLS method is also considered. The performance of these methods are evaluated based on the bias and mean squared error (MSE) of the parameter estimates, considering synthetic data sets with X-space outliers, Y-space outliers and leverage points, different sample sizes and percentage of outliers in a Monte Carlo framework. The results suggest that the proposed approach presents a competitive performance (or best) in outliers scenarios that are comparable to those found in real problems. The proposed method also exhibits a similar performance to the OLS method when no outliers are considered and about half of the computational time if compared with MM-Estimator method. Applications to real data sets corroborates the usefulness of the proposed method.	time complexity	Francisco de A. T. de Carvalho;Eufrasio de Andrade Lima Neto;Marcelo R. P. Ferreira	2017	Neurocomputing	10.1016/j.neucom.2016.12.035	robust statistics;econometrics;mathematical optimization;local regression;least trimmed squares;computer science;machine learning;mathematics;robust regression;regression analysis;statistics	ML	27.814699678828596	-24.984426077195575	198764
435bc6d5d0ee510ff1a29389ac7d1169da176288	bilinear generalized approximate message passing—part i: derivation	matrix factorization;signal processing algorithms dictionaries manganese random variables robustness principal component analysis context;matrix factorization approximate message passing belief propagation bilinear estimation matrix completion dictionary learning robust principal components analysis;matrix completion;belief propagation;rank selection strategy bilinear generalized approximate message passing approach high dimensional generalized linear regression compressive sensing matrix completion robust pca dictionary learning related matrix factorization problems big amp algorithm sum product belief propagation algorithm high dimensional limit central limit theorem arguments taylor series approximations statistical independent matrix adaptive damping mechanism finite problem sizes expectation maximization based method em based method;approximate message passing;dictionary learning;robust principal components analysis;bilinear estimation;regression analysis approximation theory compressed sensing expectation maximisation algorithm matrix decomposition message passing principal component analysis	In this paper, we extend the generalized approximate message passing (G-AMP) approach, originally proposed for high-dimensional generalized-linear regression in the context of compressive sensing, to the generalized-bilinear case, which enables its application to matrix completion, robust PCA, dictionary learning, and related matrix-factorization problems. Here, in Part I of a two-part paper, we derive our Bilinear G-AMP (BiG-AMP) algorithm as an approximation of the sum-product belief propagation algorithm in the high-dimensional limit, where central-limit theorem arguments and Taylor-series approximations apply, and under the assumption of statistically independent matrix entries with known priors. In addition, we propose an adaptive damping mechanism that aids convergence under finite problem sizes, an expectation-maximization (EM)-based method to automatically tune the parameters of the assumed priors, and two rank-selection strategies. In Part II of the paper, we will discuss the specializations of EM-BiG-AMP to the problems of matrix completion, robust PCA, and dictionary learning, and we will present the results of an extensive empirical study comparing EM-BiG-AMP to state-of-the-art algorithms on each problem.	approximation algorithm;belief propagation;bilinear filtering;bilinear transform;c++ amp;compressed sensing;converge;dictionary;expectation–maximization algorithm;generalized linear model;machine learning;message passing;norm (social);numerical analysis;runtime system;software propagation;synthetic intelligence	Jason T. Parker;Philip Schniter;Volkan Cevher	2014	IEEE Transactions on Signal Processing	10.1109/TSP.2014.2357776	mathematical optimization;computer science;theoretical computer science;machine learning;mathematics;matrix decomposition;statistics;belief propagation	ML	28.461590293169547	-30.360208512750823	199142
1697f3ccd1861193d7a3c51f4affe761c6212776	hierarchical cosine similarity entropy for feature extraction of ship-radiated noise		The classification performance of passive sonar can be improved by extracting the features of ship-radiated noise. Traditional feature extraction methods neglect the nonlinear features in ship-radiated noise, such as entropy. The multiscale sample entropy (MSE) algorithm has been widely used for quantifying the entropy of a signal, but there are still some limitations. To remedy this, the hierarchical cosine similarity entropy (HCSE) is proposed in this paper. Firstly, the hierarchical decomposition is utilized to decompose a time series into some subsequences. Then, the sample entropy (SE) is modified by utilizing Shannon entropy rather than conditional entropy and employing angular distance instead of Chebyshev distance. Finally, the complexity of each subsequence is quantified by the modified SE. Simulation results show that the HCSE method overcomes some limitations in MSE. For example, undefined entropy is not likely to occur in HCSE, and it is more suitable for short time series. Compared with MSE, the experimental results illustrate that the classification accuracy of real ship-radiated noise is significantly improved from 75% to 95.63% by using HCSE. Consequently, the proposed HCSE can be applied in practical applications.	algorithm;angularjs;conditional entropy;cosine similarity;entropy (information theory);feature extraction;nonlinear system;sonar (symantec);sample entropy;shannon (unit);simulation;time series;undefined behavior	Zhe Chen;Ya'an Li;Hongtao Liang;Jing Yu	2018	Entropy	10.3390/e20060425	entropy (information theory);mathematical optimization;conditional entropy;feature extraction;mathematics;nonlinear system;cosine similarity;chebyshev distance;sample entropy	ML	37.693120490635884	-31.204152162040362	199171
81578213e895211e962565038e228cbbd941f4f9	a reversible jump sampler for autoregressive time series	bayesian framework;mcmc methods;signal sampling;bayes methods;time series;proposals sampling methods bayesian methods councils probability convergence laplace equations signal analysis;reversible jump;autoregressive processes;audio signals;audio time series reversible jump sampler autoregressive time series reversible jump markov chain monte carlo methods mcmc methods model order uncertainty ar time series bayesian framework model space moves ar parameters synthetic time series;reversible jump markov chain monte carlo;markov processes;parameter estimation;audio signals time series autoregressive processes bayes methods monte carlo methods markov processes parameter estimation signal sampling;monte carlo methods	We use reversible jump Markov chain Monte Carlo (MCMC) methods (Green 1995) to address the problem of model order uncertainty in au-toregressive (AR) time series within a Bayesian framework. EEcient model jumping is achieved by proposing model space moves from the full conditional density for the AR parameters, which is obtained analytically. This is compared with an alternative method, for which the moves are cheaper to compute, in which proposals are made only for the new parameters in each move. Results are presented for both synthetic and audio time series.	autoregressive model;monte carlo method;reversible-jump markov chain monte carlo;sampling (signal processing);synthetic intelligence;time series	Paul T. Troughton;Simon J. Godsill	1998		10.1109/ICASSP.1998.681598	econometrics;hybrid monte carlo;markov chain monte carlo;computer science;machine learning;audio signal;time series;star model;mathematics;markov process;markov chain mixing time;estimation theory;statistics;monte carlo method	ML	37.07612751100445	-25.98598890738105	199251
3cff741bf10c2dfb4fa9007371e40514dc9d5126	generalization ability of fractional polynomial models	fractional polynomial;model selection;learning algorithm;approximation theory;learning theory	In this paper, the problem of learning the functional dependency between input and output variables from scattered data using fractional polynomial models (FPM) is investigated. The estimation error bounds are obtained by calculating the pseudo-dimension of FPM, which is shown to be equal to that of sparse polynomial models (SPM). A linear decay of the approximation error is obtained for a class of target functions which are dense in the space of continuous functions. We derive a structural risk analogous to the Schwartz Criterion and demonstrate theoretically that the model minimizing this structural risk can achieve a favorable balance between estimation and approximation errors. An empirical model selection comparison is also performed to justify the usage of this structural risk in selecting the optimal complexity index from the data. We show that the construction of FPM can be efficiently addressed by the variable projection method. Furthermore, our empirical study implies that FPM could attain better generalization performance when compared with SPM and cubic splines.	approximation algorithm;approximation error;complexity index;entity name part qualifier - adopted;functional dependency;generalization (psychology);generalization error;input/output;knot (unit);least-squares analysis;mathematical model;model selection;non-linear least squares;overfitting;polynomial;projection method (fluid dynamics);pseudo brand of pseudoephedrine;signal-to-noise ratio;sparse matrix;spline (mathematics);super paper mario;system reference manual;emotional dependency	Yunwen Lei;Lixin Ding;Yiming Ding	2014	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2013.09.009	mathematical optimization;discrete mathematics;machine learning;learning theory;mathematics;model selection;statistics;approximation theory	ML	28.61101058036112	-33.62072527426306	199737
