id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
cce2ed6f5a592a1a8e0fcf02dfb172ec4ce1f53e	image-based recognition framework for robotic weed control systems	image-base system;weed recognition;weed detection;precision agriculture;weed control robotic systems	In this paper, we introduce a novel and efficient image-based weed recognition system for the weed control problem of Broad-leaved Dock (Rumex obtusifolius L.). Our proposed weed recognition system is developed using a framework, that allows the examination of the affects for various image resolutions in detection and recognition accuracy. Moreover, it includes state-of-the-art object/image categorization processes such as feature detection and extraction, codebook learning, feature encoding, image representation and classification. The efficiency of those processes have been improved and optimized by introducing methodologies, techniques and system parameters specially tailored for the goal of weed recognition. Through an exhaustive optimization process, which is presented as our experimental evaluation, we conclude to a weed recognition system that uses an image input resolution of 200 ×150, SURF features over dense feature extraction, an optimized Gaussian Mixture Model based codebook combined with Fisher encoding, using a two level image representation. The resulting image representation vectors are classified using a linear classifier. This system is experimentally shown to yield state-of-the-art recognition accuracy of 89.09% in the examined dataset. Our proposed system is also experimentally shown to comply with the specifications of the examined applications since it provides low false-positive results of 4.38%. As a result, the proposed framework can be efficiently used in weed control robots for precision farming applications.	categorization;codebook;control system;experiment;feature detection (computer vision);feature detection (web development);feature extraction;fisher–yates shuffle;linear classifier;mathematical optimization;mixture model;robot;speeded up robust features	Tsampikos Kounalakis;George A. Triantafyllidis;Lazaros Nalpantidis	2017	Multimedia Tools and Applications	10.1007/s11042-017-5337-y	robot;computer vision;computer science;mixture model;artificial intelligence;encoding (memory);feature extraction;codebook;pattern recognition;linear classifier;precision agriculture;image resolution	Robotics	32.629370031546514	-55.80513410059029	47344
0d6178974d20e7004f4c317550d3128a0a695d55	a novel method for ship detection and classification on remote sensing images		Ship detection and classification is critical for national maritime security and national defense. As massive optical remote sensing images of high resolution are available, ship detection and classification on optical remote sensing images is becoming a promising technique, and has attracted great attention on applications including maritime security and traffic control. Some image processing-based methods have been proposed to detect ships in optical remote sensing images, but most of them face difficulty in terms of accuracy, performance and complexity. Therefore, in this paper, we propose a novel ship detection and classification approach which utilizes deep convolutional neural network (CNN) as the ship classifier. Next, in order to overcome the divergence problem of deep CNN-based classifier, a residual network-based ship classifier is proposed. In order to deepen the network without excessive growth of network complexity, inception layers are used. In addition, batch normalization is used in each convolution layer to accelerate the convergence. The performance of our proposed ship detection and classification approach is evaluated on a set of ship images downloaded from Google Earth, each in 256 × 64 pixels at the resolution 0.5 m. Ninety-five percent classification accuracy is achieved. A CUDA-enabled residual network is implemented in model training which achieved 75× speedup on 1 Nvidia Titan X GPU.		Ying Liu;Hongyuan Cui;Guoqing Li	2017		10.1007/978-3-319-68612-7_63	image processing;pattern recognition;network complexity;pixel;convolutional neural network;residual;speedup;normalization (statistics);artificial intelligence;remote sensing;computer science;classifier (linguistics)	Robotics	28.168954602763858	-54.2387525193413	47401
572c66027ad1c6eff4fbd3cef3b63dd7db22abb4	image retrieval via isotropic and anisotropic mappings	euclidean group;texture;perceptual grouping;color histogram;gabor filter;texture analysis;nearest neighbor classifier;content based image retrieval;structure;image retrieval	This paper presents an approach for content-based image retrieval via isotropic and anisotropic mappings. Isotropic mappings are defined to be mappings invariant to the action of the planar Euclidean group – invariant to the translation, rotation and reflection of image data, and hence, invariant to orientation and position. Anisotropic mappings, on the other hand, are defined to be those mappings that are correspondingly variant. Structure extraction (via a perceptual grouping process) and color histogram are shown to be representations of isotropic mappings. Texture analysis using a channel energy model comprised of even-symmetric Gabor filters is considered to be a representation of anisotropic mapping. Results of retrieval of outdoor images by query and by classification using a nearest neighbor classifier are presented.	anisotropic diffusion;color histogram;content-based image retrieval;gabor filter;isotropic position;nearest neighbor search;nearest neighbour algorithm;texture mapping	Qasim Iqbal;Jake K. Aggarwal	2001		10.1016/S0031-3203(01)00246-1	color histogram;image texture;euclidean group;computer vision;structure;image retrieval;computer science;machine learning;pattern recognition;mathematics;geometry;texture	Vision	38.51385673535216	-58.995876794345	47552
8d2343e94412c10718e9912e47ca98d05985e354	object reading: text recognition for object recognition	universiteitsbibliotheek	We propose to use text recognition to aid in visual object class recognition. To this end we first propose a new algorithm for text detection in natural images. The proposed text detection is based on saliency cues and a context fusion step. The algorithm does not need any parameter tuning and can deal with varying imaging conditions. We evaluate three different tasks: 1. Scene text recognition, where we increase the state-of-the-art by 0.17 on the ICDAR 2003 dataset. 2. Saliency based object recognition, where we outperform other state-of-the-art saliency methods for object recognition on the PASCAL VOC 2011 dataset. 3. Object recognition with the aid of recognized text, where we are the first to report multi-modal results on the IMET set. Results show that text helps for object class recognition if the text is not uniquely coupled to individual object instances.	algorithm;end-to-end principle;high- and low-level;instance (computer science);international conference on document analysis and recognition;modal logic;optical character recognition;outline of object recognition;statistical classification	Sezer Karaoglu;Jan C. van Gemert;Theo Gevers	2012		10.1007/978-3-642-33885-4_46	computer vision;speech recognition;computer science;pattern recognition;3d single-object recognition	Vision	31.69582568654974	-53.125260448382	47756
4af4f192d652b6d679bd2a8e632bdd5731138c7e	arabic calligraphy classification using triangle model for digital jawi paleography analysis	feature extraction machine learning shape unified modeling language testing information science hybrid intelligent systems;triangle model;t technology general;features extraction;hybrid intelligent systems;information science;paleography;informing science;image classification;unsupervised machine learning arabic calligraphy classification triangle model digital jawi paleography analysis ancient manuscripts digital paleography grey level co occurrence matrix arabic writing malay language;testing;matrix algebra;hybrid intelligent system;supervised machine learning;qa75 electronic computers computer science;shape;arabic;machine learning;unsupervised machine learning;feature extraction;unified modeling language;matrix algebra character recognition document image processing image classification learning artificial intelligence;document image processing;jawi;learning artificial intelligence;grey level co occurrence matrix;character recognition;features extraction paleography calligraphy jawi arabic triangle model;calligraphy	Calligraphy classification of the ancient manuscripts gives useful information to paleographers. Researches on digital paleography using calligraphy are done on the manuscripts to identify unidentified place of origin, number of writers, and the date of ancient manuscripts. Information that are used are features from characters, tangent value and features known as Grey-Level Co-occurrence Matrix (GLCM). For Digital Jawi Paleography, a novel technique is proposed based on the triangle. This technique defines three important coordinates in the image of each character and translates it into triangle geometry form. The features are extracted from the triangle to represent the Jawi (Arabic writing in Malay language) characters. Experiments have been conducted using seven Unsupervised Machine Learning (UML) algorithms and one Supervised Machine Learning (SML). This stage focuses on the accuracy of Arabic calligraphy classification. Hence, the model and test data are Arabic calligraphy letters taken from calligraphy books. The number of model is 711 for the UML and 1019 for the SML. Twelve features are extracted from the formed triangles used.	algorithm;book;co-occurrence matrix;machine learning;test data;unified modeling language	Mohd Sanusi Azmi;Khairuddin Omar;Mohammad Faidzul Nasrudin;Azah Kamilah Muda;Azizi Abdullah	2011	2011 11th International Conference on Hybrid Intelligent Systems (HIS)	10.1109/HIS.2011.6122194	natural language processing;speech recognition;computer science;artificial intelligence	SE	33.07763261538383	-65.4155045240603	48055
caa7953cf5480fbdb464ff37f24f7cf9fddd27b0	image classification based on convolutional neural networks with cross-level strategy		In the past few years, convolutional neural networks (CNNs) have exhibited great potential in the field of image classification. In this paper, we present a novel strategy named cross-level to improve the existing networks’ architecture in which different levels of feature representation in a network are merely connected in series. The basic idea of cross-level is to establish a convolutional layer between two nonadjacent levels, aiming to extract more sufficient features with multiple scales at each feature representation level. The proposed cross-level strategy can be naturally integrated into an existing network without any change on its original architecture, which makes it very practical and convenient. Four popular convolutional networks for image classification are employed to illustrate its implementation in detail. Experimental results on the dataset adopted by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) verify the effectiveness of the cross-level strategy on image classification. Furthermore, a new convolutional network with cross-level architecture is presented to demonstrate the potential of the proposed strategy in future network design.	artificial neural network;computer vision;convolutional neural network;experiment;facial recognition system;imagenet;map;mental representation;network planning and design;object detection;series and parallel circuits	Yu Liu;Baocai Yin;Jun Yu;Zengfu Wang	2016	Multimedia Tools and Applications	10.1007/s11042-016-3540-x	computer vision;computer science;artificial intelligence;machine learning;data mining;convolutional neural network;algorithm	AI	24.774229406423625	-52.76221231163254	48208
2222f8089055459d8b8fd172215f138159483f47	extended two-dimensional pca for efficient face representation and recognition	databases;face;face recognition;covariance matrix;feature extraction;principal component analysis;accuracy	In this paper a novel method called Extended Two-Dimensional PCA (E2DPCA) is proposed which is an extension to the original 2DPCA. We state that the covariance matrix of 2DPCA is equivalent to the average of the main diagonal of the covariance matrix of PCA. This implies that 2DPCA eliminates some covariance information that can be useful for recognition. E2DPCA instead of just using the main diagonal considers a radius of r diagonals around it and expands the averaging so as to include the covariance information within those diagonals. The parameter r unifies PCA and 2DPCA. r = 1 produces the covariance of 2DPCA, r = n that of PCA. Hence, by controlling r it is possible to control the trade-offs between recognition accuracy and energy compression (fewer coefficients), and between training and recognition complexity. Experiments on ORL face database show improvement in both recognition accuracy and recognition time over the original 2DPCA.	algorithm;coefficient;principal component analysis;return loss	Mehran Safayani;Mohammad T. Manzuri Shalmani;Mahmoud Khademi	2008	2008 4th International Conference on Intelligent Computer Communication and Processing		facial recognition system;face;computer vision;covariance matrix;feature extraction;computer science;covariance;machine learning;pattern recognition;accuracy and precision;statistics;principal component analysis	Robotics	34.31108075731483	-58.69978818088224	48349
83591fcdc77e04311b87d2739ec8bd313523b131	class-specific image representation for image classification using multiple scale-invariant region detectors		We propose a new class-specific image representation for image classification using multiple region detectors. The new representation is designed to solve the problem of increasing variation in object location and size within images of a class, for which traditional spatial pyramid matching shows limited classification accuracy. We propose a new region-division method that divides the image region into two class-specific regions, called class-specific region-of-interest (C-ROI) and focal region (FR). Using multiple region detectors and appropriate mixing of their responses avoids the problem of selecting a region detector that gives the best classification accuracy for a given image class, and thereby yields better results than using only one region detector. Several scale-invariant region detectors are used to obtain C-ROI and FR by considering their importance over a given image class. In experiments using several well-known datasets, the proposed method improved the accuracy and achieved results that were better than or comparable to those achieved by the related methods.	computer vision;dvd region code;experiment;focal (programming language);harris affine region detector;hessian;region of interest;sensor;super paper mario	Hui-Jin Lee;Ki-Sang Hong	2016	Pattern Analysis and Applications	10.1007/s10044-016-0529-z	computer vision;mathematical optimization;pattern recognition;mathematics	Vision	33.02178982675727	-55.49916015254398	48601
057d9240489746ad35bb73c2d1c86f5f27a733a2	fusion of multibiometrics based on a new robust linear programming	authentication;iris recognition;security of data biometrics access control image fusion image recognition linear programming;feature extraction;linear programming;robustness;robustness iris recognition linear programming noise feature extraction face authentication;face;information security multibiometrics fusion robust linear programming method casia iris distance multibiometric database;noise	Multibiometrics provides a reliable method for identity authentication and has the potential to be widely applied. The success of a multibiometrics method depends critically on its ability to fuse complementary information supplied by different modalities, where the most challenging problem is to evaluate the importance of different modalities. In addition, identity authentication at a distance has become a development trend of multibiometrics. In this paper, we propose a new robust linear programming method to fuse multibiometrics by combining the modalities optimally. The proposed method can provide a reasonable trade off between conservatism and robustness. Experimental results on CASIA-Iris-Distance, a public and challenging multibiometric database, demonstrate the effectiveness and robustness of this method.	authentication;cluster analysis;image noise;k-means clustering;linear programming;robustness (computer science)	Di Miao;Zhenan Sun;Yongzhen Huang	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.59	face;computer vision;feature extraction;computer science;linear programming;noise;machine learning;pattern recognition;iris recognition;authentication;geometry;robustness	Vision	30.537166556815723	-61.61652407826168	48658
590ad3e37a89132c7b983af15c79c8a405a7ba11	shape recognition by combining contour and skeleton into a mid-level representation		Contour and skeleton are two main stream representations for shape recognition in the literature. It has been shown that such two representations convey complementary information, however combining them in a nature way is nontrivial, as they are generally abstracted by different structures (closed string vs graph), respectively. This paper aims at addressing the shape recognition problem by combining contour and skeleton into a mid-level of shape representation. To form a midlevel representation for shape contours, a recent work named Bag of Contour Fragments (BCF) is adopted; While for skeleton, a new midlevel representation named Bag of Skeleton Paths (BSP) is proposed, which is formed by pooling the skeleton codes by encoding the skeleton paths connecting pairs of end points in the skeleton. Finally, a compact shape feature vector is formed by concatenating BCF with BSP and fed into a linear SVM classifier to recognize the shape. Although such a concatenation is simple, the SVM classifier can automatically learn the weights of contour and skeleton features to offer discriminative power. The encouraging experimental results demonstrate that the proposed new shape representation is effective for shape classification and achieves the state-of-the-art performances on several standard shape benchmarks.	benchmark (computing);code;concatenation;contour line;discriminative model;feature vector;information;microsoft research;performance;shape context	Wei Shen;Xinggang Wang;Cong Yao;Xiang Bai	2014		10.1007/978-3-662-45646-0_40	active shape model;computer vision;machine learning;pattern recognition;mathematics;topological skeleton	AI	34.77702489034476	-53.941657064506245	48731
3727c909525e7a5169844bfa0512844a42a84d98	compositing-aware image search		We present a new image search technique that, given a background image, returns compatible foreground objects for image compositing tasks. The compatibility of a foreground object and a background scene depends on various aspects such as semantics, surrounding context, geometry, style and color. However, existing image search techniques measure the similarities on only a few aspects, and may return many results that are not suitable for compositing. Moreover, the importance of each factor may vary for different object categories and image content, making it difficult to manually define the matching criteria. In this paper, we propose to learn feature representations for foreground objects and background scenes respectively, where image content and object category information are jointly encoded during training. As a result, the learned features can adaptively encode the most important compatibility factors. We project the features to a common embedding space, so that the compatibility scores can be easily measured using the cosine similarity, enabling very efficient search. We collect an evaluation set consisting of eight object categories commonly used in compositing tasks, on which we demonstrate that our approach significantly outperforms other search techniques.	compositing;cosine similarity;encode;image retrieval	Hengshuang Zhao;Xiaohui Shen;Zhe L. Lin;Kalyan Sunkavalli;Brian L. Price;Jiaya Jia	2018		10.1007/978-3-030-01219-9_31	compositing;computer vision;artificial intelligence;semantics;computer science;cosine similarity;embedding	Vision	33.17079735057653	-52.51978258556411	48818
84686a266f0fcdc90744976d37b33f5f4c02620d	finding similar and discriminative parts of deformable shape classes	representation method;discriminative parts;deformable shape category;mpeg 7 data set;data compression;deformable shape template;image matching;mpeg 7 data set similar parts discriminative parts deformable shape classes representation method matching method deformable shape category learning strategy;computational geometry;shape recognition;shape transformation;shape measurement;transform coding;shape classification shape matching deformable shape template similar and discriminative parts;computer vision;video coding;similar parts;shape transform coding shape measurement computer vision educational institutions pattern matching;shape;deformable shape classes;learning strategy;shape matching;shape classification;image representation;pattern matching;learning artificial intelligence;video coding computational geometry data compression image matching image representation learning artificial intelligence shape recognition;matching method;similar and discriminative parts	This paper presents a novel representation and matching method for deformable shapes. The proposed approach finds most expressive segments of a deformable shape category called similar and discriminative parts, which is able to distinguish the learned shape class from other groups. And it leads a learning strategy together with a matching algorithm. Then we test our method with MPEG-7 data set.	algorithm;mpeg-7;shape context	Zhenxin Wang;Jihong OuYang;Zeyang Liu;Xueyan Li	2011	2011 Seventh International Conference on Natural Computation	10.1109/ICNC.2011.6022152	computer vision;machine learning;pattern recognition;mathematics	Robotics	35.22770228195416	-53.90515539914173	48820
0caceb92cb27627930f7f2e4f8742de1a6bf2c22	image retrieval using fuzzy color histogram and fuzzy string matching: a correlation-based scheme to reduce the semantic gap		The research interest in the recent years has progressed to improve the performance of image retrieval (IR) systems by reducing the semantic gap between the low-level features and the high-level concept. In this paper, we proposed an approach to combine the two modalities in IR systems, i.e., content and text, while considering the semantics between the query image and the textual query provided by the user. For content matching, color feature is extracted and is represented using fuzzy color histogram (FCH). For text matching, fuzzy string matching with edit distance is used. Furthermore, we find the correlation between the query image and the textual query provided by the user to reduce the semantic gap. Using this correlation, we combined the two modalities with late fusion approach. The proposed approach is assessed on standard annotated database. Higher values of precision and recall show better performance of the proposed approach. Moreover, the use of correlation helps in reducing the semantic gap and providing good results through better ranking of the similar images.	approximate string matching;color histogram;image retrieval	Nidhi Goel;Priti Sehgal	2013		10.1007/978-81-322-1665-0_31	computer vision;histogram matching;pattern recognition;mathematics;histogram equalization;information retrieval	Vision	38.66741882766248	-60.508493312156034	48827
6d04bc7ad4226f43933402c25684642edd48f1a0	a fuzzy logic approach for detection of video shot boundaries	content based video processing;shot cut detection;temporal video segmentation;video processing;video segmentation;fuzzy logic;qa75 electronic computers computer science;pattern recognition;shot boundary detection	Video temporal segmentation is normally the first and important step for content-based video applications. Many features including the pixel difference, colour histogram, motion, and edge information etc. have been widely used and reported in the literature to detect shot cuts inside videos. Although existing research on shot cut detection is active and extensive, it still remains a challenge to achieve accurate detection of all types of shot boundaries with one single algorithm. In this paper, we propose a fuzzy logic approach to integrate hybrid features for detecting shot boundaries inside general videos. The fuzzy logic approach contains two processing modes, where one is dedicated to detection of abrupt shot cuts including those short dissolved shots, and the other for detection of gradual shot cuts. These two modes are unified by a mode-selector to decide which mode the scheme should work on in order to achieve the best possible detection performances. By using the publicly available test data set from Carleton University, extensive experiments were carried out and the test results illustrate that the proposed algorithm outperforms the representative existing algorithms in terms of the precision and recall rates. 2006 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.		Hui Fang;Jianmin Jiang;Yue Feng	2006	Pattern Recognition	10.1016/j.patcog.2006.04.044	fuzzy logic;computer vision;computer science;theoretical computer science;machine learning;pattern recognition;video processing	Vision	39.18079074484442	-52.55805699997436	48886
dc0d767cb90435ebecb98642c006cd8039dfcbba	a generalized region labeling algorithm for image codingrestorationand segmentation	image coding;image segmentation;image processing;chain code;binary image;video text segmentation accuracy generalized region labeling algorithm image coding image restoration image segmentation region extraction computer systems image sequences fast region labeling algorithm binary images region segmentation gray scale images color images region boundary extraction chain code extraction run length coding computational efficiency space efficiency mpeg 1 video bitstreams;feature extraction image coding image sequences image restoration image segmentation;image restoration;region segmentation;face recognition;feature extraction;image sequence;labeling image coding computational efficiency image analysis image sequence analysis image sequences layout humans face recognition surveillance;computational efficiency;text segmentation;color image;image sequences	Region extraction and labeling is a fundamental process in building computer systems that automatically analyze and interpret images and image sequences of a scene. It comprises of extracting ‘‘meaningful” regions and their attributes from an image that are of interest in a given domain, e.g., a human face in face recognition and surveillance applications, a vehicle in intelligent highway monitoring, or an industrial tool in automated manufacturing and inspection applications. In this paper, we present a generalized and fast region labeling algorithm that can be applied to numerous image processing tasks: region extraction in binary images, region segmentation from gray scale or color images, region boundary extraction, chain code extraction, run-length coding of images, and image restoration from chain codes. The general applicability of our algorithm stems from its strengths in identifying the fundamental and common computational steps and providing robust and efficient procedures to accomplish these steps without requiring multiple scans of images. Our algorithm is shown to be more effective in terms of space and computational efficiency, in comparison with many existing approaches to region labeling and extraction. Results from our experiments with over 5,000 frames obtained from MPEG-1 video bitstreams demonstrate the good performance of our algorithm in terms of video text segmentation accuracy and computational efficiency.	algorithm;binary image;chain code;circuit restoration;computation;connected-component labeling;experiment;facial recognition system;fast fourier transform;grayscale;image processing;image restoration;mpeg-1;run-length encoding;text segmentation	Jae-Chang Shim;Chitra Dorai	1999		10.1109/ICIP.1999.821562	text segmentation;image restoration;computer vision;speech recognition;color image;binary image;image processing;feature extraction;computer science;pattern recognition;chain code;image segmentation;connected-component labeling	Vision	37.95484799078907	-65.3481555419032	48971
c872782fe8f341b0e96852591efdfcb7f6db2e1f	comparison of the impact of some minkowski metrics on vq/gmm based speaker recognition	cluster algorithm;k means;system performance;speaker recognition;system evaluation;gaussian mixture model;distance metric;k means algorithm;vector quantizer	This paper evaluates the impact of three special forms of the Minkowski metric (Euclidean, City Block, and Chebychev distances) on the performance of the conventional vector quantization (VQ) and Gaussian mixture model (GMM) based closed-set text-independent speaker recognition systems, in terms of recognition rate and confidence on decisions. For the VQ based system, evaluations are carried out using the two most common clustering algorithms, LBG and K-means, and it is revealed which clustering algorithm and distance pair should be used to exploit the best attribute of both to achieve the best recognition rate for a given codebook size. In the case of GMM based system, we introduce the metrics into the GMM using a concatenation of the LBG and K-means algorithms in estimating the initial mean vectors, to which the system performance is sensitive, and explore their impact on system performance. We also make comparison of results obtained from evaluations on clean speech (TIMIT) and telephone speech databases (NTIMIT and NIST2001) with the modern classifiers VQ-UBM and GMM-UBM. It is found that there are cases where conventional VQ based system outperforms the modern systems. Moreover, the impact of distance metrics on the performance of the conventional and modern systems depends on the recognition task imposed (verification/identification).	algorithm;cluster analysis;google map maker;k-means clustering;location-based game;minkowski addition;speaker recognition;vector quantization	Cemal Hanilçi;Figen Ertas	2011	Computers & Electrical Engineering	10.1016/j.compeleceng.2010.08.001	speaker recognition;speech recognition;computer science;machine learning;pattern recognition;mathematics;k-means clustering	AI	32.54429761650421	-64.47386530936507	49137
28425390f61d714361941712ace9782cc35bb6f7	learning feature fusion strategies for various image types to detect salient objects	saliency map;learning classifier systems;xcs;pattern recognition;object detection	Salient object detection is the task of automatically localizing objects of interests in a scene by suppressing the background information, which facilitates various machine vision applications such as object segmentation, recognition and tracking. Combining features from different feature-modalities has been demonstrated to enhance the performance of saliency prediction algorithms and different feature combinations are often suited to different types of images. However, existing saliency learning techniques attempt to apply a single feature combination across all image types and thus lose generalization in the test phase when considering unseen images. Learning classifier systems (LCSs) are an evolutionary machine learning technique that evolve a set of rules, based on a niched genetic reproduction, which collectively solve the problem. It is hypothesized that the LCS technique has the ability to autonomously learn different feature combinations for different image types. Hence, this paper further investigates the application of LCS for learning image dependent feature fusion strategies for the task of salient object detection. The obtained results show that the proposed method outperforms, through evolving generalized rules to compute saliency maps, the individual feature based methods and seven combinatorial techniques in detecting salient objects from three well known benchmark datasets of various types and difficulty levels. HighlightsIncorporated a novel input instance matching scheme in a learning classifier system.Effectively learned different feature combinations for various types of images.Outperformed nine individual feature based methods and seven combinatorial methods.The new method preserves more details of objects than the state-of-the-art methods.	feature vector	Muhammad Iqbal;Syed Saud Naqvi;Will N. Browne;Christopher Hollitt;Mengjie Zhang	2016	Pattern Recognition	10.1016/j.patcog.2016.05.020	feature learning;computer vision;computer science;machine learning;pattern recognition;learning classifier system;feature	Vision	27.385851882689185	-56.016444391181324	49245
8bc80a5d721757868d85d9e40c56c15d9b5c0df0	total-text: a comprehensive dataset for scene text detection and recognition		Text in curve orientation, despite being one of the common text orientations in real world environment, has close to zero existence in well received scene text datasets such as ICDAR'13 and MSRA-TD500. The main motivation of Total-Text is to fill this gap and facilitate a new research direction for the scene text community. On top of conventional horizontal and multi-oriented text, it features curved-oriented text. Total-Text is highly diversified in orientations, more than half of its images have a combination of more than two orientations. Recently, a new breed of solutions that casted text detection as a segmentation problem has demonstrated their effectiveness against multi-oriented text. In order to evaluate its robustness against curved text, we fine-tuned DeconvNet and benchmark it on Total-Text. Total-Text with its annotation is available at https://github.com/cs-chan/Total-Text-Dataset.	benchmark (computing)	Chee Kheng Chng;Chee Seng Chan	2017	2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2017.157	artificial intelligence;computer science;computer vision;pattern recognition;robustness (computer science);algorithm design;feature extraction;curve orientation;image segmentation;annotation	Vision	31.029998902322607	-52.68419102241274	49266
27c03caf4c366c8f4bb0d74807a98350d04cd695	pseudo-gabor wavelet for face recognition	databases;algorithms;facial recognition systems;wavelets	An efficient face-recognition algorithm is proposed, which not only possesses the advantages of linear subspace analysis approaches—such as low computational complexity—but also has the advantage of a high recognition performance with the wavelet- based algorithms. Based on the linearity of Gabor-wavelet transfor- mation and some basic assumptions on face images, we can extract pseudo-Gabor features from the face images without performing any complex Gabor-wavelet transformations. The computational com- plexity can therefore be reduced while a high recognition performance is still maintained by using the principal component analysis (PCA) method. The proposed algorithm is evaluated based on the Yale data- base, the Caltech database, the ORL database, the AR database, and the Facial Recognition Technology database, and is compared with several different face recognition methods such as PCA, Gabor wavelets plus PCA, kernel PCA, locality preserving projection, and dual-tree complex wavelet transformation plus PCA. Experiments show that consistent and promising results are obtained. © 2013 SPIE and IS&T (DOI: 10.1117/1.JEI.22.2.023029)	facial recognition system;gabor wavelet	Xudong Xie;Wentao Liu;Kin-Man Lam	2013	J. Electronic Imaging	10.1117/1.JEI.22.2.023029	wavelet;computer vision;computer science;machine learning;pattern recognition;algorithm;statistics	Vision	34.33641814558192	-58.942417524835776	49290
5234a4ee19cae626b1cb94be141a99a1260e31bb	improved pattern recognition with complex artificial immune system	artificial immune system;transformation recognition;antigen presentation;complex artificial immune system;data representation;complex data;major histocompatibility complex;pattern recognition;immune response	In this paper, we introduce the application of transformation pattern recognition based on a complex artificial immune system. The key feature of the complex artificial immune system is the introduction of complex data representation. We use complex numbers as the data representation instead of binary numbers used before, besides the weight between different layers. The complex partial autocorrelation coefficients of input antigen which are considered as the antigen presentation are calculated in major histocompatibility complex (MHC) layer of the complex artificial immune system. In the simulations, the transformation of patterns, such as translation, scale or rotation, are recognized in much higher accuracy, and it has obviously higher noise tolerance ability than traditional real artificial immune system and even the complex PARCOR model.	algorithm;artificial immune system;autocorrelation;binary number;coefficient;data (computing);model of hierarchical complexity;pattern recognition;simulation	Wei Wang;Shangce Gao;Zheng Tang	2009	Soft Comput.	10.1007/s00500-009-0418-0	antigen presentation;immune system;computer science;artificial intelligence;external data representation;artificial immune system;major histocompatibility complex;complex data type	AI	31.943170561479715	-60.60489859926303	49335
396b6fc2b895b5193fc2eaa0570739f87d55e79f	detecting text in natural scene images with conditional clustering and convolution neural network	neural networks;convolution	We present a robust method of detecting text in natural scenes. The work consists of four parts. First, automatically partition the images into different layers based on conditional clustering. The clustering operates in two sequential ways. One has a constrained clustering center and conditional determined cluster numbers, which generate small-size subregions. The other has fixed cluster numbers, which generate full-size subregions. After the clustering, we obtain a bunch of connected components (CCs) in each subregion. In the second step, the convolutional neural network (CNN) is used to classify those CCs to character components or noncharacter ones. The output score of the CNN can be transferred to the postprobability of characters. Then we group the candidate characters into text strings based on the probability and location. Finally, we use a verification step. We choose a multichannel strategy to evaluate the performance on the public datasets: ICDAR2011 and ICDAR2013. The experimental results demonstrate that our algorithm achieves a superior performance compared with the state-of-the-art text detection algorithms. © 2015 SPIE and IS&T [DOI: 10.1117/1.JEI.24.5.053019]	algorithm;artificial neural network;cluster analysis;connected component (graph theory);connected-component labeling;constrained clustering;convolution;convolutional neural network;optical character recognition;precision and recall;sensor	Anna Zhu;Guoyou Wang;Yangbo Dong;Brian Kenji Iwana	2015	J. Electronic Imaging	10.1117/1.JEI.24.5.053019	computer science;machine learning;pattern recognition;data mining;convolution;cluster analysis;artificial neural network;algorithm;statistics	ML	36.258244045205274	-64.4457832378741	49339
8886b21f97c114a23b24dc7025bbf42885adc3a7	privacy protection performance of de-identified face images with and without background	active appearance model;face recognition;shape;data privacy;principal component analysis;face;privacy	This paper presents an approach to blending a de-identified face region with its original background, for the purpose of completing the process of face de-identification. The re-identification risk of the de-identified FERET face images has been evaluated for the k-Diff-furthest face de-identification method, using several face recognition benchmark methods including PCA, LBP, HOG and LPQ. The experimental results show that the k-Diff-furthest face de-identification delivers high privacy protection within the face region while blending the de-identified face region with its original background may significantly increases the re-identification risk, indicating that de-identification must also be applied to image areas beyond the face region.	alpha compositing;belief propagation;benchmark (computing);de-identification;feret (facial recognition technology);facial recognition system;principal component analysis;privacy	Zongji Sun;Li Meng;Aladdin M. Ariyaeeinia;Xiaodong Duan;Zheng-Hua Tan	2016	2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.1109/MIPRO.2016.7522350	facial recognition system;face;computer vision;face detection;active appearance model;speech recognition;information privacy;shape;computer science;pattern recognition;three-dimensional face recognition;privacy;principal component analysis	Vision	33.28128224383641	-59.552214776651034	49425
9a81d74dfec71333d98edc93110bfbad89c46278	universal forgery features idea: a solution for user---adjusted threshold in signature verification	global classification;hidden signature;signature verification;global classifier	Handwritten signature verification algorithms are designed to distinguish between genuine signatures and forgeries. One of the central issues with such algorithms is the unavailability of skilled forgeries during the template creation. As a solution, we propose the idea of universalforgery features, where a global classifier is used to classify a signature as a genuine one or as a forgery, without the actual knowledge of the signature template and its owner. This classifier is trained once, during the system tuning on a group of historical data. A global classifier trained on a set of training signatures is not to be additionally trained after implementation; in other words, additional user enrollments have no effect on the global classifier parameters. This idea effectively solves the issue of the lack of skilled forgeries during template creation. We show that this approach can be applied both in on-line and off-line signature verification systems.		Joanna Putz-Leszczynska;Andrzej Pacut	2013	Trans. Computational Collective Intelligence	10.1007/978-3-642-36815-8_7	computer science;machine learning;pattern recognition;data mining	ECom	29.411331826515223	-64.50216776103741	49438
076b9151343536d278c49321422eac4adcb4d790	hybrid index-based image search from the web	automatic image annotation;k nearest neighbour search;hybrid search;single step retrieval;indexing;image search;images;image retrieval	Existing search techniques for retrieving images from the web store text-based and content-based features separately. They use structures like inverted-index, forward-index, document-term matrix, Tries, Prefix B-Tree, String B-Tree, etc. for text-based features and R-tree, SR-tree, K-B-D Tree, etc., for content-based features. We propose to use a hybrid indexing scheme which is more intuitive for hybrid image feature vectors and can be used to both store and query non-ordered discrete and continuous features simultaneously. Also, since most of the existing hybrid image search engines do not store two types of features together, they usually perform retrieval in two distinct steps, first finding results with only text-based information and later filtering results based on content-based information. In contrast, our approach of hybrid indexing supports retrieval in a single step. We introduce a k-nearest neighbour search algorithm for the hybrid indexing scheme used.	image retrieval	Rahul Gupta;Soumya K. Ghosh;Shamik Sural;Sakti Pramanik	2011	IJDMMM	10.1504/IJDMMM.2011.041809	search engine indexing;visual word;image retrieval;computer science;concept search;pattern recognition;data mining;automatic image annotation;information retrieval;search engine	Vision	38.75379986239642	-60.71709818836516	49541
892a0e4ae1e5af7a84cdbeb424ee7ee30a698530	video summarization with lstm and deep attention models		In this paper we propose two video summarization models based on the recently proposed vsLSTM and dppLSTM deep networks, which allow to model frame relevance and similarity. The proposed deep learning architectures additionally incorporate an attention mechanism to model user interest. In this paper the proposed models are compared to the original ones in terms of prediction accuracy and computational complexity. The proposed vsLSTM+Att method with an attention model outperforms the original methods when evaluated on common public datasets. Additionally, results obtained on a real video dataset containing terrorist-related content are provided to highlight the challenges faced in real-life applications. The proposed method yields outstanding results in this complex scenario, when compared to the original methods.		Luis Lebron Casas;Eugenia Koblents	2019		10.1007/978-3-030-05716-9_6	automatic summarization;pattern recognition;computer science;machine learning;deep learning;computational complexity theory;digital forensics;artificial intelligence	NLP	24.8870198698726	-54.71822861741838	49576
a77357ed102674c50d4194c6db09229f52623600	content-based image indexing by data clustering and inverse document frequency		In this paper we present an algorithm for creating and searching large image databases. Effective browsing and searching such collections of images based on their content is one of the most important challenges of computer science. In the presented algorithm, the process of inserting data to the database consists of several stages. In the first step interest points are generated from images by e.g. SIFT, SURF or PCA SIFT algorithms. The resulting huge number of key points is then reduced by data clustering, in our case by a novel, parameterless version of the mean shift algorithm. The reduction is achieved by subsequent operation on generated cluster centers. This algorithm has been adapted specifically for the presented method. Cluster centers are treated as terms and images as documents in the term frequency-inverse document frequency (TF-IDF) algorithm. TF-IDF algorithm allows to create an indexed image database and to fast retrieve desired images. The proposed approach is validated by numerical experiments on images with different content.		Rafal Grycuk;Marcin Gabryel;Marcin Korytkowski;Rafal Scherer	2014		10.1007/978-3-319-06932-6_36	computer science;theoretical computer science;data mining;information retrieval	ML	38.59962369837719	-60.51555542916808	49926
1f1dc04357aed7645825955b46223156aba41cbd	scale-adaptive texture classification	databases;standards;training;accuracy;laplace equations;estimation databases vectors training accuracy standards laplace equations;vectors;estimation;feature extraction methods scale adaptive texture classification scale invariant texture analysis image processing implicit scale selection k nearest neighbor classifier global scale estimation algorithm scale normalized laplacian gaussian extrema scale space scale variant features;laplace equations adaptive signal processing feature extraction gaussian processes image classification image texture	Scale invariant texture analysis is a fundamental challenge in image processing. As a consequence of the scale invariance, these kind of features are often characterized by a lower discriminative power. We observed, that scale invariant features did not pose a benefit in classification scenarios with varying scales in the training set. This is supposed to be an effect caused by an implicit scale selection done by the classification method. In this work, we analyze this effect based on the k-nearest neighbor classifier. Inspired by this effect, we employ global scale estimation algorithm utilizing scale-normalized Laplacian of Gaussian extrema in scale space, to improve the classification accuracies of scale variant features in a scenario with varying scales. We propose a general framework for scale-adaptive classification, which proved to improve the classification accuracies with a variety of feature extraction methods in such a scenario.	blob detection;feature extraction;image processing;k-nearest neighbors algorithm;nearest neighbor search;nearest neighbour algorithm;scale space;support vector machine;test set	Michael Gadermayr;Sebastian Hegenbart;Andreas Uhl	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.457	computer vision;estimation;scale space;machine learning;pattern recognition;mathematics;accuracy and precision;statistics;scale-space axioms	Vision	38.1873232226329	-59.248993721517465	50548
22db548ee558ce1faaca771bec0117c9107d0ca6	algorithms for recognizing contour-traced handprinted characters	character recognition classification algorithms contextual constraints contour analysis decision trees feature extraction handprinted characters limited data experiments machine learning suboptimum classification;machine learning algorithms;errors;classification algorithm;decision tree;probability;performance evaluation;decoding;suboptimum classification;handprinted characters;contextual constraints;testing;learning machines;training data;canada;machine learning;feature extraction;classification algorithms;limited data experiments;algorithms;classification tree analysis;bayes classifier;decision trees;character recognition;algorithm design and analysis;information theory;contour analysis	A contour-tracing technique originally devised by Clemens and Mason was modified and used with several different classifiers to recognize upper case handprinted characters. Recognition accuracies obtained compare favorably with other published results, particularly when additional simple tests are performed to differentiate commonly confused characters. One suboptimum classifier, in addition to yielding near optimum performance when tested on training data, uses much less statistical information than the optimum Bayes classifier and is significantly better than the optimum classifier when training and test data are limited.	algorithm;contour line;mason;test data	Godfried T. Toussaint;Robert W. Donaldson	1970	IEEE Transactions on Computers	10.1109/T-C.1970.222972	speech recognition;information theory;computer science;machine learning;decision tree;pattern recognition	Visualization	31.823889565337172	-65.50472838254939	50755
f182ece58b03608b762049ba56e06745263b38df	recognition of plant leaves using support vector machine	image recognition;texture features;support vector machine	A method using both color and texture feature to recognize plant leaf image is proposed in this paper. After image preprocessing, color feature and texture feature plant images are obtained, and then support vector machine (SVM) classifier is trained and used for plant images recognition. Experimental results show that using both color feature and texture feature to recognize plant image is possible, and the accuracy of recognition is fascinating.	color moments;preprocessor;support vector machine;wavelet transform	Qing-Kui Man;Qing Yan;Xiaofeng Wang;Feng-Yan Lin	2008		10.1007/978-3-540-85930-7_26	support vector machine;computer vision;feature;computer science;machine learning;pattern recognition;structured support vector machine	Vision	32.261684804332816	-60.404278554250524	50778
bbd05c30742a960fdf3b2326b55692d75e6dbf70	improving recognition through object sub-categorization	bayesian classifier;probabilistic latent semantic analysis	We propose a method to improve the recognition rate of Bayesian classifiers by splitting the training data and using separate classifier to learn each sub-category. We use probabilistic Latent Semantic Analysis (pLSA) to split the training set automatically into subcategories. This sub-categorization is based on the similarity of training images in terms of object’s appearance or background content. In some cases, clear separation does not exist in the training set, and splitting results in worse performance. We compute the average difference between posteriors from the pLSA model, and observing this parameter, we can decide whether splitting is useful or not. This approach has been tested on eight object categories. Experimental results validate the benefit of splitting the training set.	bayesian network;categorization;experiment;naive bayes classifier;performance;probabilistic latent semantic analysis;test set	Al Mansur;Yoshinori Kuno	2008		10.1007/978-3-540-89646-3_84	naive bayes classifier;computer science;machine learning;pattern recognition;data mining;probabilistic latent semantic analysis	Vision	30.785110995758487	-53.91524308077404	51070
529426a8a0775926af414af2babb8c52352c7b50	extended local binary patterns for face recognition	local descriptors;local binary pattern;face recognition;feature extraction	A set of six LBP-like features derived from local intensities and differences.A labeled dominant pattern scheme is proposed to learn salient information.Utilizing whitened PCA to produce more compact, robust and discriminative features.Fused WPCA features improves the accuracy and robustness of face recognition.Proposed face recognition system is highly robust to illumination variations. This paper presents a simple and novel, yet highly effective approach for robust face recognition. Using LBP-like descriptors based on local accumulated pixel differences - Angular Differences and Radial Differences, the local differences were decomposed into complementary components of signs and magnitudes. Based on these descriptors we developed labeled dominant patterns where the most frequently occurring patterns and their labels were learned to capture discriminative textural information. Six histogram features were obtained from each given face image by concatenating spatial histograms extracted from non-overlapping subregions. A whitened PCA technique was used for dimensionality reduction to produce more compact, robust and discriminative features, which were then fused using the nearest neighbor classifier, with Euclidean distance as the similarity measure.We evaluated the effectiveness of the proposed method on the Extended Yale B, the large-scale FERET, and CAS-PEAL-R1 databases, and found that that the proposed method impressively outperforms other well-known systems with a recognition rate of 74.6% on the CAS-PEAL-R1 lighting probe set.	facial recognition system;local binary patterns	Li Liu;Paul W. Fieguth;Guoying Zhao;Matti Pietikäinen;Dewen Hu	2016	Inf. Sci.	10.1016/j.ins.2016.04.021	facial recognition system;computer vision;local binary patterns;feature extraction;computer science;machine learning;pattern recognition;mathematics	Vision	35.77532109924722	-57.72602447205677	51208
ae47ed301254d9f8e019ee7ba3bef974c3966415	core-based fingerprint image classification	classification algorithm;core point candidate detection;image segmentation;image databases;information security;information retrieval;image matching;fingerprint recognition image matching image segmentation pixel information retrieval image databases data preprocessing data mining testing information security;core based fingerprint image classification;exact results;image classification;testing;data mining;fingerprint recognition;pixel;efficient enhancing method;classification accuracy;noise core based fingerprint image classification efficient enhancing method high quality directional image core point candidate detection;data preprocessing;image classification fingerprint identification;fingerprint identification;high quality directional image;noise	This paper presents a new jngeyrint class$cation algorithm that uses only the information related to core points. The algorithm uses an eficient enhancing method of fingerprint image for high-quality directional image. The algorithm detects core point candidates roughly fi7om the directional image and adjusts the location of each core candidate for more exact result. In core analysis, the near area of each core candidate is examined. False coye points made by noise are eliminated and the type and the orientation of core point are extracted for the classifkation step. Using this information, classijkation is performed. The algorithm was tested on 6283 images and classijkation accuracy of 92.3% for the four classes(arch, left-loop, right-loop, whorl) is achieved,	algorithm;fingerprint	Byoung-Ho Cho;Jeung-Seop Kim;Jae-Hyung Bae;In-Gu Bae;Kee-Young Yoo	2000		10.1109/ICPR.2000.906210	fingerprint;computer vision;contextual image classification;computer science;noise;information security;pattern recognition;data mining;software testing;data pre-processing;image segmentation;fingerprint recognition;pixel	Vision	34.962410840304464	-62.46456911636402	51313
94a14a0ec177a32d090dc08b28a2b7e723759784	enhanced gabor wavelet correlogram feature for image indexing and retrieval	enhanced gabor wavelet correlogram;content based image indexing and retrieval;wavelet correlogram	In this paper, a new feature scheme called enhanced Gabor wavelet correlogram (EGWC) is proposed for image indexing and retrieval. EGWC uses Gabor wavelets to decompose the image into different scales and orientations. The Gabor wavelet coefficients are then quantized using optimized quantization thresholds. In the next step, the autocorrelogram of the quantized wavelet coefficients is computed in each wavelet scale and orientation. Finally, the EGWC index vector simply consists of the autocorrelogram coefficients. Due to non-orthogonality of Gabor decomposition, the resulting wavelet coefficients suffer from redundancy, which increases the computational cost and reduces the effectiveness of EGWC. Here, we present a solution to handle the redundancy problem using non-maximum suppression and adjustment of autocorrelogram distance parameters as a function of the wavelet scale. The retrieval results obtained by applying EGWC to index two image databases with 5,000 natural images and 1,792 texture images demonstrated its better performance in terms of retrieval rates with respect to the state-of-the-art content-based and multidirectional texture indexing algorithms.	algorithmic efficiency;coefficient;computation;content-based image retrieval;contourlet;data redundancy;database;evolutionary algorithm;experiment;feature vector;filter bank;gabor filter;gabor wavelet;pyramid (image processing);selectivity (electronic);zero suppression	Hamid Abrishami Moghaddam;M. Nikzad Dehaji	2011	Pattern Analysis and Applications	10.1007/s10044-011-0230-1	wavelet;computer vision;speech recognition;pattern recognition;mathematics;wavelet packet decomposition;stationary wavelet transform;gabor wavelet;wavelet transform	Vision	37.93559824433009	-61.18942405600005	51355
55c0bd443b98a56c40c6510ff996236f520690c4	rotation invariant multiview face detection using skin color regressive model and support vector regression	support vector regression;regression model;skin color;rotation invariance;skin color model;principal component analysis;face detection	In this paper, an automatic rotation invariant multiview face detection method, which utilizes modified Skin Color Model (SCM), is presented. First, Gaussian Mixture Model (GMM) and Support Vector Machine (SVM) based hybrid models are used to classify human skin regions from color images. The novelty of the adaptive hybrid model is its ability to predict the chromatic skin color band for individual images based on calibration differences of camera and luminance condition of environment. Classified skin regions are then converted to gray scale image with a threshold based on the predicted chromatic skin color bands, which further enhances detection performance. Next, Principle Component Analysis (PCA) is applied to gray segmented regions. Face detection is carried out based on the PCA-based extracted features, along with selected features, using support vector regression. The output of this procedure is used to report the final result of face detection. The proposed method is also beneficial for the rotation invariant face recognition problem.	face detection;support vector machine	Padma Polash Paul;Md. Maruf Monwar;Marina L. Gavrilova;Patrick Shen-Pei Wang	2010	IJPRAI	10.1142/S0218001410008391	support vector machine;computer vision;face detection;speech recognition;computer science;machine learning;pattern recognition;mathematics;regression analysis;principal component analysis	Vision	33.09595746436753	-59.6760695053649	51497
ded5fe8c6638a6a2fe6a4e8a1e4298f1b0c8c60d	clustering using class specific hyper graphs	image clustering;clustering method;geometric constraints;invariant feature	The aim in this paper is to develop a method for clustering together image views of the same object class. Local invariant feature methods, such as SIFT, have been proven effective for image clustering. However, they have made either relatively little use or too complex use of geometric constraints and are confounded when the detected features are superabundant. Here we make two contributions aimed at overcoming these problems. First, we rank the SIFT points (R-SIFT) using visual saliency. Second, we use the reduced set of R-SIFT features to construct a specific hyper graph (CSHG) model of holistic-structure. Based on the CSHG model, a two stage clustering method is proposed. In which, images are clustered according to the pairwise similarity of the graphs, which is a combination of the traditional similarity of local invariant feature vectors and the geometric similarity between two graphs. This method comprehensively utilizes both SIFT and geometric constraints, and hence combines both global and local information. Experiments reveal that the method gives excellent clustering performance.		Shengping Xia;Edwin R. Hancock	2008		10.1007/978-3-540-89689-0_36	correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;machine learning;pattern recognition;data mining;mathematics;cluster analysis;single-linkage clustering	Vision	37.26542692698015	-57.05960947963919	51665
2625c951f4aff444ffac34df5a70ad981cb145fd	robust hypothesis generation method using binary blob analysis for multi-lane detection				Jingchun Piao;Hyunchul Shin	2017	IET Image Processing	10.1049/iet-ipr.2016.0506	artificial intelligence;computer vision;pattern recognition;mathematics;binary number	Robotics	35.86038424745922	-62.84920951470633	51675
e39db6a72f533b5adb2ecd9feaadbbab8204e024	temporal inception architecture for action recognition with convolutional neural networks		Modeling appearance and short-term dynamic information is the mainstream strategy for action recognition based on deep learning. We consider it important to model the multi-scale temporal information, including both short-term information and long-term information, for action representation. In this paper, a novel temporal inception architecture (TIA) is proposed to solve this problem, which is a general structure that can be combined with multi-segment-based frameworks for action recognition. The TIA is composed of multiple spatial-temporal convolutional branches, in which the temporal information of different scales is extracted. Then feature maps of all branches are concatenated as the output of TIA. In our experiments, the TIA is embedded into temporal segment networks (TSN) to construct our temporal segment inception networks (TSIN) for action recognition tasks. Extensive experiments demonstrate that TSIN outperforms TSN and achieves the state-of-the-art performance on HMDB51 and UCF101.	concatenation;convolutional neural network;deep learning;embedded system;embedding;epilepsy, temporal lobe;experiment;extraction;neural tube defects;numerous;television interface adaptor;time-sensitive networking	Wei Zhang;Jiepeng Cen;Huicheng Zheng	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8545720	convolutional neural network;architecture;feature extraction;deep learning;concatenation;pattern recognition;artificial intelligence;computer science	Vision	27.71083986196396	-52.50685899796181	51694
f36ce6b107b0cef1b76f18fa77351128201109da	analysis of morphology-based features for classification of crop and weeds in precision agriculture		Determining the types of vegetation present in an image is a core step in many precision agriculture tasks. In this letter, we focus on pixel-based approaches for classification of crops versus weeds, especially for complex cases involving overlapping plants and partial occlusion. We examine the benefits of multiscale and content-driven morphology-based descriptors called attribute profiles. These are compared to the state-of-the-art keypoint descriptors with a fixed neighborhood previously used in precision agriculture, namely histograms of oriented gradients and local binary patterns. The proposed classification technique is especially advantageous when coupled with morphology-based segmentation on a max-tree structure, as the same representation can be reused for feature extraction. The robustness of the approach is demonstrated by an experimental evaluation on two datasets with different crop types, while being able to provide descriptors at a higher resolution. The proposed approach compared favorably to the state-of-the-art approaches without an increase in computational complexity, while being able to provide descriptors at a higher resolution.	agricultural crops;avian crop;computational complexity theory;feature extraction;galaxy morphological classification;gradient;local binary patterns;mathematical morphology;pixel;plant weeds;tree structure;benefit	Petra Bosilj;Tom Duckett;Grzegorz Cielniak	2018	IEEE Robotics and Automation Letters	10.1109/LRA.2018.2848305	local binary patterns;morphology (linguistics);robustness (computer science);control engineering;pixel;feature extraction;engineering;precision agriculture;artificial intelligence;histogram;pattern recognition	Vision	32.742617280845174	-55.48996546268133	51833
7009bfe59777585cac9f731a7f5920f9775d9f76	classifying fruit fly early embryonic developmental stage based on embryo in situ hybridization images	principal component analysis drosophila in situ hybridization gabor filter support vector machine;biology computing;in situ hybridization images supervised classification system texture feature extraction gabor filter principal component analysis multiclass support vector machine model parameters drosophila embryonic developmental stage classification fruit fly early embryonic developmental stage system performance evaluation;model parameters;kernel;support vector machines biology computing feature extraction gabor filters image classification principal component analysis;texture feature extraction;support vector machines;drosophila embryonic developmental stage classification;image classification;in situ hybridization;gabor filters;data mining;gabor filter;drosophila;fruit fly early embryonic developmental stage;multiclass support vector machine;feature extraction;principal component analysis;embryo feature extraction data mining principal component analysis support vector machines support vector machine classification sorting image segmentation gabor filters system performance;supervised classification system;embryo;in situ hybridization images;support vector machine;system performance evaluation	In this paper, we present a supervised classification system for sorting Drosophila embryonic in situ hybridization (ISH) images according to their developmental stages. The proposed system first segments the embryo from an image and registers it for subsequent texture feature extraction. In order to extract the most distinguishing features for classifying developmental stages, we identify several areas of interest in an embryo with peculiar traits. Gabor filter is applied on these areas to extract texture features and Principal Component Analysis (PCA) is then performed on the extracted features to reduce dimensionality while retaining significant information. We adopt multi-class Support Vector Machine (SVM) as the classifier that learns model parameters from the training examples and classifies new examples with the trained model. We evaluate the system performance by comparing it to existing algorithms. The experimental results show that the proposed system achieves good performance in classifying Drosophila embryonic developmental stages and outperforms other state-of-the-art algorithms.	algorithm;feature extraction;gabor filter;gene co-expression network;image analysis;machine learning;normalization (image processing);preprocessor;principal component analysis;processor register;sorting;supervised learning;support vector machine	Hua Zhong;Wei-bang Chen;Chengcui Zhang	2009	2009 IEEE International Conference on Semantic Computing	10.1109/ICSC.2009.86	support vector machine;computer vision;computer science;machine learning;pattern recognition	Robotics	31.454826754893702	-61.082010971522124	52120
806cb30f18172819f699738c155cd17cf6f86df7	action recognition using fast hog3d of integral videos and smith-waterman partial matching			smith–waterman algorithm	Ibrahim El-Henawy;Kareem Ahmed;Hamdi Mahmoud	2018	IET Image Processing	10.1049/iet-ipr.2016.0627	smith–waterman algorithm;artificial intelligence;machine learning;mathematics;pattern recognition	Vision	29.290139882954517	-57.09078963502398	52170
8fa16ea091bdecad1d1019a545254c9dc7ecf749	facial expression recognition using thermal image processing and efficient preparation of training-data		Using our previously developed system, we investigated the influence of training data on the facial expression accuracy using the training data of “taro” for the intentional facial expressions of “angry,” “sad,” and “surprised,” and the training data of respective pronunciation for the intentional facial expressions of “happy” and “neutral.” Using the proposed method, the facial expressions were discriminable with average accuracy of 72.4% for “taro,” “koji” and “tsubasa”, for the three facial expressions of “happy,” “neutral,” and “other”.		Yu Nakanishi;Yasunari Yoshitomi;Taro Asada;Masayoshi Tabuse	2015	JRNAL	10.2991/jrnal.2015.2.2.3	image processing;training set;speech recognition;facial expression;pronunciation;computer science	Vision	31.48804496435106	-59.649407385446665	52210
8e9288e4619ee50631b68cf248eb56f4fc97a5e8	deep discovery of facial motions using a shallow embedding layer		Unique encoding of the dynamics of facial actions has potential to provide a spontaneous facial expression recognition system. The most promising existing approaches rely on deep learning of facial actions. However, current approaches are often computationally intensive and require a great deal of memory/processing time, and typically the temporal aspect of facial actions are often ignored, despite the potential wealth of information available from the spatial dynamic movements and their temporal evolution over time from neutral state to apex state. To tackle aforementioned challenges, we propose a deep learning framework by using the 3D convolutional filters to extract spatio-temporal features, followed by the LSTM network which is able to integrate the dynamic evolution of short-duration of spatio-temporal features as an emotion progresses from the neutral state to the apex state. In order to reduce the redundancy of parameters and accelerate the learning of the recurrent neural network, we propose a shallow embedding layer to reduce the number of parameters in the LSTM by up to 98% without sacrificing recognition accuracy. As the fully connected layer approximately contains 95% of the parameters in the network, we decrease the number of parameters in this layer before passing features to the LSTM network, which significantly improves training speed and enables the possibility of deploying a state of the art deep network on real-time applications. We evaluate our proposed framework on the DISFA and UNBC-McMaster Shoulder pain datasets.	apex (geometry);artificial neural network;deep learning;long short-term memory;real-time clock;recurrent neural network;spontaneous order	Afsaneh Ghasemi;Mahsa Baktash;Simon Denman;Sridha Sridharan;Dung Nguyen Tien;Clinton Fookes	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296545	pattern recognition;redundancy (engineering);artificial intelligence;feature extraction;computer science;apex (geometry);encoding (memory);deep learning;hidden markov model;facial expression;recurrent neural network	Vision	27.274220732735852	-52.183031074469525	52267
42a2b1e7ba92a8b5678ff2bee489a89009a2e913	person re-identification by multi-resolution saliency-weighted color histograms and local structural sparse coding	person reidentification competitive performance ethz viper combinational matching strategy weighting operators visual saliency spatial information partial information global chromatic content appearance based method disjoint cameras computer vision local structural sparse coding multiresolution saliency weighted color histograms;histograms;sparse coding person re identification multi resolution saliency weighted;image coding;image resolution;image color analysis histograms visualization encoding feature extraction probes image coding;saliency weighted;person re identification;会议论文;probes;computer vision;visualization;image color analysis;image colour analysis;feature extraction;image resolution combinatorial mathematics computer vision image colour analysis;multi resolution;encoding;sparse coding;combinatorial mathematics	Person re-identification plays an important role in computer vision, aiming to identify the same person viewed by disjoint cameras at different time instants and locations. In this paper we present a novel appearance-based method by multi-resolution saliency-weighted color histograms and local structural sparse coding for re-identification work. The former descriptor captures global chromatic content while the latter exploits both partial and spatial information of individuals. Specifically, visual saliency is considered as weighting operators to increase the discriminative power of features. Finally a combinational matching strategy is employed to measure the similarity between individuals. Experimental results over two challenging benchmark datasets (VIPeR, ETHZ) demonstrate that our method obtains competitive performance.	benchmark (computing);combinational logic;computer vision;feature extraction;map;neural coding;sparse matrix	Dandan Xu;Huicheng Zheng	2013	2013 Seventh International Conference on Image and Graphics	10.1109/ICIG.2013.100	computer vision;visualization;image resolution;feature extraction;computer science;machine learning;pattern recognition;histogram;mathematics;neural coding;encoding	Vision	37.00394517820143	-53.77458167820283	52424
911eb754d935382d594d5e3e978de35695ed5ea0	face recognition using alle and sift for human robot interaction	human robot interaction;face recognition;robot vision;scale invariant feature transform;feature extraction;local linear embedding;social robot	Face recognition is a very important aspect in developing human-robot interaction (HRI) for social robots. In this paper, an efficient face recognition algorithm is introduced for building intelligent robot vision system to recognize human faces. Dimension deduction algorithms locally linear embedding (LLE) and adaptive locally linear embedding (ALLE) and feature extraction algorithm scale-invariant feature transform (SIFT) are combined to form new methods called LLE-SIFT and ALLE-SIFT for finding compact and distinctive descriptors for face images. The new feature descriptors are demonstrated to have better performance in face recognition applications than standard SIFT descriptors, which shows that the proposed method is promising for developing robot vision system of face recognition.	facial recognition system;human–robot interaction;scale-invariant feature transform	Yaozhang Pan;Shuzhi Sam Ge;Hongsheng He	2009		10.1007/978-3-642-03983-6_9	computer vision;computer science;machine learning;pattern recognition;three-dimensional face recognition;3d single-object recognition	Robotics	32.354244533519335	-58.06340082344092	52520
c1827a87828e460854ac8a3541ac20f207c3d538	informative patches sampling for image classification by utilizing bottom-up and top-down information	bottom up;saliency;top down;image classification;patch sampling	In image classification based on bag of visual words framework, image patches used for creating image representations affect the classification performance significantly. However, currently, patches are sampled mainly based on processing low-level image information or just extracted regularly or randomly. These methods are not effective, because patches extracted through these approaches are not necessarily discriminative for image categorization. In this paper, we propose to utilize both bottom-up information through processing low-level image information and top-down information through exploring statistical properties of training image grids to extract image patches. In the proposed work, an input image is divided into regular grids, each of which is evaluated based on its bottom-up information and/or top-down information. Subsequently, every grid is assigned a saliency value based on its evaluation result, so that a saliency map can be created for the image. Finally, patch sampling from the input image is performed on the basis of the obtained saliency map. Furthermore, we propose a method to fuse these two kinds of information. The proposed methods are evaluated on both object categories and scene categories. Experiment results demonstrate their effectiveness.	bag-of-words model in computer vision;bottom-up parsing;categorization;feature selection;high- and low-level;information;randomness;sampling (signal processing);top-down and bottom-up design	Shuang Bai;Tetsuya Matsumoto;Yoshinori Takeuchi;Hiroaki Kudo;Noboru Ohnishi	2012	Machine Vision and Applications	10.1007/s00138-012-0473-x	image texture;computer vision;feature detection;binary image;computer science;pattern recognition;top-down and bottom-up design;data mining;mathematics	Vision	34.964155782445424	-55.96442791824302	52770
184b01eb92b3ccb4caa52ca4b9635573c03efc59	feature aggregation in iconic model evaluation		This paper presents recent work on iconic modelmatching. The idea of iconic feature evaluation is reviewed, and methods for setting adaptive noise thresholds for use in feature combination are described. Extensions to the adaptive thresholding technique are explained and illustrated, and the relevance of this technique to feature combination is discussed. Finally demonstrations of the performance of the system are shown, with particular reference to the discrimination ability of the method with multiple models.	feature model;relevance;selectivity (electronic);thresholding (image processing)	Kay Brisdon;Geoffrey D. Sullivan;Keith D. Baker	1988		10.5244/C.2.4	computer vision;pattern recognition	NLP	24.947877271957278	-65.7428501024581	52772
b57c6554cc12a6bd3ee88473c7f9f6642dbc3cdc	a heuristic approach to caption enhancement for effective video ocr	multimedia retrieval;video ocr;video retrieval;multimedia indexing;caption transition detection;caption enhancement	We present a heuristic approach to enhancing speech synchronized captions for video OCR, as a pre-process for subsequent tasks of multimedia indexing, segmentation and retrieval. We use a bi-search based caption transition detection method to improve efficiency, which adopts a simple heuristics that the same caption content usually lasts for a period for stable viewing. We propose a combination of color mask, changing mask and region mask to perform caption enhancement based on the discriminative characteristics of captions and backgrounds. Elaborate enhancement on individual characters is further used to remove small background residues. OCR experiments show that our caption enhancement approach brings a high character accuracy of 89.24%.	heuristic	Lei Xie;Xi Tan	2008		10.1007/978-3-540-87442-3_44	computer vision;speech recognition;computer science;multimedia	Vision	36.89088362729926	-64.55628952796943	52797
a3c210e73d827e8a7ea78c91d620af249164b4ae	face verification with a kernel fusion method	kernel;range data;support vector machines;fusion;face verification;data representation;face recognition grand challenge;gaussian kernel;gabor;support vector machine;mean curvature	In this paper, the fusion of information from several data representations in a face verification environment has been analyzed. We have considered three different face-based biometric representations of each subject in a subset of the Face Recognition Grand Challenge (FRGC) database: 2D texture images, 2.5D range data and mean curvature images. From these representations, linear and Gaussian kernel matrices have been defined. Fusion techniques have been applied to obtain a unique kernel from the individual kernels. The resulting kernel has been used to train Support Vector Machines (SVMs) for verification tasks. The proposed classifier outperforms the individual kernels results and the results of classical fusion techniques (feature-level and score-level methods) in different security level systems. 2010 Elsevier B.V. All rights reserved.	2.5d;biometrics;experiment;face recognition grand challenge;gaussian orbital;kernel (operating system);radial basis function;statistical classification;support vector machine	Isaac Martín de Diego;Ángel Serrano;Cristina Conde;Enrique Cabello	2010	Pattern Recognition Letters	10.1016/j.patrec.2009.12.030	support vector machine;computer vision;kernel method;string kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;mathematics;tree kernel;polynomial kernel	AI	33.88799952681253	-58.835087277165705	53343
278ed2baeeb1dc31dbccb9f96a6a288b8a88013b	multispectral palmprint recognition using score-level fusion	image resolution;image fusion;feature extraction databases veins image recognition transforms educational institutions robustness;score level fusion;cryptography;feature extraction;score level fusion palmprint recognition multispectral;transforms cryptography feature extraction image fusion image resolution palmprint recognition;transforms;palmprint recognition;multispectral;polyu multispectral palmprint recognition authentication system personal identification texture information personal recognition multiscale palmprint recognition multiresolution palmprint recognition palmprint feature extraction nsct nonsubsampled contour transform hash table palmprint image nir spectrum blue spectrum green spectrum red spectrum score level fusion method max operators sum operators roi image libraries	With the increasing demand of highly accurate and robust authentication system, palm print has widely been used in personal identification. Multispectral palm print has been proposed to get more distinguishing feature information and improve resistance to spoof. The vivid texture information of palm print presenting at different resolutions offers abundant prospects in personal recognition. So this paper describes a method to authenticate individuals based on multi-scale and multi-resolution palm print recognition. First, we extract palm print feature based on NSCT (nonsubsampled Contour transform), and store the feature using a hash table. Then the scores generated from each set of palm print image under red, green, blue and NIR spectrum, are combined using score level fusion method. This method use SUM and MAX operators. Comparatively low values of equal error rate and high recognition rate have been obtained for all fusion techniques. Multispectral palm print verification results on the ROI image libraries of PolyU demonstrate the effectiveness and accuracy of the proposed method.	authentication;contour line;fingerprint;hash table;library (computing);max;multispectral image;palm print;region of interest;word error rate	Yibin Yu;Yaofang Tang;Jinguo Cao;JunYing Gan	2013	2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing	10.1109/GreenCom-iThings-CPSCom.2013.255	computer vision;speech recognition;image resolution;feature extraction;computer science;cryptography;pattern recognition;image fusion	Robotics	34.03365127141334	-61.95260169250284	53457
098fd6f978cc310c051fc6885623ebc0d2a88d85	texture classification using wavelet-domain bdip and bvlc features	texture classification;standard deviation;texture features;wavelet transforms;feature vector;vectors;query image texture classification block difference of inverse probabilities block variation of local correlation coefficients wavelet domain target image texture feature vector bayesian distance;wavelet transforms image texture;minimum distance;three dimensional displays;abstracts;face;abstracts wavelet transforms entropy vectors radio access networks face three dimensional displays;entropy;radio access networks	In this paper, we propose a texture classification method using local texture features BDIP (block difference of inverse probabilities) and BVLC (block variation of local correlation coefficients) in wavelet domain. BDIP and BVLC are known to be good texture features which are bounded and well normalized to reduce the effect of illumination and catch the own properties of textures effectively. In the method, a target image is first decomposed into wavelet subbands. BDIPs and BVLCs are then computed in wavelet subbands. The means and standard deviations of subband BDIPs and BVLCs and the subband standard deviations are fused into a texture feature vector. Finally, the Bayesian distance between the feature vector of a query image and that of each class is stably measured and it is classified into the class of minimum distance. Experimental results for three test databases (DBs) show the proposed method yields excellent performances.	bayesian network;coefficient;database;experiment;feature vector;gaussian blur;normalization property (abstract rewriting);performance;turing test;wavelet	Hyun Joo So;Mi Hye Kim;Nam Chul Kim	2009	2009 17th European Signal Processing Conference		image texture;computer vision;machine learning;pattern recognition;mathematics;texture compression;texture filtering	Vision	37.34406559233018	-60.387559834697676	53675
896e17adf6eb73af44256203630f4936cceeeeac	"""guest editorial introduction to the special executable issue on """"mobile iris challenge evaluation part i (miche i)"""""""		h 0 Mobile biometrics technologies are nowadays the new frontier for ecure use of data and services. Mobile biometric identification deices extend the functionality and capabilities of a traditional biometic identification system, by allowing capture of biometric traits in any lace, and compare acquired samples against a biometric database, tored either locally on the device, or remotely in a central authenticaion system. Furthermore, captured information can also be compared ith that stored within RFID tags, or smartcards or recent machine eadable identification documents (IDs). Mobile biometric identification devices must be designed for inuitive operation, and incorporate all necessary equipment for the apture/processing of the biometric traits considered. The captured ata is converted by software into digital templates for storage and atching against other records, which are held in either a local or remote database. It is to consider that, notwithstanding the coninuous advances in technology and resources, the ability to transfer ll the phases of biometric processing on a mobile device calls for aster as well as lighter procedures, and for a more efficient storge strategy. It is also to consider that, due to its inherent use on ersonal devices, mobile biometric acquisition often produces samles of unpredictable quality. This is because users, though willing to e identified, are usually not experts of biometric technologies, and herefore they are generally not aware of the conditions to acquire ptimal samples for further processing. This calls for a sufficient roustness to distortions, misalignment and capture noise, which are ypical of uncontrolled conditions. Iris is a natural candidate for mobile biometric recognition for two ain reasons: iris acquisition is little intrusive, and iris codes are mong the less expensive templates from the storage point of view. owever, present techniques must be suitably adapted to the mobile etting. These include detection, segmentation and coding, as well as atching steps. This special issue collected some relevant contributions to the moile iris recognition field, and is the first Executable Special Issue of attern Recognition Letters. The goal pursued by this new editorial ormula is to improve the reproducibility of research results. This ould allow a reliable and incontrovertible comparison of methods hile relieving the authors from the hard and often necessarily cretive task of re-implementing often poorly detailed approaches. In act, authors of the accepted papers were required to embed the most elevant chunks of executable code, if not the complete one, and the sed test data, if not publicly available, into their executable papers. n this way, the readers of MICHE I will be able to execute that code ithin the framework of the research article.	advanced spaceborne thermal emission and reflection radiometer;biometric device;biometrics;code;distortion;executable;iris challenge evaluation;iris recognition;mobile device;radio-frequency identification;remote database access;sed;smart card;test data;uncontrolled format string	Maria De Marsico;Michele Nappi;Hugo Proença	2015	Pattern Recognition Letters	10.1016/j.patrec.2015.04.001	simulation;computer science	Mobile	25.592428714357638	-58.94389504909056	53770
47fcf25c3b734251baecc98cb0487691c051237c	shape classification using spectral graph wavelets	spectral graph wavelet;laplace-beltrami;bag-of-features;support vector machines;shape descriptors;classification	Spectral shape descriptors have been used extensively in a broad spectrum of geometry processing applications ranging from shape retrieval and segmentation to classification. In this paper, we propose a spectral graph wavelet approach for 3D shape classification using the bag-of-features paradigm. In an effort to capture both the local and global geometry of a 3D shape, we present a three-step feature description framework. First, local descriptors are extracted via the spectral graph wavelet transform having the Mexican hat wavelet as a generating kernel. Second, mid-level features are obtained by embedding local descriptors into the visual vocabulary space using the soft-assignment coding step of the bag-of-features model. Third, a global descriptor is constructed by aggregating mid-level features weighted by a geodesic exponential kernel, resulting in a matrix representation that describes the frequency of appearance of nearby codewords in the vocabulary. Experimental results on two standard 3D shape benchmarks demonstrate the effectiveness of the proposed classification approach in comparison with state-of-the-art methods.		Majid Masoumi;A. Ben Hamza	2017	Applied Intelligence	10.1007/s10489-017-0955-7	support vector machine;wavelet;wavelet transform;artificial intelligence;computer science;geodesic;machine learning;pattern recognition;spectral shape analysis;heat kernel signature;ranging;mexican hat wavelet	Vision	36.93679206701703	-58.04906718103347	53941
9a4aff9008a7458e158eff4e2e98bb908aab5d49	an efficient video retrieval scheme based on facial signatures	video databases;video surveillance;video retrieval;video surveillance social networking online video databases video retrieval;video dataset video retrieval scheme facial signatures low dimensional signature facial content video surveillance social network linear discriminant analysis maximum correntropy criterion;social networking online;linear discriminant analysis video retrieval signature	The topic of retrieving videos containing a desired person by just using facial content has many applications like video surveillance, social network, etc. In this paper, we propose a compact, discriminative and low-dimensional signature to describe an person with a set of high-dimensional features. The signature is generated by linear discriminant analysis with maximum correntropy criterion that is robust to outliers and noises. Based on the proposed signatures, a new video retrieval scheme is given for fast finding the desired videos by measuring the similarities between the signature of a query and the ones in the dataset. Evaluations on a large dataset of videos show that the proposed video retrieval scheme has the potential to substantially reduce the response time and slightly increase the mean average precision of retrieval.	antivirus software;closed-circuit television;facial recognition system;information retrieval;linear discriminant analysis;response time (technology);social network	Pengyi Hao;Sei-ichiro Kamata	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738556	computer vision;computer science;video quality;video tracking;pattern recognition;multimedia	Vision	34.703860255895904	-52.33395829358244	53948
a000726b8f264d11147d8445b4a2fc48f925a1b2	smart application for ams using face recognition			facial recognition system;vhdl-ams	K. MuthuKalyani;A. VeeraMuthu	2014	CoRR		facial recognition system;computer vision;artificial intelligence;computer science	Vision	29.859260235168986	-57.4418023189469	54540
2ad34c1291631d8f6ea2b2f4ce8c25cd4d10bd6a	on the recognition of the alphabet of the sign language through size functions	image recognition;sign language;alphabet recognition;testing;visual shape recognition;computer vision;feature vector;integer valued functions;handicapped aids;k nearest neighbor rule;shape;machine vision;handicapped aids shape image recognition visual system testing machine vision computer vision data visualization;data visualization;k nearest neighbor;value function;computer vision alphabet recognition sign language size functions integer valued functions visual shape recognition feature vector k nearest neighbor rule;visual system;size functions	Size functions are integer-valued functions which represent both qualitative and quantitative properties of visual shape. In this paper the use of size functions for the understanding of the alphabet of sign language is described. First, a family of size functions able to capture important aspects of shape from the apparent outline of the various signs are presented and motivated. Each sign is represented by means of a feature vector computed from the proposed family of size functions. Then, a training set of feature vectors is built from real images. Finally, the k-nearest-neighbor rule is employed for the classification of feature vectors computed from previously unseen signs. The reported experiments indicate that size functions can be extremely effective for the recognition of signs even in the presence of shape changes due to difference in hands, pose, style of signing, and viewpoint.		Claudio Uras;Alessandro Verri	1994		10.1109/ICPR.1994.576931	computer vision;speech recognition;feature vector;visual system;machine vision;sign language;shape;computer science;machine learning;pattern recognition;mathematics;software testing;bellman equation;k-nearest neighbors algorithm;data visualization	Theory	30.336686832529896	-59.77947381621716	54676
13c606f4a651514b2cbc2db80ada16e937314b7c	evaluation of lbp-based facial emotions recognition techniques to make consistent decisions	fdrlbp;contrast;edge;class variance;texture and emotions	Decision making is one of the smouldering problems in day to day works. Human emotions play crucial role in decision-making systems. While person is in high emotion he cannot make proper decision. Robust local binary pattern (RLBP) operator is more powerful to recognize the emotions and extends the features of local binary pattern (LBP). However, there are some precincts like discriminating bright faces against dark features and vice versa and intra-class variances increase. The RLBP solves this problem by ̄nding minimum of LBP codes and their complements. However, it miss the mark for di®erent local structures a similar feature is obtained, weak contrast local patterns and similar strong contrast local patterns. Hence, the discriminative robust local binary pattern (DRLBP) method is proposed to retain the contrast information of image patterns next to considering both edge and texture information. Nevertheless, LBP family methods are highly sensitive to noise. To trounce these drawbacks this paper extends fuzzy rule-based DRLBP which is more robust to noise, low contrasted, uneven lighting conditions, variations in expressions and rotation variant images.	belief propagation;binary pattern (image generation);code;fuzzy rule;image scaling;local binary patterns;logic programming	C. Nagaraju;D. Sharadamani;C. Maheswari;D. Vishnu Vardhan	2015	IJPRAI	10.1142/S021800141556008X	edge;computer vision;local binary patterns;contrast;machine learning;pattern recognition;mathematics;statistics	AI	33.74937463043638	-55.23715432082945	54727
70e46f2ffb729f962894295137de78f29942c52e	a multiple feature vector framework for forest species recognition	multiple feature vectors;classifier combination;forest species recognition;quadtree	In this work we focus on investigating the use of multiple feature vectors for forest species recognition. As consequence, we propose a framework to deal with the extraction of multiple feature vectors based on two approaches: image segmentation and multiple feature sets. Experiments conducted on a 112 species database containing microscopic images of wood demonstrate that with the proposed framework we can increase the recognition rates of the system from about 55.7% (with a single feature vector) to about 93.2%.	database;feature vector;image segmentation	Paulo Rodrigo Cavalin;Marcelo N. Kapp;Jefferson Martins;Luiz Eduardo Soares de Oliveira	2013		10.1145/2480362.2480368	feature vector;feature;feature extraction;computer science;machine learning;quadtree;pattern recognition;data mining;programming language;feature	AI	33.28131603499477	-57.14240898660908	54882
aa2c44f3f760d28a581fff26127a8db028c88fbe	modeling of affective state response to a virtual 3d face	least squares approximations;face brain modeling three dimensional displays nose predictive models mathematical model electroencephalography;virtual reality electroencephalography emotion recognition face recognition feature extraction least squares approximations psychology;dynamic model;virtual reality;emotion recognition;3d face;psychology;model validation;affective state;face recognition;feature extraction;prediction 3d face affective state dynamic model parameter estimation model validation;parameter estimation;electroencephalography;prediction;boredom state affective state response modeling virtual environment psychoemotional state affective state modeling virtual 3d face features eeg signals electroencephalogram signals emotiv epoc device feature modifications cross covariation functions auto covariation functions least square method model validation excitement state meditation state frustration state engagement state	Virtual environment is an important part of everyday life and it influences a psycho-emotional state of a user. This paper introduces affective state modeling using features of virtual 3D face. Observations of human affective state are done using preprocessed EEG (electroencephalogram) signals: excitement, meditation, frustration, engagement/boredom. The signals are measured using Emotiv EPOC device. Virtual 3D face with its feature modifications is used as a stimulus. A model that describes a relationship between stimulus and corresponding affective state signal is proposed. The unknown parameters of a model are estimated using auto- and cross-co variation functions and the method of least squares. Model validation showed that the model was able to predict the signals representing affective state of a human in a high accuracy.	epoc (operating system);electroencephalography;emotiv systems;least squares;mathematical model	Ausra Vidugiriene;Egidijus Vaskevicius;Vytautas Kaminskas	2013	2013 European Modelling Symposium	10.1109/EMS.2013.31	psychology;computer vision;communication;social psychology	Vision	25.536866163240887	-64.15389025862974	54991
67c4b1fbd48a133d683d88953d3c1479fbb37c68	3d local transform patterns: a new feature descriptor for image retrieval		In this paper, authors proposed a novel approach for image retrieval in transform domain using 3D local transform pattern (3D-LTraP). The various existing spatial domain techniques such as local binary pattern (LBP), Local ternary pattern (LTP), Local derivative pattern (LDP) and Local tetra pattern (LTrP) are encoding the spatial relationship between the neighbors with their center pixel in image plane. The first attempt has been made in 3D using spherical symmetric three dimensional local ternary pattern (SS-3D-LTP). But, the performance of SS-3D-LTP is depend on the proper selection of threshold value for ternary pattern calculation. Also, multiscale and color information are missing in SS-3D-LTP method. In the proposed method i.e. 3D-LTraP, the first problem is overcome by using binary approach Similarly, the other lacunas are avoided by using wavelet transform which provide directional as well as multiscale information and color features are embedded in feature generation process itself. Two different databases which included natural and biomedical database (Coral 10 K and OASIS databases) are used for experimental purpose. The experimental results demonstrate a noteworthy improvement in precision and recall as compared to SS-3D-LTP and recent methods.	image retrieval	Anil Balaji Gonde;Subrahmanyam Murala;Santosh Kumar Vipparthi;Rudraprakash Maheshwari;R. Balasubramanian	2016		10.1007/978-981-10-2107-7_45	feature detection;visual word;local binary patterns;gloh;pattern recognition;top-hat transform;feature	Vision	37.691338127989205	-59.614666970088194	55027
4f56ab65fb78c97c6c220be0cccd4d62a7127251	controlling stroke size in fast style transfer with recurrent convolutional neural network			convolutional neural network	Lingchen Yang;Lumin Yang;Mingbo Zhao;Youyi Zheng	2018	Comput. Graph. Forum	10.1111/cgf.13551	computer vision;image processing;convolutional neural network;artificial neural network;computer science;computing methodologies;stroke;artificial intelligence	ML	27.600348879545326	-57.81679910301913	55102
1671a60078cebd986a3b93a304ab9ee7d979d461	time series matching for biometric visual passwords		"""User authentication through silent utterance of a secret phrase, a biometric visual password, has been previously attempted mainly using image based features extracted from video. Using state of the art face tracking, this problem can be framed as a high dimensional time series matching problem covering the motion of a select set of lip points. One major advantage is the small amount of training data needed. We deploy the time and shape correspondence (TSC) matching algorithm given its superior performance when dealing with multidimensional signals with shape and in the presence of noise. We report the results of a user study with 22 participants uttering the password """"siggraph rocks"""". This data base along with other human action data bases we created for gait and gestures are made publicly available for comparison studies by other researchers."""	algorithm;authentication;biometrics;database;password;remote desktop services;siggraph;time series;usability testing	Kaustubha Mendhurwar;Sudhir P. Mudur;Tiberiu Popa	2017		10.1145/3102163.3102239	facial motion capture;computer vision;computer graphics (images);artificial intelligence;computer science;password;blossom algorithm;utterance;speech recognition;biometrics;phrase;authentication;gesture	Vision	30.854446333464864	-60.27486337755065	55521
d5d352d12e3bfb2434176e27120c325673275365	effect of controlled image acquisition on fingerprint matching	automatic control;skin elasticity controlled image acquisition fingerprint matching automatic fingerprint identification automatic fingerprint authentication system scaling translation rotation structural distortions fingerprint minutiae features;electrical capacitance tomography;structural distortions;image databases;bifurcation;image matching;fingerprint minutiae features;automatic fingerprint identification;fingerprint matching;controlled image acquisition;skin elasticity;system performance;scaling;image acquisition;fingerprint recognition feature extraction electrical capacitance tomography automatic control system performance fingers image databases spatial databases image matching bifurcation;fingerprint recognition;translation;feature extraction;spatial databases;fingers;automatic fingerprint authentication system;rotation;bifurcation fingerprint identification feature extraction image matching;fingerprint identification	In an automatic fingerprint identification or authentication system, the matcher subsystem handles the most complex task of compensating for scaling, translation, rotation and structural distortions of the fingerprint minutiae features due to skin elasticity. We analyze the effect of controlled image acquisition on matcher performance. We show that simple steps in image acquisition can enhance the system performance vastly.	fingerprint	Nalini K. Ratha;Ruud M. Bolle	1998		10.1109/ICPR.1998.712037	translation;fingerprint;computer vision;speech recognition;feature extraction;rotation;scaling;computer science;automatic control;pattern recognition;fingerprint recognition	Vision	32.670463814229024	-62.64847763881089	55673
d4f495eeadf86a7712eb553f360c5e530722eac8	writer identification for handwritten telugu documents using directional morphological features	databases;histograms;feature extraction forensics writing histograms skeleton databases character recognition;crime investigator;forensic scientist;image classification;k curvature;anthrax letter;directional morphological feature;directional morphological features;skeleton;nearest neighbor classifier writer identification handwritten telugu document directional morphological feature forensic scientist crime investigator anthrax letter writer recognition direction erosion directional opening directional closing k curvature feature feature extraction;writer recognition;writer identification;direction erosion;feature extraction;handwritten telugu document;document image processing;writing;k curvature feature;nearest neighbor classifier;character recognition;directional closing;telugu handwritten documents;handwritten character recognition;directional opening;forensics;image classification document image processing feature extraction handwritten character recognition;k curvature writer identification telugu handwritten documents directional morphological features	Linking a person based on handwritten documents is one of the oldest techniques that is used by crime investigators and forensic scientists. The importance of writer recognition in anthrax letter cases has made this examination popular in recent years. In this paper we propose four feature set namely directional opening, directional closing, direction erosion and k-curvature features for writer recognition on Telugu handwritten documents. Each of the features is extracted from the words after dividing them into a number of cells and then subjected to a nearest neighbor classifier for writer recognition. Although the results of each of the feature set is quite encouraging, the directional opening feature outperforms other feature sets.	closing (morphology);erosion (morphology);feature model;handwritten biometric recognition;nearest neighbour algorithm;opening (morphology)	Pulak Purkait;Rajesh Kumar;Bhabatosh Chanda	2010	2010 12th International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2010.108	computer vision;contextual image classification;speech recognition;feature extraction;computer science;machine learning;pattern recognition;histogram;forensic science;writing;skeleton	Vision	33.96136765782569	-64.26085357771179	55703
cab98869c4d92d1d666a1c84ecaf01f13cfa9e67	integrating multiple global and local features by product sparse coding for image retrieval			image retrieval;neural coding;sparse	Li Tian;Qi Jia;Sei-ichiro Kamata	2016	IEICE Transactions		feature detection;visual word;image retrieval;computer science;pattern recognition;sparse approximation;data mining;automatic image annotation;information retrieval	Vision	29.63196221258609	-56.9921179101226	55893
2295cc70269851f515d721dac9e1c676f77ad31c	a unified approach to artificial intelligence, pattern recognition, image processing and computer vision in fifth-generation computer systems	computer vision;artificial intelligent;pattern recognition		artificial intelligence;computer vision;fifth generation computer;image processing;pattern recognition	D. Dutta Majumder	1988	Inf. Sci.	10.1016/0020-0255(88)90013-8	computer vision;intelligent character recognition;computer science;artificial intelligence;machine learning;sketch recognition	AI	29.826522636204295	-57.558328413960915	55938
6a23144ce29346576a7a18226c3d00e9021fa114	cs591 report: application of siamesa network in 2d transformation		Deep learning has been extensively used various aspects of computer vision area. Deep learning separate itself from traditional neural network by having a much deeper and complicated network layers in its network structures. Traditionally, deep neural network is abundantly used in computer vision tasks including classification and detection and has achieve remarkable success and set up a new state of the art results in these fields. Instead of using neural network for vision recognition and detection. I will show the ability of neural network to do image registration, synthesis of images and image retrieval in this report. 1 Part One: Image Retrieval I work with Di for part of my time this semester to help him develop a neural network that can tell whether two images are identical or not. We further extend this network to one which after being well trained will help image retrieval task.	artificial neural network;computer vision;deep learning;image registration;image retrieval	Dorothy Chang	2017	CoRR		artificial neural network;deep learning;image retrieval;image registration;computer vision;artificial intelligence;computer science	Vision	27.29845563708368	-54.58099916635011	56184
e46db7fc5d9adf58b42ab65edfd0381ba77287f2	efficient image copy detection using multiscale fingerprints	digital forensics;histograms;image coding affine transforms copy protection feature extraction fingerprint identification;image recognition;copy detection;image coding;multimedia;multiscale sift descriptor;copy protection;binary codes;multi scale sift descriptor;histogram intersection multimedia copy detection fingerprints multiscale sift descriptor visual words;visualization;hamming distance;illegal image copy identification efficient image copy detection multiscale fingerprints multiscale scale invariant feature transform;scale invariant feature transform;feature extraction;affine transforms;histograms image recognition visualization binary codes feature extraction table lookup hamming distance digital forensics forensics;table lookup;fingerprints;histogram intersection;visual words;fingerprint identification;forensics;fngerprints	A multiscale scale-invariant feature transform (SIFT) descriptor can help improve our ability to discriminate between images when using copy detection to identify illegal image copies.	fingerprint;precision and recall;scalability;scale-invariant feature transform	Hefei Ling;Hongrui Cheng;Qingzhen Ma;Fuhao Zou;WeiQi Yan	2012	IEEE MultiMedia	10.1109/MMUL.2011.75	fingerprint;computer vision;computer science;theoretical computer science;digital forensics;pattern recognition;statistics	Vision	35.94306492037817	-61.213325394186604	56229
9df4258a81e88c283c5f99f4dc990cb859f42160	a parallel-serial recognition cone system for perception: some test results	scene description;pattern recognition;recognition cones;perception;parallel-serial systems;resolution pyramids	Abstract   This paper presents some results of tests of specific “recognition cone” systems for probabilistic parallel-serial recognition and description of two-dimensional scenes of objects. SIMULA and FORTRAN encoded systems were given particular sets of transforms, and examined for their ability to handle scenes that contain (1) letters, (2) “place-settings” of several pieces of silverware and china and (3) a “real-world” outdoor scene, in color. The same parallel-serial flow through structured layers of variable-resolution probabilistic transforms serves to detect edges, find features, characterize, recognize and describe, without any sharp dividing lines between different types of processes. Thus, a wide variety of diverse sources of information contextually interact, in a relatively simple and general way.		Leonard Uhr;Robert J. Douglass	1979	Pattern Recognition	10.1016/0031-3203(79)90026-8	computer vision;computer science;artificial intelligence;machine learning;algorithm	Vision	36.868121889040424	-53.526509523849235	56291
d71e6116a5fb0588f3939d32b588dc4c50246db4	unconditional steganalysis of jpeg and bmp images and its performance analysis using support vector machine	hidden information;feature vector;feature extraction;principal component analysis;performance analysis;support vector machine	A feature based steganalytic method used for detecting both transform and spatial domain embedding techniques was developed. We developed an unconditional steganalysis which will automatically classify an image as having hidden information or not using a powerful classifier Support Vector Machine which is independent of any embedding techniques. To select the most relevant features from the total 269 features extracted, they apply Principal Component Analysis. Experimental results showed that our steganalysis scheme blindly detect the images obtained from six steganographic algorithmsF5, Outguess, S-Tool, JP Hide & Seek, LSB flipping and PVD. This method is able to detect any new algorithms which are not used during the training step, even if the embedding rate is very low. We also analyzed embedding rate versus detectability performances.	algorithm;bmp file format;jpeg;least significant bit;performance;physical vapor deposition;principal component analysis;profiling (computer programming);sensor;statistical classification;steganalysis;steganography;support vector machine	P. P. Amritha;Anoj Madathil;T. Gireesh Kumar	2010		10.1007/978-3-642-15766-0_111	support vector machine;kernel method;feature vector;feature extraction;computer science;machine learning;pattern recognition;data mining;relevance vector machine;structured support vector machine;principal component analysis	AI	35.72105681721787	-61.327582565324704	56336
1dcccd889562daec06fee30829c61a4b5a386cc2	face occlusion detection based on multi-task convolution neural network	face feature extraction mouth convolution nose yttrium neural networks;large scale face occlusion database face occlusion detection multitask convolution neural network atm security reinforcement surveillance techniques facial images face localization face segmentation feature extraction feature recognition facial occlusion detection convolutional neural networks multitask learning;component;neural nets automatic teller machines criminal law face recognition image segmentation learning artificial intelligence;multi task learning component face occlusion detection atm deep learning convolusional neurol network;deep learning;multi task learning;face occlusion detection;convolusional neurol network;atm	With the rise of crimes associated with ATM, security reinforcement by surveillance techniques has been in high agenda for both academia and industries. Though cameras are generally installed in ATMs to capture the facial images of users, the function is only limited to recording for follow-up criminal investigations, which could become useless when a criminal's face is occluded. Therefore, face occlusion detection has become very important to prevent crimes connected with ATMs. Traditional approaches to solve the problem typically consist of a succession of steps such as localization, segmentation, feature extraction and recognition. This paper proposes robust and effective facial occlusion detection based on convolutional neural networks (ConvNets) with multi-task learning. Covering of different facial parts, namely, left eye, right eye, nose and mouth, can be predicted by the multi-task CNN. In comparison with previous approaches, CNN is optimal from the system point of view as the design is based on end-to-end principle and the model operates directly on the image pixels. We created a large scale face occlusion database, consisting of over fifty thousand images, with annotated facial parts. Experimental results revealed that the proposed method is extremely effective.	atm turbo;ar (unix);artificial neural network;computer multitasking;convolution;convolutional neural network;end-to-end principle;facial recognition system;feature extraction;image segmentation;internationalization and localization;multi-task learning;pixel;succession;supervised learning	Yizhang Xia;Bailing Zhang;Frans Coenen	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7381971	multi-task learning;computer vision;face detection;speech recognition;object-class detection;computer science;artificial intelligence;machine learning;data mining;three-dimensional face recognition;component;deep learning;atmosphere	Vision	29.803251767571393	-53.241518858737535	56431
ef4efd6a5fc244b67926b819d769305d17631464	an emotion-oriented image search system with cluster based similarity measurement using pillar-kmeans algorithm	cluster based similarity;emotional context;multi query images;image search;subspace feature selection	This paper presents an image search system with an emotion-oriented context recognition mechanism. Our motivation implementing an emotional context is to express user's impressions for retrieval process in the image search system. This emotional context recognizes the most important features by connecting the user's impressions to the image queries. The Mathematical Model of Meaning (MMM: [2], [4] and [5]) is applied for recognizing a series of emotional contexts for retrieving the most highly correlated impressions to the context. These impressions are then projected to a color impression metric to obtain the most significant colors for subspace feature selection. After applying subspace feature selection, the system then clusters the subspace color features of the image dataset using our proposed Pillar-Kmeans algorithm.#R##N##R##N#Pillar algorithm is an algorithm to optimize the initial centroids for K-means clustering. This algorithm is very robust and superior for initial centroids optimization for K-means by positioning all centroids far separately among them in the data distribution. It is inspiring that by distributing the pillars as far as possible from each other within the pressure distribution of a roof, the pillars can withstand the roof's pressure and stabilize a house or building. It considers the pillars which should be located as far as possible from each other to withstand against the pressure distribution of a roof, as number of centroids among the gravity weight of data distribution in the vector space. Therefore, this algorithm designates positions of initial centroids in the farthest accumulated distance between them in the data distribution.#R##N##R##N#The cluster based similarity measurement also involves a semantic filtering mechanism. This mechanism filters out the unimportant image data items to the context in order to speed up the computational execution for image search process. The system then clusters the image dataset using our Pillar-Kmeans algorithm. The centroids of clustering results are used for calculating the similarity measurements to the image query. We perform our proposed system for experimental purpose with the Ukiyo-e image dataset from Tokyo Metropolitan Library for representing the Japanese cultural image collections.	algorithm;k-means clustering	Ali Ridho Barakbah;Yasushi Kiyoki	2010		10.3233/978-1-60750-690-4-117	machine learning;pattern recognition;data mining;mathematics	Robotics	28.72585252530016	-60.25908715992216	56551
30154cb2559a4a7836d0e1d89ff0bd86ce764537	a quick and coarse color image segmentation	cluster algorithm;pattern clustering;image segmentation;image classification;low computational complexity coarse color image segmentation color classification robust agglomerating clustering algorithm fuzzy partition;computational complexity;cluster validity;color image segmentation clustering algorithms shape iterative algorithms robustness merging intersymbol interference signal processing image processing;color image segmentation;computational complexity image segmentation image classification pattern clustering	In this paper we focus on the problem of image segmen-tation by color classification. We present a robust agglomerating clustering algorithm based on a cluster validity criteria derived from fuzzy partitions. The result is a simplified segmentation having a small number of large regions. The interest of the proposed method is that it requires a single parameter and that the computational complexity is very low. Segmentation is a very important task which is difficult because it both depends on the image type and on the aim of the analysis. Many techniques are available now [1], [2],[3],[4],[5],[6]. Clustering is a common step to get a seg-mentation. In the data space, clusters are regarded as regions of high density which are separated by regions of low density. Most popular clustering algorithms are suffering of two major drawbacks. First, the number of clusters is pre-defined, which makes them inadequate for batch processing of huge image databases. Secondly, the clusters are represented by their centroid and built using an Euclidean distance therefore inducing generally an hyperspheric cluster shape, which makes them unable to capture the real structure of the data. This is especially true in the case of color clustering where clusters are arbitrarily shaped. The algorithm we propose belongs to the class of hierarchical agglomerative algorithms and is based on fuzzy estimators for overlapping and compactness. Basically, it consists in starting with an initial clustering and iteratively merging clusters in larger ones at each step. The key idea is that merging is performed pair-wise only if the overlapping between two clusters is comparable with their compactness. Using a small initial number of clusters, the result is a simplified segmentation having a small number of large regions. The initial clusters are computed using the Fuzzy C-Mean (FCM) algorithm [7], [8], well-known for its performances in terms of robustness to outliers and speed, but any other clustering technique can be used in this step as long as it produces a sufficiently large number of clusters with respect to the actual number. Let {x 1 , x 2 , …x N } be the set of vectors to be clustered, {C 1 , C 2 , …, C K } the set of K color classes and {ν 1 , …, ν K } the centers of the different classes, where K denotes the number of classes. The basic idea of the FCM clustering is to …	algorithm;batch processing;cluster analysis;color image;computational complexity theory;database;dataspaces;energy (psychological);euclidean distance;fuzzy cognitive map;image segmentation;kinetic data structure;performance;segmentation fault;statistical classification	Patrick Lambert;Horia Grecu	2003		10.1109/ICIP.2003.1247125	image texture;computer vision;contextual image classification;binary image;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;computational complexity theory	ML	31.68768237869875	-57.33881772828798	56561
6cd09fbe2719426645eadcc2389a14c692180758	a new type of art2 architecture and application to color image segmentation	color space;unsupervised segmentation;art2;feature space;similarity measurement;number of clusters;self organization;similarity measure;spatial information;adaptive resonance theory;color image;neural network;color image segmentation	A new neural network architecture based on adaptive resonance theory (ART) is proposed and applied to color image segmentation. A new mechanism of similarity measurement between patterns has been introduced to make sure that spatial information in feature space, including both magnitude and phase of input vector, has been taken into consideration. By these improvements, the new ART2 architecture is characterized by the advantages: (i) keeping the traits of classical ART2 network such as self-organizing learning, categorizing without need of the number of clusters, etc.; (ii) developing better performance in grouping clustering patterns; (iii) improving pattern-shifting problem of classical ART2. The new architecture is believed to achieve effective unsupervised segmentation of color image and it has been experimentally found to perform well in a modified L∗u∗v∗ color space in which the perceptual color difference can be measured properly by spatial information.	adaptive resonance theory;algorithm;artificial neural network;categorization;cluster analysis;color image;color space;experiment;image segmentation;network architecture;norm (social);organizing (structure);preprocessor;self-organization;unsupervised learning;xslt/muenchian grouping	Jiaoyan Ai;Brian V. Funt;Lilong Shi	2008		10.1007/978-3-540-87536-9_10	color histogram;computer vision;self-organization;color quantization;feature vector;color image;computer science;adaptive resonance theory;machine learning;pattern recognition;mathematics;spatial analysis;image segmentation;color space;scale-space segmentation;artificial neural network	ML	36.900575417987895	-59.562788340717404	56582
8a457ea0a4a485a7a0fb1051333c0acaca8a473d	high-level feature extraction experiments for trecvid 2007	feature extraction	1. Briefly, what approach or combination of approaches did you test in each of your submitted runs? A_KL1_1: A color-based image retrieval method using three kinds of image features: a global color distribution feature, a common bitmap feature and a Wavelet texture feature. Key-frames generated by our frame clustering method with threshold 5 were used as the input of the feature extraction system. A_KL2_2: A color-based image retrieval method in the same way as A_KL1_1, where key-frames generated with threshold 20 were used as the input. A_KL3_3: SVMs based on three visual features: a modified MPEG-7-based edge histogram descriptor, a color layout descriptor and an auto-correlogram, where key-frames generated with threshold 5 were used as the input data. A_KL4_4: SVMs as for A_KL3_3 and nine kinds of Haar-like feature-based extractors were used. A_KL5_5: In addition to A_KL4_4, a Haar-Like feature-based face extractor was applied to extract human related features. A_KL6_6: In the same way as A_KL5_5, but the HaarLike feature-based extractor with lower recall and higher precision was used.	algorithm;bitmap;cluster analysis;color layout descriptor;columbia (supercomputer);data transfer object;feature extraction;feature vector;haar wavelet;image retrieval;lexicon;randomness extractor;semiconductor industry;test set	Masaki Naito;Keiichiro Hoashi;Kazunori Matsumoto;Masami Shishibori;Kenji Kita;Andrea Kutics;Akihiko Nakagawa;Fumiaki Sugaya;Yasuyuki Nakajima	2007			feature extraction;artificial intelligence;trecvid;computer science;pattern recognition	Vision	35.18024883462252	-64.51686878261827	56598
bc22ab264cdb5da768c09d4b64fd0ec1b3773103	hand vein recognition based on oriented gradient maps and local feature matching	novel approach;local feature matching;hand vein recognition;hand vein image;hand vein pattern;oriented gradient map;segmented vein region;oriented gradient maps;vein pattern;ncut database;nir hand vein image	The hand vein pattern as a biometric trait for identification has attracted increasing interests in recent years thanks to its properties of uniqueness, permanence, non-invasiveness as well as strong immunity against forgery. In this paper, we propose a novel approach for back of the hand vein recognition. It first makes use of Oriented Gradient Maps (OGMs) to represent the Near-Infrared (NIR) hand vein images, simultaneously highlighting the distinctiveness of vein patterns and texture of their surrounding corium, in contrast to the state-of-the-art studies that only focused on the segmented vein region. SIFT based local matching is then performed to associate the keypoints between corresponding OGM pairs of the same subject. The proposed approach was benchmarked on the NCUT database consisting of 2040 NIR hand vein images from 102 subjects. The experimental results clearly demonstrate the effectiveness of our approach.	benchmark (computing);biometrics;database;gradient;scale-invariant feature transform	Di Huang;Yinhang Tang;Yiding Wang;Liming Chen;Yunhong Wang	2012		10.1007/978-3-642-37447-0_33	computer vision	Vision	37.17783635235903	-57.76901317080829	56615
e46792a2eeac1391c6ec1dd8abc90dafa3ae3f85	automatic pixelwise object labeling for aerial imagery using stacked u-nets		Automation of objects labeling in aerial imagery is a computer vision task with numerous practical applications. Fields like energy exploration require an automated method to process a continuous stream of imagery on a daily basis. In this paper we propose a pipeline to tackle this problem using a stack of convolutional neural networks (U-Net architecture) arranged end-to-end. Each network works as post-processor to the previous one. Our model outperforms current state-of-the-art on two different datasets: Inria Aerial Image Labeling dataset and Massachusetts Buildings dataset each with different characteristics such as spatial resolution, object shapes and scales. Moreover, we experimentally validate computation time savings by processing sub-sampled images and later upsampling pixelwise labeling. These savings come at a negligible degradation in segmentation quality. Though the conducted experiments in this paper cover only aerial imagery, the technique presented is general and can handle other types of images. Keywords-Remote Sensing; Semantic Segmentation; U-Net; Object Labeling; Deep Convolutional Neural Networks	aerial photography;artificial neural network;computation;computer vision;convolutional neural network;discriminator;elegant degradation;end-to-end principle;experiment;generative adversarial networks;neural networks;semantic role labeling;time complexity;upsampling	Andrew Khalel;Motaz El-Saban	2018	CoRR		automation;artificial intelligence;convolutional neural network;upsampling;computation;aerial image;pattern recognition;architecture;computer science;image resolution	Vision	27.238162315595474	-53.5034122770406	56780
39637ea87b5133f57b64ddcc4282a455515cc5c8	wavelet-based color texture retrieval using the independent component color space	wavelet based color texture retrieval;color space;feature distance;independent component analysis;joints;wavelet decomposition;indexing terms;color texture;image texture;similarity measurement;kullback leibler distance kld color texture independent component analysis ica wavelet generalized gaussian density ggd;wavelet transforms;computational modeling;hidden markov models;wavelet transforms feature extraction gaussian distribution image colour analysis image retrieval image texture independent component analysis;generalized gaussian density ggd;independent component analysis ica;low dimensional marginal distributions;image color analysis;image colour analysis;feature extraction;hidden markov models image retrieval image color analysis wavelet coefficients independent component analysis wavelet analysis image databases spatial databases pixel content based retrieval;generalized gaussian density;kullback leibler distance kld;query texture images;similarity measure;independent component;rgb color spaces;gaussian distribution;wavelet coefficients;wavelet;feature distance wavelet based color texture retrieval rgb color spaces independent component analysis generalized gaussian density similarity measurement kullback leibler distance low dimensional marginal distributions wavelet decomposition feature extraction wavelet coefficients query texture images;kullback leibler distance;image retrieval	In this paper, we propose a wavelet based color texture retrieval method using the independent component color space. In color texture retrieval, the product of low dimensional marginal distributions of wavelet coefficients from different color layers are preferred to substitute or approximate their high dimensional joint distributions in order to avoid the curse of dimensionality. However, the RGB color spaces is a highly correlated color space and the extracted wavelet coefficients from different layers are also correlated, which means such a substitution or approximation will not be adequate. To solve the problem, we use independent component analysis to decor- relate the R, G and B layers into three new independent layers before applying wavelet decomposition on the color texture images. In the feature extraction (FE) step of the proposed method, generalized Gaussian density (GGD) are used to model the marginal distribution of wavelet coefficients, and the extracted model parameters are used as features. In the similarity measurement (SM) step of the proposed method, the Kullback-Leibler distance(KLD) is calculated as feature distance, using the extracted model parameters of the query texture images and those of the images in the database. Experimental results on a database of 1120 color texture images indicate that the proposed method greatly overperforms its RGB based counterpart that ignores the inter-layer correlation, and its counterpart which uses the I1I2I3 colorspace.	approximation algorithm;coefficient;color space;curse of dimensionality;feature extraction;feature model;independent component analysis;kullback–leibler divergence;marginal model;texture filtering;wavelet	Ye Mei;Dimitrios Androutsos	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4711717	normal distribution;color histogram;image texture;wavelet;independent component analysis;computer vision;index term;feature extraction;image retrieval;computer science;machine learning;pattern recognition;mathematics;color balance;kullback–leibler divergence;color space;computational model;statistics;wavelet transform	Vision	37.513036927714545	-60.88580663001198	56782
ef52f1e2b52fd84a7e22226ed67132c6ce47b829	online eye status detection in the wild with convolutional neural networks		A novel eye status detection method is proposed. Contrary to the most of the previous methods, this new method is not based on an explicit eye appearance model. Instead, the detection is based on a deep learning methodology, where the discriminant function is learned from a large set of exemplar images of eyes at different state, appearance, and 3D position. The technique is based on the Convolutional Neural Network (CNN) architecture. To assess the performance of the proposed method, it has been tested against two techniques, namely: SVM with SURF Bag of Features and Adaboost with HOG and LBP features. It has been shown that the proposed method outperforms these with a considerable margin on a two-class problem, with the two classes defined as “opened” and “closed”. Subsequently the CNN architecture was further optimised on a three-class problem with “opened”, “closed”, and “partially-opened” classes. It has been demonstrated that it is possible to implement a real-time eye status detection working with a large variability of head poses, appearances and illumination conditions. Additionally, it has been shown that an eye blinking estimation based on the proposed technique is at least comparable with the current state-of-the-art on standard eye blinking datasets.	adaboost;convolutional neural network;deep learning;discriminant;heart rate variability;local binary patterns;real-time clock;speeded up robust features	Essa R. Anas;Pedro Henríquez;Bogdan J. Matuszewski	2017		10.5220/0006172700880095	simulation;telecommunications	Vision	32.8567561516868	-56.0872670919302	56877
3ef45dfa2e0af9a17c16ead9385233f9166facac	significant region-based image retrieval		In the era of multimedia technologies, the need for information/data retrieval systems getting more attention. The data might be image, video, audio and/or text files. Digital libraries, surveillance application, web applications and many other applications that handle huge volume of data essentially have data retrieval components. In this paper, a technique of region-based image retrieval (RBIR), a branch of content-based image retrieval (CBIR), is proposed. The proposed model identifies a significant region in an image using visual attention-based mechanism and represents them using its color layout descriptors and curvelet descriptors. These features are extracted from significant region of query image and images in the database. The likeness between the query region and database image region is ranked according to a similarity measure computed from the feature vectors. The proposed model does not need a full semantic understanding of image content, uses visual metrics such as proximity, size, color contrast and nearness to image’s boundaries to locate viewer’s attention and uses curvelet transform in combination with color layout descriptor to represent the significant region. Experimental results are analyzed and compared with the state-of-the-art CBIR Systems and RBIR Technique.	image retrieval	P. Manipoonchelvi;K. Muneeswaran	2015	Signal, Image and Video Processing	10.1007/s11760-014-0657-0	image texture;computer vision;visual word;image retrieval;data mining;automatic image annotation;information retrieval	Vision	38.65155346385463	-60.477367657922116	56888
cd5c41b1934f975c5b68aac9a5c053c339b90b72	a scheme for face recognition in complex environments	moving object removal mor;lda;face recognition;face detection;pca	In this paper, the authors propose a scheme for human face recognition in complex environments. The proposed scheme consists of three phases: moving object removal, face detection and face recognition. It could be applied to certain specific environments such as computer users in office, shopping mall, and reception or pokie machine gamblers in casinos. In these environments, the target human face for recognizing will be considered as the foreground and the moving objects (such as cars, walking persons etc) as the background. The objective of this paper is to implement a scheme for human face recognition so as to improve recognition precision and reduce false alarms. The scheme can be applied to prevent computer users or gamblers from sitting too long in front of the screens in offices or pokie machines in casinos. To the best of the authors’ knowledge, this is the first time face recognition in complex environments has been taken into consideration. KEywoRDS Face Detection, Face Recognition, LDA, Moving Object Removal (MOR), PCA	clone tool;face detection;facial recognition system;linear discriminant analysis;model order reduction;principal component analysis;user (computing)	Wei Cui;Wei Qi Yan	2016	IJDCF	10.4018/IJDCF.2016010102	facial recognition system;computer vision;face detection;simulation;object-class detection;computer science;three-dimensional face recognition;3d single-object recognition;principal component analysis	Vision	30.50951252932419	-60.13388581979035	56919
4570e1932282fecc149de8ed4d0ba1c1bf4be12a	predicting gender from iris texture may be harder than it seems		Predicting gender from iris images has been reported by several researchers as an application of machine learning in biometrics. Recent works on this topic have suggested that the preponderance of the gender cues is located in the periocular region rather than in the iris texture itself. This paper focuses on teasing out whether the information for gender prediction is in the texture of the iris stroma, the periocular region, or both. We present a larger dataset for gender from iris, and evaluate gender prediction accuracy using linear SVM and CNN, comparing hand-crafted and deep features. We use probabilistic occlusion masking to gain insight on the problem. Results suggest the discriminative power of the iris texture for gender is weaker than previously thought, and that the gender-related information is primarily in the periocular region.	aggregate data;baseline (configuration management);biometrics;experiment;ground truth;interference (communication);machine learning;probabilistic database;randomness;type conversion	Andrey Kuehlkamp;K. Bowyer	2018	CoRR		iris stroma;support vector machine;discriminative model;feature extraction;artificial intelligence;probabilistic logic;biometrics;iris recognition;computer science;pattern recognition;periocular region	NLP	26.057803305401816	-56.08099095286167	57037
bd6397c45740ef7df07b4175acc720bc2921feb1	a feature for character recognition based on directional distance distributions	neural nets;image classification;neural network classifier;feature extraction character recognition image classification neural nets;feature extraction;directional distance distribution feature;character recognition spatial databases neural networks testing performance evaluation distributed computing standards development encoding classification algorithms buildings;character recognition;neural network classifier character recognition directional distance distributions features recognition rate circular input pattern array encoding white black distribution cenparmi handwritten numeral database	The performance of a character recognition system depends heavily on what features are being used. Though many kinds of features have been developed and their test performances on a standard database have been reported, there is still room to improve the recognition rate by developing an improved feature. The authors propose a new feature based on DDD (directional distance distribution) information. This new concept regards the input pattern array as being circular. It also contains very rich information by encoding in one representation both the white/black distribution and the directional distance distribution. A test performed on the CENPARMI handwritten numeral database showed a promising result of 97.3% recognition with a neural network classifier using the DDD feature.		Il-Seok Oh;Ching Y. Suen	1997		10.1109/ICDAR.1997.619858	contextual image classification;speech recognition;feature;feature extraction;computer science;machine learning;pattern recognition;time delay neural network;neocognitron;feature;artificial neural network	Vision	32.170278027515096	-65.18377034415767	57055
05cd67d4aea69a4e305a3aa5315cea19662743de	motion-appearance co-memory networks for video question answering		Video Question Answering (QA) is an important task in understanding video temporal structure. We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer. Based on these observations, we propose a motion-appearance co-memory network for video QA. Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA. Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and the results outperform state-of-the-art significantly on all four tasks of TGIF-QA.		Jiyang Gao;Runzhou Ge;Kan Chen;Ramakant Nevatia	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00688	machine learning;task analysis;dynamic random-access memory;knowledge extraction;artificial intelligence;cognition;pattern recognition;logic gate;computer science;question answering	Vision	27.456878561373	-52.38049904392223	57328
d2e8ced1ab966f090db2e2d270b55f5cc3925268	a combined method on the handwritten character recognition	online character recognition schemes;image recognition;handwriting recognition;image segmentation;image resolution;image converters;korean characters handwritten character recognition geometric features data inputs online character recognition schemes;geometric feature;skeleton;character recognition image reconstruction image segmentation image recognition pixel handwriting recognition skeleton moon image converters image resolution;image reconstruction;moon;pixel;handwriting recognition character recognition;data inputs;geometric features;korean characters;character recognition;handwritten character recognition	A novel character recognition method, called the Dempster-Shafer theory combined with Neural Network in Handwritten Character Recognition (DSNNHCR), is proposed in this paper. The DSNNHCR integrates a Recurrent Neural Network (RNN) and the Dempster-Shafer theory (D-S) to recognize handwritten characters. It first employs an RNN to effectively extract oriented features of a handwritten character. Subsequently, the method creates 3 feature variables using extracted oriented features. Finally, 3 feature variables are applied to the Dempster-Shafer theory which can powerfully estimate the similarity ratings between a recognized character and sampling characters in the character database. Experimental results demonstrate that the DSNNHCR system achieves a satisfying recognition performance.	artificial neural network;feature model;handwriting recognition;optical character recognition;random neural network;recurrent neural network;sampling (signal processing)	Moon Jeung Joe;Huen Joo Lee	1995		10.1109/ICDAR.1995.598955	iterative reconstruction;computer vision;speech recognition;document processing;image resolution;intelligent character recognition;computer science;natural satellite;intelligent word recognition;pattern recognition;handwriting recognition;image segmentation;skeleton;pixel	AI	32.681445011474395	-65.40110756503996	57413
c9d272cc836ea5bbe946718d2a4f8b3d57a6c811	cs-3dlbp and geometry based person independent 3d facial action unit detection	image fusion;image classification;three dimensional displays face databases vectors face recognition gold vegetation;face recognition;feature extraction;decision theory;random processes;mean receiver operating characteristic geometry based person independent 3d facial action unit detection cs 3dlbp 3d face data local binary pattern methodology gender independent facial action unit detection decision level fusion random forest classifiers feature extraction method orientation based 3d lbp descriptor bosphorus database;random processes decision theory face recognition feature extraction image classification image fusion	Face is the key component in understanding emotions which play significant roles in many areas from security and entertainment to psychology and education. In this paper, we propose a method to detect facial action units in 3D face data by combining novel geometric properties and a new descriptor based on the Local Binary Pattern (LBP) methodology. The proposed method enables person and gender independent facial action unit detection. The decision level fusion is used by employing the Random Forests classifiers to combine geometric and LBP based features. Unlike the previous methods which suffer from the diversity among different persons and normalize features utilizing neutral faces, our method extracts features on a single 3D face data. Besides, we show that orientation based 3D LBP descriptor can be implemented efficiently in terms of size and time without degrading the performance. We tested our method on the Bosphorus database and present comparative results with the existing methods. Our results outperform those of existing methods, achieving a mean receiver operating characteristic area under curve of 97.7%.	belief propagation;database;facial recognition system;heart rate variability;local binary patterns;neutral monism;random forest;receiver operating characteristic;sensor	Neslihan Bayramoglu;Guoying Zhao;Matti Pietikäinen	2013	2013 International Conference on Biometrics (ICB)	10.1109/ICB.2013.6612977	facial recognition system;stochastic process;computer vision;contextual image classification;local binary patterns;decision theory;feature extraction;computer science;machine learning;pattern recognition;three-dimensional face recognition;image fusion;statistics	Vision	32.56581281250776	-58.325077635662524	57502
0f055873be7a61380a7d786bd05e5915de96c1b5	face recognition based only on eyes' information and local binary pattern		In this paper the implementation of the Local Binary Pattern algorithm for face recognition is presented using the partial information of the face, the main contribution of this work is that segmenting the parts of face (forehead, eyes, mouth) can make the recognition a person using only their eyes and getting a percentage of up to 69%, which considering the limited information provided a good success rate is obtained. In the test phase AR facedatabase it was used and using the method of Viola Jones face is located and segmented to obtain templates for each person and each part of his face and Euclidean distance was used for classification task. Because in a real application do not always have all the face of the person to identify the proposed system shows that you can get good results with partial information about it, in addition the results show that in the ranking 6 always provided the right person, which is also useful in real applications.	algorithm;euclidean distance;facial recognition system;jones calculus	Francisco Rosario-Verde;Joel Perez-Siles;Luis Aviles-Brito;Jesus Olivares-Mercado;Karina Toscano-Medina;Héctor M. Pérez Meana	2015	Research in Computing Science		computer vision;computer science;artificial intelligence;communication	Vision	31.399636799334722	-59.934080453166445	57664
4b9c79b180493d38635e67e26ad5b9798c2e403a	object recognition using cellular simultaneous recurrent networks and convolutional neural network		In recent years, Convolutional Neural Networks (CNNs) have become very popular and have achieved great success in many computer vision tasks — particularly in object recognition. Partially inspired by neuroscience, CNNs share many properties with the visual system of the brain. However, the filters of convolutional layers play a vital role on overall accuracy of CNNs. In this paper, the Cellular Simultaneous Recurrent Networks (CSRNs) are applied to generate initial filters of Convolutional Networks (CNs) for features extraction and Regularized Extreme Learning Machines (RELM) are used for classification. Furthermore, Deep Belief Networks (DBN), CNNs with random and Gabor filters are implemented to evaluate the overall performance against the proposed CSRN's filters based CNs with RELM. Experiments were conducted on three popular datasets for object recognition (such as face, pedestrian, and car) to evaluate the performance of the proposed system. The experimental results show that in most of the cases, the proposed approach provides better performance on the extracted features using CSRN's filters with CNs compare to initialize with Gaussian random and DBN for object recognition.	artificial neural network;bayesian network;computer vision;convolutional neural network;deep learning;feature extraction;gabor filter;neural networks;neural network software;outline of object recognition;pedestrian detection;recurrent neural network;statistical classification	Md. Zahangir Alom;Mahbubul Alam;Tarek M. Taha;Khan M. Iftekharuddin	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966211	artificial neural network;convolutional neural network;machine learning;artificial intelligence;deep learning;deep belief network;pattern recognition;feature extraction;computer science;recurrent neural network;cognitive neuroscience of visual object recognition	Vision	27.13344689745351	-53.01439605827445	57939
b9788034f7ef244369e62d0465030c30b3fc9600	off-line signature verification, without a priori knowledge of class /spl omega//sub 2/. a new approach	fuzzy artmap;handwriting recognition;handwriting recognition forgery error analysis humans fuzzy systems pattern recognition writing protocols system testing;signature verification;random forgeries signature verification human learning fuzzy artmap based system;a priori knowledge;art neural nets;human learning;random forgeries;fuzzy artmap based system;art neural nets handwriting recognition	This work proposes a new approach to signature verification. It is inspired by the human learning and the approach adopted by the expert examiner of signatures, in which an a priori knowledge of the class of forgeries is not required in order to perform the verification task. Based on this approach, we present a Fuzzy ARTMAP based system for the elimination of random forgeries. Compared to the conventional systems proposed thus far, the presented system is trained with genuine signatures only. Six experiments have been performed on a data base of 200 signatures taken from five writers (40 signatures/writer). Evaluation of the system was measured using different numbers of training signatures.		Nabeel A. Murshed;Flávio Bortolozzi;Robert Sabourin	1995		10.1109/ICDAR.1995.598974	a priori and a posteriori;speech recognition;computer science;machine learning;pattern recognition;handwriting recognition	Logic	30.56987344020332	-64.71456561225091	57944
ae0eaf13345ec6543942a150b23ee1304362d7d1	an efficient facial expression recognition system in infrared images	databases;image recognition;facial expression recognition;feature extraction databases face recognition image recognition face cameras gabor filters;image classification;gabor filters;image texture;wavelet transforms;infrared image gauss laguerre facial expression recognition;face recognition;infrared image;infrared imaging;gauss laguerre;otcbvs database efficient facial expression recognition system infrared images visible expression databases fer algorithm gauss laguerre filter gl filters circular harmonic wavelets redundant wavelets complex texture feature extraction multiple sensors scenario k nearest neighbor ustc nvie database;feature extraction;face;wavelet transforms face recognition feature extraction filtering theory image classification image texture infrared imaging visual databases;filtering theory;cameras;visual databases	Most of the reported algorithms for facial expression recognition (FER) are based on visible expression databases. However, visible images are affected by illumination variations which can cause significant disparities in image appearance and texture. In this paper preliminary results of a new FER algorithm, using Gauss-Laguerre (GL) filter of circular harmonic wavelets to extract features for infrared images, are presented. By using GL filters with properly tuned-parameters, it is possible to generate a set of redundant wavelets that enable an accurate extraction of complex texture features from an infrared image. In addition, we utilize GL filters that are highly suitable for FER in visible images. The combination of infrared and visible FER using common feature extraction approach saves time and reduces the complexity in a multiple-sensors scenario. K-nearest neighbor is used for classification on OTCBVS and USTC-NVIE databases. The results show effective performance for using GL filters in FER for infrared images.	database;feature extraction;gauss–newton algorithm;gauss–seidel method;k-nearest neighbors algorithm;sensor;wavelet	Ahmad Poursaberi;Svetlana N. Yanushkevich;Marina L. Gavrilova	2013	2013 Fourth International Conference on Emerging Security Technologies	10.1109/EST.2013.11	computer vision;speech recognition;pattern recognition;mathematics	Vision	38.53833294253635	-59.019461198837305	58005
6dfd8dbb1e76883c8c4bfe2f1b9435c4ff2d5256	character recognition for the machine reader zone of electronic identity cards	support vector machines character recognition distortion image classification image coding image enhancement image matching;character recognition image edge detection histograms support vector machines optical character recognition software principal component analysis yttrium;support vector machine character recognition machine reader zone electronic identity cards passport image rectification inverse projective transform projective distortion removal code extraction code enhancement adaptive posterization template matching similarity measure code classification;machine reader zone e passport optical character recognition	This paper proposes an overall procedure of recognizing the machine reader zone of a real world picture of a passport. To begin with, the proposed method finds the area of passport from the input image and its rotation angle is determined. With the rectified passport image by counter-rotating the area of passport, the machine reader zone is found and an inverse projective transform is performed to remove projective distortion. Then, each code is extracted and enhanced by using adaptive posterization. Template matching with improved similarity measure is applied to classify the codes. To classify the number 0 and the character O, a support vector machine is used. The experimental results show the correct character recognition rate of 99.77% and the correct recognition rate of 83.84%.	biometric passport;code;distortion;optical character recognition;posterization;similarity measure;support vector machine;template matching	Hyeogjin Lee;Nojun Kwak	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7350826	computer vision;speech recognition;computer science;pattern recognition	Robotics	33.626586642838035	-64.36978032964097	58056
4f010c71058d841a0b4f5db309c0b912a9322f5d	multi-scale improves boundary detection in natural images	empirical study;large dataset;natural images;large scale;boundary detection;quantitative evaluation	In this work we empirically study the multi-scale boundary detection problem in natural images. We utilize local boundary cues including contra st, localization and relative contrast, and train a classifier to integrate them acro ss scales. Our approach successfully combines strengths from both larg e-scale detection (robust but poor localization) and small-scale detection (detail-pre serving but sensitive to clutter). We carry out quantitative evaluations on a variety of boundary and object datasets with human-marked groundtruth. We sho w that multi-scale boundary detection offers large improvements, ranging fro m 20% to 50%, over single-scale approaches. This is the first time that multi-scale is demonstrated to improve boundary detection on large datasets of natural images.	clutter;internationalization and localization	Xiaofeng Ren	2008		10.1007/978-3-540-88690-7_40	computer vision;machine learning;data mining;empirical research	Vision	31.402545927110634	-53.25066964764801	58185
d36fa3795ac1f1fe1923c40611cc973c91cbc770	two-stage recognition for printed thai and english characters using nearest neighbor and support vector machine	nearest neighbor ocr thai characters two stage recognition hybrid classifier svm;pattern clustering;prototypes support vector machines training image edge detection feature extraction character recognition training data;support vector machines;prototypes;training;image classification;thai characters;hybrid classifier;matrix algebra;training data;image edge detection;feature extraction;nearest neighbor;two stage recognition;ocr;svm;confusion matrices two stage recognition printed thai characters printed english characters support vector machine class classification structural feature extraction image ratios image projections outer boundaries pyramid histogram of oriented gradients phog fuzzy c mean clustering fcm nearest neighbor prototype hybrid structure nearest neighbor classifier svm classifier;character recognition;support vector machines character recognition feature extraction image classification matrix algebra pattern clustering	In this paper, we introduce a two-stage recognition process for classification of 164 classes of mixing of printed Thai and English characters. Various structural features based on image ratios, image projections, outer boundaries, Pyramid Histogram of Oriented Gradients (PHOG) are extracted from images. In the first stage, Fuzzy C Mean Clustering (FCM) is applied to create prototypes of every character. The class of nearest neighbor prototype is determined and used as the first stage classification output. A hybrid structure of nearest neighbor classifier and Support Vector Machine (SVM) are proposed for the second stage. Based on classification results obtained from the first stage, the suitable classifiers can be selected. For SVM classifier, possible class candidates for each prototype are analyzed from confusion matrices of the first stage result. For nearest neighbor classifier, in order to refine the result, accurate search on a limited set of training samples corresponding to the nearest prototypes obtained in the first stage is performed. According to experiments on data set of more than 500,000 character images with various font styles, sizes, and resolutions, we obtain the accuracy of 88.09% in the first stage and the result is improved to 97.06% in the second stage. The experiments also show improvement of the proposed scheme in comparison with conventional schemes.	confusion matrix;experiment;fuzzy cognitive map;histogram of oriented gradients;image gradient;k-nearest neighbors algorithm;nearest neighbour algorithm;printing;prototype;support vector machine	Chayut Wiwatcharakoses;Karn Patanukhom	2013	2013 International Conference on Signal-Image Technology & Internet-Based Systems	10.1109/SITIS.2013.23	support vector machine;best bin first;speech recognition;computer science;machine learning;pattern recognition	Vision	33.31917035427464	-65.48386416680064	58225
39cc55356215fef3f975c74fd024441dcdc20b65	summarization-based video caption via deep neural networks	video caption;rnn;summarization;deep learning;cnn	Generating appropriate descriptions for visual content draws increasing attention recently, where the promising progresses were obtained owing to the breakthroughs in deep neural networks. Different from the traditional SVO (subject, verb, object) based methods, in this paper, we propose a novel framework of video caption via deep neural networks. For each frame, we extract visual features by a fine-tuned deep Convulutional Neural Networks (CNN), which are then fed into a Recurrent Neural Networks (RNN) to generate novel sentences descriptions for each frame. In order to obtain the most representative and high-quality descriptions for target video, a well-devised automatic summarization process is incorporated to reduce the noises by ranking on the sentence-sequence graph. Moreover, our framework owns the merit of describing out-of-sample videos by transferring knowledge from pre-captioned images. Experiments on the benchmark datasets demonstrate our method has better performance than the state-of-the-art methods of video caption in language generation metrics as well as SVO accuracy.	automatic summarization;benchmark (computing);deep learning;natural language generation;neural networks;random neural network;recurrent neural network;sequence graph;sparse voxel octree	Guang Li;Shubo Ma;Yahong Han	2015		10.1145/2733373.2806314	natural language processing;speech recognition;computer science;machine learning;pattern recognition;deep learning	AI	25.33355074161768	-53.943316411971814	58393
53eee96bf8fecb143f7b648e57cc9aa02b32f65e	fingerprint enhancement in the singular point area	enhancement;band pass filters;fingerprint recognition image matching gabor filters laboratories pattern recognition automation design methodology frequency algorithm design and analysis computer science;band pass filters fingerprint identification image enhancement feature extraction;image enhancement;filter;feature extraction;automatic fingerprint identification system;fingerprint;bandpass filter fingerprint enhancement fingerprint singular point area minutiae extraction automatic fingerprint identification systems filter;fingerprint identification;singular point	Minutiae extraction is one of the most important steps for automatic fingerprint identification systems. However, the performance of minutiae extraction relies heavily on an enhancement algorithm. There are many enhancement algorithms that depend on the local orientation field of the fingerprint. In the singular point area, because the local orientation changes very rapidly, enhancement is not accurate, so that the result is very bad. We present a new method and design a new filter to enhance a fingerprint in the singular point area. We distinguish the singular point area first. Then we design a new filter to enhance this area. Experimental results show a significant improvement of fingerprint enhancement in the singular point area, and the time required for our algorithm is reduced.	algorithm;fingerprint;minutiae	Sen Wang;Yangsheng Wang	2004	IEEE Signal Processing Letters	10.1109/LSP.2003.819351	fingerprint;computer vision;speech recognition;computer science;pattern recognition;fingerprint recognition	EDA	34.948284697047136	-62.63856759692722	58471
b1d87b57212b9e449e65d4554a42c1b3379a7f0d	pedestrian detection via a leg-driven physiology framework	histograms;image segmentation;neural networks;legged locomotion;physiology;image edge detection;context	In this paper, we propose a leg-driven physiology framework for pedestrian detection. The framework is introduced to reduce the search space of candidate regions of pedestrians. Given a set of vertical line segments, we can generate a space of rectangular candidate regions, based on a model of body proportions. The proposed framework can be either integrated with or without learning-based pedestrian detection methods to validate the candidate regions. A symmetry constraint is then applied to validate each candidate region to decrease the false positive rate. The experiment demonstrates the promising results of the proposed method by comparing it with Dalal & Triggs method. For example, rectangular regions detected by the proposed method has much similar area to the ground truth than regions detected by Dalal & Triggs method.	ground truth;pedestrian detection;vertical bar	Gongbo Liang;Qi Li;Xiangui Kang	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532895	computer vision;simulation;computer science;machine learning;histogram;image segmentation;artificial neural network	Robotics	33.42220214318533	-56.38033720301406	58526
2c6efd3d17c582029de8278173aeef3f03f8c831	combination of signature verification techniques by svm	dp signature verification gradient feature hog svm;dynamic programming;dp;support vector machines;image matching;signature verification;support vector machines accuracy feature extraction vectors training forgery hidden markov models;hog;time series;vectors;gradient feature;svm;vectors dynamic programming handwritten character recognition image matching support vector machines time series;time series data signature verification svm off line feature on line feature gradient feature vector dynamic programming matching technique;handwritten character recognition	This paper proposes a new SVM based technique for combining signature verification techniques using off-line features and on-line features. The off-line feature based technique employs gradient feature vector representing the shape of signature image, and the on-line feature based technique employs dynamic programming (DP) matching technique for time series data of the signatures. The final decision (verification) is performed by SVM based on output from those off-line and online techniques. In the evaluation test the proposed technique achieved 92.96% verification accuracy, which is 1.4% higher than the better accuracy obtained by the individual techniques. This result shows that combining multiple techniques by SVM improves signature verification accuracy significantly.	algorithm;dynamic programming;feature vector;formal verification;gradient;online and offline;signature;time series	Takashi Ito;Wataru Ohyama;Tetsushi Wakabayashi;Fumitaka Kimura	2012	2012 International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2012.192	support vector machine;speech recognition;computer science;machine learning;pattern recognition	EDA	31.195626569095268	-64.13265052080499	58686
70b500520e0e229993f0154aa4b08b5bd7601253	a content based feature combination method for face recognition		In the last few years, Content Based Image Retrieval (CBIR) system, where images are searched based on their visual contents instead of annotated texts, has drawn enormous attention of researchers because of its growing demand from real world applications. According to many, biometric traits recognition is one of the most potential applications of CBIR. However, very few works have been published on CBIR based face recognition systems. In this research, a content based face recognition process, where color, texture, and shape features are combined to enhance the retrieval accuracy of the system, is proposed. Methodology: 0.4 0.5 0.6 0.7 0.8 0.9 1 Color Histogram [1] Affine Moment Invarinat [3] Gabor Filter [2] Proposed Method A ve ra g e P re ci si o n R at e Grayscale Color Critical Query Fig 3: Comparison of average precision rate of different methods for different databases. Three well known and computationally efficient methods: color histogram [1], Gabor filter [2], and affine moment invariant [3] are used to extract color, texture, and shape features, respectively. Fig. 1 depicts the block diagram of the proposed method. The gray blocks indicate the novel components of the proposed method. Conclusion: A novel content based face recognition method is proposed. Our feature fusion method has better performance than single feature based methods. This method can be applied to any database effectively because of its high recognition rate, ease of computation, and easy weight adjustment features. Future work includes development of a weight learning system for different features.	algorithmic efficiency;biometrics;block cipher;color histogram;computation;content-based image retrieval;database;diagram;facial recognition system;feature vector;gabor filter;grayscale;image moment;information retrieval	Madeena Sultana;Marina L. Gavrilova	2013		10.1007/978-3-319-00969-8_19	feature;three-dimensional face recognition;feature	Vision	35.41307649143511	-58.273123007033504	58714
7746ab4f35b7f3237063fac29b619f56298f022c	picsom-self-organizing image retrieval with mpeg-7 content descriptors	international standard;self organising feature maps image retrieval content based retrieval;cbir;neural self organizing technique;image indexing technique;image indexing;euclidean distance;self organizing image retrieval;computer vision;mpeg 7 content descriptors;reference systems;pictorial examples;radio frequency;euclidean distance calculation;vq;vector quantization;self organising feature maps;indexing;visual image content;picsom;tree structure;self organization;self organized map;euclidean distance calculation picsom self organizing image retrieval mpeg 7 content descriptors content based image retrieval cbir visual image content international standard neural self organizing technique pictorial examples relevance feedback rf image indexing technique vector quantization vq;humans;vector quantizer;associate members;neurofeedback;content based image retrieval;mpeg 7 standard;relevance feedback;content based retrieval;rf;image retrieval content based retrieval mpeg 7 standard radio frequency computer vision neurofeedback humans associate members indexing vector quantization;internal standard;image retrieval	"""Development of content-based image retrieval (CBIR) techniques has suffered from the lack of standardized ways for describing visual image content. Luckily, the MPEG-7 international standard is now emerging as both a general framework for content description and a collection of specific agreed-upon content descriptors. We have developed a neural, self-organizing technique for CBIR. Our system is named PicSOM and it is based on pictorial examples and relevance feedback (RF). The name stems from """"picture"""" and the self-organizing map (SOM). The PicSOM system is implemented by using tree structured SOMs. In this paper, we apply the visual content descriptors provided by MPEG-7 in the PicSOM system and compare our own image indexing technique with a reference system based on vector quantization (VQ). The results of our experiments show that the MPEG-7-defined content descriptors can be used as such in the PicSOM system even though Euclidean distance calculation, inherently used in the PicSOM system, is not optimal for all of them. Also, the results indicate that the PicSOM technique is a bit slower than the reference system in starting to find relevant images. However, when the strong RF mechanism of PicSOM begins to function, its retrieval precision exceeds that of the reference system."""	content-based image retrieval;euclidean distance;experiment;indexes;mpeg-7;mental suffering;name;optic nerve glioma, childhood;organizing (structure);radio frequency;relevance feedback;self-organization;self-organizing map;vector quantization	Jorma Laaksonen;Markus Koskela;Erkki Oja	2002	IEEE transactions on neural networks	10.1109/TNN.2002.1021885	computer vision;image retrieval;computer science;machine learning;multimedia;radio frequency;information retrieval	Vision	39.17599409849322	-63.02528072373416	58845
a03279e632a61790e15c4d07e9752c7e6e065975	towards regional fusion for high-resolution palmprint recognition	databases;digital forensics;logistic regression based regional fusion high resolution palmprint recognition high resolution palmprint matching algorithms minutiae based fingerprint matching strategy full to full palmprint comparison partial to full palmprint comparison forensic palmprint recognition latent marks full palmprints crease features region to region palmprint comparison interdigital regions hypothenar regions thenar regions region to region comparison scores sdk megamatcher 4 0 thupalmlab eer;image segmentation;image resolution;image matching;testing;accuracy;logistics;conferenceobject;regional fusion high resolution palmprints;palmprint recognition;high resolution palmprints;regression analysis;correlation;regional fusion;forensics;regression analysis digital forensics image matching image resolution image segmentation palmprint recognition;logistics databases forensics accuracy correlation image resolution testing	The existing high resolution palm print matching algorithms essentially follow the minutiae-based fingerprint matching strategy and focus on full-to-full/partial-to-full palm print comparison. These algorithms would face problems when they are applied to forensic palm print recognition where latent marks have much smaller area than full palm prints. Therefore, towards forensic scenarios, we propose a novel matching strategy based on regional fusion for high resolution palm print recognition using regions segmented by major creases features. The matching strategy includes two stages: 1) region-to-region palm print comparison, 2) regional fusion at score level. We first studied regional discriminability of a high resolution palm print under the concept of three regions, i.e., interdigital, hypothenar and thenar, which is the most significant difference between palmprits and fingerprints. Then we implemented regional fusion based on logistic regression at score level using region-to-region comparison scores obtained by a commercial SDK, Mega Matcher 4.0. Significant improvement of recognition accuracy is achieved by regional fusion on a public high resolution palm print database THUPALMLAB. The EER of logistic regression based regional fusion is 0.25%, while the EER of full-to-full palm print comparison is 1%.	algorithm;enhanced entity–relationship model;fingerprint;image resolution;logistic regression;minutiae;palm print;software development kit	Ruifang Wang;Daniel Ramos-Castro;Julian Fiérrez;Ram P. Krish	2013	2013 XXVI Conference on Graphics, Patterns and Images	10.1109/SIBGRAPI.2013.56	computer vision;speech recognition;geography;pattern recognition	Vision	33.18284512257789	-61.54485133161484	58995
7f29c2fe13d307b06825d977993645c39f6cbad8	computer vision – eccv 2012		Spatial pyramid matching (SPM) based pooling has been the dominant choice for state-of-art image classification systems. In contrast, we propose a novel object-centric spatial pooling (OCP) approach, following the intuition that knowing the location of the object of interest can be useful for image classification. OCP consists of two steps: (1) inferring the location of the objects, and (2) using the location information to pool foreground and background features separately to form the image-level representation. Step (1) is particularly challenging in a typical classification setting where precise object location annotations are not available during training. To address this challenge, we propose a framework that learns object detectors using only image-level class labels, or so-called weak labels. We validate our approach on the challenging PASCAL07 dataset. Our learned detectors are comparable in accuracy with stateof-the-art weakly supervised detection methods. More importantly, the resulting OCP approach significantly outperforms SPM-based pooling in image classification.	adobe photoshop;analysis of algorithms;belief propagation;categorization;coherence (physics);emoticon;european conference on computer vision;experiment;foreground-background;graphics;greedy algorithm;in the beginning... was the command line;inpainting;interaction;lecture notes in computer science;local optimum;markov chain;markov random field;mathematical optimization;minimum bounding box;montagejs;olga (technology);object detection;offset (computer science);open core protocol;patchmatch;pixel;retargeting;seam carving;sensor;software propagation;solver;springer (tank);super paper mario;time complexity	Andrew Fitzgibbon Svetlana Lazebnik;Pietro Perona Yoichi Sato;Cordelia Schmid	2012		10.1007/978-3-642-33709-3	artificial intelligence;computer vision;computer science	Vision	31.633815576220375	-52.92499504493403	59309
a49546babdc1fe0dd94214815143abfc3b4a4a2b	deep neural networks for handwritten chinese character recognition		Automatic handwriting recognition is an important task since it can be used to replace human beings in various activities such as identifying postal addresses on envelopes, information in bank checks, and several other tedious tasks that humans need to perform. Convolutional Neural Networks are a power machine learning method for computer vision tasks, having achieved state-of-the-art results in the recognition of handwritten Arabic digits and also in multiple distinct alphabets. In this work, we extensively explore the performance of those networks for handwritten Chinese characters recognition (HCCR). For such, we have trained several models based on popular convolutional neural networks architectures that are commonly used for large-scale image recognition, and we also employ several distinct architectural fusion methods, resulting in more than 18 classification approaches. We report the results of all 18 configurations in the well-known HWDB and ICDAR2013 datasets.	application programming interface;artificial neural network;computation;computer vision;convolutional neural network;deep learning;experiment;gradient;handwriting recognition;high- and low-level;informatics;machine learning;neural networks;optical flow;postal;smartphone;whole earth 'lectronic link	Renan Guedes Maidana;Juarez Monteiro dos Santos;Roger Leitzke Granada;Alexandre de Morais Amory;Rodrigo C. Barros	2017	2017 Brazilian Conference on Intelligent Systems (BRACIS)	10.1109/BRACIS.2017.24	convolutional code;support vector machine;convolutional neural network;chinese characters;arabic numerals;artificial neural network;machine learning;handwriting recognition;artificial intelligence;computer science	Vision	25.529669976664987	-54.95317064057327	59353
652bc41a72065f06105907245605503472618f91	integration of global and local feature for age estimation of facial images	global feature;age estimation;classifier integration;local feature	Automatic age estimation from facial images is emerging as an important research area in recent years due to its promising potential for some computer vision applications. In this paper we propose a novel approach combine the global and local facial features in parallel manner to implement the age estimation. Then after extracting global and local features, these features are integrated for fine classification. In the proposed method, global and local features are extracted by Discrete Fourier Transform (DFT) and Principal Component Analysis (PCA) respectively. We have conducted experiments on a large scale age databases (FGNET). The experimental results are very promising in showing that it is an effective method		Jie Kou;Ji-Xiang Du;Chuan-Min Zhai	2012		10.1007/978-3-642-31576-3_58	computer vision;pattern recognition;data mining	Vision	33.872216756745196	-58.55775630122137	59430
786fd9050cb803260523345d117d5ac7bb97c665	a new and robust method for character segmentation and recognition in license plate images	local algorithm;feed forward neural network;character segmentation;light and shade in images;learning methods;non uniform illumination;computational complexity;principal component analysis;robust method;license plate recognition;character recognition;back propagation	This paper provides a new and fast method for segmentation and recognition of characters in license plate images. For this purpose, various methods have been proposed in literature. However, most of them suffer from: sensitivity to non-uniform illumination distribution, existence of shade in license plate, license plate color and the need for receiving an exact image of the license plate. In the proposed algorithm, non-uniform illumination and noise are reduced by a Gaussian lowpass filter and also by an innovational Laplacian-like transform and characters are segmented by a set of indigenous and relative features. To be prepared for recognition, the segmented characters are normalized by a local algorithm. Two feed-forward neural networks with back-propagation learning method are employed for character recognition. The principal component analysis (PCA) is used to decrease input data and, consequently, computational complexity. The proposed algorithm does not necessarily need an exact plate image and can receive a band from the vehicle original image as an input, which includes the plate. Our proposed method is completely robust to the disturbances such as non-uniform brightness distribution on the various positions of a license plate image and the plate color. In order to evaluate our algorithm, we applied it on a database including 120 vehicle images with different backgrounds, plate colors, brightness distributions, distances and viewing angles. The results confirm the robustness of the proposed method against severe imaging conditions.		Amir Sedighi;Mansur Vafadust	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.02.030	computer vision;feedforward neural network;speech recognition;computer science;backpropagation;machine learning;pattern recognition;computational complexity theory;principal component analysis	Vision	38.68888003148458	-64.98435008247917	59475
6176a3a6b8633118156d84b5920ab5572c865b58	a car detection system based on hierarchical visual features	image classifier car detection system hierarchical visual feature detector shunting inhibitory neuron multiresolution processing adaptive thresholding strategy nonlinear bandpass filter;detectors;adaptive thresholding strategy;nonlinear bandpass filter;adaptive thresholding;hierarchical;image segmentation;image resolution;neural nets;bepress selected works;image classifier;training;nonlinear filter;image classification;hierarchical visual feature detector;detection;multiresolution processing;data mining;based;shunting inhibitory neuron;computer architecture;features;visualization;feature extraction;car;system;car detection system;visual features;neurons;object detection feature extraction filtering theory image classification image resolution image segmentation neural nets;multi resolution;false positive;neurons feature extraction computer vision gabor filters pattern recognition detectors support vector machines support vector machine classification data mining brain modeling;car detection system based hierarchical visual features;filtering theory;object detection;visual	In this paper, we address the problem of detecting and localizing cars in still images. The proposed car detection system is based on a hierarchical feature detector in which the processing units are shunting inhibitory neurons. To reduce the training time and complexity of the network, the shunting inhibitory neurons in the first layer are implemented as directional nonlinear filters, whereas the neurons in the second layer have trainable parameters. A multi-resolution processing scheme is implemented so as to detect cars of different sizes, and to reduce the number of false positives during the detection stage, an adaptive thresholding strategy is developed. Tested on the UIUC car database, the proposed method achieves better classification results than some of the existing car detection approaches.	linear classifier;nonlinear system;pyramid (image processing);sensitivity and specificity;sensor;thresholding (image processing)	Fok Hing Chi Tivive;Abdesselam Bouzerdoum	2009	2009 IEEE Symposium on Computational Intelligence for Multimedia Signal and Vision Processing	10.1109/CIMSVP.2009.4925645	computer vision;computer science;machine learning;pattern recognition	Vision	26.07582712969347	-61.31480363656911	59565
431c6a0a7e4674f2b9148d85159bfb533f805b0d	comparison of cbir systems with different number of feature vector components	inflight entertainment system;personalized entertainment services;passenger psychological negative stress;passenger comfort level;in flight entertainment;context adaptive system;image retrieval content based retrieval;adaptive system;image retrieval information retrieval image databases content based retrieval shape image edge detection feedback spatial databases mpeg 7 standard acceleration;feature vector reduction feature vector components content based image retrieval user relevance feedback mpeg 7;user interfaces;entertainment;passenger physical negative stress;long haul aircrafts	Content-based image retrieval (CBIR) systems with user relevance feedback are considered. The influence of the type and the number of feature vector (FV) components on the retrieval efficiency was investigated. We compared a CBIR system with a very small number of FV components (only 25 components describing color and texture) with a system with a high-dimensional FV inspired by MPEG-7 (556 coordinates describing color, texture and line directions), as well as with a system using feature vector reduction (FVR) of about 90% (with about 50 FV components from the full-length 556-component FVs). The systems are tested over the annotated Corel IK and Corel 60K datasets. Simulation results show that a decreased number of FV components does not have significant influence on the quality of image retrieval, while the processing time is reduced compared to CBIR with full-length FV and/or FVR.	content-based image retrieval;farmville;feature vector;inverse kinematics;mpeg-7;relevance feedback;simulation	Stevan Rudinac;Goran Zajic;Marija Uscumlic;Maja Rudinac;Branimir Reljin	2007	Second International Workshop on Semantic Media Adaptation and Personalization (SMAP 2007)	10.1109/SMAP.2007.23	simulation;engineering;multimedia;advertising	Vision	39.05895655651971	-62.14623695717037	59756
8ac2736683dac9a467602ee19f5a290096259148	hypernet: towards accurate region proposal generation and joint object detection		Almost all of the current top-performing object detection networks employ region proposals to guide the search for object instances. State-of-the-art region proposal methods usually need several thousand proposals to get high recall, thus hurting the detection efficiency. Although the latest Region Proposal Network method gets promising detection accuracy with several hundred proposals, it still struggles in small-size object detection and precise localization (e.g., large IoU thresholds), mainly due to the coarseness of its feature maps. In this paper, we present a deep hierarchical network, namely HyperNet, for handling region proposal generation and object detection jointly. Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space. The Hyper Features well incorporate deep but highly semantic, intermediate but really complementary, and shallow but naturally high-resolution features of the image, thus enabling us to construct HyperNet by sharing them both in generating proposals and detecting objects via an end-to-end joint training strategy. For the deep VGG16 model, our method achieves completely leading recall and state-of-the-art object detection accuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It runs with a speed of 5 fps (including all steps) on a GPU, thus having the potential for real-time processing.	end-to-end principle;graphics processing unit;image resolution;instance (computer science);map;object detection;real-time clock;sensor;tree network	Tao Kong;Anbang Yao;Yurong Chen;Fuchun Sun	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.98	computer vision;computer science;artificial intelligence;data mining	Vision	27.745202944398727	-52.56168841451822	59784
6514ff5a832ddb2c1a5d7e1183899d71c37f0acc	netvlad: cnn architecture for weakly supervised place recognition	google street view time machine netvlad cnn architecture weakly supervised place recognition large scale visual place recognition query photograph convolutional neural network generalized vlad layer vector of locally aggregated descriptors image representation image retrieval;object recognition feedforward neural nets image representation image retrieval neural net architecture;image recognition image retrieval image representation visualization computer architecture training neural networks	"""We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the """"Vector of Locally Aggregated Descriptors"""" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state of-the-art compact image representations on standard image retrieval benchmarks."""		Relja Arandjelovic;Petr Gronát;Akihiko Torii;Tomás Pajdla;Josef Sivic	2016		10.1109/CVPR.2016.572	computer vision;visual word;computer science;artificial intelligence;machine learning;pattern recognition	Vision	26.099519360589163	-53.27202203017966	59922
8ac42388dabbb7ac9fabb17d47e125beb74f6749	a new steganalysis paradigm based on image retrieval of similar image-inherent statistical properties and outlier detection	image databases;training;transform coding;image databases training feature extraction transform coding algorithm design and analysis discrete cosine transforms entropy;discrete cosine transforms;feature extraction;image inherent statistical properties information security steganalysis detection unsupervised outlier detection;entropy;visual databases image classification image matching image retrieval learning artificial intelligence statistical analysis steganography;algorithm design and analysis;unsupervised outlier detection steganalysis paradigm image retrieval image inherent statistical properties outlier detection embedding algorithm mismatch eam cover source mismatch csm image preclassification jpeg steganalysis paradigm image inherent statistical property similarity retrieval iisp similarity retrieval unsupervised outlier detection image database	Conventional steganalysis method generally encounters the problems of embedding algorithm mismatch (EAM) and cover source mismatch (CSM). These problems cause difficulties in the use of steganalysis in the real world. Learning from the idea of image pre-classified, this study presents a JPEG steganalysis paradigm combining the similarity retrieval of image-inherent statistical properties (IISP) and unsupervised outlier detection. First, cover images with similar IISP to the test image are searched from massive image database to establish an aided sample set. Outlier detection is then performed in a test set composed of the test image and its aided sample set to judge the type of the test image. Experimental results show that the proposed paradigm can effectively avoid EAM and CSM. It demonstrates better performance than the steganalysis strategy using a mixed image set as the training sample. The proposed method has high detection efficiency with the unsupervised outlier detection.	algorithm;anomaly detection;embedded atom model;image retrieval;institute of information security professionals;jpeg;programming paradigm;standard test image;steganalysis;test set;unified extensible firmware interface;unsupervised learning	Yu Dong;Tao Zhang;Xiaodan Hou;Chen Xu	2015	2015 International Conference on Wireless Communications & Signal Processing (WCSP)	10.1109/WCSP.2015.7340977	feature detection;computer science;machine learning;pattern recognition;data mining	Vision	37.31446055029014	-62.32203557766453	59964
ce114dd0b87b68bfb30b73e2b0957e3dd4856a02	multi-task deep learning for image understanding	pattern classification image segmentation learning artificial intelligence;multi task learning image segmentation deep learning;training;accuracy;three dimensional displays;solid modeling;face detection multitask deep learning image understanding speech processing image processing multitask training image segmentation object classification;face;face accuracy three dimensional displays data models face detection solid modeling training;face detection;data models	Deep learning models can obtain state-of-the-art performance across many speech and image processing tasks, often significantly outperforming earlier methods. In this paper, we attempt to further improve the performance of these models by introducing multi-task training, in which a combined deep learning model is trained for two inter-related tasks. We show that by introducing a secondary task (such as shape identification in the object classification task) we are able to significantly improve the performance of the main task for which the model is trained. Using public datasets we evaluated our approach on two image understanding tasks, image segmentation and object classification. On the image segmentation task, we observed that the multi-task model almost doubled the accuracy of segmentation at the pixel-level (from 18.7% to 35.6%) compared to the single task model, and improved the performance of face-detection by 10.2% (from 70.1% to 80.3%). For the object classification task, we observed a 2.1% improvement in classification accuracy (from 91.6% to 93.7%) compared to a single-task model. The proposed multi-task models obtained significantly higher accuracies than previously published results on these datasets, obtaining 22.0% and 6.2% higher accuracies on the face-detection and object classification tasks respectively. These results demonstrate the effectiveness of multi-task training of deep learning models for image understanding tasks.	computer multitasking;computer vision;deep learning;face detection;image processing;image segmentation;pixel;synthetic intelligence	Bo Yu;Ian Lane	2014	2014 6th International Conference of Soft Computing and Pattern Recognition (SoCPaR)	10.1109/SOCPAR.2014.7007978	face;data modeling;computer vision;face detection;computer science;machine learning;segmentation-based object categorization;pattern recognition;accuracy and precision;solid modeling;scale-space segmentation	Vision	31.556452774542432	-55.03592352353911	60159
2c61a9e26557dd0fe824909adeadf22a6a0d86b0	convolutional channel features	convolutional channel features pedestrian detection face detection edge detection object proposal generation boosting forest model ccf cnn convolutional neural networks filtered channel features computer vision deep learning methods;object detection computer vision feature extraction neural nets;feature extraction boosting computational modeling image edge detection object detection support vector machines proposals	Deep learning methods are powerful tools but often suffer from expensive computation and limited flexibility. An alternative is to combine light-weight models with deep representations. As successful cases exist in several visual problems, a unified framework is absent. In this paper, we revisit two widely used approaches in computer vision, namely filtered channel features and Convolutional Neural Networks (CNN), and absorb merits from both by proposing an integrated method called Convolutional Channel Features (CCF). CCF transfers low-level features from pre-trained CNN models to feed the boosting forest model. With the combination of CNN features and boosting forest, CCF benefits from the richer capacity in feature representation compared with channel features, as well as lower cost in computation and storage compared with end-to-end CNN methods. We show that CCF serves as a good way of tailoring pre-trained CNN models to diverse tasks without fine-tuning the whole network to each task by achieving state-of-the-art performances in pedestrian detection, face detection, edge detection and object proposal generation.	academy;computation;computer vision;convolutional neural network;deep learning;edge detection;embedded system;end-to-end principle;face detection;graphics processing unit;high- and low-level;microsoft customer care framework;mobile device;pedestrian detection;performance;unified framework	Bin Yang;Junjie Yan;Zhen Lei;Stan Z. Li	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.18	computer vision;object-class detection;computer science;machine learning;pattern recognition	Vision	27.670065478254106	-53.45265414324892	60163
76d27aac176d2f3bfc403d3fe3e19feb6757461d	viewpoint selection for photographing architectures		This paper studies the problem of how to choose good viewpoints for taking photographs of architectures. We achieve this by learning from professional photographs of world famous landmarks that are available on the Internet. Unlike previous efforts devoted to photo quality assessment which mainly rely on 2D image features, we show in this paper combining 2D image features extracted from images with 3D geometric features computed on the 3D models can result in more reliable evaluation of viewpoint quality. Specifically, we collect a set of photographs for each of 15 world famous architectures as well as their 3D models from the Internet. Viewpoint recovery for images is carried out through an image-model registration process, after which a newly proposed viewpoint clustering strategy is exploited to validate users’ viewpoint preferences when photographing landmarks. Finally, we extract a number of 2D and 3D features for each image based on multiple visual and geometric cues and perform viewpoint recommendation by learning from both 2D and 3D features using a specifically designed SVM-2K multi-view learner, achieving superior performance over using solely 2D or 3D features. We show the effectiveness of the proposed approach through extensive experiments. The experiments also demonstrate that our system can be used to recommend viewpoints for rendering textured 3D models of buildings for the use of architectural design, in addition to viewpoint evaluation of photographs and recommendation of viewpoints for photographing architectures in practice.	3d modeling;cluster analysis;experiment;feature engineering;internet;usability testing;viewpoint	Jingwu He;Linbo Wang;Wenzhe Zhou;Hongjie Zhang;Xiufen Cui;Yanwen Guo	2017	CoRR		computer vision;simulation;computer science;multimedia	Vision	32.13656523907552	-52.28314023323535	60178
67ad22cd25095c2602d46ce4cb110b46bb5d1e81	a new method for facial and corporal expression recognition		Expression recognition via facial and corporal movements plays an important role in our daily lives. In this paper, a new method for expression recognition is proposed. We collected a bi-modal dataset of facial and corporal expressions of 9 subjects performance of six expressions (sadness, surprise, happiness, fear, anger, and neutral) using Kinect (v1) and Kinect (v2). New geometrical features including a combination of angle and distance features are used to train multiple classifiers. This work concentrates on facial and corporal expression and feature extraction. For system performance assessment, we used leave-one-out subject cross-validation. The obtained results show the superior performance of the RGB-D features provided by Kinect (v2).	cross-validation (statistics);feature extraction;kinect;modal logic;sadness	Aida Daneshian;Naeem Ramzan;Nouara Achour;Mahmoud Belhocine;Cherif Larbes;Nadia Zenati-Henda	2018	2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00089	surprise;happiness;feature extraction;facial recognition system;sadness;anger;emotion recognition;expression (mathematics);artificial intelligence;pattern recognition;computer science	Vision	31.02331303589875	-59.433318792443615	60232
2b790995c319843ee9e30ae8097822d3d47322ec	text detection based on convolutional neural networks with spatial pyramid pooling	neural networks;support vector machines;training;testing;shape;image color analysis;feature extraction	Text detection is a difficult task due to the significant diversity of the texts appearing in natural scene images. In this paper, we propose a novel text descriptor, SPP-net, extracted by equipping the Convolutional Neural Network (CNN) with spatial pyramid pooling. We first compute the feature maps from the original text lines without any cropping or warping, and then generate the fixed-size representations for text discrimination. Experimental results on the latest ICDAR 2011 and 2013 datasets have proven that the proposed descriptor outperforms the state-of-the-art methods by a noticeable margin on F-measure with its merit of incorporating multi-scale text information and its flexibility of describing text regions with different sizes and shapes.	artificial neural network;convolutional neural network;feature model;international conference on document analysis and recognition;map;pyramid (geometry);self-propelled particles	Rui Zhu;Xiao-Jiao Mao;Qi-Hai Zhu;Ning Li;Yu-Bin Yang	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532514	support vector machine;computer vision;feature extraction;shape;computer science;machine learning;pattern recognition;software testing;artificial neural network	Vision	30.033500094505868	-55.87578498760249	60345
9162a739960d1db15cbaa2f52b3ca4019763437f	feature extraction based on dimension reduction and clustering for maize leaf spot images		Image recognition and feature extraction play an important role in precision agriculture. In this paper, a manifold learning algorithm was used for dimension reduction of gray and RGB color images. To clarify the boundaries of disease spots and leaf background, three clustering algorithms were applied in experiments to realize clearer maize leaf disease images. Locally, linear embedding (LLE) and Gustafson–Kessel (GK) algorithms were selected to realize image feature extraction. It was shown that the recognition rate of feature extraction for gray and color images were 95% and 99%, respectively.		Xue Wang;Qiuju Xie;Tiemin Ma;Jingfu Zhu	2018	IJPRAI	10.1142/S0218001418540290	leaf spot;pattern recognition;artificial intelligence;mathematics;feature extraction;dimensionality reduction;rgb color model;cluster analysis;nonlinear dimensionality reduction	EDA	32.56444060943201	-60.40378290592843	60415
322f7bcbe93a66d6f5e9d33c89080ddc3dad4c21	an improved logo detection method with learning-based verification for video classification	logo detection;svm video classification logo detection copyright protection feature extraction;copyright protection;video classification;feature extraction;svm;automatic logo detection logo detection method learning based verification video classification cloud services commercial video visible watermark;video watermarking feature extraction image classification	With the growth of cloud services, concerns have been raised regarding illegal sharing of the commercial video. To prevent the illegal sharing automatically, the method for classifying video as `commercial' or `noncommercial' is essentially required. Since most commercial video has a logo as a visible watermark, automatic logo detection can be an efficient method for the video classification. In this paper, we present an improved logo detection method which correctly detects the logo in any types of video using learning-based logo verification. Experimental results show that the proposed method achieves improved detection performance as compared with the existing method, and thus can be effectively used for classifying the video.	cloud computing;logo	Hyo-Young Kim;Mun-Cheon Kang;Sung-Ho Chae;Dae Hwan Kim;Sung-Jea Ko	2014	2014 IEEE Fourth International Conference on Consumer Electronics Berlin (ICCE-Berlin)	10.1109/ICCE-Berlin.2014.7034299	computer vision;computer science;video tracking;multimedia;internet privacy	EDA	31.65774545370535	-62.16930586742243	60491
2f866116f4c3d512f085b56cd8f16836a92ca7be	integration of gesture and posture recognition systems for interpreting dynamic meanings using particle filter	support vector machines context free grammars gaussian processes gesture recognition hidden markov models particle filtering numerical methods probability;application gesture recognition posture recognition integration particle filter;human computer interaction;probability;decision level fusion;support vector machines;gaussian processes;context free grammars;gaussian based likelihood function;biometrics;posture recognition system;hmm;gesture recognition system;posterior probability;integration;face recognition;hidden markov models;particle filter;context free grammar;hidden markov models biometrics face recognition particle filters human computer interaction humans;condensation algorithm;posture recognition;svm;humans;particle filters;context free grammar gesture recognition system posture recognition system particle filter decision level fusion hmm svm condensation algorithm optimal a posterior probability approximation gaussian based likelihood function;application;likelihood function;gesture recognition;production rule;optimal a posterior probability approximation;particle filtering numerical methods	This paper proposes a novel approach for determining the integration criteria using Particle filter for fusion of hand gesture and posture recognition system at decision level. For decision level fusion, integration framework requires the classification of hand gesture and posture symbols in which HMM and SVM are used to classify the alphabets and numbers from gesture and posture recognition system respectively. These classification results are input to integration framework to compute the contribution-weights. For this purpose, Condensation algorithm is employed to approximate optimal a-posterior probability using a-prior probability and Gaussian based likelihood function thus making the weights independent of classification ambiguities. Considering the recognition as a problem of regular grammar, we have developed the production rules based on Context Free Grammar for restaurant scenario. On the basis of contribution-weights, we mapped the recognized outcome over CFG rules and infer meaningful expressions. Experiments are conducted on 500 different combinations of restaurant orders with overall 98.3% inference accuracy which proves the significance of proposed approach.	approximation algorithm;blue (queue management algorithm);condensation algorithm;context-free grammar;experiment;hidden markov model;lexicon;particle filter;poor posture;regular grammar	Omer Rashid Ahmed;Ayoub Al-Hamadi;Bernd Michaelis	2010	2010 International Conference of Soft Computing and Pattern Recognition	10.1109/SOCPAR.2010.5686421	support vector machine;computer vision;speech recognition;particle filter;computer science;artificial intelligence;machine learning;pattern recognition;context-free grammar;hidden markov model;statistics	Vision	27.161661894134863	-60.61933760516331	60529
bfe4715a55aac563978fc308076ab42bdbfb6e52	weighting estimation for texture-based face recognition using the fisher discriminant	databases;software;histograms;scientific computing biometrics face recognition local binary pattern local phase quantization software;statistical analysis face recognition image texture;quantization;weighting estimation;biometrics;texture based face recognition;local binary pattern;fisher discriminant analysis weighting estimation texture based face recognition;image texture;fisher discriminant analysis;face recognition;statistical analysis;estimation;local phase quantization;pixel;scientific computing;encoding;databases pixel face recognition encoding quantization histograms estimation	Texture-based automatic face recognition (AFR) methods find global similarities between two faces by computing their local regional similarities. A novel method based on Fisher discriminant analysis is proposed to estimate each region's contribution to the global similarity score. Experimental results show that the method considerably improves recognition performance for texture-based AFR.	alternate frame rendering;facial recognition system;fisher information;linear discriminant analysis	Raul Queiroz Feitosa;Dário Augusto Borges Oliveira;A. de Lima Veiga Filho;Raphael Pithan Brito;José Luiz Buonomo de Pinho;Antonio Carlos Censi	2011	Computing in Science & Engineering	10.1109/MCSE.2010.150	facial recognition system;image texture;computer vision;estimation;kernel fisher discriminant analysis;local binary patterns;speech recognition;quantization;computer science;pattern recognition;histogram;pixel;biometrics;encoding;statistics	Vision	37.381401136369	-60.13570121748326	60589
8fd611a5223b7e46e3321daf7a9daa19f3761158	expansion of queries and databases for improving the retrieval accuracy of document portions: an application to a camera-pen system	software;groundtruth creation;human computer interaction;groundtruth;computer networks and communications;indexation;evaluation;computer vision and pattern recognition;query expansion;invariant feature;image retrieval	This paper presents a method of improving the accuracy of document image retrieval focusing on the application to a camera-pen system. In a camera-pen system, document image retrieval is employed for locating the pen-tip position on a page. A serious problem is that since the camera is mounted close to the pen-tip, the camera captures only a tiny portion of the page and the resultant image is under severe perspective distortion, resulting in lowering the retrieval accuracy. To solve this problem, we propose new geometrically invariant features as well as expansion techniques which increase the number of index features of either the database or the query images. From the experimental results, it has been found that the query expansion technique with features by combining affine and perspective invariants allows us the best performance that improves the accuracy of a baseline method more than 27%.	baseline (configuration management);database;distortion;image retrieval;query expansion;resultant	Koichi Kise;Megumi Chikano;Kazumasa Iwata;Masakazu Iwamura;Seiichi Uchida;Shinichiro Omachi	2010		10.1145/1815330.1815370	computer vision;query expansion;visual word;image retrieval;computer science;evaluation;data mining;information retrieval	Vision	38.97493815456398	-58.843241855403136	60689
90bfdcbf36b6624f6967ab184340a0a3ecffa416	automatic 3d facial segmentation and landmark detection	3d face recognition;surface curvature information automatic 3d facial segmentation landmark detection face detection facial features detection 3d face recognition 2d facial feature extraction;image segmentation;face detection image segmentation face recognition facial features image edge detection data mining image databases biometrics lighting probes;image database;surface curvature information;facial feature extraction;landmark detection;face recognition;range image;feature extraction;facial features detection;adaptive method;facial feature detection;facial features;object detection face recognition feature extraction image segmentation;facial expression;face detection;automatic 3d facial segmentation;2d facial feature extraction;object detection	This paper presents our methodology for face and facial features detection to improve 3D face recognition in a presence of facial expression variation. Our goal was to develop an automatic process to be embedded in a face recognition system, using only range images as input. To do that, our approach combines traditional image segmentation techniques for face segmentation and detect facial features by combining an adapted method for 2D facial features extraction with the surface curvature information. The experiments were performed in a large, well-known face image database available on the Biometric Experimentation Environment (BEE), including 4,950 images. The results confirms that our method is efficient for the proposed application.	biometrics;embedded system;experiment;facial recognition system;image segmentation;three-dimensional face recognition	Mauricio Pamplona Segundo;Chauã C. Queirolo;Olga R. P. Bellon;Luciano Silva	2007	14th International Conference on Image Analysis and Processing (ICIAP 2007)	10.1109/ICIAP.2007.29	facial recognition system;computer vision;face detection;speech recognition;object-class detection;feature extraction;computer science;pattern recognition;three-dimensional face recognition;image segmentation;facial expression;face hallucination	Vision	33.16955613855481	-60.38731993634099	60815
bcaf1703565f23977a43f6a4e3dd2e77e778b82e	evaluation on the compactness of supervoxels		Supervoxels are perceptually meaningful atomic spatiotemporal regions in videos, which has great potential to reduce the computational complexity of downstream video applications. Many methods have been proposed for generating supervoxels. To effectively evaluate these methods, a novel supervoxel library and benchmark called LIBSVX with seven collected metrics was recently established. In this paper, we propose a new compactness metric which measures the shape regularity of supervoxels and is served as a necessary complement to the existing metrics. To demonstrate its necessity, we first explore the relations between the new metric and existing ones. Correlation analysis shows that the new metric has a weak correlation with (i.e., nearly independent of) existing metrics, and so reflects a new characteristic of supervoxel quality. Second, we investigate two real-world video applications. Experimental results show that the new metric can effectively predict some important application performance, while most existing metrics cannot do so.		Ran Yi;Yong-Jin Liu;Yu-Kun Lai	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451659	computational complexity theory;artificial intelligence;pattern recognition;compact space;computer science	Robotics	37.90791157779287	-55.810069360986716	60863
ec3fc21299541dfe2da99cfb311238ccee8daa3c	linstar texture: a fuzzy logic cbir system for textures	content based image retrieval cbir;term set;tamura feature;fuzzy logic;fuzzy clustering;semantic gap;linguistic term;content based image retrieval	In this study, we propose a fuzzy logic CBIR system for textures, named LinStar Texture (i.e., Linguistic Star for Textures). The proposed system consists of two major phases, including database creation and query comparison. In the database creation phase, six Tamura features are extracted to describe each texture image in the database. A term set on each Tamura feature is generated through a fuzzy clustering algorithm so that degrees of appearance for the feature can be interpreted as five linguistic terms. In the query comparison phase, a user can pose textual descriptions or visual examples to find the desired textures. Furthermore, the query can be expressed as a logic composition of linguistic terms or Tamura feature values. The final similarity is then computed by aggregating each individual similarity through min-max composition rules. Experimental results reveal the proposed system is indeed effective. The retrieved images are perceptually satisfactory. The retrieval time is very fast.	algorithm;cluster analysis;content-based image retrieval;fuzzy clustering;fuzzy logic;maxima and minima	Hsin-Chih Lin;Chih-Yi Chiu;Shi-Nine Yang	2001		10.1145/500141.500223	fuzzy logic;computer vision;fuzzy clustering;computer science;machine learning;pattern recognition;information retrieval;semantic gap	AI	38.70681422818539	-60.656580571304985	60948
820ef5064bde58f6f052375099c6facdcdaf7866	influence of color on visual saliency in short videos	visual saliency;fusion process visual saliency short videos computational models static images static saliency maps;motion visual attention visual saliency color feature video;color feature;visual perception image capture image colour analysis video signal processing;motion;video;image color analysis visualization computational modeling videos gray scale observers color;visual attention	The architecture of computational models of visual attention designed for videos is generally the result of a direct extension of techniques dedicated to static images. These models try to extract the salient areas of dynamic scenes (i.e. areas that may capture visual attention) by fusing static saliency maps computed frame per frame with saliency maps independently obtained from dynamic features. The problem is that there is no evidence for assuming that visual saliency of videos can be accurately identified from such a fusion process. In addition, there is no guarantee that visual saliency is the same for still and dynamic scenes. Then we propose to investigate this issue for short videos from the perspective of color information that has been clearly identified as an important salient property in static images.	color;computation;computational model;map	Irina M. Ciortan;Éric Dinet;Alain Trémeau	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025231	computer vision;video;kadir–brady saliency detector;motion;multimedia;computer graphics (images)	Vision	38.33933966743293	-53.45456386825385	61065
99a898c0e45ebe96403c6d57afe678ef2517e88c	automatically discovering local visual material attributes	visualization plastics shape fabrics image recognition training glass;computer vision automatic local visual material attributes discovery locally recognizable material attributes crowdsourced perceptual material distances learned attributes;image recognition computer vision feature extraction	Shape cues play an important role in computer vision, but shape is not the only information available in images. Materials, such as fabric and plastic, are discernible in images even when shapes, such as those of an object, are not. We argue that it would be ideal to recognize materials without relying on object cues such as shape. This would allow us to use materials as a context for other vision tasks, such as object recognition. Humans are intuitively able to find visual cues that describe materials. Previous frameworks attempt to recognize these cues (as visual material traits) using fully-supervised learning. This requirement is not feasible when multiple annotators and large quantities of images are involved. In this paper, we derive a framework that allows us to discover locally-recognizable material attributes from crowdsourced perceptual material distances. We show that the attributes we discover do in fact separate material categories. Our learned attributes exhibit the same desirable properties as material traits, despite the fact that they are discovered using only partial supervision.	computer vision;crowdsourcing;encode;expect;humans;outline of object recognition;pixel;shape context;supervised learning	Gabriel Schwartz;Ko Nishino	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298979	computer vision;machine learning	Vision	36.69290995846611	-53.659976717189096	61096
7f090a7804c2332b4bf043780f657a8c847acc2d	image retrieval using by skin color and shape feature	snake algorithm;css curvature scale space;curvature scale space;skin color;distribution function;similarity;skin color distribution function;image retrieval	In this paper, we propose a image retrieval method using skin color feature and shape feature. Section by using a method of snake, consider color information. Section II, image retrieval using by ICSS(Improve Curvature Scale Space). As a result, we show that good results can be obtained by skin color and shape feature.		Jinyoung Park;Gye-Young Kim;Hyung-Il Choi	2007		10.1007/978-3-540-74472-6_86	color histogram;computer vision;similarity;image retrieval;computer science;distribution function;pattern recognition;mathematics;color balance	Vision	38.93889821879218	-60.194463005438756	61273
9fad890b6402d743ce70b698d0a55ccab2d019ad	a compressed-domain image filtering and re-ranking approach for multi-agent image retrieval	multi agent;compressed domain;perceptual hashing;image filtering and re ranking;image retrieval	For the limited transmission capacity and compressed images in the network environment, a compressed-domain image filtering and re-ranking approach for multi-agent image retrieval is proposed in this paper. Firstly, the distributed image retrieval platform with multi-agent is constructed by using Aglet development system, the lifecycle and the migration mechanism of agent is designed and planned for multi-agent image retrieval by using the characteristics of mobile agent. Then, considering the redundant image brought by distributed multi-agent retrieval, the duplicate images in distributed retrieval results are filtered based on the perceptual hashing feature extracted in the compressed-domain. Finally, weight-based hamming distance is utilized to re-rank the retrieval results. The experimental results show that the proposed approach can effectively filter the duplicate images in distributed image retrieval results as well as improve the accuracy and speed of compressed-domain image retrieval.	image retrieval	Jing Zhang;Zhenwei Li;Li Zhuo;Xin Liu;Ying Yang	2015	IJPRAI	10.1142/S0218001415520011	image texture;computer vision;feature detection;visual word;image processing;image retrieval;computer science;pattern recognition;automatic image annotation;information retrieval	Vision	38.48938025683768	-61.36484126854179	61365
b437468b3b345f345a80ab283e679186c123e9e5	shape feature encoding via fisher vector for efficient fall detection in depth-videos	curvature scale space;fall detection;shape contour;fisher vector encoding	FV can be considered as a generalization of the BoW. In other words, BoW is a particular case of the FV. The additional gradients improve the FV's performance greatly.Smaller codebooks can be used to construct the FV, which yields lower computational cost.FV performs well even with simple linear classifiers. Elderly people, who are living alone, are at great risk if a fall event occurred. Thus, automatic fall detection systems are in demand. Some of the early automatic fall detection systems such as wearable devices has a high cost and may cause inconvenience to the daily lives of the elderly people. In this paper, an improved depth-based fall detection system is presented. Our approach uses shape based fall characterization and a Support Vector Machines (SVM) classifier to classify falls from other daily actions. Shape based fall characterization is carried out with Curvature Scale Space (CSS) features and Fisher Vector (FV) encoding. FV encoding is used because it has several advantages against the Bag-of-Words (BoW) model. FV representation is robust and performs well even with simple linear classifiers. Extensive experiments on SDUFall dataset, which contains five daily activities and intentional falls from 20 subjects, show that encoding CSS features with FV encoding and a SVM classifier can achieve an up to 88.83% fall detection accuracy with a single depth camera. This classification rate is 2% more accurate than the compared approach. Moreover, an overall 64.67% accuracy is obtained for 6-class action recognition, which is about 10% more accurate than the compared approach.		Muzaffer Aslan;Abdulkadir Şengür;Yang Xiao;Haibo Wang;M. Cevdet Ince;Xin Ma	2015	Appl. Soft Comput.	10.1016/j.asoc.2014.12.035	computer vision;speech recognition;pattern recognition;mathematics	Crypto	31.847513531826003	-57.13168657950402	61514
1777aacdf961a62baa4d2b848744d7febb81f12d	high-order statistics of weber local descriptors for image representation	histograms;image recognition;feature extraction higher order statistics image classification image representation image texture probability transforms;high order statistics local descriptor micro structure parametric model weber s law;weber s law high order statistics local descriptor micro structure parametric model;transforms feature extraction higher order statistics image classification image representation image texture probability;high order statistics extraction weber local descriptors image representation highly discriminant visual feature extraction image classification applications robust local descriptor weber s law human perception differential excitation domain microtexton parametric probability process generative probability model hep 2 cell pattern recognition;visualization;vectors;image representation;feature extraction;image representation adaptation models vectors histograms visualization feature extraction image recognition;hep 2 cell pattern recognition high order statistics extraction weber local descriptors image representation highly discriminant visual feature extraction image classification applications robust local descriptor weber s law human perception differential excitation domain microtexton parametric probability process generative probability model;adaptation models	Highly discriminant visual features play a key role in different image classification applications. This study aims to realize a method for extracting highly-discriminant features from images by exploring a robust local descriptor inspired by Weber's law. The investigated local descriptor is based on the fact that human perception for distinguishing a pattern depends not only on the absolute intensity of the stimulus but also on the relative variance of the stimulus. Therefore, we firstly transform the original stimulus (the images in our study) into a differential excitation-domain according to Weber's law, and then explore a local patch, called micro-Texton, in the transformed domain as Weber local descriptor (WLD). Furthermore, we propose to employ a parametric probability process to model the Weber local descriptors, and extract the higher-order statistics to the model parameters for image representation. The proposed strategy can adaptively characterize the WLD space using generative probability model, and then learn the parameters for better fitting the training space, which would lead to more discriminant representation for images. In order to validate the efficiency of the proposed strategy, we apply three different image classification applications including texture, food images and HEp-2 cell pattern recognition, which validates that our proposed strategy has advantages over the state-of-the-art approaches.	computer vision;database;discriminant;excitation;extraction;frontotemporal dementia;inspiration function;pattern recognition;performance;sample variance;sturge-weber syndrome;texton;weber	Xian-Hua Han;Yen-Wei Chen;Gang Xu	2015	IEEE Transactions on Cybernetics	10.1109/TCYB.2014.2346793	computer vision;feature detection;visualization;feature extraction;computer science;machine learning;pattern recognition;histogram;mathematics	Vision	36.6214371563269	-58.458449748908656	61614
8c2fd2a755053eac88cdb87197aea0ef504c7c3a	on the robustness of fingerprint liveness detection algorithms against new materials used for spoofing		Fingerprint biometric systems may be deceived by attacks at sensor level that use fake fingers. A secure fingerprint scanner is required to possess the ability to determine if the image comes from a living individual or not. Recently, several liveness detection approaches have been proposed to address this problem. At present performances of the existing software-based solutions have been assessed with different sensors and using small data sets. Moreover, it is assumed that fake fingerprints are produced by adopting the same materials used for training the system. This paper looks at the cases where the test spoof finger is made by employing a material new for the fingerprint sensor. We propose an experimental comparison among the current fingerprint liveness detection approaches accomplished by adopting materials for training different than those used for testing. Experiments have been performed by using standard databases taken from the LivDet09 Competition.	algorithm;anomaly detection;biometrics;database;experiment;fingerprint recognition;liveness;performance;sensor;spoofing attack;wavelet	Emanuela Marasco;Carlo Sansone	2011			distributed computing;computer security	Security	30.38052834889475	-63.146385747586734	61739
41b1e470ad2f5758d3909632c51d72f9d3437ead	image captioning with triple-attention and stack parallel lstm		Abstract Image captioning aims to describe the content of images with a sentence. It is a natural way for people to express their understanding, but a challenging and important task from the view of image understanding. In this paper, we propose two innovations to improve the performance of such a sequence learning problem. First, we give a new attention method named triple attention (TA-LSTM) which can leverage the image context information at every stage of LSTM. Then, we redesign the structure of basic LSTM, in which not only the stacked LSTM but also the paralleled LSTM are adopted, called as PS-LSTM. In this structure, we not only use the stack LSTM but also use the parallel LSTM to achieve the improvement of the performance compared with the normal LSTM. Through this structure, the proposed model can ensemble more parameters on single model and has ensemble ability itself. Through numerical experiments, on the public available MSCOCO dataset, our final TA-PS-LSTM model achieves comparable performance with some state-of-the-art methods.	long short-term memory	Xinxin Zhu;Lixiang Li;Jing Liu;Ziyi Li;Haipeng Peng;Xinxin Niu	2018	Neurocomputing	10.1016/j.neucom.2018.08.069	machine learning;artificial intelligence;mathematics;sequence learning;monad (category theory);pattern recognition;closed captioning;sentence	NLP	24.91032222238899	-54.01984121885265	61749
1e54629703eae7b085e833f976fc8684f2c1bf72	comparison of combining methods of correlation kernels in kpca and kcca for texture classification with kansei information	kernel canonical correlation analysis;kernel principal component analysis;texture classification;canonical correlation analysis;principal component analysis;kernel method;combining classifiers;kansei information;correlation kernel;principal component	The authors consider combining correlations of different orders in kernel principal component analysis (kPCA) and kernel canonical correlation analysis (kCCA) with the correlation kernels. We apply combining methods, e.g., the sums of the correlation kernels, Cartesian spaces of the principal components or the canonical variates and the voting of kPCAs and kCCAs output and compare their performance in the classification of texture images. Further, we apply Kansei information on the images obtained through questionnaires to the public to kCCA and evaluate its effectiveness.		Yo Horikawa;Yujiro Ohnishi	2007		10.1007/978-3-540-73040-8_71	kernel embedding of distributions;kernel principal component analysis;computer science;machine learning;pattern recognition;mathematics;statistics;principal component analysis	AI	34.2713889808159	-58.36204323330734	61778
185e88da3d3c0aa104ba14a6bb4f5254aff8e8eb	face sketch recognition system: a content based image retrieval approach	databases;software;face recognition;feature extraction;face;algorithm design and analysis;forensics	Content Based Image Retrieval has been one of the most popular topics in the computer vision literature. CBIR offers the opportunity to research from a huge multimedia database and with appropriate methods the relevant collections of images that have characteristics similar to the case(s) of interest. In the forensic field, CBIR has many possible uses in crime fighting and has also been investigated as a potential image-based search technology. This paper presents a new use of CBIR approach in an important application that can assist law enforcement in solving many complicated crimes. It is matching forensic sketch with digital human face or face sketch recognition system.	computer vision;content-based image retrieval;sketch recognition	Salah Eddine Lahlali;Abdelalim Sadiq;Samir Mbarki	2016	2016 4th IEEE International Colloquium on Information Science and Technology (CiSt)	10.1109/CIST.2016.7805085	computer vision;computer science;data mining;face recognition grand challenge;multimedia;sketch recognition	Vision	36.090935818530404	-55.88721097994307	61893
7435f468b3a716bc1ac8a70b15b7801b48b1f8de	agglomerative clustering for feature point grouping	normalized mutual information scores agglomerative clustering feature point grouping semantic image segmentation image understanding planar homographies j linkage method multiple model estimation techniques jaccard measure frequency vector feature vector representation c linkage;vectors estimation educational institutions mutual information clustering algorithms robustness computational modeling;vectors image segmentation pattern clustering	The objective of this paper is to group feature points on different planes as a means of semantic image segmentation and understanding. The methodology is based on the ability to estimate planar homographies from grouped feature points spanning different unknown number of planes. This paper proposes an alternative to the J-linkage method, which was shown to have benefits in terms of accuracy over other multiple model estimation techniques. J-linkage is an agglomerative clustering technique that uses a set representation of support for a set of possible planar homographies and the Jaccard measure to determine the distance between support sets. The technique proposed in this paper uses a frequency vector to represent the support for a model. This formulation promotes clustering even in the presence of noise and prevents the order in which agglomerative clustering is performed from influencing the results. The feature vector representation requires an alternative distance measure to Jaccard to be exercised, that of cosine similarity. Hence, the method proposed here is called C-linkage. The results show that, compared to the J-linkage method, the proposed technique correctly classifies more points on each plane, and results in less over-segmentation while providing higher Normalized Mutual Information scores for a range of multiple model estimation problems on different datasets.	cluster analysis;cosine similarity;feature vector;file spanning;image segmentation;jaccard index;linkage (software);mutual information	Maria Scalzo-Cornacchia;Senem Velipasalar	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025903	correlation clustering;machine learning;pattern recognition;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering	Robotics	37.259449279276204	-56.87473318585045	61935
72a153bd77ff134e7da2dcc43af1c26f1f5d4108	suppression of uncertainties at emotional transitions - facial mimics recognition in video with 3-d model		Facial expression is of increasing importance for man-machine communication. It is expected that future human computer interaction systems even include emotions of the user. In this work we present an associative approach based on a multi-channel deconvolution for processing of face expression data derived from video sequences supported by a 3-D facial model generated with stereo support. Photogrammetric techniques are applied to determine real world geometric measures and to create a feature vector. Standard classification is used to discriminate between a limited number of mimics, but often fails at transitions from one detected emotion state to another. The proposed associative approach reduces ambiguities at the transitions between different classified emotions. This way, typical patterns of facial expression change is considered.	3d modeling;artificial neural network;color;deconvolution;feature data;feature detection (computer vision);feature detection (web development);feature vector;human computer;human–computer interaction;microsoft outlook for mac;nonlinear system;optical flow;photogrammetry;teuvo kohonen;zero suppression	Gerald Krell;Robert Niese;Ayoub Al-Hamadi;Bernd Michaelis	2010			computer vision;speech recognition	AI	29.557737598073274	-59.1444181430869	61969
ba18c6439bf7a1f7e3534ea03e89393c7a372a8f	a new visual speech recognition approach for rgb-d cameras		Visual speech recognition remains a challenging topic due to various speaking characteristics. This paper proposes a new approach for lipreading to recognize isolated speech segments (words, digits, phrases, etc.) using both of 2D image and depth data. The process of the proposed system is divided into three consecutive steps, namely, mouth region tracking and extraction, motion and appearance descriptors (HOG and MBH) computing, and classification using the Support Vector Machine (SVM) method. To evaluate the proposed approach, three public databases (MIRACL-VC, Ouluvs, and CUAVE) were used. Speaker dependent and speaker independent settings were considered in the evaluation experiments. The obtained recognition results demonstrate that lipreading can be performed effectively, and the proposed approach outperforms recent works in the literature for the speaker dependent setting while being competitive for the speaker independent setting.	3d computer graphics;database;experiment;lr parser;speech recognition;support vector machine;thumbnail	Ahmed Rekik;Achraf Ben-Hamadou;Walid Mahdi	2014		10.1007/978-3-319-11755-3_3	computer vision	Vision	31.27055979003846	-60.10084391059778	62002
1371c5061cdfb3e0ba4ea62f6fa0a59328933378	a multi-layer contrast analysis method for texture classification based on lbp	contrast analysis;texture classification;lbp;3dlbp;illuminate invariant	Texture classification is one of the important fields in pattern recognition and machine vision research. LBP method,13–15 proposed by Ojala, can be used to classify texture images effectively. And the LBP method has rotation-invariant, illumination-invariant, multi-resolution characteristics. But, since the contrast is not considered between neighbor pixels, the correct classification rate produced by this method has been remarkably influenced by light source type and light source orientation. The LMLCP (Local Multiple Layer Contrast Pattern) method, proposed by this paper, maps the contrast value between two near pixels to a rank value, which represent a relative contrast value range, and computes the statistic histogram referring to the work in LBP method. The LMLCP method can bring out the rapid expansion of feature dimension, so a special feature encoding method used in 3DLBP6 is adopted by this paper. The experiment, which is built based on Outex_TC_00012,12 demonstrates that the LMLCP can evidently make a more accurate classification rate than LBP method.	local binary patterns	Hengxin Chen;Yuan Yan Tang;Bin Fang;Patrick Shen-Pei Wang	2011	IJPRAI	10.1142/S0218001411008518	computer vision;local binary patterns;componential analysis;machine learning;pattern recognition;mathematics	Vision	36.54350529624325	-59.10513752123807	62060
93a451b5efa21887dae43eb73b4793ae3d184138	a study on the effectiveness of different patch size and shape for eyes and mouth detection	patch size;image processing methods;pattern recognition;haar wavelet transform;template matching	Template matching is one of the simplest methods used for eyes and mouth detection. However, it can be modified and extended to become a powerful tool. Since the patch itself plays a significant role in optimizing detection performance, a study on the influence of patch size and shape is carried out. The optimum patch size and shape is determined using the proposed method. Usually, template matching is also combined with other methods in order to improve detection accuracy. Thus, in this paper, the effectiveness of two image processing methods i.e. grayscale and Haar wavelet transform, when used with template matching are analyzed.	grayscale;haar wavelet;image processing;template matching;wavelet transform	Lim Huey Charn;Liyana Nuraini Rasid;Shahrel Azmin Suandi	2010	CoRR		computer vision;speech recognition;template matching;computer science;pattern recognition;mathematics	AI	34.134587163994084	-62.141863999119536	62119
2946d3d480608a84d4f4ef580daea7b40794bf4b	trainable look-up-tables versus neural networks for real-time colour classification	look up table;real time;neural network	The pixel-wise classification of CCD colour images into previously learned colour classes at video-rate is a demanding vision task, both with regard to the complicated cluster shapes encountered in natural scenes and to the required computing power for real-time operation. We discuss the use of a perceptron Neural Network and propose an alternative simple and low-cost classifier based on approbriately trained look-up-tables. Two different learning rules for the supervised training of this LUT classifier are presented. This LUT classifier shows all the positive features of a 3-layer perceptron Neural Network, but performs 60.000 times faster then the Neural Network PC-simulation.	neural networks;real-time transcription	Robert Massen;Joachim Gässler;Pia Böttcher;Wolfgang Reichelt	1990		10.1007/978-3-642-84305-1_46	speech recognition;artificial intelligence;machine learning;time delay neural network	ML	27.12797302666387	-57.96493941689516	62291
4233990ff88041f9f13f44ec8c62e8d2d1be6d2b	wall patch-based segmentation in architectural floorplans	architectural floorplans;patch based image segmentation;probability;image segmentation;architectural floorplans graphics recognition patch based image segmentation;probability architectural cad feature extraction image segmentation;feature extraction;topology feature extraction image segmentation vocabulary computer vision visualization text analysis;architectural cad;probability wall patch based segmentation architectural floorplans feature extraction;graphics recognition	Segmentation of architectural floor plans is a challenging task, mainly because of the large variability in the notation between different plans. In general, traditional techniques, usually based on analyzing and grouping structural primitives obtained by vectorization, are only able to handle a reduced range of similar notations. In this paper we propose an alternative patch-based segmentation approach working at pixel level, without need of vectorization. The image is divided into a set of patches and a set of features is extracted for every patch. Then, each patch is assigned to a visual word of a previously learned vocabulary and given a probability of belonging to each class of objects. Finally, a post-process assigns the final label for every pixel. This approach has been applied to the detection of walls on two datasets of architectural floor plans with different notations, achieving high accuracy rates.	automatic vectorization;engineering drawing;image segmentation;markov random field;patch (computing);pixel;spatial variability;visual word;vocabulary	Lluís-Pere de las Heras;Joan Mas Romeu;Gemma Sánchez;Ernest Valveny	2011	2011 International Conference on Document Analysis and Recognition	10.1109/ICDAR.2011.256	computer vision;feature extraction;computer science;machine learning;segmentation-based object categorization;pattern recognition;probability;image segmentation;scale-space segmentation;statistics	Vision	36.88781567193801	-65.05784708096824	62335
2fffffa99ac15427d8b712d8c430631a1b4a7649	global image feature extraction using slope pattern spectra	image features;mathematical morphology;texture classification;shape analysis;spectrum;presentation;pattern spectra;texture analysis;size distribution;image representation;feature extraction;integral image;satellite image;image analysis	A novel algorithm inspired by the integral image representation to derive an increasing slope segment pattern spectrum (called the Slope Pattern Spectrum for convenience), is proposed. Although many pattern spectra algorithms have their roots in mathematical morphology, this is not the case for the proposed algorithm. Granulometries and their resulting pattern spectra are useful tools for texture or shape analysis in images since they characterize size distributions. Many applications such as texture classification and segmentation have demonstrated the importance of pattern spectra for image analysis. The Slope Pattern Spectra algorithm extracts a global image signature from an image based on increasing slope segments. High Steel Low Alloy (HSLA) steel and satellite images are used to demonstrate that the proposed algorithm is a fast and robust alternative to granulometric methods. The experimental results show that the proposed algorithm is efficient and has a faster execution time than Vincent’s linear granulometric technique.	algorithm;computation;feature extraction;granulometry (morphology);grayscale;image analysis;mathematical morphology;rca spectra 70;run time (program lifecycle phase);shape analysis (digital geometry)	Ignace Tchangou Toudjeu;Barend J. van Wyk;Michaël A. van Wyk;Frans van den Bergh	2008		10.1007/978-3-540-69812-8_63	image texture;spectrum;computer vision;feature detection;image analysis;mathematical morphology;feature extraction;computer science;summed area table;machine learning;pattern recognition;shape analysis;mathematics;feature	Vision	37.93546646919849	-60.090288522231894	62379
07c3cdbbb399aa169579f2998bc442af0d60973d	quality index analysis on camera-based r-peak identification considering movements and light illumination	camera-based system;quality index analysis;r-peak identification	This paper presents a quality index (QI) analysis on R-peak extracted by a camera system considering movements and light illumination. Here, the proposed camera system is compared with a reference system named Shimmer PPG sensor. The study considers five test subjects with a 15 minutes measurement protocol, where the protocol consists of several conditions. The conditions are: normal sittings, head movements i.e., up/down/left/right/forward/backword, with light on/off and with moving flash on/off. A percentage of corrected R-peaks are calculated based on time difference in milliseconds (MS) between the R-peaks extracted both from camera-based and sensor-based systems. A comparison results between normal, movements, and lighting condition is presented as individual and group wise. Furthermore, the comparison is extended considering gender and origin of the subjects. According to the results, more than 90% R-peaks are correctly identified by the camera system with ±200 MS time differences, however, it decreases with while there is no light than when it is on. At the same time, the camera system shows more 95% accuracy for European than Asian men.	body mass index;extraction;global illumination;head and neck neoplasms;movement;name	Mobyen Uddin Ahmed;Hamidur Rahman;Shahina Begum	2018	Studies in health technology and informatics	10.3233/978-1-61499-868-6-84		HCI	28.610522925270594	-65.62250441043489	62457
ea80bfaf8525317bde0dacf57ace2b8610c9e719	learnable histogram: statistical context features for deep neural networks		Statistical features, such as histogram, Bag-of-Words (BoW) and Fisher Vector, were commonly used with hand-crafted features in conventional classification methods, but attract less attention since the popularity of deep learning methods. In this paper, we propose a learnable histogram layer, which learns histogram features within deep neural networks in end-to-end training. Such a layer is able to back-propagate (BP) errors, learn optimal bin centers and bin widths, and be jointly optimized with other layers in deep networks during training. Two vision problems, semantic segmentation and object detection, are explored by integrating the learnable histogram layer into deep networks, which show that the proposed layer could be well generalized to different applications. In-depth investigations are conducted to provide insights on the newly introduced layer.	artificial neural network;bag-of-words model in computer vision;deep learning;end-to-end principle;expressive power (computer science);loss function;neural networks;object detection;software deployment	Zhe Wang;Hongsheng Li;Wanli Ouyang;Xiaogang Wang	2016		10.1007/978-3-319-46448-0_15	speech recognition;machine learning;pattern recognition	ML	24.647858056385456	-52.81657352743768	62541
ebd074321306418ad4282a7cb98436ea8e5c942a	automatic features detection for overlapping face images on their 3d range models	feature detection;image colour analysis feature extraction face recognition;face detection computer vision shape color lips eyes nose humans cameras costs;face recognition;color information automatic features detection overlapping face images 3d range models 2d color images shape information;image colour analysis;feature extraction;color image	We describe an algorithm for the automatic features detection in 2D color images of human faces. The algorithm proceeds with subsequent refinements. First, it identifies the sub-images containing each feature (eyes, nose and lips). Afterwards, it processes the single features separately by a blend of techniques which use both color and shape information. The method does not require any manual setting or operator intervention.	algorithm;color	Raffaella Lanzarotti;Paola Campadelli;N. Alberto Borghese	2001		10.1109/ICIAP.2001.957028	facial recognition system;color histogram;computer vision;face detection;feature detection;speech recognition;object-class detection;color normalization;color image;feature extraction;computer science;pattern recognition;feature detection;feature	Vision	39.0470632406283	-56.21542320396602	62639
84b8eb3e09fde983c8c504931250931b100a1be8	adaptive texture image retrieval in transform domain	database indexing;image retrieval wavelet transforms gabor filters image texture analysis image databases discrete cosine transforms image analysis algorithm design and analysis performance analysis spatial databases;brodatz texture database transform domain retrieval performance adaptive integration hybrid scheme adaptive retrieval scheme texture image indexing dynamic adaptation texture patterns;image databases;adaptive integration;retrieval performance;image indexing;gabor filters;indexing terms;adaptive retrieval scheme;transform domain;image texture;wavelet transforms;adaptive filters;hybrid scheme;discrete cosine transforms;feature extraction;texture patterns;spatial databases;transforms;performance analysis;image analysis;image texture analysis;dynamic adaptation;brodatz texture database;algorithm design and analysis;database indexing image texture image retrieval feature extraction adaptive filters transforms;texture image indexing;image retrieval	A large number of algorithms have been proposed for texture image retrieval and analysis. While each algorithm has its own characteristics and is suitable for indexing some particular categories of images, there is a lacking of an effective strategy to integrate these algorithms to maximize the retrieval performance based on the algorithms available. Our first step in this direction is to build up a texture image retrieval system which adaptively chooses the “right” transform to query texture database. Experiments on the Brodatz texture set show that the adaptive image retrieval system significantly outperforms the commonly used single-algorithm based ones.	algorithm;image retrieval;overhead (computing);precomputation	Bin Zhang;Catalin I. Tomai;Aidong Zhang	2002		10.1109/ICME.2002.1035622	adaptive filter;image texture;database index;algorithm design;computer vision;visual word;image analysis;index term;feature extraction;image retrieval;computer science;pattern recognition;texture compression;texture filtering;information retrieval;wavelet transform	Vision	38.98641550958352	-61.280628555579646	62969
8dfcb3f221b7dfdc639354380140ab20c2e5c5f6	face gender classification: a statistical study when neutral and distorted faces are combined for training and testing purposes	cross database experimen;global local representation;institute;gender classification;image;cross database experiment;info eu repo semantics article;face analysis;init;imaging;new;technologies	This paper presents a thorough study of gender classification methodologies performing on neutral, expressive and partially occluded faces, when they are used in all possible arrangements of training and testing roles. A comprehensive comparison of two representation approaches (global and local), three types of features (grey levels, PCA and LBP), three classifiers (1-NN, PCA+LDA and SVM) and two performance measures (CCR and d') is provided over single- and cross-database experiments. Experiments revealed some interesting findings, which were supported by three non-parametric statistical tests: when training and test sets contain different types of faces, local models using the 1-NN rule outperform global approaches, even those using SVM classifiers; however, with the same type of faces, even if the acquisition conditions are diverse, the statistical tests could not reject the null hypothesis of equal performance of global SVMs and local 1-NNs.		Yasmina Andreu;Pedro García-Sevilla;Ramón Alberto Mollineda	2014	Image Vision Comput.	10.1016/j.imavis.2013.11.001	medical imaging;computer vision;computer science;artificial intelligence;machine learning;image;data mining;new;statistics;technology	Vision	35.64006760623791	-59.35012924821239	63021
291021acbe2ff2aaf418c15fc8323e7a4fa1ca08	deep learning system for automatic license plate detection and recognition		The detection and recognition of a vehicle License Plate (LP) is a key technique in most of the applications related to vehicle movement. Moreover, it is a quite popular and active research topic in the field of image processing. Different methods, techniques and algorithms have been developed to detect and recognize LPs. Nevertheless, due to the LP characteristics that vary from one country to another in terms of numbering system, colors, language of characters, fonts and size. Further investigations are still needed in this field in order to make the detection and recognition process very efficient. Although this domain has been covered by a lot of researchers, various existing systems operate under well-defined and controlled conditions. For example, some frameworks require complicated hardware to make good quality images or capture images from vehicles with very slow speed. For this reason the detection and recognition of LPs in different conditions and under several climatic variations remains always difficult to realize with good results. For that, we present in this paper an automatic system for LP detection and recognition based on deep learning approach, which is divided into three parts: detection, segmentation, and character recognition. To detect an LP, many pretreatment steps should be made before applying the first Convolution Neural Network (CNN) model for the classification of plates / non-plates. Subsequently, we apply a few pre-processing steps to segment the LP and finally to recognize all the characters in upper case format (A-Z) and digits (0-9), using a second CNN model with 37 classes. The performance of the suggested system is tested on two datasets which contain images under various conditions, such as poor picture quality, image perspective distortion, bright day, night and complex environment. A great percentage of the results show the accuracy of the suggested system.	algorithm;artificial neural network;color;convolution;deep learning;distortion;image processing;image quality;mathematical morphology;optical character recognition;preprocessor;real-time clock;real-time computing;smartphone;tablet computer;tensorflow;thresholding (image processing)	Zied Selmi;Mohamed Ben Halima;Adel M. Alimi	2017	2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2017.187	convolutional neural network;computer science;image processing;artificial intelligence;perspective distortion;pattern recognition;computer vision;deep learning;image segmentation;image quality;numbering;feature extraction	Vision	34.678730481923864	-63.79277012969699	63078
4471852ae2290b5a2382faf27984a800ac60aec8	face liveness detection from a single image via diffusion speed model	support vector machines face recognition;smart phones;feature extraction;malicious attacks face liveness detection diffusion speed model face recognition face verification systems linear svm classifier;face;face liveness detection spoofing diffusion speed total variation flow local speed pattern;lighting;tv;face lighting tv smart phones feature extraction videos security;security;videos	Spoofing using photographs or videos is one of the most common methods of attacking face recognition and verification systems. In this paper, we propose a real-time and nonintrusive method based on the diffusion speed of a single image to address this problem. In particular, inspired by the observation that the difference in surface properties between a live face and a fake one is efficiently revealed in the diffusion speed, we exploit antispoofing features by utilizing the total variation flow scheme. More specifically, we propose defining the local patterns of the diffusion speed, the so-called local speed patterns, as our features, which are input into the linear SVM classifier to determine whether the given face is fake or not. One important advantage of the proposed method is that, in contrast to previous approaches, it accurately identifies diverse malicious attacks regardless of the medium of the image, e.g., paper or screen. Moreover, the proposed method does not require any specific user action. Experimental results on various data sets show that the proposed method is effective for face liveness detection as compared with previous approaches proposed in studies in the literature.	adams oliver syndrome;attempt;autostereogram;data general aos;encode;face;facial recognition system;high- and low-level;inspiration function;liveness;mobile device;real-time clock;real-time computing;real-time transcription;verification of theories;photograph	Wonjun Kim;Sungjoo Suh;Jae-Joon Han	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2422574	face;computer vision;face detection;speech recognition;object-class detection;feature extraction;computer science;information security;lighting;computer security	Vision	31.600993555438624	-62.50684885528442	63254
bfb19db9586e1e7b9521fb1875ef6495224b5ff1	human object classification based on nonsubsampled contourlet transform combined with zernike moment		The surveillance systems are more and more popular because of the security needs, but the traditional ones do not meet human’s expectation. This paper proposes the algorithm to classify objects mainly based on their contour property which are represented by the amplitude of zernike moment on nonsubsampled contourlet transform of a binary contour image. This feature shows promising results by just a simple association with the aspect ratio but gives high accuracy. The aspect ratio helps contour feature in case that the image is too blurred to extract the object’s contour. It also plays as a weak filter with nearly no more computational cost except for a division to support contour feature when applying gentle boost algorithm.	contourlet	Luu The Phuong;Nguyen Thanh Binh	2015		10.1007/978-3-319-29236-6_21	computer vision	Vision	35.967903947743906	-58.95610703699922	63500
b152694cf712e8def3b2b9ae5c01215179c9388b	document authentication using printing technique features and unsupervised anomaly detection	printing;digital forensics;document handling;authorisation;printing technique features;unsupervised anomaly detection document authentication fraud detection printing technique features;unsupervised anomaly detection;document authentication document detection unsupervised anomaly detection feature extraction inkjet printed pages laser printed pages edge roughness printed character edges printing quality printers forgery automatic identification printing technique features;document authentication;feature extraction;feature extraction image edge detection printers printing contracts standards optical character recognition software;text detection;text detection authorisation character recognition digital forensics document handling feature extraction printing;character recognition;fraud detection	Automatically identifying that a certain page in a set of documents is printed with a different printer than the rest of the documents can give an important clue for a possible forgery attempt. Different printers vary in their produced printing quality, which is especially noticeable at the edges of printed characters. In this paper, a system using the difference in edge roughness to distinguish laser printed ages from inkjet printed pages is presented. Several feature extraction methods have been developed and evaluated for that purpose. In contrast to previous work, this system uses unsupervised anomaly detection to detect documents printed by a different printing technique than the majority of the documents among a set. This approach has the advantage that no prior training using genuine documents has to be done. Furthermore, we created a dataset featuring 1200 document images from different domains (invoices, contracts, scientific papers) printed by 7 different inkjet and 13 laser printers. Results show that the presented feature extraction method achieves the best outlier rank score in comparison to state-of-the-art features.	anomaly detection;authentication;edge detection;feature extraction;image resolution;paging;preprocessor;printer (computing);printing;scientific literature;unsupervised learning	Johann Gebhardt;Markus Goldstein;Faisal Shafait;Andreas Dengel	2013	2013 12th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2013.102	speech recognition;feature extraction;computer science;digital forensics;machine learning;data mining;authorization;world wide web	Web+IR	36.13829487877395	-64.29971956504261	63526
7fa0aea15f33a48e0355fd08078ab7f6dceec8df	beauty product image retrieval based on multi-feature fusion and feature aggregation		We propose a beauty product image retrieval method based on multi-feature fusion and feature aggregation. The key idea is representing the image with the feature vector obtained by multi-feature fusion and feature aggregation. VGG16 and ResNet50 are chosen to extract image features, and Crow is adopted to perform deep feature aggregation. Benefited from the idea of transfer learning, we fine turn VGG16 on the Perfect-500K data set to improve the performance of image retrieval. The proposed method won the third price in Perfect Corp. Challenge 2018 with the best result 0.270676 mAP. We released our code on GitHub: https://github.com/wangqi12332155/ACMMM-beauty-AI-challenge.	control theory;cosine similarity;feature vector;high-level programming language;image retrieval;matrix multiplication	Qi Wang;Abdul Salam Qannus;Kai Xu;Wenyin Liu;Liang Lei	2018		10.1145/3240508.3266431	computer vision;feature (computer vision);beauty;transfer of learning;feature vector;artificial intelligence;fusion;image retrieval;computer science	Vision	31.116725012471395	-56.152668311362305	63725
95182002081271ddea5bd9581fd89adc4bd133b8	optimization of road sign detection based on colour information for ocr	optimization;highway engineering;image analysis;image processing;color;optical character recognition		optical character recognition	Eric Gambardella;Shan Fu	2002			artificial intelligence;computer vision;pattern recognition;computer science	Vision	37.31693247668649	-65.2434505189105	63758
6354540c96d7f5e17226594b09ff73a51802cfd9	detection of curved text path based on the fuzzy curve-tracing (fct) algorithm	fuzzy c means algorithm;cluster algorithm;document image analysis;cluster centers curved text path detection fuzzy curve tracing algorithm artistic documents computer algorithms text extraction fuzzy curve tracing algorithm character pixels fuzzy c means algorithm;text extraction;fuzzy logic;feature extraction;character recognition fuzzy logic feature extraction;character recognition;clustering algorithms iterative algorithms image analysis character recognition text analysis optical character recognition software graphics histograms data mining algorithm design and analysis	Artistic documents often contain text along curved paths. Commonly used computer algorithms for text extraction, which only deal with text along straight lines, cannot be employed to analyze these artistic documents. In this paper, we solve this problem using the f u u y curvetracing algorithm. In our method, the character pixels are grouped based on the fuuy c-means algorithm to reduce the amount of data. Then the cluster centers are connected to form the initial curve representing the text path. Finally the character pixels are clustered again under the constraint that the path must be smooth. Results from several experiments are presented to show the effectiveness of our method.	algorithm;experiment;international symposium on fundamentals of computation theory;pixel	Hong Yan	2001		10.1109/ICDAR.2001.953796	fuzzy logic;computer vision;feature extraction;fuzzy classification;computer science;machine learning;pattern recognition;data mining	AI	35.87717848429479	-66.01851997297325	63765
13e3f317a775fe0a956f2541b3ebc0cc8d38309a	a static video summarization method based on hierarchical clustering	hierarchical clustering;cluster algorithm;video summarization;video analysis;minimum spanning tree	Video summarization is a simplification of video content for compacting the video information. The video summarization problem can be transformed to a clustering problem, in which some frames are selected to saliently represent the video content. In this work, we use a graph-theoretic divisive clustering algorithm based on construction of a minimum spanning tree to select video frames without segmenting the video into shots or scenes. Experimental results provides a visually comparison between the new approach and other popular algorithms from the literature, showing that the new algorithm is robust and efficient.	algorithm;automatic summarization;cluster analysis;computation;digital video;expect;file spanning;graph theory;hierarchical clustering;key frame;level of detail;minimum spanning tree;redundancy (engineering)	Silvio Jamil Ferzoli Guimarães;Willer Gomes	2010		10.1007/978-3-642-16687-7_11	computer science;minimum spanning tree;automatic summarization;machine learning;pattern recognition;hierarchical clustering;block-matching algorithm;multimedia	Vision	38.77052899820125	-52.39408129262916	63812
fa7f239cf21449183a14e2bb819caec14024649c	face image retrieval and annotation based on two latent semantic spaces in fiars	symbolic feature;image retrieval face space technology indexing information retrieval indium tin oxide information science image databases spatial databases visual databases;symbolic feature face image retrieval image annotation fiars latent semantic indexing visual feature;image annotation;visual feature;face recognition;indexing;latent semantic indexing;visual features;fiars;face image retrieval;semantic space;visual databases face recognition image retrieval indexing;visual databases;image retrieval	This paper describes face image retrieval and annotation based on latent semantic indexing in FIARS. Two latent semantic spaces are constructed from visual and symbolic features to develop these mechanisms. These features are corresponding to lengths of some places of a face and its parts, and keywords, respectively. One latent semantic space is constructed from visual features, the other space is constructed from both features. The former space is used for retrieving similar face images to a given face image, and the latter is used for seeking keywords to the given face image. Moreover, two types of visual futures are defined. One is specified in terms of the lengths of face parts, and the other in terms of points on the outlines of a face and its parts. As an experiment, recall and precision ratios of retrieved face images are measured from the viewpoint of whether similar face images are retrieved, and the ratios of retrieved keywords are measured using two types of the visual features from the viewpoint of whether the retrieved keywords are suitable for the given face image. To evaluate efficiency, not only face images which are stored in the database of the system, but also new face images are given as queries	futures and promises;image retrieval;java annotation;latent semantic analysis;precision and recall;spaces	Hideaki Ito;Hiroyasu Koshimizu	2006	Eighth IEEE International Symposium on Multimedia (ISM'06)	10.1109/ISM.2006.77	computer vision;search engine indexing;latent semantic indexing;visual word;object-class detection;image retrieval;computer science;pattern recognition;automatic image annotation;information retrieval	Vision	38.77152176506145	-58.127372884347146	63907
362becff39b3305f2329638775e1650626828b31	nonlinearity reduction of manifolds using gaussian blur for handshape recognition based on multi-dimensional grids	gaussian blurring;image processing;computer vision;principal component analysis;multi stage hierarchy;multi dimensional grids	This paper presents a hand-shape recognition algorithm based on using multi-dimensional grids (MDGs) to divide the feature space of a set of hand images. Principal Component Analysis (PCA) is used as a feature extraction and dimensionality reduction method to generate eigenspaces from example images. Images are blurred by convolving with a Gaussian kernel as a low pass filter. Image blurring is used to reduce the nonlinearity in the manifolds within the eigenspaces where MDG structure can be used to divide the spaces linearly. The algorithm is invariant to linear transformations like rotation and translation. Computer generated images for different hand-shapes in Irish Sign Language are used in testing. Experimental results show accuracy and performance of the proposed algorithm in terms of blurring level and MDG size.	algorithm;computer-generated holography;convolution;dimensionality reduction;feature extraction;feature vector;gaussian blur;invariant (computer science);low-pass filter;nonlinear system;principal component analysis;test set	Mohamed Farouk;Alistair Sutherland;Amin Shokry	2013			computer vision;image processing;computer science;theoretical computer science;machine learning;gaussian blur;mathematics;principal component analysis	Vision	34.848537175239116	-59.161303697890595	63946
c7ee59bd1004153f5b6e44bb34ff8de7f2b35fba	extraction of place-name from natural scenes	natural scenes character recognition feature extraction;digital camera;mobile phone;feature extraction;natural scene images character recognition place names signboards;layout character recognition data mining testing natural languages image recognition digital cameras pixel mobile handsets licenses;character recognition;natural scenes	This paper proposes an experimental character recognition system to recognise place-names written on signboards. Recently, mobile phones with digital cameras and handy digital cameras have be come popular, so we think this system is useful. In experiments, we tested a total of 112 natural scene images with 320 characters. We obtained a correct character recognition rate of 99% and a placename recognition rate of 98% (with a rejection rate 2%).	digital camera;experiment;handy board;mobile phone;optical character recognition;rejection sampling;signage	Takuma Yamaguchi;Yasuaki Nakano	2002		10.1109/IWFHR.2002.1030916	computer vision;speech recognition;feature extraction;intelligent character recognition;computer science;machine learning	Vision	34.950732792333426	-65.27835162740843	63977
649b52fcdec4dd1a0492ab226d7aeb9f3dad29ab	image categorization with spatial mismatch kernels	image classification;bag of words;kernel method;computer science;kernel;clustering algorithms;spatial dependence;linear discriminant analysis;block clustering;regular grid;layout;image analysis;string kernel;support vector machine;support vector machines	This paper presents a new class of 2D string kernels, called spatial mismatch kernels, for use with support vector machine (SVM) in a discriminative approach to the image categorization problem. We first represent images as 2D sequences of those visual keywords obtained by clustering all the blocks that we divide images into on a regular grid. Through decomposing each 2D sequence into two parallel 1D sequences (i.e. the row-wise and column-wise ones), our spatial mismatch kernels can then measure 2D sequence similarity based on shared occurrences of k-length 1D subsequences, counted with up to m mismatches. While those bag-of-words methods ignore the spatial structure of an image, our spatial mismatch kernels can capture the spatial dependencies across visual keywords within the image. Experiments on the natural and histological image databases then demonstrate that our spatial mismatch kernel methods can achieve superior results.	bag-of-words model;categorization;cluster analysis;database;experiment;homology (biology);image retrieval;kernel method;regular grid;support vector machine	Zhiwu Lu;Horace Ho-Shing Ip	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206861	support vector machine;computer vision;computer science;machine learning;pattern recognition;mathematics;biclustering	Vision	35.98172467200032	-57.24627344214704	64095
c72e13581cc69ab9cf59fb3d41015a67de68306e	deep learning lane marker segmentation from automatically generated labels		Reliable lane detection is a fundamental necessity for driver assistance, driver safety functions and fully automated vehicles. Based on other detection and classification tasks, deep learning based methods are likely to yield the most accurate outputs for detecting lane markers, but require vast amounts of labeled data. We propose to train a deep neural network for detecting lane markers in camera images without manually labeling any images. To achieve this, we project high definition maps for automated driving into our image and correct for misalignments due to inaccuracies in localization and coordinate frame transformations. The corrections are performed by calculating the offset between features within our map and detected ones in the images. By using detections in the actual image for refining the projections, our labels become close to pixel perfect. After a fast, visual quality check, our projected lane markers can be used for training a fully convolutional network to segment lane markers in images. A single worker can easily generate 20,000 of those labels within a single day. Our fully convolutional network is trained only on automatically generated labels. All of our detections are based solely on gray-scale mono camera inputs without any additional information. The resulting network regularly detects clean lane markers at distances of around 150 meters on a 1 Megapixel camera.	artificial neural network;autonomous car;convolutional neural network;deep learning;display resolution;embedded system;freeway;grayscale;internationalization and localization;map;map projection;pixel perfect;strips;sensor;software deployment	Karsten Behrendt;Jonas Witt	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8202238	labeled data;deep learning;artificial neural network;computer vision;driver safety;artificial intelligence;computer science;pixel;offset (computer science);segmentation	Robotics	27.853858119062988	-54.28582022145271	64110
ff945289df439c64dfeab1d4000c02550fc8c141	exploring the optimal visual vocabulary sizes for semantic concept detection	pattern clustering;image processing;support vector machines;information retrieval;semantic concept detection optimal visual vocabulary sizes bag of visual words feature representation bovw svm classification content based concept detection visual categorization vv unsupervised clustering assignment modes local features opponentsift descriptor surf feature soft assignment mode χ2 laplace rbf kernels;image classification;kernel semantics visualization feature extraction support vector machines tv vocabulary;radial basis function networks;machine learning;image representation;feature extraction;transforms;transforms feature extraction image classification image representation object detection pattern clustering radial basis function networks support vector machines;algorithms;digital video;object detection	The framework based on the Bag-of-Visual-Words (BoVW) feature representation and SVM classification is popularly used for generic content-based concept detection or visual categorization. However, visual vocabulary (VV) size, one important factor in this framework, is always chosen differently and arbitrarily in previous work. In this paper, we focus on investigating the optimal VV sizes depending on other components of this framework which also govern the performance. This is useful as a default VV size for reducing the computation cost. By unsupervised clustering, a series of VVs covering a wide range of sizes are evaluated under two popular local features, three assignment modes, and four kernels on two different-scale benchmarking datasets respectively. These factors are also evaluated. Experimental results show that best VV sizes vary as these factors change. However, the concept detection performance usually improves as the VV size increases initially, and then gains less, or even deteriorates if larger VVs are used since overfitting occurs. Overall, VVs with sizes ranging from 1024 to 4096 achieve best performance with higher probability when compared with other-size VVs. With regard to the other factors, experimental results show that the OpponentSIFT descriptor outperforms the SURF feature, and soft assignment mode yields better performance than binary and hard assignment. In addition, generalized RBF kernels such as X2 and Laplace RBF kernels are more appropriate for semantic concept detection with SVM classification.	bag-of-words model in computer vision;bitwise operation;categorization;cluster analysis;computation;experiment;information access;kernel (operating system);overfitting;radial basis function;speeded up robust features;verification and validation;visual descriptor;vocabulary	Jinlin Guo;Zhengwei Qiu;Cathal Gurrin	2013	2013 11th International Workshop on Content-Based Multimedia Indexing (CBMI)	10.1109/CBMI.2013.6576565	support vector machine;computer vision;contextual image classification;image processing;feature extraction;computer science;machine learning;pattern recognition	Vision	35.44826013640147	-56.472610726740044	64221
45a9237a3fa3f5e1ff41935d9dfb683b9770ffe7	block-wise two-directional 2dpca with ensemble learning for face recognition	face recognition;principal component analysis pca;feature extraction;two dimensional pca 2dpca	Two-Dimensional Principal Component Analysis (2DPCA) is a well-known feature extraction method for face recognition. One of the main drawbacks of this method, in comparison with the vector-based PCA, is that it needs many more coefficients to represent the feature matrix of an image. Two-Directional 2DPCA ((2D)^2PCA), proposed in the literature, attempts to alleviate this problem. However, it fails to improve the recognition accuracy of 2DPCA. In addition, (2D)^2PCA follows a global feature extraction approach that might fail to preserve some important local features. In this paper, we propose Block-Wise (2D)^2PCA to enhance the performance of (2D)^2PCA by preserving the local informative variations. On average, the feature matrices produced by the proposed method and those formed by (2D)^2PCA are about the same size. However, our experiments on four face recognition databases indicate that our method is superior to (2D)^2PCA in terms of the recognition accuracy.	ensemble learning;facial recognition system	Ali Mashhoori;Mansoor Zolghadri Jahromi	2013	Neurocomputing	10.1016/j.neucom.2012.12.005	facial recognition system;feature vector;feature;feature extraction;computer science;machine learning;pattern recognition;data mining;k-nearest neighbors algorithm;feature	Vision	34.15894589274251	-58.319207735461525	64233
5dca067d9d315116da20c81c9540c9ed66292d46	key frame extraction from video: framework and advances	key frame;video summarization;duplicate removal;shot boundary;feature extraction	A complete overview of key frame extraction techniques has been provided. It has been found out that such techniques usually have three phases, namely shot boundary detection as a pre-processing phase, main phase of key frame detection, where visual, structural, audio and textual features are extracted from each frame, then processed and analyzed with artificial intelligence methods, and the last post-processing phase lies in removal of duplicates if they occur in the resulting sequence of key frames. Estimation techniques and available test video collections have been also observed. At the end, conclusions concerning drawbacks of the examined procedure and basic tendencies of its development have been marked. Key Frame Extraction from Video: Framework and Advances		Sergii Mashtalir;Olena Mikhnova	2014	IJCVIP	10.4018/ijcvip.2014040105	reference frame;computer vision;computer science;multimedia;information retrieval	AI	39.12163497796958	-52.12685576087472	64273
972175013999c8f3f394cb3562ce2ce69cf73270	using context information and local feature points in face clustering for consumer photos	pattern clustering;context information;face detection data mining humans face recognition feature extraction computer errors image databases eyes bayesian methods graphical models;image matching;performance comparison;feature matching;data mining;image edge detection;local features;feature extraction;context information face clustering consumer photos face image matching local feature points;local feature points face clustering context information;face;consumer photos;lighting;face clustering;pattern clustering image matching;face detection;context;local feature points;face image matching	We introduce local feature points to achieve face clustering for consumer photos. After combining eigenfaces with context information like clothes, we further investigate the usage of local feature points to match face images. The relationships between face images are constructed by feature matching and then described as a graph. Outliers in the results of preliminary clustering are detected and are re-clustered according to matching characteristics. We report complete performance comparison for different datasets and show that the proposed method has superior performance than conventional approaches.	cluster analysis;eigenface	Wei-Ta Chu;Ya-Lin Lee;Jen-Yu Yu	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959790	face;computer vision;face detection;feature extraction;computer science;machine learning;pattern recognition;lighting	Vision	37.90332019737955	-56.922548969786	64276
158adbb203d0588f1979fecac140c025629ddaa0	trajectory-based sign language recognition using discriminant analysis in higher-dimensional feature space	nonparametric discriminant analysis;discriminant analysis sign language recognition trajectory representation;kernel principal component analysis;sign language recognition;decision boundary;principal component analysis gesture recognition;high dimensionality;trajectory representation;australian sign language data set trajectory based sign language recognition higher dimensional feature space hand movement trajectory kernel principal component analysis motion trajectory data decision boundary nonparametric discriminant analysis;sign language;trajectory three dimensional displays handicapped aids kernel accuracy feature extraction covariance matrix;feature space;motion trajectory data;discriminant analysis;principal component analysis;australian sign language data set;trajectory based sign language recognition;higher dimensional feature space;classification accuracy;hand movement trajectory;gesture recognition	This paper presents a method for recognizing sign language using hand movement trajectory. By applying Kernel Principal Component Analysis (KPCA), the motion trajectory data are firstly mapped into a higher-dimensional feature space for analysis. The advantage of using high dimensionality is that it allows a more flexible decision boundary and thus helps us to achieve better classification accuracy. Then we perform the Nonparametric Discriminant Analysis (NDA) in the higher-dimensional feature space, so that the most helpful information can be extracted. We have tested the proposed method using the Australian Sign Language (ASL) data set. The results demonstrate that our approach outperforms the current state-of-the-art for trajectory-based sign language recognition.	decision boundary;feature vector;kernel principal component analysis;language identification;linear discriminant analysis;the australian	Wun-Guang Liou;Chung-Yang Hsieh;Wei-Yang Lin	2011	2011 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2011.6012048	computer vision;speech recognition;feature vector;sign language;kernel principal component analysis;computer science;machine learning;pattern recognition;gesture recognition;mathematics;decision boundary;principal component analysis	Vision	33.243754109544334	-58.135229298899574	64285
f287de13959fd33e5ccf9db9e965a57be02c32aa	robust 3d mesh hashing based on shape features	robust 3d mesh hashing;blind detection;image processing;content hashing;image processing computational geometry cryptography;shape feature 3d mesh content hashing;authentication;computational geometry;key dependent 3d surface feature;shape feature;shape;content preserved attacks;three dimensional displays;cryptography;feature extraction;shape features;solid modeling;robustness;block shape intensity;three dimensional displays solid modeling shape robustness feature extraction authentication;block shape intensity robust 3d mesh hashing shape features key dependent 3d surface feature content preserved attacks blind detection;3d mesh	In this paper, a robust 3D mesh hashing method based on a key-dependent 3D surface feature is developed. The main objectives of the proposed hashing method are to show robustness against content-preserved attacks and to enable blind-detection without using any preprocessing techniques for the attacks. To achieve these objectives, the proposed hashing method projects all vertices to the shape coordinates of 3D SSD and curvedness, and then, it segments the shape coordinates into rectangular blocks and computes the block shape intensity using a permutation key and a random key. A hash is generated by binarizing the block shape intensity. Experimental results confirm that the proposed hashing method shows robustness against geometrical and topological attacks and provides a unique hash for each model and key.	hash function;preprocessor;solid-state drive	Suk Hwan Lee;Eung-Joo Lee;Ki-Ryong Kwon	2010	2010 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2010.5583069	feature hashing;computer vision;hash table;double hashing;hash function;linear hashing;extendible hashing;dynamic perfect hashing;image processing;feature extraction;computational geometry;shape;open addressing;computer science;cryptography;theoretical computer science;universal hashing;pattern recognition;authentication;mathematics;k-independent hashing;locality preserving hashing;solid modeling;suha;2-choice hashing;locality-sensitive hashing;robustness	Robotics	39.18447504809317	-59.58190338568283	64319
94e6c86a7c378acd1f02e387dd5c1e2a6cfd7202	a novel mobile robot localization approach based on classification with rejection option using computer vision		Abstract In this paper, we propose a novel approach for mobile robot localization from images. The proposal is based on supervised learning using topological representations for the environment. The whole system comprises feature extraction and classification methods. With respect to feature extraction, we consider standard methods in digital image processing, e.g. Scale-Invariant Feature Transform and Local Binary Patterns. For classification, we apply machine learning methods with rejection option. A thorough assessment of the proposal is carried out using data from virtual and real indoor environments. Additionally, we compare the proposed architectures with classic localization systems using an omnidirectional camera. Based on the results, Spatial Moments combined with Bayes classifier is the best performing model, providing high accuracy rate (99.94%) and small computational time (47.3 μ  s and 0.165 s for classification and extraction, respectively). Finally, we observe that localization with rejection option increases efficiency and reliability of navigation in mobile robotics.	computer vision;mobile robot;rejection sampling;robotic mapping	Leandro Bezerra Marinho;Pedro Pedrosa Rebouças Filho;Jefferson S. Almeida;João Wellington M. Souza;Amauri Holanda de Souza Júnior;Victor Hugo C. de Albuquerque	2018	Computers & Electrical Engineering	10.1016/j.compeleceng.2018.03.047	digital image processing;local binary patterns;supervised learning;omnidirectional camera;computer science;feature extraction;computer vision;mobile robot;robotics;artificial intelligence;bayes classifier	Robotics	32.25839321306018	-55.209104562112415	64328
8989ccee90ffdd71a8c81b90f5189514657db7db	glyph recognition by pattern matching with on-the-fly generated patterns	optical character recognition;pattern matching optical character recognition;pattern matching;optical character recognition glyph recognition pattern matching on the fly generated patterns performance test font rasterization engine mass digitizations historic prints	In this paper we analyze the performance of a new concept for glyph recognition as the core component of an OCR-system and present a corresponding performance test. The classification is conducted as pattern matching where the considered patterns are generated on-the-fly with a font rasterization engine. In particular, our approach supports mass-digitizations of historic prints.	anomaly detection;database;glyph;image scaling;optical character recognition;pattern matching;printing;rasterisation	Ursina Caluori;Klaus Simon	2013	2013 21st International Conference on Software, Telecommunications and Computer Networks - (SoftCOM 2013)	10.1109/SoftCOM.2013.6671876	computer vision;speech recognition;feature;computer science;pattern recognition	SE	34.9281708539829	-64.8614350649711	64391
4a5c971c490fcc16732599f0e2c3b3d7c78c74de	testing methods of component contents during rare-earth extraction process based on image feature retrieval	image feature;soft sensing;content retrieval;rare earth extraction	One online monitoring method based on the component contents of image feature retrieval is proposed to conduct the extracting and separating process for the rare-earth ion’s feature color. By analyzing different component contents of rare-earth ion color features, it is exposed with the relevance between component contents and the color feature H component. By combining the image retrieval technique based on contents, two retrieval algorithms, H component histogram and H component coherence vector, are proposed to help with the rare-earth component contents’ testing. In order to improve the soft sensing accuracy, the sensing accuracy correction algorithm based on PSO is suggested. To verify the effects of the above algorithm, the rare-earth component contents soft sensing experimenting platform based on image retrieval has been developed with VC6.0 and SQL Server. After doing imitation experiment with the component contents from the subjects of the rare-earth’s two components Pr/Nd extraction separation system, this method is proved with a fairly high testing accuracy.		Sen Wang;Zijie Gao;Rongxiu Lu	2014	JCP	10.4304/jcp.9.11.2697-2703	computer vision;computer science;pattern recognition;information retrieval	Vision	37.342370159090386	-62.57602821114021	64485
18cd7649d7413e1950795efa7a1b1388b31605de	conditional mutual infomation based boosting for facial expression recognition	facial expression recognition;expression analysis;naive bayes;local binary pattern;mutual information	This paper proposes a novel approach for facial expression r ecognition by boosting Local Binary Patterns (LBP) based classifiers. L ow-cost LBP features are introduced to effectively describle local fea tures of face images. A novel learning procedure, Conditional Mutual Infomation based Boosting (CMIB), is proposed. CMIB learns a sequence of weak classifie r that maximize their mutual information about a candidate class, con ditional to the response of any weak classifier already selected; a strong cl assifier is constructed by combining the learned weak classifiers using the Naive-Bayes. Extensive experiments on the Cohn-Kanade database illustr ated that LBP features are effective for expression analysis, and CMIB en ables much faster training than AdaBoost, and yields a classifier of improved c lassification performance.	adaboost;belief propagation;boosting (machine learning);experiment;local binary patterns;mutual information;naive bayes classifier;naruto shippuden: clash of ninja revolution 3	Caifeng Shan;Shaogang Gong;Peter W. McOwan	2005		10.5244/C.19.42	local binary patterns;naive bayes classifier;computer science;machine learning;pattern recognition;data mining;mutual information	AI	31.549267075713903	-54.9150250041364	64632
214b8a2078cc11d17cbbae9cfe0f060220f548f8	face recognition using local and global features	signal image and speech processing;local and global features;face recognition;quantum information technology spintronics;combining classifier	The combining classifier approach has proved to be a proper way for improving recognition performance in the last two decades. This paper proposes to combine local and global facial features for face recognition. In particular, this paper addresses three issues in combining classifiers, namely, the normalization of the classifier output, selection of classifier(s) for recognition, and the weighting of each classifier. For the first issue, as the scales of each classifier’s output are different, this paper proposes twomethods, namely, linear-exponential normalization method and distribution-weighted Gaussian normalization method, in normalizing the outputs. Second, although combining different classifiers can improve the performance, we found that some classifiers are redundant and may even degrade the recognition performance. Along this direction, we develop a simple but effective algorithm for classifiers selection. Finally, the existing methods assume that each classifier is equally weighted. This paper suggests a weighted combination of classifiers based on Kittler’s combining classifier framework. Four popular face recognition methods, namely, eigenface, spectroface, independent component analysis (ICA), and Gabor jet are selected for combination and three popular face databases, namely, Yale database, Olivetti Research Laboratory (ORL) database, and the FERET database, are selected for evaluation. The experimental results show that the proposed method has 5–7% accuracy improvement.	algorithm;eigenface;feret database;facial recognition system;friedrich kittler;independent computing architecture;independent component analysis;redundancy (engineering);return loss;time complexity	Jian Huang;Pong C. Yuen;Jianhuang Lai;Chun-hung Li	2004	EURASIP J. Adv. Sig. Proc.	10.1155/S1110865704312187	facial recognition system;random subspace method;margin classifier;computer vision;computer science;machine learning;pattern recognition;data mining	Vision	33.83895089579555	-58.60523453242853	64747
2c85cea5c456670a7cfbc025e5fbe60d4936e13c	palm and palm finger joint surfaces based multibiometric approach	discrete wavelet transforms;biological system modeling;biomedical imaging;active appearance model;support vector machine classification;asia	This paper presents a novel multibiometric approach based on palm and palm finger joint surfaces which are obtained from same image. In this approach, the patterns (Region-Of-Interests, ROI) were extracted by using Active Appearance Model (AAM) based hand modeling. Then preprocessing steps which include image normalization and Discrete Wavelet Transform (DWT) are sequentially applied to the both palm and palm finger joint surfaces (ROI). Afterwards these two biometric traits are joined to be long vector. Most discriminative features of long vector are extracted by using Kernel Fisher Discriminant (KFD). Finally, Support Vector Machines (SVM) are implemented for classification. Furthermore, a feature level fusion is also used in order to comparately show the performans of the multibiometric approach's success. The proposed multibiometric approach was tested on 1614 palm and palm finger joint surface images which were captured from 132 different people. The recognition results were obtained by utilizing both long vector structure and feature level fusion strategies. Moreover the palm based unibiometric approach and the palm finger joint surfaces based unibiometric approach were separately tested to make a comparison with the proposed multibiometic approach. The achieved results have demonstrated the proposed multibiometric approach's success.	active appearance model;biometrics;discrete wavelet transform;discriminant;preprocessor;region of interest;support vector machine	Ozge Makul;Murat Ekinci	2016	2016 39th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2016.7760966	medical imaging;computer vision;active appearance model;speech recognition;computer science;pattern recognition	Robotics	32.9906273612756	-61.4015129748539	65034
2c24005e3faed3a98dc0a4f6fb520406271b624e	age estimation using active appearance models and ensemble of classifiers with dissimilarity-based classification	classified image;novel technique;face image;good age estimator;dissimilarity-based classification;global classifier;different aging function;active appearance model;misclassification error;active appearance models;available fg-net database;age estimation	This paper proposes a novel technique that uses Active Appearance Models (AAMs) and Ensemble of classifiers for age estimation. In this technique, features are extracted from face images by AAMs and a global classifier is then used to obtain an idea about the age by distinguishing between child/teen-hood and adulthood, before age estimation. This is done by an ensemble containing various classifiers trained on multiple dissimilarities and thereby which reduces misclassification error. Different aging functions are considered for the classified images to estimate age more accurately. Experiments are performed on the publicly available FG-NET database. The method is found to be a good age estimator.	active appearance model	Sharad Kohli;Surya Prakash;Phalguni Gupta	2011		10.1007/978-3-642-24728-6_44	random subspace method;computer science;machine learning;pattern recognition;data mining	Vision	28.49570925864299	-58.85217281745152	65071
769d1a0aff0cf7842c7861d30ce654a029d6b467	descriptor learning based on fisher separation criterion for texture classification	supervised learning;texture classification;local binary pattern;rotation invariance;face recognition	This paper proposes a novel method to deal with the representation issue in texture classification. A learning framework of image descriptor is designed based on the Fisher separation criteria (FSC) to learn most reliable and robust dominant pattern types considering intraclass similarity and inter-class distance. Image structures are thus be described by a new FSC-based learning (FBL) encoding method. Unlike previous handcraft-design encoding methods, such as the LBP and SIFT, supervised learning approach is used to learn an encoder from training samples. We find that such a learning technique can largely improve the discriminative ability and automatically achieve a good tradeoff between discriminative power and efficiency. The commonly used texture descriptor: local binary pattern (LBP) is taken as an example in the paper, so that we then proposed the FBL-LBP descriptor. We benchmark its performance by classifying textures present in the Outex TC 0012 database for rotation invariant texture classification, KTH-TIPS2 database for material categorization and Columbia-Utrecht (CUReT) database for classification under different views and illuminations. The promising results verify its robustness to image rotation, illumination changes and noise. Furthermore, to validate the generalization to other problems, we extend the application also to face recognition and evaluate the proposed FBL descriptor on the FERET face database. The inspiring results show that this descriptor is highly discriminative.	academy;belief propagation;benchmark (computing);binary pattern (image generation);categorization;encoder;feret (facial recognition technology);facial recognition system;image noise;local binary patterns;molecular descriptor;scale-invariant feature transform;sparse matrix;supervised learning;visual descriptor	Yimo Guo;Guoying Zhao;Matti Pietikäinen;Zhengguang Xu	2010		10.1007/978-3-642-19318-7_15	computer vision;local binary patterns;computer science;machine learning;pattern recognition;mathematics;supervised learning	Vision	32.884428706934955	-54.64420779754126	65091
933e693be714bcbaba602983a371bdbe6c4dd510	"""corrigendum to """"3d face recognition: an automatic strategy based on geometrical descriptors and landmarks"""" [robot. auton. syst. 62(12) (2014) 1768-1776]"""		The authors found a mistake in the text below Table 5, on page 7. The correct text is as follows: The best accurateness was achieved with set V, with a percentage of 90.29%, which means 186 correct recognitions over a total of 206 recognitions performed. Furthermore, if we consider not only the best match, but also the two best matches, the algorithm achieves an accurateness of 95.15%. The authors found a mistake in the text of the Conclusions on page 8. The correct text is as follows: This work is a totally Geometry-based 3D face recognition method. The first phase of the algorithm, entirely devel-	algorithm;auton;facial recognition system;robot;three-dimensional face recognition	Enrico Vezzetti;Federica Marcolin;Giulia Fracastoro	2015	Robotics and Autonomous Systems	10.1016/j.robot.2014.10.015		Robotics	38.56977558790662	-64.51584074387044	65139
6badc3aaf0aeec4a31e5087ebb4913ac9299654e	a statistical model for magnitudes and angles of wavelet frame coefficients and its application to texture retrieval	wavelet frames;statistical models;rotation invariant;texture descriptor;image retrieval	This paper presents a texture descriptor based on wavelet frame transforms. At each position in the image, and for each resolution level, we consider both vertical and horizontal wavelet detail coefficients as the components of a bivariate random vector. The magnitudes and angles of these vectors are computed. At each level the empirical histogram of magnitudes is modeled by a Generalized Gamma distribution, and the empirical histogram of angles is modeled by a different version of the von Mises distribution that accounts for histograms with 2 modes. Each texture is characterized by few parameters. A new distance is presented (based on the Kullback-Leibler divergence) that allows giving relative importance to each model and to each resolution level. This distance is later conveniently adapted to provide for rotation invariance, by establishing equivalence classes over distributions of angles. Through a broad set of experiments on three different image databases, we demonstrate that our new descriptor and distance measure can be successfully applied in the context of texture retrieval. We compare our system to several relevant methods in this field in terms of retrieval performance and number of parameters used by each method. We also include some classification tests. In all the tests, we obtain superior retrieval rates for a set of fewer parameters involved. ∗Corresponding author Email addresses: esther.deves@uv.es (Esther de Ves), dacevedo@dc.uba.ar (Daniel Acevedo), ana.ruedin@dc.uba.ar (Ana Ruedin), xaro.benavent@uv.es (Xaro Benavent) Preprint submitted to Pattern Recognition February 23, 2015	ana (programming language);bivariate data;coefficient;database;email;experiment;kullback–leibler divergence;pattern recognition;statistical model;turing completeness;wavelet	Esther de Ves;Daniel G. Acevedo;Ana M. C. Ruedin;Xaro Benavent	2014	Pattern Recognition	10.1016/j.patcog.2014.03.004	statistical model;computer vision;image retrieval;computer science;pattern recognition;mathematics;statistics	Vision	37.22761286773663	-60.750823805072265	65182
c9780607cbc8ab53e6a52d67e6d67c7544a0247e	a two-stage approach to saliency detection in images	coherence based propagation;spectrum residual model;human vision;visual pop outs;image processing;saliency detection;spectrum;indexing terms;psychology humans visual system colored noise coherence cognition visual perception computer science image processing image color analysis;pattern recognition image processing image analysis object detection;pattern recognition;image analysis;image processing human vision saliency detection spectrum residual model visual pop outs coherence based propagation;object detection image processing;object detection	Researches in psychology, perception and related fields show that there may be a two-stage process involved in human vision. In this paper, we propose an approach by following a two-stage framework for saliency detection. In the first stage, we extend an existing spectrum residual model for better locating visual pop-outs, while in the second stage we make use of coherence based propagation for further refinement of the results from the first step. For evaluation of the proposed approach, 300 images with diverse contents were manually and accurately labeled. Experiments show that our approach achieves much better performance than that from the existing state-of-art.	experiment;refinement (computing);software propagation	Zheshen Wang;Baoxin Li	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517772	spectrum;computer vision;feature detection;image analysis;speech recognition;object-class detection;index term;image processing;computer science;digital image processing;human visual system model	Robotics	37.912610819742234	-53.941730547055364	65200
083fcd3ad7b1251e675f3583fc9158ad394363c6	feature encoding models for geographic image retrieval and categorization	histograms;computer vision;conferences computational modeling signal processing computer vision image retrieval histograms pattern recognition;computational modeling;signal processing;pattern recognition;conferences;image retrieval	In this work, we survey the performance of various feature encoding models for geographic image retrieval task. Recently introduced Vector-of-Locally-Aggregated Descriptors (VLAD) and its Product Quantization encoded binary version VLAD-PQ are compared with the widely used Bag-of-Word (BoW) model. Evaluation results are shown on a publicly available 21-class LULC dataset. With experiments, it is shown that VLAD outperforms classical BoW representation albeit with some increases in the computation time. Additionally, VLAD-PQ results in similar retrieval performance with VLAD but requiring no more computational or memory resources are observed.	categorization;computation;experiment;image retrieval;pq tree;time complexity	Savas Özkan;Tayfun Ates;Engin Tola;Medeni Soysal;Ersin Esen	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830171	computer vision;visual word;image retrieval;computer science;machine learning;signal processing;pattern recognition;histogram;computational model;information retrieval	Vision	34.12112247686338	-55.476114653354934	65610
718d3eafe96bacdb6b7a3d93d28076c20f636a61	adaptively weighted orthogonal gradient binary pattern for single sample face recognition under varying illumination	illumination variation;feature vector;face recognition;awogbp;tetrolet transform;scale factor;subgraph weight;adaptively weighted orthogonal gradient binary pattern;face image classification;information entropy	To overcome the limitation of traditional illumination invariant methods for single sample face recognition, a modified version of gradientface named adaptively weighted orthogonal gradient binary pattern (AWOGBP), which is proved robust to illumination variation, is proposed in this study. First, the Tetrolet transform is performed on the images to obtain low frequency and high frequency components and the retina model processing is applied to low frequency component to make the image more robust to illumination, in the meantime, the authors multiply each element in high frequency components with a scale factor to accentuate details. Then the proposed AWOGBP is used to get the feature vectors of each direction and all the feature vectors are concatenated into the general feature vector for face recognition with the weights of the sub-graph based on their information entropy which is defined as the contribution to describe the whole face images. Finally the principle component analysis method is used to reduce dimensions and the nearest neighbour classifier is used for face image classification and recognition. Experimental results on CMU PIE and Extended Yale B face databases indicate that the proposed method is significantly better as compared with related state-of-the-art methods.	binary pattern (image generation);facial recognition system;gradient;illumination (image)	Yang Hui-xian;Cai Yong-yong	2016	IET Biometrics	10.1049/iet-bmt.2014.0082	facial recognition system;scale factor;computer vision;feature vector;computer science;machine learning;pattern recognition;mathematics;statistics;entropy	Vision	35.47904047547488	-58.41056643219949	65793
364c60eeae45ac7b522a271b8cf5da129a9d3c9d	modulation signal recognition based on information entropy and ensemble learning		In this paper, information entropy and ensemble learning based signal recognition theory and algorithms have been proposed. We have extracted 16 kinds of entropy features out of 9 types of modulated signals. The types of information entropy used are numerous, including Rényi entropy and energy entropy based on S Transform and Generalized S Transform. We have used three feature selection algorithms, including sequence forward selection (SFS), sequence forward floating selection (SFFS) and RELIEF-F to select the optimal feature subset from 16 entropy features. We use five classifiers, including k-nearest neighbor (KNN), support vector machine (SVM), Adaboost, Gradient Boosting Decision Tree (GBDT) and eXtreme Gradient Boosting (XGBoost) to classify the original feature set and the feature subsets selected by different feature selection algorithms. The simulation results show that the feature subsets selected by SFS and SFFS algorithms are the best, with a 48% increase in recognition rate over the original feature set when using KNN classifier and a 34% increase when using SVM classifier. For the other three classifiers, the original feature set can achieve the best recognition performance. The XGBoost classifier has the best recognition performance, the overall recognition rate is 97.74% and the recognition rate can reach 82% when the signal to noise ratio (SNR) is −10 dB.	adaboost;clustered file system;computational complexity theory;decision tree;ensemble learning;entropy (information theory);feature selection;gradient boosting;k-nearest neighbors algorithm;mathematical model;modulation;rényi entropy;s transform;selection algorithm;selection bias;signal-to-noise ratio;simulation;stepwise regression;support vector machine;xgboost	Zhen Zhang;Yibing Li;Shanshan Jin;Zhaoyue Zhang;Lin Qi;Ruolin Zhou	2018	Entropy	10.3390/e20030198	mathematical optimization;adaboost;support vector machine;entropy (information theory);gradient boosting;s transform;mathematics;rényi entropy;feature selection;ensemble learning;artificial intelligence;pattern recognition	ML	27.03470220136396	-61.58174764558749	65807
ff3ad671f5b1ab142b302bf8e89e5477408e8485	geometrically invariant image watermarking using scale-invariant feature transform and k-means clustering	image filtering;robust watermarking;image processing;scale invariant feature transform;affine transformation;image watermarking;robust watermark;watermark synchronization;k means clustering;geometric distortion	In the traditional feature-base robust image watermarking, all bits of watermark message are bound with the feature point. If a few of points are attacked badly or lost, the performance of the watermarking scheme will decline or fail. In this paper, we present a robust image watermarking scheme by the use of k-means clustering, scale-invariant feature transform (SIFT) which is invariant to rotation, scaling, translation, partial affine distortion and addition of noise. SIFT features are clustered into clusters by k-means clustering. Watermark message is embedded bit by bit in each cluster. Because one cluster contains only one watermark bit but one cluster contains many feature points, the robustness of watermarking is not lean upon individual feature point. We use twice voting strategy to keep the robustness of watermarking in watermark detecting process. Experimental results show that the scheme is robust against various geometric transformation and common image processing operations, including scaling, rotation, affine transforms, cropping, JPEG compression, image filtering, and so on.	computer cluster;digital watermarking;k-means clustering;scale-invariant feature transform	Huawei Tian;Yao Zhao;Rongrong Ni;Jeng-Shyang Pan	2010		10.1007/978-3-642-16693-8_14	computer vision;image processing;computer science;theoretical computer science;machine learning;pattern recognition;scale-invariant feature transform;affine transformation;watermark;k-means clustering	ML	36.62291421018781	-61.14160654596143	65844
3c619ace5b427c023c6098bd5101468dbfd87d85	okapi-chamfer matching for articulate object recognition	object recognition;features extraction;text documents;information retrieval system;inverted index;information retrieval;hand posture recognition;image matching;image database;text analysis;okapi weighting formula;text information retrieval systems;computer vision;query terms;visual feature extraction;feature extraction;okapi chamfer matching;text retrieval;information retrieval systems;visual features;hand posture recognition okapi chamfer matching object recognition text information retrieval systems inverted index text documents query terms features extraction visual feature extraction okapi weighting formula chamfer distance;chamfer distance;text analysis feature extraction image matching image retrieval information retrieval systems object recognition;object recognition image databases computer vision shape image retrieval feature extraction information retrieval indexing system testing application software;image retrieval	Recent years have witnessed the rise of many effective text information retrieval systems. By treating local visual features as terms, training images as documents and input images as queries, we formulate the problem of object recognition into that of text retrieval. Our formulation opens up the opportunity to integrate some powerful text retrieval tools with computer vision techniques. In this paper, we propose to improve the efficiency of articulated object recognition by an Okapi-Chamfer matching algorithm. The algorithm is based on the inverted index technique. The inverted index is a widely used way to effectively organize a collection of text documents. With the inverted index, only documents that contain query terms are accessed and used for matching. To enable inverted indexing in an image database, we build a lexicon of local visual features by clustering the features extracted from the training images. Given a query image, we extract visual features and quantize them based on the lexicon, and then look up the inverted index to identify the subset of training images with non-zero matching score. To evaluate the matching scores in the subset, we combined the modified Okapi weighting formula with the Chamfer distance. The performance of the Okapi-Chamfer matching algorithm is evaluated on a hand posture recognition system. We test the system with both synthesized and real world images. Quantitative results demonstrate the accuracy and efficiency of our system	algorithm;chamfer;cluster analysis;coefficient;cognitive dimensions of notations;computer vision;database;document retrieval;information retrieval;inverted index;lexicon;okapi bm25;outline of object recognition;poor posture;quantization (signal processing);standard test image	Hanning Zhou;Thomas S. Huang	2005	Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1	10.1109/ICCV.2005.176	computer vision;inverted index;feature extraction;image retrieval;computer science;cognitive neuroscience of visual object recognition;pattern recognition;information retrieval	Vision	38.957160202059306	-58.78700993709669	65872
3024b636f2d6d3498e5f3f1f55bb7896b06274b1	feature reduction and hierarchy of classifiers for fast object detection in video images	image features;biology computing;object detection face detection support vector machines support vector machine classification filters computer vision image analysis classification algorithms biology computing research and development;learning;support vector machines;solutions general;feature extraction object detection face recognition image classification;reduction;filters;image classification;detection;classification;support vector;hierarchies;computer vision;accuracy;feature reduction;hierarchical classification;research and development;classifier;statistical learning;face recognition;statistical learning theory;feature extraction;theory;classification algorithms;statistics;pattern recognition;support vector machine classification;algorithms;image analysis;machines;video signals;face detection;template matching computer vision object detection image features statistical learning classifier face detection hierarchical classification;template matching;vector analysis;images;object detection	We present a two-step method to speed-up object detection systems in computer vision that use Support Vector Machines (SVMs) as classifiers. In a first step we perform feature reduction by choosing relevant image features according to a measure derived from statistical learning theory. In a second step we build a hierarchy of classifiers. On the bottom level, a simple and fast classifier analyzes the whole image and rejects large parts of the background. On the top level, a slower but more accurate classifier performs the final detection. Experiments with a face detection system show that combining feature reduction with hierarchical classification leads to a speed-up by a factor of 170 with similar classification performance.	computational complexity theory;computer vision;experiment;face detection;feature selection;hierarchical classifier;machine learning;object detection;statistical classification;statistical learning theory;support vector machine	Bernd Heisele;Thomas Serre;Sayan Mukherjee;Tomaso A. Poggio	2001		10.1109/CVPR.2001.990919	facial recognition system;random subspace method;support vector machine;computer vision;feature detection;image analysis;object-class detection;computer science;machine learning;linear classifier;pattern recognition;feature	Vision	32.374363395402824	-56.659185866529526	66057
631bc0ffc5b0639c6a316e63ee2f8fe096e2d694	an intelligent and robust multi-oriented image scene text detection	neural networks;manganese;wavelet transforms;accuracy;neural network text detection wavelet;vectors;image color analysis;wavelet transforms image classification neural nets text detection;robustness;pixel classification robust multioriented image scene text detection intelligent multioriented image scene text detection scene file neural network wavelet transformation online step offline step;neural networks vectors image color analysis accuracy robustness wavelet transforms manganese	Text in image is an important source of information. In this article, we describe an approach for detection of text in the scene file. Our method classify pixels into text and non-text areas using neural network and wavelet transformation. It is divided into two steps: a step offline and online step. The experimental results show the performance of our algorithm.	algorithm;artificial neural network;color;email;information source;international conference on document analysis and recognition;mathematical morphology;online and offline;pixel;wavelet transform	Salem Sayahi;Mohamed Ben Halima	2014	2014 6th International Conference of Soft Computing and Pattern Recognition (SoCPaR)	10.1109/SOCPAR.2014.7008043	computer vision;computer science;manganese;machine learning;pattern recognition;artificial neural network;robustness	Robotics	35.388589109035145	-64.88204414741672	66404
57274703c284db473a224a9fd25566eae3412bd5	an automatic method of location for number-plate using color features	image recognition;multilayer perceptrons image recognition image colour analysis feature extraction image classification;mlpn networks automatic method number plate location color features pixel colors neural network number plate area vehicle plate recognition feature extraction;multilayer perceptrons;image classification;neural networks lighting image color analysis vehicles euclidean distance feature extraction image recognition character recognition image converters wavelength conversion;image colour analysis;feature extraction;cross section;neural network	This paper presents an automatic method of location for a number-plate using color features. In this method, the pixel colors of the number-plate area are classified using a neural network, then color features are extracted by analyzing scanning lines of the cross-section of the number-plate. It makes full use of the number-plate color features to locate the number-plate. The experimental results show that the correct rate of number-plate location is close to 100%, and the time of number-plate location is less than 1 second. It is also observed that this system is not sensitive to variations of weather, illumination and vehicle speed. In addition, and also it does not require the size of the number-plate to be known in prior. This system is of crucial significance is to apply and spread the automatic system of vehicle plate recognition.	color	Wu Wei;Mingjun Wang;Zhongxiang Huang	2001		10.1109/ICIP.2001.959162	color histogram;computer vision;contextual image classification;feature detection;color quantization;color image;feature;binary image;feature extraction;computer science;machine learning;pattern recognition;cross section;feature;artificial neural network	Vision	34.87937780835099	-63.85063272133559	66421
194c3c64444e2463d968db689e761097138977b9	classification of medical images using local representations		In medical image retrieval, the images are usually subject to a large range of variability. In order to classify medical images, we therefore propose the use of local representations, which are small square windows taken from the images. This approach is combined with a fast approximate -nearest neighbor technique and yields state-of-the-art results on a medical image database of 1617 images.	approximation algorithm;image retrieval;medical imaging;microsoft windows;spatial variability	Roberto Paredes;Daniel Keysers;Thomas Martin Deserno;Berthold B. Wein;Hermann Ney;Enrique Vidal	2002			computer vision;visual word;image analysis;computer science;pattern recognition;automatic image annotation;information retrieval	Vision	38.12812959382554	-60.74793307294463	66511
360b57bc952c986041a2fd6e89dbd7700c98177a	real estate image classification	histograms;standards;neural networks;computer vision;image enhancement;neural networks image enhancement histograms standards computer vision lighting feature extraction;feature extraction;lighting	Posting pictures is a necessary part of advertising a home for sale. Agents typically sort through dozens of images from which to pick the most complimentary ones. This is a manual effort involving annotating images accompanied by descriptions (bedroom, bathroom, attic, etc.). When volumes are small, manual annotation is not a problem, but there is a point where this becomes too burdensome and ultimately infeasible. Here, we propose an approach based on computer vision methodology to radically increase the efficiency of such tasks. We present a high-confidence image classification framework, whose inputs are images and outputs are labels. The core of the classification algorithm is long short term memory (LSTM), and fully connected neural networks, along with a substantial preprocessing using 'contrast-limited adaptive histogram equalization (CLAHE) for image enhancement. Since, there is no standard benchmark containing a comprehensive dataset of well-annotated real estate images, we introduce Real Estate Image (REI) database for evaluating the image classification algorithms. Therein we demonstrate empirics based on our proposed framework on the new REI dataset, as well as on the SUN dataset.	adaptive histogram equalization;algorithm;anomaly detection;artificial neural network;attic;benchmark (computing);computer vision;defective by design;image editing;long short-term memory;preprocessor	Jawadul H. Bappy;Joseph R. Barr;Narayanan Srinivasan;Amit K. Roy-Chowdhury	2017	2017 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2017.48	computer vision;feature extraction;computer science;artificial intelligence;machine learning;data mining;lighting;histogram;automatic image annotation;artificial neural network	Vision	27.0281701367349	-54.41269349287236	66512
86f2e81c908a8b186773e2bc05c4abbe98a415ea	line-wise text identification in comic books: a support vector machine-based approach	support vector machine svm comic book speech balloon comic text spatial pyramid matching spm identification technique;ebdtheque comic text database line wise text identification support vector machine speech balloon ocr latin comic text lines bengali comic text lines local features scale invariant feature transform sift multiscale block local binary pattern mblbp spatial pyramid matching spm domain svm based classification latin comic e books internet website;speech feature extraction support vector machines histograms optical character recognition software databases transforms;web sites feature extraction humanities image classification image matching optical character recognition support vector machines text analysis transforms	This paper presents a study of line-wise text identification in comic books. Due to the unavailability of a single OCR system which can handle comic text of multiple scripts, the comic text identification based on script becomes an essential step for choosing the appropriate OCR. In this investigation, a new attempt has been made to explore a comic text identification technique of speech balloon to feed the identified text into the appropriate OCR. Latin and Bengali comic text lines have been considered for identification in this study. Two different local features, namely, Scale Invariant Feature Transform (SIFT) and Multi-scale Block Local Binary Pattern (MBLBP) were considered in Spatial Pyramid Matching (SPM) domain in the current study. The support vector machine (SVM)-based classification technique has been considered for line-wise text identification. To evaluate the identification system, text datasets of Latin and Bengali comics have been newly prepared from Latin comic e-books and Bengali comic books respectively. The Latin comic e-books are collected from internet on website dedicated to Comics. To conduct the experiment, samples of 1938 text lines from each comic text dataset have been used. A publicly available eBDtheque comic text database has also been considered for performance comparison of the proposed method. 1938 number of text line images from eBDtheque comic text database has also taken into account in this approach. The highest identification accuracies of 98.30% and 98.29% on an average are achieved in the experiment when Bengali and eBDtheque comic text dataset are considered.	book;e-book;feature extraction;optical character recognition;scale-invariant feature transform;speech synthesis;super paper mario;support vector machine;unavailability	Srikanta Pal;Jean-Christophe Burie;Umapada Pal;Jean-Marc Ogier	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727719	computer vision;speech recognition;computer science;pattern recognition	NLP	34.44994937954397	-64.8897659871332	66615
46b2b620e7b26bf6049aaece16c469006a95a2c7	a sift-based forensic method for copy–move attack detection and transformation recovery	image processing;computer forensics;forensics forgery cloning feature extraction digital images discrete cosine transforms;geometric transformation recovery authenticity verification copy move attack digital image forensics;discrete cosine transform;transforms computer forensics feature extraction image processing;image cloning sift based forensic method copy move attack detection transformation recovery image forensics image authenticity forensic analysis forged image detection image patch geometric transformation scale invariant features transform;feature extraction;transforms;digital image	One of the principal problems in image forensics is determining if a particular image is authentic or not. This can be a crucial task when images are used as basic evidence to influence judgment like, for example, in a court of law. To carry out such forensic analysis, various technological instruments have been developed in the literature. In this paper, the problem of detecting if an image has been forged is investigated; in particular, attention has been paid to the case in which an area of an image is copied and then pasted onto another zone to create a duplication or to cancel something that was awkward. Generally, to adapt the image patch to the new context a geometric transformation is needed. To detect such modifications, a novel methodology based on scale invariant features transform (SIFT) is proposed. Such a method allows us to both understand if a copy-move attack has occurred and, furthermore, to recover the geometric transformation used to perform cloning. Extensive experimental results are presented to confirm that the technique is able to precisely individuate the altered area and, in addition, to estimate the geometric transformation parameters with high reliability. The method also deals with multiple cloning.	cluster analysis;image segmentation;sensor	Irene Amerini;Lamberto Ballan;Roberto Caldelli;Alberto Del Bimbo;Giuseppe Serra	2011	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2011.2129512	image restoration;computer vision;feature detection;image processing;feature extraction;computer science;discrete cosine transform;digital image processing;internet privacy;computer security;top-hat transform;digital image;computer forensics	Vision	35.72942228170569	-62.282021275450525	66832
7647b25d665f8be1b3e86743ed28285eea32fd8a	face detection in non-uniform illumination conditions by using color and triangle-based approach	triangle based approach;weighting mask function;color segmentation;face detection	This investigation develops an efficient face detection scheme that can detect multiple faces in color images with complex environments and non-uniform illumination conditions. The proposed scheme comprises two stages. The first stage adopts YES color and triangle-based segmentation to search potential face regions. The second stage involves face verification using a weighting mask function. The system can handle various sizes of faces, non-uniform illumination conditions, pose and changeable expression. In particular, the scheme significantly increases the execution speed of the face detection algorithm in complex backgrounds. A result of this study performs better than previous methods in terms of speed and ability to handle non-uniform illumination conditions.	algorithm;color;computation;face detection;facial recognition system;image segmentation;local interconnect network;preprocessor;run time (program lifecycle phase);time complexity	Chiunhsiun Lin	2006		10.2991/jcis.2006.198	computer vision;pattern recognition	Vision	36.94357947181562	-59.44038213335403	66892
1d32ea6ae0c2efefac7eb67d9c342581a10aa80b	fast and accurate texture recognition with multilayer convolution and multifractal analysis		A fast and accurate texture recognition system is presented. The new approach consists in extracting locally and globally invariant representations. The locally invariant representation is built on a multi-resolution convolutional network with a local pooling operator to improve robustness to local orientation and scale changes. This representation is mapped into a globally invariant descriptor using multifractal analysis. We propose a new multifractal descriptor that captures rich texture information and is mathematically invariant to various complex transformations. In addition, two more techniques are presented to further improve the robustness of our system. The first technique consists in combining the generative PCA classifier with multiclass SVMs. The second technique consists of two simple strategies to boost classification results by synthetically augmenting the training set. Experiments show that the proposed solution outperforms existing methods on three challenging public benchmark datasets, while being computationally efficient.	algorithmic efficiency;benchmark (computing);computer vision;convolution;filter bank;multifractal system;principal component analysis;synthetic intelligence;test set	Hicham Badri;Hussein M. Yahia;Khalid Daoudi	2014		10.1007/978-3-319-10590-1_33	computer vision;machine learning;pattern recognition;data mining;mathematics;statistics	Vision	30.646529718026684	-54.71689779727236	66952
edfd84193dd1fd610fa9cc74ae08aec63013fea7	feature level fusion of face and palmprint biometrics by isomorphic graph-based improved k-medoids partitioning	cluster algorithm;isomorphic graph;biometrics;palmprint;k medoids partitioning algorithm;number of clusters;pattern recognition;face;feature level fusion;invariant feature	This paper presents a feature level fusion approach which uses the improved K-medoids clustering algorithm and isomorphic graph for face and palmprint biometrics. Partitioning around medoids (PAM) algorithm is used to partition the set of n invariant feature points of the face and palmprint images into k clusters. By partitioning the face and palmprint images with scale invariant features SIFT points, a number of clusters is formed on both the images. Then on each cluster, an isomorphic graph is drawn. In the next step, the most probable pair of graphs is searched using iterative relaxation algorithm from all possible isomorphic graphs for a pair of corresponding face and palmprint images. Finally, graphs are fused by pairing the isomorphic graphs into augmented groups in terms of addition of invariant SIFT points and in terms of combining pair of keypoint descriptors by concatenation rule. Experimental results obtained from the extensive evaluation show that the proposed feature level fusion with the improved K-medoids partitioning algorithm increases the performance of the system with utmost level of accuracy.	biometrics;equivalence partitioning;fingerprint;graph isomorphism;k-medoids;medoid	Dakshina Ranjan Kisku;Phalguni Gupta;Jamuna Kanta Sing	2010		10.1007/978-3-642-13577-4_7	face;computer vision;machine learning;pattern recognition;mathematics;graph isomorphism;biometrics	Vision	37.26194242866438	-57.23884935208559	67197
273e47f06e664b8152bada680a8da4bcd67acc35	automatic extraction of geometric lip features with application to multi-modal speaker identification	mouth;audio visual systems;pattern clustering;lts5;speaker identification;cuave database;visual databases audio visual systems feature extraction fuzzy logic pattern clustering speaker recognition;color space;automatic extraction;feature extraction face detection data mining mouth system performance spatial databases audio databases speech recognition robustness testing;testing;fuzzy based c means clustering technique;data mining;system performance;closed set audio visual system;speaker recognition;fuzzy logic;visual information;feature extraction;visual cues;spatial databases;multimodal speaker identification;speech recognition;robustness;audio visual;audio databases;geometric lip feature;face detection;cuave database automatic extraction geometric lip feature multimodal speaker identification visual information color space transformation fuzzy based c means clustering technique visual cue closed set audio visual system;color space transformation;visual cue;visual databases	In this paper we consider the problem of automatic extraction of the geometric lip features for the purposes of multi-modal speaker identification. The use of visual information from the mouth region can be of great importance for improving the speaker identification system performance in noisy conditions. We propose a novel method for automated lip features extraction that utilizes color space transformation and a fuzzy-based c-means clustering technique. Using the obtained visual cues closed-set audio-visual speaker identification experiments are performed on the CUAVE database, showing promising results	cluster analysis;color space;experiment;modal logic;speaker recognition	Ivana Arsic;Roger Vilagut;Jean-Philippe Thiran	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262594	fuzzy logic;speaker recognition;computer vision;face detection;speech recognition;sensory cue;feature extraction;computer science;pattern recognition;software testing;color space;robustness	Robotics	38.712657330098914	-62.30865517375174	67209
2fc6a66ce85185802fadee89e269aa14f602a964	image classification based on bag of visual graphs	graph theory;classification tasks image classification bag of visual graphs bovg approach spatial relationships visual word arrangements codebook graph representation image representations visual words occurrence caltech 101 dataset caltech 256 dataset;image representation graph theory image classification;graphs image classification bag of visual words spatial relationships;image classification;image representation	This paper proposes the Bag of Visual Graphs (BoVG), a new approach to encode the spatial relationships of visual words through a codebook of visual-word arrangements, represented by graphs. This graph-based codebook defines a descriptor for image representations that not only considers the frequency of occurrence of visual words, but also their spatial relationships. Experiments demonstrate that BoVG yields high-accuracy scores in classification tasks on the traditional Caltech-101 and Caltech-256 datasets.	caltech 101;codebook;encode;experiment	Fernanda B. Silva;Siome Goldenstein;Salvatore Tabbone;Ricardo da Silva Torres	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738888	computer vision;contextual image classification;graph theory;machine learning;pattern recognition;mathematics;bag-of-words model in computer vision	Robotics	36.91288241685501	-56.35719112069165	67218
b4e05e5e4832b350e625ffaaeca6e94396eb9f60	3d assisted face recognition: a survey of 3d imaging, modelling and recognition approachest	face processing;3d face recognition;image recognition;3d imaging;testing;spectrum;computer vision;face recognition;face recognition image recognition face detection decision making computer vision robustness humans photometry feature extraction testing;photometry;feature extraction;robustness;humans;face detection	3D face recognition has lately been attracting ever increasing attention. In this paper we review the full spectrum of 3D face processing technology, from sensing to recognition. The review covers 3D face modelling, 3D to 3D and 3D to 2D registration, 3D based recognition and 3D assisted 2D based recognition. The fusion of 2D and 3D modalities is also addressed. The paper complements other reviews in the face biometrics area by focusing on the sensor technology, and by detailing the efforts in 3D face modelling and 3D assisted 2D face matching.	3d modeling;3d reconstruction;biometrics;capability maturity model;facial recognition system;sensor;stereoscopy;three-dimensional face recognition	Josef Kittler;Adrian Hilton;Miroslav Hamouz;John Illingworth	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops	10.1109/CVPR.2005.377	facial recognition system;stereoscopy;spectrum;computer vision;face detection;speech recognition;object-class detection;photometry;feature extraction;intelligent character recognition;computer science;pattern recognition;three-dimensional face recognition;face recognition grand challenge;software testing;3d single-object recognition;face hallucination;robustness	Vision	29.05393966896178	-61.06861413588418	67236
c13d20ec5c3d088c06480f5d87c2c5e49f8bb030	a fusion model for road detection based on deep learning and fully connected crf		This paper presents a road detection model based on deep learning and fully connected condition random field to fuse image and point cloud data. Firstly, a convolutional neural network is trained to extract multi-scale features of the image. And a point-based deep neural network is trained to extract the multi-scale features of the point cloud. Secondly, the point cloud data is projected to the image plane. The probability maps of image and point cloud in the image plane are obtained by their corresponding multi-scale features, respectively. Thirdly, a Markov-based up-sampling method is used to get a dense height image from a sparse one which is from the point cloud data. A fully connected condition random field model based on the outputs of the two networks and the height image is constructed on the image plane. Finally, the fusion model is effectively solved by the mean-field approximate algorithm. Experiments on KITTI Road dataset show that the proposed model can effectively fuse the image and the point cloud data. Furthermore, the fusion model can also exclude the shadows, road curbs and other interferences in complex scenes.	approximation algorithm;artificial neural network;conditional random field;convolutional neural network;deep learning;error detection and correction;image plane;map;markov chain;point cloud;sampling (signal processing);sparse matrix;unary operation	Fei Yang;Jian Yang;Zhong Jin;Huan Wang	2018	2018 13th Annual Conference on System of Systems Engineering (SoSE)	10.1109/SYSOSE.2018.8428696	convolutional neural network;point cloud;random field;image segmentation;computer vision;image plane;artificial neural network;feature extraction;deep learning;distributed computing;computer science;artificial intelligence	Robotics	29.005489254917872	-53.034878719649946	67329
92ed59836dbdc5b2c0052320969b8e254c03e9ab	traffic sign recognition based on a context-aware scale-invariant feature transform approach	databases;sensors;schools and universities;information technology;detection and tracking algorithms;artificial neural networks;matrices;roads;feature extraction;principal component analysis;image registration;safety;hough transforms;computing systems;algorithms;genetic algorithms;modeling;matlab;cameras;computer vision technology;image retrieval	A new context-aware scale-invariant feature transform (CASIFT) approach is proposed, which is designed for the use in traffic sign recognition (TSR) systems. The following issues remain in previous works in which SIFT is used for matching or recognition: (1) SIFT is unable to provide color information; (2) SIFT only focuses on local features while ignoring the distribution of global shapes; (3) the template with the maximum number of matching points selected as the final result is instable, especially for images with simple patterns; and (4) SIFT is liable to result in errors when different images share the same local features. In order to resolve these problems, a new CASIFT approach is proposed. The contributions of the work are as follows: (1) color angular patterns are used to provide the color distinguishing information; (2) a CASIFT which effectively combines local and global information is proposed; and (3) a method for computing the similarity between two images is proposed, which focuses on the distribution of the matching points, rather than using the traditional SIFT approach of selecting the template with maximum number of matching points as the final result. The proposed approach is particularly effective in dealing with traffic signs which have rich colors and varied global shape distribution. Experiments are performed to validate the effectiveness of the proposed approach in TSR systems, and the experimental results are satisfying even for images containing traffic signs that have been rotated, damaged, altered in color, have undergone affine transformations, or images which were photographed under different weather or illumination conditions. © SPIE and IS&T.	scale-invariant feature transform;traffic sign recognition	Xue Yuan;Xiaoli Hao;Houjin Chen;Xue-Ye Wei	2013	J. Electronic Imaging	10.1117/1.JEI.22.4.041105	computer vision;systems modeling;genetic algorithm;feature;feature extraction;image retrieval;computer science;sensor;image registration;theoretical computer science;machine learning;information technology;feature;matrix;principal component analysis	Vision	38.78225975253452	-58.972499385967964	67493
208b6cbd4b80eb5268749f2c4633e9d1e766bd0a	a reliable iris recognition algorithm based on reverse biorthogonal wavelet transform	biometrics;iris recognition;signatures matching;signatures encoding;reverse biorthogonal wavelet transform	0167-8655/$ see front matter 2011 Elsevier B.V. A doi:10.1016/j.patrec.2011.08.018 ⇑ Corresponding author. E-mail addresses: szewczyk@dmcs.p.lodz.pl (R. S p.lodz.pl (K. Grabowski), mnapier@dmcs.p.lodz.pl (M. Nap (W. Sankowski), mariuszz@dmcs.p.lodz.pl (M. Zubert), na alski). URL: http://www.irisep.dmcs.pl (R. Szewczyk). This article describes an iris recognition algorithm designed to analyze noisy iris biometric data. The methods forming part of the authentication process were developed and optimized by the authors using visible wavelength images of an eye taken under unconstrained conditions (at a different perspectives, illuminations, occlusion grades, etc.), mainly contained in the UBIRIS.v2 database. The whole iris authentication system was submitted by the authors to the International Iris Recognition Contest NICE.II, where it took eighth place, while the iris segmentation stage itself took second place in the previous contest — NICE.I. This paper is focused on the iris feature extraction stage — the method developed by the authors to analyze noisy iris biometric data. Several techniques used for more efficient and robust analysis of such images and issues concerning the best wavelet selection are also presented in this paper. 2011 Elsevier B.V. All rights reserved.	algorithm;amiga reflections;authentication;biometrics;biorthogonal wavelet;channel (digital image);feature extraction;iris recognition;numerical aperture;signal-to-noise ratio;wavelet transform	Robert Szewczyk;Kamil Grabowski;Malgorzata Napieralska;Wojciech Sankowski;Mariusz Zubert;Andrzej Napieralski	2012	Pattern Recognition Letters	10.1016/j.patrec.2011.08.018	computer vision;speech recognition;pattern recognition;iris recognition;biometrics	Vision	34.91825326371145	-62.784502583449886	67578
8f15ce4aa02195935360811a3796dcbbf370d997	cumulative differential gabor features for facial expression classification	independent component analysis;dynamic gabor feature;facial expression classification	Emotions are written all over our faces. Facial expressions of emotions can be possibly read by computer vision and machine learning system. Regarding the evidence in cognitive science, the perception of facial dynamics is necessary for understanding the facial expression of human emotions. Our previous study proposed a temporal feature to model the levels of facial muscle activation. However, the quality of the feature suffers from various types of interference such as translation, scaling, noise, blurriness, and varying illumination. To cope with such problems, we derive a novel feature descriptor by expanding 2D Gabor features for a time series data. This feature is called Cumulative Differential Gabor feature (CDG). Then, we use a discriminative subspace for estimating an emotion class. As a result, our method gains the advantages of using both spatial and frequency components. The experimental results show the performance and the robustness to the underlying conditions.		Prarinya Siritanawan;Kazunori Kotani;Fan Chen	2015	Int. J. Semantic Computing	10.1142/S1793351X15400036	independent component analysis;computer vision;speech recognition;computer science;machine learning;pattern recognition;feature	Vision	25.477538429979454	-63.923910111767334	67581
10dbc314da725425f583530506201d8495bb0c1c	human identification by gait using time delay neural networks		This paper proposed human identification method by gait. Human gait is a type of biometric features and related to the physiological and behavioral features of a human. In this paper, a feature vector of gait motion parameters is extracted from each frame using image segmentation methods, and categorized into different categories. Two of these categories were used to form the gait motion trajectories; Category one: Gait angle velocity: angle velocity hip, angle velocity knee, angle velocity thigh and angle velocity shank. Category two: Gait angle acceleration: angle acceleration hip, angle acceleration knee, angle acceleration thigh and angle acceleration shank for each image sequence. Finally, the TDNN method with different training algorithms is used for recognition purpose. This experiment is done on our own database. This research developed a method which achieves a higher recognition rate in the training set 100% and in the testing set 83%. Also, category one establishes gait motion features to be used in human gait identification applications using different training algorithms, While category two achieved a higher recognition rate by trainrb algorithm.	algorithm;artificial neural network;biometrics;categorization;database;feature vector;gait analysis;image segmentation;test set;time delay neural network;velocity (software development)	Eman Fares Al Mashagba	2015	Computer and Information Science	10.5539/cis.v8n4p56	computer vision;effect of gait parameters on energetic cost;simulation;speech recognition;geography	Robotics	30.792115179232734	-59.43266470752676	67663
6847a733a907eb215abd466e5bdf0b57bdec4cf2	illumination invariant face recognition	fast fourier transform fft;sparse fft;face recognition;support vector machines svm;pattern recognition	We present a novel method for face recognition by enhancing the quality of the input face images, which may be too dark due to different lighting conditions. We propose to extract the FFT features or the dual-tree complex wavelet (DTCWT)-FFT features from the enhanced face images and use the Support Vector Machine as a classifier. Our experiments show that our proposed methods compare favourably to the FFT features without image enhancement, and the methods in [1] and [10] for the Extended Yale Face Database B and the CMU-PIE face database.	experiment;facial recognition system;fast fourier transform;image editing;support vector machine;wavelet	Guangyi Chen;Sridhar Krishnan;Yongjia Zhao;Wenfang Xie	2013		10.1007/978-3-642-39479-9_46	facial recognition system;computer vision;speech recognition;computer science;pattern recognition;three-dimensional face recognition	Vision	34.21174187603089	-59.0267937143331	67749
58da4e59c4d259196fc6bd807bc8c36636efa4ef	symmetrical pca in face recognition	image recognition;mirrors;neural networks;principal component analysis face recognition mirrors image recognition stability computational efficiency humans face detection character recognition neural networks;symmetry;stability;symmetrical pca;computational complexity face recognition principal component analysis symmetry feature extraction;face recognition;even odd decomposition principle;storage space;computational complexity;feature extraction;principal component analysis;recognition rate;mirror images;facial symmetry;symmetrical principal component analysis;computational cost;feature selection;humans;pattern variations;face detection;computational efficiency;character recognition;storage space symmetrical pca face recognition facial symmetry mirror images even odd decomposition principle symmetrical principal component analysis pattern variations feature selection recognition rate computational cost;principal component	Facial symmetry is obviously a useful natural characteristic of facial images, which will help develop face-oriented recognition technology and algorithms. This paper will apply it to face recognition after introducing mirror images. By combining PCA with the even-odd decomposition principle, a new algorithm called Symmetrical Principal Component Analysis is proposed, in which different energy ratios of even/odd symmetrical principal components and their different sensitivities to pattern variations are employed for feature selection. This algorithm has two outstanding advantages. Firstly, it effectively improves the stability of features and remarkably raises the recognition rate. Secondly, it greatly saves the computational and memory cost.	algorithm;computation;eigenface;facial recognition system;facial symmetry;feature selection;principal component analysis	Qiong Yang;Xiaoqing Ding	2002		10.1109/ICIP.2002.1039896	computer vision;computer science;machine learning;pattern recognition;mathematics;feature selection;face hallucination;principal component analysis	Vision	32.69627400205972	-58.51038609017277	67852
b5b34a33c965312267c1c63235f3aebf610fb3f6	neutralizing lighting non-homogeneity and background size in pcnn image signature for arabic sign language recognition		Many feature generation methods have been developed using pulse-coupled neural network. Most of these methods succeeded to achieve the invariance against object translation, rotation and scaling but could not neutralize the bright background effect and non-uniform light on the quality of the generated features. To overcome the shortcomings, the paper proposes a new method to enhance the features’ quality. The “Continuity Factor” is defined and considered as a weight factor of the current pulse in signature generation process. This factor measures the simultaneous firing strength for connected pixels. The proposed new method is applied and compared to the previous methods. Through Arabic Sign Language recognition experiments, the superiority of the new method is shown.	artificial neural network;experiment;image scaling;interference (communication);pixel;poor posture;pulse-coupled networks;scott continuity;the 100	A. Samir Elons;Magdy Aboull-Ela;Mohamed F. Tolba	2011	The 2011 International Conference on Computer Engineering & Systems	10.1007/s00521-012-0818-4	speech recognition;artificial intelligence;mathematics	Vision	39.07779509319087	-65.76453964606345	68052
8e2ce5394ff00319d2acfbfd5e6d2f8a042e0d36	on the influence of anisotropic diffusion filter on dorsal hand authentication using eigenveins	dorsal hand vein;biometric;anisotropic diffusion filter;feature extraction;eigenveins;pca;matching	In this paper, we present a competent approach for dorsal hand vein features extraction from near infrared images and PCA matching. Which is the anisotropic diffusion filter; we present first a review about this filter most used for the image enhancement. The physiological features characterize the dorsal venous network of the hand. These networks are single to each individual and can be used as a biometric system for person identification/authentication. An active near infrared method is used for image acquisition. The proposed approach uses an anisotropic diffusion technique for contrast enhancement and morphological filtering to extract the venous network and principal component analysis.		Sarah Hachemi Benziane;Abdelkader Benyettou	2018	Multidim. Syst. Sign. Process.	10.1007/s11045-017-0514-8	dorsal hand;filter (signal processing);anisotropic diffusion;computer vision;principal component analysis;mathematics;dorsum;feature extraction;biometrics;pattern recognition;authentication;artificial intelligence	Vision	33.513553069145416	-62.05739169014902	68066
123c3798779e1d85bc2a7864e1a133c4c2571080	quest: quadriletral senary bit pattern for facial expression recognition		Facial expression has significant role to analyzing human cognitive state. Deriving an accurate facial appearance representation is critical task for an automatic facial expression recognition application. This paper provides a new feature descriptor named as Quadrilateral Senary bit Pattern for facial expression recognition. The QUEST pattern encoded the intensity changes by emphasizing relationship between neighboring and reference pixels by dividing them into two quadrilaterals in local neighborhood. Thus, the resultant gradient edges reveal the transitional variation information, that improves the classification rate by discriminating expression classes. Moreover, it also enhances the capability of the descriptor to deal with view point variations and illumination changes. The trine relationship in quadrilateral structure helps to extract the expressive edges and suppressing noise elements to enhance the robustness to noisy conditions. The QUEST pattern generates a six-bit compact code, which improve the efficiency of the FER system with more discriminability. The effectiveness of proposed method is evaluated by conducting several experiments on four benchmark datasets: MMI, GEMEP-FERA, OULU-CASIA and ISED. The experimental results show better performance of the proposed method as compared to existing state-art-the approaches.		Monu Verma;Prafulla Saxena;Santosh Kumar Vipparthi;Girdhari Singh	2018	2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)		pattern recognition;senary;support vector machine;robustness (computer science);artificial intelligence;pixel;quadrilateral;facial expression;cognition;computer science	Vision	36.4323850261563	-57.65120830446123	68306
913b47c1ed8072b1248ed61a96dc3157d7c229a6	similar partial copy detection of line drawings using a cascade classifier and feature matching	line drawing;feature matching;copyright protection;line drawings;partial copy;region of interest;similar copy;object detection;image retrieval	Copyright protection of image publications is an important task of forensics. In this paper, we focus on line drawings, which are represented by lines in monochrome. Since partial copies and similar copies are always applied in plagiarisms of line drawings, we propose combining the technique of object detection and image retrieval to detect similar partial copies from suspicious images: first, detecting regions of interest (ROIs) by a cascade classifier; then, locate the corresponding source parts from copyrighted images using a feature matching method. The experimental results have proved the effectiveness of proposed method for detecting similar partial copies from complex backgrounds.	cascading classifiers;image retrieval;line drawing algorithm;monochrome;object detection;region of interest;sensor	Weihan Sun;Koichi Kise	2010		10.1007/978-3-642-19376-7_11	computer vision;image retrieval;computer science;pattern recognition;region of interest	AI	36.60434890428855	-63.118494395903014	68338
d41f3f473aa34f8c184f31bc18bb66c117d6fbc6	automatic engagement prediction with gap feature		In this paper, we propose an automatic engagement prediction method for the Engagement in the Wild sub-challenge of EmotiW 2018. We first design a novel Gaze-AU-Pose (GAP) feature taking into account the information of gaze, action units and head pose of a subject. The GAP feature is then used for the subsequent engagement level prediction. To efficiently predict the engagement level for a long-time video, we divide the long-time video into multiple overlapped video clips and extract GAP feature for each clip. A deep model consisting of a Gated Recurrent Unit (GRU) layer and a fully connected layer is used as the engagement predictor. Finally, a mean pooling layer is applied to the per-clip estimation to get the final engagement level of the whole video. Experimental results on the validation set and test set show the effectiveness of the proposed approach. In particular, our approach achieves a promising result with an MSE of 0.0724 on the test set of Engagement Prediction Challenge of EmotiW 2018.t with an MSE of 0.072391 on the test set of Engagement Prediction Challenge of EmotiW 2018.	academy;information;kerrison predictor;test set;video clip	Xuesong Niu;Hu Han;Jiabei Zeng;Xuran Sun;Shiguang Shan;Yan Huang;Songfan Yang;Xilin Chen	2018		10.1145/3242969.3264982	computer vision;clips;machine learning;pooling;test set;computer science;artificial intelligence	AI	24.966316932318207	-56.542977809608416	68340
34df09a9445089c8f23eff5b2a43a822c9713f6e	boosting chamfer matching by learning chamfer distance normalization	detection rate;template matching;matching method	We propose a novel technique that significantly improves the performance of oriented chamfer matching on images with cluttered background. Different to other matching methods, which only measures how well a template fits to an edge map, we evaluate the score of the template in comparison to auxiliary contours, which we call normalizers. We utilize AdaBoost to learn a Normalized Oriented Chamfer Distance (NOCD). Our experimental results demonstrate that it boosts the detection rate of the oriented chamfer distance. The simplicity and ease of training of NOCD on a small number of training samples promise that it can replace chamfer distance and oriented chamfer distance in any template matching application.	adaboost;chamfer;error-tolerant design;fits;hausdorff dimension;logistic regression;sparse matrix;template matching	Tianyang Ma;Xingwei Yang;Longin Jan Latecki	2010		10.1007/978-3-642-15555-0_33	computer vision;template matching;computer science;machine learning;pattern recognition;mathematics	Vision	32.41332115935235	-54.38713691433364	68833
4effb5276a0e3fbc299655a3c66ca8677c87567c	identity verification based on facial pose pool and bag of words model	bag of words model;facial pose pool;incremental clustering;gaussian mixture model;identity verification		bag-of-words model	Wangbin Chu;Yepeng Guan	2017	JACIII	10.20965/jaciii.2017.p0448	computer vision;speech recognition;computer science;bag-of-words model;machine learning;pattern recognition;mixture model	Logic	31.39127959725803	-57.870928177204114	68849
ea130dc78673c04a8cf9226f0a4e5c079d07ecf8	dimensionality reduction of features using multi resolution representation of decomposed images	disjoint set;dimensionality reduction;image decomposition;histogram of oriented gradient	A common objective in multi class, image analysis is to reduce the dimensionality of input data, and capture the most discriminant features in the projected space. In this work, we investigate a system that first finds clusters of similar points in feature space, using a nearest neighbor, graph based decomposition algorithm. This process transforms the original image data on to a subspace of identical dimensionality, but at a much flatter, color gamut. The intermediate representation of the segmented image, follows an effective, local descriptor operator that yields a marked compact feature vector, compared to the one obtained from a descriptor, immediately succeeding the native image. For evaluation, we study a generalized, multi resolution representation of decomposed images, parameterized by a broad range of a decreasing number of clusters. We conduct experiments on both non and correlated image sets, expressed in raw feature vectors of one million elements each, and demonstrate robust accuracy in applying our features to a linear SVM classifier. Compared to stateof-the-art systems of identical goals, our method shows increased dimensionality reduction, at a consistent feature matching performance.	algorithm;computer vision;dimensionality reduction;discriminant;experiment;feature vector;feedback;image analysis;intermediate representation;pixel;raw image format;scalability;visual descriptor	Avi Bleiweiss	2014		10.5220/0004917403160324	computer vision;feature detection;feature extraction;computer science;machine learning;disjoint sets;pattern recognition;mathematics;dimensionality reduction	Vision	36.88754385980815	-57.64442662799854	68979
e2686bd26e22e50a62f91562fa52f9973afc5255	robust localization of texts in real-world images	closed boundary;scene text localization;natural images;visual impairment assistance system;luminance contrast;content based image retrieval	Localization of texts in natural images could be an important stage in many applications such as content-based image retrieval, visual impairment assistance systems, automatic robot navigation in urban environments and tourist assistance systems. However due to the variations of font, script, scale, orientations, color, shadow and lighting conditions, robust scene text localization is still a challenging task. In this paper, we propose a novel method to localize not only Farsi/Arabic and Latin texts with different sizes, fonts and orientations but also low luminance contrast and poor quality ones in the natural images taken with uneven illumination conditions. Firstly, fast weighted median filtering as a nonlinear edge-preserving smoothing filter and then color contrast preserving decolorization are exploited to make the text localization system more robust for low luminance contrast and poor quality texts. In order to extract the Farsi/Arabic and Latin scene texts and also filter the nontext ones, a unified framework is proposed incorporating the maximally stable extremal regions and a novel proposed region detector called Stable Width Stroke Regions which is based on closed boundary regions. Phase congruency and Laplacian operators are exploited to extract the closed boundary regions. Finally, to extract the single text lines, the Meanshift clustering and radon transform were used. Experimental results show that the proposed method localize low luminance contrast and low quality scene texts for both Farsi/Arabic and Latin scripts encouragingly.		Shaho Ghanei;Karim Faez	2015	IJPRAI	10.1142/S0218001415550125	computer vision;speech recognition;artificial intelligence;pattern recognition	Vision	37.416545092632084	-59.51667797682868	69064
617039132b6a3f2fb7a717a16c9e2cdc39755701	novel framework for selecting the optimal feature vector from large feature spaces	2d rectangular filter;filter bank;neural networks;optimization technique;eye detection;simulated annealing;feature space;optimization problem;feature vector;lip detection;feature extraction;feature selection;optimization;face detection;neural network	There are several feature extracting techniques which can produce a large feature space for a given image. It is clear that only small numbers of these features are appropriate to classify the objects. But selecting an appropriate feature vector from the large feature space is a hard optimization problem. In this paper we address this problem using the well known optimization technique called Simulated Annealing. Also we show that how this framework can be used to design the optimal 2D rectangular filter banks for Printed Persian and English numerals classification, Printed English letters classification, Eye, Lip and Face detection problems.	feature vector;spaces	Hamed Habibi Aghdam;Saeid Payvar	2009		10.1007/978-3-642-02611-9_31	feature detection;speech recognition;feature vector;feature;computer science;machine learning;kanade–lucas–tomasi feature tracker;pattern recognition;k-nearest neighbors algorithm;feature;artificial neural network;feature model;dimensionality reduction	Vision	35.71468037066296	-63.06378854341978	69092
45c9e7cb6c0441a0ba8c7846387d06a0d6cda41c	incorporating privileged genetic information for fundus image based glaucoma detection		Visual features extracted from retinal fundus images have been increasingly used for glaucoma detection, as those images are generally easy to acquire. In recent years, genetic researchers have found that some single nucleic polymorphisms (SNPs) play important roles in the manifestation of glaucoma and also show superiority over fundus images for glaucoma detection. In this work, we propose to use the SNPs to form the so-called privileged information and deal with a practical problem where both fundus images and privileged genetic information exist for the training subjects, while the test objects only have fundus images. To solve this problem, we present an effective approach based on the learning using privileged information (LUPI) paradigm to train a predictive model for the image visual features. Extensive experiments demonstrate the usefulness of our approach in incorporating genetic information for fundus image based glaucoma detection.	cell nucleus;computation (action);experiment;extraction;genetic polymorphism;glaucoma;physical object;predictive modelling;programming paradigm;retina;single nucleotide polymorphism	Lixin Duan;Yanwu Xu;Wen Li;Lin Chen;Damon Wing Kee Wong;Tien Yin Wong;Jiang Liu	2014	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-319-10470-6_26	computer vision	Vision	29.940673207431413	-61.58819370899322	69220
721188b0feb483f7828a4b4c4c2204ca86e6890d	robust face presentation attack detection on smartphones : an approach based on variable focus		Smartphone based facial biometric systems have been well used in many of the security applications starting from simple phone unlocking to secure banking applications. This work presents a new approach of exploring the intrinsic characteristics of the smartphone camera to capture a number of stack images in the depth-of-field. With the set of stack images obtained, we present a new feature-free and classifier-free approach to provide the presentation attack resistant face biometric system. With the entire system implemented on the smartphone, we demonstrate the applicability of the proposed scheme in obtaining a stack of images with varying focus to effectively determine the presentation attacks. We create a new database of 13250 images at different focal length to present a detailed analysis of vulnerability together with the evaluation of proposed scheme. An extensive evaluation of the newly created database comprising of 5 different Presentation Attack Instruments (PAI) has demonstrated an outstanding performance on all 5 PAI through proposed approach. With the set ofcomplementary benefits of proposed approach illustrated in this work, we deduce the robustness towards unseen 2D attacks.	authentication;biometrics;focal (programming language);graphics display resolution;image resolution;laptop;online banking;printing;real life;retina display;sim lock;smartphone;ipad	Kiran B. Raja;Pankaj Shivdayal Wasnik;Ramachandra Raghavendra;Christoph Busch	2017	2017 IEEE International Joint Conference on Biometrics (IJCB)	10.1109/BTAS.2017.8272753	data mining;focal length;finance;robustness (computer science);phone;economics;biometrics;face presentation;vulnerability	Vision	28.142517968493284	-62.67062284872373	69224
fdc8e395c178c757089a29e10d47c3de641c7933	multi-script writer identification optimized with retrieval mechanism	document handling;handwriting recognition;edge hinge features;databases image edge detection writing feature extraction handwriting recognition fasteners text analysis;multi script handwritten documents;identification rates multiscript writer identification system handwritten document biometrics forensics smart meeting rooms historical document analysis edge hinge run length features individual writing style search space writer retrieval mechanism;writer identification;run length features;handwritten character recognition document handling handwriting recognition;writer retrieval;writer retrieval edge hinge features multi script handwritten documents run length features writer identification;handwritten character recognition	Identifying the writer of a handwritten document has been an active research area over the last few years with applications in biometrics, forensics, smart meeting rooms and historical document analysis. In this paper, we present a new writer identification system based on a retrieval mechanism. Texture based edge-hinge and run-length features are used to characterize the writing style of an individual. The effectiveness of the proposed system is evaluated on a total of 1583 writing samples in Arabic, German, English, French, and Greek from two different databases. The experimental evaluations reveal that reducing the search space using a writer retrieval mechanism prior to identification improves the identification rates.	biometrics;database;experiment;historical document;rejection sampling;run-length encoding	Chawki Djeddi;Imran Siddiqi;Labiba Souici-Meslati;Abdellatif Ennaji	2012	2012 International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2012.239	natural language processing;speech recognition;computer science;machine learning;pattern recognition;handwriting recognition	SE	33.92911009831673	-64.90521432454098	69243
0bcc140c26d0410510db86e049ba3ffdc7c9e0f9	palmprint verification with moments	legendre moments;pseudo zernike moments;image features;zernike moments;verification;orthogonality.;feature extraction	Palmprint verification is an approach for verifying a palmprint input by matching the input to the claimed identity template stored in a database. If the dissimilarity measure between the input and the claimed template is below the predefined threshold value, the palmprint input is verified possessing same identity as the claimed identity template. This paper introduces an experimental evaluation of the effectiveness of utilizing three well known orthogonal moments, namely Zernike moments, pseudo Zernike moments and Legendre moments, in the application of palmprint verification. Moments are the most commonly used technique in character feature extraction. The idea of implementing orthogonal moments as palmprint feature extractors is prompted by the fact that principal features of both character and palmprint are based on line structure. These orthogonal moments are able to define statistical and geometrical features containing line structure information about palmprint. An experimental study about verification rate of the palmprint authentication system using these three orthogonal moments as feature descriptors has been discussed here. Experimental results show that the performance of the system is dependent on the moment order as well as the type of moments. The orthogonal property of these moments is able to characterize independent features of the palmprint image and thus have minimum information redundancy in a moment set. Pseudo Zernike moments of order of 15 has the best performance among all the moments. Its verification rate is 95.75%, which also represents the overall performance of this palmprint verification system.	authentication;database;experiment;feature extraction;feature model;fingerprint;redundancy (information theory);verification and validation	Ying-Han Pang;Andrew Beng-Jin Teoh;David Chek Ling Ngo;Fu San Hiew	2004			redundancy (engineering);feature (computer vision);computer vision;legendre polynomials;zernike polynomials;feature extraction;velocity moments;artificial intelligence;mathematics;pattern recognition	DB	35.142051446296	-61.74111938081477	69255
fbe47da4c3df345a1174671e39e6416635f718a6	a study on combining image representations for image classification and retrieval	classifier fusion;semantic similarity;mahalanobis distance;one class classification;normal distribution;support vector data description;image database;image classification;feature space;data representation;image representation;dissimilarity;image retrieval	A flexible description of images is offered by a cloud of points in a feature space. In the context of image retrieval such clouds can be represented in a number of ways. Two approaches are here considered. The first approach is based on the assumption of a normal distribution, hence homogeneous clouds, while the second one focuses on the boundary description, which is more suitable for multimodal clouds. The images are then compared either by using the Mahalanobis distance or by the support vector data description (SVDD), respectively. The paper investigates some possibilities of combining the image clouds based on the idea that responses of several cloud descriptions may convey a pattern, specific for semantically similar images. A ranking of image dissimilarities is used as a comparison for two image databases targeting image classification and retrieval problems. We show that combining of the SVDD descriptions improves the retrieval performance with respect to ranking, on the contrary to the Mahalanobis case. Surprisingly, it turns out that the ranking of the Mahalanobis distances works well also for inhomogeneous images.	cloud computing;computer vision;data domain;database;disk image;experiment;feature vector;image retrieval;information;multimodal interaction;point cloud;tag cloud	Carmen Lai;David M. J. Tax;Robert P. W. Duin;Elzbieta Pekalska;Pavel Paclík	2004	IJPRAI	10.1142/S0218001404003459	normal distribution;image texture;computer vision;contextual image classification;semantic similarity;feature detection;visual word;feature vector;image retrieval;computer science;mahalanobis distance;machine learning;pattern recognition;mathematics;external data representation;automatic image annotation;one-class classification	Vision	37.68637765014497	-58.92303388823498	69330
6aeb31ab71c7615e34eaae6394555a39d3151179	an enhanced harmony search method for bangla handwritten character recognition using region sampling	support vector machines feature selection image classification natural language processing optical character recognition quadtrees search problems set theory;compounds;support vector machines;feature selection enhanced harmony search method bangla handwritten character recognition region sampling local region minimum number identification region space search bangla basic characters bangla compound characters bangla mixed characters svm based classifier support vector machine longest run based feature set image subregions cg based quad tree partitioning approach descriptive regions optical character recognition;nickel;feature extraction compounds accuracy character recognition support vector machines algorithm design and analysis nickel;accuracy;feature extraction;harmony search algorithm feature selection region space region sampling handwritten character recognition;character recognition;algorithm design and analysis	Identification of minimum number of local regions of a handwritten character image, containing well-defined discriminating features which are sufficient for a minimal but complete description of the character is a challenging task. A new region selection technique based on the idea of an enhanced Harmony Search methodology has been proposed here. The powerful framework of Harmony Search has been utilized to search the region space and detect only the most informative regions for correctly recognizing the handwritten character. The proposed method has been tested on handwritten samples of Bangla Basic, Compound and mixed (Basic and Compound characters)characters separately with SVM based classifier using a longest run based feature-set obtained from the image sub-regions formed by a CG based quad-tree partitioning approach. Applying this methodology on the above mentioned three types of datasets, respectively 43.75%, 12.5% and 37.5% gains have been achieved in terms of region reduction and 2.3%, 0.6% and 1.2% gains have been achieved in terms of recognition accuracy. The results show a sizeable reduction in the minimal number of descriptive regions as well a significant increase in recognition accuracy for all the datasets using the proposed technique. Thus the time and cost related to feature extraction is decreased without dampening the corresponding recognition accuracy.	feature extraction;handwriting recognition;harmony search;information;optical character recognition;quadtree;sampling (signal processing)	Ritesh Sarkhel;Amit K. Saha;Nibaran Das	2015	2015 IEEE 2nd International Conference on Recent Trends in Information Systems (ReTIS)	10.1109/ReTIS.2015.7232899	nickel;support vector machine;algorithm design;speech recognition;feature extraction;computer science;intelligent word recognition;machine learning;pattern recognition;accuracy and precision	Robotics	31.828565865581158	-64.687309423039	69364
31c03c7ee036f6b1a60351d26d410a596a2469a0	an enhanced hybrid mobilenet		Complicated and deep neural network models can achieve high accuracy for image recognition. However, they require a huge amount of computations and model parameters, which are not suitable for mobile and embedded devices. Therefore, MobileNet was proposed, which can reduce the number of parameters and computational cost dramatically. The main idea of MobileNet is to use a depthwise separable convolution. Two hyper-parameters, a width multiplier and a resolution multiplier are used to the trade-off between the accuracy and the latency. In this paper, we propose a new architecture to improve the MobileNet. Instead of using the resolution multiplier, we use a depth multiplier and combine with either Fractional Max Pooling or the max pooling. Experimental results on CIFAR database show that the proposed architecture can reduce the amount of computational cost and increase the accuracy simultaneously 1.This work is partly supported by Ministry of Science and Technology, R.O.C. under Contract No. MOST 106-2221-E-003-011.	algorithmic efficiency;computation;computer vision;convolution;convolutional neural network;deep learning;embedded system;mobile device	Hong-Yen Chen;Chung-Yen Su	2018	2018 9th International Conference on Awareness Science and Technology (iCAST)		latency (engineering);pooling;deep learning;architecture;artificial neural network;computation;theoretical computer science;convolution;artificial intelligence;multiplier (economics);computer science	Robotics	27.78179782294615	-53.755504512644805	69365
96d6e13292f38c41e8121ef43d1188342fb9c5ee	local apparent and latent direction extraction for palmprint recognition		Abstract Direction information of the palmprint provides one of the most promising features for palmprint recognition. However, more existing direction-based methods only extract the surface direction features from raw palmprint images and ignore the informative latent direction feature of the convolution layer of palmprint images. In this paper, we propose a novel double-layer direction extraction method for palmprint recognition. The method first extracts the apparent direction from the surface layer of a palmprint. Then, it further exploits the latent direction features from the energy map layer of the apparent direction. Lastly, by using the multiplication and addition schemes, the apparent and latent direction features are pooled as the histogram feature descriptor for palmprint recognition. The proposed method achieves state-of-the-art performance on four benchmark palmprint databases, namely the PolyU, IITD, GPDS and CASIA palmprint databases. In particular, the latent energy direction feature shows a promising performance for noisy palmprint image recognition.	fingerprint	Lunke Fei;Bob Zhang;Wei Zhang;Shaohua Teng	2019	Inf. Sci.	10.1016/j.ins.2018.09.032	machine learning;mathematics;convolution;histogram;pattern recognition;artificial intelligence	AI	35.535687369767146	-58.156398663879486	69413
7aa4ed5cf9b33e0c3d90616d52c0f264b416db54	persona: a method for facial analysis in video and application in entertainment		This article proposes the Persona method. The goal of the prosposed method is to learn and classify the facial actions of actors in video sequences. Persona is based on standard action units. We use a database with main expressions mapped and pre-classified that allows the automatic learning and faces selection. The learning stage uses Support Vector Machine (SVM) classifiers to identify expressions from a set of feature points tracked in the input video. After that, labeled control 3D masks are built for each selected action unit or expression, which composes the Persona structure. The proposed method is almost automatic (little intervention is needed) and does not require markers on the actor’s face or motion capture devices. Many applications are possible based on the Persona structure such as expression recognition, customized avatar deformation, and mood analysis, as discussed in this article.	3d printing;motion capture;persona (user experience);support vector machine	Adriana Braun;Rossana Baptista Queiroz;Won-Sook Lee;Bruno Feijó;Soraia Raupp Musse	2018	Computers in Entertainment	10.1145/3236495	discrete mathematics;persona;multimedia;entertainment;mathematics	Vision	28.82504761502991	-59.951920930728306	69615
fc407280151588aeef9f782166ce602dd63c6497	a large-scale study of fingerprint matching systems for sensor interoperability problem		The fingerprint is a commonly used biometric modality that is widely employed for authentication by law enforcement agencies and commercial applications. The designs of existing fingerprint matching methods are based on the hypothesis that the same sensor is used to capture fingerprints during enrollment and verification. Advances in fingerprint sensor technology have raised the question about the usability of current methods when different sensors are employed for enrollment and verification; this is a fingerprint sensor interoperability problem. To provide insight into this problem and assess the status of state-of-the-art matching methods to tackle this problem, we first analyze the characteristics of fingerprints captured with different sensors, which makes cross-sensor matching a challenging problem. We demonstrate the importance of fingerprint enhancement methods for cross-sensor matching. Finally, we conduct a comparative study of state-of-the-art fingerprint recognition methods and provide insight into their abilities to address this problem. We performed experiments using a public database (FingerPass) that contains nine datasets captured with different sensors. We analyzed the effects of different sensors and found that cross-sensor matching performance deteriorates when different sensors are used for enrollment and verification. In view of our analysis, we propose future research directions for this problem.	authentication;biometrics;database;experiment;fingerprint recognition;interoperability;law enforcement;matching;usability;verification of theories;sensor (device)	Hatim A. Aboalsamh;Mansour Alzuair	2018		10.3390/s18041008	data mining;electronic engineering;engineering;fingerprint recognition;usability;biometrics;fingerprint;authentication;interoperability problem	Mobile	29.194658096277177	-61.98937113046529	69620
31be61d548b0c4dbbd097ebe827c678c01fe2a4f	a fuzzy logic method of feature representation for shot boundary detection	compressed domain video processing and mpeg videos;mpeg videos;mpeg videos shot boundary detection compressed domain video processing;support vector machines;video signal processing;mpeg video;real time;video processing;compressed domain video processing;transform coding;fuzzy logic;streaming media;feature extraction;video signal processing feature extraction fuzzy logic support vector machines;fuzzy logic gunshot detection systems support vector machines support vector machine classification feature extraction shape video sequences humans transform coding training data;support vector machine;algorithm design and analysis;feature extraction fuzzy logic method feature representation shot boundary detection support vector machines fuzzification process trecvid07;shot boundary detection;real time systems	Unlike most approaches reported in the literature, our proposed algorithm is characterized by using the fuzzy logic method for feature representation of shot detection. Firstly, novel features are extracted in the compressed domain. Secondly, a fuzzy logic method is used to implement feature representation. Thirdly, multiple Support Vector Machines (SVMs) are constructed for further verification using features generated from the fuzzification process in step two. Our algorithm combines the merits of the generalization ability of the SVM and the comprehensibility of the fuzzy logic. We have carried out extensive experiments using the data from TRECVID07. Our proposed algorithm achieved very good detection results compared with TRECVID07 algorithms and its run-speed is 4 times faster than real-time video play.	algorithm;experiment;fuzzy logic;fuzzy set;real-time transcription;shot transition detection;support vector machine	Juan Chen;Stanley S. Ipson;Jianmin Jiang	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413644	support vector machine;computer vision;fuzzy classification;computer science;fuzzy number;machine learning;pattern recognition;fuzzy set operations	Robotics	38.74628617664745	-52.27096677280879	69630
8c5a341ba32a233f0c1e1bb53706ea4ae2dd8fcc	individuality of numerals	drugs;handwriting recognition;text analysis writing handwriting recognition computer science forensics drugs law legal factors pain power measurement;text analysis;law;conference paper;criminal justice;legal factors;cluster analysis;pain;writing;computer science;forensics;power measurement	The analysis of handwritten documents from the viewpoint of determining their writership has great bearing on the criminal justice system. In many cases, only a limited amount of handwriting is available and sometimes it consists of only numerals. Using a large number of handwritten numeral images extracted from about 3000 samples written by 1000 writers, a study of the individuality of numerals for identification/verification purposes was conducted. The individuality of numerals was studied using cluster analysis. Numerals discriminability was measured for writer verification. The study shows that some numerals present a higher discriminatory power and that their performances for the verification/identification tasks are very different.	cluster analysis;performance	Sargur N. Srihari;Catalin I. Tomai;Bin Zhang;Sangjik Lee	2003		10.1109/ICDAR.2003.1227826	criminal justice;speech recognition;computer science;artificial intelligence;machine learning;handwriting recognition;cluster analysis;writing	HCI	33.53427271974783	-64.71915533130736	70219
0efd1b3701360f174b03d0dc4091fc98e4affbb3	identification of road signs using a new ridgelet network	image recognition;visual vehicle navigation system;learning algorithm;neural networks;neural nets;image recognition driver information systems navigation learning artificial intelligence neural nets;atrocious conditions;navigation;directional multiresolution ridgelet network;neurons biological neural networks neural networks roads multiresolution analysis intelligent networks intelligent vehicles information processing radar signal processing signal resolution;binary ridgelet frame;roads;intelligent vehicles;recognition rate;information processing;signal resolution;flexible structure;recognition rate road sign identification visual vehicle navigation system directional multiresolution ridgelet network dmrr network binary ridgelet frame traffic road signs atrocious conditions learning algorithm;traffic road signs;navigation system;intelligent networks;neurons;learning artificial intelligence;dmrr network;road sign identification;multiresolution analysis;high dimension;high efficiency;driver information systems;radar signal processing;biological neural networks;neural network	Identification of road signs is a hotspot in the visual vehicle-navigation system. In this paper, a directional multiresolution ridgelet (DMRR) network is proposed based on a binary ridgelet frame to recognize traffic road signs in atrocious conditions. A detailed designing of the network and its learning algorithm are given. For the high efficiency of ridgelet in dealing with the directional information and the superiority in high dimension, this new network can obtain higher recognition rate, faster learning and has a more flexible structure than other traditional neural networks.	algorithm;artificial neural network;automotive navigation system;java hotspot virtual machine	Shuyuan Yang;Min Wang	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1465413	multiresolution analysis;computer vision;intelligent network;navigation;computer science;engineering;artificial intelligence;machine learning;artificial neural network	Robotics	25.964409527487874	-61.33063735448076	70451
98fc47426bd2c5154148804bd423e56ad5e2eb01	writer identification using curvature-free features	run lengths of local binary pattern;writer identification;cloud of line distribution;curvature free	In this chapter, we propose two novel and curvature-free features: run-lengths of Local Binary Pattern (LBPruns) and Cloud Of Line Distribution (COLD) features for writer identification. The LBPruns is the joint distribution of the traditional run-length and local binary pattern (LBP) methods, which computes the run-lengths of local binary patterns on both binarized images and gray scale images. The COLD feature is the joint distribution of the relation between orientation and length of line segments obtained from writing contours in handwritten documents. Our proposed LBPruns and COLD are textural-based curvature-free features and capture the line information of handwritten texts instead of the curvature information. The combination of the LBPruns and COLD features provides a significant improvement on the CERUG data set, handwritten documents on which contain a large number of irregular-curvature strokes. The proposed features evaluated on other two widely used data sets (Firemaker and IAM) demonstrate promising results.	belief propagation;binary pattern (image generation);code;grayscale;identity management;local binary patterns;run-length encoding	Sheng He;Lambert Schomaker	2017	Pattern Recognition	10.1016/j.patcog.2016.09.044	speech recognition;computer science;pattern recognition	Vision	34.49443350078904	-64.50919041237239	70519
84d0747c532e8a07482ecb0f839f25adc867b3d0	low-resolution character recognition by video-based super-resolution	video based super resolution;subspace method low resolution character recognition super resolution;image recognition;video signal processing character recognition image recognition image resolution;image resolution;video signal processing;character recognition image resolution image recognition digital cameras image reconstruction testing text analysis uniform resource locators target recognition training data;low resolution;digital camera;text analysis;testing;digital camera low resolution character recognition video based super resolution;digital cameras;training data;target recognition;image reconstruction;pixel;subspace method;super resolution;uniform resource locators;low resolution character recognition;character recognition	In this paper, we propose a method for recognizing low-resolution characters using a super-resolution technique. Although portable digital cameras can be used for camera based character recognition, the captured images contain several types of noises which make the recognition task difficult. We introduce a phase of super-resolution before the recognition to enhance the resolution of images obtained from a video. The proposed method uses the subspace method for the recognition of characters which are integrated from multiple low-resolution characters by the super-resolution technique. Experimental results show that the proposed method improves the recognition accuracy; we confirmed that the recognition rate for the input size of 7 x 7 pixels was 90.35%, and for the input size of 9 x 9 pixels was 99.97%.	digital camera;information;javaserver pages;optical character recognition;pixel;super smash bros.;super-resolution imaging	Ataru Ohkura;Daisuke Deguchi;Tomokazu Takahashi;Ichiro Ide;Hiroshi Murase	2009	2009 10th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2009.168	computer vision;text mining;speech recognition;image resolution;feature;computer science;three-dimensional face recognition;3d single-object recognition;computer graphics (images)	Robotics	35.451552504656604	-64.95344262732264	70550
83c84c862fca11b87a97e33e1daae7b3a4dc2672	sinogram synthesis using convolutional-neural-network for sparsely view-sampled ct			convolutional neural network	Jongha Lee;Hoyeon Lee;Seungryong Cho	2018		10.1117/12.2293244	convolutional neural network;artificial intelligence;computer science;pattern recognition	EDA	28.983513631054848	-57.09022013072687	70564
3b7281deadaf258a7e43080b7d1a69215f326653	improved competitive code for palmprint recognition using simplified gabor filter	efficient algorithm;competitive code;gabor filter;feature extraction;fast algorithm;simplified gabor sg	This paper presents a fast algorithm for extracting features using the Simplified Gabor (SG) for Competitive Coding-based palmprint recognition. The competitive code convolves the palmprint image with a bank of Gabor filters with different orientations. We use a simplified version of Gabor filters and an efficient algorithm for extracting features to modify the competitive code. Experimental results indicate that, using SG can achieve the verification accuracy similar to using common Gabor filters, while the runtime for feature extraction using SG is very fast compare to the original algorithm.	fingerprint;gabor filter	Jing Wei;Wei Jia;Hong Wang;Dan-Feng Zhu	2009		10.1007/978-3-642-04070-2_42	speech recognition;feature extraction;computer science;machine learning;pattern recognition;gabor wavelet	Vision	34.95703450688341	-60.977010211250146	70591
c3a29970a46c43bcc4484ef2e4904f0317bea28f	texture classification and retrieval using shearlets and linear regression	image recognition;shearlet;contourlets;subband dependence;texture retrieva;texture classification;texture retrieval;training;linear regression;wavelet transforms;computational modeling;vectors;pseudofeedback mechanism texture classification texture retrieval shearlet transform linear regression wavelet subbands statistical modeling image recognition image retrieval wavelet extension shearlet subband dependency energy features shearlet subband representation regression residuals contourlet domain;linear regression training vectors image recognition wavelet transforms computational modeling;期刊论文;shearlets;wavelet transforms feature extraction image classification image representation image retrieval image texture regression analysis;texture retrieval contourlets linear regression shearlets subband dependence texture classification	Statistical modeling of wavelet subbands has frequently been used for image recognition and retrieval. However, traditional wavelets are unsuitable for use with images containing distributed discontinuities, such as edges. Shearlets are a newly developed extension of wavelets that are better suited to image characterization. Here, we propose novel texture classification and retrieval methods that model adjacent shearlet subband dependences using linear regression. For texture classification, we use two energy features to represent each shearlet subband in order to overcome the limitation that subband coefficients are complex numbers. Linear regression is used to model the features of adjacent subbands; the regression residuals are then used to define the distance from a test texture to a texture class. Texture retrieval consists of two processes: the first is based on statistics in contourlet domains, while the second is performed using a pseudo-feedback mechanism based on linear regression modeling of shearlet subband dependences. Comprehensive validation experiments performed on five large texture datasets reveal that the proposed classification and retrieval methods outperform the current state-of-the-art.	classification;coefficient;computer vision;contourlet;experiment;feedback;linear iga bullous dermatosis;models, statistical;pseudo brand of pseudoephedrine;regression analysis;shearlet;wavelet	Yongsheng Dong;Dacheng Tao;Xuelong Li;Jinwen Ma;Jiexin Pu	2015	IEEE Transactions on Cybernetics	10.1109/TCYB.2014.2326059	image texture;computer vision;speech recognition;shearlet;computer science;pattern recognition;mathematics	Vision	37.409712672521835	-61.09284368429365	70734
9c57c78a839208eacd853306b3f48efbc15ff90a	mobile multi-scale vehicle detector and its application in traffic surveillance		Object detection is a major problem in computer vision. Recently, deep neural architectures have shown a dramatic boost in performance, but they are often too slow and burdensome for embedded and real-time applications such as video surveillance. In this paper, we describe a new object detection architecture that is faster than state-of-the-art detectors while improving the performance of small mobile models. Moreover, we apply this new architecture into the problem of vehicle detection, which is central to traffic surveillance systems. In more detail, our architecture uses an efficient backbone network in MobileNetV2, whose building blocks consist of depthwise convolutional layers. On top of this network, we build a feature pyramid using separable layers so that the model can detect objects at many scales. We train this network with smooth localization loss and weighted softmax loss in tandem with hard negative mining. Both training and test sets are built from recorded videos of Ho Chi Minh and Da Nang traffic or selected from DETRAC dataset. The experimental results show that our proposed solution can still achieve an mAP of 75% on the test set while using only around 3.4 million parameters and running at 100ms per image on a cheap machine.		Trung D. Q. Dang;Hy V. G. Che;Tien Ba Dinh	2018		10.1145/3287921.3287957	real-time computing;architecture;object detection;pyramid;backbone network;detector;separable space;test set;softmax function;computer science	Vision	27.866358162331725	-53.495281361854	70945
12b730f51094c68b96cb7e2305521f78562d75ae	unconstrained kannada handwritten character recognition using multi-level svm classifier		This paper presents an efficient zoning based method for recognition of handwritten Kannada characters using two sets of features, namely, crack codes (the line between the object pixel and background) and the density of the object pixels. A multi-level SVM is used for the classification purpose. The proposed method is implemented in two stages. In the first stage, similar shaped characters are combined into groups resulting in 22 classes instead of 49 classes, one class per character. Crack codes are used to assign the input character image to one of the groups. In the second stage, object pixel density is used to assign label to the input character image within that identified group. Experiments are performed on handwritten Kannada characters consisting of 24500 images with 500 samples for each character. Five-fold cross validation is used for result computation and average recognition rate of 91.02 % is obtained.	code;computation;cross-validation (statistics);experiment;feature extraction;handwriting recognition;multilevel security;optical character recognition;pixel density;xslt/muenchian grouping	Ganpat Singh G. Rajput;Rajeshwari Horakeri	2013		10.1007/978-3-642-45062-4_28	speech recognition	Vision	33.712562759705364	-65.36864541428264	70983
8aa85d2f81d7496cf7105ee0a3785f140ddaa367	efficient processing of mrfs for unconstrained-pose face recognition	graph theory;optimisation;paper;image resolution;image matching;biometrics;image classification;computer vision;cuda;renormalisation face recognition feature extraction graph theory graphics processing units image classification image matching image resolution message passing optimisation pose estimation;identification paradigm unconstrained pose face recognition pose invariant face recognition mrf matching model mrf inference time gpu dual decomposition framework optimisation multiresolution approach renormalisation group theory rgt message passing incremental subgradient approach graph construction daisy features node attributes high cross pose invariance discriminatory capability classification stage multiscale lbp histograms m2vts database feret database lfw database unseen pair matching paradigm verification paradigm;face recognition;feature extraction;graphics processing units;message passing;nvidia;algorithms;computer science;face face recognition databases graphics processing units vectors inference algorithms message passing;nvidia geforce gtx 460;renormalisation;pose estimation	The paper addresses the problem of pose-invariant recognition of faces via an MRF matching model. Unlike previous costly matching approaches, the proposed algorithm employs effective techniques to reduce the MRF inference time. To this end, processing is done in a parallel fashion on a GPU employing a dual decomposition framework. The optimisation is further accelerated taking a multi-resolution approach based on the Renormalisation Group Theory (RGT) along with efficient methods for message passing and the incremental subgradient approach. For the graph construction, Daisy features are used as node attributes exhibiting high cross-pose invariance, while high discriminatory capability in the classification stage is obtained via multi-scale LBP histograms. The experimental evaluation of the method is performed via extensive tests on the databases of XM2VTS, FERET and LFW in verification, identification and the unseen pair-matching paradigms. The proposed approach achieves state-of-the-art performance in pose-invariant recognition of faces and performs as well or better than the existing methods in the unconstrained settings of the challenging LFW database using a single feature for classification.	algorithm;baseline (configuration management);daisy digital talking book;database;distance transform;expectation propagation;feret (facial recognition technology);facial recognition system;graphics processing unit;image registration;iranian.com;lagrangian relaxation;local binary patterns;markov random field;mathematical optimization;message passing;multiresolution analysis;reference frame (video);speedup;subderivative;subgradient method	Shervin Rahimzadeh Arashloo;Josef Kittler	2013	2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)	10.1109/BTAS.2013.6712721	facial recognition system;computer vision;contextual image classification;message passing;pose;image resolution;feature extraction;computer science;graph theory;machine learning;pattern recognition;biometrics	Vision	34.36562559835877	-55.49278985570257	71075
e963ac953e084f6223a5d8379a52631508426ed7	fcn and lstm based computer vision system for recognition of vehicle type, license plate number, and registration country		We propose an advanced Automatic number-plate recognition (ANPR) system, which not only recognizes the number and the issuing state, but also the type and location of the vehicle in the input image. The system is based on a combination of existing methods, modifications to neural network architectures and improvements in the training process. The proposed system uses machine-learning approach and consists of three main parts: segmentation of input image by Fully Convolutional Network for localization of license plate and determination of vehicle type; recognition of the characters of the localized plate by a Maxout CNN and LSTM; determination of the state that has issued the license plate by a CNN. The training of these neural network models is accomplished using a manually labeled custom dataset, which is expanded with data augmented techniques. The resulting system is capable of localizing and classifying multiple types of vehicles (including motorcycles and emergency vehicles) as well as their license plates. The achieved precision of the localization is 99.5%. The whole number recognition accuracy is 96.7% and character level recognition accuracy is 98.8%. The determination of issuing state is precise in 92.8% cases.	artificial neural network;automatic number plate recognition;autostereogram;central processing unit;computer vision;convolutional neural network;feature (computer vision);feature extraction;graphics processing unit;image processing;internationalization and localization;long short-term memory;machine learning;mathematical optimization;mobile device;real life	Nauris Dorbe;Aigars Jaundalders;Roberts Kadikis;Krisjanis Nesenbergs	2018	Automatic Control and Computer Sciences	10.3103/S0146411618020104	license;whole number;image segmentation;artificial neural network;computer science;computer vision;segmentation;artificial intelligence	Vision	29.218859523298555	-53.69852940272937	71143
dce770b0063ce29aabff44403fb64be7e613d68f	improve non-graph matching feature-based face recognition performance by using a multi-stage matching strategy		In this paper, a multi-stage matching strategy that deter- mines the recognition result step by step is employed to improve the recognition performance of a non-graph matching feature-based face recognition. As the gallery size increases, correct correspondence of fea- ture points between the probe image and training images becomes more and more difficult so that the recognition accuracy degrades gradually. To deal with the recognition degradation problem, we propose a multi-stage matching strategy for the non-graph matching feature-based method. Instead of finding the best match, each step picks out one half of the best matching candidates and removes the other half. The behavior of picking and removing repeats until the number of the remaining candi- dates is small enough to decide the final result. The experimental result shows that with the multi-stage matching strategy, the recognition per- formance is remarkably improved. Moreover, the improvement level also increases with the gallery size.		Xianming Chen;Wenyin Zhang;Chaoyang Zhang;Zhaoxian Zhou	2015		10.1007/978-3-319-27863-6_23	artificial intelligence;machine learning;pattern recognition	Vision	33.30087281190702	-56.467906735714784	71155
992c7f025449b43baca7be9c43bdbb6b9e0c3b62	study of ct image texture using deep learning techniques			deep learning;image texture	Sandeep Dutta;Jiahua Fan;David Chevalier	2018		10.1117/12.2292560	computer vision;deep learning;image texture;artificial intelligence;computer science	Vision	29.508113621142254	-57.312518690718534	71182
971bd366b86a0906bf6ca2dd7951723aa6107a89	a novel method for 2dpca-ica in face recognition	2dpca;face recognition;ica	In this paper, a novel method for independent component analysis (ICA) with 2-D Principle Component Analysis (2DPCA) in face recognition is presented, called 2DPCA-ICA. In this method, 2DPCA is used for dimension reduction, and ICA for recognition. As opposed to the method for ICA based on PCA in face recognition (PCA-ICA), 2DPCA is based on 2-D face image matrix, an image covariance matrix is constructed directly using 2-D image matrix, and its eigenvectors corresponding to the several larger eigenvalues are derived for whitened image matrix. It overcomes the shortcomings of PCA- ICA. To test 2DPCA-ICA and evaluate its performance, experiments are performed on Yale and ORL (Olivetti Research Laboratory) face database. Correct recognition rate of 2DPCA-ICA across all trials is higher than that of PCA-ICA and 2DPCA. Experimental results also show that features of face image extracted are more efficient by way of 2DPCA-ICA. Therefore, 2DPCA- ICA is more valid in face recognition.	facial recognition system;independent computing architecture	Jun-ying Gan;Chun-zhi Li;Dang-pei Zhou	2007		10.1007/978-3-540-74282-1_135	facial recognition system;computer vision;speech recognition;computer science;pattern recognition;eigenface	Vision	34.224277618713984	-58.675512668017134	71266
fda66ee356f578b2f5c81fdff0c8ee7e819232a6	supervised classification for object identification in urban areas using satellite imagery		This paper presents a useful method to achieve classification in satellite imagery. The approach is based on pixel level study employing various features such as correlation, homogeneity, energy and contrast. In this study gray-scale images are used for training the classification model. For supervised classification, two classification techniques are employed namely the Support Vector Machine (SVM) and the Naïve Bayes. With textural features used for gray-scale images, Naïve Bayes performs better with an overall accuracy of 76% compared to 68% achieved by SVM. The computational time is evaluated while performing the experiment with two different window sizes i.e., 50 × 50 and 70 × 70. The required computational time on a single image is found to be 27 seconds for a window size of 70 × 70 and 45 seconds for a window size of 50 × 50.	autostereogram;computation;grayscale;machine learning;naive bayes classifier;pixel;supervised learning;support vector machine;time complexity;window function	Hazrat Ali;Adnan Ali Awan;Sanaullah Khan;Omer Shafique;Atiq Ur Rahman;Shahid Khan	2018	2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)	10.1109/ICOMET.2018.8346383	machine learning;visualization;naive bayes classifier;support vector machine;homogeneity (statistics);microsoft windows;pixel;satellite imagery;feature extraction;mathematics;artificial intelligence;pattern recognition	Vision	31.547529705070257	-55.83258632398933	71666
46b0f5c2690bed463c7959b58237a08c0083dce4	a combined feature extraction method for automated face recognition in classroom environment		Face recognition is a pattern recognition technique and one of the most important biometrics; it is used in a broad spectrum of applications. Classroom attendance management system is one of the applications. This paper proposes an optimized method of face detection using viola jones and face recognition using SURF and HOG feature extraction methods. The proposed model takes a video frame from an input device, then it detects faces in that frame using proposed optimized face detection method. Lastly, the detected faces are matched with pre-loaded customized database using proposed face recognition method. In addition we have tested our model with other existing model using two different customized datasets. Without human intervention this proposed model almost accurately completes the attendance of students in a class.	facial recognition system;feature extraction	Md. Shafiqul Islam;Asif Mahmud;Azmina Akter Papeya;Irin Sultana Onny;Jia Uddin	2017		10.1007/978-3-319-67934-1_38	face detection;input device;facial recognition system;attendance management;feature extraction;viola–jones object detection framework;artificial intelligence;attendance;computer vision;computer science;biometrics;pattern recognition	Vision	30.198858897367813	-60.01504384973841	71678
e07054643c63491af82749e8711b453aff84a325	decision-level fusion in fingerprint verification	verification;classifier combination;biometrics;combination of matchers;neyman pearson;density estimation;parzen density estimate;fingerprint;feature selection;fingerprint verification	A scheme is proposed for classifier combination at decision level which stresses the importance of classifier selection during combination. The proposed scheme is optimal (in the Neyman–Pearson sense) when sufficient data are available to obtain reasonable estimates of the join densities of classifier outputs. Four different fingerprint matching algorithms are combined using the proposed scheme to improve the accuracy of a fingerprint verification system. Experiments conducted on a large fingerprint database (∼2700 fingerprints) confirm the effectiveness of the proposed integration scheme. An overall matching performance increase of ∼3% is achieved. We further show that a combination of multiple impressions or multiple fingers improves the verification performance by more than 4% and 5%, respectively. Analysis of the results provide some insight into the various decision-level classifier combination strategies.	fingerprint	Salil Prabhakar;Anil K. Jain	2002	Pattern Recognition	10.1016/S0031-3203(01)00103-0	fingerprint verification competition;fingerprint;verification;density estimation;computer science;machine learning;pattern recognition;data mining;mathematics;feature selection;biometrics	Vision	31.353635450038436	-61.83210880996194	71732
0ae0ee0a4823e2897db36a5603caade6fd96b458	recognizing cultural events in images: a study of image categorization models	image recognition;support vector machines;training;transforms feature extraction history image colour analysis image matching image representation object recognition;visualization;chalearn dataset cultural event recognition image categorization models spatial pyramid matching regularized max pooling sift features color features;image color analysis;cultural differences image color analysis image recognition feature extraction support vector machines training visualization;feature extraction;cultural differences	The goal of this work is to study recognition of cultural events represented in still images. We pose cultural event recognition as an image categorization problem, and we study the performance of several state-of-the-art image categorization approaches, including Spatial Pyramid Matching and Regularized Max Pooling. We consider SIFT and color features as well as the recently proposed CNN features. Experiments on the ChaLearn dataset of 50 cultural events, we find that Regularized Max Pooling with CNN, SIFT, and Color features achieves the best performance.	categorization;color;expect;experiment;ibm notes;imagenet;risk management plan;scale-invariant feature transform;super paper mario	Heeyoung Kwon;Kiwon Yun;Minh Hoai;Dimitris Samaras	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2015.7301336	support vector machine;computer vision;feature detection;visualization;feature extraction;computer science;machine learning;pattern recognition;cultural diversity	Vision	31.2506810905784	-56.45740678892943	71787
e5847f1be7c009b65801878047859d8f4cec54be	gesture recognition benchmark based on mobile phone		Mobile phone plays an important role in our daily life. This paper develops a gesture recognition benchmark based on sensors of mobile phone. The built-in micro gyroscope and accelerometer of mobile phone can efficiently measure the accelerations and angular velocities along x-, y- and z-axis, which are used as the input data. We calculate the energy of the input data to reduce the effect of the phone’s posture variations. A large database is collected, which contains more than 1,000 samples of 8 gestures. The Hidden Markov Model (HMM), K-Nearest Neighbor (KNN) and Support Vector Machine (SVM) are tested on the benchmark. The experimental results indicated that the employed methods can effectively recognize the gestures. To promote research on this topic, the source code and database are made available to the public. (mpl.buaa.edu.cn or correspondence author)	benchmark (computing);gesture recognition;mobile phone	Chunyu Xie;Shangzhen Luan;Hainan Wang;Baochang Zhang	2016		10.1007/978-3-319-46654-5_48	speech recognition	ML	28.57388003249998	-60.22091514319629	71940
4df81b59aede1fe36f93049b66d9559a8f8d61dd	attributes and action recognition based on convolutional neural networks and spatial pyramid vlad encoding	liverpool;repository;university	© Springer International Publishing AG 2017.Determination of human attributes and recognition of actions in still images are two related and challenging tasks in computer vision, which often appear in fine-grained domains where the distinctions between the different categories are very small. Deep Convolutional Neural Network (CNN) models have demonstrated their remarkable representational learning capability through various examples. However, the successes are very limited for attributes and action recognition as the potential of CNNs to acquire both of the global and local information of an image remains largely unexplored. This paper proposes to tackle the problem with an encoding of a spatial pyramid Vector of Locally Aggregated Descriptors (VLAD) on top of CNN features. With region proposals generated by Edgeboxes, a compact and efficient representation of an image is thus produced for subsequent prediction of attributes and classification of actions. The proposed scheme is validated with competitive results on two benchmark datasets: 90.4% mean Average Precision (mAP) on the Berkeley Attributes of People dataset and 88.5% mAP on the Stanford 40 action dataset.		Shiyang Yan;Jeremy S. Smith;Bailing Zhang	2016		10.1007/978-3-319-54526-4_37	computer vision;computer science;artificial intelligence;machine learning;data mining;mathematics	Vision	29.143948430638766	-52.13833081653213	71944
14fd9704e6ccb5c7d5ebbc099f39d3086d098863	new feature for shadow detection by combination of two features robust to illumination changes		Abstract   Computer vision methods need to deal with shadows explicitly because shadows often have a negative effect on the results computed. A new shadow detection method is proposed. The proposed method is a shadow model based method. A new feature for detecting shadows is introduced. The feature is obtained by L*a*b* components, Peripheral Increment Sign Correlation and Normalized Vector Distance. These features are robust to illumination changes. Shadows can be treated as local illumination changes. Using these features results in removing shadow effects, in part. The histogram is generated by the three features and is treated as the feature for detecting shadows. The SVM is used for the classifier. The SVM is trained in advance by shadow data and the trained SVM is used for detecting shadows. The proposed method can extract shadows with the accuracy similar to the previous approach in shorter time. Results are demonstrated by experiments using the real videos.		Kota Higashi;Shinji Fukui;Yuji Iwahori;Yoshinori Adachi;Manas Kamal Bhuyan	2016		10.1016/j.procs.2016.08.268	computer vision;pattern recognition;computer graphics (images)	Vision	33.25153637626256	-59.024613909064435	71964
77ef07547ee0c57001ef823b40a25e3d5dfc34c7	a comprehensive study on third order statistical features for image splicing detection	image splicing detection;third order statistical feature;pca	Second order statistical features (e.g. Markov transposition probability matrix and gray level co-occurrence matrix) have been proved to be effective for passive image forgery detection in the past few years. In this paper, third order statistical features are proposed for image splicing detection. We model the thresholded adjacent difference block DCT coefficient array of an image as conditional co-occurrence probability matrix, second order Markov transition probability matrix and second order co-occurrence matrix. Since the dimensionality exponentially depends on the order, dimensionality of the third order features is much larger than that of second order features, principal component analysis (PCA) is therefore introduced in our work to overcome the high dimensionality introduced computational complexity and the possible overfitting for a kernel based supervised classifier. Experimental results show that conditional co-occurrence probability matrix outperforms second order features and PCA is proved to be an effective dimensionality reduction tool for image splicing detection. We also test the robustness of third order statistical features, despite higher dimensionality, third order statistical features demonstrate the same robustness as that of second order features.	authentication;co-occurrence matrix;coefficient;computational complexity theory;dimensionality reduction;discrete cosine transform;document-term matrix;grayscale;machine learning;markov chain;overfitting;principal component analysis;stochastic matrix	Xudong Zhao;Shilin Wang;Shenghong Li;Jianhua Li	2011		10.1007/978-3-642-32205-1_20	computer science;machine learning;pattern recognition;mathematics;statistics;principal component analysis	ML	36.49304860494107	-61.65447850697954	72041
42f6b7abba9b3b02203282ac14ed02693cde714d	local descriptor matching with support vector machines	local descriptors;support vector machines;image registration;support vector machine	Local descriptor matching is the most overlooked stage of the three stages of the local descriptor process, and this paper proposes a new method for matching local descriptors based on support vector machines. Results from experiments show that the developed method is more robust for matching local descriptors for all image transformations considered. The method is able to be integrated with different local descriptor methods, and with different machine learning algorithms and this shows that the approach is sufficiently robust and versatile.	support vector machine	Dawei Cheng;S. Q. Xie;E. Hämmerle	2010	I. J. Information Acquisition	10.1142/S0219878910002051	support vector machine;computer vision;local binary patterns;computer science;machine learning;pattern recognition	Vision	38.58801255057497	-56.84582096355764	72059
02917d82c3994c927a4ef2a9585fc189fc8e1435	a context-based model of attention		It is well known that natural visual systems rely on attentio al mechanisms that select and process relevant objects in an efficient way. Similarly, rtificial visual systems need attentional-selection mechanisms to reduce the computati onal burden of processing entire images. So, their aim is to focus on the parts containing the o bj ct of interest. In the domain of natural vision the locus of selection has been deba ted for many years (see [1] for an overview). The two extreme views are (1) that selectio n takes place at an early stage of visual processing (i.e., early selection), and (2) that i t t kes place at a late stage (i.e., late selection). In early selection, attention is guided by cons picuous changes in elementary features, such as colour, texture, or spatial frequency. Mo dels of early selection contain so-called saliency maps that contain the response respond t o conspicuous changes in a single feature, e.g., [4]. The activities in these maps repr esent locations to be attended. In late selection, attention is guided by complex feature co mbinations or even objects [7]. Models of late selection rely on object templates that a re matched to the contents of images [6]. From a computational point of view, both early and late selec tion pose considerable problems. In early selection, the likelihood of mistakes is large, since in natural images many changes of elementary features occur. As a result, the a ttentional mechanism has to visit many locations of which only a few correspond to obje cts of interest. The objectbased guidance of attention in late selection renders the se lectiv function useless, as late selection requires the location (identity) of the objects t o be known in advance. Several models have attempted to combine saliency maps with template matching (see, e.g., [4]). Below, we propose a novel approach, the COB A (COntext BAsed) model of attentional selection. The main idea underlying the COBA model is that the spatial context of an object is important for its localisation, as il lustrated in the left panel of Figure 1. The two small square images (left in the figure) are e nlarged versions of the square regions indicated by boxes in the large images. Consi dered in isolation, both small images are highly similar to faces. When considered in their natural context, the small images will not be interpreted as faces [2]. In the COBA model, attentional selection is guided by an obje ct saliency map. Active locations on the map indicate likely locations of objects. U sing automatic learning, the object saliency map is generated from feature combinations hat form a likely spatial context for objects. In this paper we focus on applying the COBA m odel to spatial contexts and on the detection of faces in natural images. Our method is related to the more global selection method proposed by Torralba and Sinha [8]. The COBA model consists of three stages. In the first stage, wi ndows are taken from natural images at a grid of scales and locations. The content s of these windows are pre-	business continuity planning;computation;face (geometry);locus;map;microsoft windows;precondition;rendering (computer graphics);template matching;texture mapping;top-hat transform	Niek Bergboer;Eric O. Postma;H. Jaap van den Herik	2004			artificial intelligence;machine learning	Vision	38.4502386091962	-55.31964397856399	72177
f7d4973d09c5eb3fa03f76c03cf6f40cf99c94af	analysis and evaluation of regression-based methods for facial pose classification	local linear regression;facial pose;pose classification;global linear regression;llr;face recognition;glr	Facial pose classification is one of the important steps in some pose invariant face recognition methods. Regression has been used for facial pose classification. In this paper, facial pose classification approaches using different types of regression are compared in terms of average classification accuracy and computation time. We also analyse the time complexity of regression-based approaches for pose classification. Performance of these approaches is also compared with other popular approaches in terms of classification accuracy. Experimental results on two publicly available face databases (PIE and FERET) show that the performance of regression-based approaches is comparable and generally outperform other approaches. Among regression-based methods, local linear regression with overlap outperforms other methods. In terms of computation time, global linear regression and nonlinear regression are comparable and better than others. We also analysed the performance of regression-based approaches after addi...		Ajay Jaiswal;Nitin Kumar;R. K. Agrawal	2015	IJAPR	10.1504/IJAPR.2015.068936	computer vision;machine learning;pattern recognition;mathematics	Vision	28.56381238323188	-58.74406250855041	72410
cf09b97bfe90989ccd6eaafab8e9481263c291ca	unsupervised feature selection and category formation for generic object recognition	unsupervised feature selection;target feature point;feature point;unsupervised object category formation;following feature;category formation;unsupervised method;object category formation;object category;spatial relation;classification result;generic object recognition	unsupervised feature selection;target feature point;feature point;unsupervised object category formation;following feature;category formation;unsupervised method;object category formation;object category;spatial relation;classification result;generic object recognition	feature selection;outline of object recognition	Hirokazu Madokoro;Masahiro Tsukada;Kazuhito Sato	2011		10.1007/978-3-642-23672-3_52	computer vision;machine learning;pattern recognition;mathematics	Vision	30.244166991618023	-56.31129372487258	72779
4db19a74f2c2cf846815bba8d680e6173a2a124c	texture similarity estimation using contours		In a study of 51 computational features sets Dong et al. [1] showed that none of these managed to estimate texture similarity well and, coincidently, none of these computed higher order statistics (HOS) over large regions (that is larger than 19×19 pixels). Yet it is well-known that the human visual system is extremely adept at extracting long-range aperiodic (and periodic) “contour” characteristics from images [5, 6]. It is our hypothesis that HOS computed over larger spatial extent in the form of contour data are important for estimating perceptual texture similarity. However, to the authors’ knowledge the use of contour data (rather than edge data) has not been proposed before as the basis for a set of feature vectors. We provide results of an experiment with 334 textures that shows that contour data is more important than local image patches, or 2nd-order global data, to human observers. We also propose a contour-based feature set that exploits the long-range HOS encoded in the spatial distribution and orientation of contour segments. We compare it against the 51 feature sets tested by Dong et al. [1, 2] and another contour model derived from shape recognition. The results show that the proposed method outperforms all the other feature sets in a pairs-of-pairs task and all but two feature sets in a ranking task. We attribute this promising performance to the fact that this new feature set encodes long-range HOS.	aperiodic graph;computation;contour line;encode;feature model;feature vector;pixel;semantic similarity;similarity measure;snappy	Xinghui Dong;Mike J. Chantler	2014			pattern recognition;artificial intelligence;computer science;texture filtering;computer vision	ML	37.54006525031536	-59.409604916575766	72837
bd1db6e4ebe146d230b2b0e34dad8e2532b64cc4	a new scheme for text line and character segmentation from gray scale images of palm leaf manuscript	filtering;image segmentation;compounds;text line segmentation;character segmentation;image;optical character recognition software;indexing;balinese script;text recognition;palm leaf manuscript;brushes	Most of text line and character segmentation methods for handwritten document image basically still depend on the binary image of the document. Unfortunately, for palm leaf manuscript images, the binarization process is a real challenge. We proposed a new binarization free scheme for text line and character segmentation for palm leaf manuscript images. Our scheme consists of 4 sub-tasks: brushing character area of gray level images with minimum filtering, average block projection profile of gray level images, selection of candidate area for segmentation path, and construction of nonlinear segmentation path. For evaluation, we compared our method with the shredding method which is applied in three different schemes of experiment. The experimental results showed that the proposed method performed optimal on the palm leaf manuscript images which contain discolored parts, with low intensity variations or poor contrast, random noises, and fading.	binary image;brushing and linking;grayscale;nonlinear system;shredding (disassembling genomic data)	Made Windu Antara Kesiman;Jean-Christophe Burie;Jean-Marc Ogier	2016	2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR)	10.1109/ICFHR.2016.0068	filter;computer vision;search engine indexing;speech recognition;computer science;image;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	36.94810577674338	-66.1112878872745	72949
11038696b6b5527817d2de0b954d17ad48ff603e	food image recognition by personalized classifier		Since the development of food diaries could enable people to develop healthy eating habits, food image recognition is in high demand to reduce the effort in food recording. Previous studies have worked on this challenging domain with datasets having fixed numbers of samples and classes. However, in the real-world setting, it is impossible to include all of the foods in the database because the number of classes of foods is large and increases continually. In addition to that, inter-class similarity and intra-class diversity also bring difficulties to the recognition. In this paper, we attempted to solve these problems by using deep convolutional neural network features to build a personalized classifier which incrementally learns the user's data and adapts to the user's eating habit. As a result, we achieved the state-of-the-art accuracy of food image recognition by the personalization of 300 food records per user.		Qing Yu;Masashi Anzawa;Sosuke Amano;Makoto Ogawa;Kiyoharu Aizawa	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451422	convolutional neural network;personalization;computer vision;feature extraction;artificial neural network;artificial intelligence;classifier (linguistics);pattern recognition;computer science	Vision	24.65259009877921	-57.75504984710919	73147
f021588ab3fef27d569e5f61969ea47f21c8bcfd	results obtained using a simple character recognition procedure on munson's handprinted data	munson s data;preprocessing character recognition character size normalization classification experimental methodology feature extraction handprinted characters limited data experiments munson s data pattern recognition;handprinted characters;statistical independence;training;testing;classification;arrays;accuracy;system evaluation;character size normalization;feature extraction;limited data experiments;pattern recognition;experimental methodology;preprocessing;character recognition;feature extraction training testing character recognition pattern recognition arrays accuracy	The number of black points in each of the 25 nonoverlapping square regions of a size-normalized character matrix were used to recognize the 3822 uppercase handprinted alphabetic characters from Munson's multiauthor data set. The recognition accuracy obtained using a Bayes' classifier, which assumes statistically independent features, compares favorably with earlier results obtained using recognition systems having complexity comparable to ours. Included are results and a recommendation regarding system evaluation procedures.	statistical classification	A. B. Shahidul Hussain;Godfried T. Toussaint;Robert W. Donaldson	1972	IEEE Transactions on Computers	10.1109/TC.1972.5008927	independence;speech recognition;feature extraction;biological classification;computer science;machine learning;pattern recognition;accuracy and precision;software testing;preprocessor;statistics	Vision	31.87352881396689	-65.14339760518477	73181
8ad20c36d242597e4be5e27aed0af7ec054840ff	a regionalized content-based image retrieval framework	content-based retrieval;feature extraction;image retrieval;image segmentation;image texture;muvis framework;arbitrary-shape regions;block-based approach;feature extraction method;grouping approach;region proximity;region topology;regionalized cbir framework;regionalized content-based image retrieval framework;regionalized features;segmentation method;texture descriptors;visual descriptors;visual scenery	Utilizing regionalized features in Content-based Image Retrieval (CBIR) has been a dynamic research area over the past years. Several systems have been developed using their specific segmentation and feature extraction methods. In this paper, a strategy to model a regionalized CBIR framework is presented. Here, segmentation and local feature extraction are not specified and considered as “blackboxes”, which allows application of any segmentation method and visual descriptors. The proposed framework further adopts a grouping approach in order to “correct” possible over segmentation faults and a spatial feature called region proximity to describe regions topology in a visual scenery by a block-based approach. Using the MUVIS framework the proposed approach is developed and tested as feature extraction module, and its retrieval performance is compared against two frame-based color-texture descriptors. Experiments are carried out on synthetic and natural image databases and results indicate that a promising retrieval performance can be obtained if the segmentation quality is reasonable; however texture descriptors in general are degraded whenever applied on arbitrary-shape regions.	black box;content-based image retrieval;database;feature extraction;synthetic intelligence;visual descriptor	Stefan Uhlmann;Serkan Kiranyaz;Moncef Gabbouj	2007	2007 15th European Signal Processing Conference		image texture;computer vision;feature detection;visual word;geography;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation;information retrieval	Vision	38.670450405010065	-61.855954325105415	73216
224bbbc9a71e5cd537653d07b8eb42548391a40d	document signature using intrinsic features for counterfeit detection	global optimization;connected component	Document security does not only play an important role in specific domains e.g. passports, checks and degrees but also in every day documents e.g. bills and vouchers. Using special high-security features for this class of documents is not feasible due to the cost and the complexity of these methods. We present an approach for detecting falsified documents using a document signature obtained from its intrinsic features: bounding boxes of connected components are used as a signature. Using the model signature learned from a set of original bills, our approach can identify documents whose signature significantly differs from the model signature. Our approach uses globally optimal document alignment to build a model signature that can be used to compute the probability of a new document being an original one. Preliminary evaluation shows that the method is able to reliably detect faked documents.	connected component (graph theory);maxima and minima;mock object;operating system;printer (computing);printing;sensor;turing test	Joost van Beusekom;Faisal Shafait;Thomas M. Breuel	2008		10.1007/978-3-540-85303-9_5	connected component;computer science;data mining;database;world wide web	Web+IR	36.03509224823928	-64.03421969061357	73236
2609079d682998da2bc4315b55a29bafe4df414e	on rank aggregation for face recognition from videos	video signal processing;video signal processing face recognition image matching;image matching;face recognition;dictionary based face recognition video based face recognition rank aggregation;youtube faces database rank aggregation video based face recognition algorithm discriminative video signature still face images intra personal variations temporal variations video matching kendall tau similarity distance measure face recognition system	Face recognition from still face images suffers due to intrapersonal variations caused by pose, illumination, and expression that degrade the performance. On the other hand, videos provide abundant information that can be leveraged to compensate the limitations of still face images and enhance face recognition performance. This paper presents a video based face recognition algorithm that computes a discriminative video signature as an ordered list of still face images. The video signature embeds diverse intra-personal and temporal variations across multiple frames, thus facilitates matching two videos with large variations. Two videos are matched by comparing their discriminative signatures using the Kendall tau similarity distance measure. Performance comparison with the benchmark results and a commercial face recognition system on the publicly available YouTube faces database show the efficacy of the proposed video based face recognition algorithm.	algorithm;benchmark (computing);facial recognition system;type signature	Himanshu S. Bhatt;Richa Singh;Mayank Vatsa	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738616	facial recognition system;computer vision;face detection;speech recognition;object-class detection;computer science;video tracking;pattern recognition;three-dimensional face recognition;3d single-object recognition	Vision	34.69861543723927	-52.455542478736085	73257
0bcd89b356dc78aaf3573086f13e94b8e7b5bee6	comparative testing of face detection algorithms	face datasets;comparative test;statistical model;video coding;face localization accuracy;face recognition;quality evaluation;interactive user interface;face detection	Face detection (FD) is widely used in interactive user interfaces, in advertising industry, entertainment services, video coding, is necessary first stage for all face recognition systems, etc. However, the last practical and independent comparisons of FD algorithms were made by Hjelmas et al. and by Yang et al. in 2001. The aim of this work is to propose parameters of FD algorithms quality evaluation and methodology of their objective comparison, and to show the current state of the art in face detection. The main idea is routine test of the FD algorithm in the labeled image datasets. Faces are represented by coordinates of the centers of the eyes in these datasets. For algorithms, representing detected faces by rectangles, the statistical model of eyes’ coordinates estimation was proposed. In this work the seven face detection algorithms were tested; article contains the results of their comparison.	algorithm;data compression;face (geometry);face detection;facial recognition system;graphical user interface;statistical model;yang	Nikolay Degtyarev;Oleg Sergeevich Seredin	2010		10.1007/978-3-642-13681-8_24	facial recognition system;statistical model;computer vision;face detection;object-class detection;computer science;machine learning;data mining	Vision	28.33339129316175	-64.2787440126405	73362
6f0c674f4c1f355f2c57a40f373307f41e32a39d	verification of computer users using keystroke dynamics	vector quantisation pattern recognition feedforward neural nets backpropagation cryptography;identification process;sum of products;fuzzy artmap;performance evaluation;sigmoid transfer function;neural networks;time measurement;hybrid sum of products;speech analysis;computer users verification;characteristic patterns;indexing terms;backpropagation;computer hacking;computer networks;radial basis function networks;protection;computer networks neural networks permission cryptography pattern recognition backpropagation algorithms computer hacking protection speech analysis time measurement;permission;radial basis function network;transfer function;cryptography;bayes rule algorithms;computer user s login string;keystroke dynamics;backpropagation algorithms;pattern recognition;feedforward neural nets;bayes rule algorithms computer users verification keystroke dynamics computer user s login string characteristic patterns pattern recognition neural network identification process fuzzy artmap radial basis function networks learning vector quantization backpropagation sigmoid transfer function hybrid sum of products;vector quantisation;potential function;learning vector quantization;neural network	This paper presents techniques to verify the identity of computer users using the keystroke dynamics of computer user's login string as characteristic patterns using pattern recognition and neural network techniques. This work is a continuation of our previous work where only interkey times were used as features for identifying computer users. In this work we used the key hold times for classification and then compared the performance with the former interkey time-based technique. Then we use the combined interkey and hold times for the identification process. We applied several neural network and pattern recognition algorithms for verifying computer users as they type their password phrases. It was found that hold times are more effective than interkey times and the best identification performance was achieved by using both time measurements. An identification accuracy of 100% was achieved when the combined hold and intekey time-based approach were considered as features using the fuzzy ARTMAP, radial basis function networks (RBFN), and learning vector quantization (LVQ) neural network paradigms. Other neural network and classical pattern algorithms such as backpropagation with a sigmoid transfer function (BP, Sigm), hybrid sum-of-products (HSOP), sum-of-products (SOP), potential function and Bayes' rule algorithms gave moderate performance.		Mohammad S. Obaidat;Balqies Sadoun	1997	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.558812	keystroke dynamics;learning vector quantization;computer science;cryptography;artificial intelligence;backpropagation;theoretical computer science;machine learning;transfer function;radial basis function network;artificial neural network	Vision	26.11214563658755	-65.36088699206144	73375
4307e8f33f9e6c07c8fc2aeafc30b22836649d8c	supervised earth mover's distance learning and its computer vision applications	supervised earth mover;ground distance;histogram distance;accurate emd value;distance value;computer vision application;ground distance matrix;optimized emd value;natural distance;trained ground distance;traditional emd;emd flow-network	Earth Mover’s Distance (EMD) is an intuitive and natural distance metric for comparing two histograms or probability distributions. We propose to jointly optimize the ground distance matrix and the EMD flow-network based on partial ordering of histogram distances in an optimization framework. Two applications in computer vision are used to demonstrate the effectiveness of the algorithm: firstly, we apply the optimized EMD value to face verification, and achieve state-of-the-art performance on public face data sets; secondly, we use the learned EMD flow-network to analyze the internal structure of a set of faces, and consistent paths that demonstrate intuitive transitions on certain facial attributes are found. 1 Supervised Earth Mover’s Distance 1.	algorithm;computer vision;distance matrix;flow network;mathematical optimization	Fan Wang;Leonidas J. Guibas	2012		10.1007/978-3-642-33718-5_32	earth mover's distance;computer vision;machine learning;pattern recognition;mathematics	Vision	35.57032643030858	-52.31041112326524	73487
4767b3bd9f65242286d9de055b919ff3fb2b86b6	enhancing supervised classifications with metamorphic relations		"""We report on a novel use of metamorphic relations (MRs) in machine learning: instead of conducting metamorphic testing, we use MRs for the augmentation of the machine learning algorithms themselves. In particular, we report on how MRs can enable enhancements to an image classification problem of images containing hidden visual markers (""""Artcodes"""").  Working on an original classifier, and using the characteristics of two different categories of images, two MRs, based on separation and occlusion, were used to improve the performance of the classifier. Our experimental results show that the MR-augmented classifier achieves better performance than the original classifier, algorithms, and extending the use of MRs beyond the context of software testing."""	algorithm;binary classification;computer vision;machine learning;metamorphic testing;object detection;software testing;statistical classification;supervised learning;theory	Liming Xu;Dave Towey;Andrew P. French;Steve Benford;Zhi Quan Zhou;Tsong Yueh Chen	2018	2018 IEEE/ACM 3rd International Workshop on Metamorphic Testing (MET)	10.1145/3193977.3193978	visualization;metamorphic testing;random forest;software;classifier (linguistics);computer science;artificial intelligence;pattern recognition;contextual image classification	SE	27.40703627788144	-56.43562675574466	73816
694b23e806db505c0d5de29a7670e898043d3ccd	non-linear fusion of local matching scores for face verification	statistics face recognition image fusion image matching;kernel discriminant analysis;kernel;sample size;high dimensionality;ta1650 facial recognition systems;image matching;local binary pattern features face verification local matching scores nonlinear fusion kernel discriminant analysis;face recognition facial features kernel feature extraction training data system testing information analysis humans biometrics data mining;training;image fusion;local binary pattern features;face verification;local binary pattern;data mining;nonlinear fusion;training data;face recognition;feature extraction;statistics;facial features;face;local matching scores;linear space	This paper presents a face verification framework for fusing matching scores that measure similarities of local facial features. The framework is aimed to handle an open set verification scenario when users who try to enroll can be unknown to the system at the training phase. The kernel discriminant analysis is adopted within the framework to explore the discriminatory information of local matching scores in a high-dimensional non-linear space. A large sample size problem is raised for system training and an effective strategy is provided for tackling this problem. We demonstrate the framework by fusing the scores calculated using local binary pattern features. The experimental results show that our method improves the verification performance significantly when compared to a number of competitive techniques.	binary pattern (image generation);feature vector;integrated project support environment;kernel (operating system);linear discriminant analysis;nonlinear system	Ziheng Zhou;Samuel Chindaro;Farzin Deravi	2008	2008 8th IEEE International Conference on Automatic Face & Gesture Recognition	10.1109/AFGR.2008.4813338	computer vision;computer science;machine learning;pattern recognition	Vision	32.96341051612216	-58.44334577717364	73838
6a67831b716ac909f5d0d287b82d2c6a483942e0	neural networks and hausdorff distance applied to number recognition in electrical meters	recognition time;hausdorff distance;pattern recognition;number recognition;recognition percentage;number recognition process;hausdorff distance method;characteristics extraction method;better result;neural networks;hausdorff distance applied;characteristic extraction method;electrical meters	NEURAL NETWORKS AND HAUSDORFF DISTANCE APPLIED TO NUMBER RECOGNITION IN ELECTRICAL METERS Miguel Rodríguez a , Geovanni Berdugo b , Daladier Jabba a , Maria G. Calle b , Miguel Jimeno a , Juan P. Tello b , Eileen Triana b & Laura Zapata b a Department of System Engineering, Engineering Division , Universidad del Norte , Barranquilla , Colombia b Department of Electrical and Electronic Engineering, Engineering Division , Universidad del Norte , Barranquilla , Colombia Published online: 19 Nov 2012.	electronic engineering;hausdorff dimension;neural networks	Miguel E. Rodríguez;Geovanni Berdugo;Daladier Jabba;Maria Calle;Miguel Jimeno;Juan P. Tello;Eileen Triana;Laura Zapata	2012	Applied Artificial Intelligence	10.1080/08839514.2012.731344	artificial intelligence;machine learning;pattern recognition	AI	29.600213267876143	-65.89510631145791	74044
cd6aaa37fffd0b5c2320f386be322b8adaa1cc68	deep face recognition: a survey		Face recognition made tremendous leaps in the last five years with a myriad of systems proposing novel techniques substantially backed by deep convolutional neural networks (DCNN). Although face recognition performance sky-rocketed using deep-learning in classic datasets like LFW, leading to the belief that this technique reached human performance, it still remains an open problem in unconstrained environments as demonstrated by the newly released IJB datasets. This survey aims to summarize the main advances in deep face recognition and, more in general, in learning face representations for verification and identification. The survey provides a clear, structured presentation of the principal, state-of-the-art (SOTA) face recognition techniques appearing within the past five years in top computer vision venues. The survey is broken down into multiple parts that follow a standard face recognition pipeline: (a) how SOTA systems are trained and which public data sets have they used; (b) face preprocessing part (detection, alignment, etc.); (c) architecture and loss functions used for transfer learning (d) face recognition for verification and identification. The survey concludes with an overview of the SOTA results at a glance along with some open issues currently overlooked by the community.	algorithm;angularjs;black box;categorization;computation;database normalization;deep learning;domain adaptation;encode;edge detection;euclidean distance;experiment;facial recognition system;feature extraction;gabor filter;graphics processing unit;human reliability;loss function;mobile device;one-to-many (data model);sensitivity and specificity;softmax function	Mei Wang;Weihong Deng	2018	CoRR		leaps;architecture;transfer of learning;face detection;convolutional neural network;machine learning;feature extraction;facial recognition system;preprocessor;artificial intelligence;computer science	Vision	29.112632213961852	-53.98255870723541	74121
fde74b961c71a418365a215da2d71f79fa7ef390	a text reading algorithm for natural images	character segmentation;natural images;text detection;text recognition;scene text detection;character recognition	Reading text in natural images has focused again the attention of many researchers during the last few years due to the increasingly availability of cheap image-capturing devices in low-cost products like mobile phones. Therefore, as text can be found on any environment, the applicability of text-reading systems is really extensive. For this purpose, we present in this paper a robust method to read text in natural images. It is composed of two main separated stages. Firstly, text is located in the image using a set of simple and fast-tocompute features highly discriminative between character and non-character objects. They are based on geometric and gradient properties. The second part of the system carries out the recognition of the previously detected text. It uses gradient features to recognize single characters and Dynamic Programming (DP) to correct misspelled words. Experimental results obtained with different challenging datasets show that the proposed system exceeds state-of-the-art performance, both in terms of localization and recognition.	algorithm;color;document classification;dynamic programming;edit distance;email filtering;expectation propagation;feedback;gaussian (software);gradient;international conference on document analysis and recognition;internationalization and localization;k-nearest neighbors algorithm;mobile phone;optical character recognition;simple features;statistical classification;test engineer	Álvaro Gonzalez;Luis Miguel Bergasa	2013	Image Vision Comput.	10.1016/j.imavis.2013.01.003	computer vision;speech recognition;computer science;noisy text analytics;pattern recognition	Vision	35.426751302384424	-64.03514681871943	74200
204205a2811f272e74eb73a33acd7b0b68591751	gait recognition by applying multiple projections and kernel pca	image features;translation invariant;kernel principal component analysis;cmu;individual recognition;order statistic;gait recognition;higher order;higher order statistics;fast fourier transform;machine learning;feature extraction;kernel pca	Recognizing people by gait has a unique advantage over other biometrics: it has potential for use at a distance when other biometrics might be at too low a resolution, or might be obscured. In this paper, an improved method for gait recognition is proposed. The proposed work introduces a nonlinear machine learning method, kernel Principal Component Analysis (KPCA), to extract gait features from silhouettes for individual recognition. Binarized silhouette of a motion object is first represented by four 1-D signals which are the basic image features called the distance vectors. The distance vectors are differences between the bounding box and silhouette, and extracted using four projections to silhouette. Classic linear feature extraction approaches, such as PCA, LDA, and FLDA, only take the 2-order statistics among gait patterns into account, and are not sensitive to higher order statistics of data. Therefore, KPCA is used to extract higher order relations among gait patterns for future recognition. Fast Fourier Transform (FFT) is employed as a preprocessing step to achieve translation invariant on the gait patterns accumulated from silhouette sequences which are extracted from the subjects walk in different speed and/or different time. The experiments are carried out on the CMU and the USF gait databases and presented based on the different training gait cycles. Finally, the performance of the proposed algorithm is comparatively illustrated to take into consideration the published gait recognition approaches.	algorithm;biometrics;database;experiment;fast fourier transform;feature extraction;gait analysis;kernel principal component analysis;local-density approximation;machine learning;minimum bounding box;nonlinear system;preprocessor	Murat Ekinci;Murat Aykut;Eyüp Gedikli	2007		10.1007/978-3-540-73499-4_55	computer vision;kernel principal component analysis;computer science;machine learning;pattern recognition;mathematics	Vision	34.485877572663	-58.75279393201019	74252
1718657623346d2402e94c2fd2b554a26780d268	a novel local structure descriptor for color image retrieval	edge orientation similarity;local structures histogram;color image retrieval;local structures descriptor;underlying color	A novel local structure descriptor (LSD) for color image retrieval is proposed in this paper. Local structures are defined based on a similarity of edge orientation, and LSD is constructed using the underlying colors in local structures with similar edge direction. LSD can effectively combine color, texture and shape as a whole for image retrieval. LSH integrates the advantages of both statistical and structural texture description methods, and it possesses high indexing capability and low dimensionality. In addition, the proposed feature extraction algorithm does not need to train on a large scale training datasets, and it can extract local structure histogram based on LSD. The experimental results on the Corel image databases show that the descriptor has a better image retrieval performance than other descriptors.	algorithm;attribute grammar;color image;database;feature extraction;feature vector;fisher kernel;image retrieval;image segmentation;time complexity;lsh	Zhiyong Zeng	2016	Information	10.3390/info7010009	color histogram;image texture;computer vision;visual word;local binary patterns;pattern recognition;mathematics;information retrieval	Vision	37.80215934426611	-58.37958825671029	74288
254d4206f4a68862e6e6c45b0938868e84cc67d0	comparison of face image quality metrics: electronic and legacy mug shots	visual databases digital photography electronic data interchange face recognition iec standards iso standards open systems standardisation;legacy mug shots face image quality metrics automated face recognition identification cards passports driver s licenses standardization iso iec jtc 1 sc 37 data interchange formats interoperability face image databases iso iec 19794 5 photographic data set digitally captured photographs controlled collection database electronic shots;iso standards;biometrics;biometrics face recognition image quality law enforcement;image quality measurement face recognition face databases feature extraction lighting;standardisation;digital photography;iec standards;face recognition;image quality metric;law enforcement;image quality;interchange format;open systems;electronic data interchange;visual databases	Automated face recognition offers an effective method for identifying individuals. Face images have been used in a number of different applications, including driver's licenses, passports and identification cards. To provide some form of standardization for photographs in these applications, ISO / IEC JTC 1 SC 37 have developed standardized data interchange formats to promote interoperability. There are many different publically available face databases available to the research community that are used to advance the field of face recognition algorithms, amongst other uses. In this paper, we examine how an existing database that has been used extensively in research (FERET) compares with two operational data sets with respect to some of the metrics outlined in the standard ISO / IEC 19794-5. The goals of this research are to provide the community with a comparison of a baseline data set and to compare this baseline to a photographic data set that has been scanned in from mug-shot photographs, as well as a data set of digitally captured photographs. It is hoped that this information will provide Face Recognition System (FRS) developers some guidance on the characteristics of operationally collected data sets versus a controlled-collection database.	algorithm;baseline (configuration management);database;effective method;feret (facial recognition technology);facial recognition system;image quality;interoperability	Kevin O'Connor;Gregory Hales;Jonathan Hight;Shimon K. Modi;Stephen J. Elliott	2011	2011 IEEE Workshop on Computational Intelligence in Biometrics and Identity Management (CIBIM)	10.1109/CIBIM.2011.5949219	computer science;data mining;world wide web;computer security	Vision	27.914418594503104	-63.32758361702465	74370
57c3235ca31e421e022965ab2b62440312411447	saliency detection based on graph and independent component analysis with reference	image segmentation;manifolds;会议论文;independent component analysis;computer vision;vectors;image color analysis;manifolds image color analysis vectors independent component analysis algorithm design and analysis image segmentation computer vision;learning artificial intelligence graph theory image segmentation independent component analysis;manifold ranking ica r saliency detection saliency map;algorithm design and analysis;ica r saliency detection algorithm saliency detection method independent component analysis with reference background cues input image segmentation superpixels learning algorithm background saliency map graph based manifold ranking algorithm	As a preprocessing step of many applications, such as object recognition, image retrieval and scene analysis, saliency detection plays an important role and remains a challenging and significant problem in computer vision. Most existing bottom-up methods utilize local or global contrast information to compute the saliency maps, whereas a few methods generate saliency maps with the use of background cues. This work presents a saliency detection method by applying independent component analysis with reference (ICA-R) algorithm to the background cues, which improves the performance of the final saliency maps. First, we segment the input image into superpixels. Second, we take superpixels on each side of image as reference signals to do ICA-R learning, respectively. Then, four saliency maps generated from the learning algorithm are integrated into one background saliency map. Finally, a graph-based manifold ranking algorithm is done to generate the final saliency maps. By doing experiments on a large publicly available database, we demonstrate that the proposed ICA-R saliency detection algorithm performs better than the state-of-the-art methods.	bottom-up proteomics;computer vision;database;experiment;graph (discrete mathematics);image retrieval;independent computing architecture;independent component analysis;map;outline of object recognition;pollard's rho algorithm for logarithms;preprocessor	Xingming Wu;Junyu Wang;Weihai Chen	2014	2014 13th International Conference on Control Automation Robotics & Vision (ICARCV)	10.1109/ICARCV.2014.7064487	independent component analysis;algorithm design;computer vision;manifold;computer science;kadir–brady saliency detector;machine learning;pattern recognition;mathematics;image segmentation	Vision	36.47322008253133	-54.36051786307146	74871
f67a73c9dd1e05bfc51219e70536dbb49158f7bc	a gaussian mixture model for classifying the human age using dwt and sammon map	contrast enhancement;pre processing;feature extraction;facial aging;gamma correction	The appearance of a human face rigorously changes w ith respect to age that makes Age Classification as a more challenging task. The algorithms such as, KNearest Neighbor (K-NN), Support Vector Machine (SVM), Radial Basis Function (RBF), motivated many Face Researchers to focus their attention in classifying the human age into various age groups. The Classification rate produced by these existing algorithms is not significant indeed. In this study , Gaussian Mixture Models (GMM) is used for classifying the facial images into different age gr oups. A combination of Discrete Wavelet Transformation (DWT) and Sammon Map are used to ext ract the facial features. The performance of this approach is tested using Album-2 of MORPH database. A maximum classification rate of 99.52% is achieved in stage-1, whereas 99.46% is achieved in stage-2 using GMM. Also the accuracy achieved using Gaussian Mixture Model, is comparatively grea ter than K-NN.	discrete wavelet transform;ext js javascript framework;google map maker;k-nearest neighbors algorithm;mixture model;radial (radio);radial basis function;sammon mapping;support vector machine	J. Nithyashri;G. Kulanthaivel	2014	JCS	10.3844/jcssp.2014.2292.2298	speech recognition;feature extraction;computer science;machine learning;gamma correction;pattern recognition	ML	32.627369159547705	-59.49773234669768	74936
1551236063c91c71016c4190d466ceff33d62279	fast and efficient signature-based sub-circuit matching				Amir Masoud Gharehbaghi;Masahiro Fujita	2016	IEICE Transactions		template matching	DB	35.14625061181827	-62.28478525629401	74996
d0a21f94de312a0ff31657fd103d6b29db823caa	facial expression analysis	detailed analysis;facial motion;facial action unit;face image;prototypic expression;corresponding prototypic facial expression;facial expression;facial expression analysis;computer analysis;facial appearance change;appearance change;active appearance model	The face is one of the most powerful channels of nonverbal communication. Facial expression provides cues about emotion, intention, alertness, pain, personality, regulates interpersonal behavior, and communicates psychiatric and biomedical status among other functions. Within the past 15 years, there has been increasing interest in automated facial expression analysis within the computer vision and machine learning communities. This chapter reviews fundamental approaches to facial measurement by behavioral scientists and current efforts in automated facial expression recognition. We consider challenges, review databases available to the research community, approaches to feature detection, tracking, and representation, and both supervised and unsupervised learning. keywords : Facial expression analysis, Action unit recognition, Active Appearance Models, temporal clustering.	3d modeling;acoustic cryptanalysis;active appearance model;algorithm;american cryptogram association;bcs-facs;cluster analysis;computer vision;database;emoticon;feature detection (web development);global variable;human computer;human–computer interaction;inter-rater reliability;linear algebra;machine learning;modified huffman coding;real-time clock;real-time computing;real-time locating system;robustness (computer science);semantic prosody;sensor;simon;simon's problem;spatial variability;spontaneous order;supervised learning;taxonomy (general);top-down and bottom-up design;unsupervised learning;vocabulary	Fernando De la Torre;Jeffrey F. Cohn	2011		10.1007/978-0-85729-997-0_19	computer vision;active appearance model;facial action coding system;computer science;facial expression;face hallucination	ML	29.693606659298823	-58.70313449351495	75000
f266cd3a9064798de727a0dc7e8461cedadbef1b	skeleton-based action recognition using lstm and cnn		Recent methods based on 3D skeleton data have achieved outstanding performance due to its conciseness, robustness, and view-independent representation. With the development of deep learning, Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM)-based learning methods have achieved promising performance for action recognition. However, for CNN-based methods, it is inevitable to loss temporal information when a sequence is encoded into images. In order to capture as much spatial-temporal information as possible, LSTM and CNN are adopted to conduct effective recognition with later score fusion. In addition, experimental results show that the score fusion between CNN and LSTM performs better than that between LSTM and LSTM for the same feature. Our method achieved state-of-the-art results on NTU RGB+D datasets for 3D human action analysis. The proposed method achieved 87.40% in terms of accuracy and ranked 1st place in Large Scale 3D Human Activity Analysis Challenge in Depth Videos.		Chuankun Li;Pichao Wang;Shuang Wang;Yonghong Hou;Wanqing Li	2017	2017 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2017.8026287	machine learning;robustness (computer science);computer vision;long short term memory;pattern recognition;convolutional neural network;computer science;deep learning;skeleton (computer programming);artificial intelligence	Vision	28.608020786726424	-52.39902162767803	75072
e49116e05ab7f6f167c0bbe500efc994e99c00b8	a comparison of facial features and fusion methods for emotion recognition		Emotion recognition is an important part of human behavior analysis. It finds many applications including human-computer interaction, driver safety, health care, stress detection, psychological analysis, forensics, law enforcement and customer care. The focus of this paper is to use a pattern recognition framework based on facial expression features and two classifiers (linear discriminant analysis and k-nearest neighbor) for emotion recognition. The extended Cohn-Kanade database is used to classify 5 emotions, namely, ‘neutral, angry, disgust, happy, and surprise’. The Discrete Cosine Transform (DCT), Discrete Sine Transform (DST), the Walsh-Hadamard Transform (FWHT) and a new 7-dimensional feature based on condensing the Facial Action Coding System (FACS) are compared. Ensemble systems using decision level, score fusion and Borda count are also studied. Fusion of the four features leads to slightly more than a 90 % accuracy.	emotion recognition	Demiyan V. Smirnov;Rajani Muraleedharan;Ravi P. Ramachandran	2015		10.1007/978-3-319-26561-2_68	artificial intelligence;pattern recognition;borda count;discrete cosine transform;discrete sine transform;feature extraction;computer science;facial expression;k-nearest neighbors algorithm;facial action coding system;linear discriminant analysis	Vision	29.474205486511735	-59.545962048677325	75080
aed9409f6a92821180d3c8b1645e8b7eff16849a	illumination normalization for sift based finger vein authentication		Recently, the biometric information such as faces, fingerprints, and irises has been used widely in a security system for biometric authentication. Among these biometric features which are unique to each individual, the blood vessel pattern in fingers is superior for identifying individuals and verifying their identities: We may obtain easily the information on blood vessels which is almost impossible to counterfeit because the pattern exists inside the body unlike the others. In this work, we propose a finger vein recognition method using an illumination normalization and a SIFT (Scale-Invariant Feature Transform) matching identification. To verify individual identification, the proposed methodology is composed of two steps: (i) we first normalize the illumination of finger vein images, and (ii) extract SIFT descriptors from the image and match them to the given data. Experimental results indicate that the proposed method is shown to be successful for authentication system.	authentication	Hwi-Gang Kim;Eun Jung Lee;Gang-Joon Yoon;Sung-Dae Yang;Eui Chul Lee;Sang Min Yoon	2012		10.1007/978-3-642-33191-6_3	computer vision	Crypto	32.482532741037396	-61.8179452202341	75221
b1c48408355465f1154f6956a3924325388e4923	a salient-point signature for 3d object retrieval	3d object retrieval;spatial mapping;salient points;3d object signature	In this paper we describe a new 3D object signature and evaluate its performance for 3D object retrieval. The signature is based on a learning approach that finds the characteristics of salient points on a 3D object and represents the points in a 2D spatial map based on a longitude-latitude transformation. Experimental results show that the signature is able to achieve good retrieval scores for both pose-normalized and randomly-rotated object queries.	randomness	Indriyati Atmosukarto;Linda G. Shapiro	2008		10.1145/1460096.1460131	computer vision;object model;pattern recognition;information retrieval	Vision	38.78997462763991	-58.02182187319974	75225
04a15ed5b0cb227aee732efafa80447d3bb5f6fe	hindu arabic character recognition using mathematical morphology	computers;handwriting recognition;image segmentation;junctions;morphology;decision trees;character recognition	Presentation of a novel decision tree based method to improve the ability of hand written character recognition using mathematical morphology. For this paper, classification of hand written digits is done into two groups: one with blob and another without blob. This paper proposes the recognition of digits using certain features that can distinguish one digit from the other.	decision tree;mathematical morphology;optical character recognition	Ratan Kumar Basak;Bipasha Mukhopadhyay;Souvik Chatterjee;Sukalyan Goswami;Amrin Zaman;Ronit Ray;Abhriya Roy;Shalini Guha;Saptarshi De;Sucheta Nag	2016	2016 IEEE 7th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)	10.1109/UEMCON.2016.7777841	speech recognition;morphology;intelligent character recognition;computer science;machine learning;decision tree;handwriting recognition;image segmentation	Robotics	33.361473903611106	-66.1116098857549	75244
0fcd434b1e0f333bce1a3c3ce2a9cf4d685df9ac	color cast detection method based on multi-feature extraction		In order to raise the accuracy rate of the color cast detection and to make the method universal, the paper carries out a color cast detection method based on multi-feature extraction. Firstly, calculate the four features that are the textural property of the luminance channel, color numbers, histogram of RGB color space and statistical characteristics of the Gabor filter, then use AdaBoost to train and classify. The experiment will be done using 11346 images in the Ciurea database. The result shows that this method has a low error rate and good classification results, which is universal to natural images taken by cameras.	feature extraction	Minjing Miao;Yuan Yuan;Juhua Liu;Hanfei Yi	2014		10.1007/978-3-662-45498-5_12	color histogram;computer vision;color normalization;color depth;geography;pattern recognition;color balance;computer graphics (images)	EDA	36.983862099018104	-60.91587281642739	75252
dc1f6a42810df2153d93c4c38f19831d93275300	hand gesture recognition using histogram of oriented gradients and partial least squares regression		In this paper we propose a real-time hand gesture recognition system that employs the techniques developed for pedestrian detection to recognize a small vocabulary of human hand gestures. Our feature set comprises of grids of Histogram of Oriented Gradient (HOG) descriptors, with fine orientation binning and multi-level spatial binning for getting descriptors at the small as well as large scale. The overlapping descriptor blocks, which are contrast normalized to handle illumination changes, have a high degree of multicollinearity, resulting in a feature set of high dimensionality (more than 8000 dimensions), rendering it unsuitable for classification using the classical machine learning algorithms. Thus, we employ Partial Least Squares (PLS) regression as a ‘class aware’ method of dimensionality reduction, to project the feature vectors on to a lower dimensional space of 10 dimensions. We examine the results obtained by PLS as well as Principal Component Analysis (PCA) which show, that PLS outperforms PCA, and gives a better projection which preserves significant discriminative information.	algorithm;dimensionality reduction;gesture recognition;histogram of oriented gradients;image gradient;machine learning;partial least squares regression;pedestrian detection;principal component analysis;product binning;real-time clock;vocabulary	Arindam Misra;Takashi Abe;Koichiro Deguchi	2011			computer vision;speech recognition;computer science;machine learning;pattern recognition	ML	33.25328884231297	-57.76328273565824	75414
9f9478ffc32872c5ccefdce49de6fa58d1597409	conceptual feedback for semantic multimedia indexing	temporal context;semantic indexing;direct multilabel approach;normalized detection scores;multimedia;temporal rescoring;supervised learning;global system performance conceptual feedback semantic multimedia indexing visual concepts images shot video shot feature extraction classification supervised learning feature descriptor fusion classifier direct multilabel approach normalized detection scores global detection scores temporal context video content local homogeneity trecvid 2012 semantic indexing task multimodal concepts temporal rescoring;video signal processing;fusion;video shot;video signal processing feature extraction image classification image fusion indexing learning artificial intelligence multimedia systems;global system performance;video content local homogeneity;image fusion;semantics;image classification;classification;multimedia systems;visual concepts;images shot;conceptual feedback semantic indexing multimedia fusion;classifier;vectors;indexing;streaming media;conceptual feedback;feature extraction;pipelines;multimedia communication;trecvid 2012 semantic indexing task;feature descriptor;learning artificial intelligence;indexing semantics multimedia communication context streaming media vectors pipelines;semantic multimedia indexing;context;multimodal concepts;global detection scores	In this paper, we consider the problem of automatically detecting a large number of visual concepts in images or video shots. State of the art systems involve feature (descriptor) extraction, classification (supervised learning) and fusion when several descriptors and/or classifiers are used. Though direct multi-label approaches are considered in some works, detection scores are often computed independently for each target concept. We propose here a method that we call “conceptual feedback” for improving the overall detection performance that implicitly takes into account the relations between concepts. The vector of normalized detection scores is added to the pool of available descriptors. It is then processed just as the other descriptors for the normalization, optimization and classification steps. The resulting detection scores are finally fused with the already available detection scores obtained with the original descriptors. The feedback of the global detection scores in the pool of descriptors can be iterated several times. It is also compatible with the use of the temporal context that also improves the overall performance by taking into account the local homogeneity of video contents. The method has been evaluated in the context of the TRECVID 2012 semantic indexing task involving the detection of 346 visual or multimodal concepts. Combined with temporal re-scoring, the proposed method increased the global system performance (MAP) from 0.2613 to 0.3014 (+15.3% of relative improvement) while the temporal re-scoring alone increased it only from 0.2613 to 0.2691 (+3.0%).	data descriptor;database normalization;iteration;mathematical optimization;multi-label classification;multimodal interaction;sensor;statistical classification;supervised learning	Abdelkader Hamadi;Philippe Mulhem;Georges Quénot	2013	2013 11th International Workshop on Content-Based Multimedia Indexing (CBMI)	10.1109/CBMI.2013.6576552	computer vision;search engine indexing;contextual image classification;classifier;fusion;feature extraction;biological classification;computer science;machine learning;pattern recognition;semantics;pipeline transport;supervised learning;image fusion;information retrieval	Vision	33.456601944393604	-52.70463174109207	75450
bfc20433dd4a93df5f13d276eaee38d9023f8771	image mining by spectral features: a case study of scenery image classification	support vector machines;image classification;gabor filters;texture features;multiple classifiers;gabor filter;wavelet transform;image mining;k nearest neighbor;ensemble classifiers;support vector machine;classification accuracy	Spectral features of images, such as Gabor filters and wavelet transform can be used for texture image classification. That is, a classifier is trained based on some labeled texture features as the training set to classify unlabeled texture features of images into some pre-defined classes. The aim of this paper is twofold. First, it investigates the classification performance of using Gabor filters, wavelet transform, and their combination respectively, as the texture feature representation of scenery images (such as  mountain ,  castle , etc.). A  k -nearest neighbor ( k -NN) classifier and support vector machine (SVM) are also compared. Second, three  k -NN classifiers and three SVMs are combined respectively, in which each of the combined three classifiers uses one of the above three texture feature representations respectively, to see whether combining multiple classifiers can outperform the single classifier in terms of scenery image classification. The result shows that a single SVM using Gabor filters provides the highest classification accuracy than the other two spectral features and the combined three  k -NN classifiers and three SVMs.	computer vision	Chih-Fong Tsai	2007	Expert Syst. Appl.	10.1016/j.eswa.2005.11.016	random subspace method;support vector machine;computer vision;computer science;machine learning;pattern recognition;mathematics	ML	31.479992001515992	-55.75425531822024	75721
31ee385d68238a8311f683a40acf4a1a83d6cf58	u-catch: using color attribute of image patches in binary descriptors		In this study, we propose a simple yet very effective method for extracting color information through binary feature description framework. Our method expands the dimension of binary comparisons into RGB and YCbCr spaces, showing more than 100% matching improvement compared to non-color binary descriptors for a wide range of hard-to-match cases. The proposed method is general and can be applied to any binary descriptor to make it color sensitive. It is faster than classical binary descriptors for RGB sampling due to the abandonment of grayscale conversion and has almost identical complexity (insignificant compared to smoothing operation) for YCbCr sampling.	binary file;effective method;exception handling;grayscale;sampling (signal processing);smoothing	Özgür Yilmaz;Alisher Abdulkhaev	2016	CoRR		computer vision;machine learning;pattern recognition;mathematics;binary pattern	Vision	36.67532382940719	-60.362301593692614	75961
4edd03283f9975164d54bc677290b0f76aa936ea	feature-level fusion of hand biometrics for personal verification based on kernel pca	analisis componente principal;methode noyau;biometrie;biometrics;biometria;data fusion;feature space;principal component analysis;fusion donnee;metodo nucleo;analyse composante principale;kernel pca;kernel method;feature fusion;fusion datos	This paper presents a novel method of feature-level fusion (FLF) based on kernel principle component analyze (KPCA). The proposed method is applied to fusion of hand biometrics include palmprint, hand shape and knuckleprint, and we name the new feature as “handmetric”. For different kind of samples, polynomial kernel is employed to generate the kernel matrixes that indicate the relationship among them. While fusing these kernel matrixes by fusion operators and extracting principle components, the handmetric feature space is established and nonlinear feature-level fusion projection could be implemented. The experimental results testify that the method is efficient for feature fusion, and could keep more identity information for verification.	biometrics;kernel principal component analysis	Qiang Li;Zhengding Qiu;Dongmei Sun	2006		10.1007/11608288_99	kernel method;speech recognition;feature vector;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;mathematics;sensor fusion;biometrics;principal component analysis	Vision	33.929875692631974	-58.92569893603426	75987
7169d0a317540f3d16a141afa96dc7fc29e19dac	shape descriptor based document image indexing and symbol recognition	image sampling;distance based hashing shape descriptor;histograms;image recognition;document image retrieval;locality sensitive hashing;distance measure;shape descriptor;image classification;text analysis;shape recognition;image indexing;graph matching;distance based hashing;shape representation;shape;indexing;shape indexing image recognition histograms pattern recognition principal component analysis image sampling text analysis image analysis pattern analysis;cryptography;principal component analysis;indexation;shape recognition cryptography document image processing image classification indexing;shape context;pattern recognition;document image processing;characters classification;document image indexing;image analysis;pattern analysis;distance based hierarchical locality sensitive hashing shape descriptor document image indexing symbol recognition shape context document image retrieval characters classification symbol classification;symbol classification;bipartite graph;context;symbol recognition;distance based hierarchical locality sensitive hashing	In this paper we present a novel shape descriptor based on shape context, which in combination with hierarchical distance based hashing is used for word and graphical pattern based document image indexing and retrieval. The shape descriptor represents the relative arrangement of points sampled on the boundary of the shape of object. We also demonstrate the applicability of the novel shape descriptor for classification of characters and symbols. For indexing, we provide anew formulation for distance based hierarchical locality sensitive hashing. Experiments have yielded promising results.	locality of reference;locality-sensitive hashing;shape context	Ehtesham Hassan;Santanu Chaudhury;Madan Gopal	2009	2009 10th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2009.63	active shape model;computer vision;image analysis;bipartite graph;shape;computer science;cryptography;machine learning;pattern recognition;information retrieval;principal component analysis	Vision	39.13494597927415	-59.55518964774987	76004
cd2758cdeddf881d6e431baf5963526e0b70f11f	recognition of isolated handwritten kannada numerals based on image fusion method	image fusion;cross validation;digital image;nearest neighbor classifier	This paper describes a system for isolated Kannada handwritten numerals recognition using image fusion method. Several digital images corresponding to each handwritten numeral are fused to generate patterns, which are stored in 8×8 matrices, irrespective of the size of images. The numerals to be recognized are matched using nearest neighbor classifier with each pattern and the best match pattern is considered as the recognized numeral.The experimental results show accuracy of 96.2% for 500 images, representing the portion of trained data, with the system being trained for 1000 images. The recognition result of 91% was obtained for 250 test numerals other than the trained images. Further to test the performance of the proposed scheme 4-fold cross validation has been carried out yielding an accuracy of 89%.	image fusion	G. G. Rajput;Mallikarjun Hangarge	2007		10.1007/978-3-540-77046-6_19	arithmetic;speech recognition;computer science;machine learning;pattern recognition;image fusion;digital image;cross-validation	Vision	33.15580146737495	-64.4209244717119	76177
6e09a41a6de4011a9e9e1424c530dfa18fccf08e	a robust multi-model approach for face detection in crowd	image segmentation;support vector machines;skin;image color analysis;feature extraction;face;face detection	The estimation of the number of people in surveillance areas is essential for monitoring crowded scenes. When density of a zone increases to a certain approximated level, people's safety can be endangered. Detection of human is a prerequisite for density estimation, tracking, activity recognition and anomaly detection even in non congested areas. This paper presents a robust hybrid approach for face detection in crowd by combining the skin color segmentation and a Histogram of Oriented Gradients(HOG) with Support Vector Machine(SVM) architecture. Initially, image enhancement is performed to improve the detection rate. An edge preserving pyramidal approach is applied for multiscale representation of an image. Skin color segmentation is done with combination of YCbCr and RGB color model, and HOG features are extracted from the segmented skin region. We trained the SVM classifier by Muct and FEI databases which consist 751 and 2800 face images respectively. The accuracy of this approach is evaluated by testing it on BAO multiple face database and on various manually collected images captured in surveillance areas. Experimental results demonstrate that the supplementary skin color segmentation with HOG is more potent for increasing the detection rate than using HOG features only. The proposed approach achieves 98.02% accuracy which is higher in comparison to Viola Jones and fast face detection method.	activity recognition;anomaly detection;approximation algorithm;closed-circuit television;clutter;computation;database;face detection;gradient;hidden surface determination;histogram of oriented gradients;image editing;image segmentation;jones calculus;numerical analysis;pixel;robustness (computer science);statistical classification;support vector machine	Sonu Lamba;Neeta Nain;Harendra Chahar	2016	2016 12th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)	10.1109/SITIS.2016.24	face;support vector machine;computer vision;face detection;speech recognition;object-class detection;feature extraction;computer science;machine learning;pattern recognition;skin;image segmentation	Vision	32.78878919536959	-56.706193419214735	76190
a8a8fde81938209311f29bd0d6c92c93efc121fd	noisy iris challenge evaluation - part i (nice.i)			iris challenge evaluation		2009		10.1007/978-0-387-73003-5_1083		NLP	29.098475432912583	-62.02803306494939	76275
5a9a8d57df9646acd559b843df3ea891b7b6d026	cascaded deep convolutional neural network for robust face alignment		Face alignment is an essential preprocessing stage for most face-related visual tasks and has been widely studied. The performance of face alignment is boosted because of the rising and development of deep learning. While achieving a great success, existing face alignment methods overlooked a fact that different facial keypoints have different patterns and should be treated differently. For example, facial key-points that fall on facial contour are more likely to be misaligned than their peers that fall on five senses (eyes, nose and mouth). In this work, we propose a cascaded convolutional neural network (ConvNet) for robust face alignment. The network consists of multiple stages and refines predictions of the facial landmarks along the stages. Each stage of the network embeds Squeeze and Excitation (SE) units and residual attention unit into a deep ConvNet. The embedding of SE units and attention mechanism enables the network to have a global receptive field and locate different landmarks adaptively. We also improve the cascading way how stages are linked. Experiments reveal that, our method decreases the alignment error and achieves the state-of-the-art performance on 300 Faces In-the-Wild Challenging (300-W) dataset.		Zhihua Huang;Wengang Zhou;Houqiang Li	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451198	computer vision;convolutional neural network;residual;pattern recognition;artificial neural network;deep learning;artificial intelligence;receptive field;computer science	Vision	28.336923481216406	-52.532232598726075	76523
d9295a00618eee230e32e05435a80539f8d9a098	disjunctive normal form of weak classifiers for online learning based object tracking		The use of a strong classifier that is combined by an ensemble of weak classifiers has been prevalent in tracking, classification etc. In the conventional ensemble tracking, one weak classifier selects a 1D feature, and the strong classifier is combined by a number of 1D weak classifiers. In this paper, we present a novel tracking algorithm where weak classifiers are 2D disjunctive normal form (DNF) of these 1D weak classifiers. The final strong classifier is then a linear combination of weak classifiers and 2D DNF cell classifiers. We treat tracking as a binary classification problem, and one full DNF can express any particular Boolean function; therefore 2D DNF classifiers have the capacity to represent more complex distributions than original weak classifiers. This can strengthen any original weak classifier. We implement the algorithm and run the experiments on several video sequences.	algorithm;binary classification;disjunctive normal form;experiment;finite difference;minimum bounding box;scalability;statistical classification	Zhu Teng;Dong-Joong Kang	2013			computer vision;machine learning;pattern recognition;mathematics	AI	32.16977702796869	-55.17369180648767	76621
775c15a5dfca426d53c634668e58dd5d3314ea89	image quality-aware deep networks ensemble for efficient gender recognition in the wild		Gender recognition is an important task in the field of facial image analysis. Gender can be detected using different visual cues, for example gait, physical appearance, and most importantly, the face. Deep learning has been dominating many classification tasks in the past few years. Gender classification is a binary classification problem, usually addressed using the facial image. In this work, we present a deep and compact CNN (GenderCNN) to estimate the gender from a facial image. We also, tackle the illumination and blurriness that appear in still images and appear more in videos. We use Adaptive Gamma Correction (AGC) to enhance the contrast and thus, get more details from the facial image. We use AGC as a pre-processing step in gender classification in still images. In videos, we propose a pipeline that quantifies the blurriness of an image using a blurriness metric (EMBM), and feeds it to its corresponding GenderCNN that was trained on faces with similar blurriness. We evaluated our proposed methods on challenging, large, and publicly available datasets, CelebA, IMDB-WIKI still images datasets and on McGill, and Point and Shoot Challenging (PaSC) videos datasets. Experiments show that we outperform or in some cases match the state of the art methods.	automatic gain control;binary classification;convolution;deep learning;experiment;gamma correction;granular computing;handwriting recognition;image analysis;image quality;internet movie database (imdb);preprocessor;statistical classification;wiki	Mohamed Selim;Suraj Sundararajan;Alain Pagani;Didier Stricker	2018		10.5220/0006626103510358	image quality;machine learning;computer vision;artificial intelligence;computer science	Vision	29.803651054462314	-54.67283754309142	76631
a07b88a0417c3803d87f57138a72dd11c0370e2c	stn-ocr: a single neural network for text detection and text recognition		Detecting and recognizing text in natural scene images is a challenging, yet not completely solved task. In recent years several new systems that try to solve at least one of the two sub-tasks (text detection and text recognition) have been proposed. In this paper we present STN-OCR, a step towards semi-supervised neural networks for scene text recognition that can be optimized end-to-end. In contrast to most existing works that consist of multiple deep neural networks and several pre-processing steps we propose to use a single deep neural network that learns to detect and recognize text from natural images in a semi-supervised way. STN-OCR is a network that integrates and jointly learns a spatial transformer network [16], that can learn to detect text regions in an image, and a text recognition network that takes the identified text regions and recognizes their textual content. We investigate how our model behaves on a range of different tasks (detection and recognition of characters, and lines of text). Experimental results on public benchmark datasets show the ability of our model to handle a variety of different tasks, without substantial changes in its overall network structure.	artificial neural network;benchmark (computing);computer multitasking;converge;deep learning;end-to-end principle;experiment;network architecture;optical character recognition;preprocessor;semi-supervised learning;semiconductor industry;sensor;super-twisted nematic display;transformer	Christian Bartz;Haojin Yang;Christoph Meinel	2017	CoRR		pattern recognition;machine learning;artificial intelligence;time delay neural network;computer science;artificial neural network;noisy text analytics	AI	25.84145756994859	-53.80762768327256	76766
d5c94aa82fe8d749f1655dac5a4e8d29c9eeed69	learning class-specific pooling shapes for image classification	kernel;multi shape matching kernel;standards;multi shape matching kernel image classification class specific pooling shapes csps representation compression;training;image classification;layout;representation compression;shape;image color analysis;class specific pooling shapes csps;shape recognition feature extraction generalisation artificial intelligence image classification image colour analysis image matching learning artificial intelligence;shape image color analysis standards kernel training layout encoding;encoding;scene image image classification spatial pyramid representation bag of feature model local feature spatial layout information feature code pooling predefined spatial shapes generalization geometric property distribution class specific pooling shape adaptive learning csps geometric pattern linear classifier training structured sparsity constraint color distribution cues shape importance svm multishape matching kernel caltech 256 scene 15 indoor 67 object image	Spatial pyramid (SP) representation is an extension of bag-of-feature model which embeds spatial layout information of local features by pooling feature codes over pre-defined spatial shapes. However, the uniform style of spatial pooling shapes used in standard SP is an ad-hoc manner without theoretical motivation, thus lacking the generalization power to adapt to different distribution of geometric properties across image classes. In this paper, we propose a data-driven approach to adaptively learn class-specific pooling shapes (CSPS). Specifically, we first establish an over-complete set of spatial shapes providing candidates with more flexible geometric patterns. Then the optimal subset for each class is selected by training a linear classifier with structured sparsity constraint and color distribution cues. To further enhance the robust of our model, the representations over CSPS are compressed according to the shape importance and finally fed to SVM with a multi-shape matching kernel for classification task. Experimental results on three challenging datasets (Caltech-256, Scene-15 and Indoor-67) demonstrate the effectiveness of the proposed method on both object and scene images.	code;computer vision;feature model;hoc (programming language);linear classifier;sparse matrix	Jinzhuo Wang;Wenmin Wang;Ronggang Wang;Wen Gao	2015	2015 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2015.7177433	layout;computer vision;contextual image classification;kernel;shape;machine learning;pattern recognition;mathematics;encoding	Vision	34.96663931279612	-53.80839869997873	76797
74a58292334c4cbd804a40484e632158e27862b0	face image retrieval using sparse representation classifier with gabor-lbp histogram	face retrieval;surveillance system;local binary pattern;gabor filter;face recognition;image representation;facial features;local binary patterns;sparse representation classifier;sparse representation;image retrieval	Face image retrieval is an important issue in the practical applications such as mug shot searching and surveillance systems. However, it is still a challenging problem because face images are fairly similar due to the same geometrical configuration of facial features. In this paper, we present a face image retrieval method which is robust to the variations of face image condition and with high accuracy. Firstly, we choose the Gabor-LBP histogram for face image representation. Secondly, we use the sparse representation classification for the face image retrieval. Using the Gabor-LBP histogram and sparse representation classifier, we achieved effective and robust retrieval results with high accuracy. Finally, experiments are conducted on ETRI and XM2VTS database to verify a proposed method. It showed rank 1 retrieval accuracy rate of 98.9% on ETRI face set, and of 99.3% on XM2VTS face set, respectively.	belief propagation;computer vision;experiment;gabor filter;image retrieval;local binary patterns;prototype;sparse approximation;sparse matrix	Hansung Lee;Yun-Su Chung;Jeongnyeo Kim;Daihee Park	2010		10.1007/978-3-642-17955-6_20	computer vision;visual word;local binary patterns;object-class detection;image retrieval;computer science;machine learning;pattern recognition;three-dimensional face recognition	Vision	33.820278422128574	-55.975159905912484	76829
10e07090344c1ced08a6692fb280a6abeb2a7ecc	zoning features and 2dlstm for urdu text-line recognition	urdu;zoning features;character recognition;cursive script	Recognition of Urdu cursive script is a challenging task due to the implicit complexities associated with it. The performance of a recognition system is immensely dependent on extracted features. There are various features extraction approaches proposed in recent years. Among many, an approach based on zoning features proved to be efficient and popular. Such zoning features represent significant information with low complexity and high speed. In this paper, we used zoning features for the classification of Urdu Nasta’liq text lines, with a combination of 2-Dimensional Long Short Term Memory networks (2DLSTM) as learning classifier. The proposed model is evaluated on publicly available UPTI dataset and character recognition rate of 93.39% is obtained. © 2016 The Authors. Published by Elsevier B.V. Peer-review under responsibility of KES International.	long short-term memory;optical character recognition	Saeeda Naz;Saad Bin Ahmed;Riaz Ahmad;Muhammad Imran Razzak	2016		10.1016/j.procs.2016.08.084	natural language processing;speech recognition;pattern recognition	Vision	34.51446756241866	-64.1466567280431	76933
d5dc34f0087e1d7a7ea6ab38d6b2e7572a9d4411	combine crossing matching scores with conventional matching scores for bimodal biometrics and face and palmprint recognition experiments	biometrics;journal;matching score level fusion;pattern recognition;infrared;visible light	In this paper, we define for the first time the crossing matching score of two biometrics traits and combine it with the conventional matching scores to perform personal authentication. The proposed method is very suitable for the bimodal biometrics systems with two similar biometrics traits such as the system with visible light and infrared face images and the system with palm images captured at these two biometrics traits, the proposed method calculates the matching scores between the testing sample and each training sample. The matching scores generated from the first and second traits are referred to as the first and second matching scores, respectively. Second, the proposed method calculates the crossing matching scores, i.e. the matching scores between the testing sample of the second biometrics trait and the training samples of the first biometrics trait. Finally, we use a weighted fusion scheme to combine the first, second and crossing matching scores for personal authentication. & 2011 Elsevier B.V. All rights reserved.	authentication;biometrics;experiment;fingerprint	Yong Xu;Qi Zhu;David Zhang	2011	Neurocomputing	10.1016/j.neucom.2011.08.011	computer vision;speech recognition;infrared;pattern recognition;visible spectrum;biometrics	Vision	33.05186583394114	-60.328382783077856	76987
0ea04b86e078b038bdab1834b6b11a2bf1673690	histogram based classification of tactile patterns on periodically distributed skin sensors for a humanoid robot	robot sensing systems;humanoid robot;histograms;kernel;classification algorithm;support vector machines;book item;histograms robot sensing systems support vector machines kernel skin;svm histogram based classification periodically distributed skin sensors humanoid robot human robot interaction kaspar image patterns tactile pattern classification codebook local neighbourhood structures pressure distribution support vector machine;skin;human robot interaction;large scale;radial basis function;humanoid robots;local features;pattern classification;sensor array;tactile sensors;control engineering computing;digital image;support vector machine;tactile sensors control engineering computing humanoid robots human robot interaction pattern classification support vector machines;geometric structure	The main target of this work is to improve human-robot interaction capabilities, by adding a new modality of sense, touch, to KASPAR, a humanoid robot. Large scale distributed skin-like sensors are designed and integrated on the robot, covering KASPAR at various locations. One of the challenges is to classify different types of touch. Unlike digital images represented by grids of pixels, the geometrical structure of the sensor array limits the capability of straightforward application of well-established approaches for image patterns. This paper introduces a novel histogram-based classification algorithm, transforming tactile data into histograms of local features termed as codebook. Tactile pattern can be invariant at periodical locations, allowing tactile pattern classification using a smaller number of training data, instead of using training data from everywhere on the large scale skin sensors. To generate the codebook, this method uses a two-layer approach, namely local neighbourhood structures and encodings of pressure distribution of the local neighbourhood. Classification is performed based on the constructed features using Support Vector Machine (SVM) with the intersection kernel. Real experimental data are used for experiment to classify different patterns and have shown promising accuracy. To evaluate the performance, it is also compared with the SVM using the Radial Basis Function (RBF) kernel and results are discussed from both aspects of accuracy and the location invariance property.	algorithm;code word;codebook;digital image;feature vector;humanoid robot;human–robot interaction;modality (human–computer interaction);pixel;quantization (image processing);radial (radio);radial basis function kernel;statistical classification;support vector machine;tactile sensor;vector quantization;vii	Ze Ji;Farshid Amirabdollahian;Daniel Polani;Kerstin Dautenhahn	2011	2011 RO-MAN	10.1109/ROMAN.2011.6005261	human–robot interaction;support vector machine;computer vision;computer science;humanoid robot;artificial intelligence;machine learning;pattern recognition	Robotics	24.929911148417503	-63.05113032548553	77000
3ce896efd07dc6d5e54b2323658fc035ef4ef769	sketch-based image retrieval with a novel bovw representation	visual vocabulary generation;weighting quantization for matching;sketch based image retrieval;会议论文;inverted indexing structure construction;bag of visual word representation	A novel Bag-of-Visual-Word BoVW based approach is developed in this paper to facilitate more effective Sketch-based Image Retrieval SBIR. We focus on constructing the visual vocabulary based on the BoVW representation with both the spatial distribution and inter-relationship of descriptors. To optimize the sketch-image matching, the weighting quantization is created by integrating both the neighbor and spatial feature information to quantify features as visual words. We emphasize on an inverted indexing by converting an image to a trigram representation with visual words and their spatial information. Our experiments have obtained very positive results.	image retrieval;sketch	Cheng Jin;Chenjie Li;Zheming Wang;Yuejie Zhang;Tao Zhang	2016		10.1007/978-3-319-27671-7_52	computer vision;visual word;computer science;pattern recognition;information retrieval	Vision	37.76745267211545	-57.857402311153976	77090
3354c8c4e8f64c2a4a7520dbca196e69ea351311	effective and fast face recognition system using complementary oc-lbp and hog feature descriptors with svm classifier			local binary patterns	Geetika Singh;Indu Chhabra	2018	JITR	10.4018/JITR.2018010106	data mining;support vector machine;facial recognition system;computer science;classifier (linguistics);pattern recognition;artificial intelligence	Vision	30.982659349714815	-57.87699255354025	77112
b7a6af028aa40668174c1166376020d52c756594	classification and feature extraction of binucleate cells using mahalanobis distance and gabor wavelet analysis	mahalanobis distance;circular gabor;gabor features;gabor filter;gabor wavelet;bpso;binary particle swarm optimisation;md;binucleate and non binucleate cells	A hybrid methodology of feature extraction and classification is proposed in this paper to classify binucleate and non-binucleate or normal cells. The proposed methodology consists of a Gabor filter-based feature extraction using two types of Gabor filters, namely circular Gabor and Gabor wavelet. Feature matrix considering mean and variance are calculated in sets of 50 for each of the filters. Thereafter, dimensionality reduction is done using a binary particle swarm optimisation (BPSO) technique to screen out the redundant features. Finally, Mahalanobis distance (MD) is used to classify the images into respective classes using the reduced set of features. To show the efficacy and robustness of the proposed hybrid technique using Gabor wavelets, the classification accuracy is calculated and compared with circular Gabor.	dilation (morphology);dimensionality reduction;feature extraction;gabor atom;gabor filter;gabor wavelet;mathematical optimization;molecular dynamics;particle swarm optimization;trivial graph format	Kahkashan Afrin	2014	IJIEI	10.1504/IJIEI.2014.067198	computer vision;mahalanobis distance;machine learning;pattern recognition;gabor wavelet;statistics	ML	34.97384559284949	-60.089058035622045	77130
3c1f234ead69826e5ca5f11a5f0bf6a3a00d078a	finger vein verification using local histogram of hybrid texture descriptors	databases;magnitude bgc local histogram texture sign;histograms;eer finger vein verification hybrid texture descriptor local histogram finger vein recognition efficacious biometric method user authentication vein patterns finger vein forgery human body binary gradient contour bgc;vein recognition cryptography feature extraction fingerprint identification gradient methods handwriting recognition image texture;veins;thumb;feature extraction;veins thumb histograms feature extraction databases mathematical model;mathematical model	Lately, finger vein has been recognized as an efficacious biometric method for user authentication due to the uniqueness of vein patterns and its insusceptibility to forgery because the vein patterns reside inside the human body. In this work, hybrid histogram descriptor is the proposed method. This method utilizes the sign and magnitude components of the texture extracted by using Binary Gradient Contour (BGC). Subsequently, the histogram is locally computed to determine the weight distribution of the sign and magnitude value for the hybrid texture descriptors. The extensive experimental results demonstrate the overall superiority of the proposed method with the EER as low as 0.353%.	authentication;biometrics;contour line;enhanced entity–relationship model;farmville;feature extraction;gradient;rs-232;signed number representations;texture filtering	Ardianto William;Thian Song Ong;Siong Hoe Lau;Michael Goh Kah Ong	2015	2015 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)	10.1109/ICSIPA.2015.7412209	computer vision;speech recognition;feature extraction;computer science;pattern recognition;mathematical model;histogram;mathematics;statistics	Vision	33.1126319839589	-60.97806949618994	77428
f9713d258c0cafee1973216d8fdb31ab307b41e6	fingerphoto recognition with smartphone cameras	smart phones;smart phones fingerprint identification operating systems computers;cameras image edge detection authentication thumb prototypes light emitting diodes;biometric database fingerphoto recognition smartphone cameras authentication method captured samples memory capacity;operating systems computers;fingerprint identification	This paper is concerned with the authentication of people on smartphones using fingerphoto recognition. In this work, fingerphotos are captured with the built-in camera of the smartphone. The proposed authentication method is analyzed for feasibility and implemented in a prototype as application for the Android operating system. Algorithms for the capture process are developed to ensure a minimum of quality of the captured photos to enable a reliable fingerphoto recognition. Several methods for preprocessing of the captured samples are analyzed and performant solutions to evaluate the photos are developed to enhance the recognition rates. This is achieved by evaluating a wide range of different parameters and configurations of the algorithms as well as various combinations of preprocessing chains for the captured samples. The operations for preprocessing are selected with respect to their computational effort to guarantee that they can be executed on a smartphone with limited computation and memory capacity. The developed prototype is evaluated in user tests with two different smartphones. Additionally, a biometric database containing photos of the two test devices from 41 test subjects is created. These fingerphotos are used to evaluate and optimize the procedures.	algorithm;android;authentication;biometrics;canonical account;computation;enhanced entity–relationship model;fingerprint;image resolution;markov chain;mathematical optimization;mobile operating system;preprocessor;prototype;randomness extractor;smartphone;usability	Chris Stein;Claudia Nickel;Christoph Busch	2012	2012 BIOSIG - Proceedings of the International Conference of Biometrics Special Interest Group (BIOSIG)		embedded system;fingerprint;computer vision;computer science;database;internet privacy;computer security	Mobile	30.22351372505043	-62.18576652755667	77462
8a802bd58cd1d76724d608d095e89d3eb7007708	image representations for improving object recognition ; nieuwe beeldpresentaties voor voorwerpherkenning			outline of object recognition	Palamandadige Fernando	2015				ML	30.381171661419675	-57.31706233161569	77553
a955373ae91a66aa422c5d9ebecd782a3435ffa0	off-line signature verification by the tracking of feature and stroke positions	relative position;handwriting recognition;signature verification;feature tracking;off line system;article;elastic matching	There are inevitable variations in the signature patterns written by the same person. The variations can occur in the shape or in the relative positions of the characteristic features. In this paper, two methods are proposed to track the variations. Given the set of training signature samples, the !rst method measures the positional variations of the one-dimensional projection pro!les of the signature patterns; and the second method determines the variations in relative stroke positions in the two-dimension signature patterns. The statistics on these variations are determined from the training set. Given a signature to be veri!ed, the positional displacements are determined and the authenticity is decided based on the statistics of the training samples. For the purpose of comparison, two existing methods proposed by other researchers were implemented and tested on the same database. Furthermore, two volunteers were recruited to perform the same veri!cation task. Results show that the proposed system compares favorably with other methods and outperforms the volunteers.? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.	algorithm;database;elastic matching;human reliability;optimal matching;pattern recognition;sudoku solving algorithms;test set	Bin Fang;Cheung Hoi Leung;Yuan Yan Tang;K. W. Tse;Paul C. K. Kwok;Y. K. Wong	2003	Pattern Recognition	10.1016/S0031-3203(02)00061-4	speech recognition;computer science;machine learning;pattern recognition;data mining;handwriting recognition	Vision	28.804170681646596	-65.40983039843796	77616
e20d9efb55aac9226f180a737b793f400ccc8ef9	dictionary based approach for facial expression recognition from static images		We present a simple approach for facial expression recognition from images using the principle of sparse representation using a learned dictionary. Visual appearance based feature descriptors like histogram of oriented gradients (HOG), local binary patterns (LBP) and eigenfaces are used. We use Fisher discrimination dictionary which has discrimination capability in addition to being reconstructive. The classification is based on the fact that each expression class with in the dictionary spans a subspace and these subspaces have non-overlapping directions so that they are widely separated. Each test feature point has a sparse representation in the union of subspaces of dictionary formed by labeled training points. To check recognition performance of the proposed approach, extensive experimentation is done over Jaffee and CK databases. Results show that the proposed approach has better classification accuracy than state-of-the-art techniques.	dictionary	Krishan Sharma;Renu Rameshan	2016		10.1007/978-3-319-68124-5_4	linear subspace;computer vision;artificial intelligence;histogram of oriented gradients;local binary patterns;eigenface;computer science;facial expression;sparse approximation;pattern recognition;subspace topology;visual appearance	Vision	32.704078083654124	-54.75149397624571	77835
3ae484ce451154dccda6b38cbebe8e767b837d10	body motion analysis for multi-modal identity verification	motion analysis;identify verification;biometrics access control;motion estimation biometrics access control feature extraction;biometrics;motion estimation;multi modal;face verification;face recognition;feature extraction;equal error rate;integration method body motion analysis multimodal identity verification body motion signature analysis softbiometric technique identity verification feature extraction face verification equal error rate verification performance motion feature;biometrics identify verification multi modal face recognition;face recognition face feature extraction computational modeling databases computer architecture tracking	This paper shows how “Body Motion Signature Analysis” – a new “soft-biometrics” technique – can be used for identity verification. It is able to extract motion features from the upper body of people and estimates so called “super-features” for input to a classifier. We demonstrate how this new technique can be used to identify people just based on their motion, or it can be used to significantly improve “hard-biometrics” techniques. For example, face verification achieves on this domain 6.45% Equal Error Rate (EER), and the combined verification performance of motion features and face reduces the error to 4.96% using an adaptive score-level integration method. The more ambiguous motion-only performance is 17.1% EER.	dynamic music;enhanced entity–relationship model;identity verification service;modal logic;soft biometrics;statistical classification	George Williams;Graham W. Taylor;Kirill Smolskiy;Christoph Bregler	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.538	facial recognition system;computer vision;speech recognition;feature extraction;computer science;multimodal interaction;pattern recognition;motion estimation;biometrics	Vision	31.6662661595628	-58.57252226298658	77931
6251dacba96013df67e24b118712c1ba693fd7eb	efficient graph-cut tattoo segmentation	police;image segmentation;skin;image retrieval	Law enforcement is interested in exploiting tattoos as an information source to identify, track and prevent gang-related crimes. Many tattoo image retrieval systems have been described. In a retrieval system tattoo segmentation is an important step for retrieval accuracy since segmentation removes background information in a tattoo image. Existing segmentation methods do not extract the tattoo very well when the background includes textures and color similar to skin tones. In this paper we describe a tattoo segmentation approach by determining skin pixels in regions near the tattoo. In these regions graph-cut segmentation using a skin color model and a visual saliency map is used to find skin pixels. After segmentation we determine which set of skin pixels are connected with each other that form a closed contour including a tattoo. The regions surrounded by the closed contours are considered tattoo regions. Our method segments tattoos well when the background includes textures and color similar to skin.	cuboid;cut (graph theory);graph cuts in computer vision;image retrieval;information source;pixel;skin (computing)	Joonsoo Kim;Albert Parra;He Li;Edward J. Delp	2015		10.1117/12.2083419	computer vision;image retrieval;skin;image segmentation;scale-space segmentation	Vision	36.942238318621776	-63.10739279416337	78089
74c9bc84e229962b408b8fa5deefcba45f8ddeab	exploring geometric property thresholds for filtering non-text regions in a connected component based text detection application		Automated text detection is a difficult computer vision task. In order to accurately detect and identity text in an image or video, two major problems must be addressed. The primary problem is implementing a robust and reliable method for distinguishing text vs non-text regions in images and videos. Part of the difficulty stems from the almost unlimited combinations of fonts, lighting conditions, distortions, and other variations that can be found in images and videos. This paper explores key properties of two popular and proven methods for implementing text detection; maximum stable external regions (MSER) and stroke width variation.	computer vision;distortion;feature detection (computer vision);feature detection (web development);maximally stable extremal regions;robustness (computer science);thresholding (image processing);video	Teresa Nicole Brooks	2017	CoRR		filter (signal processing);primary problem;machine learning;artificial intelligence;pattern recognition;computer science;connected component	Vision	37.09278856551563	-63.4428877704239	78112
4c8698436395b2b094f83ef86eda57d1814f0cd8	methods of fh feature extraction using compressive receivers.	feature extraction		feature extraction	Richard J. Mammone;S. Bajekal	1986			feature extraction;computer science	NLP	29.625070948729753	-57.94922988509162	78166
f6e70d7ffd9d7dbb56f1604d13a9464bffb5941b	local directional amplitude feature for illumination normalization with application to face recognition		Face recognition under variant illumination conditions has been one of the major research topics in the development of face recognition systems. In this paper we analyze the strength and the weakness of different types of approaches, and design an illumination robust feature by combining the directional and amplitude information as an optimal solution to the problem. We first extract and process the direction and amplitude information of the pixel changes, and then fuse them into a comprehensive feature. We conducted our experiments on CMU-PIE database and Extended Yale B database, and all the results have shown the effectiveness of our approach.	facial recognition system	Chitung Yip;Haifeng Hu;Zhihong Chen	2018		10.1007/978-3-319-97909-0_32	pixel;computer vision;amplitude;normalization (statistics);facial recognition system;computer science;artificial intelligence	Vision	35.413898116369964	-57.7061277812214	78188
40a9e72cd2ccf6fe282e97b22f1844f862de0beb	eye recognition with mixed convolutional and residual network (micore-net)		Although iris recognition has achieved big successes on biometric identification in recent years, difficulties in the collection of iris images with high resolution and in the segmentation of valid regions prevent it from applying to large-scale practical applications. In this paper, we present an eye recognition framework based on deep learning, which relaxes the data collection procedure, improves the anti-fake quality, and promotes the performance of biometric identification. Specifically, we propose and train a mixed convolutional and residual network (MiCoRe-Net) for the eye recognition task. Such an architecture inserts a convolutional layer between every two residual layers and takes the advantages from both of convolutional networks and residual networks. Experiment results show that the proposed approach achieves accuracies of 99.08% and 96.12% on the CASIA-Iris-IntervalV4 and the UBIRIS.v2 datasets, respectively, which outperforms other classical classifiers and deep neural networks with other architectures.	artificial neural network;biometrics;convolutional neural network;deep learning;flow network;image resolution;iris recognition	Zi Wang;Chengcheng Li;Huiru Shao;Jiande Sun	2018	IEEE Access	10.1109/ACCESS.2018.2812208	architecture;convolutional neural network;residual;image segmentation;feature extraction;artificial neural network;iris recognition;deep learning;distributed computing;computer science;pattern recognition;artificial intelligence	Vision	27.536700887532987	-55.538480040706936	78222
c1a371d8bff8cca6642be4035e7f55cecbf746c3	toward more realistic face recognition evaluation protocols for the youtube faces database		One of the key factors to measure the progress of a research problem is the design of appropriate evaluation protocols defined on suitable databases. Recently, the introduction of comprehensive databases and benchmarks of face videos has had a great impact on the development of new face recognition techniques. However, most of the protocols provided for these datasets are limited and do not capture requirements of unconstrained scenarios. That is why sometimes the performance of face recognition methods on current benchmarks seems to be saturated. To address this lack, the tendency is to collect new datasets, which is more expensive and sometimes the main the problem is not the data but the protocols. In this work, we propose new relevant evaluation protocols for the YouTube Faces database (REP-YTF) supporting face verification and open/closed-set identification. The proposal better fits realistic face recognition scenarios and allows us to test existing algorithms at relevant assessment points, under different openness values and taking both videos and images as the gallery. We provide an extensive experimental evaluation, by combining several well-established feature representations with three different metric learning algorithms. The obtained results show that by using the proposed evaluation protocols, there is room for improvement in the recognition performance on the YouTube Faces database.		Yoanna Martínez-Díaz;Heydi Méndez-Vázquez;Leyanis López-Avila	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2018.00082	database;facial recognition system;computer science	Vision	28.315703119726287	-61.13366991656821	78410
ca49af7542fef4e71d9997dca88a33dca573fb69	key incorporation scheme for cancelable biometrics	key incorporation scheme for cancelable biometrics;recognition capacity;cancelable iris recognition;gabor descriptor	Biometrics is becoming an important method for human identification. However, once a biometric pattern is stolen, the user will quickly run out of alternatives and all the applications where the associated biometric pattern is used become insecure. Cancelable biometrics is a solution. However, traditional cancelable biometric methods treat the transformation process and feature extraction process independently. As a result, this kind of cancelable biometric approach would reduce the recognition accuracy. In this paper, we first analyzed the limitations of traditional cancelable biometric methods, and proposed the Key Incorporation Scheme for Cancelable Biometrics approach that could increase the recognition accuracy while achieving “cancelability”. Then we designed the Gabor Descriptor based Cancelable Iris Recognition method that is a practical implementation of the proposed Key Incorporation Scheme. The experimental results demonstrate that our proposed method can significantly improve the iris recognition accuracy while achieving “cancelability”.	biometrics;feature extraction;iris recognition	Yingzi Du;Kai Yang;Zhi Zhou	2011	J. Information Security	10.4236/jis.2011.24018	speech recognition;pattern recognition;computer security	Vision	31.39313283118917	-61.36883175255094	78503
2d34c9dda3c9bcc7cdfd6876fc2dd1cf83b370e0	far at mediaeval 2014 violent scenes detection: a concept-based fusion approach		The MediaEval 2014 Violent Scenes Detection task challenged participants to automatically find violent scenes in a set of videos. We propose to first predict a set of midlevel concepts from low-level visual and auditory features, then fuse the concept predictions and features to detect violent content. With the objective of obtaining a higly generic approach, we deliberately restrict ourselves to use simple general-purpose descriptors with limited temporal context and a common neural network classifier. The system used this year is largely based on the one successfully employed by our group in 2012 and 2013, with some improvements and updated features. Our best-performing run with regard to the official metric received a MAP2014 of 45.06% in the main task and 66.38% in the generalization task.	artificial neural network;general-purpose modeling;high- and low-level	Mats Sjöberg;Ionut Mironica;Markus Schedl;Bogdan Ionescu	2014			fusion;computer vision;artificial intelligence;speech recognition;computer science	AI	25.653791993006696	-57.06629037630643	78543
11cd7fe338b89b2cd1365baaa45d1764cd9ffc6f	video key frame extraction by unsupervised clustering and feedback adjustment	unsupervised clustering;information retrieval;video retrieval;feedback;clustering;key frame extraction	In video information retrieval, key frame extraction has been recognized as one of the important research issues. Although much progress has been made, the existing approaches are either computationally expensive or ineffective in capturing salient visual content. In this paper, we first discuss the importance of key frame extraction and then briefly review and evaluate the existing approaches. To overcome the shortcomings of the existing approaches, we introduce a new algorithm for key frame extraction based on unsupervised clustering. Meanwhile, we provide a feedback chain to adjust the granularity of the extraction result. The proposed algorithm is both computationally simple and able to capture the visual content. The efficiency and effectiveness are validated by large amount of real-world videos.	algorithm;analysis of algorithms;cluster analysis;frame language;information retrieval;key frame	Yueting Zhuang;Yong Rui;Thomas S. Huang	1999	Journal of Computer Science and Technology	10.1007/BF02948517	computer vision;computer science;data mining;feedback;cluster analysis;information retrieval	AI	38.63365851567785	-52.085949071225684	78658
bdb9d847ec7aa6ee92b6a64779d75fe4dd0ab741	infrared gait recognition based on wavelet transform and support vector machine	integrable model;gait recognition;wavelet transform;thermal imaging;feature extraction;human body;image sequence;infrared thermal imaging;support vector machine;infrared	To detect human body and remove noises from complex background, illumination variations and objects, the infrared thermal imaging was applied to collect gait video and an infrared thermal gait database was established in this paper. Multi-variables gait feature was extracted according to a novel method combining integral model and simplified model. Also the wavelet transform, invariant moments and skeleton theory were used to extract gait features. The support vector machine was employed to classify gaits. This proposed method was applied to the infrared gait database and achieved 78%–91% for the probability of correct recognition. The recognition rates were insensitive for the items of holding ball and loading package. However, there was significant influence for the item of wearing heavy coat. The infrared thermal imaging was potential for better description of human body moving within image sequences. & 2010 Elsevier Ltd. All rights reserved.	ball project;gait analysis;image moment;lambert's cosine law;support vector machine;wavelet transform	Zhaojun Xue;Dong Ming;Wei Song;Baikun Wan;Shijiu Jin	2010	Pattern Recognition	10.1016/j.patcog.2010.03.011	support vector machine;computer vision;human body;speech recognition;infrared;feature extraction;computer science;machine learning;pattern recognition;wavelet transform	Vision	32.32717170634037	-60.031523684000234	78698
ecf3bbf42a66efd54b528ee046fe613bdd1ca4ce	efficient two-stage speaker identification based on universal background models	databases;biometrics access control vectors feature extraction biological system modeling databases probes computational efficiency;biometrics access control;biological system modeling;probes;system response time two stage speaker identification system universal background models gaussian mixture model binarized voice biometric templates;vectors;feature extraction;speaker recognition biometrics access control gaussian processes mixture models;computational efficiency	Conventional speaker identification systems are already field-proven with respect to recognition accuracy. Since any biometric identification requires exhaustive 1 : N comparisons for identifying a biometric probe, comparison time frequently dominates the overall computational workload, preventing the system from being executed in real-time. In this paper we propose a computational efficient two-stage speaker identification system based on Gaussian Mixture Model and Universal Background Model. Binarized voice biometric templates are utilized to pre-screen a large database and thereby reduce the required amount of full comparisons to a fraction of the total. Experimental evaluations demonstrate that the proposed system is capable of significantly accelerating the response-time of the system and, at the same time, identification performance is maintained, confirming the soundness of the scheme.	algorithmic efficiency;binary image;biometrics;central processing unit;computation;consistency model;database;mixture model;real-time clock;speaker recognition	Stefan Billeb;Christian Rathgeb;Michael Buschbeck;Herbert Reininger;Klaus Kasper	2014	2014 International Conference of the Biometrics Special Interest Group (BIOSIG)		speech recognition;feature extraction;computer science;machine learning;pattern recognition;database	EDA	29.685827732859114	-63.659284321700824	78915
e00e8dc033b7009eb70e38891ee7e0eea80402b0	violent scene detection using mid-level feature	mid level feature;video mining;video retrieval;visual feature;violent scene detection	Violent scene detection (VSD) refers to the task of detecting shots containing violent scenes in videos. With a wide range of promising real-world applications (e.g. movies/films inspection, video on demand, semantic video indexing and retrieval), VSD has been an important research problem. A typical approach for VSD is to learn a violent scene classifier and then apply it to video shots. Finding good feature representation for video shots is therefore essential to achieving high classification accuracy. It has been shown in recent work that using low-level features results in disappointing performance, since low-level features cannot convey high-level semantic information to represent violence concept. In this paper, we propose to use mid-level features to narrow the semantic gap between low-level features and violence concept. The mid-level features of a training (or test) video shots are formulated by concatenating scores returned by attribute classifiers. Attributes related to violence concept are manually defined. Compared to the original violence concept, the attributes have smaller gap to the low-level feature. Each corresponding attribute classifier is trained by using low-level features. We conduct experiments on MediaEval VSD benchmark dataset. The results show that, by using mid-level features, our proposed method outperforms the standard approach directly using low-level features.	attribute grammar;benchmark (computing);concatenation;experiment;high- and low-level;microsoft visio;scene graph;sensor	Vu Lam;Sang Phan Le;Thanh Duc Ngo;Duy-Dinh Le;Duc Anh Duong;Shin'ichi Satoh	2013		10.1145/2542050.2542070	computer vision;geography;data mining;multimedia	Vision	33.25813010370862	-53.1862846634254	78936
b0fbc20e050f4d77bac8a444d4aa030d8f5d5ccc	recognition of human's implicit intention based on an eyeball movement pattern analysis	human intention monitoring system;navigational intent;informational intent;human computer interface interaction;eyeball movement;implicit intention	We propose a new approach for a human’s implicit intention recognition system based on an eyeball movement pattern analysis. In this paper, we present a comprehensive classification of human’s implicit intention. Based on Bernard’s research, we define the Human’s implicit intention as informational and navigational intent. The intent for navigational searching is to locate a particular interesting object in an input scene. The intent for informational searching is to locate interesting area concerning a particular topic in order to obtain information from a specific location. In the proposed model, eyeball movement pattern analysis is considered for classifying the two different types of implicit intention. The experimental results show that the proposed model generates plausible recognition performance using a fixation length and counts with a simple nearest neighborhood classifier.	maniac mansion;pattern recognition	Young-Min Jang;Sangil Lee;Rammohan Mallipeddi;Ho-Wan Kwak;Minho Lee	2011		10.1007/978-3-642-24955-6_17	computer vision	ML	29.62219726294257	-59.374840677116985	78961
6720ef326c8323fcbb1ac2bf017ad1da7437b1d1	the finger vein recognition based on shearlet	image recognition;veins;wavelet transforms;feature extraction;image reconstruction;decision support systems;fingers	In recent years, finger vein recognition has become more attractive due to some obvious advantages, such as: in-vivo recognition, high anti-counterfeiting, high acceptability, and high stability, etc. But for some finger vein image, its vein structure is too simple and the useful information is too less, the conventional recognition method often behave badly for this kind of image. For this kind of problem, the multi-scale analysis represented by wavelet is a viable choice, which can extract more information from multi scale of finger vein image. However, there is also a main disadvantage for wavelet: the singularity information extracted using wavelet is point singularity, and the representation based on wavelet has too much redundancy. Therefore, the newer and better method is introduced for finger vein recognition: multi-scale geometric analysis (MGA). In this paper, DSST (Discrete Separable Shearlet Transform) is chosen as the image decomposition and feature extraction tool, which is a fast implementation of shearlet and has a better performance than other MGA method. Several kinds of feature extraction method are proposed based on DSST decomposition sub-band for finger vein recognition: Kurtosis value, energy value and Hu invariant moment. In contrast experiment, the method based on MHD (Modified Hausdorff Distance) feature, relative distance feature, template feature, wavelet feature, ridgelet feature and curvelet feature is used for recognition comparison. The experiment result show that the feature extraction method based on DSST is more applicable for finger vein image, and the feature extracted based on DSST has a better performance.	curvelet;feature extraction;finger vein recognition;geometric analysis;hausdorff dimension;hercules graphics card;holographic principle;mediawiki;shearlet;technological singularity;video-in video-out;wavelet	Xiaofei Yang;Chunhua Yang;Zhijun Yao	2016	2016 8th International Conference on Wireless Communications & Signal Processing (WCSP)	10.1109/WCSP.2016.7752499	computer vision;speech recognition;feature extraction;pattern recognition;mathematics;feature	Robotics	33.745731539038346	-62.14339765620382	79033
52f995b79c29954a38929aad2228686a4086c776	secure and robust user authentication using partial fingerprint matching		This paper proposes a robust fingerprint-based authentication algorithm that ensures secure verification even with limited-sized partial fingerprints. It has become increasingly common that a number of recent consumer devices, such as smartphones, employ fingerprint sensors for user authentication. The sensors to be embedded are generally preferred to take up less space for better usability and product design, the sensing areas are therefore limited in size. To supplement the insufficient area, devices often store multiple acquisitions from a single finger in enrollment, later to verify at least any one of them successfully match an acquisition in authentication. Considering the low information entropy of a partial image, the security aspect of small area-based systems is a major concern. On the other hand, unpredictable variability due to finger rotation, grip positions, and skin deformation has a negative impact on biometric performance. This paper presents a fingerprint matcher or verifier incorporating several efficient algorithms against these concerns on both the performance and security aspects. A method of “segmented area matching” brought an enhanced robustness to the variability, especially to finger rotation. And a method of “feature-weighted block scoring” provided with more detailed image discrimination, resulting in improved security. Experimental evaluations with extensive database of partial fingerprint images from more than 100 people, acquired by a small-sized capacitive sensor, demonstrated a significant improvement over the previously suggested algorithms in both aspects.	algorithm;authentication;biometrics;capacitive sensing;embedded system;entropy (information theory);fingerprint recognition;heart rate variability;sensor;smartphone;spatial variability;usability	Geuntae Bae;Hojae Lee;Sunghoon Son;Doha Hwang;Jongseok Kim	2018	2018 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2018.8326078	artificial intelligence;computer vision;fingerprint;robustness (computer science);image segmentation;usability;computer science;biometrics;product design;fingerprint recognition;authentication	Mobile	31.28129994270522	-62.17194734471585	79104
88bc45cf30ec05f42c4480c9e7d9904c55e43322	steganalysis by subtractive pixel adjacency matrix and dimensionality reduction		Subtractive pixel adjacency matrix (SPAM) features, introduced by Pevn’y et al. as a type of Markov chain features, are widely used for blind steganalysis in the spatial domain. In this paper, we present three improvements to SPAM as follows: 1) new features based on parallel subtractive pixels are added to the SPAM features, which only refer to collinear subtractive pixels; 2) features are extracted not only from the spatial image, but also from its grayscale-inverted image, making the feature matrices symmetrical and reducing their dimensionality by about half; and 3) a new kind of adjacency matrix is used, thereby reducing about 3/4 of the dimensionality of the features. Experimental results show that these methods for dimensionality reduction are very effective and that the proposed features outperform SPAM.	2.5d;adjacency matrix;dimensionality reduction;experiment;grayscale;jpeg;markov chain;meta content framework;pixel;steganalysis	Hao Zhang;Xijian Ping;Mankun Xu;Ran Wang	2013	Science China Information Sciences	10.1007/s11432-013-4793-x	computer vision;machine learning;pattern recognition;mathematics	ML	35.62177595175541	-60.11908541758524	79198
8388589cef1fb3bb592240b94064213908217263	3d shape segmentation based on viewpoint entropy and projective fully convolutional networks fusing multi-view features		This paper introduces an architecture for segmenting 3D shapes into labeled semantic parts. Our architecture combines viewpoint selection method based on viewpoint entropy, multi-view image-based Fully Convolutional Networks (FCNs) and graph cuts optimization method to yield coherent segmentation of 3D shapes. First, we select iteratively a fixed number of perspectives with the maximum viewpoint entropy from existing viewpoints that can cover the shapeu0027s triangles, to maximally and automatically adjust the distance between the viewpoint and the center point of the shape to make sure the shape projected to fill the render window as wide as possible. Second, the image-based FCN is used for efficient view-based reasoning about 3D shape parts. In this process, global features generated by max view pooling are concatenated with every single viewu0027s feature in the fully connected layer before upsampling. Then, the multi-view FCN outputs confidence maps per part, which are then input into the projection layer that contains the mapping relationship of every shapeu0027s triangles and their projective pixelsu0027 positions in the rendered images from selected perspectives. And then, the FCN outputs are projected back onto 3D shape surfaces. and max view pooling is applied to the output of the projection layer so that every triangle of each shape has a unique probability for each label. Finally, graph cuts algorithm is implemented for the final segmentation result.		Panpan Shui;Pengyu Wang;Fenggen Yu;Bingyang Hu;Yuan Gan;Kun Liu;Yan Zhang	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8545586	pixel;computer vision;market segmentation;architecture;pooling;image segmentation;concatenation;artificial intelligence;upsampling;cut;pattern recognition;computer science	Vision	29.66082715535913	-52.665542378062945	79300
3bedbd5986bacdffcc51b79d19a12d81c093dd67	color histogram and first order statistics for content based image retrieval		Content Based Image Retrieval (CBIR) is one of the fastest growing research areas in the domain of multimedia. Due to the in- crease in digital contents these days, users are experiencing difficulties in searching for specific images in their databases. This paper proposed a new effective and efficient image retrieval technique based on color histogram using Hue-Saturation-Value (HSV) and First Order Statistics (FOS), namely HSV-fos. FOS is used for the extraction of texture fea- tures while color histogram deals with color information of the image. Performance of the proposed technique is compared with the Variance Segment and Histogram based techniques and results shows that HSV- fos technique achieved 15% higher accuracy as compared to Variance Segment and Histogram-based techniques. The proposed technique can help the forensic department for identification of suspects.	color histogram;content-based image retrieval	Muhammad Imran;Rathiah Hashim;Noor Eliza Abd Khalid	2014		10.1007/978-3-319-07692-8_15	color histogram;computer vision;color normalization;color image;histogram matching;balanced histogram thresholding;multimedia;adaptive histogram equalization;histogram equalization;information retrieval;image histogram	Vision	38.9140893098199	-60.83856871616281	79303
8d484e0c247a744d87c943881f1533c4de44a00c	direct unsupervised text line extraction from colored historical manuscript images using dct	g400 computer science	Extracting lines of text from a manuscript is an important preprocessing step in many digital paleography applications. These extracted lines play a fundamental part in the identification of the author and/or age of the manuscript. In this paper we present an unsupervised approach to text line extraction in historical manuscripts that can be applied directly to a color manuscript image. Each of the red, green and blue channels are processed separately by applying DCT on them individually. One of the key advantages of this approach is that it can be applied directly to the manuscript image without any preprocessing, training or tuning steps. Extensive testing on complex Arabic handwritten manuscripts shows the effectiveness of the proposed approach.	discrete cosine transform	Asim Baig;Somaya Al-Máadeed;Ahmed Bouridane;Mohamed Cheriet	2016		10.1007/978-3-319-41501-7_84	speech recognition;computer science	NLP	35.786522214016024	-65.55035269223089	79346
c08c63447e64928dcb6b22ea497e013aa5c26402	a cloud-based monitoring system via face recognition using gabor and cs-lbp features	gabor;cs-lbp;cloud storage;ubiquitous monitor;face detection and recognition	Face detection and recognition is an important topic in security. Currently, ubiquitous monitoring has received a large amount of attention. This paper proposes a cloud-based ubiquitous monitoring system via face recognition. It consists of a monitoring client module for face detection and recognition and a cloud storage module for data visualization. In the monitoring client module, the center-symmetric local Gabor binary pattern feature extraction method is proposed for face recognition, which combines improved multi-scale Gabor and center-symmetric local binary pattern (CS-LBP) features. This method maintains crucial local features, reduces the Gabor filter complexity, and adds rotational invariance and more precise texture information. A large number of experiments on the ORL, Yale-B, and Yale databases show that the proposed method obtains significantly better recognition rates than the LBP, CS-LBP, and Scale Gabor methods. Furthermore, we propose a Web browser-based data visualization that renders the geographic locations of the face detection and recognition results.	adaboost;algorithm;belief propagation;binary pattern (image generation);cs games;cs-blast;cs-cipher;cloud computing;cloud storage;data visualization;database;experiment;face detection;facial recognition system;feature extraction;gabor filter;local binary patterns;real-time clock;rendering (computer graphics);requirement;return loss;tire-pressure monitoring system;web application	Chen Li;Wei Wei;Jiaxue Li;Wei Song	2016	The Journal of Supercomputing	10.1007/s11227-016-1840-6	computer vision;computer science;pattern recognition;data mining;three-dimensional face recognition	Vision	31.57332861092823	-57.82210869617386	79625
186fdd93bf6be74154a2112a71bc1cd20ec86382	radar emitter signal recognition based on sample entropy and fuzzy entropy	radar emitter;fuzzy entropy;feature extraction;sample entropy;signal recognition	Aiming at the problems of the radar emitter signal (RES) recognition based on intra-pulse feature, a novel entropy feature extraction approach is proposed. In this method the sample entropy (SampEn) and fuzzy entropy (FzzyEn) are presented to extract features from RES. The SampEn can measure the complexity of RES from a short signal data, and the FzzyEn is used as a measure of the uncertainty. Feature vectors abstracted from 6 typical RES are used as the input of support vector machine (SVM) classifier to perform the signal recognition. Experimental result indicates that in a large range of SNR the introduced method achieves a good accuracy recognition rate. Simulation verifies the method to be feasible.	radar;sample entropy	Shiqiang Wang;Dengfu Zhang;Duyan Bi;Xiaoju Yong;Cheng Li	2011		10.1007/978-3-642-31919-8_81	speech recognition;machine learning;pattern recognition;mathematics	Robotics	27.232914835254807	-61.47461182924902	79644
dc4f5a10c791f750cb1e7e5e77e09fce9dd82dd9	hmm-based recognition engine using a novel approach for statistical feature extraction	databases;training;apti training dataset hmm based recognition engine statistical feature extraction word image feature vector set hidden markov models tool kit htk arabic printed text image apti database hmm classifier;hidden markov models;engines;vectors;visual databases feature extraction hidden markov models image recognition pattern classification text analysis;feature extraction;text recognition;run length encoding hidden markov modesl arabic printed text recognition;hidden markov models feature extraction vectors training databases text recognition engines	This paper extracts statistical features using a novel approach. The feature set locally measure the characteristics of the image. The proposed approach encodes the extracted features, from a one-pixel width window that slides horizontally the word image. We then inject the feature vector set into a recognition engine. The recognition engine is built using Hidden Markov Models Tool Kit (HTK). The system is trained and tested on the Arabic Printed Text Image (APTI) database. In order to select the optimal parameters for the HMM classifier, the APTI training dataset is further divided into a smaller training subset and a verification set. The estimated parameters are, then, used in the testing phase. The presented technique provides state-of-the-art recognition results on the APTI database using HMMs. The overall system achieved a recognition rate more than 97%.	ascii art;codebook;experiment;feature extraction;feature vector;htk (software);hidden markov model;markov chain;pixel;run-length encoding	Mohammad S. Khorsheed;Samir Ouis	2014	The 2014 2nd International Conference on Systems and Informatics (ICSAI 2014)	10.1109/ICSAI.2014.7009290	speech recognition;feature;feature extraction;computer science;machine learning;pattern recognition;feature;hidden markov model	Vision	33.14753162290742	-64.77337703203256	79695
56be6b310b9a267319bd27a070d6eb12157a07c2	face recognition vendor test 2002	performance measure;protocols;video signal processing;large data sets;three dimensional;large scale;real world application;face recognition;xml;statistical testing face recognition video signal processing xml protocols;older people;xml based evaluation protocol face recognition vendor test 2002 frvt face recognition system performance statistics test computation three dimensional morphable model video sequences;statistical testing;face recognition prototypes demography video sequences protocols system testing large scale systems educational institutions statistics watches	The Face Recognition Vendor Test (FRVT) 2002 is an independently administered technology evaluation of mature face recognition systems. FRVT 2002 provides performance measures for assessing the capability of face recognition systems to meet requirements for large-scale, real-world applications. Participation in FRVT 2002 was open to commercial and mature prototype systems from universities, research institutes, and companies. Ten companies submitted either commercial or prototype systems. FRVT 2002 computed performance statistics on an extremely large data set—121,589 operational facial images of 37,437 individuals. FRVT 2002 1) characterized identification and watch list performance as a function of database size, 2) estimated the variability in performance for different groups of people, 3) characterized performance as a function of elapsed time between enrolled and new images of a person, and 4) investigated the effect of demographics on performance. FRVT 2002 showed that recognition from indoor images has made substantial progress since FRVT 2000. Demographic results show that males are easier to recognize than females and that older people are easier to recognize than younger people. FRVT 2002 also assessed the impact of three new techniques for improving face recognition: three-dimensional morphable models, normalization of similarity scores, and face recognition from video sequences. Results show that three-dimensional morphable models and normalization increase performance and that face recognition from video sequences offers only a limited increase in performance over still images. A new XML-based evaluation protocol was developed for FRVT 2002. This protocol is flexible and supports evaluations of biometrics in general. The FRVT 2002 reports can be found at http://www.frvt.org. 1 Please direct correspondence to Jonathon Phillips at jphillips@darpa.mil or jonathon@nist.gov. Proceedings of the IEEE International Workshop on Analysis and Modeling of Faces and Gestures (AMFG’03) 0-7695-2010-3/03 $ 17.00 © 2003 IEEE	biometrics;face recognition vendor test;facial recognition system;proceedings of the ieee;prototype;requirement;spatial variability;xml	P. Jonathon Phillips;Patrick Grother;Ross J. Micheals;Duane M. Blackburn;Elham Tabassi;Mike Bone	2003		10.1109/AMFG.2003.1240822	facial recognition system;three-dimensional space;communications protocol;computer vision;statistical hypothesis testing;xml;simulation;speech recognition;computer science;machine learning;data mining;face recognition grand challenge;statistics	Vision	28.43998826700681	-63.972064262888175	79797
7f97ab08fe01ee7f1d002ff1a266f0b35690521f	modulating and attending the source image during encoding improves multimodal translation		We propose a new and fully end-to-end approach for multimodal translation where the source text encoder modulates the entire visual input processing using conditional batch normalization, in order to compute the most informative image features for our task. Additionally, we propose a new attention mechanism derived from this original idea, where the attention model for the visual input is conditioned on the source text encoder representations. In the paper, we detail our models as well as the image analysis pipeline. Finally, we report experimental results. They are, as far as we know, the new state of the art on three different test sets.	conditional comment;encoder;end-to-end principle;image analysis;information;modulation;multimodal interaction;test set	Jean-Benoit Delbrouck;Stéphane Dupont	2017	CoRR		source text;encoder;feature (computer vision);encoding (memory);normalization (statistics);computer science;artificial intelligence;pattern recognition	NLP	25.43293690711824	-55.16141050492245	79919
5df11c59e3b47189486445f5833675bf08359bfe	influence of image classification accuracy on saliency map estimation		Saliency map estimation in computer vision aims to estimate the locations where people gaze in images. Since people tend to look at objects in images, the parameters of the model pre-trained on ImageNet for image classification are useful for the saliency map estimation. However, there is no research on the relationship between the image classification accuracy and the performance of the saliency map estimation. In this study, it is shown that there is a strong correlation between image classification accuracy and saliency map estimation accuracy. The authors also investigated the effective architecture based on multi-scale images and the up-sampling layers to refine the saliency-map resolution. The model achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003 datasets. In the MIT saliency benchmark, the model achieved the best performance in some metrics and competitive results in the other metrics.	benchmark (computing);computer vision;imagenet;upsampling	Taiki Oyama;Takao Yamanaka	2018	CoRR		architecture;contextual image classification;salience (neuroscience);computer vision;saliency map;computer science;artificial intelligence	Vision	25.095003897992886	-52.73795169027296	80094
36f003122c228b11fb1453405304047bd9e7418c	applicability of self-organizing maps in content-based image classification		Image databases are getting larger and diverse with the coming up of new imaging devices and advancements in technology. Content-based image classification (CBIC) is a method to classify images from large databases into different categories, on the basis of image content. An efficient image representation is an important component of a CBIC system. In this paper, we demonstrate that Self-Organizing Maps (SOM)-based clustering can be used to form an efficient representation of an image for a CBIC system. The proposed method first extracts Scale-Invariant Feature Transform (SIFT) features from images. Then it uses SOM for clustering of descriptors and forming a Bag of Features (BOF) or Vector of Locally Aggregated Descriptors (VLAD) representation of image. The performance of proposed method has been compared with systems using k-means clustering for forming VLAD or BOF representations of an image. The classification performance of proposed method is found to be better in terms of F-measure (FM) value and execution time.		Kumar Rohit;Gorthi R. K. Sai Subrahmanyam;Deepak Mishra	2016		10.1007/978-981-10-2104-6_28	cluster analysis;self-organizing map;artificial intelligence;scale-invariant feature transform;contextual image classification;pattern recognition;computer science	Vision	37.00157031671777	-59.63569789491632	80140
c8e650dcd211ff117d0218328a22359d8fe93098	gpu accelerated 3d face registration / recognition	three dimensions;large scale;facial expression;vector processor	This paper proposes a novel approach to both registration and recognition of face in three dimensions. The presented method is based on normal map metric to perform either the alignment of captured face to a reference template or the comparison between any two faces in a gallery. As the metric involved is highly suited to be computed via vector processor, we propose an implementation of the whole framework on last generation graphics boards, to exploit the potential of GPUs applied to large scale biometric identification applications. This work shows how the use of affordable consumer grade hardware could allow ultra rapid comparison between face descriptors through their highly specialized architecture. The approach also addresses facial expression changes by means of a subject specific weighting masks. We include preliminary results of experiments conducted on a proprietary gallery and on a subset of FRGC database.		Andrea F. Abate;Michele Nappi;Stefano Ricciardi;Gabriele Sabatino	2007		10.1007/978-3-540-74549-5_98	three-dimensional space;computer vision;vector processor;simulation;computer science;machine learning;facial expression;computer graphics (images)	Vision	28.276120507294518	-64.38864136484224	80336
8a53059ab7d94d11bae2f3ecbf8998b4770d290a	introduction of explicit visual saliency in training of deep cnns: application to architectural styles classification		Introduction of visual saliency or interestingness in the content selection for image classification tasks is an intensively researched topic. It has been namely fulfilled for feature selection in feature-based methods. Nowadays, in the winner classifiers of visual content such as Deep Convolutional Neural Networks, visual saliency maps have not been introduced explicitly. Pooling features in CNNs is known as a good strategy to reduce data dimensionality, computational complexity and summarize representative features for subsequent layers. In this paper we introduce visual saliency in network pooling layers to spatially filter relevant features for deeper layers. Our experiments are conducted in a specific task to identify Mexican architectural styles. The results are promising: proposed approach reduces model loss and training time keeping the same accuracy as the base-line CNN.	bottom-up parsing;computational complexity theory;computer vision;convolutional neural network;deep learning;experiment;feature selection;map;outline of object recognition;top-down and bottom-up design	Abraham Montoya Obeso;Jenny Benois-Pineau;Mireya S. García-Vázquez;Alejandro Alvaro Ramírez-Acosta	2018	2018 International Conference on Content-Based Multimedia Indexing (CBMI)	10.1109/CBMI.2018.8516465	task analysis;convolutional neural network;feature extraction;feature selection;artificial intelligence;salience (neuroscience);pooling;curse of dimensionality;contextual image classification;pattern recognition;computer science	Vision	24.827557801856972	-53.348730660024714	80615
47a87f5361b5efb20b59eb02dafb8fe076577111	data driven low-level object detection and segmentation			high- and low-level;memory segmentation;object detection	Guoyi Fu	2008				Vision	30.257964570606454	-56.88827599089051	80955
4647e5af399349579763576b6eedcf976c826789	dct-based watermarking for color images via two-dimensional linear discriminant analysis		In this paper, we propose a watermarking algorithm based on Discrete Cosine Transform (DCT) using Two-dimensional Linear Discriminant Analysis (2DLDA) for color images. At first, the color image is converted into the YIQ color space and then transformed into the frequency domain by DCT. During the embedding stage, two watermarks of reference and logo are embedded into the Q component. Then, watermark extraction is done by 2DLDA from the Q component based on the frequency domain of DCT. By considering the Human Visual System (HVS), experimental results have shown that the watermark can be correctly extracted and better robustness is provided after various image attacks.		I-Hui Pan;Ping Sheng Huang;Te-Jen Chang	2013		10.1007/978-94-007-6996-0_7	watermark;digital watermarking;frequency domain;discrete cosine transform;linear discriminant analysis;artificial intelligence;human visual system model;mathematics;color image;pattern recognition;color space	Vision	35.17075825361581	-59.9963247084389	80961
5b4eed52b4929599b0bada94b392eabfd4f69452	animal-vehicle collision mitigation system for automated vehicles	detectors;animals;large road animals animal vehicle collision mitigation system automated vehicles roadways automated systems robots ultrasonic sensors innovative technology smart cameras vision based solution haar adaboost hog adaboost local binary pattern lbp adaboost support vector machine classifiers hog features thermographic images;animals feature extraction vehicles detectors cameras image color analysis computer architecture;computer architecture;support vector machines collision avoidance computer vision image classification intelligent transportation systems road traffic control;image color analysis;obstacle detection and avoidance animal vehicle collisions avcs detection automated systems;feature extraction;vehicles;cameras	Detecting large animals on roadways using automated systems such as robots or vehicles is a vital task. This can be achieved using conventional tools such as ultrasonic sensors, or with innovative technology based on smart cameras. In this paper, we investigate a vision-based solution. We begin the paper by performing a comparative study between three detectors: 1) Haar-AdaBoost; 2) histogram of oriented gradient (HOG)-AdaBoost; and 3) local binary pattern (LBP)-AdaBoost, which were initially developed to detect humans and their faces. These detectors are implemented, evaluated, and compared to each other in terms of accuracy and processing time. Based on our evaluation and comparison results, we design a two-stage architecture which outperforms the aforementioned detectors. The proposed architecture detects candidate regions of interest using LBP-AdaBoost in the first stage, which offers robustness to false positives in real-time conditions. The second stage is based on support vector machine classifiers that were trained using HOG features. The training data are generated from our novel dataset called large animal dataset, which contains common and thermographic images of large road-animals. We emphasize that no such public dataset currently exists.	adaboost;algorithm;belief propagation;binary pattern (image generation);experiment;film-type patterned retarder;gradient;h.264/mpeg-4 avc;haar wavelet;lateral thinking;local binary patterns;moose (perl);real-time clock;region of interest;robot;sensor;support vector machine;whole earth 'lectronic link	Abdelhamid Mammeri;Depu Zhou;Azzedine Boukerche	2016	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2015.2497235	computer vision;detector;simulation;feature extraction;computer science;machine learning	Robotics	32.400908191496285	-55.81934840482578	81014
1976464b1d03bc42d09657462009e2bbed484624	a learning-based system for generating exaggerative caricature from face images with expression	eyebrows;mouth;exaggerative caricature;learning;face expression learning based system exaggerative caricature generation system;face expression;統計學習;exaggerative caricature generation system;prototypes;training;learning based system;statistical learning exaggerative caricature facial expression;emotion recognition;expression;testing;臉部;exaggerative;lph;glass;learning systems;training data;eyes;statistical learning;face recognition;shape;誇大;漫畫;facial;caricature;facial features;learning systems emotion recognition face recognition;feature selection;face;shape eyes eyebrows prototypes training data nose mouth computer science glass statistical learning;表情;computer science;facial expression;nose	In this paper, we propose a learning-based system for generating exaggerative caricatures with expression. Most of the previous works can only deal with frontal face images with neutral expression without glasses or hats, and are unable to apply more than one drawing prototype which was learned from the caricatures drawn by one single cartoonist at a time. The proposed caricature generation system exaggerates face images with expressions and learns the drawing prototypes from training data as well. Experimental results show that our system can capture the features selected by the artist and exaggerate them in similar ways.	prototype	Ting-Ting Yang;Shang-Hong Lai	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495565	face;computer vision;training set;speech recognition;shape;computer science;expression;machine learning;prototype;glass;software testing;feature selection;facial expression	Robotics	26.1209636104978	-60.03493474710168	81029
f97ae2f82db714c0bd288ac7302689af15ed30e6	comparison of 3d local and global descriptors for similarity retrieval of range data	local descriptors;range data retrieval;single view depth data description;similarity indexing;global descriptors	Recent improvements in scanning technologies such as consumer penetration of RGB-D cameras, lead obtaining and managing range image databases practical. Hence, the need for describing and indexing such data arises. In this study, we focus on similarity indexing of range data among a database of range objects (range-to-range retrieval) by employing only single view depth information. We utilize feature based approaches both on local and global scales. However, the emphasis is on the local descriptors with their global representations. A comparative study with extensive experimental results is presented. In addition, we introduce a publicly available range object database which is large and has a high diversity that is suitable for similarity retrieval applications. The simulation results indicate competitive performance between local and global methods. While better complexity trade-off can be achieved with the global techniques, local methods perform better in distinguishing different parts of incomplete depth data.	computational complexity theory;data descriptor;database;feature extraction;feature vector;overfitting;performance;range imaging;scope (computer science);simulation	Neslihan Bayramoglu;A. Aydin Alatan	2016	Neurocomputing	10.1016/j.neucom.2015.08.105	computer vision;computer science;data mining;information retrieval	DB	37.194919516035675	-55.41169218571059	81076
0d800cf4ee14b246ad45c2846aeb771a591b8530	eye movement-driven defense against iris print-attacks	iris print attack;eye movements;anti spoofing cues	This paper proposes a methodology for the utilization of eye movement cues for the task of iris print-attack detection. We investigate the fundamental distortions arising in the eye movement signal during an iris print-attack, due to the structural and functional discrepancies between a paper-printed iris and a natural eye iris. The performed experiments involve the execution of practical print-attacks against an eye-tracking device, and the collection of the resulting eye movement signals. The developed methodology for the detection of print-attack signal distortions is evaluated on a large database collected from 200 subjects, which contains both the real (‘live’) eye movement signals and the print-attack (‘spoof’) eye movement signals. The suggested methodology provides a sufficiently high detection performance, with a maximum average classification rate (ACR) of 96.5% and a minimum equal error rate (EER) of 3.4%. Due to the hardware similarities between eye tracking and iris capturing systems, we hypothesize that the proposed methodology can be adopted into the existing iris recognition systems with minimal cost. To further support this hypothesis we experimentally investigate the robustness of our scheme by simulating conditions of reduced sampling resolution (temporal and spatial), and of limited duration of the eye movement signals. 2015 Elsevier Ltd. All rights reserved. This is a pre print	distortion;enhanced entity–relationship model;experiment;eye tracking;iris recognition;printing;sampling (signal processing);simulation;tracking system	Ioannis Rigas;Oleg V. Komogortsev	2015	Pattern Recognition Letters	10.1016/j.patrec.2015.06.011	computer vision;simulation;eye movement	AI	30.32542269064065	-62.93578483213304	81134
f1f8baf7f1829a31f6793368d2140184e471db9e	application of the polar-fourier greyscale descriptor to the problem of identification of persons based on ear images	polar-fourier transform.;ear biometrics;ear recognition	  The threat caused by criminality and terrorism resulted in a significant increase in the interest in research focused on effective  methods for its reducing. One of the most important solutions for the mentioned problem is the biometric identification of  persons. Hence, it became a very important issue nowadays. A crucial aspect of biometrics is the searching for automatic methods  that may be applied for the recognition of human beings. Currently, fingerprints as well as the face are the most popular  applied biometric features. However, in order to increase the efficiency of the developed systems, new modalities are becoming  more and more popular. On one hand, researchers are looking for more effective biometrics while on the other hand the idea  of applying few modalities jointly in multimodal systems is becoming more popular. An ear is an example of lately explored  new biometric features. Its uniqueness is the most important advantage. Similarly to the face, the auricle is distinguishable  for various persons, thanks to its complex and stable structure. Therefore, in the paper an algorithm for human identification  based on ear images is presented and tested. It uses the improved version of the Polar-Fourier Greyscale Descriptor for feature  representation. The method was tested using 225 digital ear images for 45 persons and has achieved 84% efficiency.    	grayscale	Dariusz Frejlichowski	2011		10.1007/978-3-642-23154-4_1	communication;anatomy	Vision	32.46535250289285	-61.299085542288076	81186
25a07bea9031afd6a3b978847d9ce01df9c89908	deep semantics-preserving hashing based skin lesion image retrieval		This study proposes a content-based pigmented skin lesion image retrieval scheme on semantic hash clustering on the output of the deep neural networks. The skin lesion images are acquired with standard digital cameras or mobile phones. To retrieval skin lesion images efficiently online, semi-supervised deep convolutional neural network incorporated with hash functions jointly learn feature representations, for preserving similar semantics between skin lesion images, and mappings to hash codes. The target candidates are clustered by Affinity Propagation (AP) for ranking, which are selected among the outputs of layer F7 based on the Hamming distance of their semantic hash codes. Experiments on 4 disease categories of pigmented skin lesions of a set of 239 images yielded a specificity of 93.4% and a sensitivity of 80.89%.	hash function;image retrieval	Xiaorong Pu;Yan Li;Hang Qiu;Yinhui Sun	2017		10.1007/978-3-319-59081-3_34	lesion;hamming distance;convolutional neural network;image retrieval;computer science;artificial neural network;hash function;cluster analysis;pattern recognition;computer vision;affinity propagation;artificial intelligence	Vision	26.835796413102646	-56.526451207927686	81280
efbebd09b6461f53382fa3eb04a3203b758fb1a1	biometric authentication	biometric authentication;biometric authentication	A new approach to soft biometrics aims to use human labelling as part of the process. This is consistent with analysis of surveillance video where people might be imaged at too low resolution or quality for conventional biometrics to be deployed. In this manner, people use anatomical descriptions of subjects to achieve recognition, rather than the usual measurements of personal characteristics used in biometrics. As such the labels need careful consideration in their construction, and should demonstrate correlation consistent with known human physiology. We describe our original process for generating these labels and analyse relationships between them. This gives insight into the perspicacity of using a human labelling system for biometric purposes.	authentication;closed-circuit television;image resolution;soft biometrics	Silvio Barra;Maria De Marsico;Virginio Cantoni;Daniel Riccio	2014		10.1007/978-3-319-13386-7	facial recognition system;fingerprint;speech recognition;hand geometry;computer science;authorization;computer security;signature recognition	Vision	28.816570446785306	-62.101076039682994	81307
ead2ed16ad327194f0ddd4e7ef3ecc402071fae8	an automatic classification system applied in medical images	image features;image recognition multiclass classification system medical image supporting vector machine svm rbf imageclefos organizer;image recognition;imageclefos organizer;supporting vector machine;support vector machines;texture mapping;image classification;rbf;simulation experiment;support vector machines image classification medical image processing;radial basis function;medical image;multi class classification;medical image processing;svm;principle component analysis;support vector machine;biomedical imaging medical diagnostic imaging support vector machines support vector machine classification principal component analysis image recognition medical simulation hidden markov models image analysis image databases;automatic classification;multiclass classification system	In this paper, a multi-class classification system is developed for medical images. We have mainly explored ways to use different image features, and compared two classifiers: principle component analysis (PCA) and supporting vector machines (SVM) with RBF (radial basis functions) kernels. Experimental results showed that SVM with a combination of the middle-level blob feature and low-level features (down-scaled images and their texture maps) achieved the highest recognition accuracy. Using the 9000 given training images from ImageCLEFOS, our proposed method has achieved a recognition rate of 88.9% in a simulation experiment. And according to the evaluation result from the ImageCLEFOS organizer, our method has achieved a recognition rate of 82% over its 1000 testing images	electronic organizer;high- and low-level;map;multiclass classification;principal component analysis;radial (radio);radial basis function;simulation;texture mapping	Bo Qiu;Chang Xu;Qi Tian	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262713	support vector machine;computer vision;computer science;machine learning;pattern recognition	Robotics	32.52042951052776	-63.504480460401055	81335
2094dd3c48b4d5e4145698972973777a811cb8c0	on-line signature verification by exploiting inter-feature dependencies	discriminating index;handwriting recognition;interfeature dependencies;signature verification;composite vector structure;handwriting recognition feature extraction;feature extraction;indexation;online signature verification;higher dimensional vector approach;handwriting recognition shape forgery computer science acceleration real time systems credit cards protection euclidean distance testing;discriminating index online signature verification interfeature dependencies composite vector structure higher dimensional vector approach	The traditional on-line signature verification process involves use of various dynamic features such as velocity, pressure, acceleration, angles, etc. The idea is to device a composite vector structure combining more than one feature where each feature is treated independently. Our proposed research work is an attempt to exploit the inter-feature dependencies by employing a higher dimensional vector approach. The strategy adopted here is to obtain pressure strokes with respect to various velocity bands. The strokes thus obtained are found to portray a reasonably accurate basis for discriminating genuine vs forgery class. The simulation results validate our assumptions and show improvements in the discriminating index	online and offline;simulation;velocity (software development)	M. Khalid Khan;M. Aurangzeb Khan;Mohammad A. U. Khan;Imran Ahmad	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.903	speech recognition;feature extraction;computer science;machine learning;pattern recognition;handwriting recognition	Robotics	35.05990485880266	-61.6430882878336	81628
7a9dae6d0585fb4c5b926e801ea0033ae1efbf57	comparison of local feature descriptors for mobile visual search	databases;histograms;image coding;receiver operator characteristic;mobile visual search;image matching;local feature descriptors;digital signatures;compressed histogram of gradients descriptor;image matching local feature descriptors mobile visual search mpeg 7 image signatures compressed histogram of gradients descriptor scale invariant feature transform chog sift;transform coding;indexing terms;computer vision;accuracy;visualization;mpeg 7 image signatures;sift;scale invariant feature transform;mpeg 7 image signature;local features;feature extraction;visual search;mobile communication;image signature mpeg 7 image signature feature descriptor mobile visual search;mobile handsets;chog;feature descriptor;transform coding histograms visualization accuracy mobile communication computer vision databases;mobile computing;image signature;mobile handsets digital signatures feature extraction image coding image matching mobile computing	We evaluate the performance of MPEG-7 image signatures, Compressed Histogram of Gradients descriptor (CHoG) and Scale Invariant Feature Transform (SIFT) descriptors for mobile visual search applications. We observe that SIFT and CHoG outperform MPEG-7 image signatures greatly in terms of feature-level Receiver Operating Characteristic (ROC) performance and image-level matching. Moreover, CHoG descriptors demonstrate such gains while being comparable with MPEG-7 image signatures in bit-rate.	image gradient;mpeg-7;receiver operating characteristic;type signature	Vijay Chandrasekhar;David M. Chen;Andy Lin;Gabriel Takacs;Sam S. Tsai;Ngai-Man Cheung;Yuriy A. Reznik;Radek Grzeszczuk;Bernd Girod	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5649937	computer vision;computer science;machine learning;pattern recognition;scale-invariant feature transform;mathematics;mobile computing;statistics	Vision	37.674316084928094	-60.3428238190615	81680
32bab8fe6db08c9d1e906be8a9c7e8cf7a0f0b99	audio-visual recognition system with intra-modal fusion	stress;finite element methods;stress concentration;iterative algorithms;marine vehicles stress finite element methods computational intelligence security iterative algorithms boundary conditions capacitive sensors sparse matrices;boundary conditions;computational intelligence;finite element mesh;marine vehicles;security;sparse matrices;capacitive sensors;global analysis	In this paper, a new multimodal biometric recognition system based on feature fusion is proposed to increase the robustness and circumvention of conventional multimodal recognition system. The feature sets originating from the output of the visual and audio feature extraction systems are fused and being classified by RBF neural network. Other than that, 2DPCA is proposed to work in conjunction with LDA to further increase the recognition performance of the visual recognition system. The experimental result shows that the proposed system achieves a higher recognition rate as compared to the conventional multimodal recognition system. Besides, we also show that the 2DPCA+LDA achieves a higher recognition rate as compared with PCA, PCA+LDA and 2DPCA.	artificial neural network;biometrics;feature extraction;multimodal interaction;principal component analysis;radial basis function	Yee Wan Wong;Kah Phooi Seng;Li-Minn Ang;Wan Yong Khor;Fui Liau	2007	2007 International Conference on Computational Intelligence and Security (CIS 2007)	10.1109/CIS.2007.196	stress concentration;sparse matrix;boundary value problem;computer science;information security;computational intelligence;finite element method;capacitive sensing;global analysis;stress;computer security	Robotics	29.47560344882456	-58.65729747534123	81895
30ce7f07e950bdd5311361eb84f76ea47a9e7fac	minutiae-based matching state model for combinations in fingerprint matching system	authorisation;image matching;multilayer perceptrons;biometric modalities minutiae based matching state model fingerprint matching system multisample matching results fingerprint based authentication test templates enrolled template fvc2002 fingerprint databases likelihood ratio multilayer perceptron biometric systems;face time frequency analysis biological system modeling multilayer perceptrons biometrics access control conferences computational modeling;conference paper;multilayer perceptrons authorisation fingerprint identification image matching;fingerprint identification	In this paper we investigate the question of combining multi-sample matching results obtained during repeated attempts of fingerprint based authentication. In order to utilize the information corresponding to multiple input templates in a most efficient way, we propose a minutiae-based matching state model which uses relationship between test templates and enrolled template. The principle of this algorithm is that matching parameters, i.e the sets of matched minutiae, between these templates should be consistent in genuine matchings. Experiments are performed on FVC2002 fingerprint databases. Result shows that the system utilizing the proposed matching state model is able to outperform the original system with raw matching scores. Likelihood ratio and multilayer perceptron are used as combination methods.	algorithm;authentication;database;fingerprint;matched filter;matching (graph theory);minutiae;multilayer perceptron;raw image format;xfig	Xi Cheng;Sergey Tulyakov;Venu Govindaraju	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2013.21	fingerprint;computer science;machine learning;pattern recognition;data mining;database;authorization	Vision	30.09442306501873	-64.0354362999428	81957
e932bc8a51b7a5cf3de620d23297b1f47e04447b	automatic filter design for texture discrimination	optimal filtering;texture segmentation;neural network classifier;image texture;filter design;gabor filter;neural networks gabor filters filter bank multi layer neural network feedforward neural networks filtering image texture analysis feedforward systems computer science humans;network configuration;classification error;classification accuracy;classification accuracy automatic filter design texture discrimination multichannel filtering texture segmentation optimal filters node pruning classification error neural network classifier natural textures;neural network	Multichannel filtering has been shown by many researchers to provide good features for texture segmentation and classification. In this paper, we exploit neural networks to construct optimal filters and to combine the outputs of these filters for the classification of known textures. We use the neural network training together with node pruning, so that both the classification error and the number of filters or, equivalently, the number of features, are minimized. The performance of the neural network classifier is demonstrated on several experiments involving classification of natural textures. We study the effects of using different sized filters with different network configurations. We show that the number of filters, and, therefore, the processing time, can be greatly reduced while preserving the classification accuracy, using the proposed scheme compared to using a general set of filters (e.g., Gabor filters).	artificial neural network;audio filter;experiment;filter design;gabor filter	Anil K. Jain;Kalle Karu	1994		10.1109/ICPR.1994.576324	image texture;computer vision;computer science;machine learning;pattern recognition;time delay neural network;filter design;texture filtering;artificial neural network	ML	25.64205786953863	-61.9200119404069	82088
4dd17dbd84909a17a30a56402d3a2dabc309cd54	ocular surface vasculature recognition using curvelet transform	conjunctival layers;osv recognition;match score level fusion;multiple gaze directions;multidistance dataset;feature mapping;episcleral layers;biometric recognition;osv texture;curvelet transform;similarity metrics;publicly available ubiris v1 dataset;equal error rate;osv feature extraction;eer;close proximity;nonlinear feature enhancement;commercial rgb cameras;linear discriminant analysis;ocular surface vasculature recognition;cellular phones;dslr;vascular patterns	The vascular patterns seen on the white of the eye, mainly in conjunctival and episcleral layers, are termed as ocular surface vasculature (OSV). OSV is visible in images captured with commercial RGB cameras, and its unique texture can be used for biometric recognition. This study demonstrates the capabilities of curvelet transform for OSV feature extraction. Nonlinear feature enhancement and feature mapping in curvelet domain are shown to be effective in differentiating OSV texture. Linear discriminant analysis and similarity metrics are used for matching. A match-score level fusion is used across multiple gaze directions for both eyes. Using a multi-distance dataset of 50 volunteers, where eye images were acquired from 30, 150, and 250 cm using a dSLR, a best equal error rate (EER) of 0.2% is obtained. Using a second dataset of 40 volunteers acquired from 150 cm using a dSLR, a best EER of 3.1% is obtained. For a 216-participant dataset of ocular images acquired using cellular phones from close proximity, an EER of 0.9% is obtained. The proposed methodology was also tested on the publicly available UBIRIS V1 dataset, yielding an EER of 0.7%. The experimental results support the theoretically formulated advantages of the curvelet transform and its capability in successful extraction of curved structures when applied to OSV patterns.	biometrics;curvelet;enhanced entity–relationship model;feature extraction;linear discriminant analysis;mobile phone;osv	Sriram Pavan K. Tankasala;Plamen Doynov;Simona Crihalmeanu;Reza Derakhshani	2017	IET Biometrics	10.1049/iet-bmt.2015.0091	computer vision;speech recognition;computer science;machine learning;pattern recognition;linear discriminant analysis	Vision	33.017260250392006	-61.459171428031624	82100
ce2945e369603fcec1fcdc6e19aac5996325cba9	emotion recognition using phog and lpq features	databases;local model;pattern clustering;keywords cluster centers;local phase;automatic emotion recognition;face shape feature extraction image sequences support vector machines databases accuracy;lpq features;support vector machines;sspnet gemep fera dataset;appearance features extraction;pyramid of histogram of gradient extraction;image classification;shape vectors;emotion recognition;largest margin nearest neighbour phog lpq features automatic emotion recognition pyramid of histogram of gradient extraction local phase quantisation features k means clustering constraint local model face tracking image sequences shape vectors appearance features extraction sspnet gemep fera dataset emotion classification support vector machine;phog;face tracking;conference paper;accuracy;face recognition;support vector machines emotion recognition feature extraction image classification image sequences pattern clustering;shape;emotion classification;person independent;feature extraction;quantisation;image sequence;feature;nearest neighbour;face;key frames;support vector machine;largest margin nearest neighbour;data sets;local phase quantisation features;k means clustering;constraint local model;image sequences	We propose a method for automatic emotion recognition as part of the FERA 2011 competition. The system extracts pyramid of histogram of gradients (PHOG) and local phase quantisation (LPQ) features for encoding the shape and appearance information. For selecting the key frames, K-means clustering is applied to the normalised shape vectors derived from constraint local model (CLM) based face tracking on the image sequences. Shape vectors closest to the cluster centers are then used to extract the shape and appearance features. We demonstrate the results on the SSPNET GEMEP-FERA dataset. It comprises of both person specific and person independent partitions. For emotion classification we use support vector machine (SVM) and largest margin nearest neighbour (LMNN) and compare our results to the pre-computed FERA 2011 emotion challenge baseline.	baseline (configuration management);channel length modulation;cluster analysis;database;disk partitioning;emotion recognition;facial motion capture;feature selection;gradient;k-means clustering;key frame;large margin nearest neighbor;mutual information;precomputation;quantization (physics);support vector machine	Abhinav Dhall;Akshay Asthana;Roland Goecke;Tamás D. Gedeon	2011	Face and Gesture 2011	10.1109/FG.2011.5771366	computer vision;machine learning;pattern recognition;mathematics	Vision	36.538845030208044	-56.39328301210126	82334
8dd9c97b85e883c16e5b1ec260f9cd610df52dec	rule based assessment of hearing-impaired children's facial expressions		In this study, the facial data of the adults and the hearing-impaired children have been analysed and compared by rule-based facial expression recognition methods. 68 face points have been selected for calculation. Then using these points, the angle between the upper and down lips, the length of the corner of the lip, the angle between the eyebrows and the nose, and the angle of the aperture of the eye have been calculated. In these children, the positive facial expression can be comprehended by the lip's data and for the negative facial expression it has been determined that eyebrows, eyes and the lips should be included in the evaluation, as well. In addition to that, it has been stated that, a threshold value for these face features can not be determined within a rule based system but they can be used to determine the transitions of facial expression between different images of the same child.	logic programming;rule-based system	Turgut Can Aydinalev;Beste Burcu Bayhan;Hatice Kose	2018	2018 26th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2018.8404159	artificial intelligence;computer vision;rule-based system;computer science;pattern recognition;facial recognition system;aperture;facial expression	Vision	29.405093979351374	-59.77064376077446	82392
e446be4fd07bf12c8c4acfe9358bde792ad3a7f9	writer identification and writer retrieval using the fisher vector on visual vocabularies	pattern clustering;document analysis;gaussian processes;vocabulary;vocabulary document image processing feature extraction gaussian processes handwritten character recognition image retrieval pattern clustering visual databases;writer identification;feature extraction;document image processing;writer retrieval;document analysis writer identification writer retrieval;cvl database document writer retrieval document writer identification local features visual vocabulary feature clustering gaussian mixture model fisher kernel document image fisher vector handwriting similarity measurement icdar 2011 writer identification contest dataset;handwritten character recognition;databases vectors writing vocabulary kernel image segmentation training;visual databases;image retrieval	In this paper a method for writer identification and writer retrieval is presented. Writer identification is the task of identifying the writer of a document out of a database of known writers. In contrast to identification, writer retrieval is the task of finding documents in a database according to the similarity of handwritings. The approach presented in this paper uses local features for this task. First a vocabulary is calculated by clustering features using a Gaussian Mixture Model and applying the Fisher kernel. For each document image the features are calculated and the Fisher Vector is generated using the vocabulary. The distance of this vector is then used as similarity measurement for the handwriting and can be used for writer identification and writer retrieval. The proposed method is evaluated on two datasets, namely the ICDAR 2011 Writer Identification Contest dataset which consists of 208 documents from 26 writers, and the CVL Database which contains 1539 documents from 309 writers. Experiments show that the proposed methods performs slightly better than previously presented writer identification approaches.	cluster analysis;cosine similarity;database;experiment;fisher information;fisher kernel;google map maker;international conference on document analysis and recognition;mixture model;scale-invariant feature transform;vocabulary	Stefan Fiel;Robert Sablatnig	2013	2013 12th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2013.114	natural language processing;speech recognition;feature extraction;image retrieval;computer science;machine learning;pattern recognition;gaussian process	Vision	34.032632919578276	-64.57577813244873	82465
a0e898fac02cf15d6c5c1153a1d8c3508f871356	iris recognition using vector quantization	databases;kfcg;authentication mechanisms;lbg iris recognition vector quantization terrorist attacks infallible security systems emerging security authentication mechanisms kekre proportionate error algorithm kpe kekre fast codebook generation algorithm kfcg;generic algorithm;authorisation;biometrics;iris recognition;iris recognition vector quantization feature extraction security biometrics image edge detection terrorism clustering algorithms image recognition signal processing;accuracy;vector quantization;terrorist attacks;feature extraction;secure system;clustering algorithms;kpe;vector quantizer;kekre fast codebook generation algorithm;kekre proportionate error algorithm;emerging security;lbg;iris;vector quantisation;vector quantisation authorisation iris recognition security terrorism;security;infallible security systems;terrorism;kfcg biometrics iris recognition vector quantization lbg kpe	In today’s world, where terrorist attacks are on the rise, employment of infallible security systems is a must. Iris recognition enjoys universality, high degree of uniqueness and moderate user co-operation. This makes Iris recognition systems unavoidable in emerging security & authentication mechanisms. We propose an iris recognition system based on vector quantization. The proposed system does not need any pre-processing and segmentation of the iris. We have tested LBG, Kekre’s Proportionate Error Algorithm (KPE) & Kekre’s Fast Codebook Generation Algorithm (KFCG) for the clustering purpose. From the results it is observed that KFCG requires 99.79% less computations as that of LBG and KPE. Further the KFCG method gives best performance with the accuracy of 89.10% outperforming LBG that gives accuracy around 81.25%. Performance of individual methods is evaluated and presented in this paper.	algorithm;approximation algorithm;authentication;cluster analysis;codebook;computation;fast fourier transform;image segmentation;iris recognition;location-based game;preprocessor;universality probability;vector quantization	H. B. Kekre;Tanuja K. Sarode;V. A. Bharadi;A. A. Agrawal;R. J. Arora;M. C. Nair	2010	2010 International Conference on Signal Acquisition and Processing	10.1109/ICSAP.2010.45	speech recognition;genetic algorithm;feature extraction;computer science;information security;theoretical computer science;machine learning;iris recognition;accuracy and precision;authorization;cluster analysis;terrorism;computer security;vector quantization;biometrics	Robotics	33.69137933094463	-62.84913314943623	82515
861423e119acaa853d3b74d07621069cdde0362f	vision-based parking-slot detection: a benchmark and a learning-based approach		Recent years have witnessed a growing interest in developing automatic parking systems in the field of intelligent vehicle. However, how to effectively and efficiently locating parking-slots using a vision-based system is still an unresolved issue. In this paper, we attempt to fill this research gap to some extent and our contributions are twofold. Firstly, to facilitate the study of vision-based parking-slot detection, a large-scale parking-slot image database is established. For each image in this database, the marking-points and parking-slots are carefully labelled. Such a database can serve as a benchmark to design and validate parking-slot detection algorithms. Secondly, a learning based parking-slot detection approach is proposed. With this approach, given a test image, the marking-points will be detected at first and then the valid parking-slots can be inferred. Its efficacy and efficiency have been corroborated on our database. The labeled database and the source codes are publicly available at http://sse.tongji.edu.cn/linzhang/ps/index.htm.	benchmark (computing)	Lin Zhang;Xiyuan Li;Junhao Huang;Ying Shen;Dongqing Wang	2018	Symmetry	10.3390/sym10030064	computer vision;visualization;computer science;feature extraction;source code;machine learning;detector;standard test image;benchmark (computing);data mining;artificial intelligence	Vision	31.39976756704821	-52.34952971354479	82686
3ab999cf1216dd312713d2a8999fa17464f689af	weight-based facial expression recognition from near-infrared video sequences	facial expression recognition;region based weights;local binary pattern;near infrared;dynamic texture;facial features;support vector machine;information need;facial expression;illumination invariance;visible light	This paper presents a novel weight-based approach to recognize facial expressions from the near-infrared (NIR) video sequences. Facial expressions can be thought of as specific dynamic textures where local appearance and motion information need to be considered. The face image is divided into several regions from which local binary patterns from three orthogonal planes (LBP-TOP) features are extracted to be used as a facial feature descriptor. The use of LBP-TOP features enables us to set different weights for each of the three planes (appearance, horizontal motion and vertical motion) inside the block volume. The performance of the proposed method is tested in the novel NIR facial expression database. Assigning different weights to the planes according to their contribution improves the performance. NIR images are shown to deal with illumination variations comparing with visible light images.		Matti Taini;Guoying Zhao;Matti Pietikäinen	2009		10.1007/978-3-642-02230-2_25	near-infrared spectroscopy;information needs;support vector machine;computer vision;local binary patterns;computer science;machine learning;pattern recognition;visible spectrum;facial expression;face hallucination	Vision	35.44171725711171	-58.136957531116636	82747
b2b1c486098d204f4af8a9cf4dfde2b09c27f94c	on clustering human gait patterns	pattern clustering feature extraction gait analysis image matching image representation;similarity assessment human gait pattern clustering automated human gait recognition robust feature representation algorithm robust feature matching algorithms feature extraction k means based clustering approach identity clustering physical attributes;pattern matching clustering algorithms feature extraction vectors correlation gait recognition histograms	"""Research in automated human gait recognition has largely focused on developing robust feature representation and matching algorithms. In this paper, we investigate the possibility of clustering gait patterns based on the features extracted by automated gait matchers. In this regard, a k-means based clustering approach is used to categorize the feature sets extracted by three different gait matchers. Experiments are conducted in order to determine if (a) the clusters of identities corresponding to the three matchers are similar, and (b) if there is a correlation between gait patterns within each cluster and physical attributes such as gender, body area, height, stride, and cadence. Results demonstrate that human gait patterns can be clustered, where each cluster is defined by identities sharing similar physical attributes. In particular, body area and gender are found to be the primary attributes captured by gait matchers to assess similarity between gait patterns. However, the strength of the correlation between clusters and physical attributes is different across the three matchers, suggesting that gait matchers """"weight"""" attributes differently. The results of this study should be of interest to gait recognition and identification-at-a-distance researchers."""	algorithm;biometrics;categorization;cluster analysis;emoticon;experiment;gait analysis;k-means clustering;pattern matching	Brian DeCann;Arun Ross;Mark Vere Culp	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.315	computer vision;machine learning;pattern recognition	Vision	35.2286280434241	-56.499637438126456	82774
7bd3484c11cafef57686304f84892d87d526611a	a morphology method for determining the number of clusters present in spectral co-clustering documents and words	gray-scale matrix;vat image;spectral co-clustering document;morphology method;spectral clustering algorithm;input matrix;co-clustering document;filtered image;clustering document;image representation	A new algorithm for clustering documents and words simultaneously has recently been presented. As most spectral clustering algorithms, the prior knowledge of the number of clusters present is required. In this paper, we explore a method based on morphology for determining the number of clusters present in the given dataset for co-clustering documents and words. The proposed method employs some refined feature extraction techniques, which mainly include a VAT (Visual Assessment of Cluster Tendency) image representation of input matrix generated by spectral co-clustering documents and words, and the texture information obtained by filtering the VAT image. The number of clusters present in co-clustering documents and words is finally reported by computing the eigengap of gray-scale matrix of filtered image. Our experimental results show that the proposed method works well in practice.	biclustering;cluster analysis;mathematical morphology	Na Liu;Mingyu Lu	2010		10.1007/978-3-642-24983-9_13	theoretical computer science;data mining;mathematics;information retrieval	ML	36.66468717240304	-65.01610267521548	82786
b075a0fa80c45cd0462092ec17ac62e20467786e	a modified modular eigenspace approach to face recognition	eigenvalues and eigenfunctions;mouth;face recognition image recognition karhunen loeve transforms eyes nose magnetic heads surveillance image reconstruction principal component analysis identity based encryption;image recognition;identity based encryption;magnetic heads;surveillance;image classification;transforms eigenvalues and eigenfunctions face recognition image classification image retrieval image representation parameter estimation;limit set;eyes;karhunen loeve transforms;face recognition;image representation;image reconstruction;principal component analysis;transforms;rotation transform;similarity criteria;modified modular eigenspace;parameter estimation;sub images;rotation transform modified modular eigenspace face recognition image representation parameter set similarity criteria image classification image retrieval sub images eyes nose mouth head postures;parameter set;nose;head postures;image retrieval	In this paper, we describe a method for face recognition based on the eigenimage technique that allows images to be represented by a limited set of parameters and be compared according to simple similarity criteria for classification or retrieval purposes. Our method is not applied to the whole image of the face, but to sub-images representing the most salient face components (eyes, nose, mouth). The method is able to recognise with good precision faces having different head postures on the image plane, because faces are straightened through a rotation transform. Moreover, the paper reports results we have achieved with such a method on two publicly-available reference sets of images.	facial recognition system;image plane;principal component analysis	Stefano Cagnoni;Agostino Poggi;Gian Luca Porcari	1999		10.1109/ICIAP.1999.797643	iterative reconstruction;limit set;computer vision;contextual image classification;face detection;speech recognition;image retrieval;computer science;pattern recognition;three-dimensional face recognition;mathematics;estimation theory;principal component analysis	Vision	35.02099726344809	-60.77452704890319	83108
5a8971185c51b048c3625092c4a80c58d367c7a0	city scale image geolocalization via dense scene alignment	image retrieval computer vision feature extraction image matching;digital signal processing;prediction algorithms;geology digital signal processing cities and towns visualization prediction algorithms robustness image color analysis;visualization;geology;image color analysis;geo tagged images city scale image geolocalization dense scene alignment computer vision algorithm data driven approach scene matching method coarse to fine matching strategy scene retrieval global features san francisco;cities and towns;robustness	Predicting where a photo was taken is quite important and yet a challenging task for computer vision algorithms. Our motivation is to solve this difficult problem in a city scale setting by employing a data-driven approach. In order to pursue this goal, we developed a fast and robust scene matching method that follows a coarse-to-fine strategy. In particular, we combine scene retrieval via global features and dense scene alignment and use a large set of geo-tagged images of downtown San Francisco in our evaluation. The experimental results show that the proposed approach, despite its simplicity, is surprisingly effective and achieves comparable results with the state-of-the-art.	algorithm;computer vision;scene graph;visual descriptor	Semih Yagcioglu;Erkut Erdem;Aykut Erdem	2015	2015 IEEE Winter Conference on Applications of Computer Vision	10.1109/WACV.2015.102	computer vision;simulation;image-based modeling and rendering;visualization;prediction;computer science;digital signal processing;machine learning;robustness;computer graphics (images)	Vision	36.67911908168777	-54.41721700011739	83263
a714fab1934a066baeacb1652bc3ab3636b8938d	hrrp recognition in radar sensor network	radar sensor network;target recognition;high-resolution range profile;the minimum resistor-average distance;the minimum kullback–leibler distance	In this paper, several high-resolution range profile (HRRP) recognition approaches in radar sensor network (RSN) are investigated. First, we study HRRP target recognition in a radar. A decision rule based on the minimum resistor-average (MRA) distance criterion is established for HRRP sequence recognition. Simulation results show that comparing with the maximum correlation coefficient-template matching method (MCC-TMM) and the minimum Kullback–Leibler (MKL) distance criterion, the proposed MRA distance criterion can provide higher recognition ratio for different flight targets. Then we extend the usage the MKL distance criterion and MRA distance criterion to HRRP target recognition in RSNs. Simulation results exhibit that the proposed MRA distance criterion is also superior to MCC-TMM and MKL, and HRRP recognition performance can be improved by RSNs.		Chengchen Mao;Jing Yi Liang	2017	Ad Hoc Networks	10.1016/j.adhoc.2016.09.001	computer vision;machine learning;pattern recognition	Mobile	27.316353146078136	-61.301272610794946	83364
f7a99e3807ec5f6167177fb1022202126e054220	a new text detection algorithm for content-oriented line drawing image retrieval	line drawings;large scale;statistical analysis;content oriented;text detection;hough transform;image retrieval	Content retrieval of scanned line drawing images is a difficult problem, especially from real-life large scale databases. Existing algorithms don’t work well due to their low efficiency by first recognizing various types of graphical primitives and then content-oriented texts. A new method for directly detecting texts from line drawing images is proposed in this paper. We first decompose a drawing image into a set of Local Consecutive Segments (LCSs). A LCS is defined as a minimum meaningful structural unit to imitate a stroke during human-drawing process. Next, we identify candidate character LCSs by statistical analysis and merge them into character LCS blocks by geometrical analysis. Finally, Hough transforms are applied to calculate the orientations of character LCS blocks and generate candidate strings. Experimental results show that our algorithm can well detect strings in any orientation. Our method is robust to text-graphic touching, scanning degradation and drawing noises, providing an efficient approach for content retrieval of document images.	database;elegant degradation;graphical user interface;hough transform;image retrieval;line drawing algorithm;real life;robustness (computer science);sensor	Zhenyu Zhang;Tong Lu;Feng Su;Ruoyu Yang	2010		10.1007/978-3-642-15702-8_31	hough transform;computer vision;image retrieval;computer science;machine learning;statistics;computer graphics (images)	AI	37.62104223165475	-63.91804644658194	83464
701d1a2501eac11054225b79fc5cf13fa4745785	face recognition using multiple interest point detectors and sift descriptors	databases;detectors;scale invariant feature transform descriptors;laplace transforms face recognition gaussian processes;face database face recognition scale invariant feature transform descriptors scale invariant interest point detectors harris laplace detector difference of gaussians detector corner like structures blob like structures model based algorithm at t database;distance measure;gaussian processes;interest points;face database;difference of gaussians detector;at t database;corner like structures;harris laplace detector;distance measurement;face recognition;face recognition face detection detectors principal component analysis object recognition image databases independent component analysis linear discriminant analysis authentication proposals;blob like structures;laplace transforms;scale invariant interest point detectors;feature extraction;principal component analysis;model based algorithm;face;scale invariance	The use of interest point detectors and SIFT descriptors for face recognition is studied in this paper. There are two main novelties with respect to previous approaches using SIFT features. First, the use of two scale-invariant interest point detectors (namely, Harris-Laplace and difference of Gaussians) which are combined in order to detect both corner-like structures and blob-like structures in face images. Second, the distance measure used, which takes into account both the number of matching points found between two images (according to their SIFT descriptors) and the coherence of these matches in terms of scales, orientations and spacial configuration. The results obtained with our model-based algorithm are compared with those of a classic appearance-based face recognition method (PCA) over two different face databases: the well-known AT&T database and a face database created at our university.	algorithm;authentication;coherence (physics);database;difference of gaussians;facial recognition system;harris affine region detector;independent computing architecture;matlab;mathematical optimization;pattern matching;program optimization;scale-invariant feature transform;sensor;similarity measure;whole earth 'lectronic link	Cristina Fernández;Maria Asunción Vicente	2008	2008 8th IEEE International Conference on Automatic Face & Gesture Recognition	10.1109/AFGR.2008.4813313	computer vision;speech recognition;pattern recognition;mathematics	Vision	36.13361992474502	-58.31287658238734	83513
a7ec294373ccc0598cbb0bbb6340c4e56fe5d979	face recognition with relative difference space and svm	support vector machines;yale face database b;yale face database b face recognition relative difference space support vector machine multiclass recognition binary class problem binary classification;image classification;relative difference space;face recognition;support vector machines face recognition image classification;robust performance;binary class problem;binary classification;support vector machine;face recognition support vector machines support vector machine classification helium databases robustness lighting biometrics national security laboratories;multiclass recognition	In this paper, a new method based on relative difference space (RDS) and support vector machine (SVM) is proposed for multi-class recognition. First the RDS transformation converts the multi-class problem to a binary-class problem, and then SVM is used for the binary classification directly. Compared with the traditional method of difference space (DS), RDS is reversible and it overcomes the ill-transformation problem. This method is applied to face recognition in Yale Face database B, and the recognition result demonstrates its robust performance under different illumination conditions	algorithm;binary classification;facial recognition system;illumination (image);point location;reference implementation;relative change and difference;reversible computing;support vector machine	Xiaoguang He;Jie Tian;Yuliang He;Xin Yang	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.532	facial recognition system;support vector machine;speech recognition;computer science;machine learning;pattern recognition;relevance vector machine;structured support vector machine	Vision	33.49820318157385	-58.77082390368579	83577
baf584607c7a3806c7e7692304ba7ced578aad65	genetic eigenhand selection for handshape classification based on compact hand extraction	singular value decomposition based image enhancement;compact hand extraction;issn 0952 1976;hierarchical pyramid sampling;engineering applications of artificial intelligence;lighting compensation	This study proposes compact hand extraction to assist in computerized handshape recognition. First, we devised an image enhancement technique based on singular value decomposition to remove dark backgrounds by reserving the skin color pixels of a hand image. Then, the polynomial approximation YCbCr color model was used to extract the hand. After alignment, we applied lighting compensation to the adaptable singular value decomposition. Finally, a hierarchical pyramid sampling algorithmwas used to reduce the impact of variations in handshape. We also constructed a self-eigenhand recognizer with genetic algorithms (GA) for selecting discriminant eigenvector subsets for classification. Although our approach maximizes the differences in hand images for various handshapes, it also minimizes variations in lighting and pose for the same handshape. Experimental results for images from our database and a live sequence showed that our method functioned more efficiently than conventional ones that do not use compact hand extraction against complex scenes. For the 768 images included in inside testing, our classification system achieved an AAR of 99.55% and an FAR of 0.0001%. For live testing, the classification system achieved an accuracy rate of 91.7%, with an error rate of 8.3%. Regarding speed, our system was faster than conventional ones. Our images size was 160 120 pixels, operating at an average processing time of less than 1 s per handshape (using an AMD64 Athlon CPU 2.0 GHz personal computer). & 2013 Elsevier Ltd. All rights reserved.	approximation;association for automated reasoning;athlon;biometrics;central processing unit;database;discriminant;dynamic range;face detection;finite-state machine;genetic algorithm;image editing;personal computer;pixel;polynomial;real-time clock;robot;sampling (signal processing);simulation;singular value decomposition;software release life cycle;x86-64	Jing-Wein Wang;Chou-Chen Wang;Jiann-Shu Lee	2013	Eng. Appl. of AI	10.1016/j.engappai.2013.06.019	computer vision;simulation;speech recognition;artificial intelligence;machine learning	AI	34.498131637729685	-63.17704140276111	83601
8463169793f30a55058614aa122b926c05057ab7	handwritten digit recognition: benchmarking of state-of-the-art techniques	binary image;image database;handwritten digit recognition;discriminant function;support vector classifier;feature vector;the state of the art;discrimination learning;feature extraction;pattern classification;discriminative learning;k nearest neighbor;learning vector quantization	This paper presents the results of handwritten digit recognition on well-known image databases using state-of-the-art feature extraction and classi cation techniques. The tested databases are CENPARMI, CEDAR, and MNIST. On the test data set of each database, 80 recognition accuracies are given by combining eight classi ers with ten feature vectors. The features include chaincode feature, gradient feature, pro le structure feature, and peripheral direction contributivity. The gradient feature is extracted from either binary image or gray-scale image. The classi ers include the k-nearest neighbor classi er, three neural classi ers, a learning vector quantization classi er, a discriminative learning quadratic discriminant function (DLQDF) classi er, and two support vector classi ers (SVCs). All the classi ers and feature vectors give high recognition accuracies. Relatively, the chaincode feature and the gradient feature show advantage over other features, and the pro le structure feature shows e ciency as a complementary feature. The SVC with RBF kernel (SVC-rbf) gives the highest accuracy in most cases but is extremely expensive in storage and computation. Among the non-SV classi ers, the polynomial classi er and DLQDF give the highest accuracies. The results of non-SV classi ers are competitive to the best ones previously reported on the same databases. ? 2003 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	baseline (configuration management);binary image;computation;dspace;discriminant;discriminative model;experiment;feature extraction;feature vector;gradient;grayscale;hierarchical rbf;image resolution;k-nearest neighbors algorithm;learning vector quantization;mnist database;mesa;norm (social);optical character recognition;pattern recognition;peripheral;polynomial;radial basis function kernel;scalable video coding;systemverilog;test data	Cheng-Lin Liu;Kazuki Nakashima;Hiroshi Sako;Hiromichi Fujisawa	2003	Pattern Recognition	10.1016/S0031-3203(03)00085-2	random subspace method;feature learning;speech recognition;feature vector;learning vector quantization;feature;binary image;feature extraction;computer science;machine learning;linear classifier;pattern recognition;discriminant function analysis;k-nearest neighbors algorithm;feature;feature scaling;discrimination learning	ML	33.91963531952092	-59.442050409047845	83761
04cbd9edb0cde8ad701bbec606afde4bdd4b6c98	evaluating superpixels in video: metrics beyond figure-ground segmentation		There exist almost as many superpixel segmentation algorithms as applications they can be used for. Figure 1 shows two example superpixel segmentations. So far, the choice of the right superpixel algorithm for the task at hand is based on their ability to resemble human-made ground truth segmentations (besides runtime and availability). We investigate the equally important question of how stable the segmentations are under image changes as they appear in video data (stability-criteria). Further we propose a new quality measure that evaluates how well the segmentation algorithms cover relevant image boundaries (discontinuitycriteria). Instead of relying on human-made annotations, that may be biased by semantic knowledge, we present a completely data-driven measure that inherently emphasizes the importance of image boundaries. In detail, we exploit ground truth optical flow data provided by two recently published datasets for evaluation of optical flow algorithms (KITTI [3] and Sintel[2]) to evaluate the stabilityand discontinuity-citeria related to questions a) and b) in Figure 1. Both criteria are discussed, formalized and used to compare several existing superpixel algorithms with available open source implementations. For further evaluation of other algorithms, we provide the results, a Matlab implementation of the metrics and functions to interface the datasets on our website.1 1 A Metric for the Stability-Criteria	algorithm;emoticon;ground truth;image segmentation;matlab;open-source software;optical flow;reflections of signals on conducting lines	Peer Neubert;Peter Protzel	2013		10.5244/C.27.54	computer vision;scale-space segmentation	Vision	31.35938183254885	-53.097811994309794	83778
529bf843f287a8e78621742ae93bceb1648530c8	character recognition based on pcanet	image recognition;handwriting recognition;convolution;training;feature extraction;principal component analysis;character recognition	The character recognition is an important issue, which has been pursued in recent year. In the paper, we used PCANet(principal component analysis network) to learn the character features. We verified the influence of these parameters on the performance of PCANet by modifying the key parameters of the experiment. Then we made a handwritten dataset to do the experiment and to verify whether the PCANet can also be used to identify. And the results not only was okey but also the recognition rate can reach 95.56%. At last, we compared the experimental results with the experimental results on Caffe.	optical character recognition;principal component analysis	Renjun Liu;Tongwei Lu	2016	2016 15th International Symposium on Parallel and Distributed Computing (ISPDC)	10.1109/ISPDC.2016.60	speech recognition;feature extraction;intelligent character recognition;computer science;machine learning;pattern recognition;handwriting recognition;convolution;principal component analysis	AI	31.882333986127435	-58.380246528575405	83784
4a7f03413ea3621af0febd59a3391cff7d3fa57e	fast saliency-aware multi-modality image fusion	markov random fields mrfs;saliency detection;multi modality fusion;co occurrence	This paper proposes a saliency-aware fusion algorithm for integrating infrared (IR) and visible light (ViS) images (or videos) with the aim to enhance the visualization of the latter. Our algorithm involves saliency detection followed by a biased fusion. The goal of the saliency detection is to generate a saliency map for the IR image, highlighting the co-occurrence of high brightness values (‘‘hot spots’’) subsequent fusion step is employed to bias the end result in favor of the ViS image, except when a region shows clear IR saliency, in which case the IR image gains (local) dominance. By doing so, the fused image succeeds in depicting both the salient foreground object (gleaned from the IR image), against as an easily recognizable background as supplied by the ViS image. An evaluation of the proposed saliency detection method indicates improvements in detection accuracy when compared to state-of-the-art alternatives. Moreover, both objective and subjective assessments reveal the effectiveness of the proposed fusion algorithm in terms of visual context enhancement. & 2013 Elsevier B.V. All rights reserved.	algorithm;image fusion;markov random field;modality (human–computer interaction)	Jungong Han;Eric J. Pauwels;Paul M. de Zeeuw	2013	Neurocomputing	10.1016/j.neucom.2012.12.015	computer vision;co-occurrence;computer science;machine learning;pattern recognition	Vision	38.16489197769432	-54.60803414604842	83928
1c32a9198ad940ccd55196d383d3f068c33248df	hog and pairwise svms for neuromuscular activity recognition using instantaneous hd-semg images		The concept of neuromuscular activity recognition using instantaneous high-density surface electromyography (HD-sEMG) image opens up new avenues for the development of more fluid and natural muscle-computer interfaces. The state-of-the-art methods for instantaneous HD-sEMG image recognition achieve prominent performance using a computationally intensive deep convolutional networks (ConvNet) classifier, while very low performance is reported using the conventional classifiers. However, the conventional classifiers such as Support Vector Machines (SVM) can surpass ConvNet at producing optimal classification if well-behaved feature vectors are provided. This paper studies the question of extracting distinctive feature sets, thus propose to use Histograms of Oriented Gradient (HOG) as unique features for robust neuromuscular activity recognition, adopting pair wise SVMs as the classification scheme. The experimental results proved that the HOG represents unique features inside the instantaneous HD-sEMG image and fine-tuning the hyper- parameter of the pair wise SVMs, the recognition accuracy comparable to the more complex state of the art methods can be achieved.		Md. Rabiul Islam;Daniel Massicotte;Francois Nougarou;Wei-Ping Zhu	2018	2018 16th IEEE International New Circuits and Systems Conference (NEWCAS)	10.1109/NEWCAS.2018.8585731	support vector machine;distinctive feature;electronic engineering;feature vector;feature extraction;computer science;classifier (linguistics);contextual image classification;activity recognition;histogram;pattern recognition;artificial intelligence	Vision	30.744154734574398	-55.11260103348548	83951
3bbdfa097a4c39012cb322b23051e360c2f7f023	learning race from face: a survey	databases;female;anthropometry;image categorization;face database;male;cultural differences face recognition computer vision computational modeling feature extraction psychology image classification image color analysis;psychology;continental population groups;image processing computer assisted;computer vision;data clustering;computational modeling;face recognition;machine learning;feature extraction;prejudicial factors computer vision face recognition learning artificial intelligence;race classification;face;humans;biometric identification;cross cutting theme social signals multidisciplinary research psychology neuroscience computer science computer vision computer graphics machine learning computational intelligence racial face analysis security and defense surveillance human computer interface hci biometric based identification racial category race classification race detection race categorization face race perception racial face processing feature representational model racial databases systematic discussion learning scenario	Faces convey a wealth of social signals, including race, expression, identity, age and gender, all of which have attracted increasing attention from multi-disciplinary research, such as psychology, neuroscience, computer science, to name a few. Gleaned from recent advances in computer vision, computer graphics, and machine learning, computational intelligence based racial face analysis has been particularly popular due to its significant potential and broader impacts in extensive real-world applications, such as security and defense, surveillance, human computer interface (HCI), biometric-based identification, among others. These studies raise an important question: How implicit, non-declarative racial category can be conceptually modeled and quantitatively inferred from the face? Nevertheless, race classification is challenging due to its ambiguity and complexity depending on context and criteria. To address this challenge, recently, significant efforts have been reported toward race detection and categorization in the community. This survey provides a comprehensive and critical review of the state-of-the-art advances in face-race perception, principles, algorithms, and applications. We first discuss race perception problem formulation and motivation, while highlighting the conceptual potentials of racial face processing. Next, taxonomy of feature representational models, algorithms, performance and racial databases are presented with systematic discussions within the unified learning scenario. Finally, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potentially important cross-cutting themes and research directions for the issue of learning race from face.	algorithm;biometrics;categorization;computational technique;computational intelligence;computer graphics;computer science;computer vision;database;face;human computer;human–computer interaction;inference;interface device component;machine learning;neuroscience discipline;numerous;race condition;representation (action);statistical classification;taxonomy (general);theme (computing);user-computer interface	Si-Yao Fu;Haibo He;Zeng-Guang Hou	2014	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2014.2321570	face;computer vision;feature extraction;computer science;artificial intelligence;machine learning;anthropometry;cluster analysis;computational model;biometrics	Vision	28.196672143974773	-61.30957017041324	84074
cb4cdb4125aa387ed50c65d541881de9f66a8e3f	leaf disease recognition in vine plants based on local binary patterns and one class support vector machines		The current application concerns a new approach for disease recognition of vine leaves based on Local Binary Patterns (LBPs). The LBP approach was applied on color digital pictures with a natural complex background that contained infected leaves. The pictures were captured with a smartphone camera from vine plants. A 32-bin histogram was calculated by the LBP characteristic features that resulted from a Hue plane. Moreover, four One Class Support Vector Machines (OCSVMs) were trained with a training set of 8 pictures from each disease including healthy, Powdery Mildew and Black Rot and Downy Mildew. The trained OCSVMs were tested with 100 infected vine leaf pictures corresponding to each disease which were capable of generalizing correctly, when presented with vine leave which was infected by the same disease. The recognition percentage reached 97 %, 95 % and 93 % for each disease respectively while healthy plants were recognized with an accuracy rate of 100 %.	local binary patterns;support vector machine	Xanthoula Eirini Pantazi;Dimitrios Moshou;Alexandra A. Tamouridou;Stathis Kasderidis	2016		10.1007/978-3-319-44944-9_27	machine learning	ML	32.08783150498373	-60.54501964822293	84076
f183d9ddec49146337e5108c378118f61cc2d7cb	fingerprint recognition system using hybrid matching techniques	databases;fingerprint recognition system;image matching correlation methods feature extraction fingerprint identification;image matching;authentication;biometrics;fingerprint matching;gabor filters;correlation methods;correlation based techniques;reference point;fingerprint recognition biometrics gabor filters authentication feature extraction smart cards databases humans nonlinear distortion pixel;nonlinear distortion;hybrid approach;smart cards;fingerprint recognition;minutiae extraction;feature extraction;identification;pixel;biometrics fingerprint recognition system automatic person identification fingerprint based identification correlation based techniques fingerprint matching minutiae extraction ridge algorithm;fingerprint;humans;fingerprint based identification;ridge algorithm;feature extraction biometrics fingerprint matching fingerprint identification fingerprint verification;fingerprint identification;automatic person identification;fingerprint verification	With an increasing emphasis on the emerging automatic person identification application, biometrics based, especially fingerprint-based identification, is receiving a lot of attention. This research developed an automatic fingerprint recognition system (AFRS) based on a hybrid between minutiae and correlation based techniques to represent and to match fingerprint; it improved each technique individually. It was noticed that, in the hybrid approach, as a result of an improvement of minutiae extraction algorithm in post-process phase that combines the two algorithms, the performance of the minutia algorithm improved. An improvement in the ridge algorithm that used centre point in fingerprint instead of reference point was also observed. Experiments indicate that the hybrid technique performs much better than each algorithm individually.	algorithm;biometrics;experiment;fingerprint recognition;minutiae	Aliaa A. A. Youssif;Morshed U. Chowdhury;Sid Ray;Howida Youssry Nafaa	2007	6th IEEE/ACIS International Conference on Computer and Information Science (ICIS 2007)	10.1109/ICIS.2007.101	computer vision;speech recognition;pattern recognition	Vision	34.3537719859339	-62.03927765782732	84121
c12267ca4ae167795979fc402754d81007770171	boosting binary keypoint descriptors	binary keypoint descriptors memory footprint matching time floating point descriptors final descriptor image gradient pooling configuration intermediate representation image patches hash functions robustness compactness boosted binary hash function bin boost compact binary descriptor floating point competitors;image representation gradient methods image matching;image matching;boosting;image representation;binary local feature descriptors;boosting binary local feature descriptors binary embedding;gradient methods;binary embedding;boosting shape optimization accuracy error analysis hamming distance training;binary descriptors	Binary key point descriptors provide an efficient alternative to their floating-point competitors as they enable faster processing while requiring less memory. In this paper, we propose a novel framework to learn an extremely compact binary descriptor we call Bin Boost that is very robust to illumination and viewpoint changes. Each bit of our descriptor is computed with a boosted binary hash function, and we show how to efficiently optimize the different hash functions so that they complement each other, which is key to compactness and robustness. The hash functions rely on weak learners that are applied directly to the image patches, which frees us from any intermediate representation and lets us automatically learn the image gradient pooling configuration of the final descriptor. Our resulting descriptor significantly outperforms the state-of-the-art binary descriptors and performs similarly to the best floating-point descriptors at a fraction of the matching time and memory footprint.	64-bit computing;application domain;boost;cryptographic hash function;image gradient;intermediate representation;memory footprint;nonlinear system;precomputation	Tomasz Trzcinski;C. Mario Christoudias;Pascal Fua;Vincent Lepetit	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.370	discrete mathematics;computer science;machine learning;pattern recognition;mathematics;boosting	Vision	30.595295021330898	-53.9738269411325	84146
f6c8fc599add9820366860d08a6d8fc9638ebf37	facial expression recognition based on binarized statistical image features	facial expression recognition;face recognition feature extraction maximum likelihood detection nonlinear filters histograms filtering theory classification algorithms;jafffe database facial expression recognition binarized statistical image feature extraction local feature descriptor expression feature extraction bsif descriptor sparse representation based classification method src method test sample classification performance evaluation japanese female facial expression database;src facial expression recognition binarized statistical image feature sparse representation;statistical analysis emotion recognition face recognition feature extraction image classification image representation;binarized statistical image feature;src;sparse representation	This paper proposes a new algorithm for facial expression recognition based on a local feature descriptor which is used to extract binarized statistical image features (BSIF). Firstly, expression features are extracted by using BSIF descriptor. Then, the Sparse Representation-based Classification (SRC) method is used to classify the test samples in seven categories of expressions. We evaluate the performance of this method by classifying expressions in Japanese Female Facial Expression (JAFFFE) database. The experimental results show that our method improves accuracy in expression recognition tasks than traditional algorithms such as LDA+SVM, 2DPCA+SVM etc. The results testify the effectiveness of the proposed algorithm.	algorithm;effective method;ibm notes;sparse matrix;visual descriptor	Wenjin Chu;Zilu Ying;Xiaoxiao Xia	2013	2013 Ninth International Conference on Natural Computation (ICNC)	10.1109/ICNC.2013.6817995	computer vision;feature;feature extraction;computer science;machine learning;pattern recognition;feature	Robotics	34.7796889575714	-58.12233883923987	84361
30ce763efe830e08d7ec45d994480463e7306aab	a novel ocr system for calculating handwritten persian arithmetic expressions	optical character recognition software handwriting recognition support vector machines support vector machine classification writing keyboards character recognition digital arithmetic discrete wavelet transforms neural networks;support vector machines feature extraction fuzzy set theory handwritten character recognition optical character recognition performance evaluation;support vector machines feature extraction fuzzy set theory handwritten character recognition optical character recognition;arithmetic expressions character recognition feature extraction approach fuzzy support vector machines ocr system handwritten persian arithmetic expressions;digit recognition;performance evaluation;support vector machines;recognition ocr;optical character recognition;optical character;fuzzy support vector machine fsvm;optical character recognition ocr digit recognition fuzzy support vector machine fsvm;fuzzy support vector machine fsvm digit recognition optical character recognition ocr;optical character recognition software handwriting recognition support vector machines digital arithmetic keyboards character recognition feature extraction support vector machine classification writing pattern recognition;fuzzy support vector machine;optical character recognition ocr;fuzzy set theory;feature extraction;fsvm ocr system optical character recognition handwritten persian arithmetic expression feature extraction fuzzy support vector machine performance evaluation;character recognition;handwritten character recognition	In this paper we propose a novel OCR system which can recognize and calculate handwritten Persian arithmetic expressions without using a keyboard or a memory to store the intermediate results. Our research is composed of two major phases: character recognition and calculation. The recognition phase is based on a new approach for feature extraction. Fuzzy Support Vector Machines (FSVMs) are employed as the classifier. In calculation phase a simple algorithm is used for calculating the recognized arithmetic expression. The performance of the system is evaluated on a database consisting of 3400 digits and symbols written by 20 different people. 92 percent accuracy in recognition proves the good performance of our system.	algorithm;database;feature extraction;optical character recognition;statistical classification;support vector machine;symbol (formal)	Sirvan Khalighi;Parisa Tirdad;Hamid R. Rabiee;Mehdi Parviz	2009	2009 International Conference on Machine Learning and Applications	10.1109/ICMLA.2009.83	support vector machine;speech recognition;feature extraction;computer science;machine learning;pattern recognition;fuzzy set;optical character recognition	Robotics	32.700403151880444	-65.222777450976	84470
3a87b3d37711f599d90fd131a63bbb4b86cfb115	cospair: colored histograms of spatial concentric surflet-pairs for 3d object recognition	rgb d;3d object recognition;3d descriptors;point clouds	Introduction of RGB-D sensors together with the efforts on open-source point-cloud processing tools boosted research in both computer vision and robotics. One of the key areas which have drawn particular attention is object recognition since it is one of the crucial steps for various applications. In this paper, two spatially enhanced local 3D descriptors are proposed for object recognition tasks: Histograms of Spatial Concentric Surflet-Pairs (SPAIR) and Colored SPAIR (CoSPAIR). The proposed descriptors are compared against the state-of-the-art local 3D descriptors that are available in Point Cloud Library (PCL) and their object recognition performances are evaluated on several publicly available datasets. The experiments demonstrate that the proposed CoSPAIR descriptor outperforms the state-of-the-art descriptors in both category-level and instance-level recognition tasks. The performance gains are observed to be up to 9.9 percentage points for category-level recognition and 16.49 percentage points for instance-level recognition over the second-best performing descriptor.	3d single-object recognition;cloud computing;computer vision;experiment;open-source software;outline of object recognition;performance;point cloud library;robotics;sensor	K. Berker Logoglu;Sinan Kalkan;Alptekin Temizel	2016	Robotics and Autonomous Systems	10.1016/j.robot.2015.09.027	computer vision;computer science;pattern recognition;point cloud;3d single-object recognition	Vision	32.126256065710145	-53.692285485977415	84751
877d75c319bcfee52f55ed0e21dc751ba5447ff4	component based face recognition system	haar wavelet;backpropagation neural network;dimension reduction;research paper;face recognition;principal component analysis;graphic user interface;support vector machine;face detection	The goal of this research paper was to design and use a component based approach to face recognition and show that this technique gives us recognition rates of up to 92%. A novel graphical user interface was also developed as part of the research to showcase and control the process of face detection and component extraction and to display the recognition results.The paper essentially consists of two parts, face detection and face recognition. The face detection system takes a given image as the input from which a face is located and extracted using 2D Haar Wavelets and Support Vector Machines. The face region is then used to locate and extract the individual components of the face such as eyes, eyebrows, lips and nose which are then sent to the face recognition system where the individual components are recognized by using Wavelets, Principal Component Analysis and Error Backpropagation Neural Networks. Pattern dimension reduction technique is used to significantly reduce the dimensionality and complexity of the task.	backpropagation;color;dimensionality reduction;face detection;facial recognition system;graphical user interface;haar wavelet;neural networks;pattern recognition;principal component analysis;reduction (complexity);rough set;statistical classification;support vector machine	Pavan Kandepet;Roman W. Swiniarski	2007		10.1007/978-1-4020-8741-7_80	facial recognition system;support vector machine;computer vision;face detection;object-class detection;computer science;machine learning;pattern recognition;graphical user interface;eigenface;dimensionality reduction;principal component analysis	Vision	29.98652524407133	-60.26485121330474	84815
22a10d8d2a2cb9055557a3b335d6706100890afb	comparison of matrix decomposition and sift descriptor based methods for face alignment	face matrix decomposition face recognition face detection sparse matrices robustness;matrix decomposition affine transforms face recognition image classification;face recognition;matrix decomposition;sift flow based affine transformation matrix decomposition sift descriptor face alignment face analysis system face recognition system image set based classification method;robustness;face;face detection;sparse matrices;affine transformation face recognition face alignment rasl sift flow	Face alignment is an important pre-processing step for face analysis systems. Especially, the performance of face recognition systems can be improved by using aligned face images. In this work, we used matrix decomposition based and SIFT features based methods in face alignment. We performed recognition experiments by using raw versus aligned images with an image set based classification method. We also developed a SIFT-flow based affine transformation and showed that this type of alignment improves the recognition accuracies.	experiment;facial recognition system;manifold alignment;preprocessor;scale-invariant feature transform	Meltem Yalcin;Hasan Serhan Yavuz	2016	2016 24th Signal Processing and Communication Application Conference (SIU)	10.1109/SIU.2016.7496121	computer vision;object-class detection;machine learning;pattern recognition;three-dimensional face recognition;mathematics;3d single-object recognition	Vision	33.69465926697749	-57.3534938239074	84900
14e615124a3fce666fb232853d44327d6db43628	video shot segmentation using graph-based dominant-set clustering	cluster algorithm;video segmentation;data mining;dominating set;machine learning;computational complexity;video content analysis;content based video retrieval;dominant set clustering;shot boundary detection	Video shot segmentation is a solid foundation for automatic video content analysis, for most content based video retrieval tasks require accurate segmentation of video boundaries. In recent years, using the tools of data mining and machine learning to detect shot boundaries has become more and more popular. In this paper, we propose an effective video segmentation approach based on a dominant-set clustering algorithm. The algorithm can not only automatically determine the number of video shots, but also obtain accurate shot boundaries with low computation complexity. Experimental results have demonstrated the effectiveness of the proposed shot segmentation approach.	algorithm;cluster analysis;computation;data mining;digital video;machine learning;video content analysis	Li Li;Xianglin Zeng;Xi Li;Weiming Hu;Pengfei Zhu	2009		10.1145/1734605.1734645	computer vision;computer science;machine learning;video tracking;pattern recognition;scale-space segmentation;video denoising	AI	38.7088446581462	-52.378071491042455	84937
7fe7d2765b571c3a5b91dac7168cfcc6852a8871	salient object detection via bootstrap learning	training kernel computational modeling feature extraction object detection boosting support vector machines;object detection computer vision image classification learning artificial intelligence;computer vision salient object detection bootstrap learning algorithm weak saliency map image priors image classification salient pixel detection multiscale saliency map bottom up saliency model	We propose a bootstrap learning algorithm for salient object detection in which both weak and strong models are exploited. First, a weak saliency map is constructed based on image priors to generate training samples for a strong model. Second, a strong classifier based on samples directly from an input image is learned to detect salient pixels. Results from multiscale saliency maps are integrated to further improve the detection performance. Extensive experiments on six benchmark datasets demonstrate that the proposed bootstrap learning algorithm performs favorably against the state-of-the-art saliency detection methods. Furthermore, we show that the proposed bootstrap learning approach can be easily applied to other bottom-up saliency models for significant improvement.	algorithm;benchmark (computing);experiment;map;object detection;pixel;statistical classification	Na Tong;Huchuan Lu;Xiang Ruan;Ming-Hsuan Yang	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298798	computer vision;computer science;kadir–brady saliency detector;machine learning;pattern recognition	Vision	32.07583298657761	-54.36725267057275	85341
256f959926dfcc8092e107b44bb52d11e8bf65c4	infotree: a contextural tree approach to supervised pattern recognition	feature vectors contextual tree supervised pattern recognition information theory information content input pattern algorithm noisy binary printed letters infotree artificial neural networks;neural nets;feature extraction pattern recognition neural nets information theory trees mathematics noise learning artificial intelligence;pattern recognition power system modeling information theory data mining data preprocessing;trees mathematics;information content;feature extraction;pattern recognition;learning artificial intelligence;information theory;noise	A new method for pattern recognition is proposed. The model uses information theory to extract in a systematic and hierarchic way the relevant information content in the input pattern so as to avoid preprocessing. An example of the power of the algorithm is demonstrated by its use in the recognition of heavily noisy binary printed letters.	pattern recognition	Dirk Van Dyck;J. Garcia	1997		10.1109/ICIP.1997.638785	neural gas;self-information;feature;information theory;feature extraction;computer science;noise;machine learning;pattern recognition;data mining;statistics	Vision	32.20256447535604	-65.7153521481524	85355
7c9fc4b75150baadb82f26554b6f411a6f5944e9	flexible 3d object recognition framework using 2d views via a similarity-based aspect-graph approach	object representation;object recognition;human posture recognition;3d object recognition;scene recognition;aspect graph;article	This work presents a flexible framework for recognizing 3D objects from 2D views. Similarity-based aspect-graph, which contains a set of aspects and prototypes for these aspects, is employed to represent the database of 3D objects. An incremental database construction method that maximizes the similarity of views in the same aspect and minimizes the similarity of prototypes is proposed as the core of the framework to build and update the aspect-graph using 2D views randomly sampled from a viewing sphere. The proposed framework is evaluated on various object recognition problems, including 3D object recognition, human posture recognition and scene recognition. Shape and color features are employed in different applications with the proposed framework and the top three matching rates show the efficiency of the proposed method.	3d single-object recognition;algorithm;color;combinational logic;computation;emoticon;experiment;feature selection;graph (abstract data type);like button;nl-complete;numerical aperture;outline of object recognition;poor posture;randomness;relational database;xfig	Jwu-Sheng Hu;Tzung-Min Su	2008	IJPRAI	10.1142/S0218001408006685	computer vision;deep-sky object;computer science;cognitive neuroscience of visual object recognition;pattern recognition;data mining;3d single-object recognition;sketch recognition	Vision	36.80845900327701	-53.73270786749951	85380
811784517de3d44175d96673c8cfba1f1678a5b8	a novel approach in adopting finite state automata for image processing applications		This article describes how robust image processing application rely heavily on image descriptors extracted. Limited work is carried out in adopting probabilistic finite state automata PFSA models for image processing. A finite state automata for image processing FSAFIP method is presented here. Texture classification and content based image retrieval CBIR is considered. In FSAFIP, foreground and background regions of an image are identified and later split into patches. Using a tristate PFSA model, feature descriptors corresponding to background/foreground regions are constructed. A distance based large margin nearest neighbor LMNN classifier is considered in FSAFIP to impart intelligence. A performance and experimental study to evaluate performance of FSAFIP for CBIR and texture classification is presented. Comparison results in CBIR obtained prove superior performance of FSAFIP over existing methods on Corel-1K dataset. High texture classification accuracy of 99.2% is reported using FSAFIP on KHT-TIPS dataset. An improved texture classification accuracy is achieved using FSAFIP in comparison to former methods.	automaton;finite-state machine;image processing	R. Obulakonda Reddy;Kashyap D. Dhruve;R. Nagarjuna Reddy;M. Radha;N. Sree Vani	2018	IJCVIP	10.4018/IJCVIP.2018010104	image processing;computer vision;visual descriptors;content-based image retrieval;finite-state machine;large margin nearest neighbor;probabilistic logic;feature vector;artificial intelligence;computer science;classifier (linguistics)	Robotics	36.05574210615072	-59.47948904667635	85986
725bba18d41a6dedeeef01fd8e306aa5bd2a3f6b	toward integrated scene text reading	image recognition;image motion analysis;probability;image segmentation;text guidelines;word normalization;cropped word recognition;image sensors;probability document image processing image motion analysis image recognition image segmentation image sensors;discriminative semi markov model;word segmentation;image binarization;scene text recognition;skew detection;document image processing;word segmentation scene text recognition cropped word recognition character recognition discriminative semi markov model image binarization skew detection baseline estimation text guidelines word normalization;word segmentation integrated scene text reading digital camera usage worldly text abundance pattern recognition document processing unconstrained lexicons motion blur curved layouts perspective projection occlusion probabilistic methods character segmentation;character recognition;image segmentation character recognition text recognition probabilistic logic hidden markov models noise robustness;baseline estimation	The growth in digital camera usage combined with a worldly abundance of text has translated to a rich new era for a classic problem of pattern recognition, reading. While traditional document processing often faces challenges such as unusual fonts, noise, and unconstrained lexicons, scene text reading amplifies these challenges and introduces new ones such as motion blur, curved layouts, perspective projection, and occlusion among others. Reading scene text is a complex problem involving many details that must be handled effectively for robust, accurate results. In this work, we describe and evaluate a reading system that combines several pieces, using probabilistic methods for coarsely binarizing a given text region, identifying baselines, and jointly performing word and character segmentation during the recognition process. By using scene context to recognize several words together in a line of text, our system gives state-of-the-art performance on three difficult benchmark data sets.	3d projection;baseline (configuration management);benchmark (computing);digital camera;document processing;face;lexicon;pattern recognition	Jerod J. Weinman;Zachary Butler;Dugan Knoll;Jacqueline L. Feild	2013	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2013.126	text segmentation;computer vision;speech recognition;computer science;pattern recognition;probability;image sensor;image segmentation;statistics	Vision	36.848555104006245	-65.89100111172009	86099
890a4a3f7006a39b614d8836d67cc46a6a2a36c9	multi-scale score level fusion of local descriptors for gender classification in the wild	1203 ciencia de los ordenadores;info eu repo semantics article	The 2015 FRVT gender classification (GC) report evidences the problems that current approaches tackle in situations with large variations in pose, illumination, background and facial expression. The report suggests that both commercial and research solutions are hardly able to reach an accuracy over 90 % for The Images of Groups dataset, a proven scenario exhibiting unrestricted or in the wild conditions. In this paper, we focus on this challenging dataset, stepping forward in GC performance by observing: 1) recent literature results combining multiple local descriptors, and 2) the psychophysics evidences of the greater importance of the ocular and mouth areas to solve this task. We therefore make use of holistic and inner facial patches to extract features, that are later combined via a score level fusion strategy. The achieved results support the main information provided by the ocular and the mouth areas. Indeed, the combination of multiscale extracted features increases the overall accuracy to over 94 %, reducing notoriously the classification error if compared with tuned holistic and deep learning approaches.	computation;deep learning;face recognition vendor test;feature extraction;feature vector;holism;imagenet;parallel computing;relevance;statistical classification;stepping level	Modesto Castrillón Santana;Javier Lorenzo-Navarro;Enrique Ramón-Balmaseda	2016	Multimedia Tools and Applications	10.1007/s11042-016-3653-2	computer vision;computer science;artificial intelligence;machine learning;data mining;algorithm	ML	29.80975525203488	-54.7925375468109	86195
6f61b9faaaaf7c55e7587649e2eb5320dfcead41	detecting liveness of fingerprint biometrics		Biometrics refer to automated recognition of individuals based on their biological and behavioral characteristics. Biometric systems are widely used for security. But biometric systems are vulnerable to a certain type of attack. The type 1 attack or direct attack is done at the sensor level using fake input. Spoofing refers to the fraudulent action by an unauthorised person into biometric systems using fake input that reproduces one of the authorised personu0027s biometric inputs. Liveness detection provides an extra level of authentication to biometrics. The fingerprint liveness detection is performed by measuring the following features of the fingerprint. They are Gabor-Shen feature, orientation flow feature, and frequency domain feature. This approach is based on fingerprint image quality. The SVM classifier is used for classification. The ATVS database is used for conducting experiments. This technique is software based as it requires no external hardware. This approach is inexpensive.	biometrics;fingerprint;liveness	G. Arunalatha;M. Ezhilarasan	2016	IJIPT	10.1504/IJIPT.2016.10002240	support vector machine;computer vision;computer science;information security;internet privacy;computer security;fingerprint recognition	Crypto	30.461380727344025	-62.918777012428905	86482
1a767a8af36252bb557117c021393292d8a83b24	generic object detection with dense neural patterns and regionlets		This paper addresses the challenge of establishing a bridge between deep convolutional neural networks and conventional object detection frameworks for accurate and efficient generic object detection. We introduc e Dense Neural Patterns, short for DNPs, which are dense local featu res derived from discriminatively trained deep convolutional neural network s. DNPs can be easily plugged into conventional detection frameworks in the same way as other dense local features(like HOG or LBP). The effectiveness of the proposed approach is demonstrated with the Regionle ts object detection framework. It is the first approach efficiently applying deep convolutional features for conventional object detection models. Detecting generic objects in high-resolution images is one of the most valuable pattern recognition tasks, useful for large-scale image labeling , scene understanding, action recognition, self-driving vehicles and ro botics. At the same time, accurate detection is a highly challenging task due to cluttered backgrounds, occlusions, and perspective changes. Pre dominant approaches use deformable template matching with hand-designed features. However, these methods are not flexible when dealing with var able aspect ratios. Wang et al. recently proposed a radically different approach, named Regionlets, for generic object detection [4]. It extends classic cascaded boosting classifiers with a two-layer feature extraction hierarchy , and is dedicatedly designed for region based object detectio n. Despite the success of these sophisticated detection methods, the features employed in these frameworks are still traditional features based on lowlevel cues such as histogram of oriented gradients(HOG), local binary patterns(LBP) or covariance [3] built on image gradients. With the success in large scale image classification [1], object detection using a deep convolutional neural network also shows promising performance [2]. The dramatic improvements from the application of deep neural networks are believed to be attributable to their capability to learn hierarchically more complex features from large data-sets. Despite their excellent performance, the application of deep CNNs has been centered around image classification, which is computationally expensive when transferred to perform object detection. Furthermore, their for mulation does not take advantage of venerable and successful object dete tion frameworks such as DPM or Regionlets which are powerful designs for modeling object deformation, sub-categories and multiple aspect ratios. These observations motivate us to propose an approach to efficiently incorporate a deep neural network into conventional object detection fr ameworks. To that end, we introduce the Dense Neural Pattern (DNP), a local feature densely extracted from an image with an arbitrary resolution using a deep convolutional neural network trained with image classification datasets. The DNPs not only encode high-level features learned from a large image data-set, but are also local and flexible like other dense local features (like HOG or LBP). It is easy to integrate DNPs into the conventional detection frameworks. More specifically, the receptive field location of a neuron in a deep CNN can be back-tracked to exact coordinates in the image. This implies that spatial information of neural activations is preserved. Activations from the same receptive field but different feature maps can be concatenated to form a feature vector for that recepti ve field. These feature vectors can be extracted from any convolutional la yers before the fully connected layers. Because spatial locations of rec eptive fields are mixed in fully connected layers, neuron activations from fully connected layers do not encode spatial information. The convolutional layers naturally produce multiple feature vectors that are evenly distributed in the evaluated image crop ( a 224 ×224 crop for example). To obtain dense features for the whole image which may be significantly larger than the network input, we resort to “network-convolution” which shifts the crop location and forward-propagate the neural network until Object detection	analysis of algorithms;artificial neural network;computer vision;concatenation;convolution;convolutional neural network;dnp3;deep learning;discriminative model;encode;feature extraction;feature vector;gradient;high- and low-level;histogram of oriented gradients;image resolution;linear algebra;local binary patterns;map;neuron;object detection;pattern recognition;template matching	Will Y. Zou;Xiaoyu Wang;Miao Sun;Yuanqing Lin	2014	CoRR		speech recognition;computer science;machine learning;pattern recognition;data mining	Vision	27.41580859853866	-53.1522808457258	86751
37b685caf39b38b07af60eacf1a7d7ada2122372	syncspeccnn: synchronized spectral cnn for 3d shape segmentation		In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parametrizing kernels in the spectral domain spanned by graph Laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strives to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parametrization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested SyncSpecCNN on various tasks, including 3D shape part segmentation and keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.	3d modeling;artificial neural network;benchmark (computing);coefficient;convolutional neural network;data structure;deep learning;end-to-end principle;experiment;graphics processing unit;ibm notes;isometric projection;laplacian matrix;map;software incompatibility;transformer	Li Yi;Hao Su;Xingwen Guo;Leonidas J. Guibas	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.697	computer vision;theoretical computer science;machine learning;pattern recognition;mathematics	Vision	25.46449914924952	-52.60781899868193	86758
25c3068e7964d3b894916a82b1fa93c9d6792886	face recognition with histograms of oriented gradients	histograms of oriented gradients;face recognition	Histograms of Oriented Gradients have been recently used as discriminating features for face recognition. In this work we improve on that work in a number of aspects. As a first contribution, it identifies the necessity of performing feature selection or transformation, especially if HOG features are extracted from overlapping cells. Second, the use of four different face databases allowed us to conclude that, if HOG features are extracted from facial landmarks, the error of landmark localization plays a crucial role in the absolute recognition rates achievable. This implies that the recognition rates can be lower for easier databases if landmark localization is not well adapted to them. This prompted us to extract the features from a regular grid covering the whole image. Overall, these considerations allow to obtain a significant recognition rate increase (up to 10% in some subsets) on the standard FERET database with respect to previous work.	feret (facial recognition technology);feret database;facial recognition system;feature selection;image gradient;regular grid	Oscar Déniz-Suárez;Gloria Bueno García;Jesús Salido;Fernando De la Torre	2010			facial recognition system;computer vision;computer science	Vision	32.51348277506965	-57.90676218546458	86760
851020925dee78488e1e5fd02236239ec0144217	palmprint verification using circular gabor filter	biometrics;gabor filter;texture analysis;hamming distance;feature extraction;region of interest	Recently, researchers have been paying an excessive amount of attention to biometric palmprint which has gained popularity and received significant prominence due to its high stability and uniqueness. In this study, two certain filters have been taken into consideration, namely Gabor filter and Circular Gabor filter which are used to obtain feature information from two distinguishing regions of interest, square and inscribed circle areas as the central part sub-images and the two palmprint images are compared with one another in terms of their hamming distance. The outcomes of the experiment gave an indication to the fact that circular Gabor Filter had a comparatively better performance than traditional one in extracting distinct feature information.	biometrics;fingerprint;gabor filter;hamming distance;region of interest	Azadeh Ghandehari;Reza Safabakhsh	2009		10.1007/978-3-642-01793-3_69	computer vision;hamming distance;speech recognition;feature extraction;computer science;archaeology;pattern recognition;mathematics;gabor wavelet;biometrics;region of interest	Vision	34.549245180815895	-60.561386076579645	86831
cfb5fa6c2dedcd19be3d0446aeeb335bffd98511	comparative evaluation of face sequence matching for content-based video access	image matching;multimedia application;multimedia systems;video indexing;face sequence matching;television production face recognition image sequences image matching content based retrieval multimedia systems;face recognition;television production;evaluation;facial expression;performance evaluation face sequence matching content based video access tv programs content based face retrieval automated face annotation multimedia applications video authoring image matching pattern recognition;content based retrieval;face detection data mining detectors read only memory motion pictures electrical capacitance tomography pattern matching image matching pattern recognition eyes;image sequences	The paper presents comparative evaluation of matching methods of face sequences obtained from actual videos. Face information is quite important in videos, especially in news programs, dramas, and movies. Accurate face sequence matching enables many multimedia applications including content-based face retrieval, automated face annotation, video authoring, etc. However, face sequences in videos are subject to variation in lighting condition, pose, facial expression, etc., which cause difficulty in face matching. In order to cope with this problem, several face sequence matching methods are proposed by extending face still image matching, traditional pattern recognition, and recent pattern recognition techniques. They are expected to be applicable to face sequences extracted from actual videos. The performance of these methods are evaluated as the accuracy of face sequence annotation using the methods. The accuracy is evaluated using considerable amount of actual drama videos. The evaluation results reveal merits and demerits of these methods, and indicate future research direction of face matching for videos.	eigenface;image registration;pattern recognition;pose (computer vision);semiconductor industry;test set;video	Shin'ichi Satoh	2000		10.1109/AFGR.2000.840629	facial recognition system;computer vision;face detection;speech recognition;object-class detection;computer science;evaluation;three-dimensional face recognition;multimedia;facial expression	Vision	34.91761121499182	-52.56752600822633	87110
3ebb755403eb2f391b28d6cc9e6d2aec8bf1bd60	eye state detection in facial image based on linear prediction error of wavelet coefficients	databases;face detection wavelet coefficients error analysis statistical distributions face recognition fatigue monitoring predictive models support vector machines support vector machine classification;kernel;driver fatigue monitoring system;prediction error;support vector machines;training;satisfiability;statistical model;linear predictive;mean variance;wavelet transforms;radial basis function networks;monitoring system;linear prediction error of wavelet coefficients;face recognition;radial basis function;adaboost eye states detection linear prediction error of wavelet coefficients svm;feature extraction;classification algorithms;radial basis function eye state detection facial image linear prediction error wavelet coefficients feature extraction face recognition human computer interface driver fatigue monitoring system support machine vector;adaboost;eye states detection;eye state detection;svm;face;facial image;linear prediction error;wavelet transforms face recognition feature extraction radial basis function networks support vector machines;wavelet coefficients;support machine vector;human computer interface	Eye state detection in facial image is a significant issue in face recognition, human-computer interface and driver fatigue monitoring system. In this paper, we first located the eye region in the upper area of the face region with AbaBoost algorithm. The linear predictor error distribution of wavelet coefficients was proposed as the statistics model to distinguish the eye states. We collected statistics (mean, variance, skewness, and kurtosis) of the prediction error distribution as eye state features. Build on these eye state features and support machine vector (SVM) with radial basis function (RBF) kernel a non-linear classifier is obtained by training samples of eye images. Experiment results with the classifier demonstrated that our method is an effective eye state detection approach which can satisfy various situations.	adaboost;algorithm;coefficient;effective method;facial recognition system;human–computer interaction;kerrison predictor;linear classifier;nonlinear system;radial (radio);radial basis function kernel;statistical model;test set;wavelet	Erkang Cheng;Bin Kong;Rongxiang Hu;Fei Zheng	2008	2008 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2009.4913203	facial recognition system;statistical classification;support vector machine;computer vision;computer science;machine learning;pattern recognition	Robotics	27.283749766438827	-64.3391546731314	87153
04ea46be7038c427181485cd73867b91ad95e810	"""corrigendum to """"scale-invariant template matching using histogram of dominant gradients"""" [pattern recognit. 47/9 (2014) 3006-3018]"""		The authors regret that Table 2 contains an error in the original article, the correct table appears below. The authors would like to apologise for any inconvenience caused.	gradient;regret (decision theory);template matching	Jisung Yoo;Sung Soo Hwang;Seong-Dae Kim;MyungSeok Ki;Jihun Cha	2014	Pattern Recognition	10.1016/j.patcog.2014.07.002	machine learning;pattern recognition	Vision	38.671320918913885	-65.05954744852237	87268
cc72b41545e0a592ce56a566720f4df27efb72a2	texture retrieval via the scattering transform		This work studies the problem of content-based image retrieval, specifically, texture retrieval. Our approach employs a recently developed method, the so-called Scattering transform, for the process of feature extraction in texture retrieval. It shares a distinctive property of providing a robust representation, which is stable with respect to spatial deformations. Recent work has demonstrated its capability for texture classification, and hence as a promising candidate for the problem of texture retrieval. Moreover, we adopt a common approach of measuring the similarity of textures by comparing the subband histograms of a filterbank transform via the Kullback-Leibler divergence. Despite the popularity of describing histograms using parametrized probability density functions, such as the Generalized Gaussian Distribution (GGD), it is unfortunately not applicable for describing most of the Scattering transform subbands, due to the complex modulus performed on each one of them. In this work, we propose to use the Weibull distribution to model the Scattering subbands of descendant layers. Our numerical experiments demonstrated the effectiveness of the proposes approach, in comparison with several state of the arts.	content-based image retrieval;experiment;feature extraction;filter bank;kullback–leibler divergence;modulus of continuity;numerical analysis	Alexander Sagel;Dominik Meyer;Hao Shen	2015	CoRR		image texture;computer vision;speech recognition;pattern recognition;texture filtering	Vision	37.67716480135278	-59.15782966670625	87363
e4f90ae6a7becf33d90ad3a177f1d9705f02d5be	"""corrigendum to """"reidentification of persons using clothing features in real-life video"""""""				Guodong Zhang;Peilin Jiang;Kazuyuki Matsumoto;Minoru Yoshida;Kenji Kita	2017	Applied Comp. Int. Soft Computing	10.1155/2017/9635968		Vision	29.758406770096446	-57.77727760738568	87430
0c6a7cac142dd5e4e9391fed6695fbdc71e32c21	finger identification and hand posture recognition for human-robot interaction	hand finger identification;image segmentation;hand posture recognition;human robot interaction;color segmentation;service robot;hand image segmentation;robot programming;neural network	Natural and friendly interface is critical for the development of service robots. Gesture-based interface offers a way to enable untrained users to interact with robots more easily and efficiently. In this paper, we present a posture recognition system implemented on a real humanoid service robot. The system applies RCE neural network based color segmentation algorithm to separate hand images from complex backgrounds. The topological features of the hand are then extracted from the silhouette of the segmented hand region. Based on the analysis of these simple but distinctive features, hand postures are identified accurately. Experimental results on gesture-based robot programming demonstrated the effectiveness and robustness of the system.	human–robot interaction;poor posture	Xiaoming Yin;Ming Xie	2007	Image Vision Comput.	10.1016/j.imavis.2006.08.003	human–robot interaction;computer vision;computer science;artificial intelligence;machine learning;image segmentation;artificial neural network	Robotics	30.56992854751753	-59.58458577303719	87491
5b9ccbda29ddb5d441e070530966a474cfdf1a32	retrieval of scene image and video frames using date field spotting	image resolution;information retrieval;bayes methods;noise measurement;hidden markov models;decision support systems;pattern recognition	In this paper, we present a date spotting based information retrieval system for natural scene image and video frames where text appears with complex backgrounds. Text retrieval in such scene/video frames is difficult because of blur, low resolution, background noise, etc. In our proposed framework, a line based date spotting approach using Hidden Markov Model is used to detect the date information in text. Given a text line image, we apply an efficient Bayesian classifier based binarization approach to extract the text components. Next, Pyramid Histogram of Oriented Gradient (PHOG) feature is computed from the binarized image for date-spotting framework. For our experiment, three different date models have been constructed to search similar date information in scene/video text. When tested in a custom dataset of 1104 text lines, our date spotting approach provided encouraging results.	bayesian network;binary image;frame (video);gaussian blur;gradient;hidden markov model;image resolution;information retrieval;markov chain;naive bayes classifier	Partha Pratim Roy;Ayan Das;Dipak Majhi;Umapada Pal	2015	2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)	10.1109/ACPR.2015.7486594	computer vision;speech recognition;computer science;pattern recognition	Vision	36.79540002158038	-65.71402725092162	87671
2abf94ae2a80b734709ad6a498bd23cbe9ba1a55	graph based descriptor evaluation towards automatic feature selection	components;graph representation;feature selection	This paper presents the first steps towards an automated image and video feature descriptor evaluation framework, based on several points of view. First, evaluation of distance distributions of images and videos for several descriptors are performed, then a graph-based representation of database contents and evaluation of the appearance of the giant component is performed. The goal is to lay the foundations for an evaluation framework where different descriptors and their combinations can be analyzed, with the goal of later application for automatic feature selection.	feature selection;giant component;graph theory;random geometric graph;random graph;visual descriptor	Anita Keszler;Levente Kovács;Tamás Szirányi	2012			computer science;machine learning;pattern recognition;graph;feature selection	Vision	36.901486897880744	-56.080517556423054	88041
09fbfb566a8f2af9df4d3a1bf5df00d0693a22eb	conformal prediction for automatic face recognition		Automatic Face Recognition (AFR) has been the subject of many research studies in the past two decades and has a wide range of applications. The provision of some kind of indication of the likelihood of a recognition being correct is a desirable property of AFR techniques in many applications, such as for the detection of wanted persons or for performing post-processing in automatic annotation of photographs. This paper investigates the use of the Conformal Prediction (CP) framework for providing reliable confidence information for AFR. In particular we combine CP with two classifiers based on calculating similarities between images using Scale Invariant Feature Transformation (SIFT) features. We examine and compare the performance of several nonconformity measures for the particular task in terms of their accuracy and informational efficiency.	alternate frame rendering;annualized failure rate;experiment;facial recognition system;uncontrolled format string;video post-processing	Charalambos Eliades;Harris Papadopoulos	2017			conformal map;facial recognition system;computer science;artificial intelligence;pattern recognition	Vision	28.743826016963457	-62.395635427413424	88069
1b3166913b72ac6faf6e6dbc5e96a9e53cd577c6	weakly supervised object detection via object-specific pixel gradient		Most existing object detection algorithms are trained based upon a set of fully annotated object regions or bounding boxes, which are typically labor-intensive. On the contrary, nowadays there is a significant amount of image-level annotations cheaply available on the Internet. It is hence a natural thought to explore such “weak” supervision to benefit the training of object detectors. In this paper, we propose a novel scheme to perform weakly supervised object localization, termed object-specific pixel gradient (OPG). The OPG is trained by using image-level annotations alone, which performs in an iterative manner to localize potential objects in a given image robustly and efficiently. In particular, we first extract an OPG map to reveal the contributions of individual pixels to a given object category, upon which an iterative mining scheme is further introduced to extract instances or components of this object. Moreover, a novel average and max pooling layer is introduced to improve the localization accuracy. In the task of weakly supervised object localization, the OPG achieves a state-of-the-art 44.5% top-5 error on ILSVRC 2013, which outperforms competing methods, including Oquab et al. and region-based convolutional neural networks on the Pascal VOC 2012, with gains of 2.6% and 2.3%, respectively. In the task of object detection, OPG achieves a comparable performance of 27.0% mean average precision on Pascal VOC 2007. In all experiments, the OPG only adopts the off-the-shelf pretrained CNN model, without using any object proposals. Therefore, it also significantly improves the detection speed, i.e., achieving three times faster compared with the state-of-the-art method.	algorithm;artificial neural network;box;categories;convolutional neural network;detectors;ephrin type-b receptor 1, human;experiment;extracellular matrix;gradient;gradient;information retrieval;internet;iterated conditional modes;iterative method;minimum bounding box;neural network simulation;object detection;physical object;pixel;web services for devices;word-sense disambiguation;algorithm;newton per square metre	Yunhang Shen;Rongrong Ji;Changhu Wang;Xi Li;Xuelong Li	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2018.2816021	pixel;artificial intelligence;convolutional neural network;machine learning;object detection;pooling;computer science;pattern recognition	Vision	26.892690189654918	-53.40358168901744	88323
e0d888299c98b88e7fd126f76ceaffbd1f4b4bcd	an automatic video text detection, localization and extraction approach	support vector machine svm;video ocr;video text detection;feature vector;video indexing;text detection;support vector machine;multilingual texts	Text in video is a very compact and accurate clue for video indexing and summarization. This paper presents an algorithm regarding word group as a special symbol to detect, localize and extract video text using support vector machine (SVM) automatically. First, four sobel operators are applied to get the EM(edge map) of the video frame and the EM is segmented into N×2N size blocks. Then character features and characters group structure features are extracted to construct a 19-dimension feature vector. We use a pre-trained SVM to partition each block into two classes: text and non-text blocks. Secondly a dilatation-shrink process is employed to adjust the text position. Finally text regions are enhanced by multiple frame information. After binarization of enhanced text region, the text region with clean background is recognized by OCR software. Experimental results show that the proposed method can detect, localize, and extract video texts with high accuracy.	ascii art;algorithm;binary image;comparison of optical character recognition software;digital video;feature vector;sobel operator;support vector machine	Chengjun Zhu;Yuanxin Ouyang;Lei Gao;Zhenyong Chen;Zhang Xiong	2006		10.1007/978-3-642-01350-8_1	support vector machine;computer vision;speech recognition;feature vector;computer science;machine learning;video tracking;pattern recognition;block-matching algorithm	AI	36.06818417602371	-64.5711545767881	88410
e426bd17c7388592bec44cfd63b5252d3211727a	statistical analysis of gabor-filter representation	data compression;filters;data compression gabor filter representation statistical analysis face recognition system face images orientation expression illumination japanese test image set;statistical method;filters statistical analysis face recognition data compression;statistical properties;gabor filter;face recognition;statistical analysis;statistical analysis face recognition kernel lighting image databases humans neuroscience image recognition testing data compression	A successful face recognition system calculates similarity of face images based on the activation of multiscale and multiorientation Gabor kernels, but without utilizing any statistical properties of that representation [3]. A method has been developed to weight the contribution of each element (1920 kernels) in the representation according to their power of predicting similarity of faces. The same statistical method has also been used to assess how changes in orientation (horizontal and vertical), expression, illumination and background contribute to the overall variance in the kernel activations. Weighting the elements in the representation according to their discriminative power has shown to increase recognition performance on a Caucasian and on a Japanese test image-set. It has also been demonstrated that such weighting method is particularly useful when data compression is a key requirement.	data compression;facial recognition system;gabor filter;standard test image	Peter Kalocsai;Hartmut Neven;Johannes Steffens	1998		10.1109/AFGR.1998.670975	data compression;facial recognition system;computer vision;computer science;machine learning;pattern recognition;statistics	Vision	36.319281946156615	-58.68982314940254	88474
9bb14ab2aaac32e86efd9856379cb6c8de497d93	a combination fingerprint classifier	classification algorithm;decision tree;neural networks;fingerprint recognition indexing large scale systems classification tree analysis hidden markov models decision trees classification algorithms image databases image matching system testing;neural nets;hidden markov model;indexing method;large scale;henry fingerprint classification;hidden markov models;combining classifier;fingerprint recognition;feature extraction;decision trees fingerprint identification pattern classification hidden markov models feature extraction neural nets;pattern classification;decision trees;feature extraction henry fingerprint classification hidden markov models decision trees neural networks nist database pattern classification;fingerprint identification;nist database;henry fingerprint classificaton;singular point	ÐFingerprint classification is an important indexing method for any large scale fingerprint recognition system or database as a method for reducing the number of fingerprints that need to be searched when looking for a matching print. Fingerprints are generally classified into broad categories based on global characteristics. This paper describes novel methods of classification using hidden Markov models (HMMs) and decision trees to recognize the ridge structure of the print, without needing to detect singular points. The methods are compared and combined with a standard fingerprint classification algorithm and results for the combination are presented using a standard database of fingerprint images. The paper also describes a method for achieving any level of accuracy required of the system by sacrificing the efficiency of the classifier. The accuracy of the combination classifier is shown to be higher than that of two state-of-the-art systems tested under the same conditions. Index TermsÐHenry fingerprint classification, hidden Markov models, decision trees, neural networks, NIST database.	algorithm;artificial neural network;database;decision tree;fingerprint recognition;hidden markov model;markov chain;statistical classification	Andrew W. Senior	2001	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.954606	computer science;machine learning;decision tree;pattern recognition;data mining;artificial neural network;hidden markov model	ML	31.468206015460684	-64.94503297614384	88475
86c0a65182ba44763719c6e3d64d0d95b50b49f2	employing plsa model and max-bisection for refining image annotation	graph theory;image retrieval refining image annotation plsa em max bisection;image processing;maximum likelihood estimation;standard corel dataset plsa model max bisection image annotation probabilistic latent semantic analysis mb asymmetric modalities posterior probabilities label similarity graph weighted linear combination visual similarity rank two relaxation heuristics image retrieval initial semantic annotation task weighted label graph;visual databases graph theory image processing image retrieval maximum likelihood estimation;visual databases;image retrieval	We present a new method for refining image annotation by fusing probabilistic latent semantic analysis (PLSA) with max-bisection (MB). We first construct a PLSA model with asymmetric modalities to estimate the posterior probabilities of each annotating keyword for an image, and then a label similarity graph is built by a weighted linear combination of label similarity and visual similarity. Followed by the rank-two relaxation heuristics over the constructed label graph is employed to further mine the correlation of the keywords so as to capture the refining annotation, which plays a critical role in semantic based image retrieval. The novelty of our method mainly lies in two aspects: exploiting PLSA to accomplish the initial semantic annotation task and implementing max-bisection based on the rank-two relaxation algorithm over the weighted label graph to refine the candidate annotations generated by the PLSA. We evaluate our method on the standard Corel dataset and the experimental results are competitive to several state-of-the-art approaches.	algorithm;automatic image annotation;heuristic (computer science);image retrieval;lagrangian relaxation;linear programming relaxation;probabilistic latent semantic analysis;relaxation (iterative method)	Dongping Tian;Wenbo Zhang;Xiaofei Zhao;Zhongzhi Shi	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738823	computer vision;image processing;image retrieval;computer science;graph theory;machine learning;pattern recognition;mathematics;maximum likelihood;automatic image annotation;information retrieval;statistics	Vision	35.93693767748621	-55.13340851711363	88626
3703f65fc542d51db9efd460b5713ba0e48747f2	extended three-dimensional rotation invariant local binary patterns		This paper presents a new set of three-dimensional rotation invariant texture descriptors based on the well-known local binary patterns (LBP). In the approach proposed here, we extend an existing three-dimensional LBP based on the region growing algorithm using existing features developed exquisitely for two-dimensional LBPs (pixel intensities and differences). We have conducted experiments on a synthetic dataset of three-dimensional randomly rotated texture images in order to evaluate the discriminatory power and the rotation invariant properties of our descriptors as well as those of other two-dimensional and three-dimensional texture descriptors. Our results demonstrate the effectiveness of the extended LBPs and improvements against other state-of-the-art hand-crafted three-dimensional texture descriptors on this dataset. Furthermore, we prove that the extended LBPs can be used in medical datasets to discriminate between MR images of oxygenated and non-oxygenated brain tissues of newborn babies.	algorithm;belief propagation;computation;experiment;local binary patterns;p (complexity);pixel;randomness;region growing;ruby document format;synthetic data;synthetic intelligence;whole earth 'lectronic link	Leonardo Citraro;Sasan Mahmoodi;Angela Darekar;Brigitte Vollmer	2017	Image Vision Comput.	10.1016/j.imavis.2017.03.004	computer vision;machine learning;pattern recognition;mathematics	Vision	35.946619202010176	-59.532143534390144	88664
269c3403eeefdccdfbf554aa04b9ade193f1a8b1	the design of a nearest-neighbor classifier and its use for japanese character recognition	nearest neighbor searches;degradation;hierarchical prototype organization;geometrical information nearest neighbor classifier japanese character recognition nonparametric technique prototype reduction hierarchical prototype organization redundant category prototypes;nonparametric technique;neural networks;optical character recognition;prototypes;testing;character recognition prototypes neural networks nearest neighbor searches testing algorithm design and analysis computer science pattern classification degradation;prototype reduction;geometrical information;redundant category prototypes;nearest neighbor;pattern classification;computer science;japanese character recognition;nearest neighbor classifier;character recognition;algorithm design and analysis	The nearest neighbor (NN) approach is a powerful nonparametric technique for pattern classi cation tasks. Although the brute-force NN algorithm is simple and has high accuracy, its computation cost is usually very expensive, especially for applications such as Japanese character recognition in which the number of categories is large. Many methods have been proposed to improve the efciency of NN classi ers by reducing the number of prototypes and speeding up NN search. In this paper, algorithms for prototype reduction, hierarchical prototype organization and fast NN search are described. To remove redundant category prototypes and to avoid redundant comparisons, the algorithms exploit geometrical information of a given prototype set which is represented approximately by computing k-nearest/farthest neighbors of each prototype. The performance of a NN classi er using those algorithms for Japanese character recognition is reported. Given a large Japanese character training set, only a small portion of samples in the set are selected as prototypes. The fast NN search algorithm works as accurately as the straightforward algorithm while the average number of comparisons is about two third of that in the straightforward algorithm. The average number of comparisons is further reduced to less than one third of total number of prototypes if prototypes are organized hierarchically.	computation;k-nearest neighbors algorithm;nearest neighbour algorithm;nearest-neighbor interpolation;optical character recognition;prototype;search algorithm;set packing;software prototyping;test set	Tao Hong;Stephen W. K. Lam;Jonathan J. Hull;Sargur N. Srihari	1995		10.1109/ICDAR.1995.598992	algorithm design;speech recognition;degradation;computer science;machine learning;pattern recognition;prototype;software testing;optical character recognition;k-nearest neighbors algorithm;artificial neural network	AI	31.725450511204553	-65.43568976363932	88708
ccf80f7db2f91c0a2d12b5df26e3625484cad3fc	augmenting deep convolutional neural networks with depth-based layered detection for human detection	robot sensing systems;image segmentation;neural networks;classification algorithms;mathematical model;cameras	Deep convolutional neural networks are being increasingly deployed for image classification tasks as they can learn sensor and environmental independence from large quantities of training data. Most, however, have focused on classifying uploaded photographs rather than the often occluded, arbitrary height and camera angles images found commonly in robotic applications. In this work, we look at the performance of the popular AlexNet architecture to detect people in different robotic scenarios using different sensors and/or environments. Furthermore, we demonstrate how fusing this architecture with the depth-based layered detection system, a more traditional geometric feature-based classifier, leads to significant improvements in classification precision/recall, whether working with depth data alone or a combination of depth and RGB images.	artificial neural network;computer vision;convolutional neural network;robot;sensor;statistical classification	E. Martinson;V. Yalla	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759182	statistical classification;computer vision;computer science;artificial intelligence;machine learning;pattern recognition;mathematical model;image segmentation;artificial neural network	Robotics	29.562054292401637	-53.11228441796294	88715
1956e3ca97c376626aff5a7fde25c7cdac635ed0	3d face recognition by modeling the arrangement of concave and convex regions	3d face recognition;modeling technique;three dimensional;graph representation;attributed relational graph;face modeling;spatial relationships;facial expression;structural similarity	In this paper, we propose an original framework for three dimensional face representation and similarity matching. Basic traits of a face are encoded by extracting convex and concave regions from the surface of a face model. A compact graph representation is then constructed from these regions through an original modeling technique capable to quantitatively measure spatial relationships between regions in a three dimensional space and to encode this information in an attributed relational graph. In this way, the structural similarity between two face models is evaluated by matching their corresponding graphs. Experimental results on a 3D face database show that the proposed solution attains high retrieval accuracy and is reasonably robust to facial expression and pose changes.	concave function;three-dimensional face recognition	Stefano Berretti;Alberto Del Bimbo;Pietro Pala	2006		10.1007/978-3-540-71545-0_9	spatial relation;three-dimensional space;computer vision;computer science;structural similarity;machine learning;pattern recognition;graph;facial expression	Vision	39.00928160491016	-57.38049688724419	88755
e4d33362b4f99ab77fd6ceaafa183c087c79faea	design and implementation of a high performance pedestrian detection	support vector machines;image classification;support vector machines feature extraction haar transforms image classification pedestrians;high performance pedestrian detection implementation support vector machines histogram of oriented gradients hog svm haar cascade feature based tracking candidates identification part based classification pedestrian detection system high performance pedestrian detection design;pedestrians;feature extraction;support vector machines feature extraction training robustness head detectors vehicles;haar transforms	Research on pedestrian detection system still presents a lot of space for improvements, both on speed and detection accuracy. This paper presents a full implementation of a pedestrian detection system, using a part-based classification for the candidates identification and a feature based tracking for increasing the result robustness. The novelty of this approach relies on the use of part-based approach with a combination of Haar-cascade and HOG-SVM. Tests have been conducted using standard datasets showing results aligned with those of the other state-of-the-art systems available in literature. Real world tests also show high speed performance.	haar wavelet;pedestrian detection	Antonio Prioletti;Paolo Grisleri;Mohan Manubhai Trivedi;Alberto Broggi	2013	2013 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2013.6629662	computer vision;engineering;machine learning;pattern recognition;feature	Vision	32.41280216241916	-55.662775178511744	88950
3de71ddc07619c0dd6bbaa3f7b412a9262a0e761	discriminatively trained and-or tree models for object detection	directed graphs;dynamic programming;2010 detection benchmarks discriminatively trained and or tree models object detection reconfigurable and or tree models weakly annotated data geometry space image lattice shape primitives directed a cyclic and or graph unsupervised sub category learning object or node globally optimal parse trees dynamic programming algorithm structural parameters training structural svm framework pascal voc 2007;latent structural svm;object detection space exploration lattices shape training support vector machines deformable models;latent structural svm and or graph part based representation object detection;object detection directed graphs dynamic programming learning artificial intelligence;part based representation;learning artificial intelligence;and or graph;object detection	This paper presents a method of learning reconfigurable And-Or Tree (AOT) models discriminatively from weakly annotated data for object detection. To explore the appearance and geometry space of latent structures effectively, we first quantize the image lattice using an over complete set of shape primitives, and then organize them into a directed a cyclic And-Or Graph (AOG) by exploiting their compositional relations. We allow overlaps between child nodes when combining them into a parent node, which is equivalent to introducing an appearance Or-node implicitly for the overlapped portion. The learning of an AOT model consists of three components: (i) Unsupervised sub-category learning (i.e., branches of an object Or-node) with the latent structures in AOG being integrated out. (ii) Weakly supervised part configuration learning (i.e., seeking the globally optimal parse trees in AOG for each sub-category). To search the globally optimal parse tree in AOG efficiently, we propose a dynamic programming (DP) algorithm. (iii) Joint appearance and structural parameters training under latent structural SVM framework. In experiments, our method is tested on PASCAL VOC 2007 and 2010 detection benchmarks of 20 object classes and outperforms comparable state-of-the-art methods.	ahead-of-time compilation;and–or tree;baseline (configuration management);benchmark (computing);concept learning;davis–putnam algorithm;directed acyclic graph;discriminative model;dynamic programming;experiment;local binary patterns;maxima and minima;modernpascal;object detection;parse tree;parsing;quantization (signal processing);tree (data structure);unsupervised learning	Xi Song;Tianfu Wu;Yunde Jia;Song-Chun Zhu	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.421	computer vision;directed graph;computer science;theoretical computer science;machine learning;dynamic programming;pattern recognition;mathematics	Vision	34.88302345901273	-53.73250267264517	89074
d58e9c94206b72de545f7720c38f87f1c8b81839	ouhands database for hand detection and pose recognition	databases;histograms;protocols;image segmentation;training;testing;biological neural networks	In this paper we propose a publicly available static hand pose database called OUHANDS and protocols for training and evaluating hand pose classification and hand detection methods. A comparison between the OUHANDS database and existing databases is given. Baseline results for both of the protocols are presented.	database	Matti Matilainen;Pekka Sangi;Jukka Holappa;Olli Silvén	2016	2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2016.7821025	communications protocol;computer vision;computer science;pattern recognition;data mining;histogram;software testing;image segmentation;statistics	DB	31.650932370534008	-56.641190381232164	89112
58984e682a88885ba7b6abf93c27a46f71a7bd32	color eigen-subband features for endoscopy image classification	image features;k nearest neighbor classifier;lab color space;cancer;color space;sequential forward feature selection;medical image classification;medical image classification color eigen subband features endoscopy image classification feature extraction wavelet domain color channel information lab color space stationary wavelet transform k nearest neighbor classifier sequential forward feature selection;endoscopy;image classification;wavelet transforms;wavelet transform;medical image;image colour analysis;feature extraction;medical image processing;endoscopes image classification cancer discrete wavelet transforms feature extraction wavelet domain wavelet transforms statistics colonic polyps colon;endoscopes;comparative study;pattern classification;biomedical image processing;k nearest neighbor;feature selection;color channel information;pattern classification cancer wavelet transforms biomedical image processing image classification;stationary wavelet transform;classification accuracy;wavelet domain;color eigen subband features;wavelet transforms endoscopes feature extraction image classification image colour analysis medical image processing;leave one out	This paper presents a new image feature extraction approach in the wavelet domain. We incorporate color-channel information of the LAB color space into the feature extraction process by computing variances from decorrelated detail subbands of the stationary wavelet transform. We evaluate our approach on a medical image classification problem using a k-nearest neighbor classifier and sequential forward feature selection. our experimental results, which include a comparative study to the popular color wavelet energy correlation signatures show that we can produce highly discriminative feature sets in terms of leave-one-out classification accuracy.	antivirus software;channel (digital image);color space;computer vision;database;eigen (c++ library);feature (computer vision);feature extraction;feature selection;k-nearest neighbors algorithm;nearest neighbour algorithm;standard widget toolkit;stationary process;stationary wavelet transform	Roland Kwitt;Andreas Uhl	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4517678	computer vision;feature detection;speech recognition;computer science;machine learning;pattern recognition;mathematics;feature selection;feature;wavelet transform	Vision	36.389248171250905	-62.19951157819209	89207
584114d561c40fe9cd97996c37a0c6344b89aea8	saliency detection via combining region-level and pixel-level predictions with cnns		• Salient Map Fusion: The region-level salient map and the pixel-level salient map are computed by using different information of images. They are complementary and can be fused to further improve the performance. Visual saliency detection aims to highlight the most important object regions in an image. Numerous image processing applications incorporate the visual saliency to improve their performance: – image segmentation, cropping, object detection, image retrieval, etc. This paper proposes a novel saliency detection method by combining region-level saliency estimation and pixel-level saliency prediction with CNNs (denoted as CRPSD). Extensive quantitative and qualitative experiments on four public benchmark datasets demonstrate that the proposed method greatly outperforms the state-of-the-art saliency detection approaches. INTRODUCTION	benchmark (computing);experiment;image processing;image retrieval;image segmentation;map (higher-order function);object detection;pixel	Youbao Tang;Xiangqian Wu	2016		10.1007/978-3-319-46484-8_49	computer vision;computer science;artificial intelligence;kadir–brady saliency detector;machine learning	Vision	31.127610816565046	-55.29815615832053	89258
e035fc20d462817019dbc691d79aa918b10c4718	comparative study of color feature for particle filter based object tracking	probability gaussian processes image colour analysis object tracking particle filtering numerical methods;probability;gaussian processes;color;color histogram;particle filter;image colour analysis;abstracts;object tracking;particle filter object tracking color histogram;hsv color model comparative study color feature particle filter based object tracking adopted tracking model nonlinear problem nongaussian problem color models rgb ycbcr color quantization quantized color components tracked object bhattacharyya object distance predicted position posterior probability evaluation metrics displacement error der center distance measure cdm object tracking system feature selection;abstracts color;particle filtering numerical methods	A comparative study of color feature for object tracking is presented in this paper. The adopted tracking model is the particle filter, which is proven very successful for non-linear and non-Gaussian problems. The color models under study include RGB, HSV and YCbCr. Color quantization is carried out in three components and the histogram is obtained based on the quantized color components for the tracked object. Bhattacharyya distance of object and the predicted position of the object by the particle filter is used to find the posterior probability of particle filter, which is used to update the state of the filter. The evaluation metrics include Displacement Error (DER), Center Distance Measure (CDM). Experimental results show that the object tracking system with the feature selected from HSV color model outperform the system from other two color features.	color quantization;displacement mapping;nonlinear system;particle filter;tracking system;trionic	Hong-Ying Shen;Shuifa Sun;Xian-Bing Ma;Yichun Xu;Bang Jun Lei	2012	2012 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2012.6359509	demosaicing;color histogram;computer vision;color quantization;speech recognition;hsl and hsv;color normalization;particle filter;computer science;video tracking;pattern recognition;probability;gaussian process;mathematics;color balance;histogram equalization;statistics	Robotics	35.86123292859716	-57.58506274916305	89273
88ff7dee894810d632c467d5545a611c3df84a99	a biologically inspired visual pedestrian detection system	databases;pre defined nonlinear derivative filters;support vector machines feature extraction nonlinear filters object detection pattern classification;nonlinear filters;detectors;neural networks;support vector machines;training;information filtering;data mining;daimlerchrysler pedestrian classification benchmark database;computer vision;computer architecture;artificial neural networks;adaboost classifiers biologically inspired visual pedestrian detection system convolutional neural network architecture feature extraction pre defined nonlinear derivative filters pattern classifier daimlerchrysler pedestrian classification benchmark database support vector machines;feature extraction;spatial databases;pedestrian detection;classification algorithms;pattern classification;convolutional neural network architecture;support vector machine classification;pattern classifier;network architecture;neurons feature extraction training classification algorithms artificial neural networks databases data mining;neurons;support vector machine;adaboost classifiers;information filters;object detection;neural network;biologically inspired visual pedestrian detection system	In this paper, we present a biologically inspired method for detecting pedestrians in images. The method is based on a convolutional neural network architecture, which combines feature extraction and classification. The proposed network architecture is much simpler and easier to train than earlier versions. It differs from its predecessors in that the first processing layer consists of a set of pre-defined nonlinear derivative filters for computing gradient information. The subsequent processing layer has trainable shunting inhibitory feature detectors, which are used as inputs to a pattern classifier. The proposed pedestrian detection system is evaluated on the DaimlerChrysler pedestrian classification benchmark database and its performance is compared to the performance of support vector machines and Adaboost classifiers.	adaboost;artificial neural network;benchmark (computing);convolutional neural network;feature detection (computer vision);feature detection (web development);feature extraction;gradient descent;network architecture;nonlinear system;pedestrian detection;sensor;statistical classification;support vector machine;wavelet	Fok Hing Chi Tivive;Abdesselam Bouzerdoum	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633872	support vector machine;computer science;machine learning;pattern recognition;data mining;artificial neural network	Robotics	26.036728915984863	-61.29051488215554	89277
d441decd05e4513caae83e678e97e3be451b7fed	body part boosting model for carried baggage detection and classification	carrying baggage detection and classification;scalable histogram of oriented gradient;video surveillance;part model;parallelogram haar like feature;mixture model;boosting machine;support vector machine;body part baggage spatial model	In the automatic video surveillance system, the detection of a human carrying baggage is a potentially important objective for security and monitoring purposes in the public spaces. This paper introduces a new approach for detecting and classifying baggage carried by a human on the images. It utilizes the spatial information of the baggage in reference to the body of the human carrying it. A human-baggage detector is modeled by the body parts of a human, including the head, torso, leg, and baggage parts. The feature descriptors are extracted for each part based on its characteristics and these features are further trained using a support vector machine (SVM) classifier. A mixture model is built specifically for the baggage part due to a significant variation in shape, size, color, and texture. The boosting strategy constructs a strong classifier by combining a set of weak classifiers which are obtained by training the body part. The proposed method has been extensively evaluated using the public datasets. The experimental results confirm that the proposed method is viable for a state-of-the-art in the carried baggage detection and classification system.		Wahyono;Joko Hariyono;Kang-Hyun Jo	2017	Neurocomputing	10.1016/j.neucom.2016.10.038	support vector machine;computer vision;speech recognition;computer science;machine learning;pattern recognition;mixture model	Vision	33.18081661119707	-56.77018995723473	89285
edaccb0494d0a42687d96736ec509f97f914812b	driver fatigue detection system based on dsp platform		Due to non-invasiveness, monitoring driver state in computer vision (CV) has become a major way to detect driver fatigue. In contrast to other researches, we brought the driver fatigue detection system on the basis of DSP platform, which can make contribution to application on the integrated system for vehicle. However, the conventional system cannot easily be transplanted into DSP due to its storage and computation capacity. Therefore, designing an algorithm that can detect the fatigue efficiently is goal in this study. As the most important part of system, the geometric relationship and shape information within near frontal face is employed in the eye detection part, which depicts the eyebrow, eye and nose. In experiment part, a self-made database is assembled to test the performance of system. As the results of experiment, the detection rate of eye is achieved at 92.71 % and driver fatigue state is obtained at 97.5 %.		Zibo Li;Fan Zhang;Guangmin Sun;Dequn Zhao;Kun Zheng	2016		10.1007/978-3-319-27674-8_5	digital signal processing;artificial intelligence;computer science;computer vision;computation;eyebrow	EDA	31.55215021197044	-59.47978090377381	89296
8645a7ff78dc321e08dea6576c04f02a3ce158f9	sequential deep learning for disaster-related video classification		Videos serve to convey complex semantic information and ease the understanding of new knowledge. However, when mixed semantic meanings from different modalities (i.e., image, video, text) are involved, it is more difficult for a computer model to detect and classify the concepts (such as flood, storm, and animals). This paper presents a multimodal deep learning framework to improve video concept classification by leveraging recent advances in transfer learning and sequential deep learning models. Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) models are then used to obtain the sequential semantics for both audio and textual models. The proposed framework is applied to a disaster-related video dataset that includes not only disaster scenes, but also the activities that took place during the disaster event. The experimental results show the effectiveness of the proposed framework.	computer simulation;deep learning;experiment;image fusion;long short-term memory;modal logic;multimodal interaction;neural networks;numerical weather prediction;random neural network;recurrent neural network	Haiman Tian;Hector Cen Zheng;Shu-Ching Chen	2018	2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)	10.1109/MIPR.2018.00026	transfer of learning;modalities;feature extraction;deep learning;semantics;machine learning;recurrent neural network;data modeling;artificial intelligence;computer science	Vision	26.22461225398587	-54.124112502965914	89350
a9d49401ddc68ecc222f746c16f31a1de192bfbc	image features and natural clustering of worm body shapes and motion	body shape;image features			Wei Geng;Pamela C. Cosman;Joong-Hwan Baek;Charles C. Berry;William R. Schafer	2003			feature (computer vision);computer vision;cluster analysis;artificial intelligence;mathematics	Vision	29.96337303229156	-58.4066176513731	89447
746e962ce3d340c13e8dd504b3bba5353b3a4cd5	object signature features selection for handwritten jawi recognition	object signature;feature selection;trace transform;handwritten jawi recognition	The trace transform allows one to construct an unlimited number of image features that are invariant to a chosen group of image transformations. Object signature that is in the form of string of numbers is one kind of the transform features. In this paper, we demonstrate a wrapper method along with several ranking evaluation measurements to select useful features for the recognition of handwritten Jawi images. We compare the result of the recognition with those obtained by using methods where features are randomly selected or no feature selection at all. The proposed methods seem to be most promising.		Mohammad Faidzul Nasrudin;Khairuddin Omar;Choong-Yeun Liong;Mohamad Shanudin Zakaria	2010		10.1007/978-3-642-14883-5_88	speech recognition;computer science;machine learning;pattern recognition;feature selection	Vision	34.9600597868507	-61.74938210776902	89574
5048cdcafb4d9910ed29b81870a97942f76fd3fd	leaf vein segmentation using odd gabor filters and morphological operations		Leaf vein forms the basis of leaf characterization and classification. Different species have different leaf vein patterns. It is seen that leaf vein segmentation will help in maintaining a record of all the leaves according to their specific pattern of veins thus provide an effective way to retrieve and store information regarding various plant species in database as well as provide an effective means to characterize plants on the basis of leaf vein structure which is unique for every species. The algorithm proposes a new way of segmentation of leaf veins with the use of Odd Gabor filters and the use of morphological operations for producing a better output. The Odd Gabor filter gives an efficient output and is robust and scalable as compared with the existing techniques as it detects the fine fiber like veins present in leaves much more efficiently.	algorithm;gabor filter;gabor wavelet;mathematical morphology;scalability;statistical classification	Vini Katyal;Aviral	2012	CoRR		computer vision	Vision	32.54738556057936	-60.9218134557367	89602
26f6a53a17527072af4cc76afe98d65a8360cdfe	a new method for segmentation of pre-detected devanagari words from the scene images: pihu method		Abstract To date, only one method has reported on the segmentation of pre-detected Devanagari words from natural scenic images. The present paper discusses the limitations of the existing method and proposes a new robust method called the “Pihu method” (Pihu is the lovely daughter of the first author). The efficiency of the proposed method over the existing approach is demonstrated by considering a challenging dataset of natural scenic images of Devanagari script. The study shows that the character segmentation accuracy of the Pihu method was 92.11% and that of the existing method was 55.77%. The results show that it takes the Pihu method 2.70 s to segment an image, whereas the existing method takes 4.76 s.		Khushneet Jindal;Rajiv Kumar	2018	Computers & Electrical Engineering	10.1016/j.compeleceng.2017.12.017	computer vision;real-time computing;devanagari;image processing;computer science;segmentation;artificial intelligence	Robotics	36.83157755817492	-65.90034041179806	89630
b3154d981eca98416074538e091778cbc031ca29	pedestrian attribute analysis using a top-view camera in a public space	human attributes;surveillance;gender classification;bag possession classification	In this paper, we propose a method to analyze gender of the pedestrian and whether he or she has a baggage or not in a public space. The challenging part of this work is we only use top-view camera images to protect the pedestrians’ privacy. We focused on temporal changes in their position, shape, and contours over the frames because their appearances do not provide much information. We extracted the pedestrians' features using their position, area, aspect ratio, histogram of oriented gradients (HoG), and Fourier descriptors. The temporal information was taken into consideration by employing Gaussian mixture models (GMM), GMM universal background model (GMM-UBM), and bag of features (BoF) model. The attributes were classified by using support vector machines (SVM). We conducted experiments using 60-minute video captured by a top-view camera attached at an airport. Experimental results show that the classification accuracy is 69% for the gender classification and 79% for baggage possession classification.	algorithm;blob detection;experiment;fast fourier transform;google map maker;gradient;histogram of oriented gradients;mixture model;pedestrian detection;simple features;support vector machine	Toshihiko Yamasaki;Tomoaki Matsunami	2012		10.1007/978-3-642-27355-1_50	computer vision;simulation;pattern recognition;computer security	Vision	33.26559100739144	-57.1122318576103	89766
f6b3f1ad00a754633845bd2583702f4bda045bcf	content-based image classification and retrieval: a rule-based system using rough sets framework	cbir;rule based system;rough set theory;image classification;classification;data storage;image acquisition;feature extraction;vector;rough set;image datasets;knowledge discovery	Advances in data storage and image acquisition technologies have enabled the creation of large image datasets. Thus, it is necessary to develop appropriate information systems to efficiently manage these datasets. Image classification and retrieval is one of the most important services that must be supported by such systems. The most common approach used is content-based image retrieval (CBIR) systems. This paper presents a new application of rough sets to feature reduction, classification, and retrieval for image databases in the framework of content-based image retrieval systems. The suggested approach combines image texture features with color features to form a powerful discriminating feature vector for each image. Texture features are extracted, represented, and normalized in an attribute vector, followed by a generation of rough set dependency rules from the real value attribute vector. The rough set reduction technique is applied to find all reducts with the minimal subset of attributes associated with a class label for classification.	computer data storage;content-based image retrieval;database;feature vector;image texture;information system;rough set;rule-based system	Jafar M. Ali	2007	IJIIT	10.4018/jiit.2007070103	rule-based system;image texture;feature detection;visual word;rough set;image retrieval;computer science;machine learning;pattern recognition;data mining;knowledge extraction;automatic image annotation;information retrieval	Vision	38.82534364635528	-61.23676676163621	89813
9721d78a4788394e77068b24ed5ec42ae0fab965	multimodal video copy detection applied to social media	near duplicate videos;multimedia processing;multimedia information retrieval;video copy detection;youtube;multimedia data;detection algorithm;social media;video search	Reliable content-based copy detection algorithms (CBCD) are at the core of effective multimedia data management and copyright enforcement systems. CBCD techniques focus on detecting videos that are identical to or transformed versions of an original video. The fast growth of online video sharing services challenges state-of-the-art copy detection algorithms as they need to be: able to deal with vast amounts of data, computationally efficient and robust to a wide range of image and audio transformations. In this paper, we present two related multimodal CBCD algorithms that effectively fuse audio and video information by means of a compact multimodal signature based on audio and video global descriptors. We validate our algorithms with a benchmark database (MUSCLE-VCD) and obtain over a 14% relative improvement with respect to state-of-the-art systems. In addition, we illustrate the performance of our approach in a video view-count re-ranking task with YouTube data.	algorithm;algorithmic efficiency;benchmark (computing);multimodal interaction;sensor;social media;video clip;video copy detection	Xavier Anguera Miró;Pere Obrador;Nuria Oliver	2009		10.1145/1631144.1631157	video compression picture types;computer science;video tracking;multimedia;video processing;internet privacy;world wide web	Web+IR	35.05401919278385	-52.65232244564061	89909
b5c62eba74773d87cca7b5adaee49f32c3e4819b	multi-scale object retrieval via learning on graph from multimodal data	multi scale;object retrieval;multimodal data;learning on graph	Object retrieval has attracted much research attention in recent years. Confronting object retrieval, how to estimate the relevance among objects is a challenging task. In this paper, we focus on view-based object retrieval and propose a multi-scale object retrieval algorithm via learning on graph from multimodal data. In our work, shape features are extracted from each view of objects. The relevance among objects is formulated in a hypergraph structure, where the distance of different views in the feature space is employed to generate the connection in the hypergraph. To achieve better representation performance, we propose a multi-scale hypergraph structure to model object correlations. The learning on graph is conducted to estimate the optimal relevance among these objects, which are used for object retrieval. To evaluate the performance of the proposed method, we conduct experiments on the National Taiwan University dataset and the ETH dataset. Experimental results and comparisons with the state-of-the-art methods demonstrate the effectiveness of the proposed method.	algorithm;domain-driven design;experiment;feature vector;multimodal interaction;relevance	Yongsheng Zhang;Tsuyoshi Yamamoto;Yoshinori Dobashi	2016	Neurocomputing	10.1016/j.neucom.2016.05.053	computer vision;method;object model;computer science;machine learning;pattern recognition	Vision	35.50638970787073	-54.41530383098856	89949
f6feb0e9ca8992d8a03f3711d9ca1c98a11559e7	a novel bag generator for image database retrieval with multi-instance learning techniques	pattern clustering;image generation image databases information retrieval image retrieval pixel merging scattering laboratories content based retrieval image converters;image database;clustered blocks bag generator image database retrieval multiinstance learning content based image retrieval cbir query processing imabag pixel clustering;feature extraction;multi instance;learning artificial intelligence;feature extraction learning artificial intelligence content based retrieval image retrieval visual databases pattern clustering;content based image retrieval;content based retrieval;visual databases;image retrieval	In multi-instance learning, the training examples are bags composed of instances without labels and the task is to predict the labels of unseen bags through analyzing the training bags with known labels. In content-based image retrieval (CBIR), the query is ambiguous because it is hard to ask the user precisely specify what he or she wants. Such kind of ambiguity can be gracefully dealt with by multi-instance learning techniques, and previous research shows that bag generators can significantly influence the performance of a CBIR system. In this paper, a novel bag generator named ImaBag is presented, where the pixels of each image are first clustered based on their color and spatial features and then the clustered blocks are merged and converted into a specific number of instances. Experiments show that ImaBag achieves comparable results to some existing bag generators but is more efficient in retrieving images from databases.	ambiguous grammar;content-based image retrieval;database;experiment;graceful exit;pixel	Zhi-Hua Zhou;Min-Ling Zhang;Ke-Jia Chen	2003		10.1109/TAI.2003.1250242	image texture;computer vision;visual word;feature extraction;image retrieval;computer science;artificial intelligence;pattern recognition;automatic image annotation;information retrieval	Vision	34.32237133793417	-54.071534780307054	90018
9e4f11a4f9655f2218194ef727d6ad466c74d961	handwritten numeral recognition using gradient and curvature of gray scale image	feature vector;clustering;feature extraction;curvature feature;gradient feature;numeral recognition	In this paper, the authors study on the use of gradient and curvature of the gray scale character image to improve the accuracy of handwritten numeral recognition. Three procedures, based on curvature coefficient, bi-quadratic interpolation and gradient vector interpolation, are proposed for calculating the curvature of the equi-gray scale curves of an input image. Then two procedures to compose a feature vector of the gradient and the curvature are described. The efficiency of the feature vectors are tested by recognition experiments for the handwritten numeral database IPTP CDROM1 and NIST SD3 and SD7. The experimental results show the usefulness of the curvature feature and recognition rate of 99.49% and 98.25%, which are one of the highest rates ever reported for these databases (H. Kato et al., Technical Report of IEICE, PRU95-3, 1995, p. 17; R.A. Wilkinson et al., Technical Report NISTIR 4912, August 1992; J. Geist et al., Technical Report NISTIR 5452, June 1994), are achieved, respectively.	gradient;grayscale	Meng Shi;Yoshiharu Fujisawa;Tetsushi Wakabayashi;Fumitaka Kimura	2002	Pattern Recognition	10.1016/S0031-3203(01)00203-5	speech recognition;feature vector;feature extraction;computer science;machine learning;pattern recognition;mathematics;cluster analysis	Vision	34.037761576310935	-64.30282624581436	90105
c51860725254df20b2ad6711d531c9329c530b83	head gesture recognition system using adaboost algorithm with obstacle detection	geometry;feature extraction;video signal processing computer vision feature extraction gesture recognition image capture image classification image filtering image sequences learning artificial intelligence object detection;face;face detection;gesture recognition face detection wheelchairs feature extraction face geometry;gesture recognition;face detection head gesture recognition intelligent systems computer vision head gesture interface;wheelchairs;computer vision head gesture recognition system adaboost algorithm obstacle detection video sequences data acquisition image capturing filtering feature extraction classifier cascade	Head gesture recognition is the challenging task of research in the computer vision, mainly for the purpose of head gesture recognition of handicapped peoples using intelligent wheelchair or for the security purpose and human computer interaction. In this paper, an attempt is made to propose a system to recognize head gesture in real time from the video sequences using adaboost algorithm. The proposed system is based on a sequence of four main stages: data acquisition that is image capturing, pre processing means filtering, feature extraction that is rectangular features and a parallel stage with a cascade of classifiers design and classification. Various experiments were performed and the results demonstrate that, the system can successfully recognize head gestures using adaboost algorithm. The gesture recognition from the video sequences is one of the most important challenges in the computer vision. It offers to the system, the ability to identify, recognize and interpret the human head gestures in order to control some devices.	adaboost;algorithm;computer vision;data acquisition;experiment;feature extraction;gesture recognition;human computer;human–computer interaction	Rushikesh T. Bankar;Suresh S. Salankar	2015	2015 7th International Conference on Emerging Trends in Engineering & Technology (ICETET)	10.1109/ICETET.2015.18	computer vision;speech recognition;computer science;pattern recognition;gesture recognition;sketch recognition;feature	Vision	30.498058048800296	-59.5165103831722	90228
62307e5dabcb9fb1ec4c5376ddce39a23e9f67aa	a deep learning approach towards pore extraction for high-resolution fingerprint recognition		As high-resolution fingerprint images are becoming more common, the pores have been found to be one of the promising candidates in improving the performance of automated fingerprint identification systems (AFIS). This paper proposes a deep learning approach towards pore extraction. It exploits the feature learning and classification capability of convolutional neural networks (CNNs) to detect pores on fingerprints. Besides, this paper also presents a unique affine Fourier moment-matching (AFMM) method of matching and fusing the scores obtained for three different fingerprint features to deal with both local and global linear distortions. Combining the two aforementioned contributions, an EER of 3.66% can be observed from the experimental results.	artificial neural network;automated fingerprint identification;convolutional neural network;deep learning;distortion;enhanced entity–relationship model;feature learning;fingerprint recognition;image resolution	Hong-Ren Su;Kuang-Yu Chen;Wei Jing Wong;Shang-Hong Lai	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952518	artificial neural network;convolutional neural network;feature extraction;deep learning;pattern recognition;computer vision;computer science;fingerprint recognition;affine transformation;automated fingerprint identification;artificial intelligence;feature learning	Robotics	29.603530057038476	-55.39005787285961	90436
ff0962fd6ef868d3e3d68cc41ee87a2a59190dab	a subspace projection approach to feature extraction: the two-dimensional gabor transform for character recognition	neural networks;gabor transform;weight functions;feature extraction;image reconstruction;gabor wavelets;character recognition;subspace projection	This paper describes an application of the two-dimensional Gabor wavelets as feature extractors for character recognition with neural networks. Our approach is based on an analysis of the function performed by a single hidden unit in the first layer of a network presented with raw pixel data. This weight function can be approximated by a linear combination of basis functions from a fixed set. We establish the duality between this expansion and feature extraction: the projections of an image onto the same basis set play the role of precalculated features, and they are used as the input to the network. Recognizability of images reconstructed from these projections suggests that the necessary information is preserved by the corresponding feature extraction scheme. In this study, the Gabor wavelets provided the best trade-off between dimensionality reduction and quality of the reconstructed images. A local receptive field (LRF) network was trained on the NIST data base of isolated alphanumeric characters and tested on unseen parts of the same data base. The use of Gabor projections instead of original pixel data resulted in improvement from 86.35% to 89.40% for the lowercase, from 89.40% to 96.44% for the uppercase, and from 98.63% to 99.11% for digits, which corresponds to 22–66% reduction of classification error. This LRF-Gabor network became a part of a unified algorithm used by Eastman Kodak Company that finished in the tight group of leaders at the U.S. Census Bureau/NIST First OCR Systems Competition.	feature extraction;optical character recognition	Alexander Shustorovich	1994	Neural Networks	10.1016/0893-6080(94)90010-8	iterative reconstruction;gabor transform;computer vision;feature extraction;computer science;machine learning;pattern recognition;mathematics;gabor wavelet;artificial neural network	AI	35.285484373313686	-53.78769181870113	90541
69a334ccefcc37c9705aff284173e5b38fbaf8f2	balancing privacy and safety: protecting driver identity in naturalistic driving video data	de identification;human factors;driver safety;privacy	Naturalistic driving dataset is at the heart of automotive user interface research, detecting/measuring driver distraction, and many other driver safety related studies. Recent advances in the collection of large scale naturalistic driving data include the second Strategic Highway Research Program (SHRP2) consisting of more than 3000 subjects and the 100-Car study. Public access to such data, however, is made difficult due to personal identifiable information and protection of privacy. We propose de-identification filters for protecting the privacy of drivers while preserving sufficient details to infer driver behavior, such as the gaze direction, in naturalistic driving videos. Driver's gaze estimation is of particular interest because it is a good indicator of driver's visual attention and a good predictor of driver's intent. We implement and compare de-identification filters, which are made up of a combination of preserving eye regions, superimposing head pose encoded face mask and replacing background with black pixels, and show promising results.	de-identification;kerrison predictor;pixel;privacy;sensor;user interface	Sujitha Martin;Ashish Tawari;Mohan Manubhai Trivedi	2014		10.1145/2667317.2667325	simulation;engineering;internet privacy;computer security	HCI	27.902162993716264	-62.57701246633425	90546
db3566bcdca631575c4cd27ee0eadd919f064d42	color drop-out binarization method for document images with color shift	binarization;mathematical morphology;form;image classification;color shift;morphology color shift binarization form;morphology;image colour analysis;document image processing;image color analysis accuracy character recognition data mining image recognition color morphology;character recognition accuracy color drop out binarization method document images color shift phenomena camera device stand type scanner character recognition processes pseudocolor pixel generation scanned image pseudocolor problem line areas character areas form structure analysis morphological processing background subtracted image pixel character strings character lines dynamic color classification handwritten string preprints;cameras;mathematical morphology cameras document image processing image classification image colour analysis	"""A novel method using """"color drop-out"""" for document images with """"color shift"""" is proposed. Color shift phenomena sometimes occur in document images captured by a camera device or stand type scanner. It adversely affects the binarization and character recognition processes, because it generates pseudo color pixels on scanned image, which do not exist on the original document. To solve the """"pseudo color problem,"""" a binarization method based on the following three calculation steps is proposed. First, line and character areas are estimated coarsely by using form structure analysis and subtracting background from images, second, the color shift is removed by using morphological processing, third, each pixel of the background subtracted images is discriminated into character strings and lines precisely by dynamic color classification. Several character recognition experiments using low quality form samples (in which handwritten strings overlap form lines and preprints) were performed. According to the experimental results, the proposed method attains character recognition accuracy of 94.3%, which is 5.2pt. higher than that attained by a conventional method."""	binary image;experiment;image scanner;image subtraction;optical character recognition;pixel	Minenobu Seki;Eisuke Asano;Tsukasa Yasue;Hiroto Nagayoshi;Hiroshi Shinjo;Takeshi Nagasaki	2013	2013 12th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2013.32	color co-site sampling;color histogram;false color;rgb color model;computer vision;contextual image classification;speech recognition;mathematical morphology;color normalization;color depth;color image;morphology;binary image;computer science;high color;color balance;color space;computer graphics (images)	Vision	38.409525479804884	-65.87080304030715	90570
d6556d9a021cf9198d4de0dcbf89b8a47a9e05e9	weakly-supervised discriminative patch learning via cnn for fine-grained recognition		Trending research on fine-grained recognition gradually shifts from traditional multistage frameworks to an endto-end fashion with convolutional neural network (CNN). Many previous end-to-end deep approaches typically consist of a recognition network and an auxiliary localization network trained with additional part annotations to detect semantic parts shared across classes. In this paper, without the cost of extra semantic part annotations, we advance by learning class-specific discriminative patches within the CNN framework. We achieve this by designing a novel asymmetric two-stream network architecture with supervision on convolutional filters and a non-random way of layer initialization. Experimental results show that our approach is able to find high-quality discriminative patches as expected and gets comparable results to state-of-the-art on two publicly available fine-grained recognition datasets.	artificial neural network;commonsense reasoning;convolutional code;convolutional neural network;discriminative model;end-to-end principle;multistage amplifier;network architecture;randomness;sensor	Yaming Wang;Vlad I. Morariu;Larry S. Davis	2016	CoRR		speech recognition;computer science;artificial intelligence;machine learning;pattern recognition	AI	26.547495182343145	-52.37273054071431	90770
a8cd4bc012e08a2c6b2ca7618ec411a8d9e523e7	image processing and recognition - mixture of experts for classification of gender, ethnic origin, and pose of human faces	decision tree;mixture of experts;support vector machines pattern classification gender ethnic origin pose recognition human face recognition mixtures of experts divide and conquer granularity radial basis function neural networks inductive decision trees;computer vision;radial basis function networks;face recognition;radial basis function;humans support vector machines support vector machine classification face detection image databases classification tree analysis computer science kernel laboratories decision trees;pattern classification;support vector machine;decision trees face recognition computer vision radial basis function networks pattern classification;decision trees;divide and conquer	"""In this paper we describe the application of mixtures of experts on gender and ethnic classification of human faces, and pose classification, and show their feasibility on the FERET database of facial images. The FERET database allows us to demonstrate performance on hundreds or thousands of images. The mixture of experts is implemented using the """"divide and conquer"""" modularity principle with respect to the granularity and/or the locality of information. The mixture of experts consists of ensembles of radial basis functions (RBFs). Inductive decision trees (DTs) and support vector machines (SVMs) implement the """"gating network"""" components for deciding which of the experts should be used to determine the classification output and to restrict the support of the input space. Both the ensemble of RBF's (ERBF) and SVM use the RBF kernel (""""expert"""") for gating the inputs. Our experimental results yield an average accuracy rate of 96% on gender classification and 92% on ethnic classification using the ERBF/DT approach from frontal face images, while the SVM yield 100% on pose classification."""	connectionism;consensus (computer science);decision trees;decision tree;feret (facial recognition technology);feret database;face;locality of reference;programming paradigm;question (inquiry);radial (radio);radial basis function kernel;radial basis function network;renal blood flow, effective;statistical classification;support vector machine;trees (plant);mixture	Srinivas Gutta;Jeffrey Huang;P. Jonathon Phillips;Harry Wechsler	2000	IEEE transactions on neural networks	10.1109/72.857774	facial recognition system;computer science;machine learning;decision tree;pattern recognition;data mining	ML	25.36101414480458	-62.11993886043004	90832
b53dacdbcd5182817fcc67d9fbda639d9b081d53	low complexity image matching using color based sift	image matching;video coding augmented reality feature extraction fourier transforms image colour analysis image matching image retrieval internet;video coding;mpeg cdvs image retrieval keypoints color sift mobile visual search;internet;image colour analysis;feature extraction;fourier transforms;visual search standard low complexity image matching color based sift augmented reality image query internet search color information matching accuracy mobile visual search applications strict constraints feature extraction computational resources system accuracy sift extraction technique mpeg compact descriptors;augmented reality;image color analysis abstracts servers indexes force painting;image retrieval	Image matching and search is gaining significant commercial importance nowadays due to various applications it enables such as augmented reality, image-queries for internet search, etc. Many researchers have effectively used color information in an image to improve its matching accuracy. These techniques, however, cannot be directly used for large scale mobile visual search applications that pose strict constraints on the size of the extracted features, computational resources and the system accuracy. To overcome this limitation, we propose a new and effective technique to incorporate color information that can use the SIFT extraction technique. We conduct our experiments on a large dataset containing around 33, 000 images that is currently being investigated in the MPEG-Compact Descriptors for Visual Search Standard and show substantial improvement compared to baseline.	augmented reality;baseline (configuration management);color;computation;computational resource;experiment;image registration;moving picture experts group;scale-invariant feature transform;web search engine	Abhishek Nagar;Ankur Saxena;Serhat Bucak;Felix C. A. Fernandes;Kong-Posh Bhat	2013	2013 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2013.6706456	fourier transform;computer vision;augmented reality;feature detection;visual word;the internet;template matching;feature extraction;image retrieval;computer science;multimedia;information retrieval	Vision	36.880213868819766	-54.570908453879746	90865
f8be312bf161bc86bf8d66b5b745ff8d0c110c12	real-time recognition of blue traffic signs designating directions	blue traffic signs recognition;decision tree;real time recognition;real time;geometric feature;color segmentation;arrow signs;real time systems	In this research we propose a real-time system to recognize blue traffic signs designating directions. This research is complementary to the previous work done on six annular red signs. The system consists of several processing steps: We firstly label the blue objects in each frame and segment them from the background. After that we try to verify if the segmented blue object is a sign candidate, and then we segment white objects within the blue object. Finally we classify the white objects by matching them to arrow patterns according to geometrical features, or reject them if no arrow pattern is matched. Classification is done using a decision tree. Processing time is about 110 ms/frame, and recognition rate is about 81%.	decision tree;embedded system;fragmentation (computing);ip fragmentation;laptop;real-time computing;real-time transcription	Mohammed Hayyan Alsibai;Yuzo Hirai	2010	Int. J. Intelligent Transportation Systems Research	10.1007/s13177-010-0010-0	computer science;artificial intelligence;decision tree	Vision	32.591039377057875	-57.34859898917898	90905
5bc17a62eb19df3ed23d23f9a1c5571c030185ca	license plate localization based on edge-geometrical features using morphological approach	object detection edge detection feature extraction image classification;edge detection;rule based classifier license plate localization edge geometrical features morphological approach malaysian car plates plate detection difference of gaussian operation sobel vertical edge mask gamma correction edge detection plate region candidates;difference of gaussian;image classification;feature extraction;rule based classifier;rule based classifier gamma correction difference of gaussian sobel vertical mask;gamma correction;object detection;sobel vertical mask	Malaysian car plates in general appear in different character styles, types (either single or double row), sizes, spacing and character counts. Such variations cause even detecting and localizing these plates a difficult problem. The problem of localization is aggravated further during night time due to poor illumination. In this paper, we introduce the idea of edge-geometrical features in detecting these plates. The edge part is obtained from the use of Difference of Gaussian operation followed by Sobel vertical edge mask. Prior to that, gamma correction is applied to increase the detection of edges. We then apply morphological operations to get the plate region candidates. Using these regions, together with the edge image, we calculate geometrical features of these regions and use rule-based classifier to correctly identify the true plate region. Finally, we test out method using our own data set which contained 250 images captured during day time and 100 images captured during night time. The result of the proposed method shows 96.9% success rate.	difference of gaussians;gamma correction;internationalization and localization;logic programming;mathematical morphology;sensor;sobel operator	Jinn-Li Tan;Syed Abdul Rahman Abu-Bakar;Musa Mohd Mokji	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738937	computer vision;contextual image classification;speech recognition;edge detection;feature extraction;computer science;difference of gaussians;gamma correction;pattern recognition;mathematics;canny edge detector	Robotics	35.122948955893094	-64.09109835413064	90944
c3715addc24f393f58e7133a92e98fd6df31d068	multimodal face recognition based on histograms of three local descriptors using score level fusion	multimodal face recognition casia3d face database svm classifier support vector machine image classification dimensionality reduction efm enhanced fisher linear discriminate model pca principal component analysis features vector local descriptors histogram four patch local binary patterns tplbp three patch local binary patterns lpq local phase quantization illumination problems expression variations 3d image information 2d image information score level fusion;histograms;support vector machines;three dimensional displays face recognition histograms support vector machines feature extraction face lighting;face recognition;three dimensional displays;feature extraction;support vector machines face recognition local phase quantization three patch lbp four patch lbp;support vector machines face recognition image classification image fusion principal component analysis quantisation signal;face;lighting	In this paper, we propose an efficient framework of multimodal face recognition that explores 2D and 3D information based on the score level fusion. To solve the problems of illumination and expression variations, three local methods are introduced, Local Phase Quantization (LPQ), Three-Patch Local Binary Patterns (TPLBP) and Four-Patch Local Binary Patterns (TPLBP). After applying local descriptors to the input image (2D and 3D), this latter is divided into sub-regions or rectangular blocks. Then, the histogram of each sub-region is extracted and concatenated into a single features vector. Principal Component Analysis (PCA) and Enhanced Fisher linear discriminate Model (EFM) are used to reduce the dimensionality. Classification is then performed using the robust Support Vector Machine (SVM) classifier. Finally, score level fusion is used to improve the recognition performance. Experiments are implemented on CASIA3D face database. Our results show that the proposed approach achieves very high performance with RR=98.65% and EER=0.67%.	concatenation;facial recognition system;local binary patterns;multimodal interaction;principal component analysis;support vector machine	Ammar Chouchane;Mebarka Belahcene;Abdelmalik Ouamane;Salah Bourennane	2014	2014 5th European Workshop on Visual Information Processing (EUVIP)	10.1109/EUVIP.2014.7018380	computer vision;computer science;machine learning;pattern recognition	Vision	34.874871895565896	-58.59087880538434	90956
3cfd9889de657b451e426ab3d3a181427a2f35cb	personal photo organizer based on automated annotation framework	detectors;semantic concept detector;context information;machine learning personal photo organizer automated annotation framework personal photo management context information generator semantic concept detector face recognition multilabel assignment photo collection image classification image retrieval;image classification;data mining;learning artificial intelligence digital photography face recognition image classification image retrieval;training data detectors computer science face detection face recognition context modeling image classification signal processing data engineering information management;training data;digital photography;techniques used;multimedia signal processing;photo collections;face recognition;machine learning;photo collection;image edge detection;personal photo management;feature extraction;signal processing;multilabel assignment;svm personal photo collection exif semantic concept detector face detection face recognition;svm;automated annotation framework;face;learning artificial intelligence;face detection;context information generator;main component;personal photo collection;exif;context;semantic concept;personal photo organizer;image retrieval	Current researches toward solving personal photo management suffered two problems: (1) lacking of training data, and (2) no consolidated reference for classification. In this paper, we propose an automated annotation framework to address these problems. The framework was composed by three main components: the context information generator, the semantic concept detector, and the face recognition model. By assigning multi-labels for each photo, the framework makes the photo collection more structured and searchable. Our experimental results show that the techniques used in this framework are promising.	facial recognition system;image organizer;regular expression;sensor	Kai-En Tsay;Yi-Leh Wu;Maw-Kae Hor;Cheng-Yuan Tang	2009	2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2009.165	face;support vector machine;computer vision;digital photography;training set;detector;contextual image classification;face detection;feature extraction;image retrieval;computer science;machine learning;signal processing;pattern recognition;information retrieval	Robotics	34.18362957088486	-54.472412276116046	91129
e997b1bf84706d2090bf25b42b2ff2027241f229	entropy based supervised merging for visual categorization	eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems	Bag Of visual Words (BoW) is widely regarded as the standard representation of visual information present in the images and is broadly used for retrieval and concept detection in videos. The generation of visual vocabulary in the BoW framework generally includes a quantization step to cluster the image features into a limited number of visual words. This quantization achieved through unsupervised clustering does not take any advantage of the relationship between the features coming from images belonging to similar concept(s), thus enlarging the semantic gap. We present a new dictionary construction technique to improve the BoW representation by increasing its discriminative power. Our solution is based on a two step quantization: we start with k-means clustering followed by a bottom-up supervised clustering using features’ label information. Results on the TRECVID 2007 data [8] show improvements with the proposed construction of the BoW. We equally give upperbounds of improvement over the baseline for the retrieval rate of each concept using the best supervised merging criteria.	bag-of-words model in computer vision;baseline (configuration management);categorization;cluster analysis;dictionary;entropy (information theory);k-means clustering;overfitting;quantization (signal processing);vocabulary	Usman Farrokh Niaz;Bernard Mérialdo	2012		10.1007/978-3-642-33140-4_37	computer vision;speech recognition;computer science;artificial intelligence;machine learning	Vision	34.534347043473325	-55.38570139700478	91141
3c32018d9610584f9f4a8a17638bfa26e0d1ba9b	direct modeling of image keypoints distribution through copula-based image signatures	scene recognition feature extraction cbir gaussian copulae;gaussian copulae;scene recognition;feature extraction;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;content based image analysis	Local Image Descriptors (LID) aggregation models such as Bag of Words and Fisher Vectors represent an image based on the distribution of its LIDs given a global model, e.g. a visual codebook or a Gaussian Mixture.  Inspired by Copula theory, in this paper we propose a LID-based feature that represents directly the behavior of the image LID distribution, without requiring to compute a global model. Following the definition of Copula, we represent the distribution of the image LIDs by describing, on one side, its marginals, and on the other side, a Copula function. The Copula defines the dependencies between the marginals and their mapping to a multivariate probability distribution function. We test the resulting feature for scene recognition and video retrieval (Trecvid data), showing that our approach outperforms, in both tasks, the Bag of Words and the Fisher Vectors Model.	bag-of-words model;codebook;fisher information;type signature;visual descriptor	Miriam Redi;Bernard Mérialdo	2013		10.1145/2461466.2461498	computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition	ML	37.340000206121466	-60.86817759606281	91213
c57168ffb0a7409f00404de1feca9eef1d775201	classification of offline gujarati handwritten characters	discrete wavelet transforms;support vector machines feature extraction image classification image segmentation natural languages optical character recognition statistical analysis;image segmentation;accuracy;offline gujarati handwritten character classification svm support vector machines feature vector extraction statistical method structural method geometric method spatial domain transform domain segmentation algorithm;feature extraction;svm hcr icr ocr haar wavelet daubechies wavelet freeman chain code hu s invariant moment center of mass zernike moment gradient feature;feature extraction accuracy character recognition image segmentation discrete wavelet transforms;character recognition	Intelligent Character Recognition (ICR) is a specific form of optical character recognition (OCR) dealing mostly with handwritten texts. Due to their specificity, they are usually more adept in interpreting different styles and fonts of handwriting providing eventually higher recognition rates. Factors like language constructs, amount of research on ICR pertaining to the language, etc., essentially determines the amount of success achieved in its character recognition. This research mainly deals with the recognition of Gujarati Handwritten Characters. We have considered 34 consonants and 5 vowels; a total of 39 Gujarati Characters. The structure and lexicons of the language posed a challenge during the initial phase of segmentation; for that we have proposed new algorithm for segmentation. Our segmentation algorithm is able to address these concerns effectively. Different algorithms from different domains have been considered for comparative analysis like Transform Domain (DWT, DCT and DFT), from Spatial Domain; Geometric Method (Gradient feature), Structural method (Freeman chain code) and Statistical method (Zernike Moments). We have also proposed a new Combination of Structural and Statistical methods (Freeman chain code, Hu's invariant moment and center of mass) to extract feature vectors and it results into good amount of accuracy. These extracted feature vectors were further supplied as input into Support Vector Machines and their resulting accuracies were analyzed using 10 fold cross validation. SVM performs well on data sets that have many attributes and can also handle large number of classes.	algorithm;chain code;cross-validation (statistics);discrete cosine transform;discrete wavelet transform;feature vector;gradient;holographic principle;intelligent character recognition;lexicon;online and offline;optical character recognition;qualitative comparative analysis;sensitivity and specificity;support vector machine	Swital J. Macwan;Archana N. Vyas	2015	2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2015.7275831	computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;accuracy and precision;image segmentation	Robotics	33.93632246837843	-63.92037524193757	91279
aa528ffb1c392d519f1183489eccbafbc85c10b0	multiresolution hybrid approaches for automated face recognition	discrete wavelet transforms;discrete wavelet transform;filter bank;image resolution;hidden markov model;at t and essex face databases automated face recognition multiresolution feature extraction discrete wavelet transform filter banks shmm classifiers structural hidden markov model pca classifiers principal component analysis;visual databases discrete wavelet transforms face recognition feature extraction hidden markov models image classification image resolution principal component analysis;image classification;pca classifiers;structural hidden markov model;hybrid approach;face recognition;hidden markov models;feature extraction;principal component analysis;multiresolution feature extraction;automated face recognition;at t and essex face databases;face recognition hidden markov models discrete wavelet transforms principal component analysis computer science design engineering feature extraction biometrics humans independent component analysis;filter banks;local interaction;shmm classifiers;visual databases	This paper presents an evaluation of three classifiers using the discrete wavelet transform (DWT) as a feature extractor. The thrust is to investigate the impact of DWT with its various filter banks on the HMM, PCA and SHMM classifiers. In addition, we have developed a novel approach that combines the multiresolution feature of the discrete wavelet transform with the local interactions of the facial structures expressed through the structural hidden Markov model (SHMM). Tests have been carried out on the AT&T and Essex face databases, which show that DWT/SHMM outperforms both the DWT/HMM and DWT/PCA with an 8% increase in accuracy.	computation;database;discrete wavelet transform;facial recognition system;filter bank;hidden markov model;interaction;markov chain;principal component analysis;randomness extractor;statistical model;thrust	Paul Raymond Nicholl;Abbes Amira;Djamel Bouchaffra;Ronald H. Perrott	2007	Second NASA/ESA Conference on Adaptive Hardware and Systems (AHS 2007)	10.1109/AHS.2007.77	contextual image classification;speech recognition;image resolution;feature extraction;computer science;machine learning;pattern recognition;filter bank;discrete wavelet transform;hidden markov model;principal component analysis	Vision	31.754142200346166	-59.18113374149605	91286
54c6391794928649a9c9fc2f0e64c0a0959716f4	video vehicle detection and recognition based on mapreduce and convolutional neural network		With the rapid growth of traffic video data, it is necessary to improve the computing power and accuracy of image processing. In this paper, a video vehicle detection and recognition system based on MapReduce and convolutional neural network is proposed to reduce the time-consume and improve the recognition accuracy in video analysis. First, a fast and reliable deep learning algorithm based on YOLOv2 is used to detect vehicle in real-time. And then the license plate recognition algorithm based on improved convolutional neural network is presented to recognize the license plate image extracted from the detected vehicle region. Finally, the Hadoop Video Processing Interface (HVPI) and MapReduce framework are combined to apply the video vehicle detection and recognition algorithms for parallel processing. Experimental results are presented to verify that the proposed scheme has advantages of high detection rate and high recognition accuracy, and strong ability of data processing in large-scale video data.	convolutional neural network;mapreduce	Mingsong Chen;Weiguang Wang;Shi Dong;Xinling Zhou	2018		10.1007/978-3-319-93818-9_53	image processing;convolutional neural network;computer vision;video processing;deep learning;parallel processing;artificial intelligence;computer science;data processing	AI	29.40970321405017	-54.115802085776565	91579
a588d38ec81c0337b445931eadf6f443aea13380	functional map of the world		"""We present a new dataset, Functional Map of the World (fMoW), which aims to inspire the development of machine learning models capable of predicting the functional purpose of buildings and land use from temporal sequences of satellite images and a rich set of metadata features. The metadata provided with each image enables reasoning about location, time, sun angles, physical sizes, and other features when making predictions about objects in the image. Our dataset consists of over 1 million images from over 200 countries. For each image, we provide at least one bounding box annotation containing one of 63 categories, including a """"false detection"""" category. We present an analysis of the dataset along with baseline approaches that reason about metadata and temporal views. Our data, code, and pretrained models have been made publicly available."""		Gordon Christie;Neil Fendley;James Wilson;Ryan Mukherjee	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00646	machine learning;task analysis;computer vision;feature extraction;metadata;data mining;artificial intelligence;visualization;information retrieval;functional map;computer science;minimum bounding box;annotation	Vision	32.136644821076274	-52.607336983256694	91679
c51cc9dab25d9ac42edc1f6b1d91da4f46e0c375	text enhancement with asymmetric filter for video ocr	image recognition;edge enhancement;image segmentation;colored noise;image resolution;video signal processing;edge detection;optical character recognition;performance;video compression;scale consistency;image resolution text analysis optical character recognition filtering theory video signal processing edge detection;video ocr;text analysis;gabor filters;computer vision;text characters;optical character recognition software;optical character recognition software image edge detection video compression gabor filters character recognition colored noise pixel computer vision image recognition image segmentation;gabor filter;contrast;binarization process;image edge detection;pre selected scale ranges;character recognition text enhancement asymmetric filter video ocr stripes text characters scale consistency text detection text segmentation gabor filter contrast edge enhancement pre selected scale ranges performance binarization process;pixel;text enhancement;text detection;asymmetric filter;efficient estimation;vision;character recognition;text segmentation;filtering theory;stripes	Stripes are common sub-structures of text characters, and the scale of these stripes varies little within a word. This scale consistency thus provides us with a useful feature for text detection and segmentation. In this paper a new form of filter is derived from the Gabor filter, and it is shown this filter can efficiently estimate the scales of these stripes. The contrast of text in video can then be increased by enhancing the edges of only those stripes found to correspond to a suitable scale. More specifically the algorithm presented here enhances the stripes in three pre-selected scale ranges. The resulting enhancement yields much better performance from the binarization process, which is the step required before character recognition.	algorithm;gabor filter;optical character recognition;stripes	Datong Chen;Kim Shearer;Hervé Bourlard	2001		10.1109/ICIAP.2001.957007	data compression;text segmentation;vision;computer vision;text mining;speech recognition;colors of noise;edge detection;image resolution;performance;contrast;computer science;pattern recognition;edge enhancement;image segmentation;optical character recognition;pixel	Vision	37.91730773393713	-64.0270525978531	91681
419a40ef7a62ac2a348481e01bc745edf1bbc0e2	deep learning for human part discovery in images	unmanned aerial vehicle deep learning human part discovery human body part segmentation rgb images convolutional neural networks cnn network architecture deep convolutional network image classification pascal parts dataset part segmentation datasets freiburg sitting people dataset disaster dataset;neural nets image classification image colour analysis image segmentation;image segmentation robots semantics image resolution training proposals feature extraction	This paper addresses the problem of human body part segmentation in conventional RGB images, which has several applications in robotics, such as learning from demonstration and human-robot handovers. The proposed solution is based on Convolutional Neural Networks (CNNs). We present a network architecture that assigns each pixel to one of a predefined set of human body part classes, such as head, torso, arms, legs. After initializing weights with a very deep convolutional network for image classification, the network can be trained end-to-end and yields precise class predictions at the original input resolution. Our architecture particularly improves on over-fitting issues in the up-convolutional part of the network. Relying only on RGB rather than RGB-D images also allows us to apply the approach outdoors. The network achieves state-of-the-art performance on the PASCAL Parts dataset. Moreover, we introduce two new part segmentation datasets, the Freiburg sitting people dataset and the Freiburg people in disaster dataset. We also present results obtained with a ground robot and an unmanned aerial vehicle.	active sitting;activity recognition;aerial photography;coat of arms;computer vision;convolutional neural network;deep learning;dropout (neural networks);end-to-end principle;human computer;map;network architecture;overfitting;pixel;real-time computing;real-time transcription;refinement (computing);reinforcement learning;robotics;unmanned aerial vehicle	Gabriel L. Oliveira;Abhinav Valada;Claas Bollen;Wolfram Burgard;Thomas Brox	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487304	computer vision;simulation;computer science;artificial intelligence;scale-space segmentation	Robotics	29.321771617317484	-52.81971259187053	91697
b05a188b8168482f64111a3953969a5cb82c3855	integration of design by features and feature recognition approaches through a unified model		Features are real existing constituencies of product parts that associate engineering significance with shapes. A feature-based representation of product models is an essential prerequisite to the development of a new generation of Computer Aided Design systems.	feature recognition;unified model	Teresa De Martino;Bianca Falcidieno;Franca Giannini;Stefan Haßinger;Jivka Ovtcharova	1993		10.1007/978-3-642-78114-8_27	feature;machine learning;pattern recognition;data mining	Robotics	30.10323804074788	-56.039367528425274	91762
1e01869c8bcccddadebf4700d78d7384003b8c8d	recognizing emotions from an ensemble of features	feature extraction vectors face emotion recognition training face recognition mercury metals;machine vision biometrics computer vision emotion recognition;support vector machines;emotion recognition;face recognition emotion recognition geneva multimodal emotion portrayals facial expression recognition facial analysis database face detection feature generation scale invariant feature transform motion features support vector machines;face recognition;transforms;pattern classification;transforms emotion recognition face recognition pattern classification support vector machines	This paper details the authors' efforts to push the baseline of emotion recognition performance on the Geneva Multimodal Emotion Portrayals (GEMEP) Facial Expression Recognition and Analysis database. Both subject-dependent and subject-independent emotion recognition scenarios are addressed in this paper. The approach toward solving this problem involves face detection, followed by key-point identification, then feature generation, and then, finally, classification. An ensemble of features consisting of hierarchical Gaussianization, scale-invariant feature transform, and some coarse motion features have been used. In the classification stage, we used support vector machines. The classification task has been divided into person-specific and person-independent emotion recognitions using face recognition with either manual labels or automatic algorithms. We achieve 100% performance for the person-specific one, 66% performance for the person-independent one, and 80% performance for overall results, in terms of classification rate, for emotion recognition with manual identification of subjects.	algorithm;algorithmic trading;baseline (configuration management);combined modality therapy;emotion recognition;face detection;facial recognition system;multimodal interaction;scale-invariant feature transform;support vector machine	Usman Tariq;Kai-Hsiang Lin;Zhen Li;Xi Zhou;Zhaowen Wang;Vuong Ba Lê;Thomas S. Huang;Xutao Lv;Tony X. Han	2012	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2012.2194701	facial recognition system;support vector machine;computer vision;speech recognition;feature;intelligent character recognition;computer science;machine learning;pattern recognition;gesture recognition;three-dimensional face recognition;3d single-object recognition	Vision	31.269244505151264	-58.424613173706284	91787
fcc6fe6007c322641796cb8792718641856a22a7	automatic facial makeup detection with application in face recognition	face feature extraction image color analysis shape databases image edge detection face recognition;image matching face recognition feature extraction;image matching;face matcher automatic facial makeup detection automated face recognition systems face images feature vector extraction color characteristics texture characteristic shape characteristic adaptive pre processing scheme;face recognition;feature extraction	Facial makeup has the ability to alter the appearance of a person. Such an alteration can degrade the accuracy of automated face recognition systems, as well as that of meth-ods estimating age and beauty from faces. In this work, we design a method to automatically detect the presence of makeup in face images. The proposed algorithm extracts a feature vector that captures the shape, texture and color characteristics of the input face, and employs a classifier to determine the presence or absence of makeup. Besides extracting features from the entire face, the algorithm also considers portions of the face pertaining to the left eye, right eye, and mouth. Experiments on two datasets consisting of 151 subjects (600 images) and 125 subjects (154 images), respectively, suggest that makeup detection rates of up to 93.5% (at a false positive rate of 1%) can be obtained using the proposed approach. Further, an adaptive pre-processing scheme that exploits knowledge of the presence or absence of facial makeup to improve the matching accuracy of a face matcher is presented.	algorithm;digital artifact;experiment;facial recognition system;feature vector;preprocessor;spoofing attack;statistical classification	Cunjian Chen;Antitza Dantcheva;Arun Ross	2013	2013 International Conference on Biometrics (ICB)	10.1109/ICB.2013.6612994	facial recognition system;computer vision;face detection;speech recognition;object-class detection;feature extraction;computer science;machine learning;pattern recognition;three-dimensional face recognition;face hallucination	Vision	32.94058605643547	-59.49335128583186	91928
01122cf389ad64c53396fc20f347f218cbfd96c9	a simple high accuracy approach for face recognition	principal component analysis face recognition;electoral college;face recognition voting educational institutions principal component analysis pattern recognition associative memory computer science pattern matching pollution cognitive informatics;pca approach;face recognition;principal component analysis;feret dataset;feret dataset face recognition electoral college pca approach	The theory that electoral college is more stable than direct popular vote is applied in face recognition. By simply adopting most traditional PCA approach, the experiments in this paper show a remarkably higher recognition rate than any known algorithm is reached with electoral college strategy on known FERET datasets. It indicates that a significant breakthrough can be expected by embedding electoral college with more effective face recognition algorithms.	algorithm;experiment;feret (facial recognition technology);facial recognition system	Liang Chen;Naoyuki Tokuda	2007	6th IEEE International Conference on Cognitive Informatics	10.1109/COGINF.2007.4341877	psychology;machine learning;pattern recognition;data mining	Vision	32.13565783793737	-58.150684214496515	92006
58987224f8424fd26f5f3c822bc3b2c1184f6d58	novel and tuneable method for skin detection based on hybrid color space and color statistical features	hsv	Skin detection is one of the most important and primary stages in some of image processing applications such as face detection and human tracking. So far, many approaches are proposed to done this case. Near all of these methods have tried to find best match intensity distribution with skin pixels based on popular color spaces such as RGB, CMYK or YCbCr. Results show these methods cannot provide an accurate approach for every kinds of skin. In this paper, an approach is proposed to solve this problem using statistical features technique. This approach is including two stages. In the first one, from pure skin statistical features were extracted and at the second stage, the skin pixels are detected using HSV and YCbCr color spaces. In the result part, the proposed approach is applied on FEI database and the accuracy rate reached 99.25 ± 0.2. Further proposed method is applied on complex background database and accuracy rate obtained 95.40±0.31%. The proposed approach can be used for all kinds of skin using train stage which is the main advantages of it. Low noise sensitivity and low computational complexity are some of other advantages.	color space;computational complexity theory;face detection;image processing;pixel	Reza Azad;Hamid Reza Shayegh	2014	CoRR		computer vision;simulation;mathematics;computer graphics (images)	Vision	34.61906661290209	-59.81946009462716	92057
3dcf65754c6ea49ece027964348f4ee792dad758	hierarchical joint-guided networks for semantic image segmentation		Semantic image segmentation is now an exciting area of research owing to its various useful applications in daily life. This paper introduces a hierarchical joint-guided network (HJGN) which is mainly composed of proposed hierarchical joint learning convolutional networks (HJLCNs) and proposed joint-guided and making networks (JGMNs). HJLCNs exhibit high robustness in the segmentation of unseen objects that are not contained in training categories. JGMNs are very effective in filling holes and preventing incorrect segmentation predictions. The proposed HJGNs outperform the state-of-the-art methods on the PASCAL VOC 2012 testing set, reaching a mean IU of 80.4%.	image segmentation;instruction unit	Chien-Yao Wang;Jyun-Hong Li;Seksan Mathulaprangsan;Chin-Chin Chiang;Jia-Ching Wang	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952484	robustness (computer science);artificial intelligence;scale-space segmentation;pattern recognition;semantics;computer science;segmentation-based object categorization;convolution;image segmentation;computer vision;segmentation	Vision	28.010595389140487	-52.9863502629831	92086
abe9cd7105b12daaa206a1ddec3b7f4ca859ceb9	learning fingerprint orientation fields using continuous restricted boltzmann machines	learning artificial intelligence boltzmann machines fingerprint identification gabor filters gradient methods image enhancement image matching image restoration;image matching;biometrics;image restoration;gabor filters;neurons training databases noise algorithm design and analysis noise measurement backpropagation;image enhancement;machine learning;fingerprint orientation field learning input fingerprint overlapping patches clean fingerprint images rbm fingerprint matching gradient based methods fingerprint enhancement gabor based algorithms noisy fingerprint image patch restoration local orientation field pattern learning continuous restricted boltzmann machines;continuous restricted boltzmann machines;gradient methods;learning artificial intelligence;fingerprint enhancement;boltzmann machines;machine learning fingerprint enhancement biometrics continuous restricted boltzmann machines gabor filters;fingerprint identification	We aim to learn local orientation field patterns in fingerprints and correct distorted field patterns in noisy fingerprint images. This is formulated as a learning problem and achieved using two continuous restricted Boltzmann machines. The learnt orientation fields are then used in conjunction with traditional Gabor based algorithms for fingerprint enhancement. Orientation fields extracted by gradient-based methods are local, and do not consider neighboring orientations. If some amount of noise is present in a fingerprint, then these methods perform poorly when enhancing the image, affecting fingerprint matching. This paper presents a method to correct the resulting noisy regions over patches of the fingerprint by training two continuous restricted Boltzmann machines. The continuous RBMs are trained with clean fingerprint images and applied to overlapping patches of the input fingerprint. Experimental results show that one can successfully restore patches of noisy fingerprint images.	algorithm;deep learning;fingerprint recognition;gradient method;patch (computing);restricted boltzmann machine	Mihir Sahasrabudhe;Anoop M. Namboodiri	2013	2013 2nd IAPR Asian Conference on Pattern Recognition	10.1109/ACPR.2013.37	computer vision;machine learning;pattern recognition;mathematics	Vision	27.728323421249975	-55.971032823688304	92332
46f90136f51078f4bd7bbbbed785e670a211de1b	adaptive hierarchical indexing and constrained localization: matching characteristic views	unsupervised learning;image features;object recognition;index structure;statistical analysis;indexation;hybrid system;gaussian distribution	This article presents a complete hybrid object recognition system for three-di m n sional objects using the characteristic view (ChV) idea. To apply the ChV representation me thod in a recognition system investiga tions are needed concerning the processing of large object data bases. First we present two methods to reduce the number of views in the object data base. Second we developed an accumulator (AC)-based matching strategy combined with a lo calization process. This strategy bases on a hierarchical indexing structure that uses a Gaussian distributed voting. The off-line part of the matching includes a statistical analysis of the object data base and an interface to process results of a sensor configuration analysis. The cal culat d results support the construction of an adapted layer model suitable for hierarchical indexing. Further an unsupervised learning module is introduced, that investigates the measure nt errors and adapts the system online. Results of the matching are verified by a localiza ti n tool, which uses an interpretation tree search combined by a shape from angle method and a constrained alignment technique. The ar ticle shows results with real greyscale images.	accumulator (computing);database;grayscale;online and offline;outline of object recognition;titanium nitride;unsupervised learning	Gunter Bellaire;Matthias Lübbe	1995		10.1007/3-540-60697-1_108	normal distribution;unsupervised learning;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;data mining;feature;statistics;hybrid system	Vision	38.833732951068676	-57.376245042190156	92443
24853c967f39c1e6467e78d8bd097cd9833793d5	introduction to local binary patterns: new variants and applications		This chapter provides an introduction to Local Binary Patterns (LBP) and important new variants. Some issues with LBP variants are discussed. A summary of the chapters on LBP is also presented.	local binary patterns	Sheryl Brahnam;Lakhmi C. Jain;Alessandra Lumini;Loris Nanni	2013		10.1007/978-3-642-39289-4_1	local binary patterns;facial recognition system;texture descriptor;artificial intelligence;computer science;pattern recognition	Crypto	35.38468581476802	-59.3577020264636	92627
4cdd4ed3d09acd1b2d2c8fa17c759618e40792b0	spatial datbase feasibility for facial characterization using fuzzy logic queries	facial characterization;image database;fuzzy queries;relational database;spatial database;fuzzy logic;criminal justice;feasibility study;spatial information	The human face is an important and unique characteristic that can be tracked and categorized for facial recognition use in security and criminal justice applications. This feasibility study attempts to quantify what portions of the face should be captured and how to harness the enhanced capabilities of spatial database packages to extract spatial information over traditional relational databases. In addition, this study incorporates spatial databases with research in community learned perceptions of a set of faces and their translation into fuzzy queries done by a joint database research group. If successful, users can ask both detailed and flexible questions about the relationships between spatial features of the faces such as nose length to chin position.	categorization;facial recognition system;fuzzy logic;relational database;spatial database	James L. Mastros	2006		10.1145/1185448.1185586	computer vision;database theory;computer science;data mining;database;spatial database;spatial query	DB	28.76160638610298	-61.51003759034492	92636
be98adc45107870333c6e0e69067bbda799824a2	qualitative part-based models in content-based image retrieval	qualitative volumetric primitives;shape categorization;part based model;content based image retrieval;image retrieval	A qualitative, volumetric part-based model is proposed to improve the categorical invariance and viewpoint invariance in content-based image retrieval, and a novel two-step part-categorization method is presented to build it. The method consists first in transforming parts extracted from a segmented contour primitive map and then categorizing the transformed parts using interpretation rules. The first step allows noisy extracted parts to be transformed to the domain of a simple classifier. The second step computes features of the transformed parts for categorization. Content-based image retrieval experiments using real images of complex multi-part objects confirm that a model built from the categorized parts improves both the categorical invariance and the viewpoint invariance. It does so by directly addressing the fundamental limits of low-level models.	categorization;closed-circuit television;coherence (physics);content-based image retrieval;contour line;experiment;fuzzy concept;high- and low-level;object type (object-oriented programming);part-based models;statistical classification;unrestricted grammar;volumetric display	Guillaume-Alexandre Bilodeau;Robert Bergevin	2006	Machine Vision and Applications	10.1007/s00138-006-0057-8	computer vision;visual word;image retrieval;computer science;pattern recognition;information retrieval	Vision	39.01997198763952	-55.57167845454441	92738
e9fd7caf7035ec73cf815325bfda3d6feb03cc6a	incremental learning for feature extraction filter mask used in similar pattern classification	neural networks;neural nets;gabor filters feature extraction artificial neural networks learning systems pixel mathematical model equations;gabor filters;learning systems;learning system;gabor filter;artificial neural networks;incremental learning;feature extraction;pixel;pattern classification;feature extraction filter mask;mathematical model;gabor filter incremental learning feature extraction filter mask pattern classification character recognition neural networks;pattern classification character recognition feature extraction gabor filters learning artificial intelligence neural nets;learning artificial intelligence;character recognition;neural network	The incremental learning system for a feature extraction unit in the character recognition system is described and experimental results are shown. The relationship between this learning system and neural networks (NN) are explained and the specifications of this method are described as an NN application. The improved version of this system which is related to the Gabor filter was tested and an accuracy improvement was shown in the experiments of a similar pattern classification. An important goal of this research was to observe created filter masks. It was confirmed that the visual pattern of created filter masks was reasonable for the purpose.	artificial neural network;experiment;feature extraction;gabor filter;learning vector quantization;optical character recognition;pattern recognition;statistical classification;the filter	Yoshiaki Kurosawa	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633838	computer vision;feature extraction;computer science;machine learning;pattern recognition;mathematical model;artificial neural network;pixel	Robotics	31.444485634264698	-65.04702893146066	92751
00dfd28b91ef1e1bfa2e205dadaf23325b207751	face-and-clothing based people clustering in video content	hierarchical clustering;bottom up;video people indexing;skin color;indexation;cluster system;clothing;face;invariant feature	Content-based people clustering is a crucial step for people indexing within video documents. In this paper, we investigate the use of both face and clothing features. A method of extracting a keyface for each video sequence is proposed. An algorithm based on the average of the N-minimum pair distances between local invariant features is used in order to resolve the problem of face matching. An original method for clothing matching is proposed based on 3D histogram of the dominant color. A 3-levels hierarchical bottom-up clustering that combines local invariant features, skin color, 3D histogram and clothing texture is also described. Experiments and results show the efficiency of the proposed clustering system.	3d computer graphics;algorithm;cluster analysis;digital video	Elie el Khoury;Christine Sénac;Philippe Joly	2010		10.1145/1743384.1743435	face;correlation clustering;computer vision;computer science;canopy clustering algorithm;clothing;pattern recognition;top-down and bottom-up design;data mining;hierarchical clustering;cluster analysis	Vision	37.831026898681266	-57.31112581291411	92754
d479b119ee829640a5d39a9400c9e49ec308f780	facial recognition of identical twins	principal component analysis biometrics access control face recognition image matching;biometrics access control;image matching;probes cameras glass;face recognition;principal component analysis;local region pca facial recognition identical twins biometric identification systems commercial face matchers cognitec 8 3 2 0 verilook 4 0 pittpatt 4 2 1 baseline matcher	Biometric identification systems must be able to distinguish between individuals even in situations where the bio metric signature may be similar, such as in the case of identical twins. This paper presents experiments done in facial recognition using data from a set of images of twins. This work establishes the current state of facial recognition in regards to twins and the accuracy of current state-of-the art programs in distinguishing between identical twins using three commercial face matchers, Cognitec 8.3.2.0, VeriLook 4.0, and PittPatt 4.2.1 and a baseline matcher employing Local Region PCA. Overall, it was observed that Cognitec had the best performance. All matchers, how ever, saw degradation in performance compared to an experiment where the ability to distinguish unrelated persons was assessed. In particular, lighting and expression seemed to have affected performance the most.	baseline (configuration management);biometrics;british informatics olympiad;elegant degradation;experiment;facial recognition system;twin	Matthew Pruitt;Jason M. Grant;Jeffrey R. Paone;Patrick J. Flynn;Richard W. Vorder Bruegge	2011	2011 International Joint Conference on Biometrics (IJCB)	10.1109/IJCB.2011.6117476	facial recognition system;computer vision;machine learning;pattern recognition;three-dimensional face recognition;principal component analysis	Vision	29.808291220340966	-61.35106932706475	92798
3e0c72195f8bb3ed045d8b74b21093a1e2757faa	k-morik: mining patterns to classify cartified images of katharina		When building traditional Bag of Visual Words (BOW) for image classification, the k-Means algorithm is usually used on a large set of high dimensional local descriptors to build a visual dictionary. However, it is very likely that, to find a good visual vocabulary, only a sub-part of the descriptor space of each visual word is truly relevant for a given classification problem. In this paper, we explore a novel framework for creating a visual dictionary based on Cartification and Pattern Mining instead of the traditional k-Means algorithm. Preliminary experimental results on face images show that our method is able to successfully differentiate photos of Elisa Fromont, and Bart Goethals from Katharina Morik.		Élisa Fromont;Bart Goethals	2016		10.1007/978-3-319-41706-6_21	visual word;bag-of-words model in computer vision;artificial intelligence;visual dictionary;vocabulary;pattern recognition;contextual image classification;computer science	ML	34.26683280801089	-54.109561237866465	92834
402f6db00251a15d1d92507887b17e1c50feebca	3d facial action units recognition for emotional expression		The muscular activities caused the activation of certain AUs for every facial expression at the certain duration of time throughout the facial expression. This paper presents the methods to recognise facial Action Unit (AU) using facial distance of the facial features which activates the muscles. The seven facial action units involved are AU1, AU4, AU6, AU12, AU15, AU17 and AU25 that characterises happy and sad expression. The recognition is performed on each AU according to rules defined based on the distance of each facial points. The facial distances chosen are extracted from twelve facial features. Then the facial distances are trained using Support Vector Machine (SVM) and Neural Network (NN). Classification result using SVM is presented with several different SVM kernels while result using NN is presented for each training, validation and testing phase.	artificial neural network;facial recognition system;gaussian (software);kernel (operating system);left corner;support vector machine	N. Hussain;Hamimah Ujir;Irwandi Hipni Mohamad Hipiny;Jacey-Lynn Minoi	2017	CoRR		support vector machine;computer science;artificial intelligence;machine learning;pattern recognition;artificial neural network;emotional expression;facial expression	ML	30.973206942472157	-59.846131108768276	92854
c23bd1917badd27093c8284bd324332b8c45bfcf	personalized facial expression recognition in indoor environments	facial expression recognition;angry emotion;radial basis function networks emotion recognition face recognition feature extraction;emotion recognition;scared emotion;surprised emotion;personalized facial expression recognition;radial basis function networks;disgusted emotion personalized facial expression recognition indoor environments emotion analysis general expression models facial features radial basis function neural network emotion classification neutral emotion happy emotion angry emotion surprised emotion sad emotion scared emotion;face recognition;emotion classification;feature extraction;neutral emotion;indoor environment;radial basis function neural network;sad emotion;indoor environments;facial features;emotion analysis;facial expression;general expression models;happy emotion;disgusted emotion	Facial expression recognition is one of the most popular topics in emotion analysis. Most facial expression recognition systems are implemented using general expression models. Since facial expressions may be expressed differently by different people, inaccurate results are unavoidable. The proposed facial expression recognition system recognizes facial expressions using the facial features of an individual user. A radial basis function neural network is applied to classify seven emotions: neutral, happy, angry, surprised, sad, scared, and disgusted. Experiment results show that the proposed system can accurately identify emotions from facial expressions.	artificial neural network;facial recognition system;optical character recognition;radial (radio);radial basis function;regular expression	Chuan-Yu Chang;Yan-Chiang Huang	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596316	facial recognition system;facial action coding system;feature extraction;computer science;machine learning;emotion classification;facial expression	Vision	30.470380541035045	-59.146616746683904	92856
25246bc37acb9c43e82e57aaffd77999c20ba084	forensic retrieval of striations on fired bullets by using 3d geometric data	forensic science;ccd camera;neural network	Currently, optical devices, such as microscopes and CCD cameras, are mainly utilized for identification of bullets and tool marks in the field of forensic science. While these optical methods are easily manageable and effective, they are under great influence of illumination condition. Besides these appearance-based approaches, we can utilize 3D geometric data of tool marks that are free from lighting condition. Nevertheless, a perfect correspondence of two striation patterns is rarely encountered, even if the two bullets have been fired from the same firearm. Therefore, more robust methods are required. In this study, we propose a two-stage comparison method focused on 3D geometric. At first, we have aligned global shapes and evaluated a global shape similarity. Then small elevations are compared by neural networks; that is local matching. In this stage, rendered images under a unified lighting condition are utilized. By using 2-stage comparison, we have developed a robust method that searches for similar striation patterns.	algorithm;charge-coupled device;neural networks;topography	Atsuhiko Banno;Tomohito Masuda;Katsushi Ikeuchi	2005			mathematics;pattern recognition;computer vision;artificial intelligence;artificial neural network;striation;geometric data analysis	Robotics	38.89383982921392	-56.554224713887805	92864
d2839b5362393a765b8da429d16f9b5660bc926b	object detection in pleiades images using deep features	deep learning object detection very high resolution;automobiles;neural networks;feature extraction remote sensing automobiles object detection machine learning neural networks vehicle detection;vehicle detection;machine learning;feature extraction;remote sensing;pleiades imagery object detection pleiades images very high resolution imagery remote sensing deep learning technique computer vision deep neural network convolutional neural network car detection tree detection;vegetation automobiles geophysical image processing learning artificial intelligence neural nets object detection remote sensing;object detection	Extracting and identifying objects in very high resolution imagery has been a popular research topic in remote sensing. Since the beginning of this decade, deep learning techniques have revolutionized computer vision providing significant performance gains compared to traditional “shallow” techniques in various challenging vision problems. The training of deep neural networks usually requires very large training datasets. The advantage of using deep features is to exploit already trained Convolutional Neural Networks (CNN) in order to produce high level features without the burden of having to train a CNN from scratch. In this paper, we are investigating the use of deep features for the detection of small objects (cars and individual trees) in high resolution Pleiades imagery. Preliminary results show good detection performance and are very encouraging for future applications.	artificial neural network;computer vision;convolutional neural network;deep learning;high-level programming language;image resolution;object detection;pleiades (supercomputer)	Mohamed Dahmane;Samuel Foucher;Mario Beaulieu;F. Riendeau;Yacine Bouroubi;Mathieu Benoit	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7729396	computer vision;object-class detection;feature extraction;computer science;machine learning;artificial neural network;remote sensing	Vision	27.986137134526516	-54.000209650851424	92992
434ce9d0de592cebe3c9c4b2f54b77cdb38291b8	erratum to: quickly tracing detection for spread spectrum watermark based on effect estimation of the affine transformation [pattern recognition 38 (12) 2530]	affine transformation;pattern recognition;spread spectrum watermarking		pattern recognition	Guorui Feng;Ling-ge Jiang;Dong-Jian Wang;Chen He	2006	Pattern Recognition	10.1016/j.patcog.2005.10.003	arithmetic;computer vision;speech recognition;computer science;pattern recognition;affine transformation;harris affine region detector;mathematics	Vision	36.02259402961425	-62.42695628852425	93048
8d843fe1c7024e8ab85fb1a29c40ee93a574eebb	retrieval of hand-sketched envelopes in logo images	hierarchical clustering;morphological operation;multiple objectives	This paper introduces an approach for retrieving envelope of high-level object groupings in bi-level images with multiple objects. Motivated by studies in Gestalt theory, hierarchical clustering is used to detect the envelope and group its objects based on their spatial proximity, area, shape features and orientation. To decide the final grouping, the grouping outcomes are combined using an evidence accumulation method. The high-level boundary of the detected envelope is then extracted using morphological operations. For retrieval, the boundary of a query sketch is matched to the extracted envelopes from database images via dynamic space warping. Experiments on a set of bi-level logo images demonstrate the effectiveness of the approach.	approximation algorithm;black and burst;cluster analysis;concave function;day–stout–warren algorithm;gestalt psychology;hierarchical clustering;high- and low-level;logo;mathematical morphology;tree accumulation;xslt/muenchian grouping	Naif Alajlan	2007		10.1007/978-3-540-74260-9_39	computer vision;computer science;machine learning;pattern recognition;mathematics;hierarchical clustering	Vision	38.951263906729544	-54.87644981693428	93058
b8afbe37991c4d8d49d7ccb93f7aadcc468fef18	exploiting angular profiles signature for shape-based image classification and retrieval	object recognition;image classification;shape recognition;classification;feature extraction;shape signature;descriptors;angular profiles;image retrieval	Image classification and retrieval has significant importance in a wide variety of applications like object recognition, tracking, and content based retrieval, etc. Images usually consist of various objects which are segmented and then analysed for object-based classification and recognition. Owing to the absence of intensity and colour information, binary objects are difficult to recognise. They are usually represented using compact, geometrically invariant and robust features extracted from the object’s contour or interior region. These features form the basis for recognition and govern the overall performance of classification systems. A new shape signature is introduced in this paper for representing shapes through angular profiles signature which are extracted from objects enclosed within minimum bounding circles. We have evaluated the discriminatory capabilities of this signature in shape recognition and retrieval on two shape datasets. Experimental results indicate that the signature is able to represent shapes effectively, achieving overall accuracy of 94%.	angularjs;computation;computer vision;database;fast fourier transform;k-nearest neighbors algorithm;mbc-55x;object-based language;outline of object recognition;pixel;robustness (computer science)	Jamil Ahmad;Khan Muhammad;Zahoor Jan	2016	IJAPR	10.1504/IJAPR.2016.10000657	computer vision;heat kernel signature;machine learning;pattern recognition;mathematics;signature recognition	Vision	38.52893318605188	-59.181032054634336	93173
e5dc09a378a4915044663d1187a03b20d6595bc2	key-frame selection strategy based on edge points classification in 2d-to-3d conversion	2d-to-3d conversion;depth estimation;edge points classification;focus cue;key-frame selection	In 2D-to-3D video conversion, key-frame selection is essential and it affects the quality and workload of the conversion. In this paper, a key-frame selection method based on edge classification is proposed. In the proposed method, the candidate key-frames are selected out based on the occlusion area and feature point correspondence. The optimal key-frames are selected out from the candidates according to the edge point classification so that the depth of the key-frames can be estimated accurately and automatically referring to the depth estimation based on focus cue. Experiments show that the proposed method can bring 2D-to-3D conversion better objective and subject quality and it is promising in practical applications.	2d to 3d conversion;key frame	Jiangchuan Xie;Jiande Sun;Ju Liu;Qiaoli Hu	2013		10.1007/978-3-642-42057-3_100	pattern recognition	ML	38.64203946655622	-52.54744193531571	93231
6bf244704e6cbf2c7093e0bfbe2ec44ce86f0aec	analysis of cypriot icon faces using ica-enhanced active shape model representation	cyprus;cultural heritage;independent component analysis;iconography;byzantine;active shape model	Religious iconography is an integral component of the cultural heritage of Cyprus, which was once a part of the great Byzantine empire. On one hand, icons exhibit strict adherence to conventional symbols, poses and apparel. On the other hand, there is a great variety in the style of depiction that can be attributed to different schools and periods. This paper proposes an active shape model (ASM) based technique for icon face representation that can be used for style comparison and attribution. For centuries-old icons suffering from loss of paint, cracks and added noise from digitization artifacts, we apply an independent component analysis (ICA) technique to enhance the paintings' original work. The experimental results show that our method can effectively characterize Cypriot icons.	active shape model;independent computing architecture;independent component analysis	Guifang Duan;Neela Sawant;James Ze Wang;Dean R. Snow;Danni Ai;Yen-Wei Chen	2011		10.1145/2072298.2071898	active shape model;iconography;independent component analysis;computer science;cultural heritage	Vision	32.5332192737066	-59.909419347480394	93244
fd5742eb4ae4d118f37869ecc7dbc647acd49e66	biometric analysis for the recognition of spider species according to their webs		This work presents a biometric approach for spider identification based on transform domain and Support Vector Machines as classifier. The dataset is composed by 185 images of spider web. The goal of this work is to use the structure of spider web for identifying the kind of spider. The experiments were done using two different of segmentation blocks and the analysis of the whole and center of the spider web. The best accuracy is reached after to run the different combinations.	biometrics;canny edge detector;care-of address;discrete cosine transform;edge detection;experiment;otsu's method;preprocessor;principal component analysis;statistical classification;support vector machine;thresholding (image processing)	David Batista-Plaza;Carlos Manuel Travieso-González;Malay Kishore Dutta;Anushikha Singh	2017	2017 Tenth International Conference on Contemporary Computing (IC3)	10.1109/IC3.2017.8284286	support vector machine;artificial intelligence;computer vision;image processing;computer science;pattern recognition;spider;image segmentation;biometrics	Vision	35.37519237484874	-62.60902216088306	93288
f48406746b64ee01be315a08044d66d6a9e512c0	efficient direct mining of selective discriminative patterns for classification				Hong Cheng;Jiawei Han;Xifeng Yan;Philip S. Yu	2013			discriminative model;pattern recognition;artificial intelligence;computer science	ML	28.831237565668953	-57.17587953963381	93445
932fad3f68a85ee4e383f1fb5a7688c59f8054f1	vector space embedding of undirected graphs with fixed-cardinality vertex sequences for classification	graph theory;simple weighted undirected graphs;front end;fmri brain state decoding task;high dimensionality;motion pictures;support vector machines;image matching;prototypes;edit distance;pattern recognition classifiers;vector space;graph matching methods;image classification;graph matching;accuracy;fixed cardinality vertex sequences;image sequences graph theory image classification image matching image representation;graph edit distance vector space embedding fixed cardinality vertex sequences simple weighted undirected graphs vertex orderings graph matching methods pattern recognition classifiers computational cost low dimensional vector space representations fmri brain state decoding task;low dimensional vector space representations;image representation;prototypes pattern recognition support vector machines accuracy covariance matrix motion pictures pattern analysis;pattern recognition;computational cost;dissimilarity;graph classification;pattern analysis;graph embedding;vertex orderings;brain decoding graph classification graph embedding dissimilarity;brain decoding;vector space embedding;covariance matrix;graph edit distance;image sequences	Simple weighted undirected graphs with a fixed number of vertices and fixed vertex orderings can be used to represent data and patterns in a wide variety of scientific and engineering domains. Classification of such graphs by existing graph matching methods perform rather poorly because they do not exploit their specificity. As an alternative, methods relying on vector-space embedding hold promising potential. We propose two such techniques that can be deployed as a front-end for any pattern recognition classifiers: one has low computational cost but generates high-dimensional spaces, while the other is more computationally demanding but can yield relatively low-dimensional vector space representations. We show experimental results on an fMRI brain state decoding task and discuss the shortfalls of graph edit distance for the type of graph under consideration.	algorithmic efficiency;computation;graph (discrete mathematics);graph edit distance;hilbert space;matching (graph theory);pattern recognition;sensitivity and specificity	Jonas Richiardi;Dimitri Van De Ville;Kaspar Riesen;Horst Bunke	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.227	block graph;graph power;pathwidth;topological graph theory;support vector machine;covariance matrix;contextual image classification;combinatorics;discrete mathematics;graph embedding;independent set;edit distance;level structure;bipartite graph;vector space;edge space;graph theory;front and back ends;machine learning;comparability graph;pattern recognition;mathematics;accuracy and precision;prototype;graph;modular decomposition;graph operations;chordal graph;indifference graph;book embedding;line graph;circulant graph;statistics;matching	ML	35.213508565908434	-55.979306277978054	93697
56567bcf514ca5ba56086fc3d016b74ac5eca5c9	binary image steganalysis based on local texture pattern		Abstract In this paper, we propose a novel steganalytic scheme based on local texture pattern (LTP) to detect binary image steganography. We first assess how the expanded LTPs capture embedding distortions exactly. Considering curse of dimensionality when expanding LTPs, we employ Manhattan distance to measure the pixels correlation in a 5 × 5 sized block and select the pixels with closely correlation to remove some LTPs that are not interested. Although the stego image can maintain good visual quality, steganography scheme changes the inter-pixels correlation of binary image. Therefore we utilize totally 8192 LTPs histogram to define a 8192-dimensional steganalytic feature set. Original images and stego images are classified by ensemble classifier. Experimental results show that the proposed steganalytic method can more effectively detect state-of-the-art binary image steganography schemes compared with other steganalytic schemes.	binary image;steganalysis	Jialiang Chen;Wei Lu;Yanmei Fang;Xianjin Liu;Yuileong Yeung;Yingjie Xue	2018	J. Visual Communication and Image Representation	10.1016/j.jvcir.2018.06.004	pattern recognition;mathematics;computer vision;artificial intelligence;pixel;steganography;euclidean distance;histogram;steganalysis;binary image;curse of dimensionality;embedding	Vision	36.402800506310506	-60.71308913270858	93800
226fd58928d107f96d2bfa0ceb14fc56c68d2d48	a study of representations for pen based handwriting recognition of tamil characters	ocr pen based handwriting recognition tamil characters image representations indian language curvature fourier transform coefficients wavelet features neural network;handwriting recognition;handwriting recognition character recognition computer science natural languages automation graphics computer networks speech recognition humans writing;neural networks;fourier transform;neural nets;olcr;optical character recognition;character representations;pen computing;wavelet transforms;neural net work;image representation;fourier transforms;optical character recognition handwritten character recognition image representation fourier transforms wavelet transforms neural nets;school of automation;wavelets;computer science automation formerly;handwritten character recognition;neural network	In this paper we study the important issue of choosing representations that are suitable for recognizing pen based handwriting of characters in Tamil, a language of India. Four different choices, based on the following set of features are considered: (1) a sequence of directions and curvature; (2) a sequence of angles; (3) Fourier transform coefficients; and (4) wavelet features. We provide arguments in support of the representation using wavelet features. A neural network designed using these features gives excellent accuracy for recognizing Tamil characters.	artificial neural network;coefficient;handwriting recognition;wavelet	C. S. Sundaresan;S. Sathiya Keerthi	1999		10.1109/ICDAR.1999.791814	fourier transform;computer vision;speech recognition;computer science;machine learning;pattern recognition;artificial neural network	Vision	33.30441903695086	-66.0133853218402	93803
0dddce886f8ccf97f165dc9d047b885876a57667	real-time live face detection using face template matching and dct energy analysis	biometric authentication;color space;biometrics access control;image matching;real time;skin;skin area threshold biometric authentication dct energy pupil movement face template matching;morphological operation;face template matching;discrete cosine transform;face face detection correlation discrete cosine transforms streaming media face recognition skin;object detection biometrics access control discrete cosine transforms face recognition image colour analysis image matching;skin area threshold;face recognition;image acquisition;streaming media;pupil movement;discrete cosine transforms;image colour analysis;energy analysis;dct energy;face;correlation;face detection;photographic imitation real time live face detection reference face template matching dct energy analysis discrete cosine transform image acquisition source identification capability improvement biometric authentication systems face recognition ycbcr color space computational cost reduction pupil movements nostril movements 3d depth eye blinking security scheme;template matching;high frequency;object detection	This paper presents a method to detect the face of a live test subject in real time using a single camera as the image acquisition source. The objective is to improve the identification capability of biometric authentication systems based on face recognition by serving as a preprocessing tool to reject faces that do not belong to live humans and which can spoof the system into granting access. The face detection procedure involves skin area selection in the YCbCr color space and matching with a reference face template. Morphological operations such as erosion, dilation etc. have been avoided due to the uncertainty in the number of such operations that may be required under a varying real-time environment and also to reduce the computational cost of the algorithm. There are few parameters that can be considered as indicative of a live face, for example- pupil movements, nostril movements, 3D depth, and difference in number of high frequency components in live faces and photos etc. The proposed method involves the analysis of the image's DCT energy aided by detection of eye blinking and pupil movements. This yields a compact security scheme capable of resisting attacks by photographic imitation.	3d computer graphics;algorithm;algorithmic efficiency;attack (computing);authentication;biometrics;color space;computation;digital camera;dilation (morphology);discrete cosine transform;erosion (morphology);face detection;facial recognition system;liveness;mathematical morphology;preprocessor;real-time clock;real-time computing;real-time transcription;template matching	M. Hanuma Teja	2011	2011 International Conference of Soft Computing and Pattern Recognition (SoCPaR)	10.1109/SoCPaR.2011.6089267	facial recognition system;face;computer vision;face detection;speech recognition;template matching;computer science;discrete cosine transform;high frequency;skin;color space;correlation;biometrics;computer graphics (images)	Vision	33.83772770446117	-61.978706697319346	93828
5dfec3bb04f3aae1f65b996dec9d97fb46659c3c	heterogeneous graph propagation for large-scale web image search	large scale web image search;image feature voting;frequency modulation;quantisation signal image retrieval;incremental query expansion;quantization signal feature extraction frequency modulation visualization pipelines indexing;quantization signal;visualization;indexing;feature extraction;pipelines;期刊论文;image feature voting large scale web image search post processing heterogeneous graph propagation incremental query expansion;postprocessing;heterogeneous graph propagation;reranking approach heterogeneous graph propagation large scale web image search bag of visual word model bovw model information loss quantization stage image retrieval online querying process	State-of-the-art web image search frameworks are often based on the bag-of-visual-words (BoVWs) model and the inverted index structure. Despite the simplicity, efficiency, and scalability, they often suffer from low precision and/or recall, due to the limited stability of local features and the considerable information loss on the quantization stage. To refine the quality of retrieved images, various postprocessing methods have been adopted after the initial search process. In this paper, we investigate the online querying process from a graph-based perspective. We introduce a heterogeneous graph model containing both image and feature nodes explicitly, and propose an efficient reranking approach consisting of two successive modules, i.e., incremental query expansion and image-feature voting, to improve the recall and precision, respectively. Compared with the conventional reranking algorithms, our method does not require using geometric information of visual words, therefore enjoys low consumptions of both time and memory. Moreover, our method is independent of the initial search process, and could cooperate with many BoVW-based image search pipelines, or adopted after other postprocessing algorithms. We evaluate our approach on large-scale image search tasks and verify its competitive search performance.	algorithm;anatomic node;bag-of-words model in computer vision;baseline (configuration management);entity name part qualifier - adopted;experiment;genetic heterogeneity;graph (discrete mathematics);graph - visual representation;image retrieval;increment;intuition;inverted index;palatal expansion technique;pipeline (computing);precision and recall;query expansion;scalability;software propagation;video post-processing	Lingxi Xie;Qi Tian;Wengang Zhou;Bo Zhang	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2432673	frequency modulation;beam search;computer vision;search engine indexing;visualization;feature extraction;computer science;machine learning;pattern recognition;pipeline transport;incremental heuristic search;information retrieval	Vision	36.33993515576662	-55.05073019039658	93867
cba90571b415bc19029cb6d455ff630c57906907	similarity retrieval of shape images based on database classification	journal of visual communication and image representation;database classification;similar test;similarity retrieval;shape similarity retrieval;mpeg 7 art descriptor;shape similarity;visual features;issn 1047 3203	Shape is a key visual feature used to describe image content. This paper develops a novel shape-based similarity retrieval system based on database classification which exploits the contour and interior region of a shape efficiently. In this system, the database of shape images is categorized automatically into 11 classes by a simple contour feature. In query, the contour feature of the input image is used to decide which class the query image belongs to. Then, the possible classes are selected dynamically from the database and to form candidate sets with different priority orders. Then, ART region feature is employed to compare the query with the candidate sets according to the priority order. Instead of using the original contour of a shape image directly, we employ a rough version of the original contour for the classification of shapes. The similarity test results indicate that the proposed method improves retrieval accuracy and speed significantly, as compared to ART.	algorithmic efficiency;categorization;contour line;information retrieval;shape context;statistical classification	Mao-Hsiung Hung;Chaur-Heh Hsieh;Chung Ming Kuo	2006	J. Visual Communication and Image Representation	10.1016/j.jvcir.2005.09.002	active shape model;computer vision;visual word;computer science;pattern recognition;information retrieval	Vision	38.502354269753226	-60.25027414042496	94462
fe0b1c4a3d4eb2442538ad49cfc653d7e8f95b10	pedestrian detection based on hog and lbp		In this paper, we present a feature extraction approach for pedestrian detection by extracting the sparse representation of histograms of oriented gradients (HOG) feature and local binary pattern (LBP) feature using K-SVD. Moreover, we use PCA to reduce the dimension of HOG and LBP. We combine the low dimension principal features with the sparse representations of HOG feature directly for fast pedestrian detection from images. In addition, we compare the performance of sparse representations and PCA based features. Experimental results on INRIA databases show that the proposed approach provides a better detection result and spends less time.	belief propagation;pedestrian detection	Wen-Juan Pei;Yu-Lan Zhang;Yan Zhang;Qing Yan	2014		10.1007/978-3-319-09333-8_78	artificial intelligence;computer science;local binary patterns;pattern recognition;k-svd;feature extraction;sparse approximation;pedestrian detection	Vision	33.85742019624054	-57.18940962041361	94546
9fb6910bdec7529ff43c5d2efabc62de8f629ba0	leveraging the path signature for skeleton-based human action recognition		Human action recognition in videos is one of the most challenging tasks in computer vision. One important issue is how to design discriminative features for representing spatial context and temporal dynamics. Here, we introduce a path signature feature to encode information from intra-frame and inter-frame contexts. A key step towards leveraging this feature is to construct the proper trajectories (paths) for the data steam. In each frame, the correlated constraints of human joints are treated as small paths, then the spatial path signature features are extracted from them. In video data, the evolution of these spatial features over time can also be regarded as paths from which the temporal path signature features are extracted. Eventually, all these features are concatenated to constitute the input vector of a fully connected neural network for action classification. Experimental results on four standard benchmark action datasets, J-HMDB, SBU Dataset, Berkeley MHAD, and NTURGB+D demonstrate that the proposed approach achieves state-of-the-art accuracy even in comparison with recent deep learning based models.	artificial neural network;benchmark (computing);computer vision;concatenation;deep learning;encode;human metabolome database;intra-frame coding;linear temporal logic;steam	Weixin Yang;Terry Lyons;Hao Ni;Cordelia Schmid;Lianwen Jin;Jiawei Chang	2017	CoRR		pattern recognition;spatial contextual awareness;machine learning;discriminative model;computer science;deep learning;skeleton (computer programming);artificial neural network;concatenation;artificial intelligence	Vision	28.614386780861185	-52.15672257184724	94549
6eab5e0489ae270bef9807d729498fc5786004ef	a nature based fusion scheme for multimodal biometric person identity verification at a distance	degradation;speech data nature based fusion scheme multimodal biometric person identity verification face gait;biometrics access control;fusion;security of data biometrics access control;authentication;gait;biometrics;speech;personal identity;multimodal biometric person identity verification;distance measurement;brain modeling;speech data;hidden markov models;shape;nature based fusion scheme;feature extraction;biometrics speech authentication brain modeling facial features shape degradation military computing humans visual system;pattern recognition;multimodal biometric verification;facial features;face;humans;visual system;security of data;nature base;fusion nature base multimodal biometric verification;military computing	This paper presents a multimodal biometric verification scheme for face, gait and speech data as inspired by how verification is done at a distance in the natural world.	biometrics;multimodal interaction	Chiung Ching Ho;Chikkannan Eswaran	2009	2009 International Conference on Signal Acquisition and Processing	10.1109/ICSAP.2009.28	personal identity;face;computer vision;speech recognition;degradation;visual system;fusion;feature extraction;shape;computer science;speech;authentication;gait;hidden markov model;biometrics	Robotics	28.67985394929654	-61.18808550937656	94553
1dd0f2299ad7a1809633e0a1651113ffe4359709	a framework for the retrieval of multiple regions using binary partition trees and low level descriptors	abstracts mars;trees mathematics feature extraction feedback image representation image retrieval;manual setting avoidance multiple region retrieval binary partition tree low level feature descriptor visual similarity assessment bpt image representation	This paper proposes a framework for the retrieval of multiple regions characterized by low-level features. The retrieval combines the assessment of the visual similarity between regions and of the similarity of the relationship between these regions. Binary Partition Trees (BPTs) are used as a basis of the image representation. Regions of the BPT are described by low-level descriptors. Finally, relevance feedback is used to avoid the need of manually setting the weights associated to each descriptor.	high- and low-level;relevance feedback	Luis de Garrido;Philippe Salembier	2002	2002 11th European Signal Processing Conference		computer vision;visual word;machine learning;pattern recognition;mathematics	Vision	37.27317444477198	-56.757354186762115	94755
16c59487fa1cfe74f25bcb050f10f71ec4807dec	a new shape benchmark for 3d object retrieval	3d model;shape similarity;shape retrieval;geometric structure	Recently, content based 3D shape retrieval has been an active area of research. Benchmarking allows researchers to evaluate the quality of results of different 3D shape retrieval approaches. Here, we propose a new publicly available 3D shape benchmark to advance the state of art in 3D shape retrieval. We provide a review of previous and recent benchmarking efforts and then discuss some of the issues and problems involved in developing a benchmark. A detailed description of the new shape benchmark is provided including some of the salient features of this benchmark. In this benchmark, the 3D models are classified mainly according to visual shape similarity but in contrast to other benchmarks, the geometric structure of each model is modified and normalized, with each class in the benchmark sharing the equal number of models to reduce the possible bias in evaluation results. In the end we evaluate several representative algorithms for 3D shape searching on the new benchmark, and a comparison experiment between different shape benchmarks is also conducted to show the reliability of the new benchmark.	3d modeling;algorithm;benchmark (computing);bit error rate;quality of results	Rui Fang;Afzal Godil;Xiaolan Li;Asim Imdad Wagan	2008		10.1007/978-3-540-89639-5_37	active shape model;computer vision;computer science;machine learning;data mining	Vision	36.39797977414232	-55.47144476789722	94917
088b98bcda68300c7186622ad57f92f6b6538c3e	real-time traffic-sign recognition using tree classifiers	gtsrb data set real time traffic sign recognition tree classifiers tsr driver assistance system das performance evaluation k d trees random forests support vector machines svm traffic sign classification different sized histogram of oriented gradient descriptors hog descriptors distance transforms dt fisher criterion feature selection memory requirement reduction german traffic sign recognition benchmark data set;histograms;image recognition;object recognition;image classification support vector machines image processing object detection pattern recognition machine vision machine learning;traffic signs;performance evaluation;image processing;support vector machines;road traffic;tree data structures decision trees image classification image processing image recognition object detection performance evaluation road traffic support vector machines;image classification;advanced driver assistance systems;real time information;tree data structures;classification;machine learning;traffic sign recognition;machine vision;pattern recognition;traffic sign recognition advanced driver assistance systems image classification image processing machine vision object detection object recognition pattern recognition;detection and identification systems;decision trees;object detection	Traffic-sign recognition (TSR) is an essential component of a driver assistance system (DAS), providing drivers with safety and precaution information. In this paper, we evaluate the performance of k-d trees, random forests, and support vector machines (SVMs) for traffic-sign classification using different-sized histogram-of-oriented-gradient (HOG) descriptors and distance transforms (DTs). We also use the Fisher's criterion and random forests for the feature selection to reduce the memory requirements and enhance the performance. We use the German Traffic Sign Recognition Benchmark (GTSRB) data set containing 43 classes and more than 50 000 images.	benchmark (computing);distance transform;feature selection;gradient;random forest;real-time transcription;requirement;support vector machine;traffic sign recognition	Fatin Zaklouta;Bogdan Stanciulescu	2012	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2012.2225618	support vector machine;computer vision;real-time data;contextual image classification;machine vision;advanced driver assistance systems;image processing;biological classification;computer science;cognitive neuroscience of visual object recognition;machine learning;decision tree;pattern recognition;histogram;traffic sign recognition;tree	Vision	32.21727821713987	-56.552811816148676	94990
71f20752935ebf213afda05bed98d5dbcd9c98cd	length estimation of digit strings using a neural network with structure-based features	image recognition;fuzzy neural nets;image segmentation;neural networks;multilayer perceptrons;feature extraction;feedforward neural nets;parameter estimation;journal magazine article;neural network	e be vion sigan is of the be on 199 Abstract. Accurate length estimation is very helpful for the successful segmentation and recognition of connected digit strings, in particular, for an off-line recognition system. However, little work has been done in this area due to the difficulties involved. A length estimation approach is presented as a part of our automatic off-line digit recognition system. The kernel of our approach is a neural network estimator with a set of structure-based features as the inputs. The system outputs are a set of fuzzy membership grades reflecting the degrees of an input digit string of having different lengths. Experimental results on National Institute of Standards and Technology (NIST) Special Database 3 and other derived digit strings shows that our approach can achieve an about 99.4% correct estimation if the best two estimations are considered. © 1998 SPIE and IS&T. [S1017-9909(98)00901-5]	artificial neural network;online and offline;string (computer science)	Zhongkang Lu;Zheru Chi;Wan-Chi Siu	1998	J. Electronic Imaging	10.1117/1.482629	computer vision;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;time delay neural network;image segmentation;estimation theory;artificial neural network	ML	31.04630220921282	-65.02833155185296	95268
06f0895d24ff7f78a15a88a90a829bcf4a684eb9	texture classification using gabor energy features and higher order spectral features: a comparative study	filtering;filter bank;uncertainty;texture classification;gabor filters;higher order;gabor filter;statistical analysis;feature extraction;signal processing;comparative study;bandwidth;digital image;frequency;visual system;gabor filters frequency feature extraction filtering bandwidth signal processing statistical analysis visual system uncertainty filter bank;invariant feature	Several approaches to the classification and segmentation of textural content in digital images have been investigated in recent years. The extraction of features for classification has particularly received considerable attention. In this paper we contrast between two approaches for feature extraction i.e. Gabor filters and bispectral invariant features. A subset of the Brodatz album are used in a dichotomy experiment and separate SVMs are trained to classify features from each pair. Our experiments show that the bispectral invariant features produce better classification results for more texture pairs than the Gabor filters. Results also indicate that a combination of the two feature sets will yield higher accuracy, and for some texture pairs, neither works very well.	digital image;experiment;feature extraction;gabor filter	Ronald Elunai;Vinod Chandran;Sridha Sridharan	2005	Proceedings of the Eighth International Symposium on Signal Processing and Its Applications, 2005.	10.1109/ISSPA.2005.1581024	filter;computer vision;speech recognition;higher-order logic;uncertainty;visual system;feature extraction;computer science;frequency;comparative research;signal processing;pattern recognition;filter bank;mathematics;digital image;bandwidth;statistics	Vision	37.885058930955765	-61.05144522422763	95359
7fa2f9b9672fbb60b08bdd2f18bf4b4424ad91c9	toward the labeled segmentation of natural images using rough-set rules		This article introduces an approach that integrates color and texture features for the segmentation of natural images. In order to deal with the vague or imprecise information that is typically shown in this kind of scenes, our method consists in a supervised classifier based on rules obtained using the rough-set theory. Such rough classifier yields a label per pixel using as inputs only three color and three textural features computed separately. These labels are used to carry out the image segmentation. When comparing quantitatively the results from this work with state-of-the-art algorithms, it has shown to be a competitive approach to the image segmentation task. Moreover, the labeling of each pixel offers advantages over other segmentation algorithms because the outcome is intuitive to humans in two senses. On one hand, the use of simple rules and few features facilitate the understanding of the segmentation process. On the other hand, the labels in the segmented outcomes provide insight into the image content.		Fernando J. Navarro-Avila;Jonathan Cepeda-Negrete;Raúl Enrique Sánchez-Yáñez	2016		10.1007/978-3-319-39393-3_8	artificial intelligence;pixel;computer vision;computer science;image segmentation;rough set;segmentation	Vision	27.65539573773192	-55.739880368992296	95401
0a448b3c7a71ca97ec9e427a1745e4e04856d07d	deep learning methods for personnel recognition based on micro-doppler features		In this paper, we investigate the use of human gait micro-Doppler features for personnel recognition with a deep learning approach. Compared with conventional methods for radar-based human recognition, most existing schemes remain in discussing the distinction of different human motions. The proposed method employs a deep convolutional neural network (DCNN), which combines the inception parallel structure, to learn the necessary features of the time-frequency complex tensor that can be used to address the problem of recognizing different human subjects. Real data is collected relating to eight human subjects using a continuous wave radar operating at K-band. The experimental results illustrate that by using the human gait micro-Doppler features, the DCNN can achieve an accuracy rate of 96.9% on personnel recognition.	artificial neural network;convolutional neural network;deep learning;ku band;radar	Yuming Shao;Yulong Dai;Longzhi Yuan;Weidong Chen	2017		10.1145/3163080.3163095	tensor;convolutional neural network;doppler effect;radar;computer vision;continuous-wave radar;deep learning;gait (human);artificial intelligence;computer science	AI	26.77801278738434	-60.51631260449764	95403
c5e2197a365702a7c389c44e1b8d2ce106e31374	a novel approach for binarization of overlay text	binarization;video text;image segmentation;video signal processing;mrf;segmentation performance overlay text binarization text pixels extraction video frames binarization computation video text recognition ocr software optical character recognition text polarity light text dark background dark text light background k means algorithm rgb color space red green blue color space mrf model log gabor filter;optical character recognition;k means;gabor filters;会议论文;mrf video text binarization k means;image colour analysis;feature extraction;video signal processing feature extraction gabor filters image colour analysis image segmentation learning artificial intelligence optical character recognition;learning artificial intelligence;image color analysis optical character recognition software text recognition accuracy colored noise clustering algorithms	In this paper, we presents a new binarization approach to extract text pixels from complex background in video frames. The binarization computation is a crucial step for, video text recognition, which can greatly increase the recognition, accuracy of an OCR software. The proposed approach consists, of four phases. First, the text polarity is determined, i.e. light text with dark background or dark text with light background., Then the pixels in the given image are clustered into K clusters, using the K-means algorithm in the RGB color space and the, text cluster is selected based on the text polarity. Further, the, MRF Model is exploited to get the binarization result. Finally, the, result is further refined by the Log-Gabor filter. The Experimental, results on a large dataset show that the significant gains have been, obtained according to the segmentation performance on the pixel, level as well as the OCR accuracy.	algorithm;binary image;color space;comparison of optical character recognition software;computation;dark web;gabor filter;k-means clustering;log-space reduction;markov random field;pixel	Zhike Zhang;Weiqiang Wang	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.726	computer vision;materials recovery facility;speech recognition;feature extraction;computer science;machine learning;pattern recognition;image segmentation;optical character recognition;k-means clustering	Robotics	36.704534508022974	-64.67265988436047	95585
8f713e3c5b6b166c213e00a3873f750fb5939c9a	the 2d factor analysis and its application to face recognition with a single sample per person	databases;training;loading;principal component analysis face recognition;face recognition;yttrium;principal component analysis;correlation;two dimensional principal component analysis 2d factor analysis face recognition single sample per person theoretical model data reduction multivariate analysis classical factor analysis unidimensional vectors two dimensional signal principal component analysis illumination conditions pose conditions;principal component analysis face recognition loading databases yttrium correlation training;data reduction face recognition factor analysis principal component analysis	In this paper, a novel theoretical model of data reduction and multivariate analysis is proposed. The Two-dimensional Factor Analysis is an extension of classical factor analysis in which the images are treated as matrices instead of being converted to unidimensional vectors. By maximally representing the correlation among the pixels, it is able to capture meaningful information about the spatial relationships of the elements in a two-dimensional signal. The method is illustrated in the problem of face recognition with superior results when compared to other approaches based on principal component analysis. Experiments using public databases under different pose and illumination conditions show that the proposed method is significantly more effective than the two-dimensional principal component analysis while dealing with samples composed by a single image per person.	autostereogram;database;experiment;facial recognition system;factor analysis;glossary of computer graphics;information retrieval;medical imaging;pixel;principal component analysis;theory;two-way deterministic finite automaton	Alexei Manso Corrêa Machado	2015	2015 23rd European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2015.7362563	psychology;econometrics;relationship square;kernel principal component analysis;pattern recognition;eigenface;multiple correspondence analysis;statistics;principal component analysis	Vision	34.49713476028773	-57.87934253422395	95735
1a39c26a2541899c06d9bb1f043a603525913e9e	a unified approach to content-based indexing and retrieval of digital videos from television archives		This work addresses the development of a unified approach to content-based indexing and retrieval of digital videos from television archives. The proposed approach has been designed to deal with arbitrary television genres, making it suitable for various applications. To achieve this goal, the main steps of a content-based video retrieval system are addressed in this work, namely: video segmentation, key-frame extraction, content-based video indexing and the video retrieval operation itself. Video segmentation is addressed as a typical TV broadcast structuring problem, which consists in automatically determining the boundaries of each broadcasted program (like movies, news, among others) and inter-program (for instance, commercials). Specifically, to segment the videos, Electronic Program Guide (EPG) metadata is combined with the detection of two special cues, namely, audio cuts (silence) and dark monochrome frames. On the other hand, a color histogram-based approach performs key-frame extraction. Video indexing and retrieval are accomplished by using hashing and k-d tree methods, while visual signatures containing color, shape and texture information are estimated for the key-frames, by using image and frequency domain techniques. Experimental results with the dataset of a multimedia information system especially developed for managing television broadcast archives demonstrate that our approach works efficiently, retrieving videos in 0.16 seconds on average and achieving recall, precision and F1 measure values, as high as 0.76, 0.97 and 0.86 respectively.	archive;color histogram;cryptographic hash function;f1 score;information system;key frame;monochrome;television;type signature	Celso L. de Souza;Flávio Luis Cardeal Pádua;Cristiano F. G. Nunes;Guilherme Tavares de Assis;Giani David Silva	2014	Artif. Intell. Research	10.5430/air.v3n3p49	computer vision;computer science;multimedia;information retrieval	Vision	36.43482394290132	-63.318555255364494	95805
d8e0209bbccfff12a4bc10f45596cc1bf9333ebb	a novel algorithm for defect inspection of touch panels		Automatic optical inspection plays an important role to control the appearance quality of wide range of products in the product process. Recently, the high popularity of smartphones and information appliances drives significant demand of touch panels. However, the traditional frequency-based method which exploits the line structure feature of texture images is not effective for the defect detection of touch panels. The paper presents a novel spatial domain algorithm to inspect the defects on touch panel. By utilizing the characteristics of periodic patterns of the sensing circuits, an adaptive model for each pattern is learned online to effectively extract defects. The experimental results indicate that our proposed method achieves accurate detection with efficient computation. In addition, the users pay very little effort for the testing of different panel products.	algorithm;computation;information appliance;smartphone;software bug;touchscreen	Mao-Hsiung Hung;Chaur-Heh Hsieh	2015	Image Vision Comput.	10.1016/j.imavis.2015.06.001	computer vision;simulation	AI	33.550114897987946	-60.49383168487067	96094
a3a96af31c8b25c035a16d59383f1d9c8182bdc6	compact representation of bidirectional texture functions	image features;bidirectional texture function;image classification;visual databases image texture rendering computer graphics image classification principal component analysis;image texture;lighting image texture skin shadow mapping surface texture robustness statistical distributions image analysis histograms image recognition;compact representation;principal component analysis;columbia utrecht reflectance and texture database compact representation bidirectional texture functions image texture fine scale geometric surface characteristic image features principle components analysis curet database;principle component analysis;rendering computer graphics;statistical distribution;visual databases	A bidirectional texture function (BTF) describes image texture as it varies with viewing and illumination direction. Many real world surfaces such as skin, fur, gravel, etc. exhibit fine-scale geometric surface detail. Accordingly, variations in appearance with viewing and illumination direction may be quite complex due to local foreshortening, masking and shadowing. Representations of surface texture that support robust recognition must account for these effects. We construct a representation which captures the underlying statistical distribution of features in the image texture as well as the variations in this distribution with viewing and illumination direction. The representation combines clustering to learn characteristic image features and principle components analysis to reduce the space of feature histograms. This representation is based on a core image set as determined by a quantitative evaluation of importance of individual images in the overall representation. The result is a compact representation and a recognition method where a single novel image of unknown viewing and illumination direction can be classified efficiently. The CUReT (Columbia-Utrecht reflectance and texture) database is used as a test set for evaluation of these methods.	autostereogram;bidirectional texture function;cluster analysis;columbia (supercomputer);computer graphics;computer vision;core image;enriques–kodaira classification;experiment;feature vector;global illumination;grammar-based code;illumination (image);image texture;iterative method;library (computing);principal component analysis;simultaneous localization and mapping;skin (computing);surface detail;test set;texton;texture synthesis;v-optimal histograms;viewing cone	Gabriela Oana Cula;Kristin J. Dana	2001		10.1109/CVPR.2001.990645	bidirectional texture function;image texture;computer vision;pyramid;feature detection;computer science;pattern recognition;mathematics;alpha mapping;texture atlas;texture compression;texture filtering;projective texture mapping;principal component analysis;computer graphics (images)	Vision	38.61072946164088	-57.78473765065897	96274
023adaf2e3370b88dc1015798534f5706b34f0f3	an analog programmable multidimensional radial basis function based classifier	bump circuit;radial basis function rbf;analog classifier;multidimensional systems circuits speech recognition hidden markov models covariance matrix vector quantization computational efficiency cepstrum digital signal processing probability distribution;floating gate transistor;maximum likelihood;gaussian like analog circuit;multivariate gaussian function;floating gate transistors;floating gate;vector quantizer analog classifier bump circuit floating gate transistor gaussian like analog circuit radial basis function rbf;power efficiency;vector quantisation analogue circuits covariance matrices gaussian distribution radial basis function networks;bell shaped transfer;equal error rate analog programmable classifier multidimensional radial basis function based classifier probability distribution bell shaped transfer floating gate bump circuit floating gate transistors multivariate gaussian function diagonal covariance matrix gaussian mixture model winner take all circuit analog vector quantizer;receiver operating characteristic curve;radial basis function networks;analog circuits;gaussian mixture model;analog programmable classifier;radial basis function;winner take all circuit;covariance matrices;probability distribution;equal error rate;analogue circuits;vector quantizer;vector quantisation;winner take all;floating gate bump circuit;gaussian distribution;diagonal covariance matrix;multidimensional radial basis function based classifier;covariance matrix;analog vector quantizer	A compact analog programmable multidimensional radial basis function (RBF)-based classifier is demonstrated. The probability distribution of each feature in the templates is modeled by a Gaussian function that is approximately realized by the bell-shaped transfer characteristics of a proposed floating-gate circuit, which we term a floating-gate bump circuit. The maximum likelihood, the mean, and the variance of the distribution are stored in floating-gate transistors and are independently programmable. By cascading these floating-gate bump circuits, the overall transfer characteristics approximate a multivariate Gaussian function with a diagonal covariance matrix. An array of these circuits constitute a compact multidimensional RBF-based classifier that can easily implement a Gaussian mixture model. When followed by a winner-take-all circuit, the RBF-based classifier forms an analog vector quantizer. We use receiver operating characteristic curves and equal error rate to evaluate the performance of our RBF-based classifier as well as a resultant analog vector quantizer. We show that the classifier performance is comparable to that of digital counterparts. The proposed approach can be at least two orders of magnitude more power efficient than the digital microprocessors at the same task.	approximation algorithm;digital electronics;digital signal processor;microprocessor;mixture model;nmos logic;pmos logic;quantization (signal processing);radial (radio);radial basis function network;receiver operating characteristic;resultant;thermal copper pillar bump;time complexity;transfer function;transistor;video graphics array;weapon target assignment problem	Sheng-Yu Peng;Paul E. Hasler;David V. Anderson	2007	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2007.905642	normal distribution;winner-take-all;probability distribution;margin classifier;covariance matrix;radial basis function;electronic engineering;electrical efficiency;analogue electronics;computer science;machine learning;mixture model;mathematics;maximum likelihood;receiver operating characteristic;statistics	ML	25.986267350514076	-62.569494276328925	96404
41a1bd7b07d946015dfebc9c4d52e55c895c398b	image watermarking method in multiwavelet domain based on support vector machines	digital watermarking;support vector machines;digital watermark;low pass filter;image watermarking;support vector machine;multiwavelet domain	0164-1212/$ see front matter Crown Copyright 2 doi:10.1016/j.jss.2010.03.006 * Corresponding author. Address: School of Electron Electronic Science and Technology of China, Chengdu E-mail address: ph66@tom.com (H. Peng). A novel image watermarking method in multiwavelet domain based on support vector machines (SVMs) is proposed in this paper. The special frequency band and property of image in multiwavelet domain are employed for the watermarking algorithm. After performed single-level multiwavelet decomposition on each image block of an image, a mean value modulation method, which modulates mean value relationship of multiwavelet coefficients in two approximation sub-bands, is used for carrying watermark embedding. The mean value modulation method can more effectively reduce image distortion than that of conventional single coefficient. At watermark detector, SVMs is used to learn the mean value relationship. Due to good learning ability of SVMs, watermark can be correctly extracted under several different attacks. The experimental results show proposed algorithm is robust to common attacks such as JPEG, low-pass filtering, noise addition, rotation and scaling, etc. Crown Copyright 2010 Published by Elsevier Inc. All rights reserved.	algorithm;approximation;coefficient;contourlet;crown group;curvelet;digital watermarking;distortion;electron;frequency band;image scaling;jpeg;low-pass filter;modulation;multi-level cell;multiscale geometric analysis;support vector machine;wavelet	Hong Peng;Jun Wang;Weixing Wang	2010	Journal of Systems and Software	10.1016/j.jss.2010.03.006	support vector machine;computer vision;digital watermarking;computer science;theoretical computer science;pattern recognition	ML	35.39618590295394	-60.09314481008076	96454
8c8ec803fcd1e4b58c4ba200b34de14b986d8719	convolutional hashing for automated scene matching		We present a powerful new loss function and training scheme for learning binary hash functions. In particular, we demonstrate our method by creating for the first time a neural network that outperforms state-of-the-art Haar wavelets and color layout descriptors at the task of automated scene matching. By accurately relating distance on the manifold of network outputs to distance in Hamming space, we achieve a 100-fold reduction in nontrivial false positive rate and significantly higher true positive rate. We expect our insights to provide large wins for hashing models applied to other information retrieval hashing tasks as well.	artificial neural network;consistency model;content-based image retrieval;haar wavelet;hamming distance;hamming space;hash function;information retrieval;loss function;sensitivity and specificity	Martin Loncaric;Bowei Liu;Ryan Weber	2018	CoRR		false positive rate;wavelet;manifold;machine learning;pattern recognition;hash function;hamming space;computer science;artificial neural network;artificial intelligence;binary number	ML	29.423915422727475	-54.54102244909017	96511
6bb8a5f9e2ddf1bdcd42aa7212eb0499992c1e9e	a siamese long short-term memory architecture for human re-identification		Matching pedestrians across multiple camera views known as human re-identification (re-identification) is a challenging problem in visual surveillance. In the existing works concentrating on feature extraction, representations are formed locally and independent of other regions. We present a novel siamese Long Short-Term Memory (LSTM) architecture that can process image regions sequentially and enhance the discriminative capability of local feature representation by leveraging contextual information. The feedback connections and internal gating mechanism of the LSTM cells enable our model to memorize the spatial dependencies and selectively propagate relevant contextual information through the network. We demonstrate improved performance compared to the baseline algorithm with no LSTM units and promising results compared to state-of-the-art methods on Market-1501, CUHK03 and VIPeR datasets. Visualization of the internal mechanism of LSTM cells shows meaningful patterns can be learned by our method.	algorithm;baseline (configuration management);feature extraction;interaction;long short-term memory;software propagation;system image	Rahul Rama Varior;Bing Shuai;Jiwen Lu;Dong Xu;Gang Wang	2016		10.1007/978-3-319-46478-7_9	computer vision;computer science;artificial intelligence;machine learning	Vision	27.452823184747096	-52.23878163060535	96663
327a0bc6adea9616558e08e1649f2cbe1cbc7876	cattle identification using muzzle print images based on texture features approach		The increasing growth of the world trade and growing con- cerns of food safety by consumers need a cutting-edge animal identi- fication and traceability systems as the simple recording and reading of tags-based systems are only effective in eradication programs of na- tional disease. Animal biometric-based solutions, e.g. muzzle imaging system, offer an effective and secure, and rapid method of addressing the requirements of animal identification and traceability systems. In this paper, we propose a robust and fast cattle identification approach. This approach makes use of Local Binary Pattern (LBP) to extract local invariant features from muzzle print images. We also applied different classifiers including Nearest Neighbor, Naive Bayes, SVM and KNN for cattle identification. The experimental results showed that our approach is superior than existed works as ours achieves 99,5% identification accu- racy. In addition, the results proved that our proposed method achieved this high accuracy even if the testing images are rotated in various angels or occluded with different parts of their sizes.		Alaa Tharwat;Tarek Gaber;Aboul Ella Hassanien;Hassan A. Hassanien;Mohamed F. Tolba	2014		10.1007/978-3-319-08156-4_22	simulation;speech recognition;engineering;communication	Vision	31.898181565538597	-62.01222897720244	96811
c812ece008c8c29b406c4145a86f86fbaf83f932	shoeprint image retrieval based on local image features	data hiding;image coding cryptography data encapsulation fibonacci sequences;image coding;prime decomposition method;fibonacci decomposition;fibonacci sequences;substitution method;data encapsulation;decomposition method;fibonacci least significant bit method data hiding technique;least significant bit;secret message embedding;cryptography;prime decomposition method prime numbers secret message embedding stego image quality fibonacci least significant bit method data hiding technique fibonacci decomposition substitution method;image quality;mathematical model;prime numbers;prime number;difference set;data encapsulation pixel communication system security computer security data security information security communication system software software quality quality of service computer science;stego image quality	This paper deals with the retrieval of scene-of-crime (or scene) shoeprint images from a reference database of shoeprint images by using a new local feature detector and an improved local feature descriptor. Our approach is based on novel modifications and improvements of a few recent techniques in this area: (1) the scale adapted Harris detector, which is an extension to multi-scale domains of the Harris corner detector; (2) automatic scale selection by the characteristic scale of a local structure. (3) SIFT (Scale-Invariant Feature Transform), one of the most widely investigated descriptors in recent years. Like most of other local feature representations, the proposed approach can also be divided into two stages: (i) a set of distinctive local features are selected by first detecting scale adaptive Harris corners where each of them is associated with a scale factor, and then selecting as the final result only those corners whose scale matches the scale of blob-like structures around them. Here, the scale of a blob-like structure is detected by the Laplace-based scale selection, (ii). for each feature detected, an enhanced SIFT descriptor is computed to represent this feature. Our improvements lead two novel methods which we call the Modified Harris-Laplace (MHL) detector, and the enhanced SIFT descriptor. In this paper, we demonstrate the application of the proposed scheme to the shoeprint image retrieval problem using six sets of synthetic scene images, 50 images for each, and a database of 500 reference shoeprint images. The retrieval performance of the proposed approach is significantly better, in terms of cumulative matching score, than the existing methods investigated in this application area, such as edge directional histogram, power spectral distribution, and pattern & topological spectra.	bibliographic database;corner detection;harris affine region detector;image retrieval;scale-invariant feature transform;sensor;synthetic data;visual descriptor	Danny Crookes;Hongjiang Su;Ahmed Bouridane;Mourad Gueham	2007	Third International Symposium on Information Assurance and Security	10.1109/IAS.2007.18	arithmetic;discrete mathematics;theoretical computer science;fibonacci search technique;mathematics;prime number;statistics	Vision	36.52940286604213	-60.56869887767312	96826
172a5380e175d357fbee5d16a86c9b98ef3f54de	image retrieval by texture similarity	content based;image database;edp string;wavelet decomposition;energy distribution;content based image retrieval;texture similarity;high performance;high efficiency;csg vector;image retrieval	Accuracy and e/ciency are the two important issues in designing content-based image retrieval systems. In this paper, we present an e/cient image retrieval system with high performance of accuracy based on two novel features, the composite sub-band gradient vector and the energy distribution pattern string. Both features are generated from the sub-images of a wavelet decomposition of the original image. A fuzzy matching mechanism based on energy distribution pattern strings serves as a 4lter to quickly remove undesired images in the database from further consideration. The images passing the 4lter will be compared with the query image based on composite sub-band gradient vectors which are extremely powerful for discriminating detailed textures. Through several extensive experiments by exercising our prototype system with a database of 2400 images, we demonstrated that both high accuracy and high e/ciency can be achieved at the same time by our approach. ? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.	approximation algorithm;boolean algebra;color;constructive solid geometry;content-based image retrieval;database;electronic data processing;experiment;gradient;horseland;lh (complexity);ll parser;national supercomputer centre in sweden;pattern recognition;precision and recall;prototype;relevance;speedup;type signature;wavelet	Po-Whei Huang;S. K. Dai	2003	Pattern Recognition	10.1016/S0031-3203(02)00083-3	image texture;computer vision;visual word;image retrieval;computer science;pattern recognition;automatic image annotation;information retrieval	Vision	38.26458458829589	-61.06406265833482	96903
b2aebc349300ad2117bfe540f91a1be2f4944fec	multispectral iris recognition utilizing hough transform and modified lbp	multispectral iris dataset multispectral iris recognition scheme circular hough transform modified local binary pattern feature extraction technique iris region localization multispectral iris images binary thresholding techniques edge detection techniques multispectral iris image segmentation pupil boundaries iris boundaries iris feature elements mlbp technique sign features magnitude features iris texture classification performance;iris recognition image segmentation accuracy image edge detection transforms feature extraction educational institutions;circular hough transform multispectral iris recognition modified local binary pattern;iris recognition edge detection feature extraction hough transforms image classification image segmentation image texture	This paper presents a multispectral iris recognition scheme using Circular Hough Transform (CHT) and a modified Local Binary Pattern (mLBP) feature extraction technique. The CHT is used to localize the iris regions from the multispectral iris images. We also apply the binary thresholding and edge detection techniques in an effort to reduce the effects of over and under segmentation in multispectral iris images in which iris and pupil boundaries are not clearly separable. Furthermore, we apply mLBP in an attempt to elicit the iris feature elements. The mLBP technique combines both the sign and magnitude features for the improvement of iris texture classification performance. The identification and verification performance of the proposed scheme is validated using a multispectral iris dataset of 3120 images.	belief propagation;edge detection;feature extraction;hough transform;iris recognition;multispectral image;signed number representations;thresholding (image processing)	Khary Popplewell;Kaushik Roy;Foysal Ahmad;Joseph Shelton	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974110	computer vision;pattern recognition;computer graphics (images)	Vision	34.365433707472505	-62.131603049352655	96924
7cf0790f8c4a9bffb5fe3f552f8a7b306a6c6157	robust text line, word and character extraction from telugu document image	nearest neighbor searches;image segmentation;optical character recognition;text analysis;text line extraction word extraction character extraction telugu document image ocr system indian languages linguistic complexity text line segmentation vertical spatial relation nearest neighbor algorithm word segmentation;data mining;optical character recognition software;robustness image segmentation optical character recognition software nearest neighbor searches clustering algorithms carbon capture and storage educational institutions computational intelligence society computer vision character recognition;word segmentation;spatial relation;image color analysis;feature extraction;nearest neighbor;classification algorithms;document image processing;clustering algorithms;text analysis document image processing feature extraction image segmentation natural language processing optical character recognition;connected component;natural language processing	Designing an OCR system for Indian languages in general is more complex than those of European languages due the linguistic complexity. Efforts are on the way for the development of efficient OCR systems for Indian languages, especially for Telugu, a popular South Indian language. In this paper, we proposed a method for reliable extraction of text line, word and character from document images of Telugu scripts. In the text line segmentation, first we establish the relationship between the connected components and then cluster the connected components of a line using vertical spatial relation and nearest neighbor algorithm. In word segmentation, the space between two adjacent characters is computed and clustered into word space and character space. Consonant and vowel modifiers are segregated from the word image and segment the characters.	connected component (graph theory);k-nearest neighbors algorithm;nearest-neighbor interpolation;optical character recognition;text segmentation	Vijaya Kumar Koppula;Atul Negi;Utpal Garain	2009	2009 Second International Conference on Emerging Trends in Engineering & Technology	10.1109/ICETET.2009.196	natural language processing;speech recognition;computer science;pattern recognition	DB	32.92861779132161	-65.88027926117894	96991
15420dee083e3f9d30389b98ebb82ef57268b94a	a method for character string extraction using local and global segment crowdedness	local segment crowdedness;image segmentation;hough transforms character string extraction local segment crowdedness global segment crowdedness rectangular regions document image processing character recognition thinning;rectangular regions;image converters;global segment crowdedness;data mining;skeleton;hough transforms document image processing character recognition feature extraction string matching;voting;character string extraction;feature extraction;pixel;document image processing;hough transforms;image segmentation data mining voting educational institutions image converters skeleton pixel;string matching;character recognition;thinning	We propose a new method for extracting character strings which have various directions and sizes. In our method, we regard a character string as a rectangular region crowded with short segments. Then two kinds of the segment crowdedness ( local segment crowdedness and global segment crowdedness ) are introduced to extract the rectangular regions ( i.e. strings ). The method was applied to many images which involve strings in various directions and sizes. As a result, almost strings were correctly extracted.	algorithm;string (computer science)	Osamu Shiku;Kikuhito Kawasue;Akira Nakamura	1998		10.1109/ICPR.1998.711879	computer vision;speech recognition;voting;feature extraction;computer science;machine learning;pattern recognition;mathematics;image segmentation;skeleton;pixel;string searching algorithm	Comp.	37.70324191422823	-65.66492533827686	97073
78e3a7b10c15bd632b2ed2ecf464459d0baa5675	a new approach to detect core and delta of the fingerprint using extended relational graph	graph theory;boundary information extended relational graph fingerprint core detection method delta loop core loop computation time reduction;fingerprint recognition image segmentation image matching data security educational institutions data mining law legal factors access control databases;fingerprint identification graph theory object detection;fingerprint identification;object detection	This paper describes the improvement of our fingerprint core detection method using the extended relational graph. The improved approach can detect both of the core and the delta to analyze both of the core loop and the delta loop in the extended relational graph. Furthermore to reduce the computation time, each difference of the ridge direction on the boundary along the loop is used to detect the fingerprint core except the approximated straight lines generated by the boundary information in the extended relational graph. In our experiments, the core and the delta were successfully extracted in 95.6 % of the 224 samples. The processing time was reduced by 6 % from the previous approach.	approximation algorithm;computation;dirac delta function;event loop;experiment;fingerprint;time complexity	Tomohiko Ohtsuka;Akiyoshi Kondo	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530375	fingerprint;computer science;graph theory;theoretical computer science;machine learning;mathematics;distributed computing	Robotics	37.90564994759872	-65.59155729820063	97103
18bc3235d714d8a5dfb0ba3b40e279a8444ddf35	reducing the dimensions of texture features for image retrieval using multi-layer neural networks	feedforward neural network;high dimensionality;neural networks;dimension reduction;image indexing;texture features;key words gabor filter;gabor filter;texture analysis;principal component analysis;multi channel filtering;content based image retrieval;neural network;image retrieval	This paper presents neural network-based dimension reduction of texture features in content-based image retrieval. In particular, we highlight the usefulness of hetero-associative neural networks to this task, and also propose a scheme to combine the hetero-associative and auto-associative functions. A multichannel Gabor-filtering approach is used to derive 30-dimensional texture features from a set of homogeneous texture images. Multi-layer feedforward neural networks are then trained to reduce the number of feature dimensions. Our results show that the methods lead to a reduction of up to 30% while keeping or even improving the performance of similarity ranking. This has the benefit of alleviating the ill-effects of the high dimensionality of features in current image indexing methods and resulting in significant speeding up retrieval rates. Results using principal component analysis are also provided for comparison.		Jose Antonio Catalan;Jesse S. Jin;Tamás D. Gedeon	1999	Pattern Analysis & Applications	10.1007/s100440050028	image texture;computer vision;feedforward neural network;visual word;computer science;machine learning;pattern recognition;time delay neural network;texture filtering;artificial neural network;dimensionality reduction;principal component analysis	Vision	27.704490071784015	-56.718263039582986	97176
6d8a5eefbc6db4a18af5bb5dae3b3761d6bd9e51	research of face recognition based on wavelet transform and principal component analysis	image classification;orl face database face recognition pca wavelet transform;wavelet transforms;simulation experiment;wavelet transform;face recognition;minimum distance;face recognition wavelet transforms face principal component analysis training vectors;principal component analysis;orl face database;wavelet transforms face recognition image classification principal component analysis;pca;orl face database face recognition wavelet transform principal component analysis minimum distance classifier	Principal component analysis (PCA) has been applied in many face recognition systems, and achieved very good results. However, PCA has its limitations: a large amount of calculation and very low capacity of identification. In order to overcome these disadvantages, a new face recognition algorithm which is based on wavelet transform, principal component analysis and minimum distance classifier is proposed in the paper. The simulation experiments based on ORL face database show that the method not only improves the recognition rate, but also reduces the amount of computation. When the training sample is very large, the effectiveness of recognition systems is particularly important.	algorithm;computation;experiment;facial recognition system;principal component analysis;return loss;simulation;wavelet transform	Chunling Fan;Xiuting Chen;Ningde Jin	2012	2012 8th International Conference on Natural Computation	10.1109/ICNC.2012.6234703	speech recognition;machine learning;pattern recognition;eigenface	Robotics	33.34181225656613	-58.444644447183585	97207
c14257ce4a54ecb0caf14c3bc287ae9c5170a0e5	hierarchical architecture for content-based image retrieval of paleontology images	databases;interfaces;image database;indexing and retrieval;multiple scales;k means algorithm;content based image retrieval;visual interfaces;wavelets;image retrieval	In this article a research work in the field of content-based multiresolution indexing and retrieval of images is presented. Our method uses multiresolution decomposition of images using wavelets in the HSV colorspace to extract parameters at multiple scales allowing a progressive (coarse-to-fine) retrieval process. Features are automatically classified into several clusters with K-means algorithm. A model image is computed for each cluster in order to represent all the images of this cluster. The process is reiterated again and again and each cluster is sub-divided into sub-clusters. The model images are stored in a tree which is proposed to users for browsing the database. The nodes of the tree are the families and the leaves are the images of the database. A paleontology images database is used to test the proposed technique. This kind of approach permits to build a visual interface easy to use for users. Our main contribution is the building of the tree with multiresolution indexing and retrieval of images and the generation of model images to be proposed to users.© (2001) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Jérôme Landré;Frédéric Truchetet	2001		10.1117/12.451084	computer vision;visual word;image retrieval;computer science;data mining;automatic image annotation;information retrieval	Vision	38.89516545004534	-61.065539952637984	97332
54b00decf482065e8fbc7b338895e5a5ca415a94	field extraction method from existing forms transmitted by facsimile	robustness field extraction method facsimile form reading faxed form images field extraction geometric transformation form image;business forms facsimile feature extraction;facsimile;feature extraction;business forms;facsimile data mining feature extraction solid modeling robustness character recognition image recognition optical distortion information technology research and development;extraction method	In this paper, we present a field extraction method in form reading application, for existing forms transmitted by facsimile. In faxed form images, it is difficult to determine field locations exactly due to distortions on the form images. To overcome this problem, exclusive forms have been used. In order to make the field extraction easy, these exclusive forms have restrictions on the style. However, existing forms have various styles and no restrictions. Thus, it is hard to read faxed existing forms. The proposed method corrects the location of fields by estimating the geometric transformation of the form image based on the pairs of corresponding feature-points extracted on the form image and a form model. These feature-points can be extracted from table lines, underlines, character strings, and illustrations on existing forms. Moreover, the method estimates the geometric transformation with robustness for the illegal pairs of corresponding feature-points. These characteristics allow the method to read existing forms transmitted by fax. The experimental results with 50 different types of existing forms reveal the effectiveness of the method.		Takashi Hirano;Yasuhiro Okada;Fumio Yoda	2001		10.1109/ICDAR.2001.953887	computer vision;speech recognition;feature extraction;computer science;machine learning	NLP	35.48509469106289	-64.40494900273809	97466
61a52a30542418b31cd205607990fe5256f2c993	group cost-sensitive boosting with multi-scale decorrelated filters for pedestrian detection			pedestrian detection	Chengju Zhou;Meiqing Wu;Siew Kei Lam	2017			computer vision;artificial intelligence;boosting (machine learning);computer science;pattern recognition;pedestrian detection	Vision	30.721982116180765	-57.249825074723674	97527
6be39b5d2bcc3f378c66a278573c9ea6cc6f5bfc	contribution of spatio-temporal intensity variation to bottom-up saliency		We investigate the contribution of local spatio-temporal variation of image intensity to saliency. To measure different types of variation, we use the geometrical invariants of the structure tensor. With a video represented in spatial axes x and y and temporal axis t, the ndimensional structure tensor can be evaluated for different combinations of axes (2D and 3D) and also for the (degenerate) case of only one axis. The resulting features are evaluated on several spatio-temporal scales in terms of how well they can predict eye movements on complex videos. We find that a 3D structure tensor is optimal: the most predictive regions of a movie are those where intensity changes along all spatial and temporal directions. Among two-dimensional variations, the axis pair yt, which is sensitive to horizontal translation, outperforms xy and xt by a large margin, and is even superior in prediction to two baseline models of bottom-up saliency.	apache axis;baseline (configuration management);optic axis of a crystal;structure tensor;top-down and bottom-up design	Eleonora Vig;Michael Dorr;Erhardt Barth	2010		10.1007/978-3-642-32615-8_44	machine learning;computer science;structure tensor;salience (neuroscience);degenerate energy levels;horizontal translation;invariant (mathematics);intrinsic dimension;pattern recognition;artificial intelligence	Vision	37.08614271488142	-52.1203892690922	97602
cee01863f5fe2fedf6138a0e98503ff7cabb0804	the role of polarity in haar-like features for face detection	databases;contrast polarity invariance;detectors;human vision;face detector framework;training;image classification;face recognition;contrast polarity invariance haar like features face detection human vision local contrast perception face detector framework weak classifiers;feature extraction;haar like features;face;humans;face detectors feature extraction databases humans face detection training;point of view;face detection;haar transforms;image classification face recognition haar transforms;weak classifiers;local contrast perception	Human vision is primarily based on local contrast perception and its polarity. Viola and Jones proposed, in their well-known face detector framework, a boosted cascade of weak classifiers based on Haar-like features which encode local contrast and polarity information. Nevertheless contrast polarity invariance, which is not directly modeled in their framework, has been shown to be perceptually relevant for the human capability of detecting faces. In this paper we study, from both algorithmical and perceptual points of view, the effect of enhancing Haar-like features with polarity invariance and how it may improve cascaded classifiers.	encode;extrapolation;face detection;haar wavelet;jones calculus;sensor	Iago Landesa-Vazquez;José Luis Alba-Castro	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.109	facial recognition system;face;computer vision;detector;contextual image classification;face detection;haar-like features;feature extraction;computer science;machine learning;pattern recognition;mathematics	Vision	33.77113995020548	-55.22700556821735	97800
d7a663aae6659e1f6c1fc96ef351cb215f33f113	a novel template matching method for human detection	novel;image motion analysis;bepress selected works;transforms bayes methods image matching image motion analysis object detection;image matching;orientation map human detection template matching;bayes methods;training;false negative;orientation map;bayesian methods;detection;shape;image edge detection;human detection;matching;pixel;novel human method matching template detection;human;transforms;detection rate;humans shape gas discharge devices detection algorithms bayesian methods leg motion detection motion analysis tree data structures biological system modeling;human motion analysis template matching method human detection matching method generalized distance transform orientation map two stage human detection method bayesian verification false negative detection rates false positive detection rates chamfer matching method;humans;distance transform;false positive;template matching;method;template;object detection;matching method	This paper proposes a novel weighted template matching method. It employs a generalized distance transform (GDT) and an orientation map (OM). The GDT allows us to weight the distance transform more on the strong edge points and the OM provides supplementary local orientation information for matching. Based on the matching method, a two-stage human detection method consisting of template matching and Bayesian verification is developed. Experimental results have shown that the proposed method can effectively reduce the false positive and false negative detection rates and perform superiorly in comparison to the conventional Chamfer matching method.	bayesian network;chamfer;distance transform;template matching	Duc Thanh Nguyen;Wanqing Li;Philip Ogunbona	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413925	matching;template;computer vision;method;template matching;type i and type ii errors;bayesian probability;shape;computer science;machine learning;pattern recognition;mathematics;distance transform;pixel	Robotics	33.7404180643143	-56.648784490697935	97826
35c272489e1586a1093d4e317b564afddfa7e9cf	evaluating combinational illumination estimation methods on real-world images	automatic white balance;support vector machines;color constancy;image fusion;committee based;image color analysis lighting estimation training support vector machines geometry image edge detection;image colour analysis;feature extraction;support vector machines combinatorial mathematics feature extraction image colour analysis image fusion;illumination estimation;committee based illumination estimation color constancy automatic white balance;combinatorial mathematics;combinational illumination estimation method noncombinational method support vector regression direct combination trained combinational method indoor outdoor feature 3d stage feature label data high level features extensive tests high level scene content cues supervised training multiple illuminant scenes information fusion multiple subordinate illumination estimation method automatic white balancing color constancy real world images	Illumination estimation is an important component of color constancy and automatic white balancing. A number of methods of combining illumination estimates obtained from multiple subordinate illumination estimation methods now appear in the literature. These combinational methods aim to provide better illumination estimates by fusing the information embedded in the subordinate solutions. The existing combinational methods are surveyed and analyzed here with the goals of determining: 1) the effectiveness of fusing illumination estimates from multiple subordinate methods; 2) the best method of combination; 3) the underlying factors that affect the performance of a combinational method; and 4) the effectiveness of combination for illumination estimation in multiple-illuminant scenes. The various combinational methods are categorized in terms of whether or not they require supervised training and whether or not they rely on high-level scene content cues (e.g., indoor versus outdoor). Extensive tests and enhanced analyzes using three data sets of real-world images are conducted. For consistency in testing, the images were labeled according to their high-level features (3D stages, indoor/outdoor) and this label data is made available on-line. The tests reveal that the trained combinational methods (direct combination by support vector regression in particular) clearly outperform both the non-combinational methods and those combinational methods based on scene content cues.	categorization;color balance;combinational logic;embedded system;embedding;equilibrium;estimated;global illumination;high- and low-level;large;multiclass classification;numerous;online and offline;radial basis function kernel;scale-invariant feature transform;supervised learning;support vector machine;test set;tracer;uc browser	Bing Li;Weihua Xiong;Weiming Hu;Brian V. Funt	2014	IEEE Transactions on Image Processing	10.1109/TIP.2013.2277943	support vector machine;computer vision;feature extraction;computer science;machine learning;pattern recognition;mathematics;image fusion;color constancy	Vision	34.99304759745104	-53.290124130093425	97887
fbdfed192ea0e6508d931b3d00335d99e976c0d7	from selective deep convolutional features to compact binary representations for image retrieval		In the large-scale image retrieval task, the two most important requirements are the discriminability of image representations and the efficiency in computation and storage of representations. Regarding the former requirement, Convolutional Neural Network (CNN) is proven to be a very powerful tool to extract highlydiscriminative local descriptors for effective image search. Additionally, in order to further improve the discriminative power of the descriptors, recent works adopt fine-tuned strategies. In this paper, taking a different approach, we propose a novel, computationally efficient, and competitive framework. Specifically, we firstly propose various strategies to compute masks, namely SIFT-mask, SUM-mask, and MAX-mask, to select a representative subset of local convolutional features and eliminate redundant features. Our in-depth analyses demonstrate that proposed masking schemes are effective to address the burstiness drawback and improve retrieval accuracy. Secondly, we propose to employ recent embedding and aggregating methods which can significantly boost the feature discriminability. Regarding the computation and storage efficiency, we include a hashing module to produce very compact binary image representations. Extensive experiments on six image retrieval benchmarks demonstrate that our proposed framework achieves the state-of-the-art retrieval performances.	algorithmic efficiency;binary image;computation;convolutional neural network;experiment;hash function;image retrieval;loss function;max;os-tan;performance;requirement;scale-invariant feature transform;software framework;storage efficiency	Thanh-Toan Do;Tuan Hoang;Dang-Khoa Le Tan;Ngai-Man Cheung	2018	CoRR		artificial intelligence;discriminative model;convolutional neural network;machine learning;image retrieval;burstiness;pattern recognition;masking (art);hash function;binary number;binary image;computer science	Vision	25.66538925928308	-53.11690195016286	98034
5d929926469829b86c60a4d418dca507f82c6cf5	multi-scale gray level co-occurrence matrices for texture description	texture description;multi scale feature descriptor;gray level co occurrence matrix;image analysis;glcm	Texture information plays an important role in image analysis. Although several descriptors have been proposed to extract and analyze texture, the development of automatic systems for image interpretation and object recognition is a difficult task due to the complex aspects of texture. Scale is an important information in texture analysis, since a same texture can be perceived as different texture patterns at distinct scales. Gray level co-occurrence matrices (GLCM) have been proved to be an effective texture descriptor. This paper presents a novel strategy for extending the GLCM to multiple scales through two different approaches, a Gaussian scale-space representation, which is constructed by smoothing the image with larger and larger low-pass filters producing a set of smoothed versions of the original image, and an image pyramid, which is defined by sampling the image both in space and scale. The performance of the proposed approach is evaluated by applying the multi-scale descriptor on five benchmark texture data sets and the results are compared to other well-known texture operators, including the original ∗Corresponding author. Preprint submitted to Neurocomputing August 7, 2012 GLCM, that even though faster than the proposed method, is significantly outperformed in accuracy.	benchmark (computing);co-occurrence matrix;grayscale;image analysis;low-pass filter;neurocomputing;outline of object recognition;pyramid (image processing);sampling (signal processing);scale space;smoothing	Fernando Roberti de Siqueira;William Robson Schwartz;Hélio Pedrini	2013	Neurocomputing	10.1016/j.neucom.2012.09.042	image texture;computer vision;image analysis;computer science;pattern recognition;mathematics;texture compression;texture filtering;computer graphics (images)	Vision	37.70689348131578	-59.434015048996436	98226
e6a5253bc7ef99230d5a6e11ae2a09718465cfa1	attention-based multi-patch aggregation for image aesthetic assessment		Aggregation structures with explicit information, such as image attributes and scene semantics, are effective and popular for intelligent systems for assessing aesthetics of visual data. However, useful information may not be available due to the high cost of manual annotation and expert design. In this paper, we present a novel multi-patch (MP) aggregation method for image aesthetic assessment. Different from state-of-the-art methods, which augment an MP aggregation network with various visual attributes, we train the model in an end-to-end manner with aesthetic labels only (i.e., aesthetically positive or negative). We achieve the goal by resorting to an attention-based mechanism that adaptively adjusts the weight of each patch during the training process to improve learning efficiency. In addition, we propose a set of objectives with three typical attention mechanisms (i.e., average, minimum, and adaptive) and evaluate their effectiveness on the Aesthetic Visual Analysis (AVA) benchmark. Numerical results show that our approach outperforms existing methods by a large margin. We further verify the effectiveness of the proposed attention-based objectives via ablation studies and shed light on the design of aesthetic assessment systems.	ava radio company;artificial neural network;benchmark (computing);end-to-end principle;image editing;margin (machine learning);mathematical optimization;mobile app;numerical linear algebra;thumbnail	Kekai Sheng;Weiming Dong;Chongyang Ma;Xing Mei;Feiyue Huang;Bao-Gang Hu	2018		10.1145/3240508.3240554	computer vision;convolutional neural network;semantics;intelligent decision support system;machine learning;computer science;artificial intelligence;annotation	AI	25.5071572302926	-54.14930312841123	98227
21890d5bf549a4ef9210a8c524b470787c31832f	face recognition using entropy based face segregation as a pre-processing technique and conservative bpso based feature selection	face recognition;feature extraction;particle swarm optimization;entropy;stationary wavelet transform;image pre processing	The statistical appearance of the face can vary due to various factors such as pose, occlusion, expression and background which makes it a challenging task to have an efficient Face Recognition (FR) system. This paper proposes 4 novel techniques viz., Entropy based Face Segregation (EFS) as pre-processing technique, Double Wavelet Noise Removal (DWNR) as pre-processing technique, 1D Stationary Wavelet Transform (SWT) as Feature Extractor and Conservative Binary Particle Optimization (CBPSO) as Feature Selector to enhance the performance of the system. EFS is used to segregate the facial region, thus removing the cluttered background. DWNR has unique combination of 2D Discrete Wavelet Transform (DWT), Wiener Filter and 2D SWT for image denoising and contrast enhancement. The pre-processed image is then fed to unique combination of 1D DWT, 1D SWT and 1D Discrete Cosine Transform (DCT) to extract essential features. CBPSO is used to select very optimum feature subset and significantly reduce the computation time. The proposed algorithm is experimented on four benchmark databases viz., Color FERET, CMU PIE, Pointing Head Pose and Georgia Tech.	algorithm;benchmark (computing);computation;database;discrete cosine transform;discrete wavelet transform;encrypting file system;feret (facial recognition technology);facial recognition system;feature selection;noise reduction;pose (computer vision);preprocessor;randomness extractor;standard widget toolkit;stationary process;stationary wavelet transform;time complexity;viz: the computer game;wavelet noise;wiener filter	S. Hitesh Babu;Sachin A. Birajdhar;Samarth Tambad	2014		10.1145/2683483.2683529	computer vision;speech recognition;pattern recognition;mathematics;stationary wavelet transform	Vision	34.35695741157437	-59.33639622890319	98235
d9bfff7957dc16cb6c92f20c806b3192d071f9e6	rough-fuzzy clustering and multiresolution image analysis for text-graphics segmentation	texture segmentation;text graphics segmentation;feature selection;rough fuzzy clustering;m band wavelet packet	This paper presents a segmentation method, integrating judiciously the merits of rough-fuzzy computing and multiresolution image analysis technique, for documents having both text and graphics regions. It assumes that the text and non-text or graphics regions of a given document are considered to have different textural properties. The M-band wavelet packet analysis and rough-fuzzy-possibilistic c-means are used for text-graphics segmentation problem. The M-band wavelet packet is used to extract the scalespace features, which offers a huge range of possibilities of scale-space features for document image and is able to zoom it onto narrow band high frequency components. A scale-space feature vector is thus derived, taken at different scales for each pixel in an image. However, the decomposition scheme employing Mband wavelet packet leads to a large number of redundant features. In this regard, an unsupervised feature ough-fuzzy clustering exture segmentation selection method is introduced to select a set of relevant and non-redundant features for text-graphics segmentation problem. Finally, the rough-fuzzy-possibilistic c-means algorithm is used to address the uncertainty problem of document segmentation. The whole approach is invariant under the font size, line orientation, and script of the text. The performance of the proposed technique, along with a comparison with related approaches, is demonstrated on a set of real life document images. © 2015 Elsevier B.V. All rights reserved. 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 . Introduction With the advances in information technology, automated rocessing of documents has become an imperative need. As the orld moves closer to the concept of the paperless office, more nd more communication and storage of documents are performed igitally. In this background, there is a great demand for softare that automatically extracts, analyzes, and stores information rom physical documents for later retrieval. However, the docuents in digitized form require a large amount of storage space, fter being compressed using advanced techniques. Text-graphics egmentation partitions a document image into distinct regions orresponding to the text and non-text parts. In effect, it facilitates fficient searching and storage of text parts of the documents. Many techniques have been proposed to segment the docuent image into text and non-text regions [1,2]. Most popular mong them are top-down and bottom-up approaches. The topPlease cite this article in press as: P. Maji, S. Roy, Rough-fuzzy clusteri tation, Appl. Soft Comput. J. (2015), http://dx.doi.org/10.1016/j.asoc.2 own techniques are based on the difference in contrast between he foreground and background to split the document into columns, aragraphs, text lines, and may be in words. Projection profiles E-mail addresses: pmaji@isical.ac.in (P. Maji), shaswatiroy t@isical.ac.in (S. Roy). ttp://dx.doi.org/10.1016/j.asoc.2015.01.049 568-4946/© 2015 Elsevier B.V. All rights reserved. 57 58 59 [3,4] are popular top-down approaches that work by identifying the white spaces by vertical and horizontal projections. On the other hand, bottom-up methods, which are similarity based document segmentation approaches, tend to cluster pixels with similar intensities to obtain higher level descriptions. The run length smoothing algorithm [5,6] is an example of bottom-up approach, which applies region growing approach to detect text regions. The Docstrum proposed in [7] is another bottom-up method, which groups connected components of the same type using nearest neighbor information, starting from the pixel level to obtain higher level descriptions of the document such as words, text lines, paragraphs, and so on. Each of these methods assumes rectangular blocks of text and graphics, and is sensitive to different textural properties such as font size, text line orientation, and inter-character spacing. Hence, these methods are not effective when any a priori knowledge about the content and attributes of the document image is unavailable. Nicolas et al. [8] extracted the text from document image using 2-D conditional random field model by integrating contextual knowledge and machine learning technique. Another approach for the binarization of a document image was proposed based on a ng and multiresolution image analysis for text-graphics segmen015.01.049 Bayesian framework using Markov random field model of the image [9]. Junga et al. [10] achieved text segmentation by applying region growing procedure based on the response of stroke filter and then improved the segmentation by using an OCR feedback procedure. 60 61 62 63	algorithm;binary image;bottom-up parsing;bottom-up proteomics;cluster analysis;column (database);conditional random field;document layout analysis;feature vector;fuzzy clustering;fuzzy logic;graphics;image analysis;imperative programming;machine learning;markov chain;markov random field;network packet;packet analyzer;paperless office;pixel;real life;region growing;rough set;run-length encoding;scale space;smoothing;text segmentation;top-down and bottom-up design;unsupervised learning;wavelet;white spaces (radio)	Pradipta Maji;Shaswati Roy	2015	Appl. Soft Comput.	10.1016/j.asoc.2015.01.049	computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;data mining;scale-space segmentation;feature selection	Web+IR	36.17926866538829	-65.39148362170195	98326
016de25c1d6e640efc77df97a3c835f6931d7579	an ocr system with ocropus for scientific documents containing mathematical formulas	support vector machines document image processing geometry optical character recognition probability;probability;support vector machines;optical character recognition;geometry;document image processing;geometric features ocropus scientific documents mathematical formula recognition module open source ocr system text lines n gram language model conditional probability svm;text recognition support vector machines optical character recognition software image recognition mathematical model layout accuracy	This paper describes the installation of a mathematical formula recognition module into an open source OCR system: OCRopus. In particular we consider the identification of inline formulas utilizing existing modules. Text lines including math formulas are first processed using a N-gram language model to reduce the number of formula candidates by thresholding the conditional probability of words. Then the formula candidates are classified into formulas and texts by SVM using geometric features associated with the bounding boxes of symbols.	information privacy;instrument flight rules;language model;n-gram;newton–cotes formulas;ocropus;open-source software;smoothing;support vector machine;text corpus;thresholding (image processing)	Fumihiro Furukori;Shinpei Yamazaki;T. Miyagishi;Keiichiro Shirai;Masayuki Okamoto	2013	2013 12th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2013.238	support vector machine;speech recognition;computer science;machine learning;pattern recognition;probability;optical character recognition;statistics	AI	34.33709545282858	-66.14678031469903	98517
91b8a82d5e10c8dcb656b0f796c549fdb30a4f60	feature description by improved local gabor filters and ica	rbf network face recognition muti channel gabor filters;face recognition;gabor filters face feature extraction face recognition databases image recognition radial basis function networks;vectors face recognition feature extraction gabor filters image classification independent component analysis principal component analysis radial basis function networks;rbf network;muti channel gabor filters;principal component analysis feature description gabor filters ica recognition classification independent component analysis face recognition algorithm rbf network local gabor magnitude map lgmm feature vectors	A new face recognition algorithm using the RBF network is proposed based on the improved local Muti-channel Gabor Filters and fixed point ICA. The normalized face image is firstly muti-degree sampled and blocked, and then the blocked face image was filtered by multi-orientation Gabor filters with multi-scale to extract their corresponding Local Gabor Magnitude Map (LGMM), which were constructed to higher dimensional feature vectors. Next, the dimensionality of these vectors is reduced by means of principal component analysis. Finally, the independent components in the resulting vectors with dimensionality reduced were analyzed and extracted by using ICA recognition classification. Experimental results on ORL and YALE face show that the proposed algorithm, which achieves more recognition accuracy rate than other methods.	algorithm;facial recognition system;feature vector;fixed point (mathematics);independent computing architecture;principal component analysis;radial basis function network;return loss	Fan Jiang;Liya Fan	2014	2014 IEEE International Conference on Computer and Information Technology	10.1109/CIT.2014.40	facial recognition system;speech recognition;computer science;machine learning;pattern recognition;gabor wavelet	Vision	34.87300300358678	-58.839609996553314	98615
e693bba181953cf514572cec177b1e6be41394cf	automatic license plate recognition in difficult conditions — technical report		By a car license plate recognition, we mean a software system processing images and providing an alphanumeric transcription of car plates included in an image. We divide the task into four sub-tasks: license plate localization, license plate extraction, characters segmentation and characters recognition. All four sub-tasks are discussed in the context of standard approaches and own solution based on a chain of standard and soft computing image processing algorithms is presented. In this chain, the F-transform approximate pattern matching algorithm plays the crucial role. For the solution, we presented recognition ability for a dataset which includes 500 images with difficult conditions.	approximation algorithm;archive;automatic number plate recognition;benchmark (computing);database;image processing;null character;optical character recognition;pattern matching;soft computing;software system;thresholding (image processing);transcription (software)	Petr Hurtík;Marek Vajgl	2017	2017 Joint 17th World Congress of International Fuzzy Systems Association and 9th International Conference on Soft Computing and Intelligent Systems (IFSA-SCIS)	10.1109/IFSA-SCIS.2017.8023337	alphanumeric;digital image processing;license;software system;soft computing;computer science;string searching algorithm;artificial intelligence;computer vision;technical report	Robotics	35.05131369175346	-66.08686509165132	98642
4907f17540cfd348a1dbb40b8515438b0c163516	scale space co-occurrence hog features for word spotting in handwritten document images		In this paper, the authors proposed a Scale Space Co-occurrence Histograms of Oriented Gradients method SS Co-HOG for retrieving words from digitized handwritten documents. The poor performance of HOG based word spotting in handwritten documents is due to that HOG ignores spatial information of neighboring pixels whereas Co-HOG captures the spatial information of neighboring pixels through counting the occurrence of the gradient orientations of two or more neighboring pixels. The authors employed three scale parameter representation of an image and at each scale, they divide the word image into blocks and Co-HOG features are extracted from each block and finally concatenate them into form a feature descriptor. The proposed method is evaluated using precision and recall metrics through experimentation conducted on popular datasets such as IAM and GW and confirmed that their method outperforms for both the datasets.	scale space	C. Thontadari;C. J. Prabhakar	2016	IJCVIP	10.4018/IJCVIP.2016070105	natural language processing;speech recognition;pattern recognition	Vision	36.066908262668285	-64.88033852227836	98659
6425d5fe9cb2f88b8f363c3bd73d2861ac21b071	color constancy using stage classification	illuminant estimation;svm color constancy illuminant estimation stage classification;color constancy;ill posed problem color constancy stage classification light source;prediction algorithms;image classification;indexing terms;universiteitsbibliotheek;statistics layout light sources color information technology intelligent systems roads reflectivity large scale systems classification algorithms;large scale;visualization;3d model;stage classification;image edge detection;three dimensional displays;image color analysis;image colour analysis;ill posed problem;classification algorithms;image colour analysis image classification;svm;light sources	The aim of color constancy is to remove the effect of the color of the light source. Since color constancy is inherently an ill-posed problem, different assumptions have been proposed. Because existing color constancy algorithms are based on specific assumptions, none of them can be considered as universal. Therefore, how to select a proper algorithm for a given imaging configuration is an important question.	algorithm;well-posed problem	Rui Lu;Arjan Gijsenij;Theo Gevers;Koen E. A. van de Sande;Jan-Mark Geusebroek;De Xu	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414083	statistical classification;support vector machine;computer vision;contextual image classification;visualization;index term;color normalization;prediction;computer science;machine learning;pattern recognition;mathematics;color constancy	Robotics	35.99885172901024	-53.41978649123335	98822
95d00f08b5dca9b4a409098a54ba2de64ca95b7f	inter-modality face sketch recognition	databases;histograms;histogram of oriented gradients face sketch recognition inter modality;face feature extraction face recognition databases histograms shape accuracy;law;cufs database intermodality face sketch recognition law enforcement face descriptor gradient orientations feature extraction histogram of averaged oriented gradients haog;accuracy;inter modality;histogram of oriented gradients;face recognition;shape;feature extraction;gradient methods;face;law face recognition feature extraction gradient methods;face sketch recognition	Automatic face sketch recognition plays an important role in law enforcement. Recently, various methods have been proposed to address the problem of face sketch recognition by matching face photos and sketches, which are of different modalities. However, their performance is strongly affected by the modality difference between sketches and photos. In this paper, we propose a new face descriptor based on gradient orientations to reduce the modality difference in feature extraction stage, called Histogram of Averaged Oriented Gradients (HAOG). Experiments on CUFS database show that the new descriptor outperforms the state-of-the-art approaches.	elegant degradation;feature extraction;gradient descent;image gradient;modality (human–computer interaction);sketch recognition	Hamed Kiani Galoogahi;Terence Sim	2012	2012 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2012.128	facial recognition system;face;computer vision;speech recognition;feature extraction;shape;histogram of oriented gradients;computer science;pattern recognition;three-dimensional face recognition;histogram;accuracy and precision;sketch recognition;statistics	Vision	35.95143705131676	-56.5986013099352	98841
e6768d1566d3d5d63c8920f59182c7d832ad3b75	ntf vs. pca features for searching in a spectral image database		A technique for searching in a spectral image database is proposed in this study. It is based on a similarity measure between spectral image features. New and convenient spectral image features are introduced and compared here. Nonnegative tensor factorization (NTF) and principal component analysis (PCA) are applied in a spectral image domain to characterize colors of a spectral image. A new way of NTF with a multiresolution approach is used to accelerate the time complexity in the extraction of the features. The proposed method is implemented and tested with a spectral image database. The images from the database are ordered according to the similarity between them and the tested image. Three similarity measures were applied in the two spectral image feature spaces. The results of the experiments are visually represented. The best combination of the spectral image feature and similarity measure in our opinion is proposed during a discussion part. Also further work will be proposed.	color;experiment;feature (computer vision);multiresolution analysis;national transfer format;principal component analysis;similarity measure;time complexity	Alexey Andriyashin;Arto Kaarna;Timo Jääskeläinen;Jussi Parkkinen	2008			artificial intelligence;pattern recognition;mathematics	Vision	38.41717295146358	-61.094287482318386	98874
c8fbbee699b4d19d59c0ae9267f16980e032c9eb	a review on vision techniques applied to human behaviour analysis for ambient-assisted living	motion analysis;activities of daily living adls;human behaviour;ambient assisted living;info eu repo semantics article;computer vision;action recognition;activity recognition	Human Behaviour Analysis (HBA) is more and more being of interest for Computer Vision and Artificial Intelligence researchers. Its main application areas, like Video Surveillance and Ambient–Assisted Living (AAL), have been in great demand in recent years. This paper provides a review on HBA for AAL and ageing in place purposes focusing specially on vision techniques. First, a clearly defined taxonomy is presented in order to classify the reviewed works, which are consequently presented following a bottom-up abstraction and complexity order. At the motion level, pose and gaze estimation as well as basic human movement recognition are covered. Next, the mainly used action and activity recognition approaches are presented with examples of recent research works. Increasing the degree of semantics and the time interval involved in the HBA, finally the behaviour level is reached. Furthermore, useful tools and datasets are analysed in order to provide help for initiating projects.	atm adaptation layer;activity recognition;andrei toom;artificial intelligence;bottom-up parsing;computer vision;goto;host adapter;linear algebra;motion estimation;multimodal interaction;systems design;way to go	Alexandros André Chaaraoui;Pau Climent-Pérez;Francisco Flórez-Revuelta	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.03.005	simulation;computer science;artificial intelligence;activity recognition	AI	24.65454369832898	-58.66526307049645	98942
9c9ee0e2f36793a70fd09a53c06e7393e405f306	biometric verification by palmprint using contourlet transform		Palmprint is a reliable biometric that can be used for identity verification because it is stable and unique for every individual. Palmprint images contain rich, unique features for reliable human identification, which makes it a very competitive topic in biometric research. In this paper, a personal authentication method is proposed which is based on palmprint images and employs contourlet transform for feature extraction process. Contourlet transform extracts image curvatures and smoothness with multidirectional decomposition capability. The proposed method includes three steps, preprocessing, feature extraction, and classification. The central part of each palmprint is determined in preprocessing step. For feature extracting, contourlet transform is applied to the central part of palmprint and then features are extracted from created subbands. Finally, for each image, 384 features are obtained. A last, Naïve Bayes, Support Vector Machine (SVM) and k-Nearest Neighbor (kNN) classifiers are employed. Experiments on three databases resulted recognition accuracies of 99.41%, 92.38%, and 85.34% on PolyU, COEP and IITD databases, correspondingly using kNN classifier. The results demonstrate the efficiency and validity of the proposed method in personal authentication by palmprint images.	authentication;biometrics;contourlet;database;discrete cosine transform;feature extraction;fingerprint;gabor filter;identity verification service;naive bayes classifier;nearest neighbour algorithm;preprocessor;support vector machine;wavelet transform	Hedieh Sajedi	2016	Intelligent Decision Technologies	10.3233/IDT-160270	contourlet;computer vision;electronic engineering;biometrics;artificial intelligence;computer science	Vision	33.2904263164507	-61.40886841313639	98963
60a95086edaf637f5ef549a49e57d76332d323c1	a modular clutter rejection technique for flir imagery using region-based principal component analysis	clutter rejection;feature vector;principal component analysis;region of interest;automatic target recognition;learning vector quantization	I n this paper]-, a modular clutter rejection technique using region-based principal component analysis (PCA) is proposed. Our modular clutter rejection system uses dynamic ROI extraction to overcome the problem of poorly centered targets. I n dynamic R O I extraction, a representative R O I is moved in several directions with respect to the center of the potential target image to extract a number of ROIs. Each module in the proposed system applies region-based PCA to generate the feature vectors, which are subsequently used to decide about the identity of the potential target. W e also present experimental results using real-life data evaluating and comparing the performance of the clutter rejection systems with static and dynamic R O I extraction.	clutter;feature vector;principal component analysis;real life;region of interest;rejection sampling	Syed A. Rizvi;Nasser M. Nasrabadi	2002	Pattern Recognition	10.1016/S0031-3203(01)00221-7	computer vision;speech recognition;feature vector;learning vector quantization;computer science;machine learning;pattern recognition;automatic target recognition;principal component analysis;region of interest	Robotics	33.87356525350826	-57.68280035023624	98987
0dbbaab888d4cd00d17db43eb704bc983766cf61	real-time photo style transfer	automatic image manipulation technique;histograms;paper;yarn;image processing;color space;real time;histograms space technology statistical analysis image color analysis image processing character generation computer science computational efficiency pixel image converters;statistical analysis image colour analysis;global luminance histogram;color model;real time photo style transfer;psychologically opponent color theory;cuda;statistical analysis;streaming media;real time graphics;image color analysis;image colour analysis;nvidia geforce 8800 gts;orgb color space;transforms;nvidia;real time implementation;directx;real time systems;global luminance histogram real time photo style transfer automatic image manipulation technique orgb color space psychologically opponent color theory statistical analysis	This paper presents a novel approach for real-time photo style transfer. The automatic image manipulation technique is performed in the oRGB color space, which is a new color model based on the psychologically opponent color theory. We transfer color from an appropriate source image to the target image using a simple statistical analysis. In addition, we match the global luminance histogram to achieve better photographic look. Note that the whole pipeline is highly parallel, enabling a GPU-based real-time implementation. Several experimental results are shown to demonstrate the effectiveness and efficiency of the proposed method.	algorithm;color space;graphics processing unit;luminance hdr;opponent process;real-time clock;real-time transcription	Hanli Zhao;Xiaogang Jin;Jianbing Shen;Feifei Wei	2009	2009 11th IEEE International Conference on Computer-Aided Design and Computer Graphics	10.1109/CADCG.2009.5246916	computer vision;color model;color normalization;image processing;computer science;histogram;multimedia;directx;color space;statistics;computer graphics (images)	EDA	38.79443002826734	-53.91459971705596	99325
ae7d1d3049a8f9a4bce449875243f55e733a3508	a new color feature extraction method based on dynamic color distribution entropy of neighborhoods		One of the important requirements in image retrieval, indexing, classification, clustering and etc. is extracting efficient features from images. The color feature is one of the most widely used visual features. Use of color histogram is the most common way for representing color feature. One of disadvantage of the color histogram is that it does not take the color spatial distribution into consideration. In this paper dynamic color distribution entropy of neighborhoods method based on color distribution entropy is presented, which effectively describes the spatial information of colors. The image retrieval results in compare to improved color distribution entropy show the acceptable efficiency of this approach.	cluster analysis;color histogram;database;experiment;feature extraction;image retrieval;pixel;requirement;statistical classification	Fatemeh Alamdar;Mohammad Reza Keyvanpour	2011	CoRR		color histogram;computer vision;color quantization;hsl and hsv;color normalization;pattern recognition;mathematics;color balance;color space;histogram equalization;information retrieval	Vision	38.94633654205778	-60.91715432846792	99361
05ee231749c9ce97f036c71c1d2d599d660a8c81	ghostvlad for set-based face recognition		The objective of this paper is to learn a compact representation of image sets for template-based face recognition. We make the following contributions: first, we propose a network architecture which aggregates and embeds the face descriptors produced by deep convolutional neural networks into a compact fixed-length representation. This compact representation requires minimal memory storage and enables efficient similarity computation. Second, we propose a novel GhostVLAD layer that includes ghost clusters, that do not contribute to the aggregation. We show that a quality weighting on the input faces emerges automatically such that informative images contribute more than those with low quality, and that the ghost clusters enhance the network’s ability to deal with poor quality images. Third, we explore how input feature dimension, number of clusters and different training techniques affect the recognition performance. Given this analysis, we train a network that far exceeds the state-of-the-art on the IJB-B face recognition dataset. This is currently one of the most challenging public benchmarks, and we surpass the state-of-the-art on both the identification and verification protocols.	artificial neural network;benchmark (computing);computation;computer data storage;convolutional neural network;expectation propagation;facial recognition system;grammar-based code;information;network architecture;self-information;x image extension	Yujie Zhong;Relja Arandjelovic;Andrew Zisserman	2018	CoRR		pattern recognition;machine learning;convolutional neural network;network architecture;cluster (physics);computation;artificial intelligence;feature dimension;computer science;facial recognition system;weighting	Vision	25.395370454776252	-52.82853063449387	99573
2fc647b05a93088b6a1033c573bb485b33a68ab7	blind image tamper detection based on multimodal fusion	digital forensics;multimodal fusion;image tampering;image fusion;canonical correlation analysis;fisher linear discriminant;feature selection	In this paper, we propose a novel feature processing approach based on fusion of noise and quantization residue features for detecting tampering or forgery in video sequences. The evaluation of proposed residue features - the noise residue features and the quantization features, their transformation in optimal feature subspace based on fisher linear discriminant features and canonical correlation analysis features, and their subsequent fusion for emulated copy-move tamper scenarios shows a significant improvement in tamper detection accuracy.	multimodal interaction	Girija Chetty;Monica Singh;Matthew White	2010		10.1007/978-3-642-17534-3_69	computer vision;canonical correlation;speech recognition;computer science;digital forensics;machine learning;pattern recognition;image fusion;feature selection	Vision	35.25830206383706	-60.861466427724245	99579
139764a02d54e62d2d5e8571189ef1be845db8e2	combining different local binary pattern variants to boost performance	image analysis;local binary patterns;support vector machine;texture descriptors;local binary pattern histogram fourier features	0957-4174/$ see front matter 2010 Elsevier Ltd. A doi:10.1016/j.eswa.2010.11.048 ⇑ Corresponding author. E-mail addresses: loris.nanni@unibo.it, lnanni@de nam@missouristate.edu (S. Brahnam), alessandra.lum This paper focuses on the combination of variants of local binary patterns (LBP), widely considered the state of the art among texture descriptors, using the same radius and the same number of neighborhoods. We report new experiments exploring several LBP-based descriptors and propose a set of variants for the representation of images. Our experiments are of two main types. In the first set, the Fourier transform is used to extract features starting from the histogram of uniform patterns. In these experiments we test different methods of extracting features from the histogram and each method is used to train a set of support vector machines (SVMs) which are then combined. In the second set of experiments, features are extracted from histograms using different definitions of uniform patterns. These are used to train SVMs, and the results are then combined. Our results show that descriptors extracted from LPB using the same radius and the same number of neighborhoods can be combined to improve classifier performance. 2010 Elsevier Ltd. All rights reserved.	belief propagation;binary pattern (image generation);experiment;feature extraction;futures studies;gabor wavelet;local binary patterns;standalone program;support vector machine;whole earth 'lectronic link	Loris Nanni;Sheryl Brahnam;Alessandra Lumini	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.11.048	support vector machine;image analysis;local binary patterns;computer science;machine learning;pattern recognition;data mining;mathematics	Web+IR	35.72381888052735	-59.33487031912528	99947
1d925631a753abeed830ac0e543bbe480cd3e62b	a dynamic algorithm for palmprint recognition	databases;biometric systems dynamic algorithm palmprint recognition dynamical system approach iteration;biometrics access control;reglerteknik;yttrium;heuristic algorithms;feature extraction;palmprint recognition;heuristic algorithms biometrics access control feature extraction yttrium security databases;security	Most of the existing techniques for palmprint recognition are based on metrics that evaluate the distance between a pair of features. These metrics are typically based on static functions. In this paper we propose a new technique for palmprint recognition based on a dynamical system approach, focusing on preliminary experimental results. The essential idea is that the procedure iteratively eliminates points in both images to be compared which do not have enough close neighboring points in the image itself and in the comparison image. As a result of the iteration, in each image the surviving points are those having enough neighboring points in the comparison image. Our preliminary experimental results show that the proposed dynamic algorithm is competitive and slightly outperforms some state-of-the-art methods by achieving a higher genuine acceptance rate.	algorithm;dynamic problem (algorithms);dynamical system;fingerprint;iteration;method (computer programming)	David Palma;Pier Luca Montessoro;Giulia Giordano;Franco Blanchini	2015	2015 IEEE Conference on Communications and Network Security (CNS)	10.1109/CNS.2015.7346883	feature extraction;computer science;information security;yttrium;machine learning;pattern recognition;data mining	Vision	36.71845625007712	-61.606603740556395	99969
f8e478585948c28bb8ff85c847aee17acb58b708	text/non-text separation from handwritten document images using lbp based features: an empirical study		Isolating non-text components from the text components present in handwritten document images is an important but less explored research area. Addressing this issue, in this paper, we have presented an empirical study on the applicability of various Local Binary Pattern (LBP) based texture features for this problem. This paper also proposes a minor modification in one of the variants of the LBP operator to achieve better performance in the text/non-text classification problem. The feature descriptors are then evaluated on a database, made up of images from 104 handwritten laboratory copies and class notes of various engineering and science branches, using five well-known classifiers. Classification results reflect the effectiveness of LBP-based feature descriptors in text/non-text separation.	belief propagation;document classification;feature model;local binary patterns	Sourav Ghosh;Dibyadwati Lahiri;Showmik Bhowmik;Ergina Kavallieratou;Ram Sarkar	2018	J. Imaging	10.3390/jimaging4040057	mathematics;empirical research;local binary patterns;computer vision;artificial intelligence;operator (computer programming);pattern recognition	SE	35.56779155076569	-59.35011081017649	99986
e2c79f6e4afc691ed6a72b83298e6ecbfe13e0f4	a one-sample per individual face recognition algorithm based on multiple one-dimensional projection lines		This paper proposes a novel approach for face recognition when only one sample per individual is available. The proposed technique, referred to as MODPL, determines a one-dimensional projection line for each individual in the dataset. Each of these lines discriminates the corresponding individual with respect to the other people in the database. The vector consisting on the projections of the individual's raw data on the di®erent projections lines provides an excellent characterization of the individual. Results obtained using the XM2VTS database show that the proposed technique is capable of achieving classi ̄cation rates similar to the ones obtained by means of the Uniform-pursuit algorithm and at least 5% higher than other currently used techniques that deal with the one sample problem. Two additional sets of experiments were conducted on the BioID and AR databases, where the proposed algorithm showed a performance similar to the state-of-the-art algorithms. Moreover, the proposed technique allows the visualization of the most discriminative features of the individuals.	algorithm;database;experiment;facial recognition system;hidden surface determination;nl (complexity);numerical aperture	David Delgado-Gómez;Diego Ruiz-Hernández	2017	IJPRAI	10.1142/S0218001417560122	artificial intelligence;discriminative model;pattern recognition;machine learning;raw data;linear regression;visualization;facial recognition system;algorithm;computer science	Vision	31.81778882932532	-53.22587065332422	100114
5adc2c8fc8c851a7e14665c581328c7e5cca0c00	a novel inheritable color space with application to kinship verification	measurement;anthropology kinship verification inheritable feature inheritable color space generalized incs framework color similarity measure decorrelation property kinfacew i dataset kinfacew ii dataset ub kinface dataset cornell kinface dataset illumination variation gincs framework;image colour analysis anthropology decorrelation;marine vehicles;image color analysis decorrelation lighting robustness marine vehicles measurement linear programming;image color analysis;linear programming;decorrelation;robustness;lighting	Anthropology studies discover that some genetic related facial features, which are inherited by children from their parents, can be used for kinship verification. This paper investigates an important inheritable feature - color and presents a novel inheritable color space (InCS) and a generalized InCS (GInCS) framework with application to kinship verification. Specifically, a novel color similarity measure (CSM) is first defined. Second, based on this similarity measure, a new inheritable color space (InCS) is derived by balancing the criterion of minimizing the distance between kinship pairs and the criterion of maximizing the distance between non-kinship pairs. Unlike conventional color spaces, e.g. the RGB color space, the proposed InCS, which is learned automatically from the data, captures the inheritable information between parent and child. Third, theoretical and empirical analysis show that the proposed InCS exhibits the decorrelation property, which is positively related to the performance of kinship verification. Robustness to the illumination variation is also discussed. Fourth, a generalized InCS framework is presented to extend the InCS from the pixel level to the feature level for improving the performance and the robustness to illumination variation. The proposed InCS is evaluated on several popular datasets, namely the KinFaceW-I dataset, the KinFaceW-II dataset, the UB KinFace dataset, and the Cornell KinFace dataset. Experimental results show that the proposed InCS is able to (i) improve the conventional color spaces such as RGB, YUV, YIQ color spaces by a large margin, (ii) achieve robustness to the illumination variation, and (iii) outperforms other popular methods.	color space;decorrelation;pixel;similarity measure;unified extensible firmware interface	Qingfeng Liu;Ajit Puthenputhussery;Chengjun Liu	2016	2016 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2016.7477667	computer vision;decorrelation;computer science;linear programming;artificial intelligence;machine learning;lighting;measurement;statistics;robustness	Vision	36.93694015381981	-56.79396168531412	100173
f65f68b5abf9a7ffc002691f421ebb3128ff104d	detection of video-based face spoofing using lbp and multiscale dct		Despite the great deal of progress during the recent years, face spoofing detection is still a focus of attention. In this paper, an effective, simple and time-saving countermeasure against video-based face spoofing attacks based on LBP (Local Binary Patterns) and multiscale DCT (Discrete Cosine Transform) is proposed. Adopted as the low-level descriptors, LBP features are used to extract spatial information in each selected frame. Next, multiscale DCT is performed along the ordinate axis of the obtained LBP features to extract spatial information. Representing both spatial and temporal information, the obtained high-level descriptors (LBP-MDCT features) are finally fed into a SVM (Support Vector Machine) classifier to determine whether the input video is a facial attack or valid access. Compared with state of the art, the excellent experimental results of the proposed method on two benchmarking datasets (Replay-Attack and CASIA-FASD dataset) have demonstrated its effectiveness.	anomaly detection;apache axis;belief propagation;high- and low-level;local binary patterns;modified discrete cosine transform;multiscale modeling;replay attack;spoofing attack;support vector machine	Ye Tian;Shijun Xiang	2016		10.1007/978-3-319-53465-7_2	local binary patterns;support vector machine;replay attack;computer vision;artificial intelligence;discrete cosine transform;benchmarking;ordinate;spatial analysis;computer science;spoofing attack	Vision	33.48847610415442	-56.81002625742566	100201
6310b59b3d786c842121716809f09a750ee8b6f6	a fast nearest neighbor search algorithm by filtration	query processing;nearest neighbor searching;nearest neighbor;pattern recognition;ocr;nearest neighbor search;gsc classifier;filtration;article;additive binary tree;binary tree	"""Classifying an unknown input is a fundamental problem in Pattern Recognition. One standard method is """"nding its nearest neighbors in a reference set. It would be very time consuming if computed feature by feature for all templates in the reference set; this namKve method is O(nd) where n is the number of templates in the reference set and d is the number of features or dimension. For this reason, we present a technique for quickly eliminating most templates from consideration as possible neighbors. The remaining candidate templates are then evaluated feature by feature against the query vector. We utilize frequencies of features as a pre-processing to reduce query processing time burden. Our approach is simple to implement and achieves great speedup experimentally. The most notable advantage of the new method over other existing techniques occurs where the number of features is large and the type of each feature is binary although it works for other type features. We improved our OCR system at least twice (without a threshold) or faster (with higher threshold value) by using the new algorithm. 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved."""	database;experiment;nearest neighbor search;pattern recognition;preprocessor;search algorithm;speedup	Sung-Hyuk Cha;Sargur N. Srihari	2002	Pattern Recognition	10.1016/S0031-3203(01)00032-2	nearest-neighbor chain algorithm;ball tree;best bin first;binary tree;filtration;computer science;machine learning;pattern recognition;data mining;mathematics;nearest neighbor search;fixed-radius near neighbors;k-nearest neighbors algorithm	Vision	35.9244002336782	-61.54796400307994	100277
9e6803600c033ebf16b046a223969e340dd16f09	2d gabor filters-based steganalysis of content-adaptive jpeg steganography		To improve the detection accuracy for content-adaptive JPEG steganography, which often constrains the embedding changes to complex texture regions, a new steganalysis feature called the GRF (Gabor Rich Feature) based on two-dimensional (2D) Gabor filters is proposed. First, the diverse 2D Gabor filters are generated and used to filter the decompressed JPEG image. Second, five types of statistical features are extracted from the filtered images and these features are merged according to their respective symmetry. Third, all the features are combined and feature selection is performed to reduce dimensionality. Last, an ensemble classifier is used to assemble the steganalysis feature as well as the final steganalyzer. The experimental results show that the proposed steganalysis feature can achieve a performance that is competitive with the state-of-the-art steganalysis features when used for the detection of the latest content-adaptive JPEG steganography algorithms.	algorithm;ensemble learning;feature selection;gabor filter;jpeg;steganalysis;steganography	Xiaofeng Song;Fenlin Liu;Zhengui Zhang;Chunfang Yang;Xiangyang Luo;Liju Chen	2016	Multimedia Tools and Applications	10.1007/s11042-016-4157-9	computer vision;speech recognition;steganalysis;pattern recognition	Vision	35.478404350027276	-60.33542466080407	100290
63bd8a94dd52cfa9c4cdf4067d8d48ecf235dea6	making residual vector distribution uniform for distinctive image representation	vocabulary computational complexity feature extraction image representation principal component analysis vectors;flickr1m uniform residual vector distribution distinctive image representation vector of locally aggregated descriptors vlad feature space discriminative power computational complexity principal component analysis parameters hierarchical scheme hidden layer visual vocabulary coarse vocabulary image descriptor whitening operation local descriptor holidays data set ukbench data set oxford building data set;hierarchical vlad;local features image representation hierarchical vlad hidden layer image search image descriptor;hidden layer;local features;image representation;image search;vocabulary vectors visualization image representation principal component analysis accuracy quantization signal;image descriptor	Recently, image representation by vector of locally aggregated descriptors (VLADs) has been demonstrated to be super efficient in image representation. Due to the coarse division in the feature space, its discriminative power is limited. One intuitive way to address this issue is to construct a VLAD with a larger vocabulary, but this will lead to a higher dimensional VLAD and suffer more computational complexity when learning the principal component analysis parameters used to project VLAD onto a low-dimensional space. In this paper, we propose a hierarchical scheme to build the VLAD. In our approach, by generating some subwords to each visual word of a coarse vocabulary, a hidden layer visual vocabulary is constructed. With the hidden layer visual vocabulary, the feature space is divided finer. Then, we aggregate the residues in the hidden layer vocabulary to the coarse layer to obtain an image descriptor that is of the same dimension as the original VLAD. In addition, we reveal that performing the whitening operation to local descriptor can further enhance the discriminative power of the VLAD. We validate our approach with experiments mainly conducted on three benchmark data sets, i.e., Holidays data set, UKBench data set, and Oxford Building data set with Flickr1M as distractors and make comparison with the related algorithms on VLAD. The experimental results demonstrate the effectiveness of our algorithm.	aggregate data;algorithm;benchmark (computing);computational complexity theory;computer vision;decorrelation;experiment;feature vector;principal component analysis;visual word;visual descriptor;vocabulary	Zhen Liu;Houqiang Li;Wengang Zhou;Ting Rui;Qi Tian	2016	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2015.2409693	computer vision;feature detection;machine learning;pattern recognition;mathematics	Vision	35.51374647856654	-57.565890312252485	100448
905364187f6b5d55e15e737f9193b8a94d74ea0b	elastic matching algorithms for online tamil character recognition	handwriting recognition;distance measure;on line handwriting recognition;tamil character;error analysis;reconnaissance ecriture;reconnaissance caractere;estimation erreur;error estimation;estimacion error;pattern recognition;reconnaissance forme;reconocimiento patron;character recognition;handwritten character recognition;reconocimiento caracter;reconnaissance caractere manuscrit;elastic matching	We present a comparison of elastic matching schemes for writer dependent on-line handwriting recognition of isolated Tamil characters. Three different features are considered namely, preprocessed x-y co-ordinates, quantized slope values, and dominant point co-ordinates. Seven schemes based on these three features and dynamic time warping distance measure are compared with respect to recognition accuracy, recognition speed, and number of training templates. Along with these results, possible grouping strategies and error analysis is also presented in	algorithm;dynamic time warping;elastic matching;error analysis (mathematics);handwriting recognition;image warping;online and offline;point of view (computer hardware company);quantization (signal processing)	Niranjan Joshi;G. Sita;A. G. Ramakrishnan;Sriganesh Madhvanath	2004		10.1007/978-3-540-30499-9_126	computer vision;speech recognition;computer science;handwriting recognition	Vision	32.92638950912791	-64.97759959132561	100665
bafb50940a6431f767811c8117e901bbd826679f	chinese character-level writer identification using path signature feature, dropstroke and deep cnn	pipelines accuracy handwriting recognition image recognition;data augmentation;pathsignature feature;chinese character;deep convolutional neural network;chinese character deep convolutional neural network online textindependent writer identification data augmentation pathsignature feature;online textindependent writer identification	Most existing online writer-identification systems require that the text content is supplied in advance and rely on separately designed features and classifiers. The identifications are based on lines of text, entire paragraphs, or entire documents; however, these materials are not always available. In this paper, we introduce a path-signature feature to an end-to-end text-independent writer-identification system with a deep convolutional neural network (DCNN). Because deep models require a considerable amount of data to achieve good performance, we propose a data-augmentation method named DropStroke to enrich personal handwriting. Experiments were conducted on online handwritten Chinese characters from the CASIA-OLHWDB1.0 dataset, which consists of 3,866 classes from 420 writers. For each writer, we only used 200 samples for training and the remaining 3,666 for testing. The results reveal that the path-signature feature is useful for writer identification, and the proposed DropStroke technique enhances the generalization and significantly improves performance.	artificial neural network;convolutional neural network;document;end-to-end principle	Weixin Yang;Lianwen Jin;Manfei Liu	2015	2015 13th International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2015.7333821	speech recognition;computer science;machine learning;pattern recognition	SE	24.836050883750964	-61.954739794488255	100863
69ad061dd5908aa47afd776b3d58bc949110e2d1	discriminative semantic parts learning for object detection			object detection	Yurui Xie;Qingbo Wu;Bing Luo	2015	IEICE Transactions		computer vision;object-class detection;computer science;viola–jones object detection framework;machine learning;pattern recognition;sparse approximation	Vision	30.671322128952383	-56.73722966501827	100868
2231dfbbb37386d408c9cc30ab386a56beba8146	dcf-bow: build match graph using bag of deep convolutional features for structure from motion		Matching a large number of images is quite time-consuming for structure from motion (SfM) due to the image matching by comparing features between all image pairs. In this letter, a bag of deep convolutional features (DCF-BoW) model is proposed to create match graph to reduce the number of matches. First, the convolutional feature map of an image is extracted using the VGG-16 convolutional neural network trained on ImageNet. Then, each local region in the original image can be represented by a feature vector in the feature map. The feature vectors are normalized and used to construct a bag of words model, which could convert each image into a DCF-BoW representation. Finally, the match graph is constructed by selecting the top 10 images with the highest similarities, which are calculated by computing the distances between those DCF-BoW representations. The experiment results show that the proposed DCF-BoW can create the match graph effectively in short time and find the potential overlapping image pairs. The match graph created by the proposed DCF-BoW is better than those built by DBoW3 and VocabTree, which is clearly showed in precision-recall curve on the Urban data set. The results of the SfM reconstruction based on the match graph created by the proposed DCF-BoW are slightly worse than those of the exhaustive matching, while the number of matches is reduced by 97.4% and 92.1%, respectively, on the Urban and South Building data sets.		Jie Wan;A Yilmaz;Lei Yan	2018	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2018.2864116		Vision	32.90752185562864	-53.777561058775625	101236
3a37c644e2468366a6fc4f3bea7866e49ef5c461	a study on topological descriptors for the analysis of 3d surface texture		Methods from computational topology are becoming more and more popular in computer vision and have shown to improve the state-of-the-art in several tasks. In this paper, we investigate the applicability of topological descriptors in the context of 3D surface analysis for the classification of different surface textures. We present a comprehensive study on topological descriptors, investigate their robustness and expressiveness and compare them with state-of-the-art methods including Convolutional Neural Networks (CNNs). Results show that class-specific information is reflected well in topological descriptors. The investigated descriptors can directly compete with non-topological descriptors and capture complementary information. As a consequence they improve the state-of-the-art when combined with non-topological descriptors.		Matthias Zeppelzauer;Bartosz Zielinski;Mateusz Juda;Markus Seidl	2018	Computer Vision and Image Understanding	10.1016/j.cviu.2017.10.012	robustness (computer science);artificial intelligence;mathematics;convolutional neural network;machine learning;computational topology;computer vision;expressivity;surface finish;topology	Vision	37.827103370410846	-55.91586781665018	101275
97ab71d3b58d049a57743cc140062ef035bffe21	shape codebook based handwritten and machine printed text zone extraction	tabla codificacion;traitement image document;4230;document imprime;learning;caracter manuscrito;0130c;manuscript character;imagerie;image bruitee;imagen sonora;histogram;apprentissage;imagery;printed document;documento impreso;histogramme;codebook;feature extraction;table codage;noisy image;pixel;document image processing;imagineria;support vector machine;extraction caracteristique;classification accuracy;histograma;caractere manuscrit	In this paper, we present a novel method for extracting handwritten and printed text zones from noisy document images with mixed content. We use Triple-Adjacent-Segment (TAS) based features which encode local shape characteristics of text in a consistent manner. We first construct two codebooks of the shape features extracted from a set of handwritten and printed text documents respectively. We then compute the normalized histogram of codewords for each segmented zone and use it to train a Support Vector Machine (SVM) classifier. The codebook based approach is robust to the background noise present in the image and TAS features are invariant to translation, scale and rotation of text. In experiments, we show that a pixel-weighted zone classification accuracy of 98% can be achieved for noisy Arabic documents. Further, we demonstrate the effectiveness of our method for document page classification and show that a high precision can be achieved for the detection of machine printed documents. The proposed method is robust to the size of zones, which may contain text content at line or paragraph level.	code word;codebook;encode;experiment;image noise;monochrome;pixel;printing;robustness (computer science);support vector machine;test data;text segmentation;thermal-assisted switching	Jayant Kumar;Rohit Prasad;Huaigu Cao;Wael Abd-Almageed;David S. Doermann;Premkumar Natarajan	2011		10.1117/12.876725	support vector machine;computer vision;speech recognition;feature extraction;computer science;machine learning;codebook;pattern recognition;histogram;pixel;statistics	AI	35.770714052174895	-65.21995967846387	101299
706e7d8f9bc933ef5af20f6399854ec469dd8b00	performance analysis of feature extractors and classifiers for script recognition of english and gurmukhi words	probibilistic neural networks;discrete cosine transforms;gabor;k nearest neighbor;support vector machine	Script Recognition is a challenging field for the recognition of documents in a multilingual country like India where different scripts are in use. For optical character recognition of such multilingual documents, it is necessary to separate blocks, lines, words and characters of different scripts before feeding them to the OCRs of individual scripts. Many approaches have been proposed by the researchers towards script recognition at different levels (Block, Line, Word and Character Level). Normally Indian documents, in any its state language contain English words mixed with other words in its own state language. In this paper, we extract three different types of features: Structural, Gabor and Discrete Cosine Transforms(DCT) Features from Isolated English and Gurmukhi words and compare their script recognition performance using three different classifiers: Support Vector Machine (SVM), k-Nearest Neighbor and Parzen Probabilistic Neural Network (PNN).	discrete cosine transform;feature extraction;handwriting recognition;optical character recognition;probabilistic neural network;profiling (computer programming);support vector machine	Rajneesh Rani;Renu Dhir;Gurpreet Singh Lehal	2012		10.1145/2432553.2432559	natural language processing;speech recognition;computer science;pattern recognition	AI	33.21174077861009	-64.73768351168144	101350
1cf74b4b1b51627923505c7df5360f7734a5c245	on effective palmprint retrieval for personal identification	principal component analysis;search space	This paper presents a novel retrieval method for effective search of palmprints based on Principal Component Analysis (PCA) and Self-Organizing Feature Map (SOM). To reduce search space and speed up the query processing, an integration of PCA and SOM is proposed, where the coefficients obtained by PCA for global feature representation is considered as input features of SOM. The trained SOM can be used as a retrieval engine to identify similar palmprint images with respect to the query palmprint image for personal identification..	algorithm;artificial neural network;authentication;coefficient;database;experiment;feature extraction;fingerprint;network model;online and offline;principal component analysis;unsupervised learning	King Hong Cheung;Adams Wai-Kin Kong;Jane You;David Zhang	2003			query expansion;information retrieval;principal component analysis;artificial intelligence;mathematics;pattern recognition	ML	34.031020355940335	-59.993054883922305	101465
05263c08cd96f32a104678d08b156bed526a53bd	comparative study of devnagari handwritten character recognition using different feature and classifiers	compound modified quadratic discriminant function;handwriting recognition;compounds;support vector machines;devnagari script handwritten character recognition devnagari handwritten character recognition classifier comparison indian script;support vector machines curve fitting feature extraction gradient methods handwritten character recognition image classification learning artificial intelligence;image classification;different classifier;character recognition handwriting recognition shape pattern recognition support vector machines support vector machine classification gray scale mirrors machine learning euclidean distance;different classifier devnagari handwritten character recognition indian handwritten character recognition subspace method linear discriminant function support vector machine mirror image learning euclidean distance k nearest neighbour modified projection distance compound projection distance compound modified quadratic discriminant function feature set curvature information gradient information;euclidean distance;indian handwritten character recognition;classifier comparison;accuracy;modified quadratic discriminant function;mirror image learning;curvature information;linear discriminant function;feature extraction;modified projection distance;devnagari handwritten character recognition;subspace method;gradient methods;support vector machine classification;nearest neighbour;indian script;k nearest neighbour;devnagari script;support vector machine;learning artificial intelligence;curve fitting;character recognition;compound projection distance;handwritten character recognition;gradient information;feature set	In recent years research towards Indian handwritten character recognition is getting increasing attention. Many approaches have been proposed by the researchers towards handwritten Indian character recognition and many recognition systems for isolated handwritten numerals/characters are available in the literature. To get idea of the recognition results of different classifiers and to provide new benchmark for future research, in this paper a comparative study of Devnagari handwritten character recognition using twelve different classifiers and four sets of feature is presented. Projection distance, subspace method, linear discriminant function, support vector machines, modified quadratic discriminant function, mirror image learning, Euclidean distance, nearest neighbour, k-Nearest neighbour, modified projection distance, compound projection distance, and compound modified quadratic discriminant function are used as different classifiers. Feature sets used in the classifiers are computed based on curvature and gradient information obtained from binary as well as gray-scale images.	3d projection;benchmark (computing);euclidean distance;gradient descent;grayscale;handwriting recognition;linear discriminant analysis;optical character recognition;support vector machine	Umapada Pal;Tetsushi Wakabayashi;Fumitaka Kimura	2009	2009 10th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2009.244	random subspace method;support vector machine;speech recognition;computer science;machine learning;pattern recognition;handwriting recognition	Vision	33.3792939753677	-64.29908436738121	101715
d378a797197cb1ba17b129f1aeb9d8b716cd186a	co-saliency detection linearly combining single-view saliency and foreground correspondence				Huiyun Jing;Xin He;Qi Han;Xiamu Niu	2015	IEICE Transactions		pattern recognition;computer vision;artificial intelligence;computer science;salience (neuroscience)	Vision	32.43859448834101	-53.4895211430118	101856
f1468a1440cf8e6bb60668a5b3ff652801bf2723	arf @ mediaeval 2012: an uninformed approach to violence detection in hollywood movies		The MediaEval 2012 Affect Task challenged participants to automatically find violent scenes in a set of Hollywood movies. We propose to first predict a set of mid-level concept annotations from low-level visual and auditory features, then fuse the concept predictions and features to detect violent content. Instead of engineering features suitable for the task, we deliberately restrict ourselves to simple generalpurpose features with limited temporal context and a generic neural network classifier, setting a baseline for more sophisticated approaches. On 3 test movies, our system detects 49% of violent frames at a precision of 28%, outperforming all other submissions.	acoustic radiation force;artificial neural network;baseline (configuration management);high- and low-level;hollywood	Jan Schlüter;Bogdan Ionescu;Ionut Mironica;Markus Schedl	2012			simulation;engineering;multimedia;communication	NLP	25.58822756119117	-57.09733529640146	102193
1ae0264bd9b7956fabeab2b189c5f1f6bd49ba4a	graph-based discriminative learning for location recognition	image graphs;location recognition;discriminative learning	Recognizing the location of a query image by matching it to an image database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of an image database by representing it as a graph, and show how the rich information embedded in such a graph can improve bag-of-words-based location recognition methods. In particular, starting from a graph based on visual connectivity, we propose a method for selecting a set of overlapping subgraphs and learning a local distance function for each subgraph using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets.	bag-of-words model;computer vision;embedded system	Song Cao;Noah Snavely	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1007/s11263-014-0774-9	machine learning;pattern recognition;data mining;mathematics;automatic image annotation;graph database	Vision	37.16278455127257	-56.51794992802842	102250
1f4024dfd7f5b7db8d47444b7872d4d0d6bca992	grasping unknown objects using an early cognitive vision system for general scene understanding	vision system;object recognition;grasping;top down;robotics;surface texture;grasping three dimensional displays shape visualization surface texture image color analysis machine vision;three dimensional;hierarchical representation;visualization;shape;cognitive vision;three dimensional displays;image color analysis;machine vision;robotteknik och automation;scene understanding;pose estimation	Grasping unknown objects based on real-world visual input is a challenging problem. In this paper, we present an Early Cognitive Vision system that builds a hierarchical representation based on edge and texture information, which is a sparse but powerful description of the scene. Based on this representation we generate edge-based and surface-based grasps. The results show that the method generates successful grasps, that the edge and surface information are complementary, and that the method can deal with more complex scenes. We furthermore present a benchmark for visual-based grasping.	benchmark (computing);sparse matrix	Mila Popovic;Gert Kootstra;Jimmy A. Jørgensen;Danica Kragic;Norbert Krüger	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6094932	surface finish;three-dimensional space;computer vision;pose;visualization;machine vision;shape;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;top-down and bottom-up design;computer graphics (images)	Robotics	36.84388478203806	-53.24964213081417	102379
9ff7e32ad42e0787b2f0e624e28d55e402aca537	automatic recognition of coal and gangue based on convolution neural network		HuichaoHong, LixinZheng,JianqingZhu,ShuwanPan, KaitingZhou (1.Institute of Huaqiao University, Quanzhou, Fujian Province 362021, China; 2. Industrial Engineering Research Center, universities and intelligent systems in Fujian Province, Quanzhou, Fujian Province 362021, China) Abstract:We designed a gangue sorting system,and built a convolutional neural network model based on AlexNet. Data enhancement and transfer learning are used to solve the problem which the convolution neural network has insufficient training data in the training stage. An object detection and region clipping algorithm is proposed to adjust the training image data to the optimum size. Compared with traditional neural network and SVM algorithm, this algorithm has higher recognition rate for coal and coal gangue, and provides important reference for identification and separation of coal and gangue.	algorithm;artificial neural network;clipping (computer graphics);convolution;convolutional neural network;industrial engineering;network model;object detection;outline of object recognition;preprocessor;sorting;universality probability	Huichao Hong;Lixin Zheng;Jianqing Zhu;Shuwan Pan;Kaiting Zhou	2017	CoRR		convolutional neural network;support vector machine;transfer of learning;artificial neural network;pattern recognition;artificial intelligence;object detection;computer science;gangue;sorting;coal	ML	28.722907872971877	-55.85152488970549	102652
274cdbcca8f904cf727cd0c3b08021cf50a2711a	multi-resolution mobile vision system for plant leaf disease diagnosis		The process of detecting plant disease by human naked-eye is difficult and very expensive practice, particularly in developing countries like India. Designing and providing a fast-reliable automated mobile vision based solution for such tasks, is a great realistic contribution to the society. In this paper, a mobile client–server architecture for leaf disease detection and diagnosis using a novel combination of Gabor wavelet transform (GWT) and gray level co-occurrence matrix (GLCM), opens a new dimension in pattern recognition, is proposed. Mobile disease diagnosis system represents a diseased patch in multi-resolution and multi-direction feature vector. Mobile client captures and pre-processes the leaf image, segments diseased patches in it and transmits to the Pathology Server, reducing transmission cost. The Server performs the computational tasks: GWT– GLCM feature extraction and k-Nearest Neighbor classification. The result is sent back to the users screen via an SMS (short messaging service) with an accuracy rate of 93 %, in best condition. On the other part, paper also focus on design of Human-mobile interface (HMI), which is useful even for the illiterate farmers, to automatically monitor their field at any stage by just a mobile click. Android is currently used to run this system which can be easily extended to other mobile operating systems. S. Prasad (B) · S. K. Peddoju Department of Computer Science and Engineering, Indian Institute of Technology Roorkee, Haridwar 247667, India e-mail: sheetala.god.prasad@gmail.com D. Ghosh Department of Electronics and Communication Engineering, Indian Institute of Technology Roorkee, Haridwar 247667, India	algorithm;android;augmented reality;best practice;client–server model;co-occurrence matrix;computation;computer science;document-term matrix;email;experiment;feature extraction;feature vector;gabor wavelet;grayscale;human–computer interaction;mobile device;mobile operating system;pattern recognition;real-time transcription;sensor;server (computing);tablet computer;wavelet transform	Shitala Prasad;Sateesh Kumar Peddoju;Debashis Ghosh	2016	Signal, Image and Video Processing	10.1007/s11760-015-0751-y	computer vision;simulation;electrical engineering;machine learning;algorithm	Mobile	29.70567550258625	-60.33599302169637	102657
9d85f3cf2cc464b67ff0ead1a82197878da7494d	ranking based boosted multiple kernel learning for activity recognition on first-person videos		In this paper, we investigate fusion of different types of classifiers for activity recognition on first-person videos in a data-driven approach. The algorithm first uses the classifiers, which are composed of kernel and descriptor combinations, through well-known AdaBoost trials. After all trials, classifiers are ordered and assigned ranks with respect to their performances in each trial separately. These classifiers compose a candidate list according to their performance ranks. Classifiers in the candidate list are employed together on the training set again. Classifiers in most successful candidate lists are combined as final classifiers. Our experiments show improvements in recognition comparison to traditional methods.	activity recognition;adaboost;algorithm;experiment;kernel (operating system);multiple kernel learning;performance;test set	Fatih Ozkan;Elif Surer;Alptekin Temizel	2018	2018 26th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2018.8404221	computer science;kernel (linear algebra);activity recognition;artificial intelligence;adaboost;pattern recognition;histogram;ranking;training set;multiple kernel learning	Vision	25.349997429274534	-57.00729434848581	102767
dd05169c0d258413dd169345effebca3b41cf697	relational visual recognition (relationele visuele herkenning)				Laura Mocanu-Antanas	2014				Vision	28.9602429467127	-57.23355676175476	102822
5a4029bdb9be81611444aafddb2f45b08a692d99	document writer analysis with rejection for historical arabic manuscripts	histograms;feature extraction method document writer analysis historical arabic manuscript handwriting individuality ancient manuscript manuscript analysis process automatic identification historical manuscripts metadata writer name writer classification retrieval approach multipage document learning based rejection strategy writer retrieval support vector machine query manuscript;support vector machines;training;writer identification;support vector machines document image processing feature extraction handwriting recognition history image classification meta data natural language processing query processing;accuracy;decision rejection;vectors;feature extraction;writing;decision rejection writer identification historical manuscripts;feature extraction vectors histograms support vector machines writing accuracy training;historical manuscripts	Determining the individuality of handwriting in ancient manuscripts is an important aspect of the manuscript analysis process. Automatic identification of writers in historical manuscripts can support historians to gain insights into manuscripts with missing metadata such as writer name, period, and origin. In this paper writer classification and retrieval approaches for multi-page documents in the context of historical manuscripts are presented. The main contribution is a learning-based rejection strategy which utilizes writer retrieval and support vector machines for rejecting a decision if no corresponding writer can be found for a query manuscript. Experiments using different feature extraction methods demonstrate the abilities of our proposed methods. A dedicated data set based on a publicly available database of historical Arabic manuscripts was used and the experiments show promising results.	automatic identification and data capture;database;experiment;feature extraction;historical document;rejection sampling;support vector machine	Daniel Fecker;Abedelkadir Asi;Werner Pantke;Volker Märgner;Jihad El-Sana;Tim Fingscheidt	2014	2014 14th International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2014.130	natural language processing;support vector machine;speech recognition;feature extraction;computer science;machine learning;pattern recognition;histogram;accuracy and precision;writing;statistics	Vision	33.69409906285338	-64.87242343817933	103098
062b3f1156f0731a8c32ef3a76e2ce8fbe2b1a0c	on-line signature verification by stroke-dependent representation domains	stability criteria;handwriting recognition;image segmentation;expert systems;stroke dependent representation domains;signature verification;set theory;stability;multiexpert approach online signature verification stroke dependent representation domains dynamic signature verification handwritten signature;internet;stability signature verification biometry dynamic signature;online signature verification;writing;dynamic signature verification;dynamic signature;profitability;computer science;handwritten signature;set theory expert systems handwriting recognition image segmentation;biometry;multiexpert approach;conferences;handwriting recognition conferences stability criteria writing computer science internet	In this paper a new system for dynamic signature verification is presented. It is based on the consideration that each region of an handwritten signature can convey personal characteristics in diverse domains. Therefore, a multi-expert approach is considered in which each stroke of the signature is evaluated in the most profitable domain of representation. The experimental results demonstrate the effectiveness of the proposed approach.	antivirus software;expert system	Donato Impedovo;Giuseppe Pirlo	2010	2010 12th International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2010.102	the internet;speech recognition;stability;computer science;machine learning;pattern recognition;handwriting recognition;image segmentation;writing;signature recognition;statistics;profitability index;set theory	Robotics	34.10285058408289	-65.57679996490322	103118
37ca1713efa48972cfeace555224e18b5b61e4b3	a novel fuzzy approach for shape determination of traffic signs	traffic signs;fuzzy rules;out door images;color segmentation	In this paper, a novel fuzzy approach is developed to determine the shape of traffic signs. More than 1600 images of traffic signs were collected in different light conditions by a digital camera mounted in a car and used for testing this approach. Every RGB image was converted into HSV colour space, and segmented by using a set of fuzzy rules depending on the hue and saturation values of each pixel in the HSV colour space. The fuzzy rules are used to extract the colours of the road signs. Objects in each segmented image are labelled and tested for the presence of probable sign. All small objects under certain threshold are discarded, and the remaining objects are tested by a fuzzy shape recognizer which invokes another set of fuzzy rules. Four shape measures are used to decide the shape of the sign; rectangularity, triangularity, ellipticity, and the new shape measure octagonality.	color space;digital camera;finite-state machine;fuzzy logic;fuzzy rule;pixel	Hasan Fleyeh	2005			computer vision	Vision	35.150011284997476	-63.982972555786745	103166
ed3eea356aa32b8301facee44889f36491560919	recognition of object position and shape by the spread pattern of a spatial spreadingassociative neural network			artificial neural network	Kiyomi Nakamura;Akira Takarajima	1997	Systems and Computers in Japan	10.1002/(SICI)1520-684X(199709)28:10%3C9::AID-SCJ2%3E3.0.CO;2-X		Robotics	28.04541758526531	-58.483992068440934	103174
331144f5b7caeff8f296645f4260862c9628431b	negative iris recognition	iris recognition biometric recognition privacy preservation negative database;iris recognition bioinformatics feature extraction databases cryptography	Elements of a person's biometrics are typically stable over the duration of a lifetime, and thus, it is highly important to protect biometric data while supporting recognition (it is also called secure biometric recognition). However, the biometric data that are derived from a person usually vary slightly due to a variety of reasons, such as distortion during picture capture, and it is difficult to use traditional techniques, such as classical encryption algorithms, in secure biometric recognition. The negative database (NDB) is a new technique for privacy preservation. Reversing the NDB has been demonstrated to be an NP-hard problem, and several algorithms for generating hard-to-reverse NDBs have been proposed. In this paper, first, we propose negative iris recognition, which is a novel secure iris recognition scheme that is based on the NDB. We show that negative iris recognition supports several important strategies in iris recognition, e.g., shifting and masking. Next, we analyze the security and efficiency of negative iris recognition. Experimental results show that negative iris recognition is an effective and secure iris recognition scheme. Specifically, negative iris recognition can achieve a highly promising recognition performance (i.e., GAR = 98.94% at FAR = 0.01%, EER = 0.60%) on the typical database CASIA-IrisV3-Interval.	algorithm;biometrics;distortion;encryption;enhanced entity–relationship model;hard disk drive;iris recognition;ndb cluster;np-hardness;negative database;reversing: secrets of reverse engineering	Dongdong Zhao;Wenjian Luo;Ran Liu;Lihua Yue	2018	IEEE Transactions on Dependable and Secure Computing	10.1109/TDSC.2015.2507133	computer science;signature recognition;real-time computing;feature extraction;encryption;cryptography;iris recognition;three-dimensional face recognition;artificial intelligence;computer vision;negative database;biometrics;pattern recognition	Vision	30.82385146607447	-61.67807432960696	103252
27f7c0b22ba0ff3caedefee1ca5e10e4431c9858	an efficient method of image identification by combining image features	image features;global feature;interest point;interest points;method of image;image identification;image clustering;feature vector;clustering;local features;fuzzy c means clustering;feature point;descriptor;identifier;local feature;image retrieval	This paper proposes an efficient image identification method by combining image features and using image clustering. For more efficient image identification, we use global and local features in a hierarchical manner. The combined global feature reflecting general information of image helps faster retrieval of candidate images and the feature point based local feature facilitates more accurate fine matching with the candidate images. We consider the Fuzzy C-Means clustering method since it is effective for the image data which are characteristically alike and have fuzzy boundary in coordinate by their global features. The global feature vector which we use is very effective in clustering and retrieval since it represents general properties of image and its dimension is very low. As a result, the number of fine matching which requires very large computing time and high complexity is considerably decreased since searching original image of query is done by fine matching within partial database of candidate images.	cluster analysis;feature vector	Jaekyong Jeong;Chijung Hwang;Byeungwoo Jeon	2009		10.1145/1516241.1516348	image texture;computer vision;feature detection;visual word;template matching;feature vector;identifier;image retrieval;computer science;machine learning;pattern recognition;cluster analysis;automatic image annotation;feature	Vision	38.68107890695728	-60.02271033454832	103292
5413bdf83fbfe36911f4f52bf0c6f0802cd01a04	feature representation learning by sparse neural network for multi-camera person re-identification			artificial neural network;feature learning;machine learning;sparse matrix	Sergei V. Makov;Alexandr Minaev;Anton Nikitin;Viacheslav V. Voronin;Evgeny Aleksandrovich Semenishchev;Vladimir I. Marchuk	2017		10.2352/ISSN.2470-1173.2017.13.IPAS-211	deep learning;machine learning;artificial neural network;time delay neural network;artificial intelligence;pattern recognition;computer science;feature learning	AI	29.094207256066355	-56.81801802241136	103757
a513dc177bfd3fd92d9a87e84ce2a298b1cd4b9e	a real time classifier for emotion and stress recognition in a vehicle driver	mouth;video streaming;neural nets;face training emotion recognition mouth real time systems face detection streaming media;real time;training;image classification;emotion recognition;streaming media;neuro fuzzy;multi layer perceptron;face;video stream real time classifier emotion recognition stress recognition vehicle driver artificial systems classical neural networks neuro fuzzy classifiers;face detection;video streaming emotion recognition image classification neural nets;neural network;real time systems	Recently there is a great interest in artificial systems able to understand and recognize human emotions. In this paper an Emotion Recognition System based on classical neural networks and neuro-fuzzy classifiers is proposed. Emotion recognition is performed in real time starting from a video stream acquired by a common webcam monitoring the user's face. Neurofuzzy classifiers, in comparison with Multi Layer Perceptron trained by EBP algorithm, show very short training times, allowing applications with easy and automated set up procedures, to be used in a wide range of applications, from entertainment to safety. The algorithm yields very interesting performances and can be adopted to recognize emotions as well as possible pathological conditions of the individual to be monitored.	algorithm;artificial neural network;emotion recognition;neuro-fuzzy;perceptron;performance;streaming media;webcam;x86	Maurizio Paschero;Guido Del Vescovo;L. Benucci;Antonello Rizzi;Marco Santello;Gianluca Fabbri;Fabio Massimo Frattale Mascioli	2012	2012 IEEE International Symposium on Industrial Electronics	10.1109/ISIE.2012.6237345	psychology;computer vision;speech recognition;machine learning	Robotics	24.81098497940318	-60.754178223550184	103835
ac9a331327cceda4e23f9873f387c9fd161fad76	deep convolutional neural network for age estimation based on vgg-face model		Automatic age estimation from real-world and unconstrained face images is rapidly gaining importance. In our proposed work, a deep CNN model that was trained on a database for face recognition task is used to estimate the age information on the Adience database. This paper has three significant contributions in this field. (1) This work proves that a CNN model, which was trained for face recognition task, can be utilized for age estimation to improve performance; (2) Over fitting problem can be overcome by employing a pretrained CNN on a large database for face recognition task; (3) Not only the number of training images and the number subjects in a training database effect the performance of the age estimation model, but also the pre-training task of the employed CNN determines the model’s performance.	convolutional neural network;facial recognition system	Zakariya Qawaqneh;Arafat Abu Mallouh;Buket D. Barkana	2017	CoRR		pattern recognition;computer science;overfitting;machine learning;convolutional neural network;artificial intelligence;facial recognition system	Vision	29.637900815364535	-53.3925268130168	103998
a2a7f0f79b1b11b0098781f670cac78e127e452b	local tensor descriptor from micro-deformation analysis	object recognition;local image structures;high dimensionality;perforation;local tensor descriptor;object categorization;object categorization local tensor descriptor microdeformation analysis local image structures tensor matrix feature descriptor object recognition;deformable models;matrix algebra;polynomials;symmetric matrices;tensor matrix;distance measurement;object recognition matrix algebra;pixel;microdeformation analysis;artificial intelligence;approximation methods;feature descriptor;deformation analysis;tensile stress image analysis deformable models object recognition gabor filters filter bank biomedical engineering physics computer vision histograms	This paper proposes a novel method called micro-deformation analysis to analyze and describe local image structures. This method is a general analytic tool and can be applied to any high-dimensional scalar or vector functions. We derive the tensor matrix from this method as the descriptor to represent the information within local image patches. Our experimental results suggest that we can design low-dimensional local tensor descriptors with performance comparable to the popular SIFT descriptor, which is the state-of-the-art feature descriptor used for object recognition and categorization.	categorization;outline of object recognition;visual descriptor	Bangsheng Cheng	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587610	computer vision;local binary patterns;gloh;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics;pixel;polynomial;symmetric matrix	Vision	34.896641442011074	-56.07495064883948	104318
d6a20ecaeb09fc6aeda354d11d12c9fd9a3efa53	fractional dct and dwt hybridization based efficient feature extraction for gender classification		Abstract Fractional discrete Cosine transform (FrDCT), generalisation of discrete Cosine transform (DCT), is a useful mathematical tool that provides flexibility to represent data at any angle α from spatial domain axis. Fusion of discrete wavelet transform (DWT) and FrDCT is used in this paper for gender classification. Initially, 1-level DWT is calculated for 128 × 128 resized input images. LL sub-band images are then input to FrDCT which transforms the images into u -domain which is at an angle α from spatial domain axis. FrDCT transformed images are used to generate a feature vector based on 8 × 8 non-overlapping sub-blocks. Dividing an image into sub-blocks helps to remove the local spatial correlation. Thus, proposed technique maintains the computational cost as that in other existing techniques. Rbf kernel based Support Vector Machine (SVM) is used to classify the images on the basis of gender. Images of ATu0026T, Faces94 and Georgia Tech databases are used to validate the performance of the proposed technique. It is found that proposed technique outperforms as compare to other existing techniques with respect to generalization performance.	discrete cosine transform;discrete wavelet transform;feature extraction	Anjali Goel;Virendra P. Vishwakarma	2017	Pattern Recognition Letters	10.1016/j.patrec.2017.05.014	computer vision;speech recognition;pattern recognition	Vision	34.83423273343711	-59.50935528443292	104326
2505acde074ee5e6fe80160fa28b182204905713	fast and accurate biometric identification using score level indexing and fusion	security of data biometrics access control fingerprint identification;biometrics access control;posterior probability;indexes;indexation;error rate;experimental evaluation;error rate retrieval score level indexing score level fusion biometric identification user authentication error rate identification biometric system distance based indexing;security of data;fingerprint identification	Biometric identification provides a very convenient way to authenticate a user because it does not require the user to claim an identity. However, both the identification error rates and the response time increase almost in proportion to the number of enrollees. A technique which decreases both of them using only scores has the advantage that it can be applied to any kind of biometric system that outputs scores. In this paper, we propose such a technique by combining score level fusion and distance-based indexing. In order to reduce the retrieval error rate in multibiometric identification, our technique takes a strategy to select the template of the enrollee whose posterior probability of being identical to the claimant is the highest as a next to be matched. The experimental evaluation using the Biosecure DS2 dataset and the CASIA-FingerprintV5 showed that our technique significantly reduced the identification error rates while keeping down or even reducing the number of score calculations, compared to the unimodal biometrics.	authentication;automation;biometrics;institute of automation, chinese academy of sciences;logistic regression;response time (technology)	Takao Murakami;Kenta Takahashi	2011	2011 International Joint Conference on Biometrics (IJCB)	10.1109/IJCB.2011.6117591	database index;fingerprint;speech recognition;word error rate;computer science;pattern recognition;data mining;posterior probability;statistics	Robotics	29.42527932301601	-63.67547866650839	104819
11691c45deb9ca81778efecf207599510b9600fe	a captcha based on the human visual systems masking characteristics	object recognition;image segmentation;hvs;active region;english alphabets;ocr segmentation algorithm captcha human visual system hvs masking characteristics english alphabets object character recognition;visual perception character recognition image segmentation object recognition;object character recognition;ocr segmentation algorithm;human visual system;visual perception;humans visual system automatic testing search engines dictionaries uniform resource locators computer science noise robustness character recognition optical character recognition software;masking characteristics;character recognition;captcha	In this paper, a CAPTCHA is presented based on the masking characteristics of the human visual system (HVS). Knowing that noise can be masked by high activity regions and showing that edges can be masked by noise for a human observer while still being detected by machines, the suggested CAPTCHA is composed of English alphabets that are picked randomly and written with a combination of texture and edges with added noise such as to deceive the machine by randomly changing the visibility of characters for humans. The proposed CAPTCHA is highly legible and robust to brute-force attacks and sophisticated object character recognition (OCR) segmentation algorithms	algorithm;brute-force attack;captcha;human visual system model;optical character recognition;randomness	Rony Ferzli;Rida A. Bazzi;Lina J. Karam	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262439	computer vision;speech recognition;visual perception;intelligent character recognition;computer science;cognitive neuroscience of visual object recognition;captcha;image segmentation;human visual system model	Robotics	35.18843125779533	-65.366019617078	105191
158c8caec5cac0d05893d136ee40248bfdcee9bb	printer forensics based on page document's geometric distortion	printing;support vector machine page document geometric distortion intrinsic feature extraction printer forensics method projective transformation model geometric distortion feature estimation least squares method svm technique;transformation model;computer forensics;least mean squares methods;support vector machines;printers;image classification;least square method;printers forensics solid modeling feature extraction printing least squares methods image quality distortion measurement noise measurement image edge detection;distortion;support vector machines computer forensics distortion document image processing feature extraction image classification least mean squares methods printers;feature extraction;mathematical model;document image processing;intrinsic features;printer forensics;projective transformation;geometric distortion;forensics;projective transformation printer forensics intrinsic features geometric distortion	A printed document can provide intrinsic features of the printer so as to distinguish which printer it comes from. But how to extract the intrinsic features is critical in printer forensics. In this paper, the page document's geometric distortion is extracted as the intrinsic features, and a printer forensics method based on the distortion is proposed. Firstly projective transformation model is used to model the geometric distortion. After the feature point set of the model is extracted, the model's parameters considered as the geometric distortion features can be estimated, and then the model's error pattern can be obtained. During the process, the least squares method is used to estimate the model's parameters, and SVM technique is used for classification. The effectiveness of the model's parameters in the printer forensics is demonstrated by experimental results.	distortion;least squares;printer (computing);printing	Yubao Wu;Xiangwei Kong;Xingang You;Yiping Guo	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413420	support vector machine;computer vision;contextual image classification;speech recognition;homography;distortion;feature extraction;computer science;machine learning;pattern recognition;mathematical model;least squares;computer forensics	Vision	32.744300740433374	-63.461029018994196	105228
c558d928288dffead3a4f71893c749c9fd64cde1	thai font type recognition using sift	image recognition;image processing thai font type recognition sift thai document scale invariant feature transform local feature detection text image;optical character recognition;text analysis;optical character recognition software;accuracy;font type recognition;transforms feature extraction natural language processing optical character recognition text analysis;optical imaging;scale invariant feature transform;feature extraction;transforms;text recognition;character recognition;font type recognition scale invariant feature transform;natural language processing;feature extraction text recognition image recognition optical character recognition software accuracy optical imaging character recognition	This paper presents a Thai font type recognition on Thai document by using Scale-invariant feature transform (SIFT). The features are extracted by Scale-invariant feature transform (SIFT) that is widely used in image processing. Sift is an algorithm for detecting local features in order to find similar objects. Our system contains ten fonts and ten text images in each font. We use ten text images each font total one hundred images for our experiment. Our results show accuracy for 97.37% for ten Thai fonts.	algorithm;image processing;scale-invariant feature transform;sensor	Pitchaya Jamjuntr;Natasha Dejdumrong	2012	2012 Ninth International Conference on Computer Graphics, Imaging and Visualization	10.1109/CGIV.2012.23	computer vision;speech recognition;pattern recognition	Vision	35.32785797366842	-65.30914631668463	105325
d49aa6a1b78c5963d432ca89efa7d372b1d28800	an unsupervised video shot boundary detection technique using fuzzy entropy estimation of video content			entropy estimation;shot transition detection	Biswanath Chakraborty;Siddhartha Bhattacharyya;Paramartha Dutta	2012		10.1201/b11631-10	fuzzy logic;machine learning;artificial intelligence;computer science;video denoising;entropy estimation;computer vision	Vision	38.898519979840316	-52.42730446209248	105597
0aa405447a8797e509521f0570e4679a42fdac9b	discriminately decreasing discriminability with learned image filters	learned image filters;image filtering;target task;kernel;simulated image classification problems;facial attributes;distractor task;measurement;band pass filters;convolution;optimal filtering;face convolution vectors kernel training labeling measurement;training;training dataset;image classification;input signals;gabor filters;gabor bandpass filters;supervised regularization;discriminability suppression;gender;computer vision;inter dataset generalization;face recognition;vectors;crowdsourcing site;machine learning;data privacy;realistic expression recognition problem learned image filters machine learning computer vision input signals preprocessing face images gabor bandpass filters distractor task target task privacy concerns crowdsourcing site mechanical turk facial attributes discriminability suppression inter dataset generalization facial expression gender training dataset supervised regularization simulated image classification problems;pattern recognition;face;realistic expression recognition problem;facial expression;learning artificial intelligence;covariance structure;learning artificial intelligence band pass filters computer vision data privacy face recognition gabor filters image classification;labeling;mechanical turk;preprocessing face images;privacy concerns	In machine learning and computer vision, input signals are often filtered to increase data discriminability. For example, preprocessing face images with Gabor band-pass filters is known to improve performance in expression recognition tasks [1]. Sometimes, however, one may wish to purposely decrease discriminability of one classification task (a “distractor” task), while simultaneously preserving information relevant to another task (the target task): For example, due to privacy concerns, it may be important to mask the identity of persons contained in face images before submitting them to a crowdsourcing site (e.g., Mechanical Turk) when labeling them for certain facial attributes. Suppressing discriminability in distractor tasks may also be needed to improve inter-dataset generalization: training datasets may sometimes contain spurious correlations between a target attribute (e.g., facial expression) and a distractor attribute (e.g., gender). We might improve generalization to new datasets by suppressing the signal related to the distractor task in the training dataset. This can be seen as a special form of supervised regularization. In this paper we present an approach to automatically learning preprocessing filters that suppress discriminability in distractor tasks while preserving it in target tasks. We present promising results in simulated image classification problems and in a realistic expression recognition problem.	algorithm;amazon mechanical turk;composite image filter;computer vision;crowdsourcing;matlab;machine learning;matrix regularization;preprocessor;rendering (computer graphics);the turk;zero suppression	Jacob Whitehill;Javier R. Movellan	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247964	facial recognition system;face;computer vision;labeling theory;contextual image classification;kernel;information privacy;computer science;machine learning;pattern recognition;band-pass filter;convolution;facial expression;measurement	Vision	25.746200845891646	-56.30977008720116	105673
4de307029e76991cbb534c41db551da82c70815e	adaptive feature selection and extraction approaches for image retrieval based on region	segmentation;fourier descriptor;feature selection and extraction;adaptive;gaussian mixture models;feature selection;region based;normalized cut;image retrieval	Image retrieval based on region is one of the most promising and active research directions in recent year's CBIR, while region segmentation, feature selection and feature extraction of region are key issues. However, the existing approaches always adopt a uniform approach of segmentation and feature extraction for all images in the same system. In this paper, we propose adaptive image segmentation and feature extraction approach according to different category image for image retrieval system. To improve performance, we propose adaptive segmentation approach according to different category image. Textured image is segmented by Gaussian Mixture Models (GMM), while non-textured image is segmented by our proposed block-based normalized cut. To accurately describe feature of region, we propose weight assignment method for centroid pixel and its neighbor by convolution with normal distribution when image segmentation by GMM. To improve generalization, we propose adaptive number of Fourier descriptors of shape signature which depends on the energy distribution of Fourier descriptors, instead of fixed number by experience. To simply and efficiently describe the spatial relationships of multi-object or multi-region in same image, we apply simplified topological relationships. The experiments demonstrate that proposed approaches are superior to the traditional approaches.	algorithm;automatic image annotation;cluster analysis;computer vision;content-based image retrieval;convolution;experiment;feature extraction;feature selection;google map maker;high- and low-level;human–computer interaction;image segmentation;k-means clustering;machine learning;mixture model;pattern recognition;pixel;preprocessor;relevance feedback;spectral clustering;texture filtering	Haiyu Song;Xiongfei Li;Pengjie Wang	2010	Journal of Multimedia	10.4304/jmm.5.1.85-92	image texture;computer vision;feature detection;range segmentation;visual word;feature extraction;image retrieval;computer science;adaptive behavior;machine learning;segmentation-based object categorization;pattern recognition;mixture model;region growing;image segmentation;scale-space segmentation;feature selection;segmentation;feature	Vision	37.142772166651525	-59.6586508915643	105850
0547d1cc453ec64b0a262653c5cad73f6cb71bff	fast and accurate face recognition with image sets		For large-scale face recognition applications using image sets, the images of the query set typically lie in compact regions surrounded by a diffuse sea of images of the gallery set. In this study, we propose a fast and accurate method to approximate the distances from gallery images to the region spanned by the query set for large-scale applications. To this end, we propose a new polyhedral conic classifier that will enable us to compute those distances efficiently by using simple dot products. We also introduce one-class formulation of the proposed classifier that can use query set examples only. This makes the method ideal for real-time applications since testing time approximately becomes the independent of the size of the gallery set. One-class formulation is very important for large-scale face recognition problems in the sense that it can be used in a cascade system with more complex and time-consuming methods to return the most promising candidate gallery sets in the first stage of the cascade so that more complex methods can be run on those a few candidate sets. As a result, we strongly believe that the proposed method will impact future methods and it will enable to introduce face recognition methods working in real-time even for large-scale set based recognition problems. Experimental results on both small and moderate sized face recognition datasets support these claims and demonstrate the efficacy of the proposed method. More precisely, the proposed methods achieve the best accuracies on all tested datasets and we obtained improvements around 18% compared to the best performing rival methods on larger datasets.	approximation algorithm;disk image;epcc;facial recognition system;imagenet;polyhedron;real-time clock;real-time computing;real-time locating system;statistical classification	Hakan Cevikalp;Hasan Serhan Yavuz	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.184	manifold;computer vision;artificial intelligence;support vector machine;facial recognition system;dot product;conic section;three-dimensional face recognition;pattern recognition;computer science	Vision	29.595301004749608	-54.16881676527128	105990
8b46a8622956ba6a7fc8943ad6ebdb8b0a8095db	domain adaptation image classification based on multi-sparse representation			domain adaptation;sparse approximation;sparse matrix	Xu Zhang;Xiaofeng Wang;Yue Du;Xiaoyan Qin	2017	TIIS	10.3837/tiis.2017.05.016	computer vision;distributed computing;pyramid (image processing);computer science;domain adaptation;contextual image classification;sparse approximation;artificial intelligence	Vision	29.53226765252337	-56.93071342255139	106028
a0f0ecdf17284cd3f6f7ce5cb63f99e94f534175	semantic-based automatic structuring of leaf images for advanced plant species identification	categorisation;botanical concepts;leaf species identification;partition;structuring	Structuring the search space based on domain-specific vocabulary (or concepts) is capital for enhanced image retrieval. In this paper, we study the opportunities and the impact of exploiting such a strategy in a particular problem which is the leaf species identification. We believe that such a solution is promising to reduce the effect of the high variability across and within species and define more specific and relevant leaf image representations. Among botanical concepts that describe leaves (and particularly their architecture), we focus mainly on three of the most basic and commonly-used concepts: the leaf arrangement, lobation and partition. These concepts define two different structuring types: (1) One is a coarse categorisation of leaf datasets into three subsets, namely simple lobed, simple not lobed and compound (2) The other is a decomposition of the entire leaf images into semantic regions (or parts). We perform the whole structuring process automatically by defining simple geometric parameters (extracted from the leaf contour) based on the analysis of botanical definitions. A fine recognition process is then established in order to determine the species identity. It is defined, typically, using standard (texture or contour) descriptors followed by a KNN classifier. Enriched by the proposed structuring process, the search for species candidates will be restricted to the correspondent category of the query and based on a fusion of each part responses. Experiments carried out on the Scan Pictures of the ImageCLEF 2011 dataset (3070 images totalling 70 species) have shown an increase in performances for different descriptor configurations compared to global leaf representations as well as to some recent related studies.	acoustic lobing;analysis of algorithms;apex (geometry);basal (phylogenetics);categorization;data descriptor;feature selection;ginkgo cadx;i/o request packet;image retrieval;k-nearest neighbors algorithm;leaflet;machine learning;performance;sensor;spatial variability;statistical classification;texture mapping;utility functions on indivisible goods;vocabulary	Olfa Mzoughi;Itheri Yahiaoui;Nozha Boujemaa;Ezzeddine Zagrouba	2015	Multimedia Tools and Applications	10.1007/s11042-015-2603-8	partition;computer vision	AI	34.34435563296824	-61.51835612394034	106106
e43045a061421bd79713020bc36d2cf4653c044d	a new representation of skeleton sequences for 3d action recognition		This paper presents a new method for 3D action recognition with skeleton sequences (i.e., 3D trajectories of human skeleton joints). The proposed method first transforms each skeleton sequence into three clips each consisting of several frames for spatial temporal feature learning using deep neural networks. Each clip is generated from one channel of the cylindrical coordinates of the skeleton sequence. Each frame of the generated clips represents the temporal information of the entire skeleton sequence, and incorporates one particular spatial relationship between the joints. The entire clips include multiple frames with different spatial relationships, which provide useful spatial structural information of the human skeleton. We propose to use deep convolutional neural networks to learn long-term temporal information of the skeleton sequence from the frames of the generated clips, and then use a Multi-Task Learning Network (MTLN) to jointly process all frames of the clips in parallel to incorporate spatial structural information for action recognition. Experimental results clearly show the effectiveness of the proposed new representation and feature learning method for 3D action recognition.	artificial neural network;clips;concatenation;convolutional neural network;deep learning;feature learning;feature vector;kinect;multi-task learning;network interface device;pattern recognition;video clip	Qiuhong Ke;Mohammed Bennamoun;Senjian An;Ferdous Ahmed Sohel;Farid Boussaïd	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.486	convolutional neural network;computer vision;artificial intelligence;skeleton (computer programming);hidden markov model;feature extraction;computer science;artificial neural network;pattern recognition;human skeleton;feature learning;communication channel	Vision	28.207861631171486	-52.09766866569534	106400
9d3e5fe86ebc2f430d78ef5f4b4b9702fa3fb082	remote sensing image retrieval with global morphological texture descriptors	mathematical morphology global morphological texture descriptors content based remote sensing image retrieval multiscale texture descriptors circular covariance histogram rotation invariant point triplets fourier power spectrum quasi flat zone based scale space uc merced land use land cover data set;geophysical image processing;remote sensing feature extraction image retrieval histograms context image representation vectors;mathematical morphology;texture description content based image retrieval cbir mathematical morphology mm remote sensing;image texture;remote sensing;fourier analysis;remote sensing fourier analysis geophysical image processing image texture mathematical morphology	In this paper, we present the results of applying global morphological texture descriptors to the problem of content-based remote sensing image retrieval. Specifically, we explore the potential of recently developed multiscale texture descriptors, namely, the circular covariance histogram and the rotation-invariant point triplets. Moreover, we introduce a couple of new descriptors, exploiting the Fourier power spectrum of the quasi-flat-zone-based scale space of their input. The descriptors are evaluated with the UC Merced Land Use-Land Cover data set, which has been only recently made public. The proposed approach is shown to outperform the best known retrieval scores, despite its shorter feature vector length, thus asserting the practical interest of global content descriptors as well as of mathematical morphology in this context.	content management system;feature vector;fixed point (mathematics);floating point systems;gabor filter;grayscale;image retrieval;invariant (computer science);mathematical morphology;programming paradigm;quasiperiodicity;relevance;scale space;spectral density;texture filtering;time series;uc browser	Erchan Aptoula	2014	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2013.2268736	image texture;computer vision;visual word;mathematical morphology;pattern recognition;mathematics;fourier analysis;remote sensing	Vision	37.93451472661304	-59.09129798502091	106522
7cf0ed280d8d8cd78bb68711f1489eb195db2e5f	adaptive facial expression recognition method based on mbp and hmog feature	databases;computers;feature extraction face entropy information filters databases computers;nearest neighbor classification adaptive facial expression recognition method mbp feature hmog feature monogenic binary pattern histogram of monogenic orientated gradients orientation component phase component magnitude component monogenic waves texture information extraction texture information fusion information entropy expression image classification;statistical analysis emotion recognition face recognition feature extraction image classification image fusion image texture;feature extraction;nearest neighbor monogenic signal histogram of monogenic oriented gradients adaptive weights by information entropy;face;entropy;information filters	Because of the limitation of Monogenic waves' extraction from expression texture, this thesis proposes an adaptive facial expression feature fusion method based on MBP (monogenic binary pattern) and Histogram of HMOG (monogenic orientated gradients). To begin with our method, we get information of orientation, phase and magnitude component from filtered image by monogenic waves, besides, extract and fuse texture information from orientation and magnitude component to recognize facial expression. However single texture feature have some inadequacies for facial expression recognition. First of all, this paper describes a new kind of HMOG feature inspired by thoughts of HOG (histogram of oriented gradients) to indicate shape information of facial image. Secondly, acquiring best adaptive weights of expression block through information entropy. Finally, we classify the expression image using Nearest Neighbor to get the final result of effect of proposed method. In order to verify the efficiency of the methods above, we do a large number of experiments and it turns out our methods have great ability to explain expression feature.	algorithm;binary pattern (image generation);dspace;entropy (information theory);experiment;gradient;histogram of oriented gradients;million book project;robustness (computer science)	Min Hu;Xiaoyin Huang;Xiaohua Wang;Kun Li;Fuji Ren;Liangfeng Xu	2014	2014 IEEE 3rd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2014.7175712	face;computer vision;entropy;feature extraction;computer science;machine learning;pattern recognition	Vision	37.18880755863517	-60.37686305086618	106669
64a08beb073f62d2ce44e25c4f887de9208625a4	a linear structured approach and a refined fitness function in genetic programming for multi-class object classification	object recognition;genetic program;linear genetic programming;program representation;feature space;multi class classification;tree structure;error rate;object classification;program structure;fitness function	This paper describes an approach to the use of genetic programming (GP) to multi-class object recognition problems. Rather than using the standard tree structures to represent evolved classifier programs which only produce a single output value that must be further translated into a set of class labels, this approach uses a linear structure to represent evolved programs, which use multiple target registers each for a single class. The simple error rate fitness function is refined and a new fitness function is introduced to approximate the true feature space of an object recognition problem. This approach is examined and compared with the tree based GP on three data sets providing object recognition problems of increasing difficulty. The results show that this approach outperforms the standard tree based GP approach on all the tasks investigated here and that the programs evolved by this approach are easier to interpret. The investigation into the extra target registers and program length results in heuristic guidelines for initially setting system parameters.	advanced disc filing system;approximation algorithm;directed acyclic graph;experiment;feature vector;fitness function;genetic programming;heuristic;instance (computer science);machine learning;multiclass classification;outline of object recognition;parameter (computer programming);subroutine;victoria (3d figure)	Mengjie Zhang;Christopher Fogelberg;Yuejin Ma	2007	Connect. Sci.	10.1080/09540090701725557	method;feature vector;word error rate;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;multiclass classification;pattern recognition;tree structure;fitness function;algorithm	AI	25.585557884618066	-54.7296206732871	106781
e57d9e0d04f10910e2ccd89fcd7a6394f77b0012	binary image classification using genetic programming based on local binary patterns	image classification;feature extraction histograms support vector machines training vectors accuracy analysis of variance;computer vision;statistical analysis;statistical analysis computer vision genetic algorithms image classification learning artificial intelligence;gp based methods binary image classification genetic programming local binary patterns machine learning computer vision learning instances lbp image descriptor support vector machine svm one way analysis of variance anova wrapped classifiers nongp methods;genetic algorithms;learning artificial intelligence	Image classification represents an important task in machine learning and computer vision. To capture features covering a diversity of different objects, it has been observed that a sufficient number of learning instances are required to efficiently estimate the models' parameter values. In this paper, we propose a genetic programming (GP) based method for the problem of binary image classification that uses a single instance per class to evolve a classifier. The method uses local binary patterns (LBP) as an image descriptor, support vector machine (SVM) as a classifier, and a one-way analysis of variance (ANOVA) as an analyser. Furthermore, a multi-objective fitness function is designed to detect distinct and informative regions of the images, and measure the goodness of the wrapped classifiers. The performance of the proposed method has been evaluated on six data sets and compared to the performances of both GP based (Two-tier GP and conventional GP) and non-GP (Naïve Bayes, Support Vector Machines and hybrid Naïve Bayes/Decision Trees) methods. The results show that a comparable or significantly better performance has been achieved by the proposed method over all methods on all of the data sets considered.	binary image;computer vision;decision tree;experiment;feature extraction;fitness function;genetic programming;information;local binary patterns;machine learning;multiclass classification;multitier architecture;naive bayes classifier;one-shot learning;one-way function;performance;single-instance storage;statistical classification;support vector machine;visual descriptor	Harith Al-Sahaf;Mengjie Zhang;Mark Johnston	2013	2013 28th International Conference on Image and Vision Computing New Zealand (IVCNZ 2013)	10.1109/IVCNZ.2013.6727019	computer vision;contextual image classification;local binary patterns;genetic algorithm;computer science;machine learning;linear classifier;pattern recognition;data mining;structured support vector machine	AI	31.238874505820345	-55.35552995186959	106885
4db66a2f8d719bea41cdaaf940eb0e37e703aa79	exploring dorsal finger vein pattern for robust person recognition	biomedical imaging;veins;testing;indexes;feature extraction;fingers;biometric approach dorsal finger vein pattern robust person recognition finger vein based biometric recognition biometric researcher robustness anti spoofing property finger vein biometrics literature ventral vein pattern image capturing region of interest extraction roi extraction feature extraction dorsal finger vein database;veins feature extraction fingers indexes biomedical imaging testing;vein recognition feature extraction fingerprint identification image capture	Finger vein based biometric recognition has increasingly generated interest amongst biometric researchers because of the accuracy, robustness and anti-spoofing propertie. Prior efforts the are documented in the finger vein biometrics literature have only investigated the ventral vein pattern that is formed on the lower part of the finger underneath the skin surface. This paper investigates a new finger vein biometric approach by exploring the vein pattern that is present in the dorsal finger region. Thus, the dorsal finger vein pattern can be used as an independent biometric characteristic useful for the recognition of the target subject. We presented a complete automated approach with the key steps of image capturing, Region of Interest (ROI) extraction, pre-processing to enhance the vein pattern, feature extraction and comparison. This paper also introduces a new database of dorsal finger vein patterns from 125 subjects that resulted in 500 unique fingers with 10 samples each that results in a total of 5000 dorsal finger vein samples. Extensive experiments carried out on our new dorsal finger vein database achieve promising accuracy and thereby provide new insights on this new biometric approach.	biometrics;contactless smart card;enhanced entity–relationship model;experiment;feature extraction;fingerprint;minutiae;modality (human–computer interaction);multimodal interaction;preprocessor;region of interest;shingled magnetic recording	Ramachandra Raghavendra;Christoph Busch	2015	2015 International Conference on Biometrics (ICB)	10.1109/ICB.2015.7139059	medical imaging;database index;computer vision;speech recognition;finger vein recognition;feature extraction;computer science;machine learning;eye vein verification;software testing	Vision	32.707450048151614	-62.056931692528586	106903
3d15f8f63a24ade6912ea822fe85d49e51cec568	ocrapose ii: an ocr-based indoor positioning system using mobile phone images		In this paper, we propose an OCR (optical character recognition)-based localization system called OCRAPOSE II, which is applicable in a number of indoor scenarios including office buildings, parkings, airports, grocery stores, etc. In these scenarios, characters (i.e. texts or numbers) can be used as suitable distinctive landmarks for localization. The proposed system takes advantage of OCR to read these characters in the query still images and provides a rough location estimate using a floor plan. Then, it finds depth and angle-of-view of the query using the information provided by the OCR engine in order to refine the location estimate. We derive novel formulas for the query angle-of-view and depth estimation using image line segments and the OCR box information. We demonstrate the applicability and effectiveness of the proposed system through experiments in indoor scenarios. It is shown that our system demonstrates better performance compared to the state-of-the-art benchmarks in terms of location recognition rate and average localization error specially under sparse database condition.	benchmark (computing);experiment;indoor positioning system;mobile phone;optical character recognition;rough set;sparse matrix	Hamed Sadeghi;Shahrokh Valaee;Shahram Shirani	2017	CoRR		computer vision;simulation;speech recognition;machine learning	Robotics	26.978931495779552	-58.69824334644257	107081
8bb072df990fe920cc2b6a56d74d881cdf455c28	a novel framework for fast scene matching in consumer image collections	search and retrieval;image edge detection feature extraction classification tree analysis accuracy classification algorithms face detection face;occlusion;image matching;image database;consumer image collections;fast scene matching;discriminative features;unified framework;classification;digital visual media;large photo collections;accuracy;image search and retrieval;sift;photo collection;image edge detection;sift feature;clustering;feature extraction;classification algorithms;image search;blur;face;classification clustering image search and retrieval sift occlusion blur;visual databases image matching image retrieval;classification tree analysis;face detection;discriminative features fast scene matching consumer image collections digital visual media image search image retrieval large photo collections sift feature image database unified framework;visual databases;image retrieval	The widespread utilization of digital visual media has motivated many research efforts towards efficient search and retrieval from large photo collections. Traditionally, SIFT feature-based methods have been widely used for matching photos taken at particular locations or places of interest. These methods are very time-consuming due to the complexity of the features and the large number of images typically contained in the image database being searched. In this paper, we propose a fast approach to matching images captured at particular locations or places of interest by selecting representative images from an image collection that have the best chance of being successfully matched by using SIFT, and relying on only these representative images for efficient scene matching. We present a unified framework incorporating a set of discriminative features that can effectively select the images containing signature elements of particular locations from a large number of images. The proposed approach produces an order of magnitude improvement in computational time for matching similar scenes in an image collection using SIFT features. The experimental results demonstrate the efficiency of our approach compared to the traditional SIFT, PCA-SIFT, and SURF-based approaches.	computation;scale-invariant feature transform;speeded up robust features;time complexity;unified framework	Xu Chen;Madirakshi Das;Alexander C. Loui	2010	2010 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2010.5582565	face;computer vision;face detection;feature extraction;biological classification;image retrieval;computer science;machine learning;pattern recognition;scale-invariant feature transform;accuracy and precision;cluster analysis;information retrieval	Vision	36.71772292980527	-55.14126451123251	107236
6c89fa7ae5e2872912d342fc0658d203ae89dcb4	swapped! digital face presentation attack detection via weighted local magnitude pattern		Advancements in smartphone applications have empowered even non-technical users to perform sophisticated operations such as morphing in faces as few tap operations. While such enablements have positive effects, as a negative side, now anyone can digitally attack face (biometric) recognition systems. For example, face swapping application of Snapchat can easily create “swapped” identities and circumvent face recognition system. This research presents a novel database, termed as SWAPPED — Digital Attack Video Face Database, prepared using Snap chat's application which swaps/stitches two faces and creates videos. The database contains bonafide face videos and face swapped videos of multiple subjects. Baseline face recognition experiments using commercial system shows over 90% rank-1 accuracy when attack videos are used as probe. As a second contribution, this research also presents a novel Weighted Local Magnitude Pattern feature descriptor based presentation attack detection algorithm which outperforms several existing approaches.	algorithm;biometrics;experiment;facial recognition system;hot swapping;mobile app;mobile phone;morphing;snapchat;visual descriptor	Akshay Agarwal;Richa Singh;Mayank Vatsa;Afzel Noore	2017	2017 IEEE International Joint Conference on Biometrics (IJCB)	10.1109/BTAS.2017.8272754	finance;economics;computer vision;feature extraction;morphing;facial recognition system;biometrics;swap (computer programming);artificial intelligence;face presentation	Vision	28.143208353448667	-62.65997554507832	107296
a197b4ed539359eb650bd2b3d506e81f99ffe27b	experimental evaluation of the bag-of-features model for unsupervised learning of images		This paper presents the results of an experimental study of the popular Bag-of-Features (BoF) model for the application of unsupervised learning of images, or image clustering. Although this method has been extensively applied for image classification and scene recognition, there has been few works which employ it in an unsupervised way. Also, due to the fact that the BoF model requires a great amount of steps, algorithms and parameter settings, we felt like there was a lack of detailed studies about the subject. We implemented testing routines in Python which we made publicly available in GitHub. In order to assess the performance of the model, three image datasets were used, namely, Coil-20 dataset, Natural and Urban dataset and Event dataset. The results obtained indicate that the BoF method provides a good representation of simple image collections for the purpose of clustering. However, it requires fine tunning of the parameters and algorithms for each dataset and obtains poor results for more complex scene datasets. We can therefore conclude that more advanced techniques are required in order to be able to effectively extract information from large image collections.	algorithm;cluster analysis;codebook;computer vision;experiment;k-means clustering;python;unsupervised learning	Mariana Afonso;Luís F. Teixeira	2015		10.5244/C.29.146	unsupervised learning;machine learning;pattern recognition	Vision	31.5295278713066	-53.14198941541793	107490
c05ee6b5f1643519e5979505c600233b2a0fe735	improved active shape model for efficient extraction of facial feature points on mobile devices	mobile device;facial landmark active shape model facial feature points extraction mobile device facial feature detection security biometrics 3d modeling facial expression recognition asm local texture model face detection rgb color information 2d profile model;face feature extraction active shape model facial features shape analytical models;shape recognition;image texture;asm active shape model;face recognition;statistical analysis;face analysis and extraction;image colour analysis;feature extraction;mobile radio;statistical analysis face recognition feature extraction image colour analysis image texture mobile radio shape recognition;facial feature points;facial landmarks;mobile device facial feature points face analysis and extraction asm active shape model facial landmarks	Detection of facial feature is fundamental for applications such as security, biometrics, 3D modeling, and facial expression recognition. Active Shape Model (ASM) is one of the most popular local texture models for face detection. This paper addresses issues related to face detection and implements an efficient extraction algorithm for facial landmarks suitable for use on mobile devices. The original ASM was modified to enhance its performance (1) improving the initialization model using the center of the eyes by utilizing a feature ma of RGB color information, (2) building a modified model definition and fitting more landmarks than the classical ASM, and (3) extending and building a 2-D profile model for detecting faces in input images. The new scheme was evaluated on experimental test set containing over 500 images of faces and found to successfully extract facial features, clearly outperforming the original ASM.	3d modeling;active shape model;algorithm;android;biometrics;computer security;computer vision;face detection;mobile device;sensor;test set	Yong-Hwan Lee;Dong-Seok Yang;Jong-Kook Lim;Yukyong Lee;Bonam Kim	2013	2013 Seventh International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing	10.1109/IMIS.2013.51	facial recognition system;image texture;computer vision;speech recognition;feature extraction;computer science;pattern recognition;mobile device	Vision	33.002157849911	-60.83007361357671	107547
cc01e64ad8f58dc1ef11f4bb1b8de96f0564eeed	font clustering and classification in document images	feature extraction;clustering algorithms;vectors	Clustering and identification of fonts in document images impacts on the performance of optical character recognition (OCR). Therefore font features and their clustering tendency are investigated. Font clustering is implemented both from shape similarity and from OCR performance points of view. A font recognition algorithm is developed to identify the font group with which a given text was created.	algorithm;optical character recognition	Serdar Öztürk;Bülent Sankur;A. Toygar Abak	2000	2000 10th European Signal Processing Conference		computer vision;speech recognition;intelligent character recognition;computer science;intelligent word recognition;pattern recognition;cluster analysis	Vision	34.36859147676516	-65.47495560379913	107587
44e95675ec43d047a73b99b4fe4e834ee9945cc7	using co-occurrence and granulometry features for content based image retrieval		This communication presents a novel system for Content Based Image Retrieval (CBIR) using Granulometry and Color Co-occurrence Features (CCF). These features are extracted directly from images using visual codebook. Relative distance measures are used to identify the similarity between the stored images and the query image. Results show that proposed method of using Granulometry and CCF is superior to most state of the art CBIR systems. The proposed system is tested on Wang image database that contains 1000 images having different categories. The performance of the system, quantified using the Average Precision Rate (APR), is very encouraging.	apache portable runtime;codebook;content-based image retrieval;granulometry (morphology);information retrieval;microsoft customer care framework;optical granulometry;spatial variability	Lal Said;Khurram Khurshid;Asia Aman	2018	EAI Endorsed Trans. Scalable Information Systems	10.4108/eai.13-4-2018.154479	co-occurrence;computer science;content-based image retrieval;computer vision;distributed computing;artificial intelligence;granulometry	Vision	33.286236567930096	-59.43746066324948	107596
3767ff7fdc424637ac644822ab67cef19b756a4f	performance evaluation of a robust method for mathematical expression recognition	bottom up strategies;image segmentation character recognition matrix algebra document image processing;two dimensional structure;image recognition;bottom up;image segmentation;performance evaluation;robustness character recognition image recognition computer science design methodology labeling;top down;matrix algebra;top down strategies;mathematical expression recognition;robust method;document image processing;robustness;computer science;matrix recognition;matrix recognition performance evaluation robust method mathematical expression recognition projection profile cutting top down strategies bottom up strategies two dimensional structure structural analysis;structural analysis;character recognition;labeling;structure analysis;design methodology;projection profile cutting	We proposed two methods for mathematical expression recognition. One is bused on projection projile cutting and the other uses top-down and bottom-up strategies to analyze the two-dimensional structure of expressions. This paper describes the improvement of the latter method in terms of structural analysis robustness and application to matrix recognition. To evaluate the performance of our method, intensive experiments were curried out on a large variety of mathematical e,upression images which were collected from many mathematical journals. nals. In our experiments, we checked the recognition performance of our method for each type of sub-expression quantitatively. The rest of this paper is organized.as follows. In the next Section, we give the characteristics of mathematical expressions on which our structural analysis method depends. A complete description of our method is given in Section 3, and experimental results are shown in Section 5. Mathematical structures supported by MT#, except for user defined descriptions, are subject to recognition by our system. 2. Characteristics of mathematical expressions	bottom-up parsing;currying;experiment;performance evaluation;structural analysis;top-down and bottom-up design	Masayuki Okamoto;Hiroki Imai;Kazuhiko Takagi	2001		10.1109/ICDAR.2001.953767	computer vision;speech recognition;computer science;machine learning;pattern recognition;top-down and bottom-up design;structural analysis	Vision	38.52234404028244	-63.70299545366718	107864
f4be4aec2c337dadfe1d734ec7a86950f30a8eec	sparse based image classification with different keypoints descriptors	learning algorithm;image classification;histograms of oriented gradients;pattern detection;computational complexity;keypoints;sparse representation	In this paper, we apply the sparse representation based algorithm to the problem of generic image classification. Keypoints with different descriptors are used as the bases of the training matrix and test samples. A learning algorithm is also presented to select the most important keypoints as the bases of the training matrix. Experiments have been done on 25 object categories selected from Caltech101 dataset, with salient region detector and different descriptors. The results show that keypoints with histogram of oriented gradients descriptor can achieve good performance on image categories which have distinctive patterns detected as keypoints. Furthermore, the base learning algorithm is useful for improving the performance while reducing the computational complexity.	sparse	Yuanyuan Zuo;Bo Zhang	2011		10.1007/978-3-642-21090-7_38	computer vision;contextual image classification;computer science;machine learning;pattern recognition;sparse approximation;mathematics;computational complexity theory	Vision	32.93639883175203	-55.62478043147547	107930
d740250c8e42f52049a20f4bfe77d9e03155fd9b	empirical evaluation of visible spectrum iris versus periocular recognition in unconstrained scenario on smartphones	databases;protocols;smart phones authorisation iris recognition lighting;smart phones;iris recognition;iris recognition smart phones cameras lighting databases feature extraction protocols;feature extraction;lighting;varying illumination scenarios visible spectrum iris periocular recognition unconstrained scenario biometric sensor authentication applications high resolution cameras biometric information protocols real life verification scenarios probe data enrollment data publicly available smartphone database cross sensor;cameras	Smartphones are increasingly used as biométrie sensor for many authentication applications due to the computational ability and high resolution cameras that can be used to capture biométrie information. The objective of this paper is to assess the performance of iris versus periocular recognition for smartphones in non ideal conditions (change of illumination, highly pigmented iris, shadows on iris pattern) in real-life for verification in visible spectrum. We introduce various protocols for real-life verification scenarios using smartphones for iris and periocular recognition. Further, we also study the verification performance where enrollment and probe data originate from different smartphones. From the extensive set of experiments conducted on a publicly available smartphone database, it can be observed that the information from periocular region provides substantially good performance in terms of recognition accuracy in cross sensor and varying illumination scenarios as compared to iris under same conditions.	authentication;baseline (configuration management);biometrics;enhanced entity–relationship model;experiment;image resolution;real life;smartphone;uncontrolled format string	Kiran B. Raja;Ramachandra Raghavendra;Christoph Busch	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041521	computer vision;engineering;internet privacy;computer security	AI	29.554936097631924	-61.982360799505706	107963
29b4411aba70d3b3f7a045c947ab6dc6464fba03	toward high-performance online hccr: a cnn approach with dropdistortion, path signature and spatial stochastic max-pooling		This paper presents an investigation of several techniques that increase the accuracy of online handwritten Chinese character recognition (HCCR). We propose a new training strategy named DropDistortion to train a deep convolutional neural network (DCNN) with distorted samples. DropDistortion gradually lowers the degree of character distortion during training, which allows the DCNN to better generalize. Path signature is used to extract effective features for online characters. Further improvement is achieved by employing spatial stochastic max-pooling as a method of feature map distortion and model averaging. Experiments were carried out on three publicly available datasets, namely CASIA-OLHWDB 1.0, CASIA-OLHWDB 1.1, and the ICDAR2013 online HCCR competition dataset. The proposed techniques yield state-of-the-art recognition accuracies of 97.67%, 97.30%, and 97.99%, respectively.	algorithmic efficiency;artificial neural network;computer vision;convolutional neural network;distortion;experiment;machine learning;optical character recognition	Songxuan Lai;Lianwen Jin;Weixin Yang	2017	Pattern Recognition Letters	10.1016/j.patrec.2017.02.011	computer vision;computer science;machine learning;pattern recognition;data mining	Vision	29.112902592934997	-54.580448717130636	108140
01ac24bfd2dd9ff70b0e8f68c65f9f53fd736ad8	clickbait: click-based accelerated incremental training of convolutional neural networks.		Today’s general-purpose deep convolutional neural networks (CNN) for image classification and object detection are trained offline on large static datasets. Some applications, however, will require training in real-time on live video streams with a human-in-the-loop. We refer to this class of problem as Time-ordered Online Training (ToOT)—these problems will require a consideration of not only the quantity of incoming training data, but the human effort required to tag and use it. In this paper, we define training benefit as a metric to measure the effectiveness of a sequence in using each user interaction. We demonstrate and evaluate a system tailored to performing ToOT in the field, capable of training an image classifier on a live video stream through minimal input from a human operator. We show that by exploiting the time-ordered nature of the video stream through optical flow-based object tracking, we can increase the effectiveness of human actions by about 8 times.		Ervin Teng;João Diogo Falcão;Bob Iannucci	2017	CoRR		convolutional neural network;artificial intelligence;operator (computer programming);machine learning;task analysis;deep learning;computer science;video tracking;classifier (linguistics);detector;drone	Vision	26.85328405731421	-54.116200243467844	108151
9ee8c2eca244afb2fd6533ff8d9fe55e370f1123	saliency detection based on multi-instance images learning	dispersion	ABSTRACT Existing visual saliency detection methods are usually based on single image, however, without priori knowledge, the contents of single image are ambiguous, so visual saliency detection based on single image cant extract region of interest. To solve it, we propose a novel saliency detection based on multi-instance images. Our method considers humans visual psychological factors and measures visual saliency based on global contrast, local contrast and sparsity. It firstly uses multi-instance learning to get the center of clustering, and then computes feature relative dispersion. By fusing different weighted feature saliency map, the final synthesize saliency map is generated. Comparing with other saliency detection methods, our method increases the rate of hit. Keywords: Visual attention, Saliency detection, Multi-instance learning, Feature fusion. 1. INTRODUCTION Saliency detection introduces human visual attention to the field of image analysis, and tries to build visual attention model which extracts regions of interest in the image, so that regions of interest can be assigned more resources which improve the efficiency of image processing. As a result, visual attention detection is widely applied to image classification, object detection, image re trieval, image compression and so on, assigns resources to regions of interest and avoids the impact of background, decreases calculation and improves image processing. In recent years, a lot of rese arch have be done on visual saliency detectio n, and made a great pr ogress [1]-[4]. There are mainly three kinds of methods: bottom-up visual attention [3], [4], top-down visual attention[5]-[8] and collaborative visual attention[9].They are usually based on single image. Bottom-up visual attention model is driven by bottom data, and is not related with task. It builds visual attention proce ss by the saliency of bottom features on spatial location. The top-down visual attention model depends on the specific task. It adjusts the selection principle, and makes attention focus on the specific objects. Collaborative visual attention coordinates bottom-up model with top-down model, which uses bottom-up process to compute spatial saliency and uses top-down process to get outer task. It makes visual attention focus on region of interest. Usually, a natural scene image contains various objects, its contents are ambiguous. For example, there are butterfly, grass, flowers and so on in Fig. 1(a). We dont know which part is the interest of human, so it is hard to classify this image or do retrieval or do object detection and so on. If we combine multi-instance images Fi g. 1 (a), (b), (c) and (d), then we will ensure interested object is butterfly. From the above analysis, existing visual saliency detection methods cant make visual attention focus on object region of interest without priori knowledge. So, according to multi-instance images, we propose a novel visual saliency detection method based on multi-instance learning, which co nsiders human's visual psychological factors and measures visual saliency based on global contrast, local contrast and sparsity. Based on multi-instance learning framework, we design visual feature saliency fusion algorithm, and get the synthesize saliency map finally.		Shouhong Wan;Peiquan Jin;Lihua Yue;Qian Huang	2015		10.1117/12.2197036	computer vision;computer science;kadir–brady saliency detector;machine learning;pattern recognition	Vision	37.60074765785563	-60.784447929388584	108194
8ba77ce371610630b4dcdc7f340894af5cdf2d92	towards accurate visual information estimation with entropy of primitive	matching pursuit algorithms;visualization entropy feature extraction matching pursuit algorithms visual perception surveillance dictionaries;surveillance;会议论文;visualization;shot boundary detection accurate visual information estimation entropy of primitive natural image visual information evaluation sparse representation natural scene visual perception visual signal nonstructural layer hierarchical visual information construction surveillance video;feature extraction;dictionaries;visual perception;entropy;visual perception entropy video signal processing video surveillance	Recently, a novel concept referred to as Entropy of Primitive (EoP) has been proposed for evaluating the visual information of natural images. The idea originates from the sparse representation, which has been successfully applied in a wide variety of signal processing and analysis tasks. This is because of the high efficiency of sparse representation in dealing with rich, varied and directional information contained in the natural scene. In this paper, we further explore the EoP to bridge the sparse representation and visual perception. Sparse primitives are divided into three categories depending on their visual importance. Accordingly the visual signal is decomposed into structural and non-structural layers. It is found that the image sparse representation is highly relevant with the hierarchical visual information construction process in representing the natural scene. We evaluate the efficiency and robustness of the EoP in real applications, including surveillance video and shot boundary detection.	closed-circuit television;color vision;shot transition detection;signal processing;sparse approximation;sparse matrix	Xiang Zhang;Shiqi Wang;Siwei Ma;Ruiqin Xiong;Wen Gao	2015	2015 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2015.7168816	computer vision;entropy;visualization;visual perception;feature extraction;computer science;machine learning;pattern recognition;human visual system model	Vision	37.601471558529575	-52.411161840648994	108478
9b4cfd76ffd32c42a4bb580c2d72e863a49976e3	real-time face detection using boosting in hierarchical feature spaces	hierarchical feature spaces;haar-like feature;face detection method;state-of-the-art face detection system;weak classifier;hierarchical feature space;later stage;face detector;face detection system;local feature;real-time face detection;face example;face recognition;real time;face detection;principal component analysis;feature space	Boosting-based methods have recently led to the state-of-the-art-face detection systems. In these systems, weak classifiers to be boosted are based on simple, local, Haar-like features. However, it can be empirically observed that in later stages of the boosting process, the non-face examples collected by bootstrapping become very similar to the face examples, and the classification error of Haar-like feature based weak classifiers is thus very close to 50%. As a result, the performance of a face detector cannot be further improved. This paper proposed a solution to this problem, introducing a face detection method based on boosting in hierarchical feature spaces (both local and global). We argue that global features, like those derived from principal component analysis, can be advantageously used in the later stages of boosting, when local features do not provide any further benefit. We show that weak classifiers learned in hierarchical feature spaces are better boosted. Our methodology leads to a face detection system that achieves higher performance than a current state-of-the-art system, at a comparable speed.	adaboost;algorithm;boosting (machine learning);experiment;face detection;feature vector;haar wavelet;principal component analysis;real-time clock;test set;weak value	Dong Zhang;Stan Z. Li;Daniel Gatica-Perez	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334238	facial recognition system;computer vision;face detection;speech recognition;feature vector;boosting methods for object categorization;computer science;machine learning;pattern recognition;mathematics;principal component analysis	Vision	32.69097061978909	-55.7116310602013	108682
5cd53eb031065e1567d4c7f8eecdf45300065de9	facial recognition — an overview		Abstract   Research on automatic facial recognition has been ongoing for more than 20 years, and there is a large variety of solutions being proposed. The big advantage is the user-friendly way of identification by a video camera. Recently, some of the projects have resulted in commercially available products which are in use in banking and high-security applications.		Jürgen Pampus;Frank Weber	1998	Inf. Sec. Techn. Report	10.1016/S1363-4127(98)80017-4	computer vision;simulation;multimedia	AI	26.802590784054914	-59.78663012208775	108760
ace193f4db9711b5735d9837f68c3c64007b33f3	object labeling for recognition using vocabulary trees	databases;object recognition;vocabulary;trees mathematics;trees mathematics image retrieval object recognition;computer vision;labeling vocabulary image databases visual databases image retrieval information retrieval object recognition image recognition frequency image analysis;visualization;face recognition;quantitative analysis;face;recognition object labeling vocabulary trees object recognition query image database images pose descriptor image retrieval face database;image retrieval	We propose an approach to object recognition using vocabulary tree which, instead of finding the closest image in the database to the given query image, finds object labels representing the most similar objects to the query image. We can also recognize object pose if pose labels are associated to the database images. Our approach to object recognition relies on creating a specific object or pose descriptor for each group of database images representing the same object or object pose. The quantitative analysis showed that this approach is more efficient, both in terms of precision and speed, compared to original image retrieval based on vocabulary tree. The experiments are performed for object recognition on two different databases and pose recognition using available face database.	vocabulary	Slobodan Ilic	2008		10.1109/ICPR.2008.4761504	facial recognition system;natural language processing;face;computer vision;object-class detection;visualization;image retrieval;computer science;quantitative analysis;viola–jones object detection framework;cognitive neuroscience of visual object recognition;pattern recognition;3d single-object recognition;automatic image annotation	Vision	39.0162980833311	-57.82099937749168	109189
2fde38e4c4966f10e60fe44b48b99daac9d3e30a	image annotation by incorporating word correlations into multi-class svm	support vector machines;multiclass svm;support vector machines support vector machine classification image segmentation feature extraction mpeg 7 standard vocabulary tiles computer science labeling object segmentation;automatic image annotation;training;color features;transform coding;matrix algebra;image annotation;corel 5000 dataset multiclass svm support vector machine word correlations image annotation systems mpeg 7 visual descriptors color features texture features cooccurrence matrix minimal redundancy maximum relevance method;texture features;image texture;word correlations;accuracy;visualization;image color analysis;image colour analysis;image annotation systems;automatic annotation;cooccurrence matrix;mrmr image annotation word correlations svm tiling scheme mpeg 7;minimal redundancy maximum relevance method;svm;support vector machines image colour analysis image texture matrix algebra;tiling scheme;support vector machine;correlation;corel 5000 dataset;mrmr;co occurrence matrix;mpeg 7 visual descriptors;mpeg 7	Image annotation systems aim at automatically annotating images with some predefined keywords. In this paper, we propose an automatic image annotation approach by incorporating word correlations into multi-class Support Vector Machine (SVM). At first, each image is segmented into five fixed-size blocks or tiles and MPEG-7 visual descriptors are applied to represent color and texture features of blocks. Keywords are manually assigned to every block of training images. Then, multi-class SVM classifier is trained for semantic concepts. Word or concept correlations are computed by a co-occurrence matrix. The probability outputs from SVM and word correlations are combined to obtain the final results. The minimal-redundancy-maximum-relevance (mRMR) method is used to reduce feature dimensions. The experiments on Corel 5000 dataset demonstrate our approach is effective and efficient.	algorithm;automatic image annotation;co-occurrence matrix;corel linux;document-term matrix;experiment;feature selection;high- and low-level;mpeg-7;ontology (information science);relevance;support vector machine;visual descriptor	Lei Zhang;Jun Ma	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.461	computer vision;computer science;machine learning;pattern recognition;automatic image annotation	Vision	34.690223249439434	-54.83470470011559	109224
e31e92928977df3f8da9ff78fbb60b217d88da61	content based image retrieval using combined features	content based image retrieval cbir;image database;natural images;feature vector;computational complexity;feature extraction;greedy strategy;content based image retrieval;co occurrence matrix;open source;image retrieval;edge histogram descriptor ehd	In this paper a novel approach for image retrieval based on semantic contents is presented. A combination of three feature extraction methods namely color, texture, and edge histogram descriptor. Any combination of these methods, which is more appropriate for the application, can be used for retrieval. For color the histogram of images are computed, for texture co-occurrence matrix based entropy, energy, etc, are calculated and for edge density it is Edge Histogram Descriptor (EHD) that is found. For retrieval of images, an idea is based on greedy strategy to reduce the computational complexity. The entire system will be developed using MALAB (an open source product), The system will be tested with Image database containing 1000 natural images.	co-occurrence matrix;computational complexity theory;content-based image retrieval;document-term matrix;feature extraction;greedy algorithm;open-source software;texture mapping	C. Patil;V. Dalal	2011		10.1145/1980022.1980043	color histogram;image texture;computer vision;visual word;computer science;histogram matching;pattern recognition;automatic image annotation;information retrieval;image histogram	Vision	38.197093870134516	-60.19187753270863	109265
ac9ff519e19b5d0e9e677a03ae28f7ca556a5a13	near-threshold perceptual distortion prediction based on optimal structure classification	training;nonlinear distortion;computational modeling;feature extraction;predictive models;adaptation models	Perceptual distortion prediction at near-threshold level has many applications in general image/video processing tasks. This paper presents a computational model to predict the near-threshold perceptual distortions based on optimal structure classification. This model accounts for contrast sensitivity, light adaptation, and various masking effects of the human visual system (HVS), and automatically adapts to local image structures by a soft classification scheme using a Gaussian Mixture Model (GMM). The proposed model is trained and verified on the public CSIQ local masking database. We demonstrate a superior prediction performance of the proposed model compared to previous research.	cellular automaton;computational model;distortion;google map maker;human visual system model;mixture model;video processing	Yucheng Liu;Jan P. Allebach	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532328	computer vision;nonlinear distortion;feature extraction;computer science;machine learning;pattern recognition;predictive modelling;computational model	Vision	36.56762842387573	-57.79191005080624	109344
e3c3d02bb6e33a575c789ff0f6c10808a601ae01	fast human-animal detection from highly cluttered camera-trap images using joint background modeling and deep learning classification		In this paper, we couple effective dynamic background modeling with deep learning classification to develop a fast and accurate scheme for human-animal detection from highly cluttered camera-trap images using joint background modeling and deep learning classification. Specifically, first, we develop an effective background modeling and subtraction scheme to generate region proposals for the foreground objects. We then develop a cross-frame image patch verification to reduce the number of foreground object proposals. Finally, we perform complexity-accuracy analysis of deep convolutional neural networks (DCNN) to develop a fast deep learning classification scheme to classify these region proposals into three categories: human, animals, and background patches. The optimized DCNN is able to maintain high level of accuracy while reducing the computational complexity by 14 times. Our experimental results demonstrate that the proposed method outperforms existing methods on the camera-trap dataset.	artificial neural network;comparison and contrast of classification schemes in linguistics and metadata;computational complexity theory;convolutional neural network;deep learning;high-level programming language;image resolution;thresholding (image processing);verification and validation	Hayder Yousif;Jianhe Yuan;Roland Kays;Zhihai He	2017	2017 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2017.8050762	convolutional neural network;computer science;deep learning;computational complexity theory;computer vision;camera trap;classification scheme;image segmentation;artificial intelligence;pattern recognition	Vision	29.470479254235265	-53.31186212341008	109528
c18ed73e6a866dec033f51602dbe9b1d2859203e	interactive embedded face recognition	hand held device;skin detection;selected works;color space;wireless network;morphological operation;test bed;skin color;face recognition;bepress;face detection;embedded device	This paper describes the design and construction of a prototype for embedded face detection and recognition algorithms. The test-bed is called the PITS (Portable Interactive Terrorist Identification System). It makes use of a hand-held device called the Sharp Zaurus. The embedded device has a processor, camera, color display, and wireless networking. This system is different from existing systems because of its embedded nature and its’ use of Java technologies. The embedded device performs both detection and recognition. We present a skin color approach in the YCbCr color space for fast and accurate skin detection. We then process this image using a combination of morphological operators and elliptical shape of faces to segment faces from the other skin colored regions. An eigenface algorithm processes the segmented faces and matches the face to a face database.	algorithm;color space;eigenface;embedded system;face detection;facial recognition system;interactivity;java;mathematical morphology;mobile device;prototype;sharp zaurus;testbed	Douglas Lyon;Nishanth Vincent	2009	Journal of Object Technology	10.5381/jot.2009.8.1.c2	facial recognition system;computer vision;face detection;speech recognition;object-class detection;computer science;engineering;wireless network;color space;computer graphics (images);testbed	Mobile	32.771033955700304	-60.640398753981295	109594
908f4d7df85837ce2745303034d99cd8aba32cfe	partial fingerprint alignment and matching through region-based approach	health intervention;mhealth;ehealth;gamification;individualisation;framework	One of the most important issues in fingerprint recognition is that the impressions of different fingers may appear very similar to each other (high inter-class similarity) and likewise, different acquisition of the same finger may be dissimilar from each other (high intra-class variation). To compensate for high intra-class variation and inter-class similarity, this paper addresses two challenging issues in partial fingerprint recognition which are alignment and computing the similarity score. Rotational and displacement variation between registered and query fingerprints occur during scanning. As a consequence, fingerprint alignment is essential to correct for the rotational and translation difference between fingerprints prior to matching. By perfectly aligning the fingerprints, the matching could be simplified. Considering the possibility of high intra-class variation and inter-class similarity, how the similarity score of two fingerprints is computed plays an important role in system's match or non-match decision.	displacement mapping;fingerprint recognition	Omid Zanganeh;Nandita Bhattacharjee;Bala Srinivasan	2015		10.1145/2837126.2837132	computer science;software framework;data mining;ehealth;world wide web	Vision	30.97672322123168	-62.227682466223364	109694
0d05ba47bb2e7697652aae2ffe601c0e1b89c9ba	speaker recognition using neural networks and conventional classifiers	feedforward neural network;speaker identification;decision tree;full search;speaker recognition neural networks feature extraction classification tree analysis signal processing decision trees databases nearest neighbor searches vector quantization multilayer perceptrons;speech recognition vector quantisation trees mathematics decision theory feedforward neural nets;multilayer perceptron;trees mathematics;speaker verification;speaker recognition;tree structure;decision theory;speech recognition;feedforward neural nets;vector quantizer;tree network;nearest neighbor classifier;vector quantisation;multilayer perceptrons neural networks text independent speaker recognition modified neural tree network mntn hierarchical classifier decision trees feedforward neural networks learning rule pruning criteria speaker recognition experiments open set speaker identification closed set speaker identification speaker verification timit database nearest neighbor classifiers full search vq tree structured vector quantization;neural network	An evaluation of various classifiers for textindependent speaker recognition is presented. In addition, a new classifier is examined for this application. The new classifier is called the modified neural tree network (MNTN). The MNTN is a hierarchical classifier that combines the properties of decision trees and feedforward neural networks. The MNTN differs from the standard NTN in both the new learning rule used and the pruning criteria. The MNTN is evaluated for several speaker recognition experiments. These include closedand open-set speaker identification and speaker verification. The database used is a subset of the TIMIT database consisting of 38 speakers from the same dialect region. The MNTN is compared with nearest neighbor classifiers, full-search, and tree-structured vector quantization (VQ) classifiers, multilayer perceptrons (MLP’s), and decision trees. For closed-set speaker identification experiments, the full-search VQ classifier and MNTN demonstrate comparable performance. Both methods perform significantly better than the other classifiers for this task. The MNTN and full-search VQ classifiers are also compared for several speaker verification and open-set speaker-identification experiments. The MNTN is found to perform better than full-search VQ classifiers for both of these applications. In addition to matching or exceeding the performance of the VQ classifier for these applications, the MNTN also provides a logarithmic saving for retrieval. 1	artificial neural network;decision tree;experiment;feedforward neural network;hierarchical classifier;learning rule;memory-level parallelism;multilayer perceptron;speaker recognition;timit;tree network;vector quantization	Kevin R. Farrell;Richard J. Mammone;Khaled T. Assaleh	1994	IEEE Trans. Speech and Audio Processing	10.1109/89.260362	random subspace method;speaker recognition;feedforward neural network;speech recognition;decision theory;computer science;machine learning;decision tree;pattern recognition;tree structure;multilayer perceptron	ML	25.51078080361253	-62.00422832332697	109886
dbe3f4be959aac514a0b9067f0fcd16d2356c5c8	classification of infrasound surf events using parallel neural network banks	parallel neural network banks;receiver operator characteristic;ocean surf;receiver operating characteristic curve classification infrasound surf events parallel neural network banks ocean surf surf waves infrasonic signatures parallel neural network classifier bank radial basis function network;acoustic signal processing;neural network classifier;classification;receiver operating characteristic curve;three dimensional;radial basis function networks;confidence interval;parallel neural network classifier bank;radial basis function;radial basis function network;infrasonic signatures;infrasound surf events;confusion matrix;signal classification acoustic signal processing acoustic waves learning artificial intelligence ocean waves radial basis function networks;signal classification;roc curve;surf waves;acoustic waves;learning artificial intelligence;neural networks testing feature extraction cepstral analysis pipelines sensor arrays frequency robustness sampling methods time domain analysis;rbf network;ocean waves;neural network	"""One of the most active locations in the world for ocean surf is the North Shore of Oahu. Here we show surf waves from three locations, """"Pinballs,"""" """"Pipeline,"""" and """"Shark's cove,"""" on the North Shore yield distinctive infrasonic signatures that can be used to train and test a neural network classifier. A parallel neural network classifier bank (PNNCB) is developed to classify the surf events from the three different locations on Oahu. Each module in the bank is a radial basis function (RBF) network responsible for classifying one of the three surf events. However, each module is also trained to not classify the other two surf events, and this ultimately increases the overall classifier performance. Output thresholds of each module are set according to a specific three-dimensional receiver operating characteristic (ROC) curve. For the three surf events, the correct classification rate achieved is 87.1%. A confusion matrix of the complete neural network classifier bank is shown along with confidence intervals for each class and the overall accuracy."""	artificial neural network;confusion matrix;preprocessor;radial (radio);radial basis function network;receiver operating characteristic;signature;speeded up robust features;statistical classification	Fredric M. Ham;Ranjan Acharyya;Young-Chan Lee;Milton Garces;David Fee;Chelsea Whitten;Eric Rivera	2007	2007 International Joint Conference on Neural Networks	10.1109/IJCNN.2007.4371046	speech recognition;computer science;artificial intelligence;machine learning;receiver operating characteristic;artificial neural network	ML	26.081919020216368	-61.54438235504967	110067
489ec971dc6d4e858bb220d2cb286e56a01de519	lensless-camera based machine learning for image classification		Machine learning (ML) has been widely applied to image classification. Here, we extend this application to data generated by a camera comprised of only a standard CMOS image sensor with no lens. We first created a database of lensless images of handwritten digits. Then, we trained a ML algorithm on this dataset. Finally, we demonstrated that the trained ML algorithm is able to classify the digits with accuracy as high as 99% for 2 digits. Our approach clearly demonstrates the potential for non-human cameras in machine-based decisionmaking scenarios.	algorithm;cmos;computer vision;image sensor;machine learning	Ganghun Kim;Stefan Kapetanovic;Rachael Palmer;Rajesh Menon	2017	CoRR		computer science;pattern recognition;machine learning;image sensor;contextual image classification;computer vision;artificial intelligence	AI	27.280240822837172	-57.976139295465444	110219
708dbc7d3b3588d55b5cca293624b71f5458203a	attention enhanced convnet-rnn for chinese vehicle license plate recognition		As an important part of intelligent transportation system, vehicle license plate recognition requires high accuracy in an open environment. While a lot of approaches have been proposed, and achieved good performance to some extent, these approaches still have problems, for example, in the condition of characters’ distortion or partial occlusion. Segmentation-free VLPR systems compute the label in one pass using Long Short-Term Memory Network (LSTM), without individual segmentation step, their results tend to be not influenced by the segmentation accuracy. Based on the idea of Segmentation-free VLPR, this paper proposed an attention enhanced ConvNet-RNN (AC-RNN) for accurate Chinese Vehicle License Plate Recognition. The attention mechanism helps to locate the important instances in the step of recognition. While the ConvNet is used to extract features, the recurrent neural networks (RNN) with connectionist temporal classification (CTC) are applied for sequence labeling. The proposed AC-RNN was trained on a large generated dataset which contains various types of license plates in China. The AC-RNN could figure out the vehicle license even in cases of light changing, spatial distortion and partial blurry. Experiments showed that the AC-RNN performs better on the testing real images, increasing about 5% on accuracy, compared with classic ConvNet-RNN [8].	automatic number plate recognition;convolutional neural network;random neural network	Shiming Duan;Wei Hu;Ruirui Li;Wei Li;Shihao Sun	2018		10.1007/978-3-030-03335-4_36	license;sequence labeling;distortion;recurrent neural network;intelligent transportation system;real image;computer science;artificial intelligence;pattern recognition	Vision	28.930031538452177	-54.643453047310885	110226
cca43afdedcca28281f524dc06fe14bf75df67fd	learning semantic visual dictionaries: a new method for local feature encoding	greedy group sparse coding;scene classification;learning artificial intelligence dictionaries image classification image coding;scene classification semantic dictionary learning greedy group sparse coding;scene classification semantic visual dictionary learning local image feature encoding random local image patches visual patterns thumbnail images class labels cluster centers adaptation method benchmark datasets;dictionaries semantics visualization computer vision encoding conferences pattern recognition;semantic dictionary learning	In this paper, we develop a new method to learn semantic visual dictionaries for local image feature encoding. Conventional methods usually learn dictionaries from random local image patches. Different from them, we manually select a number of object classes whose visual patterns can be seen at local image patch level in complex images (Figure 1), and learn dictionaries from their thumbnail images. The benefit is that these thumbnail images have class labels, so we can cluster semantically similar images together to generate meaningful cluster centers. Some other contributions of this paper include developing an adaptation method to adapt the learned dictionaries to target datasets, and developing efficient algorithms to encode local patches with our semantic visual dictionaries. Experimental results on three benchmark datasets demonstrate the effectiveness of the proposed methods.	algorithm;benchmark (computing);data dictionary;dictionary;encode;feature (computer vision);greedy algorithm;mathematical optimization;neural coding;optimization problem;sparse matrix;thumbnail	Bing Shuai;Zhen Zuo;Gang Wang	2015	2015 IEEE International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2015.7252007	computer vision;computer science;machine learning;pattern recognition	Vision	34.68702440689577	-53.83779406392157	110479
63cd37de5dd3d9c048175fdcb51d05297fab2bd0	efficient image retrieval based on quantized histogram texture features in dct domain	dct content based image retrieval cbir statistical texture features quantized histogram;quantized histogram;content based image retrieval cbir;information retrieval;texture features;discrete cosine transform;image texture;feature extraction histograms vectors discrete cosine transforms image coding image retrieval;statistical analysis;statistical analysis content based retrieval discrete cosine transforms feature extraction image retrieval image texture;discrete cosine transforms;feature extraction;statistical texture features;corel animal database quantized histogram texture feature dct domain discrete cosine transforms content based image retrieval feature extraction;content based image retrieval;content based retrieval;dct;image retrieval	Huge number of images is available on the internet. Efficient and effective retrieval system is needed to retrieve these images by the contents or features of the images like color, texture and shape. This system is called content based image retrieval (CBIR). Conventionally features are extracted from images in pixel domain. But at present almost all images are represented in compressed form using DCT (Discrete Cosine Transformation) blocks transformation. Some critical information is removed in compression and only perceptual information is left which has significant attraction for information retrieval in compressed domain. In this paper we study the problem that how to retrieve perceptual information in compressed domain JPEG such that to improve image retrieval. Our approach is based on quantized histogram statistical texture features in DCT blocks. We show that to get best image retrieval performance by extracting the statistical texture features of quantized histogram in DCT blocks using JPEG compressed format images. Experiments on the Corel animal database using the proposed approach, give results which show that the statistical texture features of histogram are robust in retrieval of images. This shows that texture features in local compression is a significant step for effective image retrieval.	content-based image retrieval;discrete cosine transform;information retrieval;internet;jpeg;pixel;texture mapping	Fazal-e-Malik;Baharum Baharudin;Kifayat Ullah	2011	2011 Frontiers of Information Technology	10.1109/FIT.2011.24	image texture;computer vision;visual word;pattern recognition;information retrieval	Vision	39.07678753281527	-61.06375415650408	110528
ed98d50c8044874c03204a00a29b0544e2f76770	deep salient object detection with dense connections and distraction diagnosis		In this paper, we propose two novel components for improving deep salient object detection models. The first component, called saliency detection network (S-Net), introduces dense short- and long-range connections that effectively integrate multiscale features to better exploit contexts at multiple levels. Benefiting from the direct access to low- and high-level features, the S-Net can not only exploit the object context but also preserve the object boundary sharply, leading to enhanced saliency detection performance. Second, a distraction detection network (D-Net) is developed to learn to diagnose which regions of an input image are distracting and harmful for saliency prediction of the S-Net. With such distraction diagnosis, the regions that are distracting to S-Net are removed in hindsight from the input image and the resulted distraction-free image is fed to S-Net for saliency prediction. To train the D-Net, a distraction mining approach is proposed to localize the model-specific distracting regions through examining the sensitiveness of the S-Net to image regions in a principled manner. Besides, the distraction mining approach also provides a way to interpret decisions made by deep neural network (DNN) saliency detection models, which relieves the black-box issues of DNNs to some extent. Extensive experiments on seven popular benchmark datasets demonstrate the effectiveness of the combined S-Net and D-Net, which provides new state of the arts.	artificial neural network;benchmark (computing);black box;deep learning;experiment;high- and low-level;novell s-net;object detection;random access	Huaxin Xiao;Jiashi Feng;Yunchao Wei;Maojun Zhang;Shuicheng Yan	2018	IEEE Transactions on Multimedia	10.1109/TMM.2018.2830098	pattern recognition;computer science;artificial intelligence;task analysis;computer vision;feature extraction;object detection;image segmentation;distraction;artificial neural network;salience (neuroscience);exploit	Vision	26.658278915763727	-52.56263007079915	110624
f8703ae50ef94cfb59c74e772f2a804dcf6e051f	design of a detection system of faces intercepted by video based on the skin color model		In this paper, we have completed the detection of human faces in a frame of video through the establishment of the skin color model. In order to reduce the influence of illumination on face detection, the method of light compensation is used before the face detection. Then, the captured image is transformed into the YCbCr color space with good color clustering characteristics. Finally, the skin color model is created by the elliptical skin color model in the YCbCr color space, and the traditional morphological processing by opening and then closing, is improved into the morphological processing by opening and then dilating and closing. Finally, the face area is identified.	adaboost;algorithm;closing (morphology);cluster analysis;color space;computation;face detection;facial recognition system;illumination (image);technical support;template matching	Chen Yan;Zhengqun Wang;Xue Zhou	2017	2017 4th International Conference on Systems and Informatics (ICSAI)	10.1109/ICSAI.2017.8248283	control engineering;computer science;face detection;color model;cluster analysis;facial recognition system;algorithm design;ycbcr;computer vision;artificial intelligence;color space	Vision	36.97998829389936	-58.699389345438185	110802
15297f937681acbab78fbcc73ad6d78b4dbe7130	a novel approach for graphics recognition based on galois lattice and bag of words representation	symbol manipulation;detectors;symbol manipulation galois fields image classification image recognition;image recognition;local descriptors;lattices;concept lattices;image classification;visualization lattices feature extraction dictionaries vectors robustness detectors;concept lattices graphics recognition symbol classification shape descriptors local descriptors visual words bag of features;visualization;vectors;bag of features;feature extraction;dictionaries;robustness;shape descriptors;graphics recognition;symbol classification;visual words;galois fields;visual words graphics recognition approach galois lattice bag of words representation graphical symbol recognition	This paper presents a new approach for graphical symbols recognition by combining a concept lattice with a bag of words representation. Visual words define the properties of a graphical symbol that will be modeled in the Galois Lattice. The algorithm of classification is based on the Galois lattice where intentions of its concepts are visual words. The words as visual primitives allow to evaluate the classifier with a symbolic approach that no longer need a signature discretization step to build the Galois Lattice. Our approach is compared to classical approaches on different graphical symbols and we show the relevance and the robustness of our proposal for the classification task.	algorithm;bag-of-words model;discretization;formal concept analysis;graphics;relevance;robustness (computer science)	Amani Boumaiza;Salvatore Tabbone	2011	2011 International Conference on Document Analysis and Recognition	10.1109/ICDAR.2011.170	computer vision;detector;contextual image classification;visual word;visualization;feature extraction;computer science;theoretical computer science;machine learning;pattern recognition;lattice;finite field;robustness	Vision	37.453401832089334	-58.57645392051379	111078
926e97d5ce2a6e070f8ec07c5aa7f91d3df90ba0	facial expression recognition using enhanced deep 3d convolutional neural networks		Deep Neural Networks (DNNs) have shown to outperform traditional methods in various visual recognition tasks including Facial Expression Recognition (FER). In spite of efforts made to improve the accuracy of FER systems using DNN, existing methods still are not generalizable enough in practical applications. This paper proposes a 3D Convolutional Neural Network method for FER in videos. This new network architecture consists of 3D Inception-ResNet layers followed by an LSTM unit that together extracts the spatial relations within facial images as well as the temporal relations between different frames in the video. Facial landmark points are also used as inputs to our network which emphasize on the importance of facial components rather than the facial regions that may not contribute significantly to generating facial expressions. Our proposed method is evaluated using four publicly available databases in subject-independent and cross-database tasks and outperforms state-of-the-art methods.	convolutional neural network;database;experiment;facial recognition system;graphics processing unit;ibm notes;long short-term memory;map;multi media interface;network architecture;neural networks;tesla (microarchitecture);whole earth 'lectronic link	Behzad Hassani;Mohammad H. Mahoor	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.282	convolutional neural network;computer vision;machine learning;artificial intelligence;facial recognition system;pattern recognition;computer science;artificial neural network;feature extraction;facial expression;spite;three-dimensional face recognition;face hallucination	Vision	25.755411568791125	-52.19715823033623	111195
ca924c48b568f7562fb8c67a28eeb87b96f2a143	user-drawn sketch-based 3d object retrieval using distributed compressive sensing	three dimensional displays vectors shape feature extraction compressed sensing solid modeling databases;compressed sensing;2d sketch query image user drawn sketch based 3d object retrieval system distributed compressive sensing hybrid shape descriptor similarity measure high dimensional sparse feature vector view based 3d object retrieval;distributed compressive sensing 3d object retrieval;image retrieval compressed sensing;image retrieval	User-drawn sketch-based 3D object retrieval system is popularly used for the basis of Computer Aided Design, scientific simulation and visualization, and serious game. The sketch-based 3D object retrieval is dependent on the descriptors and similarity measure to find and order the 3D objects what we want to see. In this paper we present a hybrid shape descriptor that integrates the local and global features to effectively retrieve the 3D objects from 2D sketch query image. Integrated shape descriptor with the high-dimensional sparse feature vector is then optimized using distributed compressive sensing to improve the performance of the view-based 3D object retrieval. The experimental results demonstrate the effectiveness and advantages of our proposed framework.	compressed sensing;computer-aided design;feature vector;similarity measure;simulation;sparse matrix	Sang Min Yoon;Kwang-Soo Hahn;Gang-Joon Yoon	2013	2013 International Conference on ICT Convergence (ICTC)	10.1109/ICTC.2013.6675454	computer vision;visual word;computer science;pattern recognition;information retrieval	Robotics	35.669716979705875	-54.396862239577445	111241
02ffd3cad17926df7d37940179fd722ad4a71924	flattening supervoxel hierarchies by the uniform entropy slice	quadratic programming;image segmentation;video signal processing;supervoxel hierarchy flattening benchmark internet videos segmentation tree hierarchy supervised feature criteria unsupervised feature criteria binary quadratic program hoc feature criterion supervoxel selection fine level coarse level unsupervised process video analysis video multiscale decomposition entropy slice uniformity;trees mathematics;video signal processing entropy image segmentation quadratic programming trees mathematics;entropy;entropy benchmark testing image segmentation motion segmentation vectors image coding measurement	Supervoxel hierarchies provide a rich multiscale decomposition of a given video suitable for subsequent processing in video analysis. The hierarchies are typically computed by an unsupervised process that is susceptible to under-segmentation at coarse levels and over-segmentation at fine levels, which make it a challenge to adopt the hierarchies for later use. In this paper, we propose the first method to overcome this limitation and flatten the hierarchy into a single segmentation. Our method, called the uniform entropy slice, seeks a selection of supervoxels that balances the relative level of information in the selected supervoxels based on some post hoc feature criterion such as object-ness. For example, with this criterion, in regions nearby objects, our method prefers finer supervoxels to capture the local details, but in regions away from any objects we prefer coarser supervoxels. We formulate the uniform entropy slice as a binary quadratic program and implement four different feature criteria, both unsupervised and supervised, to drive the flattening. Although we apply it only to supervoxel hierarchies in this paper, our method is generally applicable to segmentation tree hierarchies. Our experiments demonstrate both strong qualitative performance and superior quantitative performance to state of the art baselines on benchmark internet videos.	benchmark (computing);earthbound;experiment;hoc (programming language);quadratic programming;supervised learning;unsupervised learning;video clip;video content analysis	Chenliang Xu;Spencer Whitt;Jason J. Corso	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.279	computer vision;entropy;computer science;machine learning;pattern recognition;mathematics;image segmentation;quadratic programming	Vision	38.504194083055665	-54.27959089889931	111347
ae8646d5e8920a3c0f89500a5caf47b3221377a5	hybrid aggregation of sparse coded descriptors for food recognition	pooling;food recognition rfid top ranked average pooling max pooling sparse coded vector food image representation vector quantization bag of features dietary life hybrid aggregation sparse coded descriptors;image recognition;image coding;vector quantization;vectors;image representation;feature extraction;vectors image representation feature extraction vector quantization encoding image recognition image coding;food image;encoding;sparse coding;vectors food processing industry image coding image representation radiofrequency identification;pooling food image image recognition sparse coding	Recent year, with the increasing of unhealthy diets which will threaten people's life due to the various resulted risks such as heart stroke, liver trouble and so on, the maintaining for healthy life has attracted much attention and then how to manage the dietary life is becoming more and more important. In this research, we aim to construct an auto-recognition system of food images and keep the daily food-log records which will contribute to manage dietary life. With the easily available food images taken by mobile phone, it prospects to give the insight about the daily dietary of users with our constructed food recognition system. In order to achieve the acceptable recognition performance of the food images, we propose to apply a sparse model for coding local descriptors extracted from the food images and various pooling methods for aggregating the xoded descriptors. Sparse coding: an extension of vector quantization for local descriptors, which is popularly used in Bag-of-Features (BoF) for image representation, can reconstruct the local descriptors more effective, and then obtain more discriminated feature for food image representation. However, in order to emphasize the strongest activated pattern, the widely applied aggregation strategy of the sparse coded vector is only to retain the maximum coefficient in all (named as Max-pooling), which would completely ignore the frequency: an important signature for identifying different types of images, of the activated patterns. Therefore, we explore a hybrid aggregation strategy named as top-ranked average pooling (TRAP), which integrates not only the maximum activated magnitude but also the stronger activated number for image representation. Experiments validate that the proposed hybrid aggregation strategy combined with sparse model can greatly improve the recognition rates compared with the conventional BOF model and the state-of-the-art methods on two databases: our constructed RFID and the public PFID.	coefficient;database;mobile phone;neural coding;performance;sparse matrix;vector quantization	Riko Kusumoto;Xian-Hua Han;Yen-Wei Chen	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.265	computer vision;feature extraction;computer science;machine learning;pattern recognition;mathematics;neural coding;vector quantization;encoding;pooling	Vision	27.604599404725132	-60.68046311503921	111480
2716850eca56765791bbdc7075a6d893c6484f4c	laplacian and bilaplacian based features for shape classification	dirichlet laplacian;clamped plate;shape classification;buckling problem	Finite difference schemes of various eigenvalue problems are used to generate size, rotation, and translation invariant sets of features for shape recognition and classification of binary images. These feature sets are based on the eigenvalues of the Dirichlet Laplacian, the clamped plate problem, and the buckling problem. The stability and effectiveness of these features is demonstrated by using them in the classification of 6 types of computer generated and hand-drawn shapes. The classification was done using 4 to 20 features fed to simple feed-forward neural networks trained using the backpropagation algorithm. All features performed very well and correct classification rates of up to 99.7% were achieved on the computer generated shapes and 97.2% on the hand-drawn shapes.		Mohamed Ben Hadj Rhouma;Lotfi Hermi;Mohamed A. Khabou	2009			topology;pattern recognition;geometry;laplacian smoothing	Vision	32.34242185190109	-61.016829727463026	111484
6fca5ebcc7cd7930e01cd9d7c1f33ca4f0af1cd3	deep learning for plant identification in natural environment		Plant image identification has become an interdisciplinary focus in both botanical taxonomy and computer vision. The first plant image dataset collected by mobile phone in natural scene is presented, which contains 10,000 images of 100 ornamental plant species in Beijing Forestry University campus. A 26-layer deep learning model consisting of 8 residual building blocks is designed for large-scale plant classification in natural environment. The proposed model achieves a recognition rate of 91.78% on the BJFU100 dataset, demonstrating that deep learning is a promising technology for smart forestry.		Yu Sun;Yuan Liu;Guan Wang;Haiyan Zhang	2017		10.1155/2017/7361042	simulation	Vision	25.010239395317956	-59.424925547108984	111548
8135731e67fd730da3605aca8e54fc1fdd5fa73a	histogram ratio features for color texture classification	histograms;texture classification;color histogram;color processing;classification;feature extraction	This paper presents a color texture classification method using ratio features extracted from the color histogram. Combining pairs of bins and computing corresponding count ratios, ratio features are created that characterize the given color texture in an auto-correlative sense. The method is compared against the traditional color histogram method and improved results are obtained.		Georgios S. Paschos;Maria Petrou	2003	Pattern Recognition Letters	10.1016/S0167-8655(02)00244-1	color histogram;computer vision;color normalization;color image;feature extraction;biological classification;computer science;histogram matching;machine learning;pattern recognition;histogram;color balance;histogram equalization	Vision	37.43531156515287	-60.8690805382158	111963
3b5367149448aef0c27644daf05df3231ed15374	a framework of rapid regional tsunami damage recognition from post-event terrasar-x imagery using deep neural networks		Near real-time building damage mapping is an indispensable prerequisite for governments to make decisions for disaster relief. With high-resolution synthetic aperture radar (SAR) systems, such as TerraSAR-X, the provision of such products in a fast and effective way becomes possible. In this letter, a deep learning-based framework for rapid regional tsunami damage recognition using post-event SAR imagery is proposed. To perform such a rapid damage mapping, a series of tile-based image split analysis is employed to generate the data set. Next, a selection algorithm with the SqueezeNet network is developed to swiftly distinguish between built-up (BU) and nonbuilt-up regions. Finally, a recognition algorithm with a modified wide residual network is developed to classify the BU regions into wash away, collapsed, and slightly damaged regions. Experiments performed on the TerraSAR-X data from the 2011 Tohoku earthquake and tsunami in Japan show a BU region extraction accuracy of 80.4% and a damage-level recognition accuracy of 74.8%, respectively. Our framework takes around 2 h to train on a new region, and only several minutes for prediction.	aperture (software);artificial neural network;computation;data pre-processing;deep learning;flow network;image resolution;preprocessor;real-time locating system;requirement;selection algorithm;synthetic data;time complexity	Yanbing Bai;Chang Gao;Sameer Singh;Magaly Koch;Bruno Adriano;Erick Mas;Shunichi Koshimura	2018	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2772349	selection algorithm;residual;artificial neural network;mathematics;artificial intelligence;computer vision;deep learning;remote sensing;synthetic aperture radar	Vision	28.627247409568362	-54.57100604682697	111990
dcc32b80ed2508d7001dd8893a20d864429dfd26	comparing gesture recognition accuracy using color and depth information	moving object;signed digit;sign language recognition;human computer interaction;smart home;dynamic program;kinect;computer vision;real world application;dtw;virtual reality environment;dynamic time warping;gesture recognition;scale invariance	In human-computer interaction applications, gesture recognition has the potential to provide a natural way of communication between humans and machines. The technology is becoming mature enough to be widely available to the public and real-world computer vision applications start to emerge. A typical example of this trend is the gaming industry and the launch of Microsoft's new camera: the Kinect. Other domains, where gesture recognition is needed, include but are not limited to: sign language recognition, virtual reality environments and smart homes. A key challenge for such real-world applications is that they need to operate in complex scenes with cluttered backgrounds, various moving objects and possibly challenging illumination conditions. In this paper we propose a method that accommodates such challenging conditions by detecting the hands using scene depth information from the Kinect. On top of our detector we employ a dynamic programming method for recognizing gestures, namely Dynamic Time Warping (DTW). Our method is translation and scale invariant which is a desirable property for many HCI systems. We have tested the performance of our approach on a digits recognition system. All experimental datasets include hand signed digits gestures but our framework can be generalized to recognize a wider range of gestures.	computer vision;dynamic programming;dynamic time warping;gesture recognition;human–computer interaction;kinect;sensor;virtual reality	Paul Doliotis;Alexandra Stefan;Christopher McMurrough;David Eckhard;Vassilis Athitsos	2011		10.1145/2141622.2141647	computer vision;speech recognition;computer science;scale invariance;dynamic time warping;gesture recognition	Vision	26.931426053557164	-59.678908579709024	112053
9f11d78c5d1f854364fc6eb7f86a917c196d2f86	a colour features-based methodology for variety recognition from bulk paddy images	image recognition;variety recognition;neural networks;paddy rice;classification;colour features;image acquisition;machine vision;feature extraction;bulk rice grains;agriculture;feature selection;bulk paddy images;bulk food grains	Abstract: The paper presents a methodology for recognition of varieties from bulk paddy sample images based on colour features extracted from different colour models such as RGB, HSV and YCbCr. The colour features used in the work are mean, range and variance. Feature set reduction is carried out based on the range of feature values and a reduced feature set consisting of seven significant colour features is adopted. A feed-forward neural network is used as classifier. The average recognition accuracy of 94.33% is achieved using the reduced seven colour features. The work finds application in developing a machine vision system in agriculture sciences wherein automation of recognition and classification of bulk food grains becomes possible.	application release automation;artificial neural network;feedforward neural network;machine vision;mean squared error;statistical classification	Basavaraj S. Anami;N. M. Naveen;N. G. Hanamaratti	2015	IJAIP	10.1504/IJAIP.2015.070776	computer vision;agriculture;machine vision;feature extraction;computer science;machine learning;pattern recognition;feature selection;artificial neural network	Vision	30.9739843065408	-55.977738339970756	112085
c718bdd75678a13aa2c279ea56b5a17786e5217c	human skin detection through correlation rules between the ycb and ycr subspaces based on dynamic color clustering		This paper presents a novel rule-based skin detection method that works in the YCbCr color space. The method is based on correlation rules that evaluate the combinations of chrominance values to identify the skin pixels in the YCb and YCr subspaces. The correlation rules depend on the shape and size of dynamically generated skin color clusters, which are computed on a statistical basis in the YCb and YCr subspaces for each single image, and represent the areas that include most of the candidate skin pixels. Comparisons with six well-known rule-based methods in literature carried out on four publicly available databases show that the proposed method outperforms the others in terms of quantitative performance evaluation parameters. Moreover, the qualitative analysis shows that the method achieves satisfactory results also in critical scenarios, including severe variations in illumination conditions.	cluster analysis	Nadia Brancati;Giuseppe De Pietro;Maria Frucci;Luigi Gallo	2017	Computer Vision and Image Understanding	10.1016/j.cviu.2016.12.001	computer vision;machine learning;data mining;mathematics	Vision	36.4400317844088	-57.95671472237027	112110
a33053d3abe7502b9eea29b2bac6747aeed95a8a	text localization in natural scene images by mean-shift clustering and parallel edge feature	mean shift clustering;performance evaluation;image database;mean shift;parallel edge feature;text localization;connected component;natural scenes	A new text localization method using the parallel edge feature of text strokes is proposed, based on the observation that text-stroke consists of two edges in parallel. First, mean-shift clustering is employed to group similar pixels into clusters. The connected components in each cluster are considered as candidates for text strokes. Then, parallel edges are detected to verify whether the connected components are text strokes. The contribution of this paper is the presentation of a new feature of parallel edges along the stroke, providing structural information for the text localization. The performance, evaluated on ICDAR2003 image database, shows that the proposed algorithm works successfully with most of the text images.	algorithm;cluster analysis;connected component (graph theory);mean shift;multiple edges;pixel	Huy Le Phat;Toan Nguyen Dinh;Sang-Cheol Park;Gueesang Lee	2011		10.1145/1968613.1968749	computer vision;mean-shift;computer science;machine learning;pattern recognition	Vision	37.05828091432852	-64.38942846190245	112249
e44c36f219d6f7f36890b01e5942bd2a2518d0fa	visual-based view-invariant human motion analysis: a review	pose representation and estimate;view invariant;electronic and computer engineering;human motion;human motion analysis;behaviour understanding	This paper provides a comprehensive survey of research on view-invariant human motion analysis. Recent research has shown that view-invariant related issues has been one of the bottlenecks for human motion understanding. The priority in this paper has been given to view-invariant pose representation and estimation, behaviour understanding. Research challenges and future directions are discussed in the end.		Xiaofei Ji;Honghai Liu;Yibo Li;David J. Brown	2008		10.1007/978-3-540-85563-7_93	computer vision;simulation;computer science;artificial intelligence	Vision	24.672411061086915	-58.636019996315156	112443
b6774026da9b6369b819fa158ffa85f3a04fe5b5	texture segmentation using siamese network and hierarchical region merging		This paper proposes an texture segmentation algorithm. In the proposed texture segmentation algorithm, the feature vectors at each pixel of an input image are extracted by using the deep neural networks such as the deep convolutional network (CNN) or the Siamese Network. Then they are used as input of the hierarchical region merging. Unlike the semantic segmentation such as fully connected network (FCN) or U-Net which are based on the supervised learning, the proposed algorithm can correctly segment the texture regions whose texture is taken from the other types of the texture. The effectiveness of the proposed texture segmentation algorithm is experimentally confirmed by using the famous texture images taken from book by P. Brodatz.		Ryusuke Yamada;Hidenori Ide;Novanto Yudistira;Takio Kurita	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8545348	supervised learning;pixel;computer vision;image segmentation;artificial intelligence;cluster analysis;feature vector;feature extraction;artificial neural network;segmentation;pattern recognition;computer science	Vision	28.78979411127759	-55.28585063912468	112473
1394683ae94332ca8d2ae8a81fe940e712120ac0	an efficient pose invariant face recognition system		This paper proposes an efficient face recognition system which is invariant to pose. It presents a transformation to generate features of the frontal face from a given posed image of a subject. The proposed system has been tested on three databases viz. IITK, FERET and CMU-PIE. It has been observed that it performs better than the existing well known system.	database;feret (facial recognition technology);facial recognition system;feature vector;pattern recognition;pose (computer vision);principal component analysis;viz: the computer game	Jeet Kumar;Aditya Nigam;Surya Prakash;Phalguni Gupta	2011		10.1007/978-81-322-0491-6_14	3d pose estimation;three-dimensional face recognition;3d single-object recognition	Vision	32.88141079492009	-58.222243975953646	112490
2de33637fae84ca94d89b3b3d681670191c6281d	robust face pseudo-sketch synthesis and recognition using morphological-arithmetic operations and hog-pca		In this paper, we propose a simple but yet effective method for synthesizing a pseudo face sketch (pseudo-sketch) from a photo, to be used for face recognition based on sketches drawn by a forensic artist. In contrast to current methods, the proposed method does not require training samples while fairly maintains the salient facial features as the artist do. We also propose a matching method on the basis of the Histograms of Oriented Gradients (HOG) descriptor and Principal Component Analysis (PCA), called HOG-PCA, to handle the similarities between a forensic sketch and a synthesized pseudo-sketch. In this method, we first extract the HOG features for the sketch and pseudo-sketch at regular grid and overlapped patches. The PCA is then applied to address the redundancy in feature representation due to several overlapped patches. Finally, the Nearest Neighbors Classifier (NNC) with the cosine distance is used to classify the sketch and pseudo-sketch pairs as matched or mismatched. Experimental results on CUHK and AR face sketch databases demonstrate that our proposed methods outperform state-of-the-art methods.	ar (unix);cosine similarity;database;effective method;experiment;facial recognition system;image gradient;mathematical morphology;principal component analysis;redundancy (engineering);regular grid;simulation;sketch-based modeling;thresholding (image processing);uncontrolled format string	Abduljalil Radman;Shahrel Azmin Suandi	2018	Multimedia Tools and Applications	10.1007/s11042-018-5786-y	computer science;computer vision;redundancy (engineering);salient;artificial intelligence;principal component analysis;sketch;histogram;facial recognition system;pattern recognition;regular grid;classifier (linguistics)	Vision	36.66858002872393	-58.95523357222035	112534
d4d3047384eed8157be93bd743941dc38bff013b	a hybrid approach to face detection under unconstrained environments	image preprocessing;skin detection;color based method;skin region;skin;hybrid approach;skin face recognition feature extraction image colour analysis;face recognition;image colour analysis;feature extraction;skin detection face detection unconstrained environments color based method gray scale based method image preprocessing skin modeling skin region;face detection;face detection skin facial features mouth lighting robustness image segmentation machine vision shape eyes;unconstrained environments;gray scale based method;skin modeling;exhaustive search	To detect faces in natural and unconstrained environments, we propose an approach which combines the advantages of both color and gray scale based methods. The idea consists of first preprocessing the images using a state-of-the-art approach for skin modeling in order to determine the potential skin regions. Thus, a scanning of the whole image when searching for faces is avoided. Then, in contrast to the existing methods, we consider the fact that the skin detection step still may produce unsatisfactory results or even fail and therefore we apply an exhaustive search in and around the detected skin regions using a new gray scale based approach. The experimental results show that the proposed approach inherits the speed from the color based methods and the efficiency from the gray scale based ones	academy;belief propagation;brute-force search;color;face detection;grayscale;linear discriminant analysis;preprocessor	Abdenour Hadid;Matti Pietikäinen	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.80	facial recognition system;computer vision;face detection;speech recognition;feature extraction;computer science;pattern recognition;brute-force search;skin	Vision	34.100916538377135	-62.05601895429423	112648
0e55bb95f0347b9ac745da72729016c50347cf19	dynamic gesture recognition with wi-fi based on signal processing and machine learning	discrete wavelet transforms;support vector machine svm wi fi dynamic gesture recognition signal processing machine learning discrete wavelet transform dwt dynamic time warping dtw;support vector machines;decision support systems conferences information processing discrete wavelet transforms support vector machines gesture recognition signal processing;wireless lan discrete wavelet transforms gesture recognition learning artificial intelligence signal processing support vector machines ubiquitous computing;dynamic gesture recognition dtw dynamic time warping algorithm svm support vector machine dwt discrete wavelet transform hand gestures modern communication system information carriers wi fi signals machine learning signal processing;signal processing;decision support systems;information processing;gesture recognition;conferences	"""Wi-Fi signals have been typically acting as information carriers in modern communication system, but recent research has revealed their powerful capability in detecting and identifying various targets. With Wi-Fi, we can now """"see"""" people's location, activity, and even hand gestures. In this paper, a new method of dynamic gesture recognition using Wi-Fi based on signal processing and machine learning is proposed. In our work, power profiles of received Wi-Fi signals are acquired for signal processing. The discrete wavelet transform (DWT) is applied to extract features and eliminate noise. And a support vector machine (SVM) improved by dynamic time warping (DTW) algorithm is built to classify and recognize different gestures. The experimental result shows that, by applying the method, nine predefined dynamic gestures can be effectively recognized, with an average recognition rate up to 94.8%, using only a small amount of training samples."""	algorithm;discrete wavelet transform;dynamic time warping;gesture recognition;image noise;machine learning;sensor;signal processing;support vector machine	Ge Zhou;Ting Jiang;Yue Liu;Wei Liu	2015	2015 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2015.7418290	speech recognition;computer science;machine learning;pattern recognition	Mobile	25.86485365646015	-65.16668292590917	112802
3d074d711af6efcb85d1d62e8bc85c09465b8b10	on matching altered fingerprints	databases;image matching;skin;image restoration;distortion;background fingerprint database altered fingerprint matching person finger ridge structure abrading cutting plastic surgery automated fingerprint identification systems afis fingerprint impressions genuine minutiae loss spurious minutiae minutiae spatial distribution distortion fingerprint matcher minutiae structure restoration z cut skin distortion thin plate spline tps;fingerprint recognition;law enforcement;skin distortion fingerprint identification image matching image restoration;switches;friction;skin fingerprint recognition image restoration databases law enforcement friction switches;fingerprint identification	Fingerprint alteration refers to changes made in a person's finger ridge structure by means of abrading, cutting, or performing plastic surgery on the fingertips. Fingerprint alteration is a serious attack on Automated Fingerprint Identification Systems (AFIS) since it can reduce the similarity between fingerprint impressions from the same finger due to the loss of genuine minutiae, increase in spurious minutiae and distortion in spatial distribution of the minutiae. We investigate the capability of a state-of-the-art commercial fingerprint matcher to match altered fingerprints to their pre-altered mates by removing spurious minutiae in the altered region. We also attempt to restore minutiae structure in a well-known type of fingerprint alteration, called `Z'-cut, which tears local skin patches of a finger and switches them with each other. Nonrigid skin distortion introduced during switching of local patches is modeled by thin-plate spline (TPS). Experimental results show that removing spurious minutiae in the altered region and relocating the minutiae in the restored local patches improve the matching performance and help to retrieve correct mates from a large background fingerprint database.	automated fingerprint identification;distortion;fingerprint recognition;minutiae;network switch;thin plate spline	Soweon Yoon;Qijun Zhao;Anil K. Jain	2012	2012 5th IAPR International Conference on Biometrics (ICB)	10.1109/ICB.2012.6199812	image restoration;fingerprint;computer vision;speech recognition;distortion;network switch;computer science;friction;pattern recognition;skin;fingerprint recognition	Vision	32.29141089655685	-62.9558604429964	113041
70d72b6c84b7de97ef978faa737e2b48c648bad0	multi-modal classification in digital news libraries	multi-modal classification;feature vector;multimodal classification;digital news library;digital libraries;multimodal video classification;video content;multimodal information;video classification;content management;fisher linear discriminant;feature extraction;image classification;robust multi-modal video classification;image retrieval;information resources;trecvid news video archive;comprehensive approach;broadcast news;discriminative feature;feature selection;digital source;video databases;algorithms;layout;robustness;digital video broadcasting;image segmentation;support vector machines;computer science	This paper describes a comprehensive approach to construct robust multimodal video classification on a specific digital source, broadcast news. Broadcast news has a very stable structure and every segment has its specific purpose. Video classification can support fundamental understanding of the structure of the video and the content. The variety of video content makes it hard to classify; however, it also provides multimodal information. Our approach tries to solve two important issues of multimodal classification. The first one is to select few discriminative features from many raw features and the second one is to efficiently combine multiple sources. We applied Fisher's Linear Discriminant (FLD) for feature selection and concatenated the projections into a single synthesized feature vector as the combination strategy. Experimental results on the 2003 TRECVID news video archive show that our approach achieves very robust and accurate performance.	library (computing);modal logic	Ming-yu Chen;Alexander G. Hauptmann	2004		10.1109/JCDL.2004.1336122	layout;support vector machine;computer vision;contextual image classification;digital library;feature vector;feature extraction;content management;image retrieval;computer science;multimedia;image segmentation;feature selection;digital video broadcasting;information retrieval;robustness	PL	34.59252484972594	-52.90008157883806	113217
aef59def2a65901de9d520d0442b42bb4a448f06	facial expression recognition	optical flow;hopfield network;facial expression;frequency analysis;image processing;feature vector;neural network;feature extraction;learning artificial intelligence	1. Locating faces in the scene (e.g., in an image; this step is also referred to as face detection), 2. Extracting facial features from the detected face region (e.g., detecting the shape of facial components or describing the texture of the skin in a facial area; this step is referred to as facial feature extraction), 3. Analyzing the motion of facial features and/or the changes in the appearance of facial features and classifying this information into some facialexpression-interpretative categories such as facial muscle activations like smile or frown, emotion (affect) categories like happiness or anger, attitude categories like (dis)liking or ambivalence, etc. (this step is also referred to as facial expression interpretation).	face detection;facial recognition system;feature extraction;optical character recognition;sensor	Maja Pantic	2009		10.1007/978-0-387-73003-5_98	grayscale;three-dimensional face recognition;feature extraction;facial expression;face hallucination;computer vision;psychology;artificial intelligence;pattern recognition	Vision	29.849335869220212	-59.21393609350856	113289
a978b2a84955b48a0c82d4773e795aafce05068d	radial fourier analysis (rfa) image descriptor	lighting fourier transforms frequency domain analysis performance evaluation image coding histograms spectral analysis;spectral analysis data compression fourier analysis fourier transforms frequency domain analysis image coding image representation;local image patch spectral analysis spectrum based local image descriptor radial fourier analysis image descriptor rfa image descriptor fourier transform image gradient spatial domain frequency domain transformed gradient frequency low frequency fourier coefficients keypoint representation image blurring jpg compression image description	This article presents a spectrum-based local image descriptor, namely, Radial Fourier Analysis (RFA) image descriptor. The RFA descriptor uses Fourier transform to convert the image gradients in the local region of a keypoint from spatial domain to frequency domain. The transformed gradient frequencies are then analysed to obtain the principle description within the local region. The principle description is represented by low frequency Fourier coefficients and they are directly used in the descriptor to represent the keypoint. Experimental results using a series of performance evaluation procedures showed that RFA descriptor demonstrates higher performances against different image variations comparing to benchmark local image descriptors. The results also indicated that the RFA descriptor is particularly reliable when used on the images, which are degraded by blurring and JPG compression. According to the performance evaluations presented in this paper, spectral analysis shows strong potential for local image description. It paves the way for future research in alternative spectrum-based techniques such as Wavelet transform to precisely analyse local image patches.	benchmark (computing);coefficient;computation;distortion;fourier analysis;gaussian blur;image gradient;jpeg;performance evaluation;radial (radio);scale-invariant feature transform;spectral density estimation;time complexity;visual descriptor;wavelet transform	Stephen Ching-Feng Lin;Chin Yeow Wong;Guannan Jiang;Md. Arifur Rahman;Ngai Ming Kwok	2014	2014 11th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2014.6980942	computer vision;mathematical optimization;discrete-time fourier transform;speech recognition;discrete fourier transform;mathematics;spectral density estimation;fourier analysis;least-squares spectral analysis;non-uniform discrete fourier transform;phase correlation	Vision	37.986017801289584	-59.446226517924856	113424
b132843fa131c257cf3156698ee7d9dbdc3c87ad	machine learning for multi-view eye-pair detection	eye detection;restricted boltzmann machine rbm;location;eye pair detection;machine learning;support vector machine;face detection	While face and eye detection is well known research topics in the field of object detection, eye-pair detection has not been much researched. Finding the location and size of an eye-pair in an image containing a face can enable a face recognition application to extract features from a face corresponding to different entities. Furthermore, it allows us to align different faces, so that more accurate recognition results can be obtained. To the best of our knowledge, currently there is only one eye-pair detector, which is a part of the Viola-Jones object detection framework. However, as we will show in this paper, this eye-pair detector is not very accurate for detecting eye-pairs from different face images. Therefore, in this paper we describe several novel eye-pair detection methods based on different feature extraction methods and a support vector machine (SVM) to classify image patches as containing an eye-pair or not. To find the location of an eye-pair on unseen test images, a sliding window approach is used, and the location and size of the window giving the highest output of the SVM classifier are returned. We have tested the different methods on three different datasets: the IMM, the Caltech and the Indian face dataset. The results show that the linear restricted Boltzmann machine feature extraction technique and principal component analysis result in the best performances. The SVM with these feature extraction methods is able to very accurately detect eye-pairs. Furthermore, the results show that our best eye-pair detection methods perform much better than the Viola-Jones eye-pair detector. (C) 2014 Elsevier Ltd. All rights reserved.		Mahir Faik Karaaba;Lambert Schomaker;Marco Wiering	2014	Eng. Appl. of AI	10.1016/j.engappai.2014.04.008	support vector machine;computer vision;face detection;object-class detection;computer science;viola–jones object detection framework;machine learning;pattern recognition;location	AI	32.08491035246846	-52.974069910972084	113448
718e3c08d3821b69a22dfd95384d090574c66705	fast and robust face detection and tracking framework	test video sequence robust face detection face tracking framework neural network classifiers kalman filter;face face detection kalman filters tracking biological neural networks support vector machines;kalman filter face detection face tracking combined cascade of neural network classifiers convolutional neural network;neural nets;pattern classification face recognition image sequences kalman filters neural nets;kalman filters;face recognition;pattern classification;image sequences	Face detection and tracking framework is described in the paper. Face detection is based on combined cascade of neural network-classifiers. Tracking is performed using Kalman filter. The framework was experimentally researched on a test video sequence and adjusted to obtain high processing speed.	algorithm;artificial neural network;biometrics;cuda;experiment;face detection;kalman filter;message passing interface	Ihor Paliy;Volodymyr Dovgan;Ognian Boumbarov;Stanislav Panev;Anatoly Sachenko;Yuriy Kurylyak;Diana Zagorodnya	2011	Proceedings of the 6th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems	10.1109/IDAACS.2011.6072790	facial recognition system;kalman filter;computer vision;computer science;machine learning;pattern recognition;artificial neural network	Robotics	32.0348328239538	-55.440843429231506	113505
a2ee05054259de6f19e7ac73544081ac24c91e49	fast image gradients using binary feature convolutions		The recent increase in popularity of binary feature descriptors has opened the door to new lightweight computer vision applications. Most research efforts thus far have been dedicated to the introduction of new large-scale binary features, which are primarily used for keypoint description and matching. In this paper, we show that the side products of small-scale binary feature computations can efficiently filter images and estimate image gradients. The improved efficiency of low-level operations can be especially useful in time-constrained applications. Through our experiments, we show that efficient binary feature convolutions can be used to mimic various image processing operations, and even outperform Sobel gradient estimation in the edge detection problem, both in terms of speed and F-Measure.	16-bit;3d computer graphics;algorithmic efficiency;canny edge detector;computation;computer vision;convolution;edge detection;experiment;feature vector;high- and low-level;image gradient;image processing;integer (computer science);map;medical imaging;pyramid (image processing);relational operator;simd;sobel operator	Pierre-Luc St-Charles;Guillaume-Alexandre Bilodeau;Robert Bergevin	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2016.138	computer vision;feature detection;computer science;machine learning;pattern recognition;feature	Vision	31.12559154294266	-54.8968540595003	113845
7c040e77966a9240b5b495350b82f6f73053962e	a modified technique for face recognition under degraded conditions		Abstract In this paper an improved face recognition algorithm under degrading conditions is proposed. The proposed algorithm uses a combination of preprocessing techniques coupled with discriminative feature extractors to obtain the best distinctive features for classification. Preprocessing approach is the fusion of multi-scale Weber and enhanced complex wavelet transform. Combination of multiple feature extraction based on Gabor filters, block-based local phase quantization (LPQ) coupled with principal component analysis (PCA) proved to be very effective to improve correct rate of recognition. We have also used two known classifiers, extreme learning machine (ELM), and sparse classifier (SC), and fused their outputs to obtain best recognition rate. Experimental results show improved performance of proposed algorithm under poor illumination, partial occlusion and low-quality images in uncontrolled conditions. Our best recognition results using second version of face recognition grand challenge (FRGC 2.0.4) which is the most challenging database, indicated more than 28% improvement over previous works.	facial recognition system	Soodeh Nikan;Majid Ahmadi	2018	J. Visual Communication and Image Representation	10.1016/j.jvcir.2018.08.007	artificial intelligence;mathematics;discriminative model;face recognition grand challenge;pattern recognition;principal component analysis;complex wavelet transform;feature extraction;quantization (signal processing);extreme learning machine;facial recognition system	Vision	33.792095503982786	-58.594685394436794	114256
31934e46d26d7503d75a63ded1d2a09c452bdafa	shrec 2011: robust feature detection and description benchmark	feature detection;3d object retrieval;psi_mic;shape matching;pattern recognition;shape descriptors;image analysis;shape retrieval;categories and subject descriptors according to acm ccs h 3 2 information storage and retrieval information search and retrieval retrieval models i 2 10 artificial intelligence vision and scene understanding shape	Feature-based approaches have recently become very popular in computer vision and image analysis applications, and are becoming a promising direction in shape retrieval. SHREC’11 robust feature detection and description benchmark simulates the feature detection and description stages of feature-based shape retrieval algorithms. The benchmark tests the performance of shape feature detectors and descriptors under a wide variety of transformations. The benchmark allows evaluating how algorithms cope with certain classes of transformations and strength of the transformations that can be dealt with. The present paper is a report of the SHREC’11 robust feature detection and description benchmark results.	algorithm;benchmark (computing);computer vision;feature detection (computer vision);feature detection (web development);image analysis;sensor	Edmond Boyer;Alexander M. Bronstein;Michael M. Bronstein;Benjamin Bustos;Tal Darom;Radu Horaud;Ingrid Hotz;Yosi Keller;Johannes Keustermans;Artiom Kovnatsky;Roee Litman;Jan Reininghaus;Ivan Sipiran;Dirk Smeets;Paul Suetens;Dirk Vandermeulen;Andrei Zaharescu;Valentin Zobel	2011		10.2312/3DOR/3DOR11/071-078	computer vision;feature detection;visual word;image analysis;computer science;machine learning;pattern recognition;feature detection;feature	Vision	38.032291906841564	-58.11889284954266	114265
b91a18f1453302feac0b1487b63fd167a7ec287a	a nearest neighbor approach to letter recognition	camera control;collision detection;nearest neighbor;nearest neighbor classifier	The nearest neighbor classifier (NNC) is a non-parametric classification technique that classifies a test pattern to the class of its nearest neighbor in the training data. In this research, we applied the NNC to a standard letter recognition data and obtained a superior classification rate in comparison to extant approaches. A prime drawback to the NNC technique has been the relative inefficiency of the model. A modified NNC was implemented and applied to the same recognition problem. It is found that if we choose a suitable threshold minimum difference for classification, we can reduce the CPU time by half without lowering the performance of the classifier.	central processing unit;k-nearest neighbors algorithm;nearest neighbour algorithm;test card	Aiyuan Ji;Roy George	2006		10.1145/1185448.1185631	nearest-neighbor chain algorithm;large margin nearest neighbor;nearest neighbor graph;best bin first;speech recognition;computer science;machine learning;pattern recognition;nearest neighbor search	AI	32.52544317607074	-64.48393768398634	114407
10c79df4f44b5e4c08f984f34370d292f31ef309	multi-modal 2d and 3d biometrics for face recognition	biometrics access control;statistical significance;biometrics face recognition probes humans lighting facial features testing gabor filters support vector machines support vector machine classification;face recognition;image acquisition;weighted sums;principal component analysis;statistical analysis multimodal 2d biometrics multimodal 3d biometrics face recognition 2d face data 3d face data biometric recognition gallery image acquisition probe image acquisition pca based approach;sensor fusion;sensor fusion face recognition biometrics access control principal component analysis	Results are presented for the largest experimental study to date that investigates the comparison and combination of 2D and 3D face data for biometric recognition. To our knowledge, this is also the only such study to incorporate significant time lapse between gallery and probe image acquisition. Recognition results are presented for gallery and probe datasets of 166 subjects imaged in both 2D and 3D, with six to thirteen weeks time lapse between gallery and probe images of a given subject. Using a PCA-based approach tuned separately for 2D and for 3D, we find no statistically significant difference between the rank-one recognition rates of 83.1% for 2D and 83.7% for 3D. Using a certainty-weighted sum-of-distance approach to combining 2D and 3D, we find a multi-modal rank-one recognition rate of 92.8%, which is statistically significantly greater than either 2D or 3D alone.	biometrics;experiment;facial recognition system;modal logic;weight function	Kyong I. Chang;Kevin W. Bowyer;Patrick J. Flynn	2003		10.1109/AMFG.2003.1240842	facial recognition system;computer vision;face detection;speech recognition;computer science;machine learning;pattern recognition;three-dimensional face recognition;sensor fusion;statistical significance;principal component analysis	Vision	31.87045650074206	-59.443145230508314	114780
6af65e2a1eba6bd62843e7bf717b4ccc91bce2b8	a new weighted sparse representation based on mslbp and its application to face recognition	face recognition;feature extraction;multi-scale lbp feature;sparse representation-based classification (src);weighted sparse representation	Face recognition via sparse representation-based classification has received more and more attention in recent years. This approach has achieved state-of-the-art results, which outperforms traditional methods, especially when face image pixels are corrupted or occluded. In this paper, we propose a new weighted sparse representation method called WSRC-MSLBP which utilizes the multi-scale LBP (MSLBP) feature to measure similarity between face images, and to form the weight matrix. The proposed WSRC-MSLBP method not only represents the test sample as a sparse linear combination of all the training samples, but also makes use of locality of local binary pattern. Experimental results on publicly available databases show that the proposed WSRC-MSLBP method is more effective than sparse representation-based classification algorithm and the original weighted sparse representation method.	algorithm;binary pattern (image generation);database;facial recognition system;local binary patterns;locality of reference;pixel;sparse approximation;sparse matrix	He-Feng Yin;Xiaojun Wu	2013		10.1007/978-3-642-40705-5_10	computer vision;machine learning;pattern recognition;sparse approximation;mathematics	Vision	34.44438646319162	-57.487685937427635	114812
46cec2446c235e40646b9be344ef83f528aed3d2	combining multiple matchers for a high security fingerprint verification system	verification;fingerprint matching;combination of matchers;logistic transform;fingerprint verification	Integration of various ®ngerprint matching algorithms is a viable method to improve the performance of a ®ngerprint veri®cation system. Dierent ®ngerprint matching algorithms are often based on dierent representations of the input ®ngerprints and hence complement each other. We use the logistic transform to integrate the output scores from three dierent ®ngerprint matching algorithms. Experiments conducted on a large ®ngerprint database con®rm the eectiveness of the proposed integration scheme. Ó 1999 Published by Elsevier Science B.V. All rights reserved.	algorithm;fingerprint;naruto shippuden: clash of ninja revolution 3	Anil K. Jain;Salil Prabhakar;Shaoyun Chen	1999	Pattern Recognition Letters	10.1016/S0167-8655(99)00108-7	fingerprint verification competition;verification;computer science;theoretical computer science;machine learning;pattern recognition	AI	32.928332857777825	-61.314952298920666	114836
44dc961944ac2d011b9d99e9324e2a9ce6ed6668	a sparse linear model for saliency-guided decolorization	monochrome imaging sparse linear model saliency guided decolorization image feature preservation input color space feature space color systems image dependent feature space representative dictionary learning chromatic redundancy reduction linear projection;image colour analysis;feature extraction;learning artificial intelligence feature extraction image colour analysis;learning artificial intelligence;image color analysis gray scale color dictionaries principal component analysis visualization image edge detection;saliency decolorization sparse model	Different from most existing decolorization techniques that emphasize preserving image features revealed in the input color space, our proposed method focuses on exploring those in a higher-dimensional feature space. The shift of paradigm is motivated by that decolorization is often sensitive to adopting the various color systems. The results of converting the same color image expressed in different color spaces could vary significantly. We instead consider constructing an image-dependent feature space by learning a representative dictionary, and carry out decolorizing an image by retaining the structures there. To this end, for a given image, the atoms of the dictionary are systematically collected to reflect the visually important/salient contents, and also to concisely reduce chromatic redundancy. A sparse linear model with respect to the learned dictionary is then assumed. Finally, a linear projection to grayscale respecting the inner products in the feature space can be optimized to accomplish the conversion.	color image;color space;dictionary;feature vector;grayscale;linear model;programming paradigm;redundancy (information theory);sparse matrix	C T Liu;Tyng-Luh Liu	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738228	color histogram;computer vision;feature detection;color image;feature extraction;computer science;machine learning;pattern recognition;mathematics;color balance;feature	Vision	36.39988502530795	-53.927526978093745	114857
cb8b4dcf3194d5424e4765d745a5fc9da0d2eacf	data-driven representation learning in multimodal feature fusion			machine learning;multimodal interaction	Huan Song	2018				AI	29.040269028651377	-57.345019832308296	115099
361697dcf11076bb792571ce62aa17dd5fa97edf	detection of facial components based on svm classification and invariant feature	image segmentation;support vector machines;support vector machine svm;image fusion;image classification;support vector machines face recognition feature extraction image classification image colour analysis image fusion image segmentation object detection radial basis function networks;classification;cascaded classifiers;radial basis function networks;face support vector machines kernel nose feature extraction facial features;face recognition;deformation;face deformation facial components detection svm classification support vector machines invariant feature facial features determination personal identification pattern recognition skin color information data input space feature space nonparametric model fusion pixel segmentation rbf kernel radial basis function kernel facial features location;image colour analysis;feature extraction;face detection;object detection;invariant feature	"""Facial features determination is essential in many applications such as personal identification several approaches have been proposed, but an effective method for face detection is still a research problem. In this paper we focus on a recent method called the support vector machines (SVM) has been adapted and applied to the problem of pattern recognition such as face detection. The idea information of the skin color is used to reduce the search region and the main idea based on SVM is to project the data input space (belonging to two different classes) non-linearly separable in a larger space called feature space so that data are linearly separable. About fusion a non-parametric model is applied for the segmentation of the pixels of skin color. This last is used to reduce area of research within the image. However the SVMs help us to find exactly the faces in the segmented area. We implemented the SVM using a RBF kernel as a classification technique for face detection by block"""" approach of considering the face as a set of components (eyes, nose and mouth). The method succeeds in locating facial features in the facial region exactly and is insensitive to face deformation. The method is executable in a reasonably short time."""	effective method;executable;face detection;feature vector;linear separability;parametric model;pattern recognition;pixel;radial basis function kernel;support vector machine	Rachid Aliradi;Naima Bouzera;Abdelkrim Meziane;Abdelkader Belkhir	2013	2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)	10.1109/WI-IAT.2013.212	computer vision;machine learning;pattern recognition;mathematics	Vision	33.854513068636315	-60.69784958007242	115277
daaeae5b1094dbaf043ecd6f46a7d9487782f292	research on the handwriting character recognition technology based on the image statistical characteristics		The research on the image pattern recognition has always been a hot topic. In this paper, the automatic identification technology of image is studied, the research contents include image preprocessing, image feature extraction and image content identification. BP neural network is used for the research on the image content identification. The processing methods in this article include the following steps, first, the pre-processing of the image. Including image de-noising and feature extraction; second, training the BP neural network with the processed handwriting character image; third, the recognition test of the unknown handwritten character. 95% recognition accuracy is realized, and the research has some practical application value.		Yongfeng Sun;Zhonghua Guo;Weijiang Qiu	2017		10.1007/978-981-13-0896-3_2	artificial neural network;feature extraction;computer vision;handwriting;artificial intelligence;computer science	Vision	32.874536593471085	-65.65069896278114	115393
c3515bc7df7f2aa95f89812202d56cba9ed56397	an experimental tattoo de-identification system for privacy protection in still images	sift tattoo de identification privacy protection;transforms data privacy feature extraction image colour analysis image matching image motion analysis image restoration image texture object detection security of data;scale invariant feature transform experimental tattoo deidentification system privacy protection still images skin detection region of interest detection feature extraction tattoo database tattoo matching tattoo detection skin swapping quality evaluation tattoo localization ad hoc method skin colour feature texture feature sift feature motion blur distance size illumination tattoo appearance removal;skin image color analysis privacy databases feature extraction face visualization	An experimental tattoo de-identification system for privacy protection in still images is described in the paper. The system consists of the following modules: skin detection, region of interest detection, feature extraction, tattoo database, matching, tattoo detection, skin swapping, and quality evaluation. Two methods for tattoo localization are presented. The first is a simple ad-hoc method based only on skin colour. The second is based on skin colour, texture and SIFT features. The appearance of each tattoo area is de-identified in such a way that its skin colour and skin texture are similar to the surrounding skin area. Experimental results for still images in which tattoo location, distance, size, illumination, and motion blur have large variability are presented. The system is subjectively evaluated based on the results of tattoo localization, the level of privacy protection and the naturalness of the de-identified still images. The level of privacy protection is estimated based on the quality of the removal of the tattoo appearance and the concealment of its location.	de-identification;experiment;experimental system;feature extraction;gaussian blur;gradient;hoc (programming language);inpainting;internationalization and localization;paging;privacy;region of interest;scale-invariant feature transform;spatial variability	Darijan Marcetic;Slobodan Ribaric;Vitomir Struc;Nikola Pavesic	2014	2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.1109/MIPRO.2014.6859766	computer vision;multimedia;internet privacy	Vision	32.76599739493159	-62.599040111301086	115463
2d43794828d94a1d5cd756de2bf4e2df6f90641a	a feature point clustering approach to the segmentation of form documents	form optical character recognition;image storage;pattern clustering;character extraction;image segmentation;form document segmentation;optical character recognition;data mining;optical character recognition software;feature point clustering approach;feature extraction;document image processing;character recognition feature extraction optical character recognition software data mining image segmentation clustering algorithms pattern clustering image storage optical devices storage automation;clustering algorithms;image segmentation optical character recognition feature extraction document image processing;pattern clustering problem form document segmentation feature point clustering approach form optical character recognition character extraction;storage automation;character recognition;pattern clustering problem;optical devices	Among various kinds of documents, forms are one of the important types. The prerequisite for form optical character recognition (Form OCR) is the extraction of characters from form documents. The authors present a clustering based technique for extracting characters from form documents. In this method, they treat the character extraction process as a pattern clustering problem. The feasibility of the novel method is demonstrated through experimenting various kinds of forms. Experimental results reveal the feasibility of the novel method.	cluster analysis	Kuo-Chin Fan;Jeng-Ming Lu;Jiing-Yuh Wang	1995		10.1109/ICDAR.1995.601973	computer vision;feature extraction;computer science;machine learning;pattern recognition;image segmentation;cluster analysis;optical character recognition	NLP	35.3477325406557	-65.74719788910454	115539
5ea94b5e58c7c476c8d75b22743a3f98debe44cd	human object annotation for surveillance video forensics	surveillance;forensic science;video;article	A system that can automatically annotate surveillance video in a manner useful for locating a person with a given description of clothing is presented. Each human is annotated based on two appearance features: primary colors of clothes and the presence of text/logos on clothes. The annotation occurs after a robust foreground extraction stage employing a modified Gaussian mixture model-based approach. The proposed pipeline consists of a preprocessing stage where color appearance of an image is improved using a color constancy algorithm. In order to annotate color information for human clothes, we use the color histogram feature in HSV space and find local maxima to extract dominant colors for different parts of a segmented human object. To detect text/logos on clothes, we begin with the extraction of connected components of enhanced horizontal, vertical, and diagonal edges in the frames. These candidate regions are classified as text or nontext on the basis of their local energy-based shape histogram features. Further, to detect humans, a novel technique has been proposed that uses contourlet transform-based local binary pattern (CLBP) features. In the proposed method, we extract the uniform direction invariant LBP feature descriptor for contourlet transformed high-pass subimages from vertical and diagonal directional bands. In the final stage, extracted CLBP descriptors are classified by a trained support vector machine. Experimental results illustrate the superiority of our method on large-scale surveillance video data. © 2013 SPIE and IS&T [DOI: 10.1117/1.JEI.22.4.041115]	algorithm;binary pattern (image generation);closed-circuit television;color histogram;color quantization;color space;contourlet;image processing;local binary patterns;logo;maxima and minima;mixture model;preprocessor;support vector machine;visual descriptor	Muhammad Fraz;Iffat Zafar;Giounona Tzanidou;Eran A. Edirisinghe;M. Saquib Sarfraz	2013	J. Electronic Imaging	10.1117/1.JEI.22.4.041115	computer vision;speech recognition;video;forensic science;computer graphics (images)	Vision	36.56657842404361	-63.07575268478983	115566
80f5a474feb868df728e768bf605a4e64c36600a	a multi-task neural approach for emotion attribution, classification and summarization		Emotional content is a crucial ingredient in user-generated videos. However, the sparsely expressed emotions in the user-generated video cause difficulties to emotions analysis in videos. In this paper, we propose a new neural approach---Bi-stream Emotion Attribution-Classification Network (BEAC-Net) to solve three related emotion analysis tasks: emotion recognition, emotion attribution and emotion-oriented summarization, in an integrated framework. BEAC-Net has two major constituents, an attribution network and a classification network. The attribution network extracts the main emotional segment that classification should focus on in order to mitigate the sparsity problem. The classification network utilizes both the extracted segment and the original video in a bi-stream architecture. We contribute a new dataset for the emotion attribution task with human-annotated ground-truth labels for emotion segments. Experiments on two video datasets demonstrate superior performance of the proposed framework and the complementary nature of the dual classification streams.		G. P. Tu;Yanwei Fu;Boyang Li;Jiarui Gao;Yu-Gang Jiang;Xiangyang Xue	2018	CoRR			AI	25.810507329894417	-54.470893083994696	115699
36e682d97323aa5eda9ab134ccda478799c0ff05	web video data clustering and recognition using histograms of phoneme symbols	mahalanobis distance;histograms;web video data clustering;pattern clustering;histograms data mining video sharing principal component analysis speech processing videoconference information retrieval content based retrieval youtube image processing;principal component analysis information retrieval multimedia systems pattern clustering;multimedia data mining video content clustering phoneme recognition;information retrieval;k nearest neighbor method;data mining;multimedia systems;multimedia information retrieval;data clustering;accuracy;phoneme symbols histograms;web video content recognition;space use;three dimensional displays;principal component analysis;animation;mahalanobis distance web video data clustering web video data recognition phoneme symbols histograms web video content clustering web video content recognition multimedia information retrieval pca k nearest neighbor method;web video data recognition;phoneme recognition;k nearest neighbor;web video content clustering;correlation;video content clustering;pca;multimedia data mining	The clustering and recognition of Web video content play an important role in multimedia information retrieval. This paper proposes a method for both clustering and recognizing Web video content using a histogram of phoneme symbols (HoPS). HoPS contains information about speech and sound intervals. In this study, three experiments were conducted.The first experiment allocated HoPS feature of video intervals in a 3D space using PCA and quantification method IV (Q-IV). The second experiment applied the k-nearest neighbor (k-NN) method to analyze the difficulties in clustering. The third experiment recognized unknown video intervals by using the distance between HoPS of the query and a category average. The accuracy of the recognition results were 44.3% and 36.9% using the Mahalanobis distance and the correlation distance for the category average of training data, respectively.	cluster analysis;digital video;experiment;information retrieval;k-nearest neighbors algorithm;learning vector quantization;principal component analysis;serial vector format;support vector machine;video clip	Yuichi Yaguchi;Yusuke Sakai;Keisuke Yoshida;Ryuichi Oka	2009	2009 Ninth IEEE International Conference on Computer and Information Technology	10.1109/CIT.2009.34	speech recognition;computer science;machine learning;pattern recognition;information retrieval;statistics;principal component analysis	Vision	27.354436902739312	-65.12313002736369	115721
29b88b96322e77f7024e88cefba207d58d15acc0	data mining-based facial expressions recognition system		In this paper, we introduce a new facial-expression analysis system designed to automatically recognize facial expressions, able to manage facial-expression intensity variation as well as reducing the doubt and confusion between facial-expression classes. Our proposed approach introduces a new method to segment efficiently facial feature contours using Vector Field Convolution (VFC) technique. Relying on the detected con- tours, we extract facial feature points which go with facial-expression deformations. Then we have modeled a set of distances among the detected points to define prediction rules through data mining technique. An experimental study was conducted to evaluate the per- formance of our proposed solution under varying factors.	data mining;pattern recognition	Hazar Mliki;Nesrine Fourati;Mohamed Hammami;Hanêne Ben-Abdallah	2013		10.3233/978-1-61499-330-8-185	computer vision;computer science;artificial intelligence;data mining	Vision	35.83395477213013	-56.8213135298302	115909
236a1b66bc406aded6572cc784b8f4117f389293	lip signatures for automatic person recognition	g400 computer science;image segmentation;training token lip signatures automatic person recognition lip feature evaluation acoustic signal recognition accuracy optimum dynamic window length upper lip lower lip identification error rates single digit test;face recognition;feature extraction;error rate;lips acoustic testing spatial databases feature extraction error analysis facial features speaker recognition robustness authentication application software;image segmentation face recognition feature extraction	This paper evaluates lip features for person recognition, and compares performance with that of the acoustic signal. Recognition accuracy is found to be equivalent in the 2 domains, agreeing with the findings of Chibeluushi. The optimum dynamic window length for both acoustic and visual modalities is found to be about 100ms. Recognition performance of the upper lip is considerably better than the lower lip, achieving 15% and 35% identification error rates respectively, using a single digit test and training token.	acoustic cryptanalysis;signature;window function	John S. D. Mason;Jason Brand;Roland Auckenthaler;Farzin Deravi;Claude C. Chibelushi	1999		10.1109/MMSP.1999.793890	facial recognition system;computer vision;speech recognition;feature extraction;word error rate;computer science;pattern recognition;three-dimensional face recognition;image segmentation	Vision	32.965313674188465	-63.28962882522034	115958
e5127a0cec519026f70674a3d25b2a8b22649e0f	strategies of shape and color fusions for content based image retrieval	object representation;image features;pattern recognition;content based image retrieval;image retrieval	  The aim of this paper is to discuss a fusion of the two most popular image features - color and shape - in the aspect of content-based  image retrieval. It is clear that these representations have their own advantages and drawbacks. Our suggestion is to combine  them to achieve better results in various areas, e.g. pattern recognition, object representation, image retrieval, by using  optimal variants of particular descriptors (both, color and shape) and utilize them in the same time. To achieve such goal  we propose two general strategies (sequential and parallel) for joining elementary queries. They are used to construct a system,  where each image is being decomposed into regions, basing on shapes with some characteristic properties - color and its distribution.  In the paper we provide an analysis of this proposition as well as the initial results of application in Content Based Image  Retrieval problem. The original contribution of the presented work is related to the fusion of several shape and color descriptors  and joining them into parallel or sequential structures giving considerable improvements in content-based image retrieval.  The novelty is based on the fact that many existing methods (even complex ones) work in the same domain (shape or color),  while the proposed approach joins features from different areas.    	content-based image retrieval	Pawel Forczmanski;Dariusz Frejlichowski	2007		10.1007/978-3-540-75175-5_1	image texture;computer vision;feature detection;visual word;color image;binary image;image retrieval;computer science;pattern recognition;automatic image annotation;feature;information retrieval	Vision	38.28927641198927	-58.841140397630326	116152
45ee3446edfce581849748fa1625a9e4c99e73a6	ensemble of multi-view learning classifiers for cross-domain iris presentation attack detection		The adoption of large-scale iris recognition systems around the world has brought to light the importance of detecting presentation attack images (textured contact lenses and printouts). This work presents a new approach in iris Presentation Attack Detection (PAD), by exploring combinations of Convolutional Neural Networks (CNNs) and transformed input spaces through binarized statistical image features (BSIF). Our method combines lightweight CNNs to classify multiple BSIF views of the input image. Following explorations on complementary input spaces leading to more discriminative features to detect presentation attacks, we also propose an algorithm to select the best (and most discriminative) predictors for the task at hand. An ensemble of predictors makes use of their expected individual performances to aggregate their results into a final prediction. Results show that this technique improves on the current state of the art in iris PAD, outperforming the winner of LivDet-Iris 2017 competition both for intraand cross-dataset scenarios, and illustrating the very difficult nature of the cross-dataset scenario.	aggregate data;algorithm;complementarity theory;convolutional neural network;ensemble learning;experiment;facial recognition system;image fusion;iris recognition;performance;reduction (complexity);sensor	Andrey Kuehlkamp;Allan da Silva Pinto;Anderson Rocha;K. Bowyer;Adam Czajka	2018	CoRR	10.1109/TIFS.2018.2878542	discriminative model;task analysis;computer vision;convolutional neural network;feature (computer vision);artificial intelligence;feature extraction;pattern recognition;computer science;iris recognition;lens (optics)	Vision	25.19175546843227	-55.29685135072449	116297
90c1f068b5c9c8c0cf03f08e2eec5b38d3cc17d0	a comparison of deep learning with global features for gastrointestinal disease detection		This paper presents our approach for the 2017 Multimedia for Medicine Medico Task of the MediaEval 2017 Benchmark. We propose a system based on global features and deep neural networks, and preliminary results comparing the approaches are presented.	artificial neural network;benchmark (computing);deep learning	Konstantin Pogorelov;Michael Riegler;Pål Halvorsen;Carsten Griwodz;Thomas de Lange;Kristin Ranheim Randel;Sigrun Losada Eskeland;Duc-Tien Dang-Nguyen;Olga Ostroukhova;Mathias Lux;Concetto Spampinato	2017			image processing;deep learning;artificial neural network;machine learning;artificial intelligence;computer science	ML	26.141090388961608	-57.41186141128189	116403
c0d80603d845132cd482fbdfdd16ede334edd6f2	preliminary results of a user identification using haptic information	filtering;anthropometry;banking;user identification;handwriting recognition;neural networks;neural nets;authentication;information filtering;biometrics;haptic interfaces authentication artificial neural networks neurons fingerprint recognition biometrics filtering;haptics;haptic information;neural nets handwriting recognition haptic interfaces identification technology message authentication;handwritten signatures user identification haptic information;artificial neural networks;fingerprint recognition;identification;identification technology;success rate;speech recognition;neural networks authentication haptics biometrics identification handwritten signature;message authentication;neurons;virtual environment;haptic interfaces;information filters;handwritten signature;handwritten signatures;neural network	In this paper we explore the identification of handwritten signatures with haptic information. We enhance our previous work by considering two methods: 1) we filter the incoming haptics information in the handwritten signature, and 2) we further reevaluate the highly probable candidates to enhance the identification success rate. In this paper we reach an identification success rate of 87%.	algorithm;authentication;customer relationship management;electronic signature;experiment;haptic technology;online and offline;open research;pattern recognition	Fawaz A. Alsulaiman;Jongeun Cha;Abdulmotaleb El-Saddik	2008	2008 IEEE Conference on Virtual Environments, Human-Computer Interfaces and Measurement Systems	10.1109/VECIMS.2008.4592744	filter;identification;message authentication code;computer vision;speech recognition;computer science;virtual machine;machine learning;authentication;haptic technology;fingerprint recognition;artificial neural network;biometrics	Visualization	25.87974128615887	-65.11625350969396	116461
06ed26c8f2a1698cc3bc4a89a414c78b8382b887	effecting an improvement to the fitness function. how to evolve a more identifiable face	c800 psychology;gaussian blur;evolve;criminal face construction fitness function face identification evofit gaussian blur;gaussian processes;ga;construction industry;image restoration;face recognition facial composite witness crime evolve ga;psychology;face recognition;shape;face shape facial features construction industry psychology face recognition videos;image restoration criminal law face recognition gaussian processes;crime;facial composite;face identification;facial features;witness;criminal law;face;criminal face construction;evofit;fitness function;videos	Constructing the face of a criminal from the selection of individual facial parts is a hard task. We have been working on a new system called EvoFIT that involves the selection and breeding of complete faces. The approach is theoretically better founded and produces more identifiable composites than those from a traditional 'feature' system. In the current paper, we explored three new methods of presenting faces to a person using EvoFIT. A better quality face was evolved if (1) the external parts of the face were subjected to a Gaussian blur, allowing a user to focus on the important inner region of the face, (2) if the faces were simplified to make them sketch-like in appearance, and (3) if users took longer in deciding which faces to select for breeding. Taken together, these approaches would appear to make a marked improvement in the ability to evolve an identifiable likeness of a target.	composite pattern;cooperative breeding;fitness function;gaussian blur	Charlie D. Frowd;Jo Park;Alex H. McIntyre;Vicki Bruce;Melanie Pitchford;Steve Fields;Mary Kenirons;Peter J. B. Hancock	2008	2008 Bio-inspired, Learning and Intelligent Systems for Security	10.1109/BLISS.2008.28	psychology;computer vision;communication;social psychology	AI	33.841875862492046	-62.116116805544195	116462
988461f7d1bdf4bdbb5549265b089c9928803ab9	feature-level fusion of deep convolutional neural networks for sketch recognition on smartphones	sketch recognition;fusion;deep learning;convolutional neural networks;smartphone	Understanding hand-free sketches with automated methods is a challenging task due to the diversity and abstract structures of the sketches. In this study, we propose a robust fusion scheme, namely feature-level fusion that use deep convolutional neural networks (CNNs) for recognizing hand-free sketches and develop a sketch recognition application for smartphones based on client-server application architecture. We employ inter-layer CNN features to capture different levels of abstractions of sketches along with fusion operator. Our results on TU-Berlin hand-free sketch benchmark dataset show that, our proposed feature-level fusion scheme achieves a recognition accuracy of 69.175%. This result is promising when compared with the human recognition accuracy of 73.1% on the same dataset.	applications architecture;artificial neural network;benchmark (computing);client–server model;convolutional neural network;server (computing);sketch recognition;smartphone	Emel Boyaci;Mustafa Sert	2017	2017 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2017.7889398	computer vision;speech recognition;fusion;computer science;artificial intelligence;machine learning;deep learning;sketch recognition	Vision	25.81796020951322	-54.7140906493836	116527
8be43db95d9176d1e6bc35415b8387730e75a20f	block-based image steganalysis: algorithm and performance evaluation	reliability;image coding;decomposed image blocks;performance evaluation;image databases;stego image;training;natural images;transform coding;seongho;steganography;computer science block based image steganalysis algorithm and performance evaluation university of southern california c c j kuo cho;discrete cosine transforms;feature extraction;markov processes;decomposed image blocks lock based image steganalysis performance evaluation stego image;electrical engineering;lock based image steganalysis;steganography feature extraction discrete cosine transforms testing voting image sampling convergence markov processes vector quantization classification tree analysis;steganography image coding	Traditional image steganalysis techniques are conducted with respect to the entire image. In this work, we aim to differentiate a stego image from its cover image based on steganalysis results of decomposed image blocks. As a natural image often consists of heterogeneous regions, its decomposition will lead to smaller image blocks, each of which is more homogeneous. We classify these image blocks into multiple classes and find a classifier for each class to decide whether a block is from a cover or stego image. Consequently, the steganalysis of the whole image can be conducted by fusing steganalysis results of all image blocks through a voting process. Experimental results will be given to show the advantage of the proposed block-based image steganalysis approach.	algorithm;performance evaluation;steganalysis	Seongho Cho;Byung-Ho Cha;Jingwei Wang;C.-C. Jay Kuo	2010		10.1109/ISCAS.2010.5537499	computer vision;feature detection;transform coding;feature extraction;theoretical computer science;pattern recognition;reliability;mathematics;steganography;markov process;statistics	Vision	37.12147453934658	-62.0903857403254	116670
f1fd807c3fbd9e2434b5a1947cbe40b599d6ae34	remote sensing image retrieval using convolutional neural network features and weighted distance		Remote sensing image retrieval (RSIR) is a fundamental task in remote sensing. Most content-based RSIR approaches take a simple distance as similarity criteria. A retrieval method based on weighted distance and basic features of convolutional neural network (CNN) is proposed in this letter. The method contains two stages. First, in offline stage, the pretrained CNN is fine-tuned by some labeled images from the target data set, then used to extract CNN features, and labeled the images in the retrieval data set. Second, in online stage, we use the fine-tuned CNN model to extract the CNN feature of the query image and calculate the weight of each image class and apply them to calculate the distance between the query image and the retrieved images. Experiments are conducted on two RSIR data sets. Compared with the state-of-the-art methods, the proposed method is simplified but efficient, significantly improving retrieval performance.	artificial neural network;convolutional neural network;image retrieval;online and offline	Famao Ye;Hui Xiao;Xuqing Zhao;Meng Dong;Wei Luo;Weidong Min	2018	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2018.2847303	image retrieval;data modeling;mathematics;convolutional neural network;remote sensing;artificial intelligence;computer vision;feature extraction;data set	Vision	29.67336568652903	-55.61788777241592	116804
7e1103d3f741898e8fb9b9fd1796ac28efc9e247	a sparse local feature descriptor for robust face recognition	face recognition;sift;local feature descriptor	A good face recognition algorithm should be robust against variations caused by occlusion, expression or aging changes etc. However, the performance of holistic feature based methods would drop dramatically as holistic features are easily distorted by those variations. SIFT, a classical sparse local feature descriptor, was proposed for object matching between different views and scales and has its potential advantages for face recognition. However, face recognition is different from the matching of general objects. This paper investigates the weakness of SIFT used for face recognition and proposes a novel method based on it. The contributions of our work are two-fold: first, we give a comprehensive analysis of SIFT and study its deficiencies when applied to face recognition. Second, based on the analysis of SIFT, a new sparse local feature descriptor, namely SLFD, is proposed. Experimental results on AR database validates our analysis of SIFT. Comparison experiments on both AR and FERET database show that SLFD outperforms the SIFT, LBP based methods and also some other existing face recognition algorithms in terms of recognition accuracy.	facial recognition system;sparse	Na Liu;Jianhuang Lai;Wei-Shi Zheng	2011		10.1007/978-3-642-25449-9_5	computer vision;computer science;machine learning;pattern recognition;three-dimensional face recognition;3d single-object recognition	Vision	35.365424375454246	-57.127676286297934	116819
a03f51c5a56401986e451e9d50ace2bc4686e12c	face recognition using local gradient binary count pattern	databases;facial recognition systems	A local feature descriptor, the local gradient binary count pattern (LGBCP), is proposed for face recognition. Unlike some current methods that extract features directly from a face image in the spatial domain, LGBCP encodes the local gradient information of the face’s texture in an effective way and provides a more discriminative code than other methods. We compute the gradient information of a face image through convolutions with compass masks. The gradient information is encoded using the local binary count operator. We divide a face into several subregions and extract the distribution of the LGBCP codes from each subregion. Then all the histograms are concatenated into a vector, which is used for face description. For recognition, the chi-square statistic is used to measure the similarity of different feature vectors. Besides directly calculating the similarity of two feature vectors, we provide a weighted matching scheme in which different weights are assigned to different subregions. The nearest-neighborhood classifier is exploited for classification. Experiments are conducted on the FERET, CAS-PEAL, and AR face databases. LGBCP achieves 96.15% on the Fb set of FERET. For CAS-PEAL, LGBCP gets 96.97%, 98.91%, and 90.89% on the aging, distance, and expression sets, respectively. © 2015 SPIE and IS&T [DOI: 10.1117/1.JEI.24.6.063003]	ar (unix);code;concatenation;convolution;database;encode;feret (facial recognition technology);facial recognition system;feature vector;gradient descent;k-nearest neighbors algorithm;map;performance;pixel;prewitt operator;visual descriptor;window function	Xiaochao Zhao;Yaping Lin;Bo Ou;Junfeng Yang;Zhelun Wu	2015	J. Electronic Imaging	10.1117/1.JEI.24.6.063003	computer vision;computer science;machine learning;pattern recognition;mathematics	Vision	35.58295544990535	-58.87656760847833	117193
9d82ddd36eecb6c25dcb62561a877b55a736009a	automatic target recognition in sar imagery using pulse-coupled neural network segmentation cascaded with virtual training data generation csom-based classifier	image segmentation;neural networks;support vector machines;target recognition;principal component analysis;concurrent self organizing maps csom automatic target recognition atr pulse coupled neural network pcnn segmentation support vector machine svm;image segmentation synthetic aperture radar support vector machines target recognition neural networks principal component analysis neurons;synthetic aperture radar gabor filters image segmentation neural nets object detection principal component analysis radar target recognition;mstar public release database automatic target recognition sar imagery pulse coupled neural network segmentation virtual training data generation csom based classifier vtdg concurrent self organization maps atr algorithm object detection pcnn image segmentation feature selection gabor filtering principal component analysis pca support vector machine classification military ground vehicles;neurons;synthetic aperture radar	The paper presents an original neural network approach for automatic target recognition (ATR) in the synthetic aperture radar (SAR) imagery using a pulse-coupled neural network (PCNN) segmentation module combined with a classifier based on virtual training data generation (VTDG) using concurrent self-organization maps (CSOM). The proposed ATR algorithm has the following stages: (a) object detection using PCNN image segmentation; (b) feature selection using Gabor filtering (GF) cascaded with principal component analysis (PCA); (c) support vector machine (SVM) classification using VTDG-CSOM to improve the classifier performances. The proposed model has been applied for the recognition of three classes of military ground vehicles represented by the set of 2987 images of the MSTAR public release database. The experimental results have confirmed the method effectiveness, leading to a total success rate of 97.36%.	algorithm;artificial neural network;automatic target recognition;emoticon;feature selection;gabor filter;grammatical framework;image segmentation;network segmentation;object detection;performance;principal component analysis;pulse-coupled networks;self-organization;support vector machine;synthetic data	Victor-Emil Neagoe;Serban-Vasile Carata;Adrian-Dumitru Ciotec	2015	2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2015.7326517	support vector machine;computer vision;synthetic aperture radar;computer science;machine learning;pattern recognition;image segmentation;scale-space segmentation;artificial neural network;principal component analysis	Vision	26.46458883680281	-60.84272779945699	117268
a793e9ae2dbef531f61b720b37b552e5b5ced0d9	investigating visual feature extraction methods for image annotation	discrete wavelet transforms;image query;high level semantic word;discrete wavelet transform;image segmentation;gabor transform;automatic image annotation;training;semantic label set;image database;content based image annotation;gabor filters;image annotation;visual databases content based retrieval discrete cosine transforms discrete wavelet transforms feature extraction gabor filters gaussian distribution image retrieval image segmentation;data mining;discrete cosine transform;bayesian decision automatic image annotation feature distribution expectation maximization algorithm;feature vector;visualization;gabor filter;gaussian mixture model;feature distribution;image color analysis;discrete cosine transforms;feature extraction;expectation maximization algorithm;visual features;semantic image block;image segmentation technology;low level feature vector distribution;bayesian decision;gabor filter visual feature extraction method automatic image annotation discrete cosine transform gabor transform discrete wavelet transform image database high level semantic word semantic label set low level feature vector distribution image segmentation technology semantic image block gaussian mixture model content based image annotation image query;content based retrieval;feature extraction discrete wavelet transforms discrete cosine transforms shape image databases spatial databases visual databases image analysis performance analysis bayesian methods;gaussian distribution;extraction method;visual feature extraction method;visual databases;image retrieval	In order to investigate the performance of visual feature extraction method for automatic image annotation, three visual feature extraction methods, namely discrete cosine transform, Gabor transform and discrete wavelet transform, are studied in this paper. These three methods are used to extract low-level visual feature vectors from images in a given database separately, then these feature vectors are mapped to high-level semantic words to annotate images with labels in a given semantic label set. As it is more efficient to depict the visual features of an image by the feature distribution than to resort to image segmentation technology for semantic image blocks, this paper is going to find out which of the three feature extraction methods performs better in image annotation based on the distribution of feature vectors from the image. The performance of three different kinds of feature extraction method is fully analyzed, and it is found that discrete cosine transform method is more suitable for Gaussian mixture model in automatic image annotation.	automatic image annotation;control theory;discrete cosine transform;discrete wavelet transform;feature extraction;feature vector;high- and low-level;image segmentation;mixture model	Rukun Hu;Shuai Shao;Ping Guo	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5346144	normal distribution;gabor transform;computer vision;feature detection;visualization;feature vector;expectation–maximization algorithm;feature extraction;image retrieval;computer science;machine learning;kanade–lucas–tomasi feature tracker;discrete cosine transform;pattern recognition;mixture model;image segmentation;discrete wavelet transform;automatic image annotation;top-hat transform;feature	Vision	38.2778992530702	-61.71347633195429	117513
e8fa2e242369dcf50ab5cd1745b29bfc51aadf2a	image captioning with object detection and localization		Automatically generating a natural language description of an image is a task close to the heart of image understanding. In this paper, we present a multi-model neural network method closely related to the human visual system that automatically learns to describe the content of images. Our model consists of two sub-models: an object detection and localization model, which extract the information of objects and their spatial relationship in images respectively; Besides, a deep recurrent neural network (RNN) based on long short-term memory (LSTM) units with attention mechanism for sentences generation. Each word of the description will be automatically aligned to different objects of the input image when it is generated. This is similar to the attention mechanism of the human visual system. Experimental results on the COCO dataset showcase the merit of the proposed method, which outperform previous benchmark models.	algorithm;artificial neural network;benchmark (computing);computer vision;data structure alignment;human visual system model;internationalization and localization;long short-term memory;natural language;object detection;random neural network;recurrent neural network	Zhongliang Yang;Yu-Jin Zhang;Sadaqat ur Rehman;Yongfeng Huang	2017		10.1007/978-3-319-71589-6_10	machine learning;computer vision;pattern recognition;artificial intelligence;object detection;artificial neural network;computer science;deep learning;natural language;coco;human visual system model;recurrent neural network;closed captioning	AI	26.351254013383908	-53.35350294167063	117516
22a13d517eb1b79407bd43763ff7f0144b5ccc8a	background modeling using adaptive pixelwise kernel variances in a hybrid feature space	kernel;probability;selected works;gaussian processes;joints;image texture;estimation;image color analysis;feature extraction;mathematical model;bepress;standard backgrounding benchmark background modeling adaptive pixelwise kernel variances hybrid feature space background subtraction probabilistic models gaussian mixtures joint domain range density estimates spatial information texture information local binary patterns scale invariant local ternary patterns joint domain range based estimates foreground scores background scores;kernel mathematical model adaptation models image color analysis equations estimation joints;adaptation models;solid modelling;solid modelling feature extraction gaussian processes image texture probability	Recent work on background subtraction has shown developments on two major fronts. In one, there has been increasing sophistication of probabilistic models, from mixtures of Gaussians at each pixel [7], to kernel density estimates at each pixel [1], and more recently to joint domainrange density estimates that incorporate spatial information [6]. Another line of work has shown the benefits of increasingly complex feature representations, including the use of texture information, local binary patterns, and recently scale-invariant local ternary patterns [4]. In this work, we use joint domain-range based estimates for background and foreground scores and show that dynamically choosing kernel variances in our kernel estimates at each individual pixel can significantly improve results. We give a heuristic method for selectively applying the adaptive kernel calculations which is nearly as accurate as the full procedure but runs much faster. We combine these modeling improvements with recently developed complex features [4] and show significant improvements on a standard backgrounding benchmark.	background subtraction;benchmark (computing);feature vector;heuristic;kernel (operating system);kernel density estimation;local binary patterns;local ternary patterns;mixture model;pixel;scale-invariant feature transform	Manjunath Narayana;Allen R. Hanson;Erik G. Learned-Miller	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247916	image texture;estimation;kernel;kernel embedding of distributions;feature extraction;machine learning;pattern recognition;probability;mathematical model;gaussian process;mathematics;variable kernel density estimation;statistics	Vision	39.00222707991907	-55.0477926102713	117596
0286fb92129c79f4f57371c00307b9bb77d4c8bd	texture features for dct-coded image retrieval and classification	image retrieval discrete cosine transforms discrete wavelet transforms image texture analysis image databases information retrieval wavelet analysis performance analysis image coding feature extraction;image coding;image resolution;data compression;decoding;image database;image classification;transform coding;texture features;dct coded image databases dct coded image retrieval dct coded image classification multiresolution wavelet transform performance texture analysis texture features discrete cosine transform image decompression inverse dct multiresolution reordered features dct coefficients subband energy features brodatz texture database texture pattern retrieval accuracy correct classification rate texture pattern retrieval texture pattern classification;image texture discrete cosine transforms image coding image classification image resolution decoding data compression feature extraction image retrieval visual databases wavelet transforms transform coding;discrete cosine transform;image texture;wavelet transforms;texture analysis;wavelet transform;discrete cosine transforms;feature extraction;visual databases;image retrieval	The multiresolution wavelet transform has been shown to be an effective technique and achieved very good performance for texture analysis. However, a large number of images are compressed by the methods based on discrete cosine transform (DCT). Hence, the image decompression of inverse DCT is needed to obtain the texture features based on the wavelet transform for the DCT-coded image. This paper proposes the use of the multiresolution reordered features for texture analysis. The proposed features are directly generated by using the DCT coefficients from the DCT-coded image. Comparisons with the subband-energy features extracted from the wavelet transform, conventional DCT using the Brodatz texture database indicate that the proposed method provides the best texture pattem retrieval accuracy and obtains much better correct classification rate. The proposed DCT based features are expected to be very useful and efficient for texture pattem retrieval and classification in large DCT-coded image databases. The detail simulation results can be found in web page: http://www.cs.ccu.edu.rw/-hyl/mrdcr/.	coefficient;data compression;database;discrete cosine transform;image compression;image retrieval;multiresolution analysis;simulation;wavelet transform;web page	Yu-Len Huang;Ruey-Feng Chang	1999		10.1109/ICASSP.1999.757475	image texture;computer vision;speech recognition;image retrieval;computer science;pattern recognition;mathematics;texture compression;texture filtering;statistics;wavelet transform	Vision	38.84033885418008	-61.3198028046398	117657
f7806428ce9e88f2c57f7a663657528ed31035fa	a som combined with knn for classification task	human performance;anova test som classification task artificial neural networks automatic classification self organizing maps ann k nearest neighbor statistical classifier car plates knn recognition;image classification;statistical analysis;self organising feature maps;k nearest neighbor;self organized map;statistical analysis character recognition image classification learning artificial intelligence self organising feature maps;learning artificial intelligence;neurons support vector machine classification databases artificial neural networks training labeling histograms;automatic classification;character recognition;artificial neural network	Classification is a common task that humans perform when making a decision. Techniques of Artificial Neural Networks (ANN) or statistics are used to help in an automatic classification. This work addresses a method based in Self-Organizing Maps ANN (SOM) and K-Nearest Neighbor (KNN) statistical classifier, called SOM-KNN, applied to digits recognition in car plates. While being much faster than more traditional methods, the proposed SOM-KNN keeps competitive classification rates with respect to them. The experiments here presented contrast SOM-KNN with individual classifiers, SOM and KNN, and the results are classification rates of 89.48±5.6, 84.23±5.9 and 91.03±5.1 percent, respectively. The equivalency between SOM-KNN and KNN recognition results are confirmed with ANOVA test, which shows a p-value of 0.27.	artificial neural network;experiment;k-nearest neighbors algorithm;neural networks;statistical classification	Leandro Augusto da Silva;Emilio Del-Moral-Hernandez	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033525	human performance technology;contextual image classification;feature;computer science;artificial intelligence;machine learning;linear classifier;pattern recognition;k-nearest neighbors algorithm;artificial neural network	ML	30.704278794284686	-65.18326663725999	117729
6519a878bfa06786741cd6396c8909767a7acb25	fast face sketch-photo image synthesis and recognition	sketch recognition;sketch photo synthesis;face recognition;feature extraction;principle component analysis	Face sketch recognition has great practical value in the criminal detection, security and other fields. Especially, it can help the police narrow down potential suspects in criminal detection effectively. Face sketch represents the original photos in a simple and recognizable form, so sketch and photo are images of two different modes. In order to identify the corresponding sketch face image in a lot of photo face images, this paper presents an improved sketch–photo transformation algorithm, and it uses the effective characteristics of the photo image more reasonably during transforming a photo image into sketch. In this way, it can reduce the difference between the sketch and photo image to improve the matching effect, and save the recognition time. Many experiments on CUHK Face Sketch database including 188 sketch–photos prove the effectiveness of the method in this paper.	sketch	Zhenxue Chen;Kaifang Wang;Chengyun Liu	2016	IJPRAI	10.1142/S0218001416560085	facial recognition system;computer vision;speech recognition;feature extraction;computer science;artificial intelligence;machine learning;sketch recognition;principal component analysis	Vision	32.391026308300546	-58.35704093456879	117838
321db1059032b828b223ca30f3304257f0c41e4c	comparative evaluation of age classification from facial images	discrete wavelet transforms;hybrid variance age classification discrete cosine transform dct discrete wavelet transform dwt dual tree complex wavelet transform dtcwt variance k nearest neighbour knn;hybrid variance classifiers age classification facial images feature selection transform domain feature extraction techniques discrete cosine transform dct discrete wavelet transform dwt dual tree complex wavelet transform dtcwt k nearest neighbour classifiers knn classifiers;feature extraction discrete wavelet transforms discrete cosine transforms face estimation;estimation;discrete cosine transforms;feature extraction;trees mathematics age issues discrete cosine transforms discrete wavelet transforms face recognition feature extraction feature selection image classification;face	Researchers have made efforts to achieve age classification using spatial and transform domain techniques with various classifiers. Spatial Domain techniques are based on human perception and susceptible to noise and image processing operations. Transform domain techniques provide high flexibility and robustness in selection of features and better classification efficiency. This paper uses transform domain feature extraction techniques to achieve maximum possible age classification efficiency. The transforms used in this paper to extract features are Discrete Cosine Transform (DCT), Discrete Wavelet Transform (DWT), and Dual Tree Complex Wavelet Transform (DTCWT). The features extracted from facial images are classified into a range of age groups viz. child, adolescent, young, middle aged and old aged using variance, k-nearest neighbour (kNN) and hybrid variance as classifiers. The experimental results prove that the feature extraction using DTCWT with Hybrid variance classifiers provides better classification efficiency than that of DCT and DWT.	complex wavelet transform;discrete cosine transform;discrete wavelet transform;feature extraction;image noise;image processing;informatics;k-nearest neighbors algorithm;matlab;qualitative comparative analysis;statistical classification;viz: the computer game	Raunak M. Borwankar;Gaurav S. Pednekar;Saurabh A. Deshpande;Purva S. Sawant;Satishkumar S. Chavan	2015	2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2015.7275951	face;wavelet;estimation;transform coding;speech recognition;s transform;harmonic wavelet transform;lapped transform;second-generation wavelet transform;continuous wavelet transform;feature extraction;machine learning;discrete cosine transform;pattern recognition;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;statistics;wavelet transform	Vision	36.6614838851964	-62.235999627086635	117874
fe0cf8eaa5a5f59225197ef1bb8613e603cd96d4	improved face verification with simple weighted feature combination		Since the appearance of deep learning, face verification (FV) has made great progress with large scale datasets, well-designed networks, new loss functions, fusion of models and metric learning methods. However, incorporating all these methods obviously takes a lot of time both at training and testing stages. In this paper, we just select training images randomly without any clean and alignment procedure. Then we propose a simple weighted average method which combines features of the last two layers with different weights on the modified VGGNet, named as CB-VGG. It is significantly reducing the complexity of time that one model can be treated as two models. LMNN is used as a post-processing procedure to improve the discrimination of the combined features. Our experiments show relatively competitive results on LFW, CFP, and CACD datasets.	cb unix;care-of address;computers, freedom and privacy conference;deep learning;experiment;farmville;large margin nearest neighbor;loss function;preprocessor;randomness;video post-processing	Xinyu Zhang;Jiang Zhu;Mingyu You	2017		10.1007/978-981-10-7302-1_2	deep learning;large margin nearest neighbor;machine learning;artificial intelligence;computer science;pattern recognition;weighted arithmetic mean	Vision	29.95129843780903	-53.928557770780024	117896
8547d34a1ee7557036e0a6623b10868914fc6da0	image forgery localization based on multi-scale convolutional neural networks		In this paper, we propose to utilize Convolutional Neural Networks (CNNs) and the segmentation-based multi-scale analysis to locate tampered areas in digital images. First, to deal with color input sliding windows of different scales, we adopt a unified CNN architecture. Then, we elaborately design the training procedures of CNNs on sampled training patches. With a set of tampering detectors based on CNNs for different scales, a series of complementary tampering possibility maps can be generated. Last but not least, a segmentation-based method is proposed to fuse these maps and generate the final decision map. By exploiting the benefits of both the small-scale and large-scale analyses, the segmentation-based multi-scale analysis can lead to a performance leap in forgery localization of CNNs. Numerous experiments are conducted to demonstrate the effectiveness and efficiency of our method.	convolutional neural network;digital image;experiment;map;microsoft windows;sensor	Yaqi Liu;Qingxiao Guan;Xianfeng Zhao	2018		10.1145/3206004.3206010	architecture;convolutional neural network;artificial intelligence;digital image;machine learning;computer science;computer vision	Vision	28.520374894464616	-54.132140837421105	118185
a7f870847ca4c5f8dd832988f333cb4eeeaff95f	micro-expression recognition using color spaces	biological patents;biomedical journals;tensile stress;tensor analysis;tensors emotion recognition face recognition image colour analysis;text mining;europe pubmed central;color;facial action coding system micro expression recognition color spaces tensor analysis local binary patterns;citation search;citation networks;dynamic texture histograms micro expression recognition involuntary facial expressions genuine emotions two perceptual color spaces cielab cieluv novel color space model tensor independent color space tics micro expression color video clip fourth order tensor spatial information temporal information color information rgb independent color components regions of interests roi facial action coding system;gold;face recognition;research articles;image color analysis;abstracts;feature extraction;期刊论文;open access;facial action coding system;life sciences;clinical guidelines;face;local binary patterns;full text;image color analysis tensile stress color feature extraction face face recognition gold;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search;micro expression recognition;color spaces	Micro-expressions are brief involuntary facial expressions that reveal genuine emotions and, thus, help detect lies. Because of their many promising applications, they have attracted the attention of researchers from various fields. Recent research reveals that two perceptual color spaces (CIELab and CIELuv) provide useful information for expression recognition. This paper is an extended version of our International Conference on Pattern Recognition paper, in which we propose a novel color space model, tensor independent color space (TICS), to help recognize micro-expressions. In this paper, we further show that CIELab and CIELuv are also helpful in recognizing micro-expressions, and we indicate why these three color spaces achieve better performance. A micro-expression color video clip is treated as a fourth-order tensor, i.e., a four-dimension array. The first two dimensions are the spatial information, the third is the temporal information, and the fourth is the color information. We transform the fourth dimension from RGB into TICS, in which the color components are as independent as possible. The combination of dynamic texture and independent color components achieves a higher accuracy than does that of RGB. In addition, we define a set of regions of interests (ROIs) based on the facial action coding system and calculated the dynamic texture histograms for each ROI. Experiments are conducted on two micro-expression databases, CASME and CASME 2, and the results show that the performances for TICS, CIELab, and CIELuv are better than those for RGB or gray.	behavioral tic;belief propagation;code system;color space;database;dimensions;experiment;locality of reference;mutual information;pattern recognition;performance;region of interest;regular expression;spaces;video clip	Sujing Wang;Wen-Jing Yan;Xiaobai Li;Guoying Zhao;Chunguang Zhou;Xiaolan Fu;Minghao Yang;Jianhua Tao	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2496314	gold;face;computer vision;text mining;local binary patterns;speech recognition;hsl and hsv;facial action coding system;tensor;feature extraction;computer science;machine learning;stress;color space	Vision	34.38105306539226	-55.440277650175126	118376
7bb801f89531bc7ac9ac0d1d91fbe20dd8c89c49	automatic object annotation from weakly labeled data with latent structured svm	cccp training algorithm;support vector machines image color analysis training feature extraction histograms three dimensional displays estimation;histograms;object recognition;support vector machines;training;color features;latent structured svm training algorithm;gradient features;automatic object annotation;estimation;three dimensional displays;image color analysis;image colour analysis;feature extraction;cccp training algorithm automatic object annotation weakly labeled data positive images color features gradient features feature statistics latent structured svm training algorithm;feature statistics;learning artificial intelligence;support vector machines feature extraction image colour analysis learning artificial intelligence object recognition;positive images;weakly labeled data	In this paper we present an approach to automatic object annotation. We are given a set of positive images which all contain a certain object and our goal is to automatically determine the position of said object in each image. Our approach first applies a heuristic to identify initial bounding boxes based on color and gradient features. This heuristic is based on image and feature statistics. Then, the initial boxes are refined by a latent structured SVM training algorithm which is based on the CCCP training algorithm. We show that our approach outperforms previous work on multiple datasets.	algorithm;experiment;exploit (computer security);gradient;heuristic;minimum bounding box;structured support vector machine	Christian X. Ries;Fabian Richter;Stefan Romberg;Rainer Lienhart	2014	2014 12th International Workshop on Content-Based Multimedia Indexing (CBMI)	10.1109/CBMI.2014.6849838	support vector machine;computer vision;estimation;feature extraction;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;histogram;statistics	Vision	35.40968800269619	-53.38001862148421	118529
942d261d81513ee2cc8abfc908bc540ae1097da0	selection of classifiers for the construction of multiple classifier systems	pattern classification handwritten character recognition;system performance handwriting recognition entropy educational institutions information theory lamps laboratories us department of defense contracts mutual information;unconstrained handwritten numeral recognition classifier selection multiple classifier systems component classifiers combination method information theory;multiple classifiers;pattern classification;multiple classifier system;handwritten character recognition;information theory	Most studies on combining multiple classifiers have focused on combination methods, but a few studies have investigated on how to select component classifiers from a classifier pool. Multiple classifier systems performance varies with the component classifiers as well as the combination method. In this paper, methods based on information theory are proposed for selecting component classifiers, provided that the number of component classifiers is fixed in advance. These methods are applied to the classifier pool and examine the possible classifier sets. The system is compared to other multiple classifier systems on the recognition of unconstrained handwritten numerals.	information theory;linear classifier	Hee-Joong Kang;David S. Doermann	2005	Eighth International Conference on Document Analysis and Recognition (ICDAR'05)	10.1109/ICDAR.2005.213	random subspace method;margin classifier;speech recognition;cascading classifiers;quadratic classifier;information theory;computer science;machine learning;pattern recognition;statistics	Robotics	31.855781355521344	-64.98783900015806	118561
23c613e2287eac984830df2637be03783fc05e86	image community detection	computer vision;directed graph anaysis community detection content based image search;signal processing;communities signal processing conferences pattern analysis transmission line matrix methods computer vision;pattern analysis;transmission line matrix methods;communities;conferences	In this work, we propose a new method which can detect image communities inside an image set. The proposed method differs from previous works by representing image relations with directed graphs and performing community analysis on these directed graphs. By analyzing resulting image communities, we can observe the robustness of the proposed method against image deformations (e.g. cutting, text overlay, color changes). The proposed method can be applied to text-based image search results to achieve content-based image search. We believe that the proposed method can be successfully used to bridge the gap between available text-based methods and content-based image search.	directed graph;image retrieval;text-based (computing)	Ersin Esen;Savas Özkan;Ilkay Atil;Mehmet Ali Arabaci;Seda Tankiz	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830335	computer vision;feature detection;binary image;image processing;computer science;theoretical computer science;machine learning;signal processing;digital image processing;multimedia;automatic image annotation	Vision	37.180386317595854	-64.21064829849666	118775
fa7fdf52c47190fbcff0b2b6f25a8b11e56b87ea	texture classification using dominant wavelet packet energy features	mahalanobis distance;distinct natural textures;electronic mail;brodatz album;texture classification;natural texture images texture classification orthonormal wavelet bases energy signatures performance wavelet packet decomposition dominant energy features wavelet packet coefficients mahalanobis distance classifier feature selection distinct natural textures brodatz album feature set;performance;mahalanobis distance classifier;filters;image classification;wavelet packets wavelet transforms image texture analysis frequency read only memory classification tree analysis filters moon power engineering and energy electronic mail;wavelet packet;image texture;power engineering and energy;wavelet transforms;energy signatures;orthonormal wavelet bases;feature extraction image texture image classification wavelet transforms;feature extraction;moon;natural texture images;wavelet packet decomposition;dominant energy features;feature selection;image texture analysis;classification tree analysis;energy value;wavelet packets;frequency;high performance;read only memory;wavelet;wavelet packet coefficients;feature set	This paper proposes a high performance texture classification method using dominant energy features from wavelet packet decomposition. We decompose the texture images with a family of real orthonnormal wavelet bases and compute the energy signatures using the wavelet packet coefficients. Then we select few number of most dominant energy values as features and employ a Mahalanobis distance classifier to classify a set of distinct natural textures selected from the Brodatz album. In our experiments, the proposed method employed a reduced feature set and involved less computation in classification time while still archiving high accuracy rate (94.8%) for classifying twenty classes of natural texture images.	archive;coefficient;computation;experiment;network packet;type signature;wavelet packet decomposition	Moon-Chuen Lee;Chi-Man Pun	2000		10.1109/IAI.2000.839620	computer vision;speech recognition;computer science;machine learning;pattern recognition;mathematics;wavelet packet decomposition;feature selection	Robotics	36.42284492892956	-61.84086618976142	118781
5ef2b6ce8560523518e3a41e883ce3fc09fe9b0a	color description of low resolution images using fast bitwise quantization and border-interior classification	histograms;image classification feature extraction;support vector machines;image color analysis feature extraction spatial resolution quantization signal histograms support vector machines;quantization signal;image color analysis;feature extraction;rgb images color description low resolution images fast bitwise quantization image classification image preprocessing feature extraction color features extraction image resolution border interior classification extractor logarithmic distance function;quantisation signal feature extraction image classification image colour analysis image resolution;spatial resolution	Image classification often require preprocessing and feature extraction steps that are directly related to the accuracy and speed of the whole task. In this paper we investigate color features extracted from low resolution images, assessing the influence of the resolution settings on the final classification accuracy. We propose a border-interior classification extractor with a logarithmic distance function in order to maintain the discrimination capability in different resolutions. Our study shows that the overall computational effort can be reduced in 98%. Besides, a fast bitwise quantization is performed for its efficiency on converting RGB images to one channel images. The contributions can benefit many applications, when dealing with a large number of images or in scenarios with limited network bandwidth and concerns with power consumption.	bitwise operation;computation;extractor (mathematics);feature extraction;image resolution;preprocessor	Moacir Ponti;Camila T. Picon	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178200	color histogram;support vector machine;computer vision;feature detection;color quantization;image resolution;color image;feature extraction;computer science;machine learning;pattern recognition;histogram;mathematics;sub-pixel resolution;feature	Vision	37.67247795027962	-59.991418449243056	118853
f7154850b6d955fc449ebf1bdedcb7f139dbc638	fractal model based face recognition for ubiquitous environments	face recognition;facial features	In Ubiquitous environments, it is required to identify the user in order to utilize the knowledge of the user's behavior and situation. In this paper, we propose a novel method of face recognition using dominant facial region extraction and fractal model. In order to improve the performance of the face recognition system, we propose an algorithm to extract the dominant facial region from the face images that includes the most discriminated part of the face, and for each dominant facial region it was presented by its fractal model and stored in database. The fractal model of the dominant facial region is then utilized as fractal facial features for face recognition. To further improve the performance of the face recognition system , we also propose the techniques of weighting mask and DC_Free MSE. Finally, some experimental results are presented and demonstrate the excellent performance of our face recognition approach.	facial recognition system;fractal	Shuenn-Shyang Wang;Su-Wei Lin;Cheng-Ming Cho	2008		10.1007/978-3-540-69293-5_59	computer vision;face detection;speech recognition;computer science;three-dimensional face recognition;face hallucination	Vision	33.04007558061141	-59.60576870234528	118872
814d185e3332420994bad13199a444e34efc6c98	detecting usm image sharpening by using cnn		Abstract Image sharpening is a basic digital image processing scheme utilized to pursue better image visual quality. From image forensics point of view, revealing the processing history is essential to the content authentication of a given image. Hence, image sharpening detection has attracted increasing attention from researchers. In this paper, a convolutional neural network (CNN) based architecture is reported to detect unsharp masking (USM), the most commonly used sharpening algorithm, applied to digital images. Extensive experiments have been conducted on two benchmark image datasets. The reported results have shown the superiority of the proposed CNN based method over the existed sharpening detection method, i.e., edge perpendicular ternary coding (EPTC).	sensor;unsharp masking	Jingyu Ye;Zhangyi Shen;Piyush Behrani;Feng Ding;Yun Q. Shi	2018	Sig. Proc.: Image Comm.	10.1016/j.image.2018.04.016	artificial intelligence;theoretical computer science;computer vision;digital image;digital image processing;convolutional neural network;ternary operation;computer science;architecture;unsharp masking;sharpening	Vision	25.321476014393706	-55.51406725572703	118875
1e6b3ae784416252d98d8d3433ecacddfc0839fc	object class recognition with supervised nonlinear neighborhood embedding of visual words	object recognition;supervised nonlinear neighborhood embedding;subspace method;positional information;bag of words;visual words	This paper develops a supervised nonlinear subspace of bag-of-features for category classification. Bag-of-features represents an image as an orderless distribution of features, which selects the visual words by clustering and uses the similarity with each visual word as the features for classification. In this paper, we propose to model the ensemble of visual words with a supervised nonlinear neighborhood embedding method to a more discriminative space for category classification. The supervised nonlinear neighborhood embedding(SNNE) is used to model visual words and extract the discrimitive information specialized for each category. The projection length in subspace is used as features for classification. The SNNE subspace method can model the nonlinear variations induced by various kinds of visual words and extract more discriminative feature for object recognition. The proposed method is evaluated using the Cal-tech and GRAZ01 database. We confirm that the proposed method is comparable with state-of-the-art methods without absolute position information.	cluster analysis;nonlinear system;outline of object recognition;supervised learning;visual word	Xian-Hua Han;Yen-Wei Chen;Xiang Ruan	2009		10.1145/1734605.1734615	computer vision;machine learning;pattern recognition;mathematics	Vision	34.074157463586054	-52.10628250851576	119138
624cda4e7cc845b6d3939b672d310ec21f78ccc0	multi-level iris video image thresholding	pupil;multilevel iris video image thresholding;histograms;image recognition;pattern clustering;image segmentation;video signal processing biometrics access control image segmentation pattern clustering;video signal processing;biometrics access control;multi level iris video image thresholding iris recognition video image multi level image thresholding;biometrics;iris recognition;illumination condition multilevel iris video image thresholding biometrics single image based segmentation method orientation invariant thresholding k mean clustering pupil;multi level image thresholding;threshold scheme;illumination condition;orientation invariant thresholding;pixel;k mean clustering;image segmentation iris recognition histograms biometrics lighting entropy focusing design methodology principal component analysis image sensors;video image;entropy;lighting;iris;k means clustering;multi level iris video image thresholding;single image based segmentation method	Iris recognition has been shown to be one of the most accurate biometrics. However, under non-ideal situations, its recognition accuracy can be reduced dramatically. Under such situations, video images can be used to improve the accuracy. The traditional single image based segmentation method could be inefficient. Video image based thresholding method can help improve the segmentation efficiency. However, traditional thresholding methods are not designed for iris images. In this paper, the multi-level iris video image thresholding method is proposed. It takes advantage of the correlations between consecutive images for video based thresholding. It is an orientation invariant thresholding scheme. K-mean clustering is used to find the clusters and PCA is used to quickly project the image to the clusters. The experimental results show the proposed method is effective. The thresholded images show clear pupil and iris areas, which can help further iris segmentation and processing. In addition, the proposed method can be applied to non-video images if they are obtained from the same sensor with similar illumination conditions.	autostereogram;biometrics;cluster analysis;computation;disk image;global illumination;iris recognition;pixel;robustness (computer science);sensor;thresholding (image processing)	Yingzi Du;N. Luke Thomas;Emrah Arslanturk	2009	2009 IEEE Workshop on Computational Intelligence in Biometrics: Theory, Algorithms, and Applications	10.1109/CIB.2009.4925684	shift-and-add;computer vision;computer science;machine learning;pattern recognition;balanced histogram thresholding;thresholding;image segmentation;k-means clustering;computer graphics (images)	Vision	36.77289290670465	-58.580950148559694	119414
035effbaa3bbcde32d08661d82ab648f1840877f	generic object recognition via shock patch fragments.	object recognition;natural images	We propose a new methodology to partition a natural image into regions based on the shock graph of its contour fragments. We show that these regions, or shock patch fragments, are often object fragments, thus effecting a partial segmentation of the image. We utilize shock patch fragments to recognize objects with dominant shape cues eliminating the need to segment out the entire object from the image first. Our preliminary results with minimal training are promising with respect to the state of the art recognition systems.	outline of object recognition;patch (computing);shape context	Özge Can Özcanli;Benjamin B. Kimia	2007		10.5244/C.21.104	computer vision;deep-sky object;computer science;cognitive neuroscience of visual object recognition;pattern recognition;3d single-object recognition	Vision	38.975929628952635	-56.01137191340648	119453
5c22077f9457a262d3ee6ef065d6d934a9bce34e	erratum to: fuzzy art-based place recognition for visual loop closure detection		The automatic place recognition problem is one of the key challenges in SLAM approaches for loop closure detection. Most of the appearance-based solutions to this problem share the idea of image feature extraction, memorization, and matching search. The weakness of these solutions is the storage and computational costs which increase drastically with the environment size. In this regard, the major constraints to overcome are the required visual information storage and the complexity of similarity computation. In this paper, a novel formulation is proposed that allows the computation time reduction while no visual information are stored and matched explicitly. The proposed solution relies on the incremental building of a bio-inspired visual memory using a Fuzzy ART network. This network considers the properties discovered in primate brain. The performance evaluation of the proposed method has been conducted using two datasets representing different large scale outdoor environments. The method has been compared with RatSLAM and FAB-MAP approaches and has demonstrated a decreased time and storage costs with broadly comparable precision recall performance. K. Rebai (B) · O. Azouaoui Centre de Développement des Technologies Avancées (CDTA), Algiers, Algeria e-mail: karima.rebai@gmail.com O. Azouaoui e-mail: azouaoui@rocketmail.com K. Rebai Ecole Nationale Supérieure de Technologie (ENST), Algiers, Algieria N. Achour Université des Sciences et de la Technologie Houari Boumediene (USTHB), Algiers, Algeria e-mail: noura.achour@gmail.com	british informatics olympiad;closed innovation;computation;computational model;cycle detection;encode;email;feature (computer vision);feature extraction;for loop;hebbian theory;information extraction;internationalization and localization;linear algebra;performance evaluation;pixel;precision and recall;scan line;sensitivity and specificity;simulation;simultaneous localization and mapping;time complexity;topological graph;vc dimension;veritas cluster server	Karima Rebai;Ouahiba Azouaoui;Nouara Achour	2013	Biological Cybernetics	10.1007/s00422-013-0550-x	computer vision;artificial intelligence;machine learning;mathematics	Robotics	37.3480342605561	-57.91393412758284	119553
b54d43f7afde36276ac5094599ac9e35db0d475d	approaching the computational color constancy as a classification problem through deep learning	white balancing;computational color constancy;machine learning;convolutional neural network;illumination estimation	Computational color constancy refers to the problem of computing the illuminant color so that the images of a scene under varying illumination can be normalized to an image under the canonical illumination. In this paper, we adopt a deep learning framework for the illumination estimation problem. The proposed method works under the assumption of uniform illumination over the scene and aims for the accurate illuminant color computation. Specifically, we trained the convolutional neural network to solve the problem by casting the color constancy problem as an illumination classification problem. We designed the deep learning architecture so that the output of the network can be directly used for computing the color of the illumination. Experimental results show that our deep network is able to extract useful features for the illumination estimation and our method outperforms all previous color constancy methods on multiple test datasets.	algorithm;artificial neural network;computation;computer vision;convolutional neural network;deep learning;global illumination;high- and low-level;high-level programming language;pixel	Seoung Wug Oh;Seon Joo Kim	2017	Pattern Recognition	10.1016/j.patcog.2016.08.013	computer vision;color normalization;computer science;machine learning;pattern recognition;mathematics;color balance;convolutional neural network	Vision	25.04045331700886	-52.2838598771429	119694
4345b71b0e1d4f3d9ae0c7c655cf3b2ad1522b4e	evaluation of histograms local features and dimensionality reduction for 3d face verification			dimensionality reduction	Ammar Chouchane;Mebarka Belahcene;Abdelmalik Ouamane;Salah Bourennane	2016	JIPS	10.3745/JIPS.02.0037	support vector machine;dimensionality reduction;computer science;histogram;computer vision;pattern recognition;artificial intelligence	Vision	30.92899195350926	-57.84826954897803	119887
4c8cab55a91092d6a70e2dd8375afa2c07b564b0	motion analysis based cross-database voting for face spoofing detection		With the rapid development of face recognition systems in various practical applications, numerous face spoofing attacks under different environment and devices have emerged. The countermeasure of face spoofing attacks in cross-database have caused increasing attention. This paper proposes a face spoofing detection method with motion analysis based cross-database voting. We employ the consistency motion information of different databases like eye-blink, mouth movements and facial expression etc. Then the motion information maps of a video is classified to real or fake by CNN model. Furthermore, cross-database voting strategy is constructed to transfer motion characteristics from a database to another for face spoofing inference. Experimental results demonstrate that the proposed method outperforms its comparisons taking benefits of motion analysis based CNN classification and cross-database voting.		Lifang Wu;Yaowen Xu;Meng Jian;Wei Cai;Chuncan Yan;Yukun Ma	2017		10.1007/978-3-319-69923-3_57	motion analysis;database;voting;facial recognition system;spoofing attack;inference;facial expression;mouth movements;countermeasure;computer science	Vision	30.872161546345794	-52.39404693316316	120093
2a98351aef0eec1003bd5524933aed8d3f303927	facial identity and expression recognition by using active appearance model with efficient second order minimization and neural networks	second order;high dimensionality;neural networks;iterative algorithms;neural nets;real time;face recognition active appearance model neural networks shape jacobian matrices face detection noise shaping noise robustness humans iterative algorithms;expression recognition;second order minimization;active appearance model;multilayer perceptron;noise robustness;feature vector;face recognition;shape;feature extraction;noise shaping;humans;face detection;neural nets face recognition feature extraction;jacobian matrices;facial identity;neural networks facial identity expression recognition active appearance model second order minimization;neural network	This paper proposes a technique for real-time recognition of facial Identity and expression which uses the active appearance model (AAM) with efficient second order minimization algorithm and neural network, especially the multilayer perceptron. The efficient second order minimization allows AAM to have the ability of correct convergence with a little loss of frame rate. And the correctly extracted facial shape with AAM prevents the recognition of facial identity and expression from undergoing a large error. In addition, high dimensional feature vectors of facial identity and expression, which consist of facial shape and texture, can be dealt by the multilayer perceptron with a very high recognition rate of over 98%.	active appearance model;algorithm;neural networks;real-time clock	Hyun-Chul Choi;Se-Young Oh	2007	2007 International Symposium on Computational Intelligence in Robotics and Automation	10.1109/CIRA.2007.382901	computer vision;face detection;active appearance model;feature vector;noise shaping;feature extraction;shape;computer science;machine learning;pattern recognition;multilayer perceptron;second-order logic;artificial neural network	Robotics	32.029539246614966	-59.68706551614724	120207
78c9e96fa04a5ebf2033f211e6dbd320e2330f78	a wavelet based multiresolution algorithm for rotation invariant feature extraction	rotation invariance and wavelets;wavelet decomposition;data mining;feature vector;rotation invariance;feature extraction;radial symmetry;content based image retrieval;content based retrieval	The present work aims at proposing a new wavelet representation formula for rotation invariant feature extraction. The algorithm is a multilevel representation formula involving no wavelet decomposition in standard sense. Using the radial symmetry property, that comes inherently in the new representation formula, we generate the feature vectors that are shown to be rotation invariant. We show that, using a hybrid data mining technique, the algorithm can be used for rotation invariant content based image retrieval (CBIR). The proposed rotation invariant retrieval algorithm, suitable for both texture and nontexture images, avoids missing any relevant images but may retrieve some other images which are not very relevant. We show that the higher precision can however be achieved by pruning out irrelevant images.	algorithm;feature extraction;wavelet	Challa S. Sastry;Arun K. Pujari;Bulusu Lakshmana Deekshatulu;Chakravarthy Bhagvati	2004	Pattern Recognition Letters	10.1016/j.patrec.2004.07.011	computer vision;feature vector;symmetry in biology;feature extraction;computer science;machine learning;pattern recognition;mathematics	Vision	38.69394381812284	-59.74251813301537	120315
497dbdd73755538b628d73e50dca7159f4280ccd	image search engine based on color histogram and zernike moment		Though numerous approaches were proposed for image recognition, in this paper we proposed implementation of CBIR, using color descriptor combined to the shape features (Zernike moments). A practical and efficient system has been implemented based on image division and extraction color histogram of color from each block, and combined the result vector to the global shape one. We experimented our system on several image bases, and it gave satisfactory results.	color histogram;web search engine	Nawal Chifa;Abdelmajid Badri;Yassine Ruichek;Aicha Sahel	2017		10.1007/978-3-319-68179-5_55	adaptive histogram equalization;image histogram;zernike polynomials;color histogram;computer vision;color normalization;balanced histogram thresholding;histogram matching;pattern recognition;artificial intelligence;histogram equalization;computer science	Vision	38.55344044326578	-60.32423654356343	120638
81372c291340b532aa804ec42cc20ff222e915d6	iris recognition based on hilbert-huang transform	hilbert huang transform;main frequency center;hilbert huang transform hht;iris recognition;empirical mode decomposition emd;journal;g120 applied mathematics	As a reliable approach for human identification, iris recognition has received increasing attention in recent years. This paper proposes a new analysis method for iris recognition based on Hilbert–Huang transform (HHT). We first divide a normalized iris image into several subregions. Then the main frequency center information based on HHT of each subregion is employed to form the feature vector. The proposed iris recognition method has nice properties, such as translation invariance, scale invariance, rotation invariance, illumination invariance and robustness to high frequency noise. Moreover, the experimental results on the CASIA iris database which is the largest publicly available iris image data sets show that the performance of the proposed method is encouraging and comparable to the best iris recognition algorithm found in the current literature.	algorithm;feature vector;hilbert–huang transform;iris recognition	Zhijing Yang;Zhihua Yang;Lihua Yang	2009	Advances in Adaptive Data Analysis	10.1142/S1793536909000291	computer vision;speech recognition;hilbert–huang transform;pattern recognition;iris recognition;mathematics	AI	34.15912457897806	-60.801340044885386	120777
7f66e6895e347ce7153154c66618365e4513be45	a binarization algorithm specialized on document images and photos	pixel gray scale histograms systems engineering and theory robustness digital cameras algorithm design and analysis graphics equations transforms;historical documents binarization algorithm document images document photos;document image processing	In this paper, a new method for document images or photos binarization is presented. The method is simple, fast and robust and appropriate for normal as well as for special cases of documents like photos, historical documents etc. The proposed method is applied to problematic cases of documents and it is compared to other traditional methods.	algorithm;binary image;historical document;robustness (computer science)	Ergina Kavallieratou	2005	Eighth International Conference on Document Analysis and Recognition (ICDAR'05)	10.1109/ICDAR.2005.1	computer vision;computer science;information retrieval;computer graphics (images)	Vision	37.443743917457134	-65.77586276154769	120864
af8e22ef8c405f9cc9ad26314cb7a9e7d3d4eec2	a new facial expression recognition based on curvelet transform and online sequential extreme learning machine initialized with spherical clustering	facial expression recognition;online sequential extreme learning machine;local curvelet transform;makale bilimsel dergi makalesi cok yazarli;spherical clustering;firat universitesi kutuphanesi teknoloji	In this paper, a novel algorithm is proposed for facial expression recognition by integrating curvelet transform and online sequential extreme learning machine (OSELM) with radial basis function (RBF) hidden node having optimal network architecture. In the proposed algorithm, the curvelet transform is firstly applied to each region of the face image divided into local regions instead of whole face image to reduce the curvelet coefficients too huge to classify. Feature set is then generated by calculating the entropy, the standard deviation and the mean of curvelet coefficients of each region. Finally, spherical clustering (SC) method is employed to the feature set to automatically determine the optimal hidden node number and RBF hidden node parameters of OSELM by aim of increasing classification accuracy and reducing the required time to select the hidden node number. So, the learning machine is called as OSELM-SC. It is constructed two groups of experiments: The aim of the first one is to evaluate the classification performance of OSELM-SC on the benchmark datasets, i.e., image segment, satellite image and DNA. The second one is to test the performance of the proposed facial expression recognition algorithm on the Japanese Female Facial Expression database and the Cohn-Kanade database. The obtained experimental results are compared against the state-of-the-art methods. The results demonstrate that the proposed algorithm can produce effective facial expression features and exhibit good recognition accuracy and robustness.	benchmark (computing);cluster analysis;coefficient;curvelet;experiment;histogram equalization;image segmentation;k-nearest neighbors algorithm;network architecture;radial (radio);radial basis function;viola–jones object detection framework	Aysegül Uçar;Yakup Demir;Cüneyt Güzelis	2014	Neural Computing and Applications	10.1007/s00521-014-1569-1	computer vision;machine learning;pattern recognition;mathematics	ML	34.57045151691387	-59.43765564205956	121006
888e6e6e7bf06a962e9737f591e18fb8620cb6e1	quaternion decomposition based discriminant analysis for color face recognition	eigenvalues and eigenfunctions;color;training;face recognition;image color analysis;feature extraction;quaternions	In this paper, we propose a novel quaternion decomposition based discriminant analysis (QDDA) method for color face recognition. Unlike traditional approaches that handle color face images by vector representation or by each color channel individually, QDDA makes use of the quaternion to encode all color channels such that we can process all these channels in a holistic way and consider their relations simultaneously. In order to extract more discriminant color information from the image, a decomposition operation is performed to the quaternion matrix. A linear discriminant analysis is finally implemented to the obtained subcomponents for feature extraction. Experimental results have demonstrated the effectiveness of QDDA by comparing with other quaternion based methods.	channel (digital image);color image;encode;facial recognition system;feature extraction;holism;linear discriminant analysis;pixel	Rushi Lan;Yicong Zhou	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844504	facial recognition system;computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;mathematics;quaternion	Robotics	34.78356827314422	-57.589097746206946	121007
070d12268ad3b146294c3802558373d15c3cc754	writer identification using a hybrid method combining gabor wavelet and mesh fractal dimension	writer identification;fractal dimension;hybrid method;pattern recognition;gabor wavelets	Writer identification is to determine the writer from unknown handwritings, which has been becoming an active research topic in pattern recognition field. This paper proposes a new hybrid method combining Gabor function and mesh fractal dimension for off-line, text-independent writer identification. Compared to the existing Gabor-based method for off-line, text-independent writer identification, this new method is capable of extracting more useful features to distinguish the handwritings. Experimental results show that this new method can achieve much better identification results than the existing Gabor-based method.	fractal dimension;gabor wavelet	Jianjun Zhang;Zhenyu He;Yiu-ming Cheung;Xinge You	2009		10.1007/978-3-642-04394-9_65	computer vision;speech recognition;computer science;pattern recognition;fractal dimension;gabor wavelet	Robotics	33.89914159554411	-63.39484782741365	121024
9ab16fbe9d11e9e3c9547dd6d0a62c9306779dc5	face recognition using support vector model classifier for user authentication	e commerce;local binary pattern;wavelet transforms;face recognition;svm	We present an online SVM-based face-recognition system using user facial features.The Olivetti, NCKU, and FERET Research Lab database of user facial features were used.The global precision of face recognition was over 97% with cross-validation scheme.Our scheme provided a higher precision of face recognition than that of the existing schemes (89%). Most existing user authentication approaches for detecting fraud in e-commerce applications have focused on Secure Sockets Layer (SSL)-based authentication to inspect a username and a password from a server, rather than the inspection of personal biometric information. Because of the lack of support for mutual authentication or two-way authentication between a consumer and a mercantile agent, one-way SSL authentication cannot prevent man-in-the-middle attacks. In practice, in user authentication systems, machine learning and the generalisation capability of support vector models (SVMs) are used to guarantee a small classification error. This study developed an online face-recognition system by training an SVM classifier based on user facial features associated with wavelet transforms and a spatially enhanced local binary pattern. A cross-validation scheme and SVMs associated with the Olivetti Research Laboratory database of user facial features were used for solving classification precision problems. Experimental results showed that the classification error decreased with an increase in the size of the training samples. By using the aggregation of both the low-resolution and the high-resolution face image samples, the global precision of face recognition was over 97% with tenfold cross-validation scheme for an image data size of 168 and 341, respectively. Overall, the proposed scheme provided a higher precision of face recognition compared with the average precision for low-resolution face image (approximately 89%) of the existing schemes.		Wen-Hui Lin;Ping Wang;Chen-Fang Tsai	2016	Electronic Commerce Research and Applications	10.1016/j.elerap.2016.01.005	e-commerce;facial recognition system;support vector machine;local binary patterns;computer science;machine learning;pattern recognition;data mining;wavelet transform	HCI	29.770720658049594	-63.105616894934776	121549
7ad0796aa921b915e96f27fa1dbb9fc89e77c799	a new problem in face image analysis - finding kinship clues for siblings pairs		Human face conveys to other human beings, and potentially to computers, much information such as identity, emotional states, intentions, age and attractiveness. Among this information there are kinship clues. Face kinship signals, as well as the human capabilities of capturing them, are studied by psychologist and sociologists. In this paper we present a new research aimed at analyzing, with image processing/pattern analysis techniques, facial images for detecting objective elements of similarity between siblings. To this end, we have constructed a database of high quality pictures of pairs of siblings, shot in controlled conditions, including frontal, profile, expressionless and smiling face images. A first analysis of the database has been performed using a commercial identity recognition software. Then, for discriminating siblings, we combined eigenfaces, SVM and a feature selection algorithm, obtaining a recognition accuracy close to that of a human rating panel.	computer;display resolution;eigenface;feature selection;image analysis;image processing;pattern recognition;selection algorithm;sensor	Andrea Bottino;Matteo De Simone;Aldo Laurentini;Tiago F. Vieira	2012				Vision	29.80659491243454	-59.89368864081307	121618
edfce091688bc88389dd4877950bd58e00ff1253	a talking profile to distinguish identical twins	databases;image motion analysis;video signal processing;image classification;psychology;probes;accuracy;face recognition;face recognition talking profile identical twin biometric youtube dataset motion speed video frame rate exceptional motion reporting model emrm approach face classification identity signature facial motion;statistics;face;face face recognition databases accuracy probes psychology statistics;video signal processing face recognition image classification image motion analysis	Identical twins pose a great challenge to face recognition due to high similarities in their appearances. Motivated by the psychological findings that facial motion contains identity signatures and the observation that twins may look alike but behave differently, we develop a talking profile to use the identity signatures in the facial motion to distinguish between identical twins. The talking profile for a subject is defined as a collection of multiple types of usual face motions from the video. Given two talking profiles, we compute the similarities of all same type of face motion in both profiles and then perform the classification based on those similarities. Our approach, named Exceptional Motion Reporting Model (EMRM), is unrelated with appearance, and can handle realistic facial motion in human subjects, with no restrictions of speed of motion, or video frame rate. The experimental results on a video database containing 39 pairs of twins demonstrate that identical twins can be distinguished by their talking profiles. Moreover, we also apply our approach on non-twin population on a moderate YouTube dataset, with results verifying that the talking profile can be the potential biometric.	authentication;biometrics;facial recognition system;signature;twin	Xiang Lin;Hossein Nejati;Lewis Foo;Keng Teck Ma;Dong Guo;Terence Sim	2013	2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)	10.1109/FG.2013.6553700	psychology;computer vision;speech recognition;three-dimensional face recognition;communication	Vision	29.416064821394837	-61.14546530161625	121638
a021b2bc1ec07ca751f6ecec5527a4205f843e1b	fast lidar-based road detection using fully convolutional neural networks		In this work, a deep learning approach has been developed to carry out road detection using only LIDAR data. Starting from an unstructured point cloud, top-view images encoding several basic statistics such as mean elevation and density are generated. By considering a top-view representation, road detection is reduced to a single-scale problem that can be addressed with a simple and fast fully convolutional neural network (FCN). The FCN is specifically designed for the task of pixel-wise semantic segmentation by combining a large receptive field with high-resolution feature maps. The proposed system achieved excellent performance and it is among the top-performing algorithms on the KITTI road benchmark. Its fast inference makes it particularly suitable for real-time applications.	algorithm;artificial neural network;benchmark (computing);convolutional neural network;deep learning;graphics processing unit;high- and low-level;home automation;image resolution;map;pixel;point cloud;real-time clock;real-time computing	Luca Caltagirone;Samuel Scheidegger;Lennart Svensson;Mattias Wahde	2017	2017 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2017.7995848	computer vision;simulation;computer science;machine learning;pattern recognition	Robotics	28.61637437080067	-53.15594707657092	122280
7a1a25309c11c063d275978cd5996dbf0900bdf2	features extraction techniques for pollen grain classification	plant biometric;pollen grain identification;pattern recognition;palynology	An extensive study on pollen grain identification is presented in this work. A combination of geometrical and texture characteristics is proposed as pollen grain discriminative features as well as the usage of the most popular feature extraction techniques. Multi-Layer Neural Network and Least Square Support Vector Machines (LS-SVM) with Radial Basis Function were used as classifier systems. K-fold and hold-out cross-validation techniques were applied in order to achieve reliable results. When testing with a 17-species database, the combination of the proposed set of features processed by Linear Discriminant Analysis and the LS-SVM has provided the best performance, reaching a 94.92%±0.61 of success rate. Subsequently, the combination of both classifier methods provided better results, achieving 95.27%±0.49 of accuracy.	pollen	Marcos del Pozo-Baños;Jaime Roberto Ticay-Rivas;Jesús B. Alonso;Carlos Manuel Travieso-González	2015	Neurocomputing	10.1016/j.neucom.2014.05.085	palynology;computer science;pattern recognition	DB	31.852610189185246	-61.24533213765325	122413
4919663c62174a9bc0cc7f60da8f96974b397ad2	human age estimation using enhanced bio-inspired features (ebif)	databases;biologically inspired features;asm;aging;gabor filters;facial feature extraction;image enhancement;face recognition;shape;estimation;muscles drop;wrinkles;feature extraction;medical image processing;age estimation;hair whitening;facial feature extraction hair whitening muscles drop wrinkles biologically inspired features bif human age estimation image enhancement;facial features;medical image processing face recognition feature extraction image enhancement;face;asm automatic age estimation facial feature extraction;humans;bif;face estimation shape gabor filters aging databases humans;active shape model;automatic age estimation;human age estimation	The Aging process is a non-reversible process, causing human face characteristics to change over time as hair whitening, muscles drop and wrinkles. Recently, age estimation from facial images has emerged as a prominent research area. One of the most successful works is based on biologically inspired features (BIF). In this paper we extend BIF by incorporating fine detailed facial features, automatic initialization using active shape models and analyzing a more complete facial area by including the forehead details. Furthermore, we combine regression-based and classification-based models and test them experimentally on standard datasets showing the superiority of our proposed algorithm (extended BIF - EBIF) over the state-of-the-art methods.	algorithm;british informatics olympiad;decorrelation;experiment	Mohamed Y. Eldib;Motaz El-Saban	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5651440	active shape model;facial recognition system;face;computer vision;estimation;speech recognition;feature extraction;shape;computer science;pattern recognition;mathematics	Vision	32.248300527101506	-60.07923909944522	122447
9215f2482c54cca36528de99d6aeb321a45dad93	object representation based on contour features and recognition by a hopfield-amari network	object representation;contour feature;shape recognition;optimized threshold;contour recognition;curve bend function;neural network	In this paper, a type of Hopfield-Amari neural network is built based on a so-called curve bend function (CBF) for recognition of planar shapes (contours). Two kinds of features, the real-valued features and binary features, are defined by means of the CBF for given contours to characterize the shapes. The overlap between features are reduced effectively in the process of a network construction. The experimental results demonstrate that the proposed system is powerful and reliable in solving shape recognition problems.	hopfield network	Alan M. N. Fu;Hong Yan	1997	Neurocomputing	10.1016/S0925-2312(97)00026-X	computer vision;computer science;machine learning;pattern recognition;mathematics;artificial neural network	Vision	32.48888885061531	-60.86622182030153	122950
0a974c5f01866fce45d800558b14ffdff9a9e747	finding the best feature detector-descriptor combination	detectors;daisy descriptor feature detector descriptor combination image correspondence problem feature matching computer vision 3d inference dtu robot dataset mser difference of gaussian detectors sift;gaussian processes;combined descriptor detector evaluation;image matching;interest points;geometry;layout;combined descriptor detector evaluation interest point detector interest point descriptor feature evaluation;image representation feature extraction gaussian processes image matching;interest point descriptor;image representation;feature extraction;pixel;robots;feature evaluation;interest point detector;feature extraction detectors geometry pixel layout robots cameras;cameras	Addressing the image correspondence problem by feature matching is a central part of computer vision and 3D inference from images. Consequently, there is a substantial amount of work on evaluating feature detection and feature description methodology. However, the performance of the feature matching is an interplay of both detector and descriptor methodology. Our main contribution is to evaluate the performance of some of the most popular descriptor and detector combinations on the DTU Robot dataset, which is a very large dataset with massive amounts of systematic data aimed at two view matching. The size of the dataset implies that we can also reasonably make deductions about the statistical significance of our results. We conclude, that the MSER and Difference of Gaussian (DoG) detectors with a SIFT or DAISY descriptor are the top performers. This performance is, however, not statistically significantly better than some other methods. As a byproduct of this investigation, we have also tested various DAISY type descriptors, and found that the difference among their performance is statistically insignificant using this dataset. Furthermore, we have not been able to produce results collaborating that using affine invariant feature detectors carries a statistical significant advantage on general scene types.	computer vision;correspondence problem;daisy digital talking book;difference of gaussians;feature detection (computer vision);feature detection (web development);maximally stable extremal regions;scale-invariant feature transform;sensor	Anders Lindbjerg Dahl;Henrik Aanæs;Kim Steenstrup Pedersen	2011	2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission	10.1109/3DIMPVT.2011.47	computer vision;machine learning;pattern recognition;mathematics;feature	Vision	33.98897529318744	-52.67571310114872	122998
0c53ef79bb8e5ba4e6a8ebad6d453ecf3672926d	weakly supervised patchnets: describing and aggregating local patches for scene recognition	image recognition;image coding;feature extraction image recognition image representation neural nets probability;training;semantics;image recognition image coding visualization semantics feature extraction dictionaries training;visualization;feature extraction;dictionaries;image representation scene recognition patchnet vsad semantic codebook;weakly supervised patchnet local patches scene recognition image recognition hybrid representation cnn convolutional neural networks patch level feature extraction hybrid visual representation vsad robust feature representations semantic probabilities	Traditional feature encoding scheme (e.g., Fisher vector) with local descriptors (e.g., SIFT) and recent convolutional neural networks (CNNs) are two classes of successful methods for image recognition. In this paper, we propose a hybrid representation, which leverages the discriminative capacity of CNNs and the simplicity of descriptor encoding schema for image recognition, with a focus on scene recognition. To this end, we make three main contributions from the following aspects. First, we propose a patch-level and end-to-end architecture to model the appearance of local patches, called PatchNet. PatchNet is essentially a customized network trained in a weakly supervised manner, which uses the image-level supervision to guide the patch-level feature extraction. Second, we present a hybrid visual representation, called VSAD, by utilizing the robust feature representations of PatchNet to describe local patches and exploiting the semantic probabilities of PatchNet to aggregate these local patches into a global representation. Third, based on the proposed VSAD representation, we propose a new state-of-the-art scene recognition approach, which achieves an excellent performance on two standard benchmarks: MIT Indoor67 (86.2%) and SUN397 (73.0%).	aggregate data;artificial neural network;cell hybridization;class;computer vision;convolutional neural network;customize;end-to-end principle;feature extraction;line code;name;neural network simulation;probability;scale-invariant feature transform	Zhe Wang;Limin Wang;Yali Wang;Bowen Zhang;Yu Qiao	2017	IEEE Transactions on Image Processing	10.1109/TIP.2017.2666739	computer vision;feature detection;visualization;feature;feature extraction;computer science;machine learning;pattern recognition;semantics;feature	Vision	26.2709257205072	-52.347218843776325	123082
c2b8b49526e3dd537b641a6495e49a3d1a0ebbf2	extended feature-fusion guidelines to improve image-based multi-modal biometrics	palmprint;multi modal biometrics;guidelines;fingerprint;face;feature level fusion	The feature-level, unlike the match score-level, lacks multi-modal fusion guidelines. This work demonstrates a practical approach for improved image-based biometric feature-fusion. The approach extracts and combines the face, fingerprint and palmprint at the feature-level for improved human identification accuracy. Feature-fusion guidelines, proposed in recent work, are extended by adding the palmprint modality and the support vector machine classifier. Guidelines take the form of strengths and weaknesses as observed in the applied feature processing modules during preliminary experiments. The guidelines are used to implement an effective biometric fusion system at the feature-level to reduce the equal error rate on the SDUMLA and IITD datasets, using a novel feature-fusion methodology.	biometrics;experiment;fingerprint;modal logic;support vector machine	Dane Brown;Karen Bradshaw	2016		10.1145/2987491.2987512	speech recognition;engineering;pattern recognition;data mining	Mobile	29.737631245523144	-61.89406588923253	123230
df0ea0a6422d3cad7058b8e252cec820522de556	high resolution feature extraction from optical coherence tomography acquired internal fingerprint	enhancement;image matching;skin;biometrics;gabor filters;internal fingerprint;optical coherence tomography;fingerprint recognition;three dimensional displays;feature extraction;fingers;curve fitting	Biometric fingerprint scanners scan the external skin features onto a 2D image. The performance of the automatic fingerprint identification system suffers if the finger skin is wet, worn out, fake fingerprint is used et cetera. Swept source optical coherence tomography (OCT) can be used to scan the internal skin features, up to the depth of the papillary layer. OCT is contactless and scans in three dimensions. The papillary contour represents an internal fingerprint, which does not suffer external skin problems. In this paper, we present a feature extraction method that extracts features at high resolution from the internal fingerprint. First curvature of an internal fingerprint cross-section is removed by fitting a third order polynomial and shifting each column in depth by the value of the fitted curve. A 2D image of the internal fingerprint is formed by concatenating the individual cross-sections, averaged across the papillary contour. The internal fingerprint image is then enhanced and features are extracted at high resolution. We have evaluated performance of feature extraction by matching extracted minutiae to those extracted manually. Matching accuracy shows that features can be extracted at high resolution from an OCT internal fingerprint.	automated fingerprint identification;biometrics;ct scan;concatenation;contactless smart card;fits;feature extraction;fingerprint recognition;image resolution;minutiae;pixel;polynomial;tomography	Rethabile Khutlang;Nontokozo P. Khanyile;Sisanda Makinana;Fulufhelo Vincent Nelwamondo	2016	2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)	10.1109/SNPD.2016.7515971	computer vision;speech recognition;feature extraction;computer science;skin;fingerprint recognition;biometrics;statistics;curve fitting	Vision	32.45677575063969	-62.8867579483789	123333
fe980cec19d2275fd73f0de5b652643d0545a9f6	improving colour iris segmentation using a model selection technique	model selection;t technology;hog;colour iris segmentation	Analysis of circle and ellipse based iris segmentation models.A novel model selection method to improve colour iris segmentation.Showing the effectiveness of HOG feature for model selection.Analysis of the experimental results on both mobile and static camera data. In this paper, we propose a novel method to improve the reliability and accuracy of colour iris segmentation for captures both from static and mobile devices. Our method is a fusion strategy based on selection among the segmentation outcomes of different segmentation methods or models. First, we present and analyse an iris segmentation framework which uses three different models to show that improvements can be obtained by selection among the outcomes generated by the three models. Then, we introduce a model selection method which defines the optimal segmentation based on a ring-shaped region around the outer segmentation boundary identified by each model. We use the histogram of oriented gradients (HOG) as features extracted from the ring-shaped region, and train a SVM-based classifier which provides the selection decision. Experiments on colour iris datasets, captured by mobile devices and static camera, show that the proposed method achieves an improved performance compared to the individual iris segmentation models and existing algorithms.	model selection	Yang Hu;Konstantinos Sirlantzis;W. Gareth J. Howells	2015	Pattern Recognition Letters	10.1016/j.patrec.2014.12.012	computer vision;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation;model selection;statistics	Vision	33.628093546022576	-56.837531648977304	123584
ef22f0241394450966bc7bc92eaf3b7e179d71b6	image tampering detection based on local texture descriptor and extreme learning machine	databases;splicing;histograms;watermarking;forgery;image color analysis;feature extraction	This paper proposed a new image tampering detection method based on local texture descriptor and extreme learning machine (ELM). The image tampering includes both splicing and copy-move forgery. First, the image was decomposed into three color channels (one luminance and two Chroma), and each channel was divided into non-overlapping blocks. Local textures in the form of local binary pattern (LBP) were extracted from each block. The histograms of the patterns of all the blocks were concatenated to form a feature vector. The feature vector was then fed to an ELM for classification. The ELM is a powerful and fast classification approach. The experiments was performed using two publicly available databases. The experimental results showed that the proposed method achieved a high detection accuracy in both the databases.	algorithm;authentication;binary pattern (image generation);channel (digital image);color space;concatenation;database;elm;experiment;feature selection;feature vector;local binary patterns;statistical classification	Musaed Alhussein	2016	2016 UKSim-AMSS 18th International Conference on Computer Modelling and Simulation (UKSim)	10.1109/UKSim.2016.39	computer vision;local binary patterns;pattern recognition;data mining;mathematics	Vision	35.50131081035984	-60.75260160578981	123703
8d8f96a9adb7a2624be0f07056a9e0f845a6178e	neural network-based text location for news video indexing	database indexing;video databases;neural nets;histogram analysis news video indexing text location retrieval video clips multimedia databases automatic annotation textual information texture discrimination filters text regions;neural nets multimedia databases database indexing video databases;video indexing;neural networks indexing pixel text recognition filters image recognition laboratories information retrieval multimedia databases histograms;automatic annotation;indexation;multimedia databases;multimedia database;neural network	The retrieval of video clips from multimedia databases has been increasingly spotlighted. Texts in videos include useful information for automatic annotation or indexing. Text location is the first step for recognizing the textual information. This paper proposes a neural network-based text location method for news video indexing. Text can be characterized by texture, location, alignment, and font size. The proposed method classifies text pixels and non-text pixels using a network that operates as a set of texture discrimination filters. We find and locate text regions using histogram analysis after removing errors in the classification results. Experimental results show that the proposed method is effective at locating texts.	artificial neural network	Ki-Young Jeong;Keechul Jung;Eun Yi Kim;Hang Joon Kim	1999		10.1109/ICIP.1999.817127	database index;computer science;machine learning;data mining;world wide web;information retrieval;artificial neural network	Vision	35.33538564440832	-65.00104133726121	123767
0de97a6139a6ce9dc6297593ed13bbbf8039b9f9	multimodal video classification with stacked contractive autoencoders	deep learning;video classification;multimodal;stacked contractive autoencoder	In this paper we propose a multimodal feature learning mechanism based on deep networks (i.e., stacked contractive autoencoders) for video classification. Considering the three modalities in video, i.e., image, audio and text, we first build one Stacked Contractive Autoencoder (SCAE) for each single modality, whose outputs will be joint together and fed into another Multimodal Stacked Contractive Autoencoder (MSCAE). The first stage preserves intra-modality semantic relations and the second stage discovers inter-modality semantic correlations. Experiments on real world dataset demonstrate that the proposed approach achieves better performance compared with the state-of-the-art methods. HighlightsA two-stage framework for multimodal video classification is proposed.The model is built based on stacked contractive autoencoders.The first stage is single modal pre-training.The second stage is multimodal fine-tuning.The objective functions are optimized by stochastic gradient descent.	multimodal interaction	Yanan Liu;Xiaoqing Feng;Zhiguang Zhou	2016	Signal Processing	10.1016/j.sigpro.2015.01.001	computer vision;computer science;artificial intelligence;machine learning;multimodal interaction;mathematics;deep learning	ML	26.039203914175083	-52.983957053992874	123819
48c2dad8a3a22ba9495248e8df4a62b3997c8c84	chain code histogram based facial image feature extraction under degraded conditions		In this work, we have introduceda feature extraction method based on Chain Code Histogram (CCH) for facial images. A face veri- fication (FV) system has been developed by using CCH feature. The performance of the above system is comparable with that of subspace analysis methods, i.e. Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) under degraded condition. All the exper- imental results are shown upon a subset of IITG MV multi-biometric database which is a real time degraded office environment database. Fi- nally, a better improved verification performance is reported by using the combination of both features which strongly validates the different information representation of both methods.	chain code;feature extraction	Soyuj Kumar Sahoo;Jitendra Jain;S. R. Mahadeva Prasanna	2011		10.1007/978-3-642-22720-2_33	computer vision;speech recognition;computer science;pattern recognition	ML	33.17617685637347	-59.25469721382954	123879
00869507c550e2d840d730b04ca1e4346f205da0	learning invariant features through topographic filter maps	topographic filter maps;learning (artificial intelligence);invariant feature vectors;quashing function;quantization;image recognition;feature extraction;image classification;spatial pooling operation;object recognition;spaced image patches;generic supervised classifier;learned feature descriptors;linear transformation;topographic map;filter bank;robustness;detectors;learning artificial intelligence	Several recently-proposed architectures for high-performance object recognition are composed of two main stages: a feature extraction stage that extracts locally-invariant feature vectors from regularly spaced image patches, and a somewhat generic supervised classifier. The first stage is often composed of three main modules: (1) a bank of filters (often oriented edge detectors); (2) a non-linear transform, such as a point-wise squashing functions, quantization, or normalization; (3) a spatial pooling operation which combines the outputs of similar filters over neighboring regions. We propose a method that automatically learns such feature extractors in an unsupervised fashion by simultaneously learning the filters and the pooling units that combine multiple filter outputs together. The method automatically generates topographic maps of similar filters that extract features of orientations, scales, and positions. These similar filters are pooled together, producing locally-invariant outputs. The learned feature descriptors give comparable results as SIFT on image recognition tasks for which SIFT is well suited, and better results than SIFT on tasks for which SIFT is less well suited.	computer vision;edge detection;feature extraction;machine learning;map;nonlinear system;outline of object recognition;quantization (signal processing);sensor;topography;unsupervised learning	Koray Kavukcuoglu;Marc'Aurelio Ranzato;Rob Fergus;Yann LeCun	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206545	computer vision;topographic map;detector;contextual image classification;quantization;feature extraction;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;filter bank;linear map;robustness	Vision	31.890514547256164	-54.77943836678189	124039
0dd569d1b8116e56f4f43146f2ea6289991daeeb	classification of coral reef submarine images and videos using a novel z with tilted z local binary pattern (z⊕tzlbp)	classification;coral reef;feature extraction;active contour;clahe;knn;fcm	In this paper, a novel feature descriptor named Z with Tilted Z Local Binary Pattern (Z⊕TZLBP) is proposed for extracting coral reef image features efficiently. The aim is to reduce LBP’s computational complexity by reducing the size of the feature vector. This is achieved in the proposed Z⊕TZLBP by dividing the neighborhood pixels into two non overlapped groups of Z and TZ (Tilted Z), and computing LBP wherein the centre pixel is also treated as one of the neighbors. KNN classification with four different distance metrices has been used for classification purpose. Metric F-measure is used to evaluate the performance of the proposed system. Experiments conducted with various coral image and video data sets show that the proposed feature descriptor outperforms Local Binary Pattern (LBP), Uniform Pattern, Center-Symmetric Local Binary Pattern and Orthogonal Combination of Local Binary Pattern and also guarantees accurate and efficient feature extraction.		N. Ani Brown Mary;Dejey Dharma	2018	Wireless Personal Communications	10.1007/s11277-017-4981-x	feature (computer vision);adaptive histogram equalization;computer science;pixel;local binary patterns;feature extraction;feature vector;computer vision;active contour model;data set;artificial intelligence;pattern recognition	Vision	36.331149532038296	-59.49950576518178	124048
0d47297df69c67683fc97cfa4a831c6a2881282f	salient target detection based on the combination of super-pixel and statistical saliency feature analysis for remote sensing images		The saliency analysis has become the important tool to detect the salient targets. However, due to complex target features and abundant background information interference, the traditional models are weak in salient target detection of remote sensing images. In this paper, a novel model based on the combination of super-pixel and statistical saliency feature analysis is proposed. The proposed model consists of three main steps. First, the statistical saliency feature map based on histogram statistical saliency analysis in the Lab color space is introduced. Then, information saliency feature map is obtained based on the combination of super-pixel segmentation and information entropy, and the statistical saliency feature map and the information saliency feature map are fused and enhanced to generate the final saliency map. Finally, the complete and accurate salient targets and regions of interest (ROIs) are obtained based on the improved Otsu segmentation method. Experimental evaluations show that the proposed model outperforms the state-of-the-art salient detection models.		Libao Zhang;Yue Wang;Yang Sun	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451210	entropy (information theory);pixel;computer vision;image segmentation;pattern recognition (psychology);remote sensing;salience (neuroscience);artificial intelligence;feature extraction;pattern recognition;histogram;lab color space;computer science	Robotics	37.949246366124456	-54.43161540759335	124091
c2dec4cc53da3ca69d57545984c82334309c1260	neural network based face recognition with moment invariants	image segmentation radial basis function networks face recognition feature extraction zernike polynomials image classification;olivetti research laboratory;image segmentation;image classification;radial basis function networks;face recognition;radial basis function;feature extraction;rbf neural network;zernike moment;classification error;zernike polynomials neural network based face recognition moment invariants pattern features human face technology pseudo zernike moments two dimensional images zernike moments legendre moments shape information human face localization radial basis function neural network classifier rbf neural network classification error rate olivetti research laboratory face database face segmentation feature extraction;experimental evolution;zernike polynomials;neural network;moment invariant;neural networks face recognition face detection shape feature extraction humans pattern recognition image recognition error analysis data mining	This paper introduced an experimental evolution of the effectiveness of utilizing various moments as pattern features in human face technology. In this paper, we apply pseudo Zernike moments (PZM) for recognition of human faces in two-dimensional images, and we compare their performance with other type of moments. The moments that we have used are Zernike moments (ZM), pseudo Zernike moments (PZM) and Legendre moments (LM). We have used shape information for human face localization, also we have used a radial basis function (RBF) neural network as a classifier for this application. The performance of classification is dependent on the moment order as well as the type of moment invariant, but the classification error rate was below %10 in all cases. Simulation results on the face database of Olivetti Research Laboratory (ORL) indicate that high order degree pseudo Zernike moments contain very useful information about face recognition process, while low order degree moments contain information about face expression.	artificial neural network;facial recognition system	Javad Haddadnia;Karim Faez	2001		10.1109/ICIP.2001.959221	velocity moments;computer vision;contextual image classification;radial basis function;experimental evolution;feature extraction;computer science;machine learning;zernike polynomials;pattern recognition;mathematics;image segmentation	Vision	33.45435664619921	-63.30850390587428	124110
d32c93011ea04f70ff5b1b8a202864921cbca6e1	hierarchical sparse method with applications in vision and speech recognition	pooling operation;hierarchical learning;feature extraction;isolated word;sparse coding	A new approach for feature extraction using neural response has been developed in this paper through combining the hierarchical architectures with the sparse coding technique. As far as proposed layered model, at each layer of hierarchy, it concerned two components that were used are sparse coding and pooling operation. While the sparse coding was used to solve increasingly complex sparse feature representations, the pooling operation by comparing sparse outputs was used to measure the match between a stored prototype and the input sub-image. It is recommended that value of the best matching should be kept and discarding the others. The proposed model is implemented and tested taking into account two ranges of recognition tasks i.e. image recognition and speech recognition (on	adaptive filter;algorithm;computer vision;experiment;feature extraction;feature recognition;mnist database;neural coding;outline of object recognition;prototype;sparse approximation;sparse matrix;speech recognition;vocabulary	Ramadhan Abdo Musleh Alsaidi;Hong Li;Yantao Wei;Rokan Khaji;Yuan Yan Tang	2013	IJWMIP	10.1142/S0219691313500161	speech recognition;feature extraction;k-svd;computer science;machine learning;pattern recognition;sparse approximation;neural coding	ML	25.70631519548237	-53.91229100140618	124121
a93f30dfb6fd2ad6e33e8ff30608a5dda3e19b6b	building recognition on subregion's multiscale gist feature extraction and corresponding columns information based dimensionality reduction		In this paper, we proposed a new building recognition method named subregion’s multiscale gist feature (SM-gist) extraction and corresponding columns information based dimensionality reduction (CCI-DR). Our proposed building recognition method is presented as a two-stage model: in the first stage, a building image is divided into 4 × 5 subregions, and gist vectors are extracted from these regions individually. Then, we combine these gist vectors into a matrix with relatively high dimensions. In the second stage, we proposed CCI-DR to project the high dimensional manifold matrix to low dimensional subspace. Compared with the previous building recognitionmethod the advantages of our proposedmethod are that (1) gist features extracted by SM-gist have the ability to adapt to nonuniform illumination and that (2) CCI-DR can address the limitation of traditional dimensionality reduction methods, which convert gist matrices into vectors and thus mix the corresponding gist vectors from different feature maps. Our building recognition method is evaluated on the Sheffield buildings database, and experiments show that our method can achieve satisfactory performance.		Bin Li;Wei Pang;Yuhao Liu;XiangChun Yu;AnAn Du;YeCheng Zhang;Zhezhou Yu	2014	J. Applied Mathematics	10.1155/2014/898705	machine learning	Vision	34.68038641291249	-58.122344523135176	124129
e2d11f4c69162801b059f50fb61ccbd9accb944a	spatial pyramid deep hashing for large-scale image retrieval		Effective feature representations and similarity measurements are crucial for large-scale image retrieval, and conventional methods often learn hash functions from a predefined hand-crafted feature space. Meanwhile, the spatial structure in raw images always lost in most previous methods. Encouraged by the recent advances in convolutional neural networks(CNNs), a novel Spatial Pyramid Deep Hashing(SPDH) algorithm is developed for the task of fast image retrieval. In our SPDH algorithm, the CNN with a spatial pyramid pooling and a locally-connected layer with binary activation functions is utilized to build the end-to-end relation between the raw image data and the binary hashing codes for fast indexing. Different from the fully-connected layer, the locally-connected layer can consider each local spatial bin as an independent unit and only connect the local bin to preserve the spatial pyramid structure for hash codes. The learning of both the hash function and the feature representations are jointly optimized via backward propagation with classification or similarity loss function on the large-scale labeled dataset such as ImageNet. Moreover, a spatial pyramid binary pattern matching algorithm is developed to achieve partial local similar matching among the images. Our experimental results have shown that our SPDH method can outperform several state-of-the-art hashing algorithms on the CIFAR-10, SIVAL and the Oxford buildings datasets.	algorithm;alpha compositing;binary pattern (image generation);bridging (networking);code;convolutional neural network;cryptographic hash function;database;deep learning;end-to-end encryption;feature vector;hash function;image retrieval;imagenet;information retrieval;loss function;pattern matching;pyramid (geometry);raw image format;regular expression;self-propelled particles;software propagation;lsh	Wanqing Zhao;Hangzai Luo;Jinye Peng;Jianping Fan	2017	Neurocomputing	10.1016/j.neucom.2017.03.021	artificial intelligence;convolutional neural network;machine learning;bin;binary pattern;hash function;image retrieval;search engine indexing;feature hashing;pattern recognition;feature vector;computer science	Vision	26.00424675295895	-52.74065424841733	124130
65bcd500182ef2ccb66bf5b195eff0529aeec674	indian sign language recognition: an approach based on fuzzy-symbolic data	key frame;fuzzy membership functions and symbolic data;sign language;hearing impaired;sign boundary	In this paper, the task of recognizing signs made by hearing impaired people at sentence level has been addressed. A novel method of detecting sign boundaries in a video of continuous signs has been proposed and extraction of spatial features to capture hand movements of a signer through fuzzy membership functions has been proposed. Frames of a given video of a sign are preprocessed to extract face and hand components of a signer. The centroids of the extracted components are exploited to extract spatial features. The concept of interval valued type symbolic data has been explored to capture variations in the same sign made by the different signers at different instances of time. A suitable symbolic similarity measure is studied to establish matching between reference and test signs and a simple nearest neighbor classifier is used to recognize an unknown sign as one among the known signs by specifying a desired level of threshold. An extensive experimentation is conducted on a significantly large corpus of Indian regional signs created by us during the course of our research work in order to evaluate the performance of the proposed system.	experiment;nearest neighbour algorithm;semantic similarity;sensor;similarity measure;text corpus	H. S. Nagendraswamy;B. M. Chethana Kumara;R. Lekha Chinmayi	2016	2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2016.7732176	speech recognition;sign language;computer science;artificial intelligence	Vision	28.462719974676894	-65.25500589208198	124199
982d4f1dee188f662a4b5616a045d69fc5c21b54	learning to link human objects in videos and advertisements with clothes retrieval	videos advertising feature extraction receivers object detection internet electronic mail;human part features extraction human objects linking videos clothes retrieval human object level video advertising content relevant ads video stream support content relevant advertising discriminatively trained part human objects detection ads selection human clothing advertising deep convolutional neural network cnn face features human gender recognition human parts alignment;video streaming advertising clothing face recognition feature extraction image retrieval learning artificial intelligence object detection	In this paper, we present a new method for human object-level video advertising. A framework that aims to embed content-relevant ads within a video stream is investigated in this context. In particular, to support content-relevant advertising, we employ the discriminatively trained part based model to detect human objects in a video and then select the ads that are related to the detected human objects. For human clothing advertising, we design a deep Convolutional Neural Network (CNN) using face features to recognize human genders in a video stream. Human parts alignment is then implemented to extract human part features that are used for clothes retrieval. Our novel framework is examined in various types of videos. Experimental results demonstrate the effectiveness of the proposed method for human object-level video advertising.	convolutional neural network;discriminative model;streaming media;video	Haijun Zhang;Shuang Wang;Xiong Cao;Heng Yue;Ke Wang	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727859	computer vision;object-class detection;computer science;multimedia	Vision	33.39603638088627	-52.45647008000944	124454
94aca9db81d1f64dfffd5e9196445b3f30c0dcde	rehashing for bayesian geometric hashing	bayesian voting scheme;bayesian geometric hashing;bayesian approach;hash table size;hash table;computational performance;recognition rate;model-based recognition technique;optimal recognition rate;hash entry;hash bin;minimisation;object recognition;geometry;pattern matching	Geometric hashing is a model-based recognition technique based on matching of transformation-invariant object representations stored in a hash table. In the last decade, a number of enhancements have been suggested to the basic method improving its performance and reliability. One of the important enhancements is rehashing, improving the computational performance by dealing with the problem of non-uniform occupancy of hash bins. However, the proposed rehashing schemes aim to redistribute the hash entries uniformly, which is not appropriate for Bayesian approach, another enhancement optimizing the recognition rate in presence of noise. In this paper, we derive the rehashing for Bayesian voting scheme, thus improving the computational performance by minimizing the hash table size and the number of bins accessed, while maintaining optimal recognition rate.	computation;double hashing;geometric hashing;hash table	Michael Lifshits;Ilya Blayvas;Roman Goldenberg;Ehud Rivlin;Michael Rudzsky	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334478	minimisation;hash table;double hashing;hash function;dynamic perfect hashing;bayesian probability;computer science;theoretical computer science;cognitive neuroscience of visual object recognition;machine learning;pattern matching;universal hashing;pattern recognition;k-independent hashing;rolling hash;2-choice hashing	Vision	38.98198634025444	-57.016335488030144	124464
10dfa40a24848b15a2a589c519517738ce62fcb6	video copy detection based on deep cnn features and graph-based sequence matching	video copy detection;convolution neural networks;deep learning;computer vision	This paper introduces a novel content-based video copy detection method using the deep CNN features. An efficient deep CNN feature is employed to encode the image content while retaining the discrimination capability. Taking advantage of the extremely fast Euclidean distance similarity of deep CNN features, a keyframe-based copy retrieval method that exhaustively searches the copy candidates from the large keyframe database without indexing is proposed. Moreover, a graph-based sequence matching algorithm is employed to obtain the copy clips and accurately locate the video segments. The experimental evaluation has been performed to show the efficacy of the proposed deep CNN features. The promising results demonstrate the effectiveness of our proposed approach.	video copy detection	Xin Zhang;Yuxiang Xie;Xidao Luan;Jingmeng He;Lili Zhang;Lingda Wu	2018	Wireless Personal Communications	10.1007/s11277-018-5450-x	clips;computer science;real-time computing;deep learning;search engine indexing;euclidean distance;blossom algorithm;video copy detection;computer vision;artificial intelligence;graph	Embedded	36.5443641390437	-55.07374765649224	124475
17dedba7f67995e25294d298aab0da001fae6f59	a novel fast face recognition method of two-dimensional principal component analysis based on bp neural networks	two dimensional principal component analysis;image recognition;face reconstruction;neural nets;bp neural networks;training;image classification;backpropagation;two dimensional component analysis 2dpca;feature vector;artificial neural networks;support vector machines svms face recognition two dimensional component analysis 2dpca bp based neural networks face reconstruction;face recognition;fast face recognition method;bp based neural networks;feature extraction;image reconstruction;principal component analysis;principal component analysis backpropagation face recognition image classification neural nets;component analysis;face;support vector machine;support vector machines svms;face recognition principal component analysis neural networks covariance matrix image databases vectors feature extraction scattering spatial databases pattern recognition;image recognition fast face recognition method two dimensional principal component analysis bp neural networks;neural network	Two-dimensional principal component analysis technique is an important and well-developed area of image recognition and to date this method has been put forward. A new face recognition method two-dimensional principal component analysis (2DPCA) based on BP neural networks, named 2DPCA-BP method, was proposed. 2DPCA was used to obtain a family of projected feature vectors, in which face image was projected into this family of projected feature vectors to get the feature matrix. BP-based neural network was used as classifier for its good learning capability. Experiment proved that 2DPCA-BP is better than 2DPCA-SVMs in velocity and its recognition accuracy is 98.246%. The CVL database showed that the system achieved excellent performance.	backpropagation;computer vision;facial recognition system;feature extraction;feature vector;neural networks;principal component analysis;statistical classification;velocity (software development)	Wenjing Han;Jing Li;Nongliang Sun	2008	2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application	10.1109/PACIIA.2008.220	neural gas;iterative reconstruction;face;support vector machine;contextual image classification;speech recognition;feature vector;feature extraction;computer science;backpropagation;machine learning;pattern recognition;artificial neural network;principal component analysis	Vision	33.25224386314917	-58.332164928352725	124602
e9d0dc987d03204e49c268dd66e5e131a67a81ab	the research of multi-spectral hand texture fusion method under a non-constraint condition	data fusion;biometrics identification;palm vein;normalization;multispectral imaging	As different ways of Biometric Identification, palm print, palm veins and palm shapes identification have their own characteristics. The main stream of research is forming especially in the fields of none or low constraint condi- tions in this area. The palm print and shape is easy to be acquired to get pictures of high qualities by which we can make further research. However, under the same infrared band, one is hard to extract clear images of the hand veins of dif- ferent individuals. This article is aim to solve this problem. Base on a method of multiband and multispectral palm print extracting, we could reach the aim let- ting quite a few people's hand under permission. Considering the fact that the None/Low constraint condition extracting could break the parallel position be- tween the hand and the imaging sensor and cause affine deformation. Using the outline of the palm to revise and uniform the hand, we can realize the correction registration and the normalization of the palm print image. The experimental result will prove the advantage of this method we implemented.	constraint algorithm	Weiqi Yuan;Yonghua Tang;Ting Fang	2012		10.1007/978-3-642-35136-5_18	computer vision;engineering;communication;engineering drawing	Robotics	39.15059569202745	-64.26630198743128	124762
72efd2bcb28ac2b03d3ec24810e68872aac16f83	a real-time chinese traffic sign detection algorithm based on modified yolov2		Traffic sign detection is an important task in traffic sign recognition systems. Chinese traffic signs have their unique features compared with traffic signs of other countries. Convolutional neural networks (CNNs) have achieved a breakthrough in computer vision tasks and made great success in traffic sign classification. In this paper, we present a Chinese traffic sign detection algorithm based on a deep convolutional network. To achieve real-time Chinese traffic sign detection, we propose an end-to-end convolutional network inspired by YOLOv2. In view of the characteristics of traffic signs, we take the multiple 1 × 1 convolutional layers in intermediate layers of the network and decrease the convolutional layers in top layers to reduce the computational complexity. For effectively detecting small traffic signs, we divide the input images into dense grids to obtain finer feature maps. Moreover, we expand the Chinese traffic sign dataset (CTSD) and improve the marker information, which is available online. All experimental results evaluated according to our expanded CTSD and German Traffic Sign Detection Benchmark (GTSDB) indicate that the proposed method is the faster and more robust. The fastest detection speed achieved was 0.017 s per image.	algorithm;artificial neural network;benchmark (computing);computational complexity theory;computer vision;convolutional neural network;end-to-end principle;fastest;image resolution;map;norm (social);real-time clock;real-time transcription;sensor;traffic sign recognition	Jianming Zhang;Manting Huang;Xiaokang Jin;Xudong Li	2017	Algorithms	10.3390/a10040127	traffic sign recognition;convolutional neural network;machine learning;artificial intelligence;object detection;computational complexity theory;algorithm;computer science	Vision	29.005799591841853	-54.52126767962631	124971
2af680736f32ae37d579a8b5656eec1c6b158dec	biologically significant facial landmarks: how significant are they for gender classification?	image classification face recognition feature selection gender issues;image classification;face three dimensional displays feature extraction accuracy databases nose;frgc v2 biologically significant facial landmarks fully automatic gender classification algorithm 3d euclidean distance geodesic distance feature selection automatic landmark detection 3d face databases;gender issues;face recognition;feature selection	Automatic gender classification has many applications in human computer interaction. However, to determine the gender of an unseen face is challenging because of the diversity and variations in the human face. In this paper, we explore the importance of biologically significant facial landmarks for gender classification and propose a fully automatic gender classification algorithm. We extract 3D Euclidean and Geodesic distances between these landmarks and use feature selection to determine the relative importance of the biological landmarks for classifying gender. Unlike existing techniques, our algorithm is fully automatic since all landmarks are automatically detected. Experiments on one of the largest 3D face databases FRGC v2 show that our algorithm outperforms all existing techniques by a significant margin.	algorithm;cellular automaton;database;distance (graph theory);feature selection;human computer;human–computer interaction;statistical classification	Syed Zulqarnain Gilani;Faisal Shafait;Ajmal S. Mian	2013	2013 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2013.6691488	computer vision;contextual image classification;computer science;machine learning;pattern recognition;feature selection	Vision	31.50832339325806	-58.384471532328114	125028
57c7810a9e119008d82bd2714623a82ba95024be	gif image retrieval in cloud computing environment		GIF images have been used in the last years, especially on social media. Here it is explored a content-based image retrieval system to work specifically with GIF file format. Its implementation is extended to a cloud computing environment. Given the Tumblr GIF dataset, it is created a “search by example” image retrieval system. To describe the images, low-level features are used: (1) color, (2) texture and (3) shape. The system performs the search using just GIF images as query images. To obtain faster results on the retrieval process, a hashing indexing approach is used. The system showed a complexity of (O(n^2)) for indexing and O(log(n)) for retrieval. Additionally, better results were obtained (in relation to precision and recall) for simple images, instead of images with a lot of movements.	cloud computing;gif;image retrieval	Evelyn Paiz-Reyes;Nadile Nunes-de-Lima;Sule Yildirim Yayilgan	2018		10.1007/978-3-319-93000-8_30	computer vision;artificial intelligence;computer science;image retrieval;hash function;precision and recall;content-based image retrieval;search engine indexing;file format;cloud computing	Vision	38.33744012642886	-59.79697016207057	125079
5d8e785c652216698b7dda4f9932ea6504e83636	learning object-class segmentation with convolutional neural networks		After successes at image classification, segmentation is the next step towards image understanding for neural networks. We propose a convolutional network architecture that includes innovative elements, such as multiple output maps, suitable loss functions, supervised pretraining, multiscale inputs, reused outputs, and pairwise class location filters. Experiments on three data sets show that our method performs on par with current in computer vision methods with regards to accuracy and exceeds them in speed.	artificial neural network;computer vision;conditional random field;convolutional neural network;experiment;graphics processing unit;loss function;map;network architecture;parallel computing;supervised learning	Hannes Schulz;Sven Behnke	2012			computer vision;computer science;artificial intelligence;machine learning	Vision	25.86256697132019	-52.51791793552359	125202
a0d5990eb150cdcb1c8b2967e6a4fe7a5d85063b	region-aware scattering convolution networks for facial beauty prediction		This paper proposes a scattering convolutional network with region-aware facial attributes to obtain a mid-level representation for facial beauty prediction (FBP). Different from the previous works that only focus on the discriminative representation for prediction, this paper also considers the invariant properties of the facial representation that reduces the variances caused by the image transformations, such as rotations and translation. The proposed region-aware scattering convolution network (RegionScatNet) is based on a deep convolution network of scattering transforms (ScatNet) integrated with facial texture and shape features. It consists of three components, including: 1) Region Extraction to obtain the significant facial perception region with implicit shape features using region-aware mask; 2) Attributes Decomposition to separate the extracted region into detail and structure facial layers by a guided filter; 3) Scattering Convolution that computes the roto-translation invariant representation of facial detail and structure for FBP by cascading three-layer wavelets filters and non-linear modulus pooling. The comparisons with related deep learning-based methods illustrate the effectiveness of RegionScatNet for FBP. The evaluations with various prediction model (like SVR and Gaussian process) and with different facial variances (like rotaion) indicate the robustness of the RegionScatNet-based features.	convolution;deep learning;flow-based programming;gaussian process;modulus of continuity;multitier architecture;nonlinear system;wavelet	Lingyu Liang;Duorui Xie;Lianwen Jin;Jie Xu;Mengru Li;Luojun Lin	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296805	discriminative model;computer vision;wavelet;artificial intelligence;deep learning;feature extraction;invariant (mathematics);scattering;gaussian process;computer science;convolution;pattern recognition	Vision	29.5983680469194	-55.05396151072328	125561
8c1f25283166a89e5d769b850965a18680498976	age classification from facial images: is frontalization necessary?		In the majority of the methods proposed for age classification from facial images, the preprocessing steps consist of alignment and illumination correction followed by the extraction of features, which are forwarded to a classifier to estimate the age group of the person in the image. In this work, we argue that face frontalization, which is the correction of the pitch, yaw, and roll angles of the headpose in the 3D space, should be an integral part of any such algorithm as it unveils more discriminative features. Specifically, we propose a method for age classification which integrates a frontalization algorithm before feature extraction. Numerical experiments on the widely used FGnet Aging Database confirmed the importance of face frontalization achieving an average increment in accuracy of 4.43%.	algorithm;experiment;feature extraction;floor and ceiling functions;image resolution;numerical method;preprocessor;statistical classification;yaws	Aníbal Báez-Suárez;Christophoros Nikou;Juan Arturo Nolazco-Flores;Ioannis A. Kakadiaris	2016		10.1007/978-3-319-50835-1_69	artificial intelligence;computer science;pattern recognition;convolutional neural network;local binary patterns;computer vision;discriminative model;feature extraction;preprocessor;classifier (linguistics);image resolution	Vision	32.18223775302679	-59.04395586161581	125628
91464d007932cca408cf1fd340cd813a581111eb	reconnaissance automatique des gestes de la langue française parlée complétée. (automatic recognition of french cued speech gestures)		Cued Speech facilitates hearing-impaired people communication by completing lipreading. Basically, its purpose is to add manual gestures nearby the face in order to disambiguate the lip motion which is not self-sufficient for a complete understanding of the message. The goal of Telephony for Hearing IMpaired Project is to elaborate a terminal which allows communication based on French Cued Speech. Amongst the manifoldness of functionalities it requires, it is mandatory to automatically recognize French Cued Speech manual gestures. The subject of this work is the segmentation, the analysis and the recognition of Cued Speech gestures. It requires image and video processing techniques as well as data fusion, classification and gesture recognition techniques. In order to achieve this goal, we have developed several original algorithms, such as (1) a bio-inspired filter which quantifies the amount of motion in a video by integrating retinal processing, (2) a new combination technique for multi-classification via SVMs or unary classifiers based on belief theories, from which a transform from belief function to probability is derived, (3) a partial decision method based on the generalisation of the Pignistic Transform, in order to authorize some uncertainty when processing ambiguous gestures.		Thomas Burger	2007				Vision	28.128084121599056	-65.27768855098127	125663
526c79c6ce39882310b814b7918449d48662e2a9	facial expression analysis under partial occlusion	eye region occlusion;eyebrows;mouth eyes image databases humans face eyebrows glass virtual reality medical services biomedical equipment;mouth;facial expression recognition;maximum correlation classifier;image databases;correlation methods emotion recognition feature extraction wavelet transforms image classification;research outputs;image database;mouth occlusion;virtual reality;image classification;emotion recognition;expression classification accuracy;gabor filters;research publications;facial expression recognition feature extraction gabor wavelet transform confusion matrix facial expression analysis partial facial occlusion eye region occlusion eyebrow occlusion mouth occlusion expression classification accuracy gabor filters maximum correlation classifier cosine similarity measure;correlation methods;glass;facial expression analysis;wavelet transforms;feature vector;gabor filter;partial facial occlusion;eyes;medical services;feature extraction;confusion matrix;cosine similarity measure;eyebrow occlusion;face;medical application;humans;gabor wavelet transform;facial expression;classification accuracy;similarity measure;biomedical equipment	Six basic facial expressions are investigated when the human face is partially occluded, i.e. when the eyes and eyebrows or the mouth regions are occluded. Such occlusions occur when a person wears glasses (e.g. in VR application) or a mouth mask (e.g. in medical application). More specifically, we are interested in finding the part of the face that contains sufficient information in order to correctly classify these six expressions. Two facial image databases are employed in our experiments. Each image from the database is convolved with a set of Gabor filters having various orientations and frequencies. The new feature vectors are classified by using a maximum correlation classifier and the cosine similarity measure approaches. We find that, overall, the facial expression recognition method provides robustness against partial occlusion, the classification accuracy only decreasing from 89.7% (no occlusion) to 84% (eyes region occlusion) and 83.5% (mouth region occlusion) for the first database and from 94.5% (no occlusion) to 91.5% (eyes region occlusion) and 87.2% (mouth region occlusion) for the second database, respectively.	convolution;cosine similarity;database;experiment;gabor filter;regular expression;similarity measure	Ioan Buciu;Irene Kotsia;Ioannis Pitas	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1416338	face;computer vision;contextual image classification;speech recognition;confusion matrix;feature vector;feature extraction;computer science;machine learning;pattern recognition;mathematics;virtual reality;glass;facial expression;wavelet transform	Vision	32.58678282483498	-59.918883744107646	125672
ffd8fcc1914fabc6908afeb849b2dd67644ce774	learning metrics for content-based medical image retrieval	measurement biomedical imaging feature extraction vectors principal component analysis standards image retrieval;l1 distance based measures content based medical image retrieval medical image analysis active research field feature design metric design information theoretic metric learning sift bag of words based system plug in similarity imageclef 2011 benchmarking dataset;feature extraction;medical image processing;learning artificial intelligence;medical image processing benchmark testing content based retrieval feature extraction image retrieval learning artificial intelligence;content based retrieval;benchmark testing;image retrieval	Application of content-based image retrieval (CBIR) to medical image analysis has recently become an active research field. While many previous studies have focused on the feature design, the metric design, another key CBIR component, has not been well investigated in this application context. This paper presents a medical CBIR that adapts its similaritymetric from data by using information theoretic metric learning. Also we systematically compare our SIFT bag-of-words-based system with various plug-in similarity measures available in literature. The proposed systems are evaluated with the ImageCLEF-2011 benchmarking dataset. Our experimental results demonstrate the advantage of the proposed metric learning approach and L1 distance-based measures.	bag-of-words model;content-based image retrieval;context (computing);image analysis;medical image computing;plug (physical object);plug-in (computing);scale-invariant feature transform;silo (dataset);taxicab geometry;theory	John Collins;Kazunori Okada	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6610262	benchmark;computer vision;visual word;feature extraction;image retrieval;computer science;machine learning;pattern recognition;automatic image annotation;information retrieval	Vision	38.18930087594863	-62.168568983892435	125750
46b26e68c5e5074562d5c280bb29dde7c57bad7d	learning features for object recognition	object recognition;evolutionary genetics;sar image;approaches to learning;synthetic aperture radar	Features represent the characteristics of objects and selecting or synthesizing effective composite features are the key factors to the performance of object recognition. In this paper, we propose a co-evolutionary genetic programming (CGP) approach to learn composite features for object recognition. The motivation for using CGP is to overcome the limitations of human experts who consider only a small number of conventional combinations of primitive features during synthesis. On the other hand, CGP can try a very large number of unconventional combinations and these unconventional combinations may yield exceptionally good results in some cases. Our experimental results with real synthetic aperture radar (SAR) images show that CGP can learn good composite features. We show results to distinguish objects from clutter and to distinguish objects that belong to several classes.	aperture (software);clutter;computation;genetic programming;outline of object recognition;synthetic intelligence	Yingqiang Lin;Bir Bhanu	2003		10.1007/3-540-45110-2_117	computer vision;synthetic aperture radar;cognitive neuroscience of visual object recognition;machine learning;human evolutionary genetics	Vision	27.37166069580899	-56.2203649670628	125898
1243b54cccfa419746167d16a412ea3f00fa499f	towards the synergy between compression and content-based analysis: a pattern-driven approach	nonlinear filters;image coding;data compression;content based analysis;visualization;image coding feature extraction visualization maximum likelihood detection adaptation model nonlinear filters tiles;adaptation model;feature extraction;pattern driven image compression technique;image coding data compression;intensity structural discontinuities;maximum likelihood detection;structural lines;pattern based image model;tiles;homogeneous patterns;structural lines content based analysis pattern driven image compression technique intensity structural discontinuities pattern based image model complex composite patterns homogeneous patterns;complex composite patterns	This paper presents a novel pattern-driven image compression technique for exploring the synergy between content-based analysis and compression. Within the pattern-driven paradigm, image data are considered as relational and classifiable entities, which are low-level visual patterns including: (1) flat or homogeneous patterns (HP), (2) structural lines, curves and boundaries indicating the intensity/structural discontinuities (SD), and (3) complex/composite patterns (CP). Concisely, the pattern-based image model f can be loosely defined as . With such a pattern-driven model, efficient compression can be achieved by designing and developing novel ways of modeling and encoding disparate low-level visual patterns. For a given image, disparate low-level visual patterns are automatically separated, modeled, and encoded by: (1) A geometric filter, captures and models the predominant structural lines/curves (SD), (2) A linear filter, breaks various image regions into variable size triangular tiles and applies linear regression techniques to efficiently model tiles with HP, and (3) non-linear filters, model CP content such as cluttered background and textures, using advanced learning and transform based coding techniques. After all the filtering pipelines, adaptive entropy encoding schemes are employed to further reduce the bit cost by eliminating the statistical redundancies exhibited by all the features extracted and modeling parameters. The feasibility and efficiency of the proposed technique were corroborated by quantitative experiments and comparisons with the existing compression standards JPEG and JPEG2000. Our results show that the newly devised technique brings evident advantages including better support for compressed-domain analysis and more satisfactory subjective quality, mainly due to: the separation of the visual patterns allows the customization of the encoding schemes to maximize the coding efficiency using a compact set of extracted features and parameters. Furthermore, depending on the given application, different features can be prioritized and configured to match the data characteristics and the visual patterns therein for performance optimization. Since different patterns are segmented and modeled explicitly during the compression process, the proposed technique holds a great potential for achieving a good synergy between compression and compressed-domain analysis. For example, when applied to high-fidelity Ortho-imagery in GIS, city planning, and map updating applications, the structural information explicitly extracted matches the vector features such as road networks and regional boundaries. This correlation could be substantial for utilizing a new compression format with better support for compressed-domain analysis. Future research will further improve the compression efficiency and use specific applications to quantitatively demonstrate the synergy between compression and compressed-domain analysis within the pattern-driven framework.	algorithmic efficiency;data compression;domain analysis;entity;entropy encoding;experiment;geographic information system;high- and low-level;image compression;jpeg 2000;mathematical optimization;model f keyboard;nonlinear system;performance tuning;pipeline (computing);programming paradigm;software design pattern;synergy	Hai Wei;Sakina Zabuawala;Joseph Yadegar;Julio de la Cruz;Hector J. Gonzalez	2011	2011 Data Compression Conference	10.1109/DCC.2011.84	data compression;computer vision;simulation;visualization;feature extraction;computer science;statistics	Vision	26.15957811017919	-58.038323926411444	125922
1aaffc7a0adaff322f8dabc5573d1237f518169c	a multi-boosted hmm approach to lip password based speaker verification	mouth;image motion analysis;hidden markov models motion segmentation training computer vision decision support systems speech mouth;lip password;image segmentation;training;speech;speaker recognition hidden markov models image motion analysis image segmentation learning artificial intelligence;computer vision;speaker verification;speaker recognition;motion segmentation;hidden markov models;multi boosted hmms lip password lip motion speaker verification;decision support systems;multi boosted hmms;decision boundary multiboosted hmm approach lip password based speaker verification multiboosted hidden markov model lip motion segmentation password sequence boosting learning framework random subspace method data sharing scheme;learning artificial intelligence;lip motion	This paper presents a multi-boosted Hidden Markov Model (HMM) approach to lip password (i.e. the password embedded in the lip motion) based speaker verification, where the speaker is verified by both of lip password and the underlying characteristics of lip motions. That is, the target speaker saying the wrong password or an impostor even knowing the correct password will be detected as well. To this end, we firstly propose an effective lip motion segmentation algorithm to segment the password sequence into a small set of discrete subunits. Then, we integrate HMMs with boosting learning framework associated with the random subspace method (RSM) and data sharing scheme (DSS) to model the segmental sequence of the input subunit discriminatively so that a precise decision boundary is formulated for these subunits verification. Finally, the speaker is verified based on all verification results of the subunits learned from multi-boosted HMMs. Experimental results show the promising results.	algorithm;decision boundary;discriminative model;embedded system;hidden markov model;markov chain;password;random subspace method;response surface methodology;sensor;speaker recognition	Xin Liu;Yiu-ming Cheung	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288349	speaker recognition;computer vision;speech recognition;computer science;speech;pattern recognition;image segmentation	EDA	31.089363305687982	-60.370033370721224	125964
9e5ed1ed15f62411b8c5f3091e3660fb9b1de654	binocular rivalry detection in natural image pairs	databases;image coding;complexity theory;natural stereoscopic image binocular rivalry binocular difference;binocular rivalry;visualization;feature extraction;stereo image processing;stereo image processing databases visualization complexity theory visual systems image coding feature extraction;visual systems;natural stereoscopic image;stereo image processing natural scenes;binocular rivalry detection model natural image pairs perceptual dominance natural scene stereoscopic images visual discomfort visual fatigue binocular difference threshold region background complexity human visual system;binocular difference	When different images are presented to two eyes, they compete for perceptual dominance, such that a region of one image is visible while corresponding region of the other is suppressed. This visual phenomenon is called binocular rivalry. Binocular rivalry may be introduced in stereoscopic images of natural scene, leading to strong visual discomfort and visual fatigue. When binocular differences exceed a threshold, binocular rivalry occurs. Binocular difference threshold of corresponding regions increases with background complexity of the region. Based on these features of human visual system, we proposed a detection model of binocular rivalry for natural image pairs. A binocular rivalry database was established and used to evaluate our detection model with the database. Experimental results demonstrate that the proposed model can achieve higher hit rate and lower false alarm rate compared with existing method.	binocular vision;stereoscopy	Yapeng Xue;Wenhao Hong;Yu Cao;Lu Yu	2016	2016 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2016.7552976	binocular disparity;computer vision;visualization;feature extraction;computer science;stereopsis;binocular rivalry;machine learning	Vision	38.427903663927296	-53.60126333173557	125968
cdb1cc6048e3cd51191a7c23ffe4a29144ee694c	surface networks via general covers		Developing deep learning techniques for geometric data is an active and fruitful research area. This paper tackles the problem of sphere-type surface learning by developing a novel surface-to-image representation. Using this representation we are able to quickly adapt successful CNN models to the surface setting. The surface-image representation is based on a covering map from the image domain to the surface. Namely, the map wraps around the surface several times, making sure that every part of the surface is well represented in the image. Differently from previous surface-to-image representations we provide a low distortion coverage of all surface parts in a single image. We have used the surface-to-image representation to apply standard CNN models to the problem of semantic shape segmentation and shape retrieval, achieving state of the art results in both.		Niv Haim;Nimrod Segol;Heli Ben Hamu;Haggai Maron;Yaron Lipman	2018	CoRR			Vision	29.733015088978565	-52.74528548084273	126158
5ee63b90f7cc9bebca2ab161cd9e0958763e2c31	an efficient deep learning hashing neural network for mobile visual search		Mobile visual search applications are emerging that enable users to sense their surroundings with smart phones. However, because of the particular challenges of mobile visual search, achieving a high recognition bitrate has becomes a consistent target of previous related works. In this paper, we propose a few-parameter, low-latency, and high-accuracy deep hashing approach for constructing binary hash codes for mobile visual search. First, we exploit the architecture of the MobileNet model, which significantly decreases the latency of deep feature extraction by reducing the number of model parameters while maintaining accuracy. Second, we add a hash-like layer into MobileNet to train the model on labeled mobile visual data. Evaluations show that the proposed system can exceed state-of-the-art accuracy performance in terms of the MAP. More importantly, the memory consumption is much less than that of other deep learning models. The proposed method requires only 13 MB of memory for the neural network and achieves a MAP of 97.80% on the mobile location recognition dataset used for testing.	artificial neural network;code;deep learning;feature extraction;hash function;map;megabyte;smartphone	Heng Qi;Wu Liu;Liang Liu	2017	2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2017.8309050	architecture;visual search;latency (engineering);machine learning;artificial intelligence;theoretical computer science;deep learning;artificial neural network;computer science;feature extraction;exploit;hash function	ML	27.30191504199193	-53.31482417271795	126380
53efafd77793c3fb375db34f8b03d82a481571ae	gray-scale character recognition by gabor jets projection	binarization;gabor jet projection;image recognition;imprecise segmentation;video caption understanding;lvq classifier;video signal processing;information science;edge detection;optical character recognition;localized directional edge features;low resolution;phase shift;gabor filters;ocr method;gray scale;vq classifier;feature vector;video indexing;optical character recognition software;gabor filter;image binarization;font type variation;indexing;gray scale character recognition;feature extraction;video caption understanding gray scale character recognition gabor jet projection video indexing binarization low resolution image binarization gabor feature extraction video contents gabor filters cluttered images localized directional edge features phase shift invariance edge positions lvq classifier vq classifier character deformation font type variation imprecise segmentation ocr method;pattern classification;gabor feature extraction;video contents;robustness;pattern classification video signal processing optical character recognition filtering theory indexing edge detection feature extraction vector quantisation;edge positions;gray scale character recognition indexing feature extraction robustness gabor filters information science laboratories image recognition optical character recognition software;vector quantisation;character recognition;filtering theory;phase shift invariance;cluttered images;character deformation	We propose a gray-scale character recognition method for video indexing. It is robust against the problems of binarization against a complex background and low resolution. Unlike a traditional character recognition scheme through image binarization, we directly extract Gabor features (called Gabor jets) from video contents. The use of the Gabor filters contributes to freeing a tricky binarization process for cluttered images, and furthermore provides localized directional edge features, which have phase-shift invariance to edge positions. To form a feature vector to be classified, we accumulate the extracted Gabor features along projection lines in local regions, and then categorize them with a standard LVQ classifier. The projective accumulation provides robustness under character deformation caused by variation of font types or imprecise segmentation. We compare the proposed method by experiments with a typical OCR method, for which correct binarization is advantageously given. The proposed method shows similar or superior performance to the other method in understanding video captions.		Hiroshi Yoshimura;Minoru Etoh;Kenji Kondo;Naokazu Yokoya	2000		10.1109/ICPR.2000.906081	computer vision;search engine indexing;speech recognition;edge detection;image resolution;feature vector;feature extraction;information science;computer science;pattern recognition;mathematics;phase;optical character recognition;gabor wavelet;grayscale;robustness	Vision	37.933732692755	-63.4919152445527	126438
4634d5685b7c03d2c82a8aee9fbaa2fc297ce173	synthetic iris presentation attack using idcgan		Reliability and accuracy of iris biometric modality has prompted its large-scale deployment for critical applications such as border control and national ID projects. The extensive growth of iris recognition systems has raised apprehensions about susceptibility of these systems to various attacks. In the past, researchers have examined the impact of various iris presentation attacks such as textured contact lenses and print attacks. In this research, we present a novel presentation attack using deep learning based synthetic iris generation. Utilizing the generative capability of deep con-volutional generative adversarial networks and iris quality metrics, we propose a new framework, named as iDCGAN (iris deep convolutional generative adversarial network) for generating realistic appearing synthetic iris images. We demonstrate the effect of these synthetically generated iris images as presentation attack on iris recognition by using a commercial system. The state-of-the-art presentation attack detection framework, DESIST is utilized to analyze if it can discriminate these synthetically generated iris images from real images. The experimental results illustrate that mitigating the proposed synthetic presentation attack is of paramount importance.		Naman Kohli;Daksha Yadav;Mayank Vatsa;Richa Singh;Afzel Noore	2017	2017 IEEE International Joint Conference on Biometrics (IJCB)	10.1109/BTAS.2017.8272756	computer vision;finance;economics;software deployment;deep learning;iris recognition;biometrics;artificial intelligence	Vision	27.923133545033938	-62.602047980785635	126519
4694eebd8e5c9849d1d1fcae9d3c17ae52b7023e	recognizing faces in recorded meetings via mrc-boosting	real world meeting video;real world meeting video face recognition technique recorded meeting mrc boosting nonlinear access;video signal processing;mrc boosting;recorded meeting;nonlinear access;face recognition;image quality;video signal processing face recognition;face recognition videos loudspeakers magnetic heads focusing microphone arrays object detection robustness watches training data;face recognition technique	Person-based indices and timelines can enable fast and non-linear access to recorded meetings. This paper focuses on how to automatically construct those indices and timelines by using face recognition techniques. While there exist extensive research in generic face recognition, recognizing faces in recorded meetings is still an understudied area. Real-world meeting videos impose several interesting and unique challenges including complex lighting, low imaging quality, and large variations in head pose and size. In this paper, a promising approach based on MRC-Boosting is presented to address these challenges, which achieves encouraging performance on real-world meeting videos and shows superior accuracy and robustness compared to two popular existing approaches	algorithm;existential quantification;experiment;face detection;facial recognition system;gradient boosting;nonlinear system;online and offline;timeline;video post-processing	Xun Xu;Yong Rui;Thomas S. Huang	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262860	image quality;facial recognition system;computer vision;speech recognition;computer science;face recognition grand challenge;multimedia	Vision	34.47986695763732	-52.13538046780984	126619
f4a0dbfec5e4a2f10dfc0ecf9c99e6e23aa77fd1	principal component analysis-based visual saliency detection	analytical models;biological system modeling;computational modeling;image color analysis;feature extraction;principal component analysis	In this paper, a novel patch-wise saliency detection algorithm is proposed based on principal component analysis (PCA). As a powerful statistical procedure in data analysis, PCA is fully exploited to convert color space and produce compact patch representation. Specifically, images are first converted to linearly uncorrelated channels and divided into non-overlapped patches. Then the patches are represented by the coefficients of principal components using PCA analysis. Based on the compact representation of patches, two types of distinctiveness are introduced: 1) center-surround contrast and 2) global rarity. Experimental results demonstrate that the PCA-based color space conversion and patch representation can improve the accuracy of human fixations prediction. And the proposed algorithm outperforms the mainstream algorithms on predicting human fixations. Additional experiments on salient object detection and image retargeting show that the proposed model can achieve better performance than traditional models.	algorithm;channel (digital image);coefficient;color space;experiment;object detection;principal component analysis;retargeting;seam carving;top-down and bottom-up design	Bing Yang;Xiaoyun Zhang;Li Chen;Zhiyong Gao	2016	IEEE Transactions on Broadcasting	10.1109/TBC.2016.2617291	computer vision;feature extraction;computer science;machine learning;pattern recognition;computational model;principal component analysis	Vision	38.36055644558845	-54.21501914673685	126626
184e4a62fc9c3c8ea8948aceebb1debe0b5fc54a	generative part-based gabor object detector	gabor feature;vaitoskirja;visual classification;part based object class detector;gabor features;gaussian mixture model;generative learning;doctoral dissertation;part detector;object detection;hybrid generative discriminative detector	Discriminative part-based models have become the approach for visual object detection. The models learn from a large number of positive and negative examples with annotated class labels and location (bounding box). In contrast, we propose a part-based generative model that learns from a small number of positive examples. This is achieved by utilizing “privileged information”, sparse class-specific landmarks with semantic meaning. Our method uses bio-inspired complex-valued Gabor features to describe local parts. Gabor features are transformed to part probabilities by unsupervised Gaussian Mixture Model (GMM). GMM estimation is robustified for a small amount of data by a randomization procedure inspired by random forests. The GMM framework is also used to construct a probabilistic spatial model of part configurations. Our detector is invariant to translation, rotation and scaling. On part level invariance is achieved by pose quantization which is more efficient than previously proposed feature transformations. In the spatial model, invariance is achieved by mapping parts to an “aligned object space”. Using a small number of positive examples our generative method performs comparably to the state-of-the-art discriminative method. c © 2015 Elsevier Ltd. All rights reserved.	british informatics olympiad;generative model;google map maker;hoc (programming language);mathematical optimization;minimum bounding box;mixture model;object detection;part-based models;random forest;sparse matrix;tuple space	Ekaterina Riabchenko;Joni-Kristian Kämäräinen	2015	Pattern Recognition Letters	10.1016/j.patrec.2015.08.004	computer vision;computer science;machine learning;pattern recognition;mixture model;mathematics;generative model	Vision	31.465127257064253	-54.61009287313944	126707
0d2a1a3897d50ba490a2ffaaecc3135f573a7023	discriminative training for object recognition using image patches	detectors;image recognition;object recognition;log linear model automatic discriminative training object recognition image patch histogram;layout;data mining;error analysis;object recognition data mining image recognition detectors error analysis layout humans feature extraction object detection computer science;automatic discriminative training;feature extraction;error rate;discriminative training;humans;computer science;learning artificial intelligence;image patch histogram;object detection;image recognition object recognition learning artificial intelligence feature extraction;log linear model	We present a method for automatically learning discriminative image patches for the recognition of given object classes. The approach applies discriminative training of log-linear models to image patch histograms. We show that it works well on three tasks and performs significantly better than other methods using the same features. For example, the method decides that patches containing an eye are most important for distinguishing face from background images. The recognition performance is very competitive with error rates presented in other publications. In particular, a new best error rate for the Caltech motorbikes data of 1.5% is achieved.	discriminative model;experiment;linear model;log-linear model;outline of object recognition;wheels	Thomas Deselaers;Daniel Keysers;Hermann Ney	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.134	layout;computer vision;detector;feature extraction;word error rate;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;log-linear model	Vision	35.4381431964766	-53.38359594642202	126798
77f62eeaf36936007b929cc91f3294720d7abb6a	designing and optimizing the method for pedestrian detection based on adaboost algorithm	cascade	The designing based on Adaboost algorithm not only achieved the nighttime pedestrian detection module of auxiliary driving system, but also realized the system optimization on issues of low detection speed and precision. In this design, variable step length and partition scanning track methods are used to improve the detection speed and variance normalization approach was applied to eliminate the influences to the detection result caused by light factors, moreover, multi-scale fusion technology was utilized to make analysis to the detected rectangular box, in this case, the redundant portion of detection result can be removed, and thus improved the detection rate and reduced the false alarm rate.	adaboost;cascading classifiers;cuboid;database normalization;digital image processing;genetic algorithm;incidence matrix;machine vision;mathematical optimization;pattern recognition;pedestrian detection;program optimization	Yunpeng Su;Binwen Fan;Qiliang Yang	2014	2014 11th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2014.6980895	machine learning;pattern recognition	Robotics	33.43638626803821	-55.55517903278725	126963
f39e4e3463a7dc96b3085a24e28b26847066ccad	medical image classification via multiscale representation learning	fisher vector;image classification;multiscale feature learning;sparse autoencoder	Multiscale structure is an essential attribute of natural images. Similarly, there exist scaling phenomena in medical images, and therefore a wide range of observation scales would be useful for medical imaging measurements. The present work proposes a multiscale representation learning method via sparse autoencoder networks to capture the intrinsic scales in medical images for the classification task. We obtain the multiscale feature detectors by the sparse autoencoders with different receptive field sizes, and then generate the feature maps by the convolution operation. This strategy can better characterize various size structures in medical imaging than single-scale version. Subsequently, Fisher vector technique is used to encode the extracted features to implement a fixed-length image representation, which provides more abundant information of high-order statistics and enhances the descriptiveness and discriminative ability of feature representation. We carry out experiments on the IRMA-2009 medical collection and the mammographic patch dataset. The extensive experimental results demonstrate that the proposed method have superior performance.		Qiling Tang;Yangyang Liu;Haihua Liu	2017	Artificial intelligence in medicine	10.1016/j.artmed.2017.06.009	machine learning;discriminative model;autoencoder;scaling;medical imaging;convolution;contextual image classification;computer science;artificial intelligence;feature detection;computer vision;feature learning;pattern recognition	Vision	24.805748692777264	-52.61675426724424	127040
1cf16a96d857a6d8c152c1cee3b95b5d3b48aad1	tensor locality preserving projections for face recognition	locality preserving projection;face recognition tensor lpp;lpp;tensors computer vision face recognition feature extraction;tensor;computer vision;face recognition;machine learning;matrix decomposition;feature extraction;pattern classification;pattern recognition;face detection;dimensional reduction;tensor representation face recognition tensor locality preserving projection automated face detection biometrics computer vision pattern recognition feature extraction machine learning pattern classification tensor embedding method;tensors	Automated face detection and recognition is one of the most attentional branches of biometrics and it is also the one of the most active and challenging tasks for computer vision and pattern recognition .Over the past few years, some embedding methods have been proposed for feature extraction and dimensionality reduction in various machine learning and pattern classification tasks. Locality Preserving Projection (LPP) has been used in such applications as face recognition and image. In this paper, we propose some novel tensor embedding methods which, unlike previous methods, take data directly in the form of tensors of arbitrary order as input. These methods allow the relationships between dimensions of a tensor representation to be efficiently characterized. Extensive experiments show that our methods are not only more effective but also more efficient.	biometrics;computer vision;dimensionality reduction;experiment;face detection;facial recognition system;feature extraction;locality of reference;machine learning;pattern recognition	Dazhao Zheng;Xiufeng Du;Limin Cui	2010	2010 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2010.5642000	facial recognition system;computer vision;tensor;computer science;machine learning;pattern recognition;mathematics	Vision	34.44569372036225	-55.96960175712875	127296
47c0054bdad197c667d6f1f6c1ced6192ffd1fec	hand posture classification and recognition using the modified census transform	image recognition;protocols;human computer interaction;image databases;hand posture recognition;transforms gesture recognition human computer interaction image classification table lookup;image classification;feature lookup tables;feature lookup tables hand posture classification hand posture recognition modified census transform human computer interaction;modified census transform;face recognition;hand posture classification;feature extraction;spatial databases;transforms;pattern recognition;lookup table;face recognition pattern recognition human computer interaction spatial databases protocols image databases face detection feature extraction image recognition lighting;lighting;face detection;table lookup;illumination invariance;vision;gesture recognition	Developing new techniques for human-computer interaction is very challenging. Vision-based techniques have the advantage of being unobtrusive and hands are a natural device that can be used for more intuitive interfaces. But in order to use hands for interaction, it is necessary to be able to recognize them in images. In this paper, we propose to apply to the hand posture classification and recognition tasks an approach that has been successfully used for face detection (B. Froba and A. Ernst, 2004). The features are based on the modified census transform and are illumination invariant. For the classification and recognition processes, a simple linear classifier is trained, using a set of feature lookup-tables. The database used for the experiments is a benchmark database in the field of posture recognition. Two protocols have been defined. We provide results following these two protocols for both the classification and recognition tasks. Results are very encouraging	benchmark (computing);experiment;face detection;human–computer interaction;linear classifier;lookup table;poor posture	Agnès Just;Yann Rodriguez;Sébastien Marcel	2006	7th International Conference on Automatic Face and Gesture Recognition (FGR06)	10.1109/FGR.2006.62	computer vision;speech recognition;feature;computer science;pattern recognition;three-dimensional face recognition	Vision	30.616681220820567	-59.69456387814998	127326
71ac9b4899274e91dc4327b76f4d9cd5b8fcd1a4	sparse robust filters for scene classification of synthetic aperture radar (sar) images	scene classification;hierarchical group sparse coding;sparse robust filters;synthetic aperture radar	With the increasing resolution of Synthetic Aperture Radar (SAR) images, extracting their discriminative features for scenes classification has become a challenging task, because SAR images are very sensitive to target aspect brought by shadowing effects, interaction of the signature with the environment, and so on. Moreover, SAR images are remarkably polluted by the multiplicative speckle noise, which makes the conventional feature extractors inefficient. In this paper we advance new Sparse Robust Filters (SRFs) for automatic learning of discriminant features of scenes. A Hierarchical Group Sparse Coding (HGSC) model is proposed to learn a set of sparse and robust filters, to capture the multiscale local descriptors that are robust to noises. Some experiments are taken on a TerraSAR-X images dataset (in the middle of the Swabian Jura, the Nordlinger Ries, HH, observed on July, 2007), and a Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, to evaluate the performance of our proposed method. The experimental results show that our method can achieve higher classification accuracy compared with other related approaches.	sparse;synthetic data	Shuyuan Yang;Min Wang;Hezhao Long;Zhi Liu	2016	Neurocomputing	10.1016/j.neucom.2015.08.103	computer vision;synthetic aperture radar;pattern recognition	Vision	34.509476469729506	-56.5622864876817	127531
c7fa5dd1e597e1f063c9974660c95e10de3143ed	revisit lsb matching	histograms;steganography data compression image coding image matching statistical analysis;image coding;histogram characteristic function;steganalysis;image compression steganalysis algorithm spatial domain least significant bit matching steganography histogram characteristic function lsb matching steganography;data compression;image matching;gray scale;steganalysis algorithm;least significant bit;steganography;statistical analysis;image compression;image color analysis;feature extraction;pixel;multimedia communication;characteristic function;lsb matching steganography;steganalysis steganography;spatial domain least significant bit matching;histograms gray scale pixel image color analysis image coding feature extraction multimedia communication	In this paper, we propose a steganalysis algorithm to detect spatial domain least significant bit (LSB) matching steganography, which is much harder than the detection of LSB replacement. We use features based on histogram of run length and histogram characteristic function to detect the LSB Matching. Experimental results on two datasets demonstrate that this method has superior results compared with other recently proposed algorithms, and shows that the proposed method is efficient to detect the LSB matching steganography on compressed or uncompressed images.	algorithm;characteristic function (convex analysis);experiment;grayscale;least significant bit;most significant bit;run-length encoding;steganalysis;steganography	Xiao Yi Yu;Aiming Wang	2010	2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIHMSP.2010.105	data compression;least significant bit;computer vision;characteristic function;steganalysis;feature extraction;image compression;computer science;theoretical computer science;pattern recognition;histogram;mathematics;steganography;pixel;grayscale;statistics	Robotics	37.39168099799018	-62.116865533706765	127617
eab297517c6c88d5f20716135034121b2376dd0a	digital image watermarking technique based on dense descriptor		Enthused by the robustness and simplicity of WLD(Weber’s Local Descriptor) descriptor, a new digital image watermarking is proposed. The proposed watermarking technique is based on WLD Descriptor. WLD descriptor is a histogram representation of an image that consists of two components of a pixel: its differential excitation and orientation. Differential excitation is the difference of center pixel with its surrounding neighbors and orientation is the gradient orientation of center pixel. WLD descriptor of an image is robust against various geometric and photometric attacks. This feature of WLD descriptor engrossed it to be used in digital image watermarking.	digital image;digital watermarking	Ekta Walia;Anu Suneja	2011		10.1007/978-3-642-22577-2_82	digital image;pixel;computer science;computer vision;robustness (computer science);distributed computing;digital watermarking;histogram;artificial intelligence	EDA	38.08342432494336	-59.25644864181641	127921
f3f33d6ff64decfc0051bb7fe8f2129f2b332b60	american sign language (asl) recognition based on hough transform and neural networks	canny edge detection;american sign language;edge detection;sobel edge detection;feature vector;feature extraction;hough transform;neural network	The work presented in this paper aims to develop a system for automatic translation of static gestures of alphabets and signs in American sign language. In doing so, we have used Hough transform and neural networks which is trained to recognize signs. Our system does not rely on using any gloves or visual markings to achieve the recognition task. Instead, it deals with images of bare hands, which allows the user to interact with the system in a natural way. An image is processed and converted to a feature vector that will be compared with the feature vectors of a training set of signs. The extracted features are not affected by the rotation, scaling or translation of the gesture within the image, which makes the system more flexible.#R##N##R##N#The system was implemented and tested using a data set of 300 samples of hand sign images; 15 images for each sign. Experiments revealed that our system was able to recognize selected ASL signs with an accuracy of 92.3%.	artificial neural network;hough transform	Munib Qutaishat;Moussa Habeeb;Bayan Takruri;Hiba Abed Al-Malik	2007	Expert Syst. Appl.	10.1016/j.eswa.2005.11.018	hough transform;computer vision;speech recognition;edge detection;feature vector;feature extraction;computer science;machine learning;pattern recognition;canny edge detector;artificial neural network	Vision	32.413839252661525	-64.57397711981605	127939
1e0f4eef7c576a70b418ed963d4c5d69d13adb3b	a hybrid approach for generating secure and discriminating face template	quantization;matching performance;template security;cmu;fisherface;design and development;biometrics access control;hybrid power systems biometrics protection cryptography algorithm design and analysis data security spatial databases facial features face recognition quantization;fuzzy commitment scheme;biometrics;face databases;satisfiability;random multispace quantization biohashing algorithm;protection;hybrid approach;face recognition;hybrid power systems;hybrid method;facial feature vector;noninvertibility analysis;binary templates;discriminability preserving transform;cryptography;spatial databases;biometric template protection;face template protection;facial features;random multispace quantization biohashing algorithm discriminating face template biometric template protection biometric system biometric cryptosystem template security matching performance hybrid algorithm random projection discriminability preserving transform fuzzy commitment scheme face databases binary templates noninvertibility analysis fisherface facial feature vector;random projection;biometric system;discriminating face template;biometric cryptosystem;face recognition biometrics access control cryptography;biometric data security;fisherface biometric data security face recognition face template protection;algorithm design and analysis;hybrid algorithm;commitment scheme;data security	Biometric template protection is one of the most important issues in deploying a practical biometric system. To tackle this problem, many algorithms, that do not store the template in its original form, have been reported in recent years. They can be categorized into two approaches, namely biometric cryptosystem and transform-based. However, most (if not all) algorithms in both approaches offer a trade-off between the template security and matching performance. Moreover, we believe that no single template protection method is capable of satisfying the security and performance simultaneously. In this paper, we propose a hybrid approach which takes advantage of both the biometric cryptosystem approach and the transform-based approach. A three-step hybrid algorithm is designed and developed based on random projection, discriminability-preserving (DP) transform, and fuzzy commitment scheme. The proposed algorithm not only provides good security, but also enhances the performance through the DP transform. Three publicly available face databases, namely FERET, CMU-PIE, and FRGC, are used for evaluation. The security strength of the binary templates generated from FERET, CMU-PIE, and FRGC databases are 206.3, 203.5, and 347.3 bits, respectively. Moreover, noninvertibility analysis and discussion on data leakage of the proposed hybrid algorithm are also reported. Experimental results show that, using Fisherface to construct the input facial feature vector (face template), the proposed hybrid method can improve the recognition accuracy by 4%, 11%, and 15% on the FERET, CMU-PIE, and FRGC databases, respectively. A comparison with the recently developed random multispace quantization biohashing algorithm is also reported.	approximation-preserving reduction;biometrics;categorization;commitment scheme;cryptosystem;database;encryption;feret (facial recognition technology);facial recognition system;feature vector;html5 in mobile devices;hybrid algorithm;random projection;spectral leakage;speech enhancement	Yi C. Feng;Pong C. Yuen;Anil K. Jain	2010	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2009.2038760	facial recognition system;commitment scheme;quantization;hybrid algorithm;computer science;cryptography;pattern recognition;data mining;data security;computer security;algorithm;biometrics;satisfiability	Vision	30.888351841239974	-61.569495376226854	127986
f038cb1fe723ec537049af8f9073e722b4ddb94f	errata: benchmark for license plate character segmentation		Automatic license plate recognition (ALPR) has been the focus of many researches in the past years. In general, ALPR is divided into the following problems: detection of on-track vehicles, license plate detection, segmentation of license plate characters, and optical character recognition (OCR). Even though commercial solutions are available for controlled acquisition conditions, e.g., the entrance of a parking lot, ALPR is still an open problem when dealing with data acquired from uncontrolled environments, such as roads and highways when relying only on imaging sensors. Due to the multiple orientations and scales of the license plates captured by the camera, a very challenging task of the ALPR is the license plate character segmentation (LPCS) step, because its effectiveness is required to be (near) optimal to achieve a high recognition rate by the OCR. To tackle the LPCS problem, this work proposes a benchmark composed of a dataset designed to focus specifically on the character segmentation step of the ALPR within an evaluation protocol. Furthermore, we propose the Jaccard-centroid coefficient, an evaluation measure more suitable than the Jaccard coefficient regarding the location of the bounding box within the ground-truth annotation. The dataset is composed of 2000 Brazilian license plates consisting of 14000 alphanumeric symbols and their corresponding bounding box annotations. We also present a straightforward approach to perform LPCS efficiently. Finally, we provide an experimental evaluation for the dataset based on five LPCS approaches and demonstrate the importance of character segmentation for achieving an accurate OCR.	benchmark (computing)	Gabriel Resende Gonçalves;Sirlene Pio Gomes da Silva;David Menotti;William Robson Schwartz	2016	J. Electronic Imaging	10.1117/1.JEI.25.6.069801	computer vision;artificial intelligence;license;pattern recognition;jaccard index;computer science;optical character recognition;parking lot;image segmentation;mathematical alphanumeric symbols;minimum bounding box;annotation	Vision	31.466119244989684	-53.69947776836354	127995
a7c88d6c64ea9cb7a0e11b69a1eeee155dad1328	three dimensional palmprint recognition	databases;image recognition;image coding;feature coding;biometrics access control;3d palmprint recognition;image fusion;biometrics;palmprint authentication system;structured light;structured light imaging technology;data mining;three dimensional;score level fusion;image recognition biometrics access control feature extraction image coding image fusion;three dimensional displays;feature extraction;imaging;3d palmprint identification;mean curvature image extraction;score level fusion 3d palmprint recognition palmprint authentication system structured light imaging technology mean curvature image extraction competitive coding technique;mean curvature;competitive coding technique;meteorology;biometrics authentication pattern recognition image recognition data acquisition time measurement surface reconstruction image reconstruction costs data security;feature coding 3d palmprint identification biometrics mean curvature	Palmprint has been widely studied as its high accuracy and low cost. Most of the previous studies are based on two dimensional (2D) image of the palmprint. However, 2D image can be easily forged, which will threaten the security of palmprint authentication system. Furthermore, 2D image can be easily affected by noise, such as scrabbling and dirty in the palm. To overcome these shortcomings, we develop a three dimensional (3D) palmprint identification system. The structured-light imaging technology is adopted to collect the 3D palmprint data, from which the stable Mean Curvature Image (MCI) is extracted. Then the Competitive Coding (CompCode) technique is used to code the 3D palmprint pattern according the MCI. By using score level fusion of MCI and its CompCode, promising recognition performance is achieved on our established 3D palmprint database.	authentication;fingerprint;imaging technology;structured light	Li Wei;Lei Zhang;David Zhang	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5346053	medical imaging;three-dimensional space;computer vision;speech recognition;structured light;feature extraction;computer science;mean curvature;pattern recognition;image fusion;biometrics	Robotics	31.918805305663437	-62.555543448631745	128100
735a92c8ccc51c0525c32d716e2ecb696e9d0b7a	automated insect identification through concatenated histograms of local appearance features	zoology biology computing feature extraction image classification;biology computing;image classification;local features;feature extraction;insects concatenated codes histograms detectors computer vision thermal pollution entropy water resources pediatrics water pollution;classification system;discrimination task automated insect identification local appearance feature histograms stone fly larvae classification system hessian affine detector kadir entropy detector principal curvature based region detector pcbr detector concatenated feature histogram cfh methodology local region descriptors feature vectors;zoology	This paper describes a fully automated stone fly-larvae classification system using a local features approach. It compares the three region detectors employed by the system: the Hessian-affine detector, the Kadir entropy detector and a new detector we have developed called the principal curvature based region detector (PCBR). It introduces a concatenated feature histogram (CFH) methodology that uses histograms of local region descriptors as feature vectors for classification and compares the results using this methodology to that of Opelt [Opelt, A, et.al., 2006.] on three stonefly identification tasks. Our results indicate that the PCBR detector outperforms the other two detectors on the most difficult discrimination task and that the use of all three detectors outperforms any other configuration. The CFH methodology also outperforms the Opelt methodology in these tasks	concatenation	Natalia Larios;Hongli Deng;Wei Zhang;Matt Sarpola;Jenny Yuen;Robert Paasch;Andrew Moldenke;David A. Lytle;Ruiz Correa;Eric N. Mortensen;Linda G. Shapiro;Thomas G. Dietterich	2007		10.1109/WACV.2007.13	computer vision;contextual image classification;feature extraction;computer science;machine learning;pattern recognition;principal curvature-based region detector;mathematics;hessian affine region detector	Vision	34.33451867118237	-61.22205155671286	128156
781741646b08b89cb16ff9d2c1b8f87cd85a71d8	point-cloud-based place recognition using cnn feature extraction		This paper proposes a novel point-cloud-based place recognition system that adopts a deep learning approach for feature extraction. By using a convolutional neural network pre-trained on color images to extract features from a range image without fine-tuning on extra range images, significant improvement has been observed when compared to using handcrafted features. The resulting system is illumination invariant, rotation invariant and robust against moving objects that are unrelated to the place identity. Apart from the system itself, we also bring to the community a new place recognition dataset containing both point cloud and grayscale images covering a full 360◦ environmental view. In addition, the dataset is organized in such a way that it facilitates experimental validation with respect to rotation invariance or robustness against unrelated moving objects separately.	artificial neural network;cloud computing;convolutional neural network;deep learning;feature extraction;grayscale;pattern recognition;point cloud;range imaging;resultant;robustness (computer science)	Ting Sun;Haoyang Ye;Dit-Yan Yeung	2018	CoRR		convolutional neural network;pattern recognition;grayscale;point cloud;deep learning;feature extraction;robustness (computer science);computer science;place identity;invariant (mathematics);artificial intelligence	Robotics	30.989527181246	-52.28719452100229	128225
afbcc25ec1fbace64309e7711e2f30e5a0568611	multi-lingual scene text detection based on fully convolutional networks		In the paper, we propose a method based on transfer learning to detect multi-lingual text in natural scenes. First, a semantic segmentation map of the input image is obtained through a fully convolution network (FCN). In this map, each pixel is classified to text or none-text. And then, the candidate boxes of text regions are computed based on the map. In this procedure, VGG network is trained to obtain a basic character classifier of single language. Based on this VGG model, FCN has the ability to classify each pixel to text or none-text for multi-lingual with doing transfer learning. Finally, the bounding boxes of text are carry out by filtering the unsatisfied candidates with some rules. The experimental results show that our method achieves good performance on the task of multi-lingual text detection. And compared with other advanced method, the time cost of our method is shortest.		Shaohua Liu;Yan Shang;Jizhong Han;Xi Wang;Hongchao Gao;Dongqin Liu	2017		10.1007/978-3-319-77380-3_40	pixel;artificial intelligence;computer vision;transfer of learning;computer science;filter (signal processing);pattern recognition;convolution	Vision	28.88159170094372	-55.364645095108884	128368
7f415aee0137acab659c664eb1dff15f7b726bdd	scene text recognition using structure-guided character detection and linguistic knowledge	part based tree structured models tsms;cropped word recognition;posterior probability scene text recognition word spotting cropped word recognition character recognition part based tree structured models;character recognition;character recognition text recognition image recognition optical character recognition software feature extraction object recognition image color analysis	Scene text recognition has inspired great interests from the computer vision community in recent years. In this paper, we propose a novel scene text-recognition method integrating structure-guided character detection and linguistic knowledge. We use part-based tree structure to model each category of characters so as to detect and recognize characters simultaneously. Since the character models make use of both the local appearance and global structure informations, the detection results are more reliable. For word recognition, we combine the detection scores and language model into the posterior probability of character sequence from the Bayesian decision view. The final word-recognition result is obtained by maximizing the character sequence posterior probability via Viterbi algorithm. Experimental results on a range of challenging public data sets (ICDAR 2003, ICDAR 2011, SVT) demonstrate that the proposed method achieves state-of-the-art performance both for character detection and word recognition.	computer vision;ibm spectrum protect (tivoli storage manager);iccv;information privacy;international conference on document analysis and recognition;language model;optical character recognition;shadow mapping;tree structure;viterbi algorithm	Cunzhao Shi;Chunheng Wang;Baihua Xiao;Song Gao;Jinlong Hu	2014	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2014.2302522	natural language processing;speech recognition;document processing;feature;intelligent character recognition;computer science;intelligent word recognition;pattern recognition;3d single-object recognition;sketch recognition	Vision	32.37098442299416	-65.79263962140196	128441
229aeb608b63557422ba8718126b9de855f8f964	context-aware lane marking detection on urban roads	roads histograms image color analysis training feature extraction shape context;mser lane marking detection context aware features;roads data mining image texture intelligent transportation systems;adaboost training context aware lane marking detection urban road automatic lane marking detection intelligent transportation system context aware color context aware texture context aware shape feature road surface hard negative mining technique maximum stable extreme region detector mser detector	Automatic lane marking detection plays an important role in intelligent transportation systems. We present an effective lane marking detection technique that utilizes the context-aware information of lane marking on the urban roads. The proposed technique consists of two innovations. First, the context-aware color, texture and shape features which characterise both lane markings and their road context are designed to represent the lane markings on the road surface. Second, a hard negative mining technique is developed based on the Maximum Stable Extreme Region (MSER) detector and adaboost training. Experiments on a real world dataset demonstrate the superior performance of the proposed approach.	adaboost;experiment;item unique identification;maximally stable extremal regions	Tao Chen;Shijian Lu	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351264	computer vision;simulation;speech recognition	Robotics	33.0408141876198	-53.94696347845003	128568
97a0aba4e9a95db17c3d4367f59aad1f02e04b55	how far did we get in face spoofing detection?		The growing use of control access systems based on face recognition shed light over the need for even more accurate systems to detect face spoofing attacks. In this paper, an extensive analysis on face spoofing detection works published in the last decade is presented. The analyzed works are categorized by their fundamental parts, i.e., descriptors and classifiers. This structured survey also brings a comparative performance analysis of the works considering the most important public data sets in the field. The methodology followed in this work is particularly relevant to observe temporal evolution of the field, trends in the existing approaches, Corresponding author: Luciano Oliveira, tel. +55 71 3283-9472 Email addresses: luiz.otavio@ufba.br (Luiz Souza), lrebouca@ufba.br (Luciano Oliveira), mauricio@dcc.ufba.br (Mauricio Pamplona), papa@fc.unesp.br (Joao Papa) to discuss still opened issues, and to propose new perspectives for the future of face spoofing detection.	access control;anomaly detection;categorization;data descriptor;email;facial recognition system;information privacy;spoofing attack	Luiz Souza;Mauricio Pamplona Segundo;Luciano Oliveira;João Papa	2018	Eng. Appl. of AI	10.1016/j.engappai.2018.04.013	artificial intelligence;machine learning;computer science;facial recognition system;data mining;spoofing attack	Vision	28.27065823341168	-61.68596819957627	128623
eb7d98290214b06485057b4993a69a1408e35cc6	text extraction, enhancement and ocr in digital video	vision ordenador;traitement image document;document analysis;image segmentation;optical character recognition;text extraction;computer vision;analyse documentaire;segmentation image;reconocimento optico de caracteres;pattern recognition;document image processing;analisis documental;vision ordinateur;digital video;reconnaissance forme;reconocimiento patron;reconnaissance optique caractere	In this paper we address the problem of text extraction, enhancement and recognition in digital video. Compared with optical character recognition (OCR) from document images, text extraction and recognition in digital video presents several new challenges. First, the text in video is often embedded in complex backgrounds, making text extraction and separation difficult. Second, image data contained in video frames is often digitized and/or subsampled at a much lower resolution than is typical for document images. As a result, most commercial OCR software can not recognize text extracted from video. We have implemented a hybrid wavelet/neural network segmenter to extract text regions and use a two stage enhancement scheme prior to recognition. First, we use Shannon interpolation to raise the image resolution, and second we postprocess the block with normal/inverse text classification and adaptive thresholding. Experimental results show that our text extraction scheme can extract both scene text and graphical text robustly and reasonable OCR results are achieved after enhancement.	artificial neural network;comparison of optical character recognition software;digital video;document classification;embedded system;image resolution;interpolation;shannon (unit);thresholding (image processing);wavelet	Huiping Li;David S. Doermann;Omid E. Kia	1998		10.1007/3-540-48172-9_29	computer vision;speech recognition;computer science;multimedia;image segmentation;optical character recognition	Vision	37.16244689105047	-65.95910351539341	128643
84409f60ef775f72dedbc0292ab265532516359b	ms-unique: multi-model and sharpness-weighted unsupervised image quality estimation		In this paper, we train independent linear decoder models to estimate the perceived quality of images. More specifically, we calculate the responses of individual non-overlapping image patches to each of the decoders and scale these responses based on the sharpness characteristics of filter set. We use multiple linear decoders to capture different abstraction levels of the image patches. Training each model is carried out on 100,000 image patches from the ImageNet database in an unsupervised fashion. Color space selection and ZCA Whitening are performed over these patches to enhance the descriptiveness of the data. The proposed quality estimator is tested on the LIVE and the TID 2013 image quality assessment databases. Performance of the proposed method is compared against eleven other state of the art methods in terms of accuracy, consistency, linearity, and monotonic behavior. Based on experimental results, the proposed method is generally among the top performing quality estimators in all categories.	color space;image quality;imagenet;key whitening;multi-model database;unsupervised learning	Mohit Prabhushankar;Dogancan Temel;Ghassan AlRegib	2018	CoRR	10.2352/ISSN.2470-1173.2017.12.IQSP-223	unsupervised learning	Robotics	27.62389592583056	-54.86551667176948	128707
4337da5eb3930a234dfd61e01996a9ff20974862	combining color descriptors for improved codebook modelbased image retrieval		In this paper we present the design of an image ContentBased Indexing and Retrieval (CBIR) system which, based upon existing implementations of a number of well-known color descriptors, makes use of the bag-of-words or codebook model in order to construct a robust approach to the retrieval of images from a database in a query-by-example context. A new object image database was constructed specifically for this task, in an attempt to challenge the invariance properties of the system under controlled conditions of illumination, point of view and scale. The system permits the combined use of up to two of the different color descriptors considered. The experiments run over a subset of the image database show an improvement of the obtained results under some of the tested combinations, as well as the effect of the variation of the employed codebook size.	codebook;image retrieval	Aitor Álvarez;Guanqun Cao;Hasan Sheikh Faridul;Yu Hu	2010			u-matrix;image retrieval;visual word;search engine indexing;codebook;computer vision;artificial intelligence;mathematics;pattern recognition	Vision	37.298350683796244	-59.34083285158618	128730
13bc63ba4abc9fe519298995d7558ff402aa0016	land cover mapping with higher order graph-based co-occurrence model				Wenzhi Zhao;William J. Emery;Yanchen Bo;Jiage Chen	2018	Remote Sensing	10.3390/rs10111713		Mobile	28.22260689417614	-57.23642182945839	128776
9d655038dca076951bcf59c5c28d4a0fa8c97df4	fast features for face authentication under illumination direction changes	face authentication;280203;learning;institute for integrated and intelligent systems;polynomial coefficients;facial feature extraction;discrete cosine transform;faculty of engineering and information technology;feature extraction;eigenfaces;principal component analysis;error rate;pre2009 image processing;gabor wavelets;histogram equalization;illumination changes	In this letter we propose a facial feature extraction technique which utilizes polynomial coefficients derived from 2D Discrete Cosine Transform (DCT) coefficients obtained from horizontally and vertically neighbouring blocks. Face authentication results on the VidTIMIT database suggest that the proposed feature set is superior (in terms of robustness to illumination changes and discrimination ability) to features extracted using four popular methods: Principal Component Analysis (PCA), PCA with histogram equalization pre-processing, 2D DCT and 2D Gabor wavelets; the results also suggest that histogram equalization pre-processing increases the error rate and offers no help against illumination changes. Moreover, the proposed feature set is over 80 times faster to compute than features based on Gabor wavelets. Further experiments on the Weizmann database also show that the proposed approach is more robust than 2D Gabor wavelets and 2D DCT coefficients. 2003 Elsevier B.V. All rights reserved.	authentication;coefficient;discrete cosine transform;eigenface;experiment;feature extraction;gabor filter;histogram equalization;polynomial;preprocessor;principal component analysis;wavelet	Conrad Sanderson;Kuldip K. Paliwal	2003	Pattern Recognition Letters	10.1016/S0167-8655(03)00070-9	computer vision;speech recognition;feature extraction;word error rate;computer science;machine learning;discrete cosine transform;pattern recognition;eigenface;gabor wavelet;histogram equalization;principal component analysis	Vision	34.19247483561466	-59.264861410484215	128940
d27abdfbf85acbd0c1cff6c80879b014d701ea3d	affective labeling in a content-based recommender system for images	video signal processing;facial expressions affective labeling affective user modeling content based recommender system emotion detection;implicitly acquired affective label affective labeling content based recommender system multimedia content implicit image acquisition emotion detection technique video sequence user facial expression gabor low level feature extraction k nearest neighbor machine learning technique valence arousal dominance space cbr system generic metadata explicitly acquired affective label;image classification;face recognition;recommender system;feature extraction;meta data;learning artificial intelligence;labeling recommender systems accuracy motion pictures multimedia communication feature extraction streaming media;recommender systems;object detection;video signal processing face recognition feature extraction image classification image sequences learning artificial intelligence meta data object detection recommender systems;image sequences	Affective labeling of multimedia content has proved to be useful in recommender systems. In this paper we present a methodology for the implicit acquisition of affective labels for images. It is based on an emotion detection technique that takes as input the video sequences of the users' facial expressions. It extracts Gabor low level features from the video frames and employs a k nearest neighbors machine learning technique to generate affective labels in the valence-arousal-dominance space. We performed a comparative study of the performance of a content-based recommender (CBR) system for images that uses three types of metadata to model the users and the items: (i) generic metadata, (ii) explicitly acquired affective labels and (iii) implicitly acquired affective labels with the proposed methodology. The results show that the CBR performs best when explicit labels are used. However, implicitly acquired labels yield a significantly better performance of the CBR than generic metadata while being an unobtrusive feedback tool.	case-based reasoning;dimensionality reduction;emotion recognition;gabor filter;k-nearest neighbors algorithm;machine learning;open research;recommender system;semantic role labeling;sequence labeling;unobtrusive javascript;voice activity detection	Marko Tkalcic;Ante Odic;Andrej Kosir;Jurij F. Tasic	2013	IEEE Transactions on Multimedia	10.1109/TMM.2012.2229970	computer vision;contextual image classification;feature extraction;computer science;machine learning;multimedia;metadata;recommender system	Vision	33.409316476864944	-52.5920957955118	129396
fee2e86bdad7afa063c883443b8d0636c2f00c81	early handwritten music recognition with hidden markov models	handwriting recognition;computational modeling;optical imaging;hidden markov models;feature extraction;image reconstruction;music	This work presents a statistical method to tackle the Handwritten Music Recognition task for Early notation, which comprises more than 200 different symbols. Unlike previous approaches to deal with music notation, our strategy is to perform a holistic recognition without any previous segmentation or staff removal process. The input consists of a page of a music book, which is processed to extract and normalize the staves contained. Then, a feature extraction process is applied to define such sections as a sequence of numerical vectors. The recognition is based on the use of Hidden Markov Models for the optical processing and smoothed N-grams as language model. Experimentation results over a historical archive of Hispanic music reported an error around 40 %, which confirms our proposal as a good starting point taking into account the difficulty of the task.	archive;feature extraction;grams;hidden markov model;holism;language model;markov chain;n-gram;numerical analysis;smoothing	Jorge Calvo-Zaragoza;Alejandro Héctor Toselli;Enrique Vidal	2016	2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR)	10.1109/ICFHR.2016.0067	computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;optical imaging;music;handwriting recognition	Vision	36.021466093875034	-65.64182924419494	129492
8b6aab4c6b03d4537cb9870f9119b310a77aa066	multimodality in biosecure: evaluation on real vs. virtual subjects	handwriting recognition;statistical protocol;multimodal databases;virtual databases;databases biometrics protocols authentication large scale systems protection fingerprint recognition intelligent networks europe data security;statistical analysis;score fusion methods;biometric evaluation;speech recognition;sensor fusion;multimodal evaluation;a priori independent modalities;virtual subjects;biosecure network of excellence;biosecure multimodality;statistical protocol biosecure multimodality virtual subjects biosecure network of excellence biometric evaluation multimodal evaluation multimodal databases score fusion methods a priori independent modalities virtual databases;statistical analysis handwriting recognition sensor fusion speech recognition;network of excellence	In this paper, we present the BioSecure Network of Excellence and its objectives in terms of biometric evaluation. A particular focus is given in this project to multimodal evaluation, which requires special attention due to the lack of large-size available multimodal databases. We show in this paper that the evaluation of score fusion methods for two a priori independent modalities is possible on standard size (roughly 100 persons) virtual databases, but at the price of a careful statistical protocol	biometrics;database;multimodal interaction;technical standard	Bernadette Dorizzi;Sonia Garcia-Salicetti;Lorène Allano	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1661469	computer vision;speech recognition;computer science;data mining;sensor fusion;handwriting recognition	Visualization	28.78182190333252	-63.81918630176582	129613
683ec608442617d11200cfbcd816e86ce9ec0899	dual linear regression based classification for face cluster recognition	face cluster recognition;linear regression;dual linear regression;face probes face recognition equations clustering algorithms linear regression image recognition;regression analysis face recognition;probe subject dual linear regression classification face cluster recognition spatio temporal relation image vector dlrc gallery subject;image set based face recognition linear regression dual linear regression face cluster recognition;image set based face recognition	"""We are dealing with the face cluster recognition problem where there are multiple images per subject in both gallery and probe sets. It is never guaranteed to have a clear spatio-temporal relation among the multiple images of each subject. Considering that the image vectors of each subject, either in gallery or in probe, span a subspace, an algorithm, Dual Linear Regression Classification (DLRC), for the face cluster recognition problem is developed where the distance between two subspaces is defined as the similarity value between a gallery subject and a probe subject. DLRC attempts to find a """"virtual"""" face image located in the intersection of the subspaces spanning from both clusters of face images. The """"distance"""" between the """"virtual"""" face images reconstructed from both subspaces is then taken as the distance between these two subspaces. We further prove that such distance can be formulated under a single linear regression model where we indeed can find the """"distance"""" without reconstructing the """"virtual"""" face images. Extensive experimental evaluations demonstrated the effectiveness of DLRC algorithm compared to other algorithms."""	algorithm;file spanning;statistical classification	Liang Chen	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.342	computer vision;feature;computer science;linear regression;machine learning;pattern recognition;mathematics	Vision	35.43994710627633	-52.71831254671088	129646
48beeaffb5f8eb2051b499fa449f80c1f1cdf9e2	visual saliency fusion based multi-feature for semantic image retrieval		In this paper, a saliency fusion based content-based image retrieval method is proposed. Different saliency detection methods were conducted firstly and the output saliency maps were fused by double low rank matrix recovery method. Then the images were segmented into foreground and background according to the fusion result. As the foreground and background had the different impacts on the semantic understanding of the image, different features represented in the form of histogram were extracted. Finally, a fusion of z-score normalized Chi-Square distance is adopted as the similarity measurement. This proposal has been implemented on three widely used benchmark databases and the results evaluated in terms of mean Average Precision (mAP), precision, recall, and F1-measure show that our proposal outperforms the referred state-of-the-art approaches.	image retrieval	Jia-nan Chen;Cong Bai;Ling Huang;Zhi Liu;Shengyong Chen	2017		10.1007/978-981-10-7302-1_11	normalization (statistics);image retrieval;salience (neuroscience);visual word;bag-of-words model;low-rank approximation;recall;histogram;computer science;artificial intelligence;pattern recognition	Vision	35.48407345122724	-57.36374026361166	129896
0b31264949cb79ea7a59ad30afab4022946ebf98	feature points tracking and emotion classification	eyebrows;neural network facial recognition feature points emotion classification feature tracking;target tracking emotion recognition face recognition feature extraction image classification neural nets;emotion recognition;feature extraction face facial features eyebrows biological neural networks image color analysis emotion recognition;image color analysis;feature extraction;facial features;face;neural networks feature points tracking facial recognition facial emotion classification;biological neural networks	The field of the facial recognition is especially animated these last year's considering the big number of research which was concerned with this subject, more particularly in the field of detection, classification and tracking. In this paper we present a method for feature points tracking and Emotion Classification based on the degree of characterization. We also tested facial emotion classification by evaluating the best distance between facial feature points using degree of characterization. After that emotions were classified using neural networks.	artificial neural network;facial recognition system;statistical classification	Ahmed Fnaiech;Mounir Sayadi;Philippe Gorce	2016	2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)	10.1109/ATSIP.2016.7523090	computer vision;feature;feature extraction;machine learning;pattern recognition;three-dimensional face recognition;feature	Robotics	30.91108914322889	-59.12304340925624	129986
461052af565d7195c2c84a413d1118c412dc6935	improving saliency detection via multiple kernel boosting and adaptive fusion	kernel;kernel feature extraction context boosting computational modeling adaptation models support vector machines;support vector machines;adaptive fusion saliency detection regional descriptor multiple kernel boosting support vector regression;quality prediction model saliency detection multiple kernel boosting adaptive fusion initial saliency map regional descriptor regression problem support vector regression mkb svr learning;computational modeling;boosting;feature extraction;adaptation models;context;support vector machines image fusion image segmentation learning artificial intelligence regression analysis	This letter proposes a novel framework to improve the saliency detection performance of an existing saliency model, which is used to generate the initial saliency map. First, a novel regional descriptor consisting of regional self-information, regional variance, and regional contrast on a number of features with local, global, and border context is proposed to describe the segmented regions at multiple scales. Then, regarding saliency computation as a regression problem, a multiple kernel boosting method based on support vector regression (MKB-SVR) is proposed to generate the complementary saliency map. Finally, an adaptive fusion method via learning a quality prediction model for saliency maps is proposed to effectively fuse the initial saliency map with the complementary saliency map and obtain the final saliency map with improvement on saliency detection performance. Experimental results on two public datasets with the state-of-the-art saliency models validate that the proposed method consistently improves the saliency detection performance of various saliency models.	algorithm;computation;kernel (operating system);map;self-information;support vector machine	Xiaofei Zhou;Zhi Liu;Guangling Sun;Linwei Ye;Xiangyang Wang	2016	IEEE Signal Processing Letters	10.1109/LSP.2016.2536743	support vector machine;computer vision;kernel;feature extraction;computer science;kadir–brady saliency detector;machine learning;pattern recognition;mathematics;computational model;boosting	Vision	34.963074913452544	-54.44698301811506	130059
7b965dfeaa1357594d0ac91ed48974460db34a05	automatic texture feature selection for image pixel classification	supervised texture classification;texture feature selection;texture classification;pixel classification;texture features;multiple evaluation windows;gabor filter;multiple texture methods;feature selection;extraction method	Pixel-based texture classifiers and segmenters are typically based on the combination of texture feature extraction methods that belong to a single family (e.g., Gabor filters). However, combining texture methods from different families has proven to produce better classification results both quantitatively and qualitatively. Given a set of multiple texture feature extraction methods from different families, this paper presents a new texture feature selection scheme that automatically determines a reduced subset of methods whose integration produces classification results comparable to those obtained when all the available methods are integrated, but with a significantly lower computational cost. Experiments with both Brodatz and real outdoor images show that the proposed selection scheme is more advantageous than well-known general purpose feature selection algorithms applied to the same problem.		Domenec Puig;Miguel Angel García	2006	Pattern Recognition	10.1016/j.patcog.2006.05.016	image texture;computer vision;computer science;machine learning;pattern recognition;mathematics;texture compression;feature selection;texture filtering	Vision	36.27745752826198	-59.56091308082165	130066
2aada26ae7f6197e84de07ae922330331dd33556	handwritten bangla compound character recognition using gradient feature	character recognition	Recognition of handwritten characters of Indian script is difficult because of the presence of many complex shaped compound characters (cluster characters) as well as variability involved in the writing style of different individuals. This paper deals with recognition of off-line Bangla handwritten compound characters using modified quadratic discriminant function (MQDF). The features used for recognition purpose are mainly based on directional information obtained from the arc tangent of the gradient. To get the feature, at first, a 2 times 2 mean filtering is applied 4 times on the gray level image and a non-linear size normalization is done on the image. A Roberts filter is then applied on the normalized image to obtain gradient image. Next, the arc tangent of the gradient (direction of gradient) is initially quantized into 32 directions and the strength of the gradient is accumulated with each of the quantized direction. Finally, the frequencies of these directions are down sampled using Gaussian filter to get 392 dimensional feature vectors. Using 5-fold cross validation technique we obtained 85.90% accuracy from a dataset of Bangla compound characters containing 20,543 samples.	computer cluster;discriminant;feature vector;gaussian blur;gradient;grayscale;handwriting recognition;nonlinear system;online and offline;quadratic classifier;spatial variability	Umapada Pal;Tetsushi Wakabayashi;Fumitaka Kimura	2007	10th International Conference on Information Technology (ICIT 2007)	10.1109/ICIT.2007.62	computer science;intelligent word recognition	Vision	34.09289927082796	-64.23644434917723	130286
a018d298f136d957ea6444bb84088a51e7cdeb8e	pansharpening by convolutional neural networks	enhancement;segmentation;machine learning;multiresolution;super resolution;convolutional neural networks	A new pansharpening method is proposed, based on convolutional neural networks. We adapt a simple and effective three-layer architecture recently proposed for super-resolution to the pansharpening problem. Moreover, to improve performance without increasing complexity, we augment the input by including several maps of nonlinear radiometric indices typical of remote sensing. Experiments on three representative datasets show the proposed method to provide very promising results, largely competitive with the current state of the art in terms of both full-reference and no-reference metrics, and also at a visual inspection.	artificial neural network;computer vision;convolutional neural network;data-intensive computing;deep learning;experiment;image processing;map;nonlinear system;reference architecture;super-resolution imaging;three-layer architecture;visual inspection	Giuseppe Masi;Davide Cozzolino;Luisa Verdoliva;Giuseppe Scarpa	2016	Remote Sensing	10.3390/rs8070594	computer vision;computer science;machine learning;pattern recognition;convolutional neural network;segmentation;superresolution	ML	24.895921136484557	-52.09823242517669	130398
49d3be727a838f0ae15c37a93a3d61b6cba967d6	salient region detection by jointly modeling distinctness and redundancy of image content	unified clustering framework;retargeting process;models content distinctness;models content redundancy;cluster saliency;salient region detection;retargeting error;clustering parameter;image content;clustering assumption;novel saliency propagation	unified clustering framework;retargeting process;models content distinctness;models content redundancy;cluster saliency;salient region detection;retargeting error;clustering parameter;image content;clustering assumption;novel saliency propagation		Yiqun Hu;Zhixiang Ren;Deepu Rajan;Liang-Tien Chia	2010		10.1007/978-3-642-19309-5_40	computer vision;computer science;pattern recognition;data mining	Vision	37.753074058617834	-53.815163458795965	130436
81c3812ca6f6294f51664c558a666b44a91d3ed1	spatial pyramid vlad	vectors vocabulary computer vision principal component analysis conferences visualization pattern recognition;normalization spatial pyramid vlad image retrieval feature representation;normalization;feature representation;vlad;image retrieval image coding;vector of locally aggregated descriptors spatial pyramid vlad image retrieval dimensionality reduction image compression normalization method;spatial pyramid;image retrieval	In recent years, VLAD has become a popular method which encoding powerful local descriptors to the compact representations. By using this approach, an image can be represented by just a few dozen bytes while preserving excellent retrieval results after the dimensionality reduction and compression. However, throwing away the spatial information is one of the biggest weaknesses of VLAD. This paper adopts the spatial pyramid pooling method to incorporate the spatial information into the VLAD vectors. Furthermore, a new normalization method is proposed to hold this advantage. By the proposed method, the performance of VLAD can be boosted through combining spatial information. The experimental results show that our approach outperforms VLAD in almost all configurations.	byte;dimensionality reduction;geographic information system;grammar-based code;pyramid (geometry)	Renhao Zhou;Qingsheng Yuan;Xiaoguang Gu;Dongming Zhang	2014	2014 IEEE Visual Communications and Image Processing Conference	10.1109/VCIP.2014.7051576	computer vision;image retrieval;computer science;normalization;pattern recognition;information retrieval	Vision	35.66959854053146	-57.28518519356678	130850
5295c1a593876f706ae0d2512642edd1ca95b6d9	surface material recognition using active multi-modal extreme learning machine		Visual, haptic, and auditory modalities can provide different properties about surface materials and therefore form important perception methods for common material. Nevertheless, how to effectively combine various modalities is an extremely challenging problem. To this end, the active multi-modal framework based on extreme learning machine with multi-scale local receptive fields is developed to fuse the information of different modalities. We conduct multi-modal experiments on the TUM haptic texture database. The experimental results show the highly representative characteristics can be extracted from surface material by the proposed architecture and the three modalities fusion achieves the best performance. The proposed active multi-modal fusion framework shows significant performance improvements.		Huaping Liu;Jing Fang;Xinying Xu;Fuchun Sun	2018	Cognitive Computation	10.1007/s12559-018-9571-z	machine learning;active perception;computer science;artificial intelligence;modalities;extreme learning machine;perception;architecture;haptic technology;receptive field	Vision	25.64988769275176	-56.095473252542995	130895
0cde7c40caa2270297370b4649aba021e00fae81	movie posters classification into genres based on low-level features	image colour analysis bayes methods edge detection feature extraction humanities image classification;movie poster multi label classification data transformation method;motion pictures histograms image color analysis feature extraction visualization animation image edge detection;rakel movie posters classification low level features visual clutter automated movie genre detection low level feature extraction colors edges poster images poster classification multilabel classification task multilabel data transformation distance ranking naïve bayes	A person can quickly grasp the genre (drama, comedy, cartoons, etc.) from a movie poster, regardless of visual clutter and the level of details. Bearing this in mind, it can be assumed that simple properties of a movie poster should play a significant role in automated detection of movie genres. Therefore, low-level features based on colors and edges are extracted from poster images and used for poster classification into genres. In this paper, poster classification is modeled as a multilabel classification task, where a single movie may belong to more than one class (genre). To simplify and solve the multilabel problem, two methods for multi-label data transformation are described and evaluated given the classification results obtained by distance ranking, Naïve Bayes and RAKEL. Experiments are conducted on a set of 1500 posters with 6 movie genres. Results provide insights into the properties of the discussed algorithms and features.	algorithm;clutter;color;gist;high- and low-level;mind;multi-label classification;multiclass classification;naive bayes classifier;optical character recognition;ordered pair;speeded up robust features;test set	Marina Ivasic-Kos;Miran Pobar;Luka Mikec	2014	2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.1109/MIPRO.2014.6859750	computer vision;speech recognition;computer science;pattern recognition	AI	34.138290462350646	-54.372709137712896	131282
fbcaac601034cc7b1af4d8199243442b17da583b	application of bsif, log-gabor and mrmr transforms for iris and palmprint based bi-modal identification system		Verification of individual identity through the process of biometric identification involves comparison between an encoded value and a stored value of the biometric feature in question. The effectiveness of a multimodal user authentication system is greater, but so is its complexity. The system error rate is reduced by the fact that multiple biometric features are combined, thus solving the weakness of the single biometric. Performance of individual authentication through palm-print- and iris-based bimodal biometric system is proposed in the present study. To this end, Log-Gabor filter and BSIF (Binarised Statistical Image Feature) coefficients are employed to obtain the iris and palm-print traits, and subsequently selection of the features vector is conducted with mRMR (Minimum Redundancy Maximum Relevance) transforms in higher coefficients. To match the iris or palm-print feature vector, the Hamming Distance is applied. According to the experiment outcomes, the proposed system not only has a significantly high recognition rate but it also affords greater security compared to the single biometric system.	authentication;biometrics;coefficient;database;experiment;feature vector;fingerprint;hamming distance;log gabor filter;modal logic;multimodal interaction;palm print;relevance	Bilal Attallah;Amina Serir;Youssef Chahir;Abdelwahhab Boudjelal	2017	2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)	10.1109/ATSIP.2017.8075600	word error rate;hamming distance;redundancy (engineering);feature extraction;iris recognition;artificial intelligence;biometrics;feature vector;pattern recognition;computer science;authentication	SE	31.164462795390715	-61.45903980995774	131410
0b875d54bd3ebb87ff63f0cb46bf53e1fa3299c6	robust local binary pattern feature sets for periocular biometric identification	best feature extraction approach;kernel correlation feature analysis;pixel feature extraction kernel force iris recognition discrete cosine transforms;kernel;distance measure;biometrics access control;large dataset;robust local binary pattern feature sets;walsh transform;filter based techniques;iris recognition;local binary pattern;force;face recognition;statistical analysis;local features;discrete cosine transforms;feature extraction;pixel;periocular biometric identification;facial features;local feature extraction methods;statistical analysis biometrics access control face recognition feature extraction;feature analysis;facial features periocular biometric identification robust local binary pattern feature sets best feature extraction approach filter based techniques local feature extraction methods local walsh transform binary pattern encoding kernel correlation feature analysis;experience base;local walsh transform binary pattern encoding;extraction method;false accept rate	In this paper, we perform a detailed investigation of various features that can be extracted from the periocular region of human faces for biometric identification. The emphasis of this study is to explore the BEST feature extraction approach used in stand-alone mode without any generative or discriminative subspace training. Simple distance measures are used to determine the verification rate (VR) on a very large dataset. Several filter-based techniques and local feature extraction methods are explored in this study, where we show an increase of 15% verification performance at 0.1% false accept rate (FAR) compared to raw pixels with the proposed Local Walsh-Transform Binary Pattern encoding. Additionally, when fusing our best feature extraction method with Kernel Correlation Feature Analysis (KCFA) [36], we were able to obtain VR of 61.2%. Our experiments are carried out on the large validation set of the NIST FRGC database [6], which contains facial images from environments with uncontrolled illumination. Verification experiments based on a pure 1–1 similarity matrix of 16028×8014 (~128 million comparisons) carried out on the entire database, where we find that we can achieve a raw VR of 17.0% at 0.1% FAR using our proposed Local Walsh-Transform Binary Pattern approach. This result, while may seem low, is more than the NIST reported baseline VR on the same dataset (12% at 0.1% FAR), when PCA was trained on the entire facial features for recognition [6].	baseline (configuration management);binary pattern (image generation);biometrics;experiment;feature extraction;feature model;hadamard transform;image resolution;local binary patterns;pixel;similarity measure;test set;uncontrolled format string	Juefei Xu;Miriam Cha;Joseph L. Heyman;Shreyas Venugopalan;Ramzi Abiantun;Marios Savvides	2010	2010 Fourth IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS)	10.1109/BTAS.2010.5634504	pattern recognition;facial recognition system;computer vision;kernel;local binary patterns;speech recognition;hadamard transform;feature extraction;computer science;machine learning;pattern recognition;iris recognition;force;pixel	Vision	34.54665978861348	-58.55363551485658	131459
e5e453771109f60be145c2b09121f7e85fa5b1dc	face recogntion in open world environment	image morphing;false alarm rate face recognition open world environment unregistered probe faces local binary pattern feature lbp feature gabor feature face recognizer borderline patterns morphing procedure target faces random nontarget faces grid search optimal morphing degree pair and operator complementary parallel classifier robust sparse coding method feret target person appearance;face face recognition target recognition training classification algorithms feature extraction radio access networks;face recognition;feature extraction;image morphing face recognition feature extraction;parallel sub classifiers open world face recognition face morphing local binary pattern gabor wavelets	Face recognition in open world environment is a very challenging task due to variant appearances of the target persons and a large scale of unregistered probe faces. In this paper we combine two parallel classifiers, one based on the Local Binary Pattern (LBP) feature and the other based on the Gabor features, to build a specific face recognizer for each target person. Faces used for training are borderline patterns obtained through a morphing procedure combing target faces and random non-target ones. Grid-search is applied to find an optimal morphing-degree-pair. By using an AND operator to integrate the prediction of the two complementary parallel classifiers, many false positives are eliminated in the final results. The proposed algorithm is compared with the Robust Sparse Coding method, using selected celebrities as the target persons and the images from FERET as the non-target faces. Experimental results suggest that the proposed approach is better at tolerating the distortion of the target person's appearance and has a lower false alarm rate.	algorithm;distortion;feret (facial recognition technology);facial recognition system;finite-state machine;local binary patterns;morphing;neural coding;open world;sparse approximation	Jieqiong Qiu;Ya Zhang;Jun Sun	2013	2013 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2013.6706423	facial recognition system;computer vision;face detection;feature extraction;computer science;machine learning;pattern recognition;three-dimensional face recognition	Vision	33.23206691110971	-55.255869701436445	131511
215107131fea5f32b164459233036a93e756c6a2	end to end video segmentation for driving : lane detection for autonomous car		Safety and decline of road traffic accidents remain important issues of autonomous driving. Statistics show that unintended lane departure is a leading cause of worldwide motor vehicle collisions, making lane detection the most promising and challenge task for self-driving. Today, numerous groups are combining deep learning techniques with computer vision problems to solve self-driving problems. In this paper, a Global Convolution Networks (GCN) model is used to address both classification and localization issues for semantic segmentation of lane. We are using color-based segmentation is presented and the usability of the model is evaluated. A residual-based boundary refinement and Adam optimization is also used to achieve state-of-art performance. As normal cars could not afford GPUs on the car, and training session for a particular road could be shared by several cars. We propose a framework to get it work in real world. We build a real time video transfer system to get video from the car, get the model trained in edge server (which is equipped with GPUs), and send the trained model back to the car.		Wenhui Zhang;Tejas Mahale	2018	CoRR			Vision	29.31778926128356	-53.60382160947578	131544
5484ad04ac0a256b51fd1a3eae48483480862ab1	a survey on ear biometrics	person verification identification;ear recognition detection;biometrics;earprints	Recognizing people by their ear has recently received significant attention in the literature. Several reasons account for this trend: first, ear recognition does not suffer from some problems associated with other non-contact biometrics, such as face recognition; second, it is the most promising candidate for combination with the face in the context of multi-pose face recognition; and third, the ear can be used for human recognition in surveillance videos where the face may be occluded completely or in part. Further, the ear appears to degrade little with age. Even though current ear detection and recognition systems have reached a certain level of maturity, their success is limited to controlled indoor conditions. In addition to variation in illumination, other open research problems include hair occlusion, earprint forensics, ear symmetry, ear classification, and ear individuality.  This article provides a detailed survey of research conducted in ear detection and recognition. It provides an up-to-date review of the existing literature revealing the current state-of-art for not only those who are working in this area but also for those who might exploit this new approach. Furthermore, it offers insights into some unsolved ear recognition problems as well as ear databases available for researchers.	biometrics;capability maturity model;database;facial recognition system;open research	Ayman Abaza;Arun Ross;Christina Hebert;Mary Ann F. Harrison;Mark S. Nixon	2013	ACM Comput. Surv.	10.1145/2431211.2431221	speech recognition;biometrics	Vision	29.04805314329659	-61.32841365926228	131563
9af7fafab3964e38be38688058a6fd499812f7aa	recognition of traffic signs by convolutional neural nets for self-driving vehicles				Wael Farag	2018	KES Journal	10.3233/KES-180385	machine learning;computer science;artificial neural network;pattern recognition;artificial intelligence	Vision	27.88364620951836	-57.91917116229184	131717
ebe6cc0ecfa21d56f16969cf2c7f99227c245acf	superpixels shape analysis for carried object detection	histograms;image segmentation;greater cleveland regional transit authority superpixel shape analysis object detection video surveillance systems public safety appearance modeling distance learning attribute based reidentification methods semantic attributes;shape;image color analysis;feature extraction;video surveillance object detection;clothing;shape image color analysis image segmentation clothing cameras histograms feature extraction;cameras	Video surveillance systems generate enormous amounts of data which makes the continuous monitoring of video a very difficult task. Re-identification of subjects in video surveillance systems plays a significant role in public safety. Recent work has focused on appearance modeling and distance learning to establish correspondence between images. However, real-life scenarios suggest that the majority of clothing worn tends to be non-discriminative. Attributes- based re-identification methods try to solve this problem by incorporating semantic attributes which are mid-level features learned a prior. In this paper we present a framework to recognize attributes with applications to carried objects detection. We present a supervised approach based on the contours and shapes of superpixels and histogram of oriented gradients. An experimental evaluation is described using a dataset that was recorded at the Greater Cleveland Regional Transit Authority.	closed-circuit television;cluster analysis;color;experiment;gradient;histogram of oriented gradients;object detection;real life;shape analysis (digital geometry);shape context;supervised learning	Blanca Delgado;Khalid Tahboub;Edward J. Delp	2016	2016 IEEE Winter Applications of Computer Vision Workshops (WACVW)	10.1109/WACVW.2016.7470116	computer vision;geography;pattern recognition;computer graphics (images)	Vision	33.69369154231194	-52.88868866821768	131743
0fd0c5984ed14c593854f0218ce9b1bd3b06e306	a fast mobile face recognition system for android os based on eigenfaces decomposition	mahalanobis distance;mobile device;face recognition;system design;face detection	This paper presents a speed-optimized face recognition system designed for mobile devices. Such applications may be used in the context of pervasive and assistive computing for the support of elderly suffering from dementia in recognizing persons or for the development of cognitive memory games Eigenfaces decomposition and Mahalanobis distance calculation have been utilized whereas the recognition application has been developed for Android OS. The initial implementation and the corresponding results have proven the feasibility and value of the proposed system.	algorithm;android;eigenface;face detection;facial recognition system;illumination (image);mobile device;operating system;pattern recognition;personal digital assistant;pervasive informatics;pose (computer vision);smartphone	Charalampos Doukas;Ilias Maglogiannis	2010		10.1007/978-3-642-16239-8_39	facial recognition system;computer vision;face detection;speech recognition;object-class detection;computer science;mahalanobis distance;pattern recognition;mobile device;systems design	Mobile	30.157889289822993	-60.01482744183096	131786
30c22a1637587f1881879da02d682ce14d63403a	fish identification in underwater video with deep convolutional neural network: snumedinfo at lifeclef fish task 2015		This paper describes our participation at the LifeCLEF Fish task 2015. The task is about video-based fish identification. Firstly, we applied foreground detection method with selective search to extract candidate fish object window. Then deep convolutional neural network is used to classify fish species per window. Classification results are post-processed to produce final identification output. Experimental results showed effective performance in spite of challenging task condition. Our approach achieved best performance in this task.	artificial neural network;convolutional neural network;mopy fish	Sungbin Choi	2015			computer science;artificial intelligence;machine learning;communication	AI	27.98454167637575	-59.809652005012445	131795
e1320c7b41ab925e86b75bb1ccbe88e2911547d2	directional gaussian derivative filter based palmprint authentication	databases;palmprint authentication;image coding;directional gaussian derivative filter;hamming codes;gaussian processes;biometrics access control;image matching;authentication;biometrics;data mining;message authentication biometrics access control feature extraction filtering theory gaussian processes hamming codes image coding image matching;distance measurement;hamming distance;fingerprint recognition;feature extraction;hamming distance directional gaussian derivative filter palmprint authentication feature extraction image matching;filters authentication feature extraction biometrics data mining pixel fingerprint recognition iris hamming distance phase measurement;message authentication;filtering theory	This paper presents a novel approach of palmprint authentication based on the directional gaussian derivative filters. In this approach, the palmprint image is firstly filtered using some directional Gaussian derivative filters in different directions. Then the orientation and the phase of each pixel of the palmprint image are extracted to form a feature. In the matching phase, the Hamming distance is employed to measure the similarity of different palmprint. The experimental results show that the proposed approach can effectively discriminate between palmprints and the EER is about 0.139%.	authentication;enhanced entity–relationship model;fingerprint;hamming distance;pixel	Xiukun Li;Xiangqian Wu;Kuanquan Wang	2008	2008 International Conference on Computational Intelligence and Security	10.1109/CIS.2008.92	message authentication code;computer vision;hamming distance;feature extraction;computer science;pattern recognition;data mining;gaussian process;authentication;hamming code;mathematics;fingerprint recognition;biometrics;statistics	Robotics	35.14493965903273	-61.23725681143429	131835
5690460953e16049e79075b397e2c0095bf1ee8f	design of an object-based video retrieval system using sca and invariant moments		In recent years, it has become more important to process multimedia data efficiently. Especially, in the case of multimedia information, the user in- terface technique and retrieval technique are necessary. Video information takes large portion among the multimedia information. In this paper, we present a video retrieval system. For the video retrieval, we propose SCA (Single Color- izing Algorithm) and CSB (Color- and Spatial-based Binary) tree map algo- rithm. The SCA reduces the dimensions of the color features. The CSB tree map is a kind of clustering algorithm that increases the number of groups by binary tree structure, and determines suitable numbers of the group to extract optimized objects. In addition, we apply invariant moments to above pre- processed images. Through experiments, the proposed video retrieval system presents high performance comparing with existing retrieval methods.		Jang-Hui Kim;Dae-Seong Kang	2009		10.1007/978-3-642-10844-0_59	computer vision;visual word;computer science;multimedia;information retrieval	Vision	38.808768175911005	-60.69130114940052	131874
01f04f6faa3f12eda2bbbdf36ef26a7f22bf39f7	robust face detection for video summary using illumination-compensation and morphological processing	mathematical morphology;image processing;video signal processing;golden ratio;low complexity;skin color;face recognition;video signal processing face recognition image colour analysis mathematical morphology;image colour analysis;morphological image processing;robust face detection;video summary;robustness face detection skin change detection algorithms colored noise neural networks aerospace engineering image color analysis image processing information analysis;face detection;illumination compensation;noise removal robust face detection video summary illumination compensation skin color morphological image processing;high performance;noise removal	This paper presents a simple and robust face detection algorithm that can be utilized to video summary. Because the characteristic of the face can be change from unexpected condition like as shadows or lighting, we firstly process the illumination-compensation for maintaining face components. And then, we analyze color-based face region in YCbCr space to obtain the skin color. Also, we try the morphological image processing called closing algorithm to improve the detection performance and reducing the number of false detected face regions. The steps of the closing algorithm are dilation and erosion that find the candidates in face region and removes noise from the non-face region. For localization of the face region, we make a proper face location based on golden ratio. We evaluate our algorithm in the various genres. Experimental results demonstrate the effectiveness of our face detection algorithm that leads 96.7 % precision ratio on average. The proposed method is applicable to video summary because of these high performances with low complexity.	algorithm;closing (morphology);complexity;dilation (morphology);erosion (morphology);face detection;image processing;mathematical morphology;motion compensation;performance	Jae-Ung Yun;Hyung-Jin Lee;Anjan Kumar Paul;Joong-Hwan Baek	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.638	computer vision;speech recognition;object-class detection;computer science;computer graphics (images)	Vision	37.360210816349785	-63.36988281032807	131890
a996b518cb5dcab27713cfda6a29826e49c9df1e	text zone classification using unsupervised feature learning	text classification text zone classification unsupervised feature learning digitization process document zone classification hand crafted features zone classifier training database dependent feature feature vector patches extraction pooling feature encoding visual word codebook text classifier;visual databases feature extraction image classification image coding text detection unsupervised learning	Text zone classification is a vital step in the digitization process, without which OCR systems perform poorly. Prior methods to document zone classification have relied on large sets of hand-crafted features for training zone classifiers. Such features are usually database-dependent, and their computation is time consuming. In this work we propose a novel method for text zone classification that relies on the approach of unsupervised feature learning. Within our method, feature vectors of document zones are automatically learned by patches extraction, encoding and pooling, where feature encoding is based on a codebook of visual words. The training phase of the text classifier takes into consideration the unbalance between text zones and non-text zones of all types. The proposed method has been tested on publicly available standard databases, and achieved competitive or better results compared to state-of-the-art methods. The results show that our approach matches well the task of text classification, and is robust to zone shapes, orientations and size.	codebook;computation;database;document classification;feature learning;text-based user interface	Nibal Nayef;Jean-Marc Ogier	2015	2015 13th International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2015.7333867	computer science;machine learning;linear classifier;pattern recognition;data mining;feature	AI	35.816120076526516	-64.5173667064726	131951
489a9da80b3c61b383108ac39c0b1942585bd4dd	video-based human action recognition using spatial pyramid pooling and 3d densely convolutional networks		In recent years, the application of deep neural networks to human behavior recognition has become a hot topic. Although remarkable achievements have been made in the field of image recognition, there are still many problems to be solved in the area of video. It is well known that convolutional neural networks require a fixed size image input, which not only limits the network structure but also affects the recognition accuracy. Although this problem has been solved in the field of images, it has not yet been broken through in the field of video. To address the input problem of fixed size video frames in video recognition, we propose a three-dimensional (3D) densely connected convolutional network based on spatial pyramid pooling (3D-DenseNet-SPP). As the name implies, the network structure is mainly composed of three parts: 3DCNN, DenseNet, and SPPNet. Our models were evaluated on a KTH dataset and UCF101 dataset separately. The experimental results showed that our model has better performance in the field of video-based behavior recognition in comparison to the existing models.		Wanli Yang;Yimin Chen;Chen Huang;Ming-ke Gao	2018	Future Internet	10.3390/fi10120115	computer network;convolutional neural network;computer science;artificial neural network;pooling;pyramid;artificial intelligence;pattern recognition	Vision	28.378407341088586	-52.527695283453504	131979
d9cc8bc5c4a4b29ab40f75b721bd9e5140d2baf6	object detection for crime scene evidence analysis using deep learning		Object detection is the key module in most visual-based surveillance applications and security systems. In crime scene analysis, the images and videos play a significant role in providing visual documentation of a scene. It allows police officers to recreate a scene for later analysis by detecting objects related to a specific crime. However, due to the presence of a large volume of data, the task of detecting objects of interest is very tedious for law enforcement agencies. In this work, we present a Faster R-CNN (Region-based Convolutional Neural Network) based real-time system, which automatically detects objects which might be found in an indoor environment. To test the effectiveness of the proposed system, we applied it to a subset of ImageNet containing 10 object classes and Karina dataset. We achieved an average accuracy of 74.75%, and the mean time taken to detect objects per image was 0.12 seconds in Nvidia-TitanX GPU.	addendum;algorithm;convolutional neural network;deep learning;documentation;graphics processing unit;imagenet;network architecture;object detection;real-time clock;real-time computing;real-time locating system;sensor;super-resolution imaging;titan (supercomputer)	Surajit Saikia;Eduardo Fidalgo;Enrique Alegre;Laura Fernández-Robles	2017		10.1007/978-3-319-68548-9_2	deep learning;documentation;convolutional neural network;artificial intelligence;crime scene;computer science;computer vision;pattern recognition;object detection;law enforcement	Vision	29.77994403131338	-52.91959167978256	132096
8663099986e81fd6cd063cf2adf6c2f9c8e2a9c5	improved localization accuracy by locnet for faster r-cnn based text detection		Although Faster R-CNN based approaches have achieved promising results for text detection, their localization accuracy is not satisfactory in certain cases. In this paper, we propose to use a LocNet to improve the localization accuracy of a Faster R-CNN based text detector. Given a proposal generated by region proposal network (RPN), instead of predicting directly the bounding box coordinates of the concerned text instance, the proposal is enlarged to create a search region so that conditional probabilities to each row and column of this search region can be assigned, which are then used to infer accurately the concerned bounding box. Experiments demonstrate that the proposed approach boosts the localization accuracy for Faster R-CNN based text detection significantly. Consequently, our new text detector has achieved superior performance on ICDAR-2011, ICDAR-2013 and MULTILIGUL text detection benchmark tasks.	benchmark (computing);minimum bounding box;reverse polish notation	Zhuoyao Zhong;Lei Sun;Qiang Huo	2017	2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2017.155	task analysis;artificial intelligence;computer science;pattern recognition;conditional probability;visualization;feature extraction;detector;minimum bounding box	Robotics	30.512223128568483	-52.723451298351854	132342
f20bfe99ff678e54a196e8d6d8fa8901b69b66f3	the description and retrieval of a sequence of moving objects using a shape variation map	mpeg 7 visual descriptor;moving object;shape variation map;shape descriptor;shape variation;shape sequence;object movement;cumulant	Motion information in a video clip plays an important role in characterizing the content of the clip. In this paper, we propose the  shape variation descriptor  to describe and retrieve video clips based on the shape variation caused by object movement along time. Similar to accumulator cells, a  shape variation map  was first constructed by cumulating binary shape images in the sequence of the video clip. A shape descriptor called ART, obtained from the map, then becomes a feature set for the shape variation of the clip. Experimental results over hundreds of video and animation clips show that our approach of retrieving video clips based on shape variation compares favorably against an alternative edge based method.		Min-Seok Choi;Whoi-Yul Kim	2004	Pattern Recognition Letters	10.1016/j.patrec.2004.05.010	active shape model;computer vision;heat kernel signature;shape analysis;mathematics;multimedia;statistics;cumulant;computer graphics (images)	Vision	38.76157193175398	-52.14421968078463	132530
b2b6a96b89675b1957801cd9b545c02536faefc6	deep convolutional triplet network for quantitative medical image analysis with comparative case study of gamma image classification	formatting;style;styling	Alternative feature generation for quantitative image analysis is proposed. The proposed method reorganize Deep Convolutional Neural Networks to learn representation in Triplet Network. The features are compared with texture features using series of classifiers in a Gamma image classification task that contains visual information but has no known suitable features. Experiment show that features from the Triplet Network method outperform in the classification task suggesting a useful way of extracting feature for task without known suitable feature but advantageous for further investigation.	artificial neural network;computer vision;convolutional neural network;image analysis;medical image computing;neural network software;triplet state	Phawis Thammasorn;Landon Wootton;Eric Ford;Matthew Nyflot	2017	2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2017.8217813	computer science;artificial intelligence;machine learning;convolutional neural network;feature extraction;contextual image classification;comparative case	Vision	24.640237681596982	-53.514480286978994	132556
ff97121d55eba18eb78642da548a4fad4a29add6	3-d gabor convolutional neural network for damage mapping from post-earthquake high resolution images		Post-earthquake high resolution (HR) remote sensing image classification is crucial for disaster assessment and emergency rescue. 3-D convolutional neural networks (3-D CNNs) exhibit promising performance in remote sensing image classification. However, 3-D CNNs lack the theoretical underpinnings to perform multiresolution approximation for filter learning in view of the scale variance of natural objects. Gabor filtering can effectively extract multiresolution spatial information including edges and textures, which have a potential to reinforce the robustness of learned features in 3-D CNNs against the orientation and scale changes. In this paper, we propose a combined 3-D convolutional neural network and Gabor filters (GNN) method for post-earthquake HR image classification. Instead of choosing a single scale, GNN extends the spatial information to several scales by Gabor filters to take advantage of correlations among multiple scales for damage mapping. The experimental results show that GNN can reflect the multiscale information of complex scenes, obtain good classification results for mapping post-earthquake damage using HR remote sensing images.	convolutional neural network	Yanling Hao;Genyun Sun;Aizhu Zhang;Hui Huang;Jun Rong;Ping Ma;Paul C Rogers	2018		10.1007/978-3-030-00563-4_14	convolutional neural network;robustness (computer science);filter (signal processing);computer vision;spatial analysis;gabor filter;contextual image classification;computer science;artificial intelligence	AI	38.35100301503825	-56.020725581375565	132590
b54088b296660d2fecb85a6635e599f576211d1b	comparative evaluation of signal-based and descriptor-based similarity measures for sar-optical image matching		This paper compares different similarity measures for the matching of very-high-resolution SAR and optical images over urban areas. It is meant to provide guidance about the performance of both signal-based and descriptor-based similarity measures in the context of this non-trivial case of multi-sensor correspondence matching. Using an automatically generated training dataset, thresholds for the distinction between correct matches and wrong matches are determined. It is shown that descriptor-based similarity measures outperform signal-based similarity measures significantly.	data descriptor;image registration;image resolution	Chunping Qiu;Michael Schmitt;Xiao xiang Zhu	2017	2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2017.8128240	optical imaging;artificial intelligence;computer vision;synthetic aperture radar;adaptive optics;computer science	Vision	37.45906080769736	-55.91806461014104	132612
04e32b7741a172b0b22411eef2db78f3e4199017	design of a neural networks classifier for face detection	conference;meeting	Face detection and recognition has many applications in a variety of fields such as security system, videoconferencing and identification. Face classification is currently implemented in software. A hardware implementation allows real-time processing, but has higher cost and time to-market. The objective of this work was to implement a classifier based on neural networks MLP (Multi-layer Perception) for face detection. The MLP was used to classify face and non-face patterns. The system described using C language on a P4 (2.4 Ghz) to extract weight values. Then a Hardware implementation achieved using VHDL based Methodology. We targeted Xilinx FPGA as the implementation support.	activation function;algorithm;backpropagation;experiment;face detection;feature extraction;field-programmable gate array;machine learning;memory-level parallelism;neural networks;performance;preprocessor;real-time clock;real-time computing;real-time locating system;software propagation;vhdl	Fethi Smach;Mohamed Atri;Johel Mitéran;Mohamed Abid	2005			artificial intelligence;machine learning;data mining	Vision	25.554889389185497	-60.880246417809644	132661
0da0a5158e1dab17e8973d9ed2b25acf093409fd	learning-based depth estimation from 2d images using gist and saliency	telecomunicaciones;saliency 2d to 3d image conversion depth maps gist descriptor;query processing image fusion learning artificial intelligence;three dimensional displays databases image edge detection visualization color image color analysis filtering;depth fusion learning based depth estimation 2d images 3d displays automatic 2d to 3d conversion algorithms machine learning query image photometric similarity gist descriptor	Although there has been a significant proliferation of 3D displays in the last decade, the availability of 3D content is still scant compared to the volume of 2D data. To fill this gap, automatic 2D to 3D conversion algorithms are needed. In this paper, we present an automatic approach, inspired by machine learning principles, for estimating the depth of a 2D image. The depth of a query image is inferred from a dataset of color and depth images by searching this repository for images that are photometrically similar to the query. We measure the photometric similarity between two images by comparing their GIST descriptors. Since not all regions in the query image require the same visual attention, we give more weight in the GIST-descriptor comparison to regions with high saliency. Subsequently, we fuse the depths of the most similar images and adaptively filter the result to obtain a depth estimate. Our experimental results indicate that the proposed algorithm outperforms other state-of-the-art approaches on the commonly-used Kinect-NYU dataset.	2d to 3d conversion;algorithm;gist;kinect;machine learning;stereo display	Jose L. Herrera;Janusz Konrad;Carlos R. del-Blanco;Narciso N. García	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351709	computer vision;computer science;pattern recognition;information retrieval	Vision	36.5630963831389	-54.74367673591025	132878
27434a48e7e10b011c862c297d5c29110816ec5c	what characterizes a shadow boundary under the sun and sky?	statistical learning shadow boundary sun sky robust shadow detection color image visual feature extraction illumination occluders support vector machine svm;support vector machines;computer graphics;edge detection;shadow detection;visualization;statistical learning;vectors;image edge detection;image color analysis;image colour analysis;feature extraction;lighting image edge detection sun image color analysis visualization support vector machines vectors;sun;visual features;lighting;support vector machine;physical model;support vector machines computer graphics feature extraction image colour analysis;color image	Despite decades of study, robust shadow detection remains difficult, especially within a single color image. We describe a new approach to detect shadow boundaries in images of outdoor scenes lit only by the sun and sky. The method first extracts visual features of candidate edges that are motivated by physical models of illumination and occluders. We feed these features into a Support Vector Machine (SVM) that was trained to discriminate between most-likely shadow-edge candidates and less-likely ones. Finally, we connect edges to help reject non-shadow edge candidates, and to encourage closed, connected shadow boundaries. On benchmark shadow-edge data sets from Lalonde et al. and Zhu et al., our method showed substantial improvements when compared to other recent shadow-detection methods based on statistical learning.	benchmark (computing);color image;machine learning;shadow volume;support vector machine	Xiang Huang;Gang Hua;Jack Tumblin;Lance Williams	2011	2011 International Conference on Computer Vision	10.1109/ICCV.2011.6126331	support vector machine;computer vision;computer science;machine learning;computer graphics (images)	Vision	34.13975765082364	-54.10984692985865	132912
b5cd7c414388cd42d2df85af6ec99a2a90762b84	shape features for candidate photo selection in sketch recognition	databases;sketch recognition;face recognition sketch recognition shape features;face recognition shape features candidate photo selection sketch recognition search procedure complex state of the art technique real mug shot databases;face recognition;shape;image edge detection;feature extraction;shape features;visual databases face recognition shape recognition;shape face image edge detection face recognition databases forensics feature extraction;face;forensics	Sketch recognition for forensic applications is a very challenging task and several solutions have been recently proposed. Considering that real mug shot databases can be very large, one important aspect to consider in this scenario is also the efficiency of the search procedure. This work proposes the use of shape features for a preliminary selection of the candidate photos to be successively analyzed by more complex state-of-the-art techniques. The proposed features can be computed and matched in a very short time, and at the same time they are able to significantly reduce the search space, thus allowing to speed up the recognition process.	database;experiment;sketch recognition	Simone Buoncompagni;Annalisa Franco;Dario Maio	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.304	facial recognition system;face;computer vision;speech recognition;feature;feature extraction;shape;intelligent character recognition;computer science;pattern recognition;three-dimensional face recognition;geometry;3d single-object recognition;forensic science;sketch recognition	Vision	36.6130438309555	-55.57211686054714	132966
ff371d2419e2ad87c277bfe123bde948b52d15b2	offline signature verification based on pseudo-cepstral coefficients	ls svm model;databases;histograms;least squares approximations;handwriting recognition;ls svm;unique minimum phase sequence;support vector machines;support vector machines cepstral analysis feature extraction handwriting recognition least squares approximations;image databases;signature verification;forgery;biometrics;ls svm model offline signature verification pseudocepstral coefficient pressure distribution static image handwritten signature offline verification system gray scale images unique minimum phase sequence feature vector forgery;spectrum;gray scale images;static image;system performance;gray scale;handwriting recognition image analysis information analysis gray scale histograms system performance image databases spatial databases robustness forgery;feature vector;cepstral analysis;pseudocepstral coefficient;shape;ls svm offline signature verification;feature extraction;pressure distribution;spatial databases;offline verification system;offline signature verification;writing;robustness;image analysis;handwritten signature;information analysis	"""Features representing information about pressure distribution from a static image of a handwritten signature are analyzed for an offline verification system. From gray-scale images, its histogram is calculated and used as """"spectrum'' for calculation of pseudo-cepstral coefficients. Finally, the unique minimum-phase sequence is estimated and used as feature vector for signature verification. The optimal number of pseudo-coefficients is estimated for best system performance. Experiments were carried out using a database containing signatures from 100 individuals. The robustness of the analyzed system for simple forgeries is tested out with a LS-SVM model. For the sake of completeness, a comparison of the results obtained by the proposed approach with similar works published using pseudo-dynamic feature for offline signature verification is presented."""	antivirus software;cepstrum;coefficient;database;feature vector;grayscale;minimum phase;online and offline;pseudo-spectral method	Jesus Francisco Vargas Bonilla;Miguel Angel Ferrer-Ballester;Carlos Manuel Travieso-González;Jesús B. Alonso	2009	2009 10th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2009.68	spectrum;support vector machine;image analysis;speech recognition;feature vector;feature extraction;shape;computer science;machine learning;pattern recognition;histogram;handwriting recognition;pressure coefficient;data analysis;writing;grayscale;biometrics;robustness	Robotics	34.883880946682986	-61.75920952798737	133029
4d7c95b55a69616d7081215ed666077cbcef991f	visual phrases for automatic images annotation	visual phrase;topological criterion;automatic image annotation;image fusion;bag of visual words approach;semantics;image retrieval image fusion;regions;visualization;region of interest;automatic annotation;iron artificial intelligence noise measurement;bag of visual words approach visual phrase automatic image annotation topological criterion;bag of visual words;radio access networks;economic indicators;image retrieval	"""Visual characteristics of objects of a class vary with the considered instance and the shooting conditions. In this paper we proposed a visual characterization of object parts, called """"Visual Phrase"""", robust to these variations. A Visual Phrase is a set of regions of interest built according to predefined criteria; a topological criterion was studied in this paper. An automatic annotation method is proposed based on our definition and characterization of Visual Phrases. Experiments on VOC2009 collection is presented, we show that the late fusion of our method with a standard Bag of Visual Words approach on full images provides better results than those obtained via each approach independently."""	bag-of-words model in computer vision;experiment;region of interest	Rami Albatal;Philippe Mulhem;Yves Chiaramella	2010	2010 International Workshop on Content Based Multimedia Indexing (CBMI)	10.1109/CBMI.2010.5529909	natural language processing;computer vision;visualization;image retrieval;computer science;economic indicator;pattern recognition;semantics;bag-of-words model in computer vision;image fusion;automatic image annotation;information retrieval;region of interest	Vision	38.347719027049585	-62.67952559244433	133154
84da0c3cf6740c0510ee0b364634b793ab497aa1	a spontaneous facial expression recognition method using head motion and aam features	hidden markov models active appearance model mathematical model accuracy equations educational institutions pain;facial expression recognition;head motion;aam;artificial intelligent;visual databases artificial intelligence face recognition feature extraction;face recognition;spontaneous expression recognition;feature extraction;infrared facial expression database spontaneous facial expression recognition method aam features artificial intelligence head motion features static appearance feature extraction eyes location algorithm;artificial intelligence;facial expression;infrared;two layer classification spontaneous expression recognition head motion aam eyes location;eyes location;two layer classification;visual databases	Facial expression recognition by artificial intelligence has become a hotspot and produced promising results in recent years. However, most recent work focuses on posed expressions whose involved muscles and dynamics are different from those of spontaneous ones. So this paper proposed a novel two-layer approach, combining dynamic head motion features with static appearance features, to recognize spontaneous expressions. Different from other methods, our method improved an existing eyes location algorithm and explored new head motion features calculated from locations of pupils to recognize fear firstly, and then used appearance features extracted by the AAM algorithm to recognize the remaining images. Comparative experiments on the natural visible and infrared facial expression database (NVIE) show the effectiveness of our approach.	active appearance model;algorithm;apex (geometry);artificial intelligence;attachments;automatic acoustic management;experiment;java hotspot virtual machine;onset (audio);real life;spontaneous order	Yanpeng Lv;Shangfei Wang	2010	2010 Second World Congress on Nature and Biologically Inspired Computing (NaBIC)	10.1109/NABIC.2010.5716302	facial recognition system;computer vision;speech recognition;infrared;feature extraction;computer science;artificial intelligence;pattern recognition;facial expression	AI	31.205357432563417	-59.167543838960114	133185
5bda86b5b5d57e4a57180ff262c2baf8bca85835	visual tracking using learned color features	image color analysis target tracking visualization encoding lighting robustness computer vision;video sequences visual tracking color feature space learning robust object tracking computer vision color based trackers luminance information color representations image description tracking sequences varying lighting conditions camera views linear transformation pixel value encoding feature extraction marginal regression sparse feature code calculation;computer vision;visualization;image color analysis;robustness;feature learning visual tracking color features marginal regression;lighting;target tracking;video coding brightness computer vision feature extraction image colour analysis image resolution image sensors image sequences learning artificial intelligence object tracking regression analysis;encoding	Robust object tracking is a challenging task in computer vision. Color features have been popularly used in visual tracking. However, most conventional color-based trackers either rely on luminance information or use simple color representations for image description. During the tracking sequences, the perceived color of the target may change because of the varying lighting conditions. In this paper, we learn the color patterns offline from pixels sampled from images across different camera views. In the new color feature space, the proposed tracking method performs robustly in various environment. The new color feature space is learned by learning a linear transformation and a dictionary to encode pixel values. To speedup the feature extraction, we use the marginal regression to calculate the sparse feature codes. Experimental results demonstrate that significant improvement can be achieved by using our learned color features, especially on the video sequences with complicated lighting conditions.	baseline (configuration management);code;color;computer vision;dictionary;encode;feature extraction;feature vector;moe;marginal model;multitier architecture;neural coding;online and offline;pixel;sparse matrix;speedup;tier 1 network;tier 2 network;video tracking	Ting Liu;Rahul Rama Varior;Gang Wang	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178316	color histogram;false color;rgb color model;computer vision;feature detection;color quantization;hsl and hsv;visualization;color normalization;color depth;color image;computer science;pattern recognition;lighting;color balance;feature;encoding;robustness;computer graphics (images)	Vision	36.74926806404875	-52.17102347279923	133311
03c0a644f8cfa6f78e594a0535e43ceb58b7f887	the implications of iris-recognition technologies	databases;authentication;biometrics;iris recognition;implications;data mining;gait body motion iris recognition technologies physiological characteristics physical biometrics dna fingerprints hand geometry vein patterns face structure skin luminescence palm prints iris patterns periocular features retina patterns ear shape lip prints heartbeats tongue prints body odor body scent signature handwriting vocal prints keystroke dynamics;recognition;feature extraction;iris;technologies;iris recognition feature extraction databases authentication biometrics data mining	Biometrics are the unique characteristics of the individual that differentiate him or her from any other person. Down and Sands [1] explained that the physiological characteristics refer to the inherited traits that are shaped in the early embryonic stages of the human development. Physical biometrics include, among other things, DNA, fingerprints, hand geometry, vein patterns, face structure, skin luminescence, palm prints, iris patterns, periocular features, retina patterns, ear shape, lip prints, heartbeats, tongue prints, and body odor/scent [2]?[8]. Behavioral characteristics are not inherited but acquired and learned throughout the life of the individual [1]. These include, but also are not limited to, signature, handwriting, vocal prints, keystroke dynamics, and gait?body motion [3]. As a result, the biometrics of a person cannot be stolen, forgotten, or forged. It is what we are [2].	iris recognition	Anas Aloudat;Katina Michael	2016	IEEE Consumer Electronics Magazine	10.1109/MCE.2016.2556901	computer vision;speech recognition;feature extraction;computer science;iris recognition;authentication;computer security;biometrics;technology	Visualization	25.237846105190055	-64.76627278777651	133385
7f77b5909546c5a36248ce7a3bef75517a5c15d8	biosoft - a multimodal biometric database incorporating soft traits		Biometrics is an automated authentication mechanism that allows the identification or verification of individuals based on unique physiological and behavioral characteristics. In addition to novel biometric recognition frameworks and protocols, standard databases containing sample biometric traits are essential for validating the obtained results. In this paper, we introduce a new multimodal database named BioSoft which consists of biometric data collected from 75 individuals. In comparison to the already existing databases, BioSoft contains a set of 23 soft biometric traits corresponding to each enrolled individual. This property makes our database very useful due to the unavailability of any other manually extracted multimodal database incorporating soft biometric characteristics. Additionally, the primary biometric modalities of face, ear, iris, voice, handwriting and fingerprints (obtained from two different sensors) are present in this database. Thus our database contains both physiological and behavioral characteristics of individuals, thus making it applicable for validating a wide variety of approaches.	algorithm;authentication;baseline (configuration management);biometrics;database;experiment;fingerprint;handwriting recognition;multimodal interaction;sensor;unavailability	Debanjan Sadhya;Parth Pahariya;Rishi Yadav;Apoorv Rastogi;Ayush Kumar;Lakshya Sharma;Sanjay Kumar Singh	2017	2017 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA)	10.1109/ISBA.2017.7947693	unavailability;iris recognition;data mining;database;biometrics;handwriting;authentication;engineering	Vision	29.232139251088284	-63.27496932937958	133496
284be8be0c6bedc36dfe43229bc84345ab0aedc2	faster training of mask r-cnn by focusing on instance boundaries		We present an auxiliary task to Mask R-CNN, an instance segmentation network, which leads to faster training of the mask head. Our addition to Mask R-CNN is a new prediction head, the Edge Agreement Head, which is inspired by the way human annotators perform instance segmentation. Human annotators copy the contour of an object instance and only indirectly the occupied instance area. Hence, the edges of instance masks are particularly useful as they characterize the instance well. The Edge Agreement Head therefore encourages predicted masks to have similar image gradients to the groundtruth mask using edge detection filters. We provide a detailed survey of loss combinations and show improvements on the MS COCO Mask metrics compared to using no additional loss. Our approach marginally increases the model size and adds no additional trainable model variables. While the computational costs are increased slightly, the increment is negligible considering the high computational cost of the Mask R-CNN architecture. As the additional network head is only relevant during training, inference speed remains unchanged compared to Mask R-CNN. In a default Mask R-CNN setup, we achieve a training speed up of 29% and an overall improvement of 8.1% on the MS COCO metrics compared to the baseline.	algorithmic efficiency;baseline (configuration management);computer multitasking;depth map;discrete laplace operator;edge detection;entity–relationship model;gradient;human-based computation;image scaling;smoothing;sobel operator;sparse matrix;the mask;total loss;variable shadowing	Roland S. Zimmermann;Julien N. Siems	2018	CoRR		architecture;pattern recognition;speedup;artificial intelligence;coco;inference;computer science;edge detection	Vision	25.202941115956836	-52.41015577977278	133585
5f44bd89920a99bd9b77b2967a3f077f333cb936	partial matching of 3d cultural heritage objects using panoramic views	3d object retrieval;cultural heritage;3d pottery dataset;partial matching	In this paper, we present a method for partial matching and retrieval of 3D objects based on range image queries. The proposed methodology addresses the retrieval of complete 3D objects using range image queries that represent partial views. The core methodology relies upon Bag-of-Visual-Words modelling and enhanced Dense SIFT descriptor computed on panoramic views and range image queries. Performance evaluation builds upon standard measures and a challenging 3D pottery dataset originating from the Hampson Archaeological Museum collection.	3d modeling;bag-of-words model in computer vision;encode;museum informatics;performance evaluation;range imaging	Konstantinos Sfikas;Ioannis Pratikakis;Anestis Koutsoudis;Michalis A. Savelonas;Theoharis Theoharis	2014	Multimedia Tools and Applications	10.1007/s11042-014-2069-0	computer vision;cultural heritage;data mining;multimedia	Vision	36.969264411199205	-54.92662025593014	133780
273690ed19b2acf61a6d2b9a144f82ebecf6cf65	robust 3d local sift features for 3d face recognition	3d face recognition;depth information;facial region segmentation;3d local scale invariant feature transform	In this paper, a robust 3D local SIFT feature is proposed for 3D face recognition. For preprocessing the original 3D face data, facial regional segmentation is first employed by fusing curvature characteristics and shape band mechanism. Then, we design a new local descriptor for the extracted regions, called 3D local Scale-Invariant Feature Transform 3D LSIFT. The key point detection based on 3D LSIFT can effectively reflect the geometric characteristic of 3D facial surface by encoding the gray and depth information captured by 3D face data. Then, 3D LSIFT descriptor extends to describe the discrimination on 3D faces. Experimental results based on the common international 3D face databases demonstrate the higher-qualified performance of our proposed algorithm with effectiveness, robustness, and universality.	three-dimensional face recognition	Yue Ming;Yi Jin	2015		10.1007/978-3-319-22873-0_31	computer vision;speech recognition;pattern recognition;three-dimensional face recognition;mathematics;face hallucination	Vision	36.98598897378417	-58.76203630399029	133807
7203f97008828996d6f6e4e6a25fe56124c3b9a2	fusion of submanifold and local texture features for palmprint authentication	databases;biometrics access control feature extraction authentication data mining databases principal component analysis;biometrics access control;authentication;data mining;global features biometrics palmprints roc curve local features;palmprint recognition edge detection feature extraction image fusion image texture;feature extraction;principal component analysis;polyu pamprint dataset submanifold local texture feature fusion palmprint based authentication system locality sensitive discriminant analysis ground truth values directional edge information extraction directional local extrema pattern	In this paper, a palmprint based authentication system is proposed. The proposed system makes use of locality sensitive discriminant analysis and directional local extrema patterns. A locality sensitive discriminant analysis has the capability to project the palmprints into a new space, where the palmprints belonging to same class become closer, whereas those of different classes move far apart. Locality sensitive discriminant analysis gathers the discriminating information based on the local texture of the data and available ground truth values. A directional local extrema pattern extracts the directional edge information based on local extrema in 0°, 45°, 90°, and 135° directions in an image. A palmprint contains several lines in various directions. A directional local extrema pattern may prove beneficial to extract edge information from palmprints. The two kinds of features are fused at score level for authentication purpose. The PolyU pamprint dataset has been used for evaluation, and the results are found promising. To establish the improvements in palmprint recognition, the results of proposed system has been compared with several well established state of the art algorithms.	algorithm;authentication;fingerprint;ground truth;linear discriminant analysis;locality of reference;maxima and minima;receiver operating characteristic;rejection sampling	Asha Rani;Manisha Verma;Balasubramanian Raman	2015	2015 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2015.7457848	computer vision;feature extraction;computer science;machine learning;pattern recognition;data mining;authentication;principal component analysis	Vision	33.319949698258924	-61.42926409100726	133867
40c968801262e261996eeee5e6cd19655829e13b	segmentation and recognition of vocalized outlines in pitman shorthand	recognition accuracy;handwritten shorthand scheme;data recording rate;promising result;vocalized outline segmentation;mobile computing devices;two stage method;consonant stroke;image segmentation;diphthong symbol;recognition process;vowels classification;handwritten shorthand schemes;image classification;pitman shorthand;diphthong symbols;vocalized outlines;consonant outline;mobile computing device;handwritten character recognition;novel approach;vocalized outline recognition;mobile computer	There is a wish to be able to enter text into mobile computing devices at the speed of speech. Only handwritten shorthand schemes can achieve this data recording rate. A novel approach to the recognition of vocalized outlines in Pitman's shorthand is proposed in this paper. In this approach, the recognition process is divided into two steps. The consonant outline is first recognized by a two-stage (segmentation and classification) approach. Afterwards surrounding vowel and diphthong symbols are classified and their positions in relation to an associated consonant stroke are determined. Promising results of over 70% recognition accuracy have been obtained.	mobile computing;outline (list)	Ma Yang;Graham Leedham;Colin Higgins;Swe Myo Htwe	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334153	computer vision;contextual image classification;speech recognition;computer science;pattern recognition;image segmentation;mobile computing	Vision	32.77948007542004	-63.78838267003694	133901
d7f421eb553bf73c237a579b3b5acca187bff85c	iris recognition using adaboost and levenshtein distances	biometrics;iris recognition;processor array;adaboost;levenshtein distance;string matching	This paper presents an efficient IrisCode classifier, built from phase features which uses AdaBoost for the selection of Gabor wavelets bandwidths. The final iris classifier consists of a weighted contribution of weak classifiers. As weak classifiers we use three-split decision trees that identify a candidate based on the Levenshtein distance between phase vectors of the respective iris images. Our experiments show that the Levenshtein distance has better discrimination in comparing IrisCodes than the Hamming distance. Our process also differs from existing methods because the wavelengths of the Gabor filters used, and their final weights in the decision function, are chosen from the robust final classifier, instead of being fixed and/or limited by the programmer, thus yielding higher iris recognition rates. A pyramidal strategy for cascading filters with increasing complexity makes the system suitable for real-time operation. We have designed a processor array to accelerate the computation of the Levenshtein distance. The processing elements are simple basic cells, interconnected by relatively short paths, which makes it suitable for a VLSI implementation.	adaboost;iris recognition;levenshtein distance	Joan Climent;Roberto A. Hexsel	2012	IJPRAI	10.1142/S0218001412660012	adaboost;speech recognition;computer science;machine learning;pattern recognition;iris recognition;biometrics;string searching algorithm	Vision	26.825012135423634	-61.85372032826823	134070
5e07dac585f49d9c58a0faa3625f1ec1ccad579d	finger veins feature extraction algorithm based on image processing methods		Recently more interest in the recognition algorithms based on human veins is observable. In the literature we can find results confirm that this trait provide huge accuracy level. This feature is used for instance in cash machines. In the last years, more financial institutions took into consideration vein-based identification technology. Its popularity is connected with ease of use and analyzed trait uniqueness. A method to extract finger veins features with image processing algorithms is presented in this paper. In the preliminary stage of the research, the device to collect finger veins images was created. The second part of the work is implementation of the algorithm to process input images. The authors used soft computing algorithm that is artificial neural network to find specific structures on the image. The last stage of the work is connected with confirmation of the results obtained with artificial neural network.	algorithm;feature extraction;finger tree;image processing	Maciej Szymkowski;Khalid Saeed	2018		10.1007/978-3-319-99954-8_8	image processing;uniqueness;digital image processing;trait;computer science;feature extraction;artificial neural network;artificial intelligence;pattern recognition;algorithm;soft computing	EDA	29.907027678424853	-65.39829657781837	134127
62c443f900330e9022b6e96413c6b9ba8401d563	perceptual video hashing based on tucker decomposition with application to indexing and retrieval of near-identical videos	tucker decomposition;achlioptas s random matrix;parallel factor analysis;perceptual video hashing;video indexing and retrieval;near identical videos	The perceptual video hash function defines a feature vector that characterizes a video depending on its perceptual contents. This function must be robust to the content preserving manipulations and sensitive to the content changing manipulations. In the literature, the subspace projection techniques such as the reduced rank PARAllel FACtor analysis (PARAFAC), have been successfully applied to extract perceptual hash for the videos. We propose a robust perceptual video hash function based on Tucker decomposition, a multi-linear subspace projection method. We also propose a method to find the optimum number of components in the factor matrices of the Tucker decomposition. The Receiver Operating Characteristics (ROC) curves are used to evaluate the performance of the proposed algorithm compared to the other state-of-the-art projection techniques. The proposed algorithm shows superior performance for most of the image processing attacks. An application for indexing and retrieval of near-identical videos is developed using the proposed algorithm and the performance is evaluated using average recall/precision curves. The experimental results show that the proposed algorithm is suitable for indexing and retrieval of near-identical videos.	algorithm;feature vector;hash function;image processing;receiver operating characteristic;robustness (computer science);tucker decomposition	R. Sandeep;Saksham Sharma;Mayank Thakur;P. K. Bora	2015	Multimedia Tools and Applications	10.1007/s11042-015-2695-1	computer science;theoretical computer science;machine learning;pattern recognition;tucker decomposition;statistics	Vision	34.49333431705741	-59.49605859667137	134260
159d16cdc48135632c2d5790e5baaf8d0631f510	structcap: structured semantic embedding for image captioning		Image captioning has attracted ever-increasing research attention in multimedia and computer vision. To encode the visual content, existing approaches typically utilize the off-the-shelf deep Convolutional Neural Network (CNN) model to extract visual features, which are sent to Recurrent Neural Network (RNN) based textual generators to output word sequence. Some methods encode visual objects and scene information with attention mechanism more recently. Despite the promising progress, one distinct disadvantage lies in distinguishing and modeling key semantic entities and their relations, which are in turn widely regarded as the important cues for us to describe image content. In this paper, we propose a novel image captioning model, termed StructCap. It parses a given image into key entities and their relations organized in a visual parsing tree, which is transformed and embedded under an encoder-decoder framework via visual attention. We give an end-to-end formulation to facilitate joint training of visual tree parser, structured semantic attention and RNN-based captioning modules. Experimental results on two public benchmarks, Microsoft COCO and Flickr30K, show that the proposed StructCap model outperforms the state-of-the-art approaches under various standard evaluation metrics.	computer vision;convolutional neural network;encode;embedded system;encoder;end-to-end principle;entity;holism;long short-term memory;parse tree;parsing;random neural network;recurrent neural network;test set;tree structure;vantage-point tree;visual objects	Fuhai Chen;Rongrong Ji;Jinsong Su;Yongjian Wu;Yunsheng Wu	2017		10.1145/3123266.3123275	computer vision;automatic image annotation;multimedia;convolutional neural network;image retrieval;deep learning;parsing;computer science;visual objects;machine learning;closed captioning;structured prediction;artificial intelligence	Vision	26.173836732589823	-53.40277329812331	134326
3ccfcd22a4f742af4a13f4c89c7215d515b36681	dim infrared image enhancement based on convolutional neural network		Long-range infrared images are always suffering from dim targets and background clutters. To improve the contrast between target and background, we propose a novel infrared image enhancement approach by highlighting target and suppressing background clutters. Predicting the target and background plays a key role in improving the contrast of dim infrared images that targets are embedded by background clutters. Taking full advantage of machine learning on prediction, we design the convolutional neural network (CNN) architecture in our study. To overcome the lack of large training data, the handwritten images in MNIST dataset are employed to simulate the properties of long-rang infrared images including dim targets, background clutters and low contrast. The target and background sub-images are predicted from the original dim infrared image based on the filters in the first layer of the trained CNN. Finally, the dim infrared image is enhanced by amplifying the targets and subtracting background clutters. The results of subjective and quantitative tests prove the performance of the proposed algorithm in contrast improvement.		Zun-Lin Fan;Duyan Bi;Lei Xiong;Shiping Ma;Lin-Yuan He;Wenshan Ding	2018	Neurocomputing	10.1016/j.neucom.2017.07.017	architecture;infrared;artificial intelligence;spatial filter;convolutional neural network;artificial neural network;pattern recognition;computer vision;training set;mnist database;computer science	AI	27.48475473743928	-55.07383617843805	134555
33a48bc196d92068ed545d695fb1f3cfbcf85803	using tagged images of low visual ambiguity to boost the learning efficiency of object detectors	multimedia data augmentation;user tagged images;bootstrapping;social bootstrapping;semantic segmentation;visual ambiguity	Motivated by the abundant availability of user-generated multimedia content, a data augmentation approach that enhances an initial manually labelled training set with regions from user tagged images is presented. Initially, object detection classifiers are trained using a small number of manually labelled regions as the training set. Then, a set of positive regions is automatically selected from a large number of loosely tagged images, pre-segmented by an automatic segmentation algorithm, to enhance the initial training set. In order to overcome the noisy nature of user tagged images and the lack of information about the pixel level annotations, the main contribution of this work is the introduction of the visual ambiguity term. Visual ambiguity is caused by the visual similarity of semantically dissimilar concepts with respect to the employed visual representation and analysis system (i.e. segmentation, feature space, classifier) and, in this work, is modelled so that the images where ambiguous concepts co-exist are penalized. Preliminary experimental results show that the employment of visual ambiguity guides the selection process away from the ambiguous images and, as a result, allows for better separation between the targeted true positive and the undesired negative regions.	algorithm;convolutional neural network;feature vector;object detection;pixel;sensor;statistical classification;test set;user-generated content	Elisavet Chatzilari	2013		10.1145/2502081.2502208	computer vision;computer science;operating system;machine learning;pattern recognition;bootstrapping	Vision	30.853785462413335	-52.9856217681362	134626
975f83eab7a05f78df2ebfe9f42ecb3e84676c36	classification and verification of online handwritten signatures with time causal information theory quantifiers		We present a new approach for online handwritten signature classification and verification based on descriptors stemming from Information Theory. The proposal uses the Shannon Entropy, the Statistical Complexity, and the Fisher Information evaluated over the Bandt and Pompe symbolization of the horizontal and vertical coordinates of signatures. These six features are easy and fast to compute, and they are the input to an One-Class Support Vector Machine classifier. The results produced surpass state-of-the-art techniques that employ higher-dimensional feature spaces which often require specialized software and hardware. We assess the consistency of our proposal with respect to the size of the training sample, and we also use it to classify the signatures into meaningful groups. Introduction The word biometrics is associated to human traits or behaviors which can be measured and used for individual recognition. In fact, the biometry recognition, as a personal authentication signal processing, can be used in applications where users need to be security identified.1 Clearly, these kind of systems can either verify or identify. Two types of biometrics can be defined according to the personal traits considered: physical/physiological or behavioral. Physical/physiological biometrics is about catering the biological traits of users, like fingerprints, iris, face, hand, etc. Behavioral biometrics takes into account dynamic traits of users, such as, voice, handwritten and signature expressions. One of the main advantages of biometric systems is that users do not have to remember passwords or carry access keys. Another important advantage lies in the difficulty to steal, imitate or generate genuine biometric data, leading to enhanced security.1 As mentioned, behavioral biometrics is based on measurements extracted from an activity performed by the user, in conscious or unconscious way, that are inherent to his/her own personality or learned behavior. In this aspect, behavioral biometrics has interesting pros, like user acceptance and cancelability, but it still lacks of some level of the uniqueness physiological biometrics has. Among the pure behavioral biometric traits, the handwritten signature and the way we sign is the one with widest social and legal acceptance.2–6 Identity verification by signature analysis requires no invasive measurements and people are familiar with the use of signatures in their daily life. Also, it is the modality confronted with the highest level of attacks. A signature is a handwritten depiction of someone’s name or some other mark of identification written on documents and devices as proof of identification. The formation of signature varies from person to person, or even from the same person due to the psychophysical state of the signer and the conditions under which the signature apposition process occurs. Hilton7 studied how signatures are produced, and found that the signature has at least three attributes: form, movement and variation; being movement the most important, because signatures are produced by moving a writing device. The study also noted that a person’s signature does evolve over time and, with the vast majority of users, once the signature style has been established the modifications are usually slight. The movement is produced by muscles of fingers, hand, wrist, and in some writers the arm; these muscles are controlled by nerve impulses. When one person is signing these nerve impulses are controlled by the brain without any particular attention to detail. The signing processes can be described then, at high level, as how the central nervous system (the brain) recovers information from long term memory in which parameters such as size, shape, timing etc. are specified. At the peripheral level, commands are generated for muscles. In consequence, the signing process is believed to be a reflex action (ballistic action8) rather than a deliberate action. Then, the production of genuine signatures is associated to a ballistic handwriting, which is characterized by a spurt of activity, without positional feedback, whereas the production of forgery signature is associated to a deliberate handwriting which is characterized by a conscious attempt to produce a visual pattern with the aid of positional feedback.9, 10 Handwritten signature verification is a problem in which the input signature (a test signature) is classified as genuine or forged. This process is usually performed in three main phases:2–6 • Data acquisition and pre-processing. Two different categories of systems can be identified, depending on whether there is electronic access to the handwritten process or not. a) Online or dynamic recognition, in which the pen’s instantaneous information trajectories, and also information like pressure, speed or pen-up movements can be captured. b) Offline or static recognition: those that record signatures as images on paper which can be later digitized by means of a scanner, and processed. In the latter, the pre-processing phase involves filtering, noise reduction and smoothing. Online signature verification offers reliable identity protection, as it employs dynamic information not available on the signature image itself but in the process of signing. As a consequence, online signature verification systems usually achieve better accuracy than offline systems. • Feature extraction. Two types of features can be used. a) Function features of the signature: time functions whose values constitute the feature set. b) Parameter features: the signature is characterized as a vector of elements, each one representative of the value of the feature. Usually, the last one yields better performance, but it is also time-consuming. • Classification. In the verification process, the authenticity of the test signature is evaluated by matching it against those stored in the knowledge base developed during the enrollment stage. This process produces a single response that attests to the authenticity of the test signature. When template matching techniques are considered, a questioned sample is matched against templates of authentic/forgery signatures. Distance-based classifiers, mostly when parameters are used as features, are usually developed with statistical techniques, e.g. with Mahalanobis and Euclidean distances. The performance of a signature verification system is commonly assessed in terms of the percentage Equal Error Rate. On the one hand, template matching attempts at finding similarities between the input signature and those in a data base. Most approaches use Dynamic Time Warping to perform this match.5, 6 On the other hand, distance-based classifiers rely on the use of features derived from the signatures. Two opposite mechanisms describing the signing process can be found in the literature. The nonlinear character and chaotic behavior of several physiological complex processes are well established.11, 12 In particular, Longstaff and Heath13 found evidence of chaotic behavior on the underlying dynamics of time series related to velocity profiles of handwritten texts. Taking into account the inherent behavioral nature of the online signing process, the input information could be associated to deterministic (nonlinear low dimensional chaotic) signals, and the handwritten signature variations as a consequence of chaos (sensibility to initial conditions). In opposition, most of the research in the field of signal verification considers the input information as well described by a random process.2–6 Then, the dynamic input information acquired through a time sampling procedure must be consequently considered as discrete time random sequence. In any case, the signature analysis taken as a time-based sequence characterization process is strongly related to the way in which a reference model is established. From the stochastic point of view, Hidden Markov Models are among the most commonly used in the literature, and the ones with the best performance in signature verification.2–6 Our proposal relies on the use of time causal quantifiers based on Information Theory for the characterization of online handwritten signatures: normalized permutation Shannon entropy, permutation statistical complexity and permutation Fisher information measure. These quantifiers have proved to be useful in the identification of chaotic and stochastic dynamics throughout the associated time series.14, 15 Their evaluation is simple and fast, making them apt for the signature verification problem. We apply our proposal to the well know MCYT online signature data base.16 Next section describes the database used in this study, followed by a section where we detail the quantifiers employed and by their application to the data. In addition to the usual data flow, we present an exploratory data analysis (EDA) of the features that enhances their appropriateness for this problem. The expressiveness and usefulness of these descriptors for the problem of online signature classification and verification follows in the sequence: we experiment their application to the test-bed.	access key;action potential;antivirus software;authentication;biometrics;biostatistics;causal filter;causality;chaos theory;data acquisition;database;dataflow;digital signal processing;digital signature;dynamic time warping;dynamical system;electronic signature;emoticon;entropy (information theory);euclidean distance;feature extraction;filter (signal processing);fingerprint;fisher information;hidden markov model;high-level programming language;image scaling;information theory;initial condition;knowledge base;markov chain;modality (human–computer interaction);noise reduction;nonlinear system;norm (social);online and offline;password;peripheral;portable document format;preprocessor;reference model;sampling (signal processing);shannon (unit);signal processing;smoothing;spatial variability;stemming;stochastic process;support vector machine;template matching;testbed;time series;type signature;vector graphics;velocity (software development);whole earth 'lectronic link	Osvaldo A. Rosso;Raydonal Ospina;Alejandro C. Frery	2016	CoRR			Security	26.021550053222256	-64.20425783729718	134684
451839b481d816a06b5d2159f215b0245462c0b0	emotion recognition based on 2d-3d facial feature extraction from color image sequences	human computer interaction;emotion recognition;computer vision;feature extraction	In modern human computer interaction systems, emotion recognition from video is becoming an imperative feature. In this work we propose a new method for automatic recognition of facial expressions related to categories of basic emotions from image data. Our method incorporates a series of image processing, low level 3D computer vision and pattern recognition techniques. For image feature extraction, color and gradient information is used. Further, in terms of 3D processing, camera models are applied along with an initial registration step, in which person specific face models are automatically built from stereo. Based on these face models, geometric feature measures are computed and normalized using photogrammetric techniques. For recognition this normalization leads to minimal mixing between different emotion classes, which are determined with an artificial neural network classifier. Our framework achieves robust and superior classification results, also across a variety of head poses with resulting perspective foreshortening and changing face size. Results are presented for domestic and publicly available databases.	artificial neural network;color image;computer stereo vision;computer vision;database;emotion recognition;euclidean distance;feature (computer vision);feature extraction;gradient descent;human computer;human–computer interaction;image processing;imperative programming;microsoft outlook for mac;pattern recognition;photogrammetry;texture mapping	Robert Niese;Ayoub Al-Hamadi;Axel Panning;Bernd Michaelis	2010	Journal of Multimedia	10.4304/jmm.5.5.488-500	computer vision;speech recognition;feature;feature extraction;computer science;machine learning;pattern recognition;three-dimensional face recognition;feature	Vision	31.856736929364896	-58.48728917736888	134759
4f2901836a3f14663a75a18249e6a2ca725e0614	mapping learning in eigenspace for harmonious caricature generation	support vector regression;subspace;machine learning;caricature;facial features;pca;principal component	This paper proposes a mapping learning approach for caricature auto-generation. Simulating the artist's creativity based on the object's facial feature, our approach targets discovering what are the principal components of the facial features, and what's the difference between facial photograph and caricature measured by those components. In training phase, PCA approach is adopted to obtain the principal components. Then, machine learning of SVR (Support Vector Regression) is carried out to learn the mapping model in principal component space. With the mapping model, in application phase, users just need to input a frontal facial photograph for the caricature generation. The caricature is exaggerated based on the original face while reserving essential similar features. Experiments proved comparatively that our approach could generate more harmonious caricatures.	experiment;fear, uncertainty and doubt;machine learning;principal component analysis;support vector machine	Junfa Liu;Yiqiang Chen;Wen Gao	2006		10.1145/1180639.1180783	computer vision;speech recognition;computer science;artificial intelligence;machine learning;principal component analysis	AI	25.8831025017871	-59.91691898200253	134793
16bb8bad73b902552cffd2d9a3b281caf04aa368	amat: medial axis transform for natural images		We introduce Appearance-MAT (AMAT), a generalization of the medial axis transform for natural images, that is framed as a weighted geometric set cover problem. We make the following contributions: i) we extend previous medial point detection methods for color images, by associating each medial point with a local scale; ii) inspired by the invertibility property of the binary MAT, we also associate each medial point with a local encoding that allows us to invert the AMAT, reconstructing the input image; iii) we describe a clustering scheme that takes advantage of the additional scale and appearance information to group individual points into medial branches, providing a shape decomposition of the underlying image regions. In our experiments, we show state-of-the-art performance in medial point detection on Berkeley Medial AXes (BMAX500), a new dataset of medial axes based on the BSDS500 database, and good generalization on the SK506 and WH-SYMMAX datasets. We also measure the quality of reconstructed images from BMAX500, obtained by inverting their computed AMAT. Our approach delivers significantly better reconstruction quality w.r.t. to three baselines, using just 10% of the image pixels. Our code and annotations are available at https://github.com/tsogkas/amat.	apache axis;cluster analysis;experiment;medial graph;pixel;set cover problem	Stavros Tsogkas;Sven J. Dickinson	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.295	pixel;iterative reconstruction;artificial intelligence;computer vision;encoding (memory);pattern recognition;medial axis;cluster analysis;computer science;binary number;image segmentation;set cover problem	Vision	32.47559392593521	-53.095941093078736	134796
3304af0717e2c33664facf614dad7e7e6bdf7c9c	text line segmentation in handwritten documents based on connected components trajectory generation		Text line segmentation in handwritten documents is an important step in many high level processing such as handwritten document enhancement and text recognition. In this paper we describe a novel approach of text line segmentation based on tracking. In this sense, we consider each connected component in the image as a moving object in its respective line and find its best match given its history motion, i.e. the closest connected component that lie in its trajectory. Direction of motion gives direction of handwritten text and is the output of our tracking algorithm. We apply our approach to images of ICDAR 2013 handwritten segmentation contest and report an overall detection rate of (86.51%).	connected component (graph theory)	Insaf Setitra;Abdelkrim Meziane;Zineb Hadjadj;Nawfel Bengherbia	2017		10.1007/978-3-319-93647-5_13	connected-component labeling;pattern recognition;artificial intelligence;computer science;trajectory;segmentation;connected component	Robotics	36.643229929128964	-65.72944760114406	134859
02f6b6ceb0bcd3cc10bdc7a3c03d24abb3437771	retinex preprocessing of uncalibrated images for color-based image retrieval	databases;color constancy;ceramics;image database;retinex;uncalibrated images;color image;image retrieval	orsed 200 Abstract. In image databases, variations in imaging conditions and preprocessing may result in similar originals that exhibit a low measure of similarity when color information is used in standard image retrieval methods. We examine the performance of various colorbased retrieval strategies to see whether, and to what degree, the effectiveness of retrieval improves with Retinex-based preprocessing, regardless of the strategy adopted. The results of experiments performed on four different databases are reported and discussed. © 2003 SPIE and IS&T. [DOI: 10.1117/1.1526844]	database;experiment;image retrieval;preprocessor	Gianluigi Ciocca;Daniele Marini;Alessandro Rizzi;Raimondo Schettini;Silvia Zuffi	2003	J. Electronic Imaging	10.1117/1.1526844	color histogram;computer vision;visual word;color image;image retrieval;computer science;color balance;automatic image annotation;color constancy;information retrieval;computer graphics (images)	Vision	39.14950780472465	-60.2162943458442	135583
c29c0229d219b5eb518f08a55a88c6269e5da386	rejecting motion outliers for efficient crowd anomaly detection		Crowd anomaly detection is a key research area in vision-based surveillance. Most of the crowd anomaly detection algorithms are either too slow, bulky, or power-hungry to be applicable for battery-powered surveillance cameras. In this paper, we present a new crowd anomaly detection algorithm. The proposed algorithm creates a feature for every superpixel that includes the contribution from the neighboring superpixels only if their direction of motion conforms with the dominant direction of motion in the region. We also propose using univariate Gaussian discriminant analysis with the K-means algorithm for classification. Our method provides superior accuracy over numerous deep learning-based and handcrafted feature-based approaches. We also present a low-power FPGA implementation of the proposed method. The algorithm is developed such that features are extracted over non-overlapping pixels. This allows gating inputs to numerous modules resulting in higher power efficiency. The maximum energy required per pixel is 2.43 nJ in our implementation. 126.65 Mpixels can be processed per second by the proposed implementation. The speed, power, and accuracy performance of our method make it competitive for surveillance applications, especially battery-powered surveillance cameras.	algorithm;anomaly detection;closed-circuit television;deep learning;field-programmable gate array;k-means clustering;linear discriminant analysis;low-power broadcasting;performance per watt;pixel;rejection sampling	Muhammad Umar Karim Khan;Hyun-Sang Park;Chong-Min Kyung	2019	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2018.2856189	artificial intelligence;anomaly detection;computer vision;deep learning;computer science;pattern recognition;feature extraction;pixel;outlier;hidden markov model;linear discriminant analysis;univariate	Vision	27.432100195691	-57.85113106434462	135601
2d87c82bcfe8bac31a708e2806eab54b8adb8c18	adaptive intrusion recognition for ultraweak fbg signals of perimeter monitoring based on convolutional neural networks		Intrusion recognition based on the fiber-optic sensing perimeter security system is a significant method in security technology. Nevertheless, it is of great challenge to distinguish among multitudinous intrusion signals. Many studies have been conducted to solve this problem, which are absolutely dependent on the handcrafted features, and the process of feature extraction is time-consuming and unreliable. In this paper, we present an adaptive intrusion recognition method for ultra-weak FBG signals of perimeter monitoring based on convolutional neural networks. The advantage of the proposed method is its ability to extract optimal vibration features automatically from the raw sensing vibration signals. A fiber-optic sensing perimeter security system was developed to evaluate the computational efficiency of the proposed recognition method. The experiment results demonstrated that the proposed method could recognize the intrusion in the perimeter security system effectively with the best recognition accuracy among all of the comparative methods.		Fang Liu;Sihan Li;Zhenhao Yu;Xiaoxiong Ju;Honghai Wang;Quan Qi	2018		10.1007/978-3-030-04221-9_32	convolutional neural network;perimeter;pattern recognition;feature extraction;artificial intelligence;perimeter security;feature engineering;computer science;intrusion	AI	30.295071968175627	-63.342489040944294	135673
be64c1d3df7770e1165b7f533434dcdb4763e4a0	probabilistic diffusion classifiers for object detection	graph theory;histograms;kernel;feature detection;probability;unconstrained image;probabilistic diffusion classifier;bayes methods;training;image classification;prototype based classification;object localization;stochastic processes;probability bayes methods feature extraction graph theory image classification image colour analysis markov processes object detection;image color analysis;image colour analysis;feature extraction;markov processes;probabilistic logic;stochastic diffusion approach;bipartite graph;color object localization;object detection stochastic processes bipartite graph kernel prototypes bayesian methods robustness probability distribution laboratories histograms;object detection;unconstrained image probabilistic diffusion classifier object detection stochastic diffusion approach prototype based classification feature detection bipartite graph bayesian interpretation markov chain color object localization;bayesian interpretation;markov chain	This paper presents a stochastic diffusion approach to prototype-based classification. Relations between exemplary objects and their features are modeled in a bipartite graph. A Bayesian interpretation of the model leads to a Markov chain over the set of objects. In contrast to related graph diffusion approaches, our dual treatment of objects and features easily copes with out of sample objects. Applied to problems in color object localization in unconstrained images, our method performs robust and yields promising results.	british undergraduate degree classification;latent variable;map;markov chain;markov property;microsoft outlook for mac;molecular descriptor;object detection;prototype	Christian Bauckhage	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761249	computer vision;markov chain;contextual image classification;kernel;bipartite graph;feature extraction;graph theory;machine learning;pattern recognition;probability;feature detection;histogram;mathematics;probabilistic logic;markov process;statistics	Vision	35.944494521737205	-53.251544841394036	135847
197d123d4f025a3876e08a8cbed95f5807f88675	text detection in natural scenes with salient region	icdar 2003 database text detection natural scenes salient region bionic method text localization multiscale contrast center surround histogram color spatial distribution conditional random fields model text string connected component analysis;image color analysis histograms image edge detection humans pattern recognition computational modeling conferences;text analysis;conditional random fields;visual databases document image processing image colour analysis text analysis;conditional random fields text detection salient regions;spatial distribution;image colour analysis;conditional random field;document image processing;salient regions;text detection;spatial relationships;connected component;natural scenes;visual databases	In this paper, we present a novel approach to detect text in natural scenes. This approach is a type of bionic method, which imitates how human beings detect text exactly and robustly. Practically, human beings follow two steps to detect text: the first step is to find salient regions in a scene and the second step is to determine whether these salient regions are text or not. Therefore, two similar steps namely salient regions computation and text localization are used in our method. In the step of salient regions computation, a set of salient features including multi-sacle contrast, modified center-surround histogram, color spatial distribution and similarity of stroke width are used to describe an image, following with computation of salient regions based on the combination of Conditional Random Fields model and above features. Because sole letter rarely appear, in the step of text localization, salient regions are segmented and the connected components are grouped into text strings based on their features such as spatial relationships, color difference and stroke width. As an elementary unit, the text string is refined by connected component analysis. We tested the effectiveness of our method on the ICDAR 2003 database. The experimental results show that the proposed method provides promising performance in comparison with existing methods.	algorithm;cc system;computation;conditional random field;connected component (graph theory);connected-component labeling;international conference on document analysis and recognition;string (computer science)	Quan Meng;Yonghong Song	2012	2012 10th IAPR International Workshop on Document Analysis Systems	10.1109/DAS.2012.85	computer vision;text mining;computer science;machine learning;pattern recognition;conditional random field	AI	36.88852251428623	-64.32247543328262	135970
c85277a3ffc477ec4df3641349ec4f9e1b61e215	a preprocessing method for naxi pictograph character recognition	binary processing;character segmentation;document layout analysis;feature extraction;naxi pictograph recognition;preprocessing;skew correction	Preprocessing, a major component of Character Recognition System, has direct effect on the recognition system by its performance. A preprocessing method for NaXi Pictograph Character Recognition has been developed based on the characteristics of NaXi Pictograph in glyph and way of writing. The whole procedure of preprocessing constitutes of binary processing, skew correction, document layout analysis, Character Segmentation and Normalization processing, Edge Detection, as well as the extraction of directional line element feature, Wide-gridding feature, permeability number feature, Moment Invariant Feature, to name a few. These features can be used in Coarse Classification, fine classification, and Post-processing of the system, since their good stability and fully reflection of the characteristics of NaXi Pictograph.	document layout analysis;edge detection;glyph;image moment;pictogram;preprocessor;video post-processing	Hai Guo;Jing-ying Zhao;Ming-jun Da	2010	JCIT		computer science	Vision	34.43450884994288	-65.51614608736601	136136
84adce5bbd89124e8bcd38ddfeda16503c415cf5	unsupervised segmentation evaluation for image annotation		Unsupervised segmentation evaluation measures are usually validated against human-generated ground-truth. Nevertheless, with the recent growth of image classification methods that use hierarchical segmentation-based representations, it would be desirable to assess the performance of unsupervised segmentation evaluation to select the most suitable levels to perform recognition tasks. Another problem is that unsupervised segmentation evaluation measures use only low-level features, which makes difficult to evaluate how well an object is outlined. In this paper we propose to use four semantic measures, that combined with other state-of-the-art measures improve the evaluation results and also, we validate the results of each unsupervised measure against an image annotation algorithm ground truth, showing that using measures that try to emulate human behaviour is not necessarily what an automatic recognition algorithm may need. We employed the Stanford Background Dataset to validate an image annotation algorithm that includes segmentation evaluation as starting point, and the proposed combination of unsupervised measures showed the best annotation accuracy results.	algorithm;automatic image annotation;computer vision;ground truth;high- and low-level;image segmentation;map;unsupervised learning	Annette Morales-González;Edel B. García Reyes;Luis Enrique Sucar	2015		10.5220/0005314201480155	segmentation-based object categorization;scale-space segmentation;automatic image annotation	Vision	31.379343467296472	-53.184151245514805	136194
2d54cf6523aec83ff6d877eec6d88d09ca00b8b6	a hierarchical approach for region-based image retrieval	image segmentation;coarse segmentation region based image retrieval hierarchical approach wavelet transform human visual processing automated image segmentation color texture feature;low resolution;texture features;region based image retrieval;image texture;image segmentation image retrieval wavelet transforms image colour analysis image texture;wavelet transforms;feature vector;wavelet transform;image colour analysis;image retrieval image segmentation information retrieval wavelet transforms humans frequency pixel computer science image resolution image databases;visual processing;image retrieval	We propose a hierarchical approach for region-based image retrieval, which is based on wavelet transform for its decomposition property similarity with human visual processing. First, automated image segmentation is performed fast in the low-low (LL) frequency subband of wavelet transform which shows desirable low resolution of image. In the proposed system, boundaries between segmented regions are deleted to improve the robustness of region-based image retrieval against uncertainty of segmentation. Second, region feature vector is hierarchically represented by information in all wavelet subbands and each feature component of a feature vector is a combined color-texture feature. Such feature vector captures the distinctive feature (e.g., semantic texture) inside one region finely. Through experiment results and comparison with other methods, the proposed method shows good tradeoff between retrieval effectiveness and efficiency as well as easy implementation for region-based image retrieval.	feature vector;image resolution;image retrieval;image segmentation;ll parser;wavelet transform	Yongqing Sun;Shinji Ozawa	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1398455	image texture;computer vision;feature detection;range segmentation;visual word;color image;binary image;image processing;image retrieval;computer science;segmentation-based object categorization;digital image processing;pattern recognition;region growing;image segmentation;image fusion;scale-space segmentation;automatic image annotation;top-hat transform;feature;information retrieval;wavelet transform	Vision	39.04365872513485	-61.90742135521705	136262
940ab1962bef94c8531f2460f37b0ec963d93f3a	spatial coding for large scale partial-duplicate web image search	high dimensionality;partial duplicate;web image search;orientation quantization;large scale;local features;spatial coding;spatial relationships;mean average precision;bag of words;image retrieval	The state-of-the-art image retrieval approaches represent images with a high dimensional vector of visual words by quantizing local features, such as SIFT, in the descriptor space. The geometric clues among visual words in an image is usually ignored or exploited for full geometric verification, which is computationally expensive. In this paper, we focus on partial-duplicate web image retrieval, and propose a novel scheme, spatial coding, to encode the spatial relationships among local features in an image. Our spatial coding is both efficient and effective to discover false matches of local features between images, and can greatly improve retrieval performance. Experiments in partial-duplicate web image search, using a database of one million images, reveal that our approach achieves a 53% improvement in mean average precision and 46% reduction in time cost over the baseline bag-of-words approach.	analysis of algorithms;bag-of-words model;baseline (configuration management);computation;encode;experiment;image retrieval;information retrieval;patch (computing);query expansion;scale-invariant feature transform;spatial anti-aliasing;speeded up robust features;time complexity	Wengang Zhou;Yijuan Lu;Houqiang Li;Yibing Song;Qi Tian	2010		10.1145/1873951.1874019	spatial relation;image texture;computer vision;visual word;image retrieval;computer science;bag-of-words model;pattern recognition;automatic image annotation;information retrieval	Vision	36.58957766817658	-55.22377968697232	136391
6370a7f8752ddff07c30f327a71d3fb5f856daf6	damage assessment from social media imagery data during disasters		Rapid access to situation-sensitive data through social media networks creates new opportunities to address a number of real-world problems. Damage assessment during disasters is a core situational awareness task for many humanitarian organizations that traditionally takes weeks and months. In this work, we analyze images posted on social media platforms during natural disasters to determine the level of damage caused by the disasters. We employ state-of-the-art machine learning techniques to perform an extensive experimentation of damage assessment using images from four major natural disasters. We show that the domain-specific fine-tuning of deep Convolutional Neural Networks (CNN) outperforms other state-of-the-art techniques such as Bag-of-Visual-Words (BoVW). High classification accuracy under both event-specific and cross-event test settings demonstrate that the proposed approach can effectively adapt deep-CNN features to identify the severity of destruction from social media images taken after a disaster strikes.	bag-of-words model in computer vision;convolutional neural network;experiment;machine learning;requirement;social media	Dat T. Nguyen;Ferda Ofli;Muhammad Imran;Prasenjit Mitra	2017		10.1145/3110025.3110109	semi-supervised learning;machine learning;bayesian optimization;convolutional neural network;data mining;markov random field;artificial intelligence;situation awareness;social media;computer science	AI	26.377109103821926	-54.27914272676414	136847
ac7280aa944028167d5b99cb0594777e7360cb7b	key-region detection for document images -- application to administrative document retrieval	administrative document image retrieval region detection mser;administrative document image retrieval;special characteristics key region detection document images administrative document retrieval structural information;information retrieval;detectors transforms text analysis feature extraction indexing computer vision layout;information retrieval document image processing;document image processing;mser;region detection	In this paper we argue that a key-region detector designed to take into account the special characteristics of document images can result in the detection of less and more meaningful key-regions. We propose a fast key-region detector able to capture aspects of the structural information of the document, and demonstrate its efficiency by comparing against standard detectors in an administrative document retrieval scenario. We show that using the proposed detector results to a smaller number of detected key-regions and higher performance without any drop in speed compared to standard state of the art detectors.	dendrogram;document retrieval;sensor	Hongxing Gao;Marçal Rusiñol;Dimosthenis Karatzas;Josep Lladós;Tomokazu Sato;Masakazu Iwamura;Koichi Kise	2013	2013 12th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2013.53	document retrieval;computer vision;visual word;document clustering;computer science;document layout analysis;data mining;maximally stable extremal regions;vector space model;information retrieval	Vision	36.24345347836429	-65.03239743400025	137197
4f600c9569f2fa24207ce6b8b0a7b617c3ebdef5	definition of fingerprint scanner image quality specifications by operational quality	fingerprint recognition;image quality	This paper analyzes two recently released image quality specifications for single-finger scanners and proposes three new specifications targeted to different types of applications. A comparison of the potential effects on fingerprint recognition accuracy of the various specifications is carried out using an approach based on the definition of “operational quality”. The experimental results show that the three new image quality specifications proposed in this work have an accuracy/cost tradeoff better than the existing ones.	fingerprint recognition;image quality	A. Alessandroni;Raffaele Cappelli;Matteo Ferrara;Davide Maltoni	2008		10.1007/978-3-540-89991-4_4	computer vision;engineering;data mining;computer security	SE	28.111821143679382	-63.00831582193893	137207
4e150ca0044ff435040a3da685c03b9f943aeb02	person re-identification from cctv silhouettes using generic fourier descriptor		Person re-identification in public areas (such as airports, train stations and shopping malls) has recently received increased attention within computer vision research due, in part, to the demand for enhanced levels of security. Re-identifying subjects within non-overlapped camera networks can be considered as a challenging task. Illumination changes in different scenes, variations in camera resolutions, field of view and human natural motion are the key obstacles to accurate implementation. This study assesses the use of Generic Fourier Shape Descriptors (GFD) on person silhouettes for re-identification and furthermore identifies which sections of a subjectsu0027 silhouette is able to deliver optimum performance. Human silhouettes of 90 subjects from the CASIA dataset walking 0° and 90° to a fixed CCTV camera were used for the purpose of re-identification. Each subjectu0027s video sequence comprised between 10 and 50 frames. For both views, silhouettes were segmented into eight algorithmically-defined areas: head and neck, shoulders, upper 50%, lower 50%, upper 15%, middle 35%, lower 40% and whole body. A GFD was used independently on each segment at each angle. After extracting the GFD feature for each frame, a linear discriminant analysis (LDA) classifier was used to investigate re-identification accuracy rate, where 50% of each subjectu0027s frames were used for training and the other 50% were used for testing. The results show that 97% identification accuracy rate at the 10th rank is achieved by using GFD on the upper 50% segment of the human silhouette front (0°) side. For 90° images, using GFD on the upper 15% silhouette segment resulted in almost 98% accuracy rate at the 10th rank.	algorithm;closed-circuit television;computer vision;global illumination;linear discriminant analysis	Rawabi Alsedais;Richard M. Guest	2017	2017 International Carnahan Conference on Security Technology (ICCST)	10.1109/CCST.2017.8167840	fourier transform;computer vision;artificial intelligence;silhouette;computer science;feature extraction;image segmentation;linear discriminant analysis;field of view	Vision	33.88929375278317	-56.68606746269235	137231
5a281258b7fd3f5672f48c72732177655bec9481	tud-mmc at mediaeval 2016: context of experience task		This paper provides a three-step framework to predict user assessment of the suitability of movies for an inflight viewing context. For this, we employed classifier stacking strategies. First of all, using the different modalities of training data, twenty-one classifiers were trained together with a feature selection algorithm. Final predictions were then obtained by applying three classifier stacking strategies. Our results reveal that different stacking strategies lead to different evaluation results. A considerable improvement can be found for the F1-score when using the label stacking strategy.	f1 score;feature selection;memory management controller;selection algorithm;stacking;statistical classification	Bo Wang;Cynthia C. S. Liem	2016			computer science	NLP	25.21346966138284	-56.94004255017299	137393
b8a198efac2ea44fc034682580fbec244b3e70b4	generalizing capacity of face database for face recognition	face recognition;generalizing capacity;face database;lighting;feature space;information retrieval;data set;eigenfaces;image reconstruction;space technology;error rate;image classification;prototypes	A face image can be represented by a point in a feature space such as spanned by a number of eigenfaces. In methods based on nearest neighbor classiication, the representational capacity of face database depends on how prototypical face images are chosen to account for possible image variations and also how many pro-totypical images or their feature points are available. In this paper, we propose a novel method for generalizing the representational capacity of available face database. Any two feature points of the same class (individual) are generalized by the feature line passing through the points. The feature line covers more of the face space than the feature points and thus expands the capacity of the available database. In the feature line representation, the classiication is based on the distance between the feature point of the query image and each of the feature lines of the prototypical images. Experiments are presented using a data set from ve databases: the MIT, Cambridge, Bern, Yale and our own. There are 620 images of 124 individuals subject to varying viewpoint, illumination, and expression. The results show that the error rate of the proposed method is about 55%-60% of that of the standard Eigenface method of Turk and Pentland. They also demonstrate that the recognition result can be used for inferring how the position of the input face relative to the two retrieved faces.	database;eigenface;face space;facial recognition system;feature vector;the turk	Stan Z. Li;Juwei Lu	1998			facial recognition system;computer vision;contextual image classification;feature detection;computer science;machine learning;pattern recognition;eigenface;k-nearest neighbors algorithm;feature	Vision	39.16948099777651	-57.582046236660645	137427
ef912e0373b9139764b7bbfc51c05ed8f4bbd1fd	natural scene text detection by multi-scale adaptive color clustering and non-text filtering	non text filtering;multi scale adaptive color clustering;ccs extraction;natural scene text detection	In recent years, natural scene text detection gains increasing attention because it plays an important role in many computer related techniques. In this paper, we propose a text detection method consisting of two major steps: connected components (CCs) extraction and non-text filtering. For CCs extraction, a multi-scale adaptive color clustering approach is proposed, which can extract text from images in different color complexities and is robust to contrast variation. For non-text filtering, we combine text covariance descriptor (TCD) with histogram of oriented gradients (HOG) to construct feature vectors and use them to distinguish text from background at character and text line levels. Besides, a new text line generation strategy combining both refined and unrefined CCs is applied, which can retrieve some mis-eliminated characters and generate more integrated text lines. Experiments are conducted on two publicly available datasets, the ICDAR 2013 and the ICDAR 2011 datasets, the obtained F-measures on which are 0.76 and 0.75, respectively. Comparative results with some state-of-the-art text detection algorithms demonstrate that the proposed method achieves competitive performance on text detection.	cluster analysis	Hui Wu;Beiji Zou;Yu-qian Zhao;Zailiang Chen;Chengzhang Zhu;Jianjing Guo	2016	Neurocomputing	10.1016/j.neucom.2016.07.016	computer science;machine learning;pattern recognition;data mining;information retrieval	Vision	36.919340274049446	-63.79805129484931	137744
62899115ff3fd00c8b58e0f93b638e47386d223c	detecting fingerprint distortion from a single image	visual databases fingerprint identification image matching video signal processing;video signal processing;image matching;public domain fingerprint database fingerprint distortion detection single image friction ridge skin elastic distortion fingerprint matching systems distortion detection techniques fingerprint video traditional fingerprint sensing techniques;quality assessment databases force fingerprint recognition image quality skin accuracy;fingerprint identification;visual databases	Elastic distortion of friction ridge skin is one of the major challenges in fingerprint matching. Since existing fingerprint matching systems cannot match seriously distorted fingerprints, criminals may purposely distort their fingerprints to evade identification. Existing distortion detection techniques require availability of specialized hardware or fingerprint video, limiting their use in real applications. In this paper we conduct a study on fingerprint distortion and develop an algorithm to detect fingerprint distortion from a single image which is captured using traditional fingerprint sensing techniques. The detector is based on analyzing ridge period and orientation information. Promising results are obtained on a public domain fingerprint database containing distorted fingerprints.	acoustic fingerprint;algorithm;autostereogram;distortion;fingerprint recognition;image quality;sensor;vulnerability (computing)	Xuanbin Si;Jianjiang Feng;Jie Zhou	2012	2012 IEEE International Workshop on Information Forensics and Security (WIFS)	10.1109/WIFS.2012.6412616	computer vision;speech recognition;computer science;pattern recognition;fingerprint recognition	Vision	31.52760166086244	-62.90120104164141	137814
35ba70db2816b0313257eedc0d9a117057d4a443	comparative study of subspace-based techniques in the task of partially occluded reconstruction of faces		Facial recognition systems in controlled environments have presented satisfactory identification results. However, we can not make the same assertion when the collection environment is uncontrolled. The factors responsible for these low recognition rates are variations in illumination, pose, expression and occlusion, which introduce intraclass variations and degrade recognition performance. Compared with problems of pose, illumination and expression, the problem related to occlusion is relatively little studied in the area. In the literature there are some techniques based on subspace with initiatives to reconstruct the partly occluded face. However, there is no study showing the pros and cons of each variation. The objective of this work is to investigate the different existing techniques based on subspace, and with this to present the pros and cons of each technique. In this paper, the Wavelet transform was used to extract a set of characteristics of face images. According to the results we can see that the Fast Recursive PCA, Recursive and GPCA strategies achieved better performance, in terms of recognition rate, after evaluation with the Extreme Learning Machine classifier.		Jonas Mendon&#231;a Targino;Sarajane Marques Peres;Clodoaldo Ap. M. Lima	2018		10.1145/3229345.3229383	wavelet transform;extreme learning machine;biometrics;assertion;recursion;facial recognition system;classifier (linguistics);subspace topology;pattern recognition;artificial intelligence;computer science	Vision	33.26858445781084	-58.306694400066604	137954
c1d7c0cd41fc3e0c297300301d31a30ea194d30b	a novel object detection method based on fuzzy sets theory and surf.				Wahyu Rahmaniar;Wen-June Wang	2015		10.3233/978-1-61499-522-7-570	computer vision;machine learning;algorithm	Robotics	29.539244348441876	-57.78174698543544	138050
7342db912cc4321ab39e8ecd616a89b41d7addcb	aurora image classification based on multi-feature latent dirichlet allocation		Due to the rich physical meaning of aurora morphology, the classification of aurora images is an important task for polar scientific expeditions. However, the traditional classification methods do not make full use of the different features of aurora images, and the dimension of the description features is usually so high that it reduces the efficiency. In this paper, through combining multiple features extracted from aurora images, an aurora image classification method based on multi-feature latent Dirichlet allocation (AI-MFLDA) is proposed. Different types of features, whether local or global, discrete or continuous, can be integrated after being transformed to one-dimensional (1-D) histograms, and the dimension of the description features can be reduced due to using only a few topics to represent the aurora images. In the experiments, according to the classification system provided by the Polar Research Institute of China, a four-class aurora image dataset was tested and three types of features (MeanStd, scale-invariant feature transform (SIFT), and shape-based invariant texture index (SITI)) were utilized. The experimental results showed that, compared to the traditional methods, the proposed AI-MFLDA is able to achieve a better performance with 98.2% average classification accuracy while maintaining a low feature dimension.	ai winter;aurora;computer vision;deep learning;distortion;experiment;feature selection;galaxy morphological classification;grayscale;latent dirichlet allocation;preprocessor;scale-invariant feature transform;standard test image;statistical classification;support vector machine;topic model	Yanfei Zhong;Rui Huang;Ji Zhao;Bei Zhao;Tingting Liu	2018	Remote Sensing	10.3390/rs10020233	artificial intelligence;geology;scientific expeditions;computer vision;latent dirichlet allocation;scale-invariant feature transform;invariant (mathematics);contextual image classification;histogram;feature dimension	AI	30.965146284505867	-55.65813781994977	138085
8dbd4e78ec483599c4cf53a305fb633c5728d8b2	conditional linear discriminant analysis	auxiliary variable;global fisher like dimensionality reduction criterion linear discriminant analysis supervised pixel based image segmentation feature extraction conditioned fisher criteria;image segmentation;limiting factor;global fisher like dimensionality reduction criterion;conditioned fisher criteria;image segmentation feature extraction image classification;image classification;linear discriminate analysis;linear discriminant analysis image segmentation data mining pixel nonlinear filters lungs feature extraction labeling filter bank gabor filters;feature extraction;classification accuracy;linear discriminant analysis;dimensional reduction;supervised pixel based image segmentation	Dimensionality reduction by means of linear discriminant analysis (LDA) can generally lead to considerable improvements in classification accuracy and computation time. However, in supervised, pixel-based, image segmentation, the limiting factor of LDA that it cannot extract more than K - 1 features (K the number of classes) often prevents successfully employing it as K is typically small. Based on the observation that the kind of feature to extract should often depend on the kind of image structure that is in the vicinity, we propose to condition LDA on auxiliary variables extracted from the manual segmentations (which are only available in the training phase). The conditioned Fisher criteria obtained through this are subsequently combined to construct our final global Fisher-like dimensionality reduction criterion. This conditional LDA is capable of extracting more features than standard LDA, which can considerably improve the segmentation accuracy as our experiments show	computation;dimensionality reduction;experiment;image segmentation;linear discriminant analysis;pixel;reduction criterion;supervised learning;time complexity	Marco Loog	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.402	computer vision;contextual image classification;kernel fisher discriminant analysis;limiting factor;feature extraction;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;linear discriminant analysis;scale-space segmentation;dimensionality reduction	Vision	31.5643329852434	-55.886435470665745	138121
e03f1f4e673f6a2937c48a0afb4dfd442aabbc5a	forest - a flexible object recognition system		Despite the growing importance of image data, image recognition has succeeded in taking a permanent role in everyday life in specific areas only. The reason is the complexity of currently available software and the difficulty in developing image recognition systems. Currently available software frameworks expect users to have a comparatively high level of programming and computer vision skills. FOREST – a flexible object recognition framework – strives to overcome this drawback. It was developed for non-expert users with littleto-no knowledge in computer vision and programming. While other image recognition systems focus solely on the recognition functionality, FOREST covers all steps of the development process, including selection of training data, ground truth annotation, investigation of classification results and of possible skews in the training data. The software is highly flexible and performs the computer vision functionality autonomously by applying several feature detection and extraction operators in order to capture important image properties. Despite the use of weakly supervised learning, applications developed with FOREST achieve recognition rates between 86 and 99% and are comparable to state-of-the-art recognition systems.	algorithm;computer vision;feature detection (computer vision);feature detection (web development);ground truth;high-level programming language;image processing;interactivity;outline of object recognition;point of view (computer hardware company);random forest;requirement;semiconductor industry;software framework;statistical classification;supervised learning;test set	Julia Möhrmann;Gunther Heidemann	2015			computer vision	Vision	27.221627295967828	-58.74570989200484	138429
7a277c9dbf9db1951a89fddd21e9d22128d5a753	iris authentication in handheld devices - considerations for constraint-free acquisition	databases;electronic engineering;nist;iris biometrics;performance evaluation;authentication;smart phones;iris recognition;smartphones;recognition;focus;consumer biometrics;visible only optical system iris authentication workflow constraint free acquisition biometric mobile acquisition techniques handheld imaging devices iris image quality iris size acquisition wavelength smartphone model pixel resolution;article;cameras;tracking;consumer biometrics iris biometrics smartphones;iris recognition authentication smart phones cameras databases nist performance evaluation;smart phones image resolution iris recognition	As a near ideal biometric, iris authentication is widely used and mobile acquisition techniques are known. But iris acquisition on handheld imaging devices, such as smartphones, poses multiple, unique challenges. In this paper, a range of factors that affect the quality of iris images are reviewed. Iris size, image quality and acquisition wavelength are found to be key factors. Experimental results are presented confirming the lower limits of iris size for useful authentication performance. The authentication workflow for handheld devices is described. A case study on a current smartphone model is presented, including calculation of the pixel resolution that can be achieved with a visible-only optical system. Based on these analyses, system requirements for unconstrained acquisition in smartphones are discussed. Several design strategies are presented and key research challenges are outlined together with potential solutions.		Shejin Thavalengal;Petronel Bigioi;Peter M. Corcoran	2015	IEEE Transactions on Consumer Electronics	10.1109/TCE.2015.7150600	computer vision;nist;computer science;engineering;iris recognition;authentication;tracking;internet privacy;computer security;focus	Visualization	29.795107909364546	-62.25855938159785	138483
fa8f68c6e9e9de10ebafb8daac627bd47a73d518	human eyebrow recognition in the matching-recognizing framework	matching recognizing framework;eyebrow recognition;fast template matching;fourier spectrum distance;face recognition;期刊论文;discriminative similarity;matching similarity	This paper studies the problem of automatically recognizing human eyebrows using a frontal view. In the matching-recognizing framework for image-based object classification, we design an automatic human eyebrow recognition system via fast template matching and Fourier spectrum distance. Fast template matching is used to locate the target subregion of a gallery template or a pure eyebrow image in a probe original eyebrow image, whereas Fourier spectrum distance is used to determine the final identity of the probe original eyebrow image. We conducted a number of experiments to demonstrate the efficacy of the proposed system and corroborate the validity of eyebrow recognition on the BJUT eyebrow database. Moreover, we also tested the system on the color FERET database. Experimental results show that our approach can be directly applied to face recognition by only replacing eyebrow templates with face templates, and may achieve higher accuracy in eyebrow recognition than in small face recognition. This is a strong argument for eyebrow recognition to replace face recognition as an independent biometric in certain scenarios, especially where relatively large eyebrows can be cropped.		Yujian Li;Houjun Li;Zhi Cai	2013	Computer Vision and Image Understanding	10.1016/j.cviu.2012.10.007	facial recognition system;computer vision;speech recognition;computer science;pattern recognition;three-dimensional face recognition;mathematics	Vision	35.16785192569768	-57.25612508189005	138589
a2b24511ab903adb0529fe36d043dde64f15abc6	soft-biometric detection based on supervised learning	support vector machines;glass;learning systems;image edge detection;image color analysis;feature extraction;cameras	In the past 5 years, people re-identification has been a popular topic as an application using computer vision techniques. Among the models used for people re-identification, soft-biometric traits based models have great potential due to the semantic meaning and robust performance they have. In this paper, we will exploit the performance of supervised learning based method on the detection of three soft-biometric traits: Glasses, Cap and Clothes Pattern. Simple features like edge and frequency are extracted from sample images and used for learning. Two supervised learning methods - Support Vector Machine (SVM) and Extreme Learning Machine (ELM) are employed and compared. Different normalization methods are compared as well. Experiments are carried out on images from FERET dataset and images collected online, and discussion is provided.	biometrics;computer vision;feret (facial recognition technology);microsoft edge;simple features;supervised learning;support vector machine	Zhi Zhou;Glen Hong Ting Ong;Eam Khwang Teoh	2014	2014 13th International Conference on Control Automation Robotics & Vision (ICARCV)	10.1109/ICARCV.2014.7064310	semi-supervised learning;unsupervised learning;support vector machine;computer vision;feature extraction;computer science;online machine learning;machine learning;pattern recognition;glass;supervised learning;generalization error	Vision	31.083299003335906	-56.6041982652485	138815
2eb5288f52175c48b795ddafb1b147ae3c247fec	face recognition based on adaptive soft histogram local binary patterns	会议论文;face recognition;local binary pattern lbp;soft histogram local binary pattern	In this paper we propose the adaptive soft histogram local binary pattern (ASLBP) for face recognition. ASLBP is an extension of the soft histogram local binary pattern (SLBP). Different from the local binary pattern (LBP) and its variants, ASLBP is based on adaptively learning the soft margin of decision boundaries with the aim to improve recognition accuracy. Experiments on the CMU-PIE database show that ASLBP outperforms LBP and SLBP. Although ASLBP is designed to increase the performance of SLBP, the proposed learning process can be generalized to other LBP variants. © Springer International Publishing 2013.	facial recognition system;local binary patterns	Huixing Ye;Roland Hu;Huimin Yu;Robert I. Damper	2013		10.1007/978-3-319-02961-0_8	local binary patterns;histogram matching;pattern recognition	Vision	34.6432566379089	-58.70536423358853	138863
1369e9f174760ea592a94177dbcab9ed29be1649	geometrical facial modeling for emotion recognition	emotion recognition;face feature extraction emotion recognition mouth training facial features hidden markov models;face recognition;learning artificial intelligence emotion recognition face recognition;learning artificial intelligence;emotion identification geometrical facial modeling emotion recognition facial changes person internal emotional states facial expression analysis facial motion facial affect facial muscular movements evolutive process organisms behavior facial elements human face characteristics machine learning facial features	Facial expressions are the facial changes in response to a person's internal emotional states, intentions, or social communications. Facial expression analysis has been an active research topic for behavioral scientists since the work of Darwin in 1872. It includes both measurement of facial motion and recognition of expression. There are two different ways to analyze facial expressions: one considers facial affect (emotion) and the other facial muscular movements. Many researchers argue that there is a set of basic emotions which were preserved during evolutive process because they allow the adaption of the organisms behavior to distinct daily situations. This paper discusses emotion recognition based on analysis of facial elements. Different feature sets are proposed to represent the characteristics of the human face and their performance is evaluated using Machine Learning techniques. The results indicate that the selected facial features represent a valid approach for emotion identification.	c4.5 algorithm;darwin;emotion recognition;experiment;facial recognition system;machine learning	Giampaolo L. Libralon;Roseli A. Francelin Romero	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6707085	facial recognition system;computer vision;facial action coding system;computer science;artificial intelligence;face hallucination	Vision	25.607879802925392	-63.95294748499365	138874
8e1bf34e0da6c89f2ee64a8399ebc74b082df2aa	font type extraction and character prototyping using gabor filters	filtering;prototypes gabor filters filtering shape character recognition frequency humans image analysis visual system optical character recognition software;prototypes;gabor filters;optical character recognition software;gabor filter;shape;human visual system;image analysis;humans;frequency;visual system;character recognition	In this paper, we present an automatic method for character prototyping and font type characterization in machine-printed document images at a character level. To do so, we use a generic textural approach, which considers text as a texture, instead of working at a pixel level like most of the methods proposed so far. In this way, Gabor filtering seems to be an appropriate tool for texture characterization, since its design has been inspired by the human visual system. The objective of the paper is then to verify this hypothesis by applying our method on a corpus composed of what we call “typographically rich and recurrent” machine-printed document images.	gabor filter;pixel;printing;prototype;text corpus	Bénédicte Allier;Hubert Emptoz	2003		10.1109/ICDAR.2003.1227772	filter;computer vision;image analysis;speech recognition;visual system;shape;computer science;frequency;pattern recognition;prototype;human visual system model;gabor wavelet	Vision	35.14483181728676	-65.39647871259636	139064
fd8d24784d31dd169e50ca0249b70bac030836bc	unstructured scene object localization algorithm based on sparse overcomplete representation	image objects;code space;sparse deco;in line;combinatorial search;image representations;会议论文;score matching;dynamic threshold;neuronal response;object localization;matching methods;scene object;energy model;model solution;computational model;overcomplete representations	Unstructured scene has many uncertainties and unpredictable states. It brings difficulties to the object localization, which is pixel-based processing. Themethod of analog visual information processing is an effective way to solve the problem mentioned above. Sparse overcomplete representation is an image representation model which is more in line with visual mechanism. However, the overcomplete representation not only increases the combinatorial search difficulty of sparse decomposition, but also changes the symmetry between input and code space. Furthermore, it makes the model solution and calculation method complicated. In order to solve the afore mentioned problem and effectively use this model to achieve automatic image object localization, this paper takes the unstructured scenes object localization as the background. Firstly, the overcomplete representation computational model which is based on energy model and score matching method is established. Then an automatic object localization method based on the neuronal response and dynamic threshold strategy is proposed and applied to the movement object localization. On this basis, the error analysis is done. Experimental results show that the method can achieve the movement object localization. © 2012 Springer-Verlag.	algorithm;sparse	Peng Lu;Yuhe Tang;Eryan Chen;Huige Shi;Shanshan Zhang	2012		10.1007/978-3-642-31588-6_79	computer vision;mathematical optimization;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;combinatorial search;computational model	Vision	37.824526062912376	-54.70603259310551	139132
27a918a368e971a15b545abf63353e269a8ce8a2	weedmap: a large-scale semantic weed mapping framework using aerial multispectral imaging and deep neural network for precision farming		The ability to automatically monitor agricultural fields is an important capability in precision farming, enabling steps towards more sustainable agriculture. Precise, high-resolution monitoring is a key prerequisite for targeted intervention and the selective application of agro-chemicals. The main goal of this paper is developing a novel crop/weed segmentation and mapping framework that processes multispectral images obtained from an unmanned aerial vehicle (UAV) using a deep neural network (DNN). Most studies on crop/weed semantic segmentation only consider single images for processing and classification. Images taken by UAVs often cover only a few hundred square meters with either color only or color and near-infrared (NIR) channels. Although a map can be generated by processing single segmented images incrementally, this requires additional complex information fusion techniques which struggle to handle high fidelity maps due to their computational costs and problems in ensuring global consistency. Moreover, computing a single large and accurate vegetation map (e.g., crop/weed) using a DNN is non-trivial due to difficulties arising from: (1) limited ground sample distances (GSDs) in high-altitude datasets, (2) sacrificed resolution resulting from downsampling high-fidelity images, and (3) multispectral image alignment. To address these issues, we adopt a stand sliding window approach that operates on only small portions of multispectral orthomosaic maps (tiles), which are channel-wise aligned and calibrated radiometrically across the entire map. We define the tile size to be the same as that of the DNN input to avoid resolution loss. Compared to our baseline model (i.e., SegNet with 3 channel RGB (red, green, and blue) inputs) yielding an area under the curve (AUC) of [background=0.607, crop=0.681, weed=0.576], our proposed model with 9 input channels achieves [0.839, 0.863, 0.782]. Additionally, we provide an extensive analysis of 20 trained models, both qualitatively and quantitatively, in order to evaluate the effects of varying input channels and tunable network hyperparameters. Furthermore, we release a large sugar beet/weed aerial dataset with expertly guided annotations for further research in the fields of remote sensing, precision agriculture, and agricultural robotics.	aerial photography;agricultural robot;artificial neural network;autostereogram;baseline (configuration management);benchmark (computing);computation;computer vision;decimation (signal processing);deep learning;experiment;graphics processing unit;ibm sequoia;image resolution;machine learning;map;multispectral image;pixel;robotics;sensor;statistical classification;supervised learning;switzerland;unmanned aerial vehicle	Inkyu Sa;Marija Popovic;Raghav Khanna;Zetao Chen;Philipp Lottes;Frank Liebisch;Juan I. Nieto;Cyrill Stachniss;Achim Walter;Roland Siegwart	2018	Remote Sensing	10.3390/rs10091423	vegetation;computer vision;geology;multispectral image;artificial intelligence;rgb color model;artificial neural network;weed;precision agriculture;upsampling;communication channel	AI	27.15281643588288	-53.58026609530057	139172
31049a8c7166002457ab499f88667395ba299500	improved kernel descriptors for effective and efficient image classification		Kernel descriptors have been proven to outperform existing histogram based local descriptors as such descriptors are extracted from the match kernels which measure similarities between image patches using different pixel attributes (gradient, colour or LBP pattern). The extraction of kernel descriptors does not require coarse quantization of pixel attributes. Instead, each pixel equally participates in matching between two image patches. In this paper, by leveraging the kernel properties, we propose a unique approach which simultaneously increases the effectiveness and efficiency of the existing kernel descriptors. Specifically, this is done by improving the similarity measure between two different patches in terms of any pixel attribute. The proposed kernel descriptors are more discriminant, take less time to be extracted and have much lower dimensions. Our experiments on Scene Categories and Caltech 101 databases show that our proposed approach outperforms the existing kernel descriptors.	caltech 101;computational complexity theory;database;discriminant;experiment;gradient;kernel (operating system);loadable kernel module;pixel;quantization (signal processing);similarity measure	Priyabrata Karmakar;Shyh Wei Teng;Dengsheng Zhang;Ying Liu;Guojun Lu	2017	2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2017.8227446	artificial intelligence;pattern recognition;caltech 101;kernel (linear algebra);pixel;quantization (signal processing);computer science;feature extraction;contextual image classification;similarity measure;histogram	Vision	35.23310674147832	-57.7675922567443	139182
e1c50cf0c08d70ff90cf515894b2b360b2bc788b	facial behavior as behavior biometric? an empirical study	kernel discriminant analysis;empirical study;biometrics access control;behavior biometric;behavioural sciences computing;data collection;biometrics;psychology;biometrics humans face recognition kernel information technology australia character recognition speech recognition facial features eyes;subspace analysis;kernel principle component analysis behavior biometric physiological characteristics behavioral characteristics biometric recognition human facial behavior human identification kernel subspace analysis japanese female facial expression database;biometric recognition;face recognition;feature extraction;principal component analysis;physiological characteristics;kernel principle component analysis;human facial behavior;human identification;principle component analysis;behavioral characteristics;psychology behavioural sciences computing biometrics access control face recognition feature extraction gesture recognition principal component analysis;facial expression;facial behaviors;individual difference;kernel subspace analysis;japanese female facial expression database;gesture recognition;facial fiducial points;facial fiducial points biometrics face recognition facial behaviors subspace analysis kernel principle component analysis kernel discriminant analysis	Physiological and/or behavioral characteristics of humans such as face, gait and/or voice have been used in biometric recognition technology. Apart from those characteristics reported in the literature, the hypothesis of this research was to initially investigate if human facial behaviors could also be used as another behavioral traits for human identification. We used kernel subspace analysis method to analyze the data so as to support our hypothesis. We used the Japanese Female Facial Expression (JAFFE) database as it provides the facial behavior traits for data collection. The experimental results indicate that facial behaviors may provide information about individual differences, thus may be used as another behavioral biometric.	biometrics;feature extraction;fiducial marker;kernel (operating system);linear discriminant analysis;regular expression	Pohsiang Tsai;Tom Hintz;Tony Jan	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4414085	computer vision;speech recognition;computer science;machine learning;pattern recognition;gesture recognition;statistics;principal component analysis	Robotics	26.011468677055525	-63.99945251235837	139276
54988519ad28be4ab7a2bc562f5f5d92ad823606	supervised image segmentation using learning and merging	gmm;multi-label segmentation;region merging;image texture;gaussian processes;learning artificial intelligence;mixture models;image classification;image segmentation	The segmentation problem can be viewed as a learning and merging problem based on superpixels (image segments), which can incorporate a group of cues to guide the segmentation. So the proposed multi-label segmentation algorithm mainly consists of two stages: the learning stage and the merging stage. In the learning stage, Gaussian Mixture Models (GMMs) firstly learn color models for different components of objects. Based on the likelihood, we execute the alpha-expansion algorithm only once in order to alleviate the shrinking bias. The initial labels help determine whether a superpixel is too noisy, and the contour responses between superpixels can distinguish spurious boundaries. Those superpixels containing too much noisy pixels and spurious boundaries will be unlabeled. In the merging stage, unlabeled superpixels may have similar color information while differing in texture information. Therefore, they can be correctly classified by a novel region merging algorithm based on maximal similarity. In this way the advantages of features in different levels are enhanced by uniting them in different stages. Finally, the proposed method is evaluated on the Berkeley segmentation benchmark, the Graz benchmark and the Grabcut benchmark. Experimental results show that our method obtains the highest accuracy on the Graz benchmark, and the performance on other benchmarks can also be comparable or better than current leading algorithms. © 2013 University of Trieste and University of Zagreb.	image segmentation;supervised learning	Xiyu Yu;Fugen Zhou;Xiangzhi Bai;Bin Guo;Dongjie Tan	2013			image texture;computer vision;range segmentation;machine learning;segmentation-based object categorization;pattern recognition;mathematics;region growing;image segmentation;scale-space segmentation	Vision	32.28673455229001	-54.43693918457242	139292
b4a10af70b18f6bd7b9824a0b13a96655efc08c6	on the convergence of image compression and object recognition	object recognition;entropy coding;segmentation;transform coding;object segmentation;object oriented image compression;image compression;object oriented;performance analysis;performance model;space complexity;compression ratio;region growing	Over the past four decades, image compression and object recognition have evolved from pixel-level to region-level processing, and thence to feature-based resolution. For example, compression has progressed from entropy coding of a bit or pixel stream, to transform coding applied to rectangular encoding blocks, to feature-based compression that employs segmentation of isospectral or isotextural regions. Similarly, object recognition has progressed from operations on individual pixel intensities or spectral signatures, to region- or feature-based processing employing segmentation. Recent progress in image compression indicates that significantly decreased bit rate (thus, significantly increased compression ratio) can be obtained by isolating or segmenting, then compressing scene objects, which is called object-based compression or OBC.In this paper, the relationship between object segmentation and compression is presented theoretically, then exemplified in terms of current research in OBC. The relationship between object compression and recognition is also discussed theoretically. Recent work in object recognition is shown to be closely related theoretically to OBC. Two recently-developed paradigms, Muñoz et al.'s region growing algorithm and Campbell et al.'s Quick-Sketch, which support very efficient compression and accurate recognition/retrieval of image objects, are summarized. Additionally. a performance model is given for OBC that isolates space complexity bottlenecks for future enhancement and implementation.	algorithm;dspace;entropy encoding;image compression;internet branding;object-based language;outline of object recognition;pixel;region growing;sketch;transform coding;type signature	Mark S. Schmalz	2005		10.1145/1167253.1167343	data compression;color cell compression;computer vision;block truncation coding;image compression;computer science;machine learning;segmentation-based object categorization;pattern recognition;lossless compression;3d single-object recognition;context-adaptive binary arithmetic coding;texture compression	ML	25.368857728434424	-55.691284257275626	139339
83d06e972f6e760d9da300d46a23437eb425a300	block-based, memory-efficient jpeg2000 images indexing in compressed-domain	database indexing;discrete wavelet transforms;memory efficiency;wavelet decomposed coefficient matrix;transform coding indexing image coding feature extraction discrete wavelet transforms information retrieval image retrieval image databases decoding wavelet domain;image coding;data compression;decoding;image databases;block based image indexing;information retrieval;vistex image database;performance;transform coding database indexing code standards data compression feature extraction visual databases image retrieval image coding wavelet transforms;speed up;image database;retrieval performance;low complexity;image indexing;feature extraction block based image indexing jpeg2000 compressed domain memory efficiency wavelet decomposed coefficient matrix performance vistex image database complexity;code standards;complexity;transform coding;subband based image indexing;speed up subband based image indexing jpeg2000 image compression memory efficiency feature extraction wavelet subband variance retrieval performance vistex image database;block based;wavelet transforms;transform coding indexing image coding discrete wavelet transforms feature extraction information retrieval image retrieval image databases wavelet domain wavelet coefficients;image compression;indexing;feature extraction;compressed domain;subband;indexation;jpeg2000;image retrieval database indexing visual databases data compression feature extraction wavelet transforms image coding;wavelet domain;wavelet coefficients;wavelet subband variance;visual databases;image retrieval	The contribution of this paper is the development of a fast, subband-based JPEG2000 image indexing system in the compressed domain which achieves high memory efficiency. This is the extended work on a previously block-based indexing system. The feature extracted is variance of each wavelet subband in the compressed domain with the emphasis that subbands are not buffered to maintain memory efficiency. Retrieval performance on VisTex image database indexing has shown the effectiveness and speed up of execution of the proposed features.	jpeg 2000	Ziyou Xiong;Thomas S. Huang	2002		10.1109/IAI.2002.999896	data compression;database index;computer vision;search engine indexing;complexity;transform coding;performance;feature extraction;speedup;image retrieval;image compression;computer science;pattern recognition;jpeg 2000;information retrieval;wavelet transform	Vision	38.830081014340884	-61.39009654375234	139679
89d1bcd34dfb066de1b838a8fd0c4afbc06c18da	joint application of rough set-based feature reduction and fuzzy ls-svm classifier in motion classification	rough set theory;feature reduction;multi class classification;computational complexity;wavelet packet transform;feature extraction;principal component analysis;indexation;feature selection;classification accuracy;rough set;neural network;least squares support vector machine	This paper presents an effective classification scheme consisting of the rough set theory (RST)-based feature selection and the fuzzy least squares support vector machine (LS-SVM) classifier for the surface electromyographic (sEMG)-based motion classification. The wavelet packet transform (WPT) is exploited to decompose the four-class motion EMG signals to the non-overlapped sub-bands and the energy characteristic of each sub-band is adopted to form the original feature set. In order to reduce the computation complexity, the RST is utilized to get the reduction feature set without compromising classification accuracy. In the feature reduction phase, cluster separation index (CSI) is introduced to evaluate the performance of the proposed algorithm. In the sequel, the Fuzzy LS-SVM is constructed for the multi-class classification task. The RST-based feature selection is independent of the classifier design. Consequently the classification performance will vary with different classifiers. We make the comparison between the proposed classification scheme and the commonly used classification scheme, such as the combination of the principal component analysis (PCA)-based feature selection and the neural network (NN) classifier. The results of comparative experiments show that the diverse motions can be identified with high accuracy by the proposed scheme. Compared with other feature extraction and selection algorithms and classifiers, superior performance of the proposed classification scheme illustrates the potential of the SVM techniques combined with WPT and RST in EMG motion classification.	algorithm;amputees;artificial neural network;bands;cluster headache;comparison and contrast of classification schemes in linguistics and metadata;computation;electromyography;entity name part qualifier - adopted;experiment;feature extraction;feature selection;generalization (psychology);hepatitis b surface antigens;intel matrix raid;learning classifier system;least squares support vector machine;limb prosthesis;limb structure;linear separability;motion;multiclass classification;neural network simulation;numerous;principal component analysis;rough set;seizures;set theory;wavelet analysis;wavelet packet decomposition	Zhiguo Yan;Zhizhong Wang;Hongbo Xie	2007	Medical & Biological Engineering & Computing	10.1007/s11517-007-0291-x	rough set;computer science;machine learning;linear classifier;pattern recognition;data mining;mathematics;feature selection;feature;one-class classification;dimensionality reduction	ML	27.041692047345204	-61.71873608885255	139762
6dbeb4299cb40c24b98e4d82951472c557d181f3	texture characterization using shape co-occurrence patterns		Texture characterization is a key problem in image understanding and pattern recognition. In this paper, we present a flexible shape-based texture representation using shape co-occurrence patterns. More precisely, texture images are first represented by a tree of shapes, each of which is associated with several geometrical and radiometric attributes. Then, four typical kinds of shape co-occurrence patterns based on the hierarchical relationships among the shapes in the tree are learned as codewords. Three different coding methods are investigated for learning the codewords, which can be used to encode any given texture image into a descriptive vector. In contrast with existing works, the proposed approach not only inherits the shape-based method’s strong ability to capture geometrical aspects of textures and high robustness to variations in imaging conditions but also provides a flexible way to consider shape relationships and to compute high-order statistics on the tree. To the best of our knowledge, this is the first time that co-occurrence patterns of explicit shapes have been used as a tool for texture analysis. Experiments on various texture and scene data sets demonstrate the efficiency of the proposed approach.	algorithm;co-occurrence matrix;code word;computer vision;concentrate dosage form;deep learning;description;encode;experiment;marginal model;pattern recognition;physical object;shape context;silo (dataset);biologic segmentation	Gui-Song Xia;Gang Liu;Xiang Bai;Liangpei Zhang	2017	IEEE Transactions on Image Processing	10.1109/TIP.2017.2726182	artificial intelligence;robustness (computer science);computer vision;texture filtering;active shape model;co-occurrence;shape analysis (digital geometry);mathematics;feature extraction;texture compression;pattern recognition;image texture	Vision	37.96067832051143	-57.2723299407211	139949
beae5d1a0c426fba8312f29d2b4a995e2b18cd0c	a new chain-code quantization approach enabling high performance handwriting recognition based on multi-classifier schemes	erbium;quantization;image recognition;electronic mail;handwriting recognition;chain code;tk7882 p3 pattern recognition;feature space;machine vision;pattern recognition;robustness;cross validation;quantization handwriting recognition erbium character recognition proposals pattern recognition image recognition electronic mail robustness machine vision;multiple classifier system;proposals;high performance;character recognition;high speed	In this paper initially we propose a novel approach to classify handwritten characters based on a directional decomposition of the corresponding chain-code representation. This is alternative to previous transformations of the chain-codes proposed by the authors, namely the ordered and random decomposition of the bit-planes resulting from the binary representation of the chain-codes. Subsequently we utilize the power of the recently developed multiple classifier schemes using sntuple classifiers to integrate the complimentary information encapsulated in all three transformations into a more powerful and robust character recognition system. The results obtained through a series of cross-validation experiments show that the proposed fusion scheme not only outperforms its constituent parts and a number of other successful classifiers, but also enables significant savings in memory requirements compared to the original sntuple-based recognition system.	binary number;chain code;cross-validation (statistics);experiment;handwriting recognition;linear classifier;optical character recognition;requirement	Sanaul Hoque;Konstantinos Sirlantzis;Michael C. Fairhurst	2003		10.1109/ICDAR.2003.1227779	computer vision;erbium;speech recognition;feature vector;quantization;machine vision;computer science;machine learning;pattern recognition;handwriting recognition;chain code;cross-validation;robustness	AI	36.01038711115757	-59.3110251285353	140068
ddaf62811b5f45bc3674f9cb10e8b0238796557d	a comparison of thermal image descriptors for face analysis	histograms;image recognition;thermal image descriptors fuzzy color texture histogram fcth haar based features localized binary patterns facial expression recognition thermal domain color skins illumination changes face recognition face detection electromagnetic spectrum infrared range radiation detection thermographic cameras thermal imaging face analysis;infrared imaging emotion recognition face recognition image colour analysis image texture;face recognition;face recognition face image color analysis feature extraction histograms image recognition lighting;image color analysis;feature extraction;image descriptors thermal images face recognition facial expressions;face;lighting	Thermal imaging is a type of imaging that uses thermographic cameras to detect radiation in the infrared range of the electromagnetic spectrum. Thermal images are particularly well suited for face detection and recognition because of the low sensitivity to illumination changes, color skins, beards and other artifacts. In this paper, we take a fresh look at the problem of face analysis in the thermal domain. We consider several thermal image descriptors and assess their performance in two popular tasks: face recognition and facial expression recognition. The results have shown that face recognition can reach accuracy levels of 91% with Localized Binary Patterns. Also, despite the difficulty of facial expression detection, our experiments have revealed that Haar based features (FCTH - Fuzzy Color and Texture Histogram) offers the best results for some facial expressions.	experiment;face detection;facial recognition system;haar wavelet;illumination (image);skin (computing);visual descriptor	Ricardo Carrapiço;André Mourão;João Magalhães;Sofia Cavaco	2015	2015 23rd European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2015.7362499	computer vision;speech recognition;object-class detection;pattern recognition;three-dimensional face recognition;face hallucination	Vision	34.45794065805859	-59.79988283837476	140174
89c86b38e2c901f9f8b0146f5cd6a299c66fec90	matching forensic sketches to mug shot photos	heterogeneous face recognition;feature based representation;minimum distance matching;mug shot image;sift feature descriptor;forensic medicine;image matching;training;vectors face recognition feature extraction image matching statistical analysis;face recognition forensic sketch matching mug shot image local feature based discriminant analysis sift feature descriptor multiscale local binary pattern partitioned vector feature based representation minimum distance matching;photography;viewed sketch;discriminant analysis;accuracy;image enhancement;multiscale local binary pattern;local feature based discriminant analysis;face recognition;paintings;statistical analysis;vectors;image interpretation computer assisted;forensic sketch;local features;feature extraction;principal component analysis;forensics face feature extraction face recognition training accuracy principal component analysis;reproducibility of results;partitioned vector;heterogeneous face recognition face recognition forensic sketch viewed sketch local feature discriminant analysis feature selection;artificial intelligence;algorithms;feature selection;pattern recognition automated;face;humans;subtraction technique;forensic sketch matching;biometry;forensics;local feature discriminant analysis;algorithms artificial intelligence biometry face forensic medicine humans image enhancement image interpretation computer assisted paintings pattern recognition automated photography reproducibility of results subtraction technique	The problem of matching a forensic sketch to a gallery of mug shot images is addressed in this paper. Previous research in sketch matching only offered solutions to matching highly accurate sketches that were drawn while looking at the subject (viewed sketches). Forensic sketches differ from viewed sketches in that they are drawn by a police sketch artist using the description of the subject provided by an eyewitness. To identify forensic sketches, we present a framework called local feature-based discriminant analysis (LFDA). In LFDA, we individually represent both sketches and photos using SIFT feature descriptors and multiscale local binary patterns (MLBP). Multiple discriminant projections are then used on partitioned vectors of the feature-based representation for minimum distance matching. We apply this method to match a data set of 159 forensic sketches against a mug shot gallery containing 10,159 images. Compared to a leading commercial face recognition system, LFDA offers substantial improvements in matching forensic sketches to the corresponding face images. We were able to further improve the matching performance using race and gender information to reduce the target gallery size. Additional experiments demonstrate that the proposed framework leads to state-of-the-art accuracys when matching viewed sketches.		Brendan Klare;Zhifeng Li;Anil K. Jain	2011	IEEE transactions on pattern analysis and machine intelligence	10.1109/TPAMI.2010.180	face;computer vision;speech recognition;feature extraction;computer science;photography;machine learning;pattern recognition;accuracy and precision;forensic science;statistics;principal component analysis	Vision	35.60672860375508	-55.69729123378921	140487
f39abdc029f4b3e8bf684b7391d3ccb1b805f5d6	plant image retrieval using color, shape and texture features	texture features;feature extraction;content based image retrieval;gabor wavelets;image retrieval	We present a content-based image retrieval system for plant image retrieval, intended especially for the house plant identification problem. A plant image consists of a collection of overlapping leaves and possibly flowers, which makes the problem challenging. We studied the suitability of various wellknown color, shape and texture features for this problem, as well as introducing some new texture matching techniques and shape features. Feature extraction is applied after segmenting the plant region from the background using the max-flow min-cut technique. Results on a database of 380 plant images belonging to 78 different types of plants show promise of the proposed new techniques and the overall system: in 55% of the queries, the correct plant image is retrieved among the top-15 results. Furthermore, the accuracy goes up to 73% when a 132-image subset of well-segmented plant images are considered.	adaptive histogram equalization;approximation algorithm;color;content-based image retrieval;database;feature extraction;gabor filter;max-flow min-cut theorem;maxima and minima;maximum flow problem;minimum cut;preprocessor;the computer journal	Hanife Kebapci;Berrin A. Yanikoglu;Gözde B. Ünal	2011	Comput. J.	10.1093/comjnl/bxq037	image texture;computer vision;visual word;feature extraction;image retrieval;computer science;pattern recognition;automatic image annotation;information retrieval	Vision	36.29700595881111	-62.95140093365105	140929
c19b5cce6c542380ba3bd05fb128918499428914	pcn: part and context information for pedestrian detection with cnns		Pedestrian detection has achieved great improvements in recent years, while complex occlusion handling is still one of the most important problems. To take advantage of the body parts and context information for pedestrian detection, we propose the part and context network (PCN) in this work. PCN specially utilizes two branches which detect the pedestrians through body parts semantic and context information, respectively. In the Part Branch, the semantic information of body parts can communicate with each other via recurrent neural networks. In the Context Branch, we adopt a local competition mechanism for adaptive context scale selection. By combining the outputs of all branches, we develop a strong complementary pedestrian detector with a lower miss rate and better localization accuracy, especially for occlusion pedestrian. Comprehensive evaluations on two challenging pedestrian detection datasets (i.e. Caltech and INRIA) well demonstrated the effectiveness of the proposed PCN.	artificial neural network;automation;experiment;pal;pedestrian detection;program composition notation;recurrent neural network;sensor	Shiguang Wang	2017	CoRR		artificial intelligence;machine learning;computer vision;computer science;pedestrian;detector;recurrent neural network;pedestrian detection	AI	28.758729475301475	-52.42193923552032	141145
919a4f67be92ddf70ae7beaa16d88ef2b5901413	a cost-effective fingerprint recognition system for use with low-quality prints and damaged fingertips	enhancement;classification;recognition;neural net;fingerprint recognition;fingerprint;cost effectiveness	The development of a robust algorithm allowing good recognition of low-quality fingerprints with inexpensive hardware is investigated. A threshold FFT approach is developed to simultaneously smooth and enhance poor quality images derived from a database of imperfect prints. Features are extracted from the enhanced images using a number of approaches including a novel wedge ring overlay minutia detector that is particularly robust to imperfections. Finally, a number of neural net and statistically based classifiers are evaluated for the recognition task. Results for various combinations of the process are presented and discussed with regard to their utility in such a system.	fingerprint recognition	Andrew John Willis;L. Myers	2001	Pattern Recognition	10.1016/S0031-3203(00)00003-0	fingerprint;computer vision;speech recognition;cost-effectiveness analysis;biological classification;computer science;machine learning;data mining;three-dimensional face recognition;fingerprint recognition;artificial neural network	Vision	31.589125309387878	-63.46936145960981	141316
ab4c46ffb118e1f5678c487b68d21d4bb2d144b9	a semi-automatic deshredding method based on curve matching	会议论文	We present a semi-automatic method to reconstruct shredded documents. The novelty of the method lies in the way it performs pairwise matching of chads. The technique divides chad contours into curves using corner detection and introduces a procedure to assess the match of two curves. The proposed curve matching technique is robust to translation and rotation and can cope with shape deformations due to shredding by allowing overlapping of chads during matching. The alignment of text lines, crossing characters and color information on the chads is also utilized to improve matching performance. Visual interfaces are designed to allow for user input in identifying correctly matching chad pairs and reconstructing the document. The effectiveness of the method is demonstrated by solving the first and second puzzles of the DARPA shredder challenge.	computational anatomy;corner detection;darpa shredder challenge 2011;semiconductor industry;shredding (disassembling genomic data)	Shize Shang;Husrev T. Sencar;Nasir D. Memon;Xiangwei Kong	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7026120	computer vision;simulation;computer science;machine learning;mathematics;statistics	Robotics	37.68251789826127	-64.03382292765262	141333
1ff3a535c8e488d3bf417d7aac91b4d325b59ab6	an investigation into the use of partial face in the mobile environment	brody transform;partial faces;eigen template;face recognition;active pixel;lbp	Face recognition has been extensively explored in diversified applications on ubiquitous devices. Most of the research has been primarily focused on full frontal/profile of facial images while proposing novice techniques to pursue this problem. The resource constraint in Mobile devices adds more complexity to the face recognition process. To reduce computational requirements some investigations are made to use the partial faces for recognition process. However, the inadequate information in partial faces makes the problem much more challenging and therefore limited attempts have been made in this direction.#R##N##R##N#Our Active pixel based approach is capable of recognizing the persons using either full or partial face information. The technique reduces the computational resources compared to the LBP which was claimed as one of the most suitable approach on mobile devices. We have carried out the experiments on the YALE facial databases. Other works [1, 2] have used 50% vertical portion and showed the accuracy of correct recognition 94% within best five matches. In our dynamic partial matching we have used 10% to 34% image and obtained correct recognition rate from 96% to 100% within best three matches.		G. Mallikarjuna Rao;Praveen Kumar;G. Vijaya Kumari;Amit Pande;G. R. Babu	2011		10.1007/978-3-642-24031-7_53	facial recognition system;computer vision;face detection;computer science;artificial intelligence;three-dimensional face recognition	Web+IR	34.339955156351344	-56.25994121533496	141460
e13bf906f4ad11e70df586e768d64d0c2ec1827a	a hierarchical method for traffic sign classification with support vector machines	feature extraction support vector machines accuracy image color analysis image edge detection training shape;support vector machines;image classification;traffic information systems image classification support vector machines;traffic information systems;realtime applications hierarchical method traffic sign classification support vector machines driver assistance systems german traffic sign recognition benchmark gtsrb	Traffic sign classification is an important function for driver assistance systems. In this paper, we propose a hierarchical method for traffic sign classification. There are two hierarchies in the method: the first one classifies traffic signs into several super classes, while the second one further classifies the signs within their super classes and provides the final results. Two perspective adjustment methods are proposed and performed before the second hierarchy, which significantly improves the classification accuracy. Experimental results show that the proposed method gets an accuracy of 99.52% on the German Traffic Sign Recognition Benchmark (GTSRB), which outperforms the state-of-the-art method. In addition, it takes about 40 ms to process one image, making it suitable for realtime applications.	benchmark (computing);experiment;machine learning;preprocessor;real-time clock;real-time computing;support vector machine;traffic sign recognition	Gangyi Wang;Guanghui Ren;Zhilu Wu;Yaqin Zhao;Lihui Jiang	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6706803	support vector machine;contextual image classification;computer science;machine learning;pattern recognition;data mining	Vision	32.09042377402446	-56.27186410314964	141605
b273591a8c9bd0717f3b8ea30cf9ae5eed191732	automatic behavior pattern classification for social robots	socially responsible;behavior detection;cognitive developmental robotics;artificial neural networks;social robots;automatic detection;motion pattern analysis;pattern classification;pattern analysis;artificial neural network;social robot	In this paper, we focus our attention on providing robots with a system that allows them to automatically detect behavior patterns in other robots, as a first step to introducing social responsive robots The system is called ANPAC (Automatic Neural-based Pattern Classification) Its main feature is that ANPAC automatically adjusts the optimal processing window size and obtains the appropriate features through a dimensional transformation process that allow for the classification of behavioral patterns of large groups of entities from perception datasets Here we present the basic elements and operation of ANPAC, and illustrate its applicability through the detection of behavior patterns in the motion of flocks.	social robot	Abraham Prieto;Francisco Bellas;Pilar Caamaño;Richard J. Duro	2010		10.1007/978-3-642-13769-3_11	computer vision;computer science;artificial intelligence;social robot;machine learning;aisoy1;artificial neural network	Robotics	25.18003983010993	-63.743208849546065	141763
8a59d581ab3c8c6c39d6976dc28a37ae84f3fcc4	besnet: boundary-enhanced segmentation of cells in histopathological images		We propose a novel deep learning method called Boundary-Enhanced Segmentation Network (BESNet) for the detection and semantic segmentation of cells on histopathological images. The semantic segmentation of small regions using fully convolutional networks typically suffers from inaccuracies around the boundaries of small structures, like cells, because the probabilities often become blurred. In this work, we propose a new network structure that encodes input images to feature maps similar to U-net but utilizes two decoding paths that restore the original image resolution. One decoding path enhances the boundaries of cells, which can be used to improve the quality of the entire cell segmentation achieved in the other decoding path. We explore two strategies for enhancing the boundaries of cells: (1) skip connections of feature maps, and (2) adaptive weighting of loss functions. In (1), the feature maps from the boundary decoding path are concatenated with the decoding path for entire cell segmentation. In (2), an adaptive weighting of the loss for entire cell segmentation is performed when boundaries are not enhanced strongly, because detecting such parts is difficult. The detection rate of ganglion cells was 80.0% with 1.0 false positives per histopathology slice. The mean Dice index representing segmentation accuracy was 74.0%. BESNet produced a similar detection performance and higher segmentation accuracy than comparable U-net architectures without our modifications.		Hirohisa Oda;Holger Roth;Kosuke Chiba;Jure Sokolic;Takayuki Kitasaka;Masahiro Oda;Akinari Hinoki;Hiroo Uchida;Julia A. Schnabel;Kensaku Mori	2018		10.1007/978-3-030-00934-2_26	computer science;dice;computer vision;pattern recognition;concatenation;decoding methods;deep learning;false positive paradox;segmentation;artificial intelligence;image resolution;weighting	Vision	25.548178801071366	-52.28308528621205	141830
4ac315832d2d3b9584e369da0a7e82bc274b7623	foveation: an alternative method to simultaneously preserve privacy and information in face images	facial recognition systems	This paper presents a real-time foveation technique proposed as an alternative method for image obfuscation while simultaneously preserving privacy in face deidentification. Relevance of the proposed technique is discussed through a comparative study of the most common distortions methods in face images and an assessment on performance and effectiveness of privacy protection. All the different techniques presented here are evaluated when they go through a face recognition software. Evaluating the data utility preservation was carried out under gender and facial expression classification. Results on quantifying the tradeoff between privacy protection and image information preservation at different obfuscation levels are presented. Comparative results using the facial expression subset of the FERET database show that the technique achieves a good tradeoff between privacy and awareness with 30% of recognition rate and a classification accuracy as high as 88% obtained from the common figures of merit using the privacy-awareness map.		Víctor E. Alonso;Rogerio A. Enríquez-Caldera;Luis Enrique Sucar	2017	J. Electronic Imaging	10.1117/1.JEI.26.2.023015	computer vision;computer science;three-dimensional face recognition;internet privacy;face hallucination	Vision	30.601623727052836	-61.818269051714154	141993
f6c1d46e7c0d80e473d14ef4edfd017335b4e064	two-tier image features clustering for iris recognition on mobile		Nowadays, many smartphones are provided with built-in sensors for the acquisition and the recognition of specific biometric traits of the user. This policy has been adopted since the massive use of such devices brought the user to store sensible data in them as well as effectuate sensitive transactions on-the-move. As a consequence, many biometric systems have been migrated from stand alone to mobile environments. The methodology proposed in the following presents an approach to the iris recognition in visible spectrum. Iris images are first enhanced by a fuzzy color/contrast preserving technique and then passed to a two-tier clustering: the first is based on the linear decomposition of the iris into superpixels; the second one exploits an unsupervised learning network model to built a feature vector of the iris. According to the performance obtained in terms of time and recognition rate, the method is compliant with the needs of real-time and in-movement environments.	iris recognition;multitier architecture	Andrea F. Abate;Silvio Barra;Francesco D'Aniello;Fabio Narducci	2016		10.1007/978-3-319-52962-2_23	computer vision	Vision	30.351001378857617	-53.69853963224816	142033
39a31831797e6001c145de0dc5aea4a7060a2346	accurate image search using the contextual dissimilarity measure	busqueda informacion;sensibilidad contexto;analisis imagen;agregacion;pattern clustering;vision ordenador;context aware;mise a jour;iterative algorithms;distance measure;recherche image;dissimilarity measure;particle measurements;information retrieval;visual vocabulary size;vocabulary;contextual dissimilarity measure;bag of features based image search;video retrieval;size measurement;layout;effet dimensionnel;aggregation;multiple assignment;sinkhorn scaling algorithm;computer vision;actualizacion;distance regularization;recherche information;image representation;size effect;clustering method;multimedia databases;image search;rank aggregation;agregation;context dependent;image analysis;vision ordinateur;sensibilite contexte;efecto dimensional;image video retrieval;frequency;clustering methods;multimedia database;analyse image;vocabulary frequency iterative algorithms clustering methods size measurement particle measurements costs image retrieval layout image representation;image retrieval bag of features based image search contextual dissimilarity measure sinkhorn scaling algorithm clustering method visual vocabulary size distance measure multiple assignment rank aggregation;distance regularization image search image retrieval;pattern clustering image retrieval;updating;image retrieval	This paper introduces the contextual dissimilarity measure, which significantly improves the accuracy of bag-of-features-based image search. Our measure takes into account the local distribution of the vectors and iteratively estimates distance update terms in the spirit of Sinkhorn's scaling algorithm, thereby modifying the neighborhood structure. Experimental results show that our approach gives significantly better results than a standard distance and outperforms the state of the art in terms of accuracy on the Nisteacuter-Steweacutenius and Lola data sets. This paper also evaluates the impact of a large number of parameters, including the number of descriptors, the clustering method, the visual vocabulary size, and the distance measure. The optimal parameter choice is shown to be quite context-dependent. In particular, using a large number of descriptors is interesting only when using our dissimilarity measure. We have also evaluated two novel variants: multiple assignment and rank aggregation. They are shown to further improve accuracy at the cost of higher memory usage and lower efficiency.	algorithm;assignment (computer science);cluster analysis;condylion mediale;context-sensitive language;data descriptor;distance;estimated;experiment;image retrieval;image scaling;lola (computing);nonlinear dimensionality reduction;population parameter;stochastic matrix;test scaling;trionic;vii;vocabulary;statistical cluster	Hervé Jégou;Cordelia Schmid;Hedi Harzallah;Jakob J. Verbeek	2010	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2008.285	layout;computer vision;image analysis;image retrieval;computer science;machine learning;context-dependent memory;frequency;pattern recognition;mathematics	Vision	37.41832405598061	-56.515273406782285	142069
0d0888b934f2b9e4af9eb0ec1a7bff2d27ab0ea2	classification of fused images using radial basis function neural network for human face recognition	eigenvalues and eigenfunctions;radial basis function networks eigenvalues and eigenfunctions face recognition image fusion;recognizing unknown individuals fused images classification radial basis function neural network human face recognition visual thermal images discriminating information projected eigenspace object tracking classification beyond visible spectrum database benchmark;training;image fusion;testing;spectrum;classification;eigenspace projection;radial basis function networks;visualization;artificial neural networks;radial basis function networks humans face recognition lighting face detection computer science electronic mail fingerprint recognition access control surveillance;face recognition;thermal imaging;radial basis function neural network;object tracking;success rate;classification image pixel fusion eigenspace projection radial basis function neural network face recognition;pattern recognition;image pixel fusion;face;lighting	Here an efficient fusion technique for automatic face recognition has been presented. Fusion of visual and thermal images has been done to take the advantages of thermal images as well as visual images. By employing fusion a new image can be obtained, which provides the most detailed, reliable, and discriminating information. In this method fused images are generated using visual and thermal face images in the first step. In the second step, fused images are projected into eigenspace and finally classified using a radial basis function neural network. In the experiments Object Tracking and Classification Beyond Visible Spectrum (OTCBVS) database benchmark for thermal and visual face images have been used. Experimental results show that the proposed approach performs well in recognizing unknown individuals with a maximum success rate of 96%.	artificial neural network;benchmark (computing);experiment;facial recognition system;radial (radio);radial basis function	Mrinal Kanti Bhowmik;Debotosh Bhattacharjee;Mita Nasipuri;Dipak Kumar Basu;Mahantapas Kundu	2009	2009 World Congress on Nature & Biologically Inspired Computing (NaBIC)	10.1109/NABIC.2009.5393594	facial recognition system;face;spectrum;computer vision;visualization;biological classification;computer science;machine learning;video tracking;pattern recognition;lighting;software testing;image fusion;artificial neural network	Vision	35.70971816576057	-62.43699979075346	142075
49007af4b328735981dbec9fd6b35e4a9b2ed9b9	support vector machines for traffic signs recognition	artificial neural networks joints conferences;zernike polynomials automated highways feature extraction image classification image representation learning artificial intelligence road traffic search problems shape recognition simulated annealing support vector machines;traffic signs;zernike moments;fuzzy artmap;support vector machines;binary image;road traffic;search algorithm;automated highways;image classification;shape recognition;joints;simulated annealing;classification;artificial neural networks;recognition;simulated annealing search algorithm;simulated annealing search algorithm support vector machine traffic sign shape recognition system shape classification speed limit sign binary image zernike moment feature representation grid search algorithm;shape classification;image representation;feature extraction;zernike moment;grid search algorithm;svm;search problems;traffic sign shape recognition system;feature representation;support vector machine;learning artificial intelligence;speed limit sign;conferences;zernike polynomials	In many traffic sign recognition system, one of the main tasks is to classify the shapes of traffic sign. In this paper, we have developed a shape-based classification model by using support vector machines. We focused on recognizing seven categories of traffic sign shapes and five categories of speed limit signs. Two kinds of features, binary image and Zernike moments, were used for representing the data to the SVM for training and test. We compared and analyzed the performances of the SVM recognition model using different feature representations and different kernels and SVM types. Our experimental data sets consisted of 350 traffic sign shapes and 250 speed limit signs. Experimental results have shown excellent results, which have achieved 100% accuracy on sign shapes classification and 99% accuracy on speed limit signs classification. The performance of SVM model highly depends on the choice of model parameters. Two search algorithms, grid search and simulated annealing search have been implemented to improve the performances of our classification model. The SVM model were also shown to be more effective than Fuzzy ARTMAP model.	binary image;fuzzy concept;performance;search algorithm;simulated annealing;support vector machine;traffic sign recognition	Min Shi;Haifeng Wu;Hasan Fleyeh	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4634347	support vector machine;computer vision;computer science;machine learning;pattern recognition;mathematics;artificial neural network	Vision	32.0377699355199	-56.01877816764331	142352
1006cce5d95829053583b1ddfb42dea9cc7b9468	human action recognition using fusion of depth and inertial sensors		In this paper we present a human action recognition system that utilizes the fusion of depth and inertial sensor measurements. Robust depth and inertial signal features, that are subject-invariant, are used to train independent Neural Networks, and later decision level fusion is employed using a probabilistic framework in the form of Logarithmic Opinion Pool. The system is evaluated using UTD-Multimodal Human Action Dataset, and we achieve 95% accuracy in 8-fold cross-validation, which is not only higher than using each sensor separately, but is also better than the best accuracy obtained on the mentioned dataset by 3.5%.	sensor	Zain Fuad;Mustafa Unel	2018		10.1007/978-3-319-93000-8_42	computer science;artificial intelligence;pattern recognition;computer vision;artificial neural network;fusion;probabilistic logic;inertial frame of reference;inertial measurement unit;sensor fusion	Robotics	30.05419860548702	-55.22797856972054	142539
d3745104ad50a5e7c1c6d8bd5845b801296d1548	automatic selection of user samples for a non-collaborative face verification system		This paper describes the challenges that involve developing a software capable of capturing users’ faces on mobile devices in a non-collaborative environment. The goal is to generate a set of quality training samples of the user’s face for the construction of a model that can be used in a later phase of biometric identification. To this end, a supervised learning system is integrated to determine when a photo should be taken. This learning is supported by a varied input data set that contains information regarding the pose of the device, its manipulation and other environmental factors such as lighting. The software also has different ways of working with the objective of not wasting resources and be little invasive. Working modes are managed with an easy-to-maintain and scalable rules-based system. The experimental results show the robustness of the proposal.		Fernando E. Casado;Carlos V. Regueiro;Roberto Iglesias;Xose Manuel Pardo;Eric López	2017		10.1007/978-3-319-70833-1_45	computer vision;supervised learning;robustness (computer science);scalability;artificial intelligence;facial recognition system;android (operating system);software;computer science;machine learning;mobile device;biometrics	Vision	25.67434269106566	-58.7278594857378	142547
ef0a3259e4bd845b98f86b7be899556ded0b6e83	fast k nearest neighbors search algorithm based on wavelet transform	k nearest neighbors search;tecnologia electronica telecomunicaciones;search algorithm;wavelet transform;vector quantization;texture image classification;k nearest neighbor;tecnologias;grupo a;image retrieval	This letter proposes a fast k nearest neighbors search algorithm based on the wavelet transform. This technique exploits the important information of the approximation coefficients of the transform coefficient vector, from which we obtain two crucial inequalities that can be used to reject those vectors for which it is impossible to be k nearest neighbors. The computational complexity for searching for k nearest neighbors can be largely reduced. Experimental results on texture classification verify the effectiveness of our algorithm.	k-nearest neighbors algorithm;search algorithm;wavelet transform	Yu-Long Qiao;Zhe-Ming Lu;Sheng-He Sun	2006	IEICE Transactions	10.1093/ietfec/e89-a.8.2239	nearest-neighbor chain algorithm;best bin first;speech recognition;image retrieval;computer science;machine learning;pattern recognition;mathematics;nearest neighbor search;k-nearest neighbors algorithm;vector quantization;wavelet transform;search algorithm	Vision	36.76293811942579	-61.64310412727954	142859
6585c69f402b4deecb21e77db9e5537a79e3da6e	word retrieval from kannada document images using hog and morphological features		This paper presents a method to retrieve words from Kannada documents. It works on Histogram of Oriented Gradients (HOG) and Morphological filters. A large dataset of 50000 words is created using 250 document pages belongs to different categories. A preprocessed document image is segmented using simple morphological filters. The histogram channels are designed over four-sided cells (i.e. R-HOG) to compute gradients of a word image. In parallel, morphological erosion, opening, top and bottom hat transformations are applied on each word. The densities of the resultant images are estimated. Later on, HOG and morphological features are fused. Then, the cosine distance is used to measure the similarity between two words i.e., query and candidate word, based on it, the relevance of the word is estimated by generating distance ranks. Then correctly matched words are selected at threshold 98%. The experimental results confirm the efficiency of our proposed method in terms of the average precision rate 91.23%, and average recall rate 84.78% as well as average F-measure 89.47%.		Mallikarjun Hangarge;C. Veershetty;Rajmohan Pardeshi;Gururaj Mukarambi	2016		10.1007/978-981-10-4859-3_7	natural language processing;speech recognition;linguistics	Vision	35.85909799751139	-65.09028903289942	142899
fb8f7d172d47286101fc7f8b34a863162adc0e8e	histogram of oriented lines for palmprint recognition	image recognition;subspace lea;subspace learning biometric histogram of oriented gradients hog palm line palmprint recognition;hog variant;hol descriptor;journal;histogram of oriented gradients;feature extraction;histogram of oriented lines;palmprint recognition;histogram of oriented gradients histogram of oriented lines palmprint recognition subspace learning methods illumination variance translation variance rotation variance image recognition hol descriptor hog variant;learning artificial intelligence;palmprint recognition feature extraction learning artificial intelligence;illumination variance;rotation variance	Subspace learning methods are very sensitive to the illumination, translation, and rotation variances in image recognition. Thus, they have not obtained promising performance for palmprint recognition so far. In this paper, we propose a new descriptor of palmprint named histogram of oriented lines (HOL), which is a variant of histogram of oriented gradients (HOG). HOL is not very sensitive to changes of illumination, and has the robustness against small transformations because slight translations and rotations make small histogram value changes. Based on HOL, even some simple subspace learning methods can achieve high recognition rates.	computer vision;database;fingerprint;gradient;guinness world records;hol (proof assistant);histogram of oriented gradients;linear discriminant analysis;performance;principal component analysis	Wei Jia;Rong-Xiang Hu;Ying-Ke Lei;Yang Zhao;Jie Gui	2014	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2013.2258010	computer vision;speech recognition;feature extraction;histogram of oriented gradients;computer science;histogram matching;machine learning;pattern recognition	Vision	33.344717019792576	-58.76145140720137	142961
20f6d5a4a04e69b1eb860ca9dd86e414f94aa590	characteristic extraction of chinese signature identification based on b-spline function and wavelet transform	characteristic value;signature;spline wavelet;stroke;wavelet transform	This paper presents a hybrid method to extract stable signature characteristic. We first develop a group of 4th order B-spline wavelet based on the better properties of B-spline function. After applying the novel B-spline wavelet to each stroke of the signature through wavelet decomposition, we design a set of formulas to synthesize characteristic value of each stroke so as to obtain signature characteristic values which have many good performances such as rotation invariance, translation invariance and scale invariance. Furthermore, a proper classifier is developed which can be more effective than the traditional Euclidean distance. The simulation results show that this method has better stability and reliability. © 2011 Springer-Verlag Berlin Heidelberg.	b-spline;spline (mathematics);wavelet transform	Yongjian Zhao;Haining Jiang	2011		10.1007/978-3-642-23777-5_70	continuous wavelet transform;discrete wavelet transform	Crypto	33.90241973110147	-62.887061812914155	143055
2f1485994ef2c09a7bb2874eb8252be8fe710db1	dynamic image networks for action recognition	approximate rank pooling dynamic image networks action recognition video representation video analysis convolutional neural networks cnn ranking machine temporal evolution single rgb image;videos standards computer architecture neural networks feature extraction pattern recognition image recognition;video signal processing approximation theory image colour analysis image recognition image representation neural nets	We introduce the concept of dynamic image, a novel compact representation of videos useful for video analysis especially when convolutional neural networks (CNNs) are used. The dynamic image is based on the rank pooling concept and is obtained through the parameters of a ranking machine that encodes the temporal evolution of the frames of the video. Dynamic images are obtained by directly applying rank pooling on the raw image pixels of a video producing a single RGB image per video. This idea is simple but powerful as it enables the use of existing CNN models directly on video data with fine-tuning. We present an efficient and effective approximate rank pooling operator, speeding it up orders of magnitude compared to rank pooling. Our new approximate rank pooling CNN layer allows us to generalize dynamic images to dynamic feature maps and we demonstrate the power of our new representations on standard benchmarks in action recognition achieving state-of-the-art performance.	approximation algorithm;artificial neural network;autostereogram;convolutional neural network;deep learning;end-to-end principle;image processing;learning to rank;map;pixel;raw image format;video content analysis;visual inspection	Hakan Bilen;Basura Fernando;Efstratios Gavves;Andrea Vedaldi;Stephen Gould	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.331	computer vision;image processing;computer science;machine learning;pattern recognition	Vision	26.88235405982957	-52.33321548997242	143093
b161c0d0a68774e1db5c568e306d0dcaa66c8ff4	text line identification from a multilingual document	natural languages optical character recognition software feature extraction educational institutions digital images character recognition training data decision making data mining system testing;text analysis document image processing feature extraction learning artificial intelligence optical character recognition;learning;optical character recognition;text analysis;multilingual document image;natural languages;data mining;decision making text line identification multilingual document image optical character recognition ocr language identification feature extraction learning knowledge base;training data;optical character recognition software;accuracy;feature extraction;pixel;ocr;document image processing;language identification;system testing;text line identification;learning artificial intelligence;multi lingual document;classification accuracy;feature extraction document image processing multi lingual document language identification top profile bottom profile;digital images;character recognition;bottom profile;knowledge based systems;knowledge base;top profile	In India, a document may contain text lines in more than one language forms. For Optical Character Recognition (OCR) of such a multilingual document, it is necessary to identify different language forms of the input document, before feeding the documents to the OCRs of individual language. In this paper, a simple but efficient technique of language identification for Kannada, Hindi and English text lines from a printed document is presented. The proposed system is based on the characteristic features of top-profile and bottom-profile of individual text lines of the input document image. The feature extraction is achieved by finding the behavior of the characteristics of the top and bottom profiles of individual text lines. The system is trained to learn the behavior of the top and bottom profiles with a training data set of 800 text lines. Range of feature values of top and bottom profiles for all the three languages are obtained and stored in knowledge base for later use during decision-making. For a new text line, necessary features are extracted from the top and bottom profiles and the feature values obtained are compared with the stored knowledge base. A new text line is classified to the type of the language that falls within that range. The proposed system is tested on 600 text lines and an overall classification accuracy of96.6% is achieved.	algorithm;feature extraction;knowledge base;language identification;optical character recognition;printing;test set;text segmentation	P. A. Vijaya;M. C. Padma	2009	2009 International Conference on Digital Image Processing	10.1109/ICDIP.2009.51	natural language processing;speech recognition;computer science;document layout analysis;pattern recognition	NLP	33.070024041450615	-65.91536639988827	143370
9cef7b87788e2c95a1d4b433386b032a8ef51f03	on the combination of ridgelets descriptors for symbol recognition	classifier fusion;shape descriptor;ridgelets descriptors;multi resolution;graphics recognition;symbol recognition	In this paper we propose an original solution to combine the scales of multi-resolution shape descriptors. More precisely, a classifier fusion scheme is applied to a set of shape descriptors obtained from the ridgelets transform. The Ridgelets coefficients are grouped into different descriptors according to their resolution. Then a classifier is trained for each descriptor and a final classification is obtained using the classifier fusion scheme. We have applied this approach to symbol recognition using the GREC 2003 database. In this perspective, we increase the recognition rates of previous works on ridgelets-based descriptors.		Oriol Ramos Terrades;Ernest Valveny;Salvatore Tabbone	2007		10.1007/978-3-540-88188-9_5	computer vision;computer science;machine learning;pattern recognition	Vision	36.24154904748671	-59.41651162921348	143619
e4ce893980d4a13eda3cd49a60b1c1116634f628	face recognition using layered linear discriminant analysis and small subspace	databases;computational complexity layered linear discriminant analysis biometrics layered face recognition method fishers linear discriminant analysis;layered face recognition biometrics lda pca;small sample size;statistical analysis computational complexity face recognition;large dataset;layered linear discriminant analysis;biometrics;layered face recognition method;linear discriminate analysis;probes;accuracy;lda;face recognition;statistical analysis;computational complexity;principal component analysis;fishers linear discriminant analysis;face;face face recognition probes principal component analysis databases linear discriminant analysis accuracy;layered;linear discriminant analysis;pca;tk electrical engineering electronics nuclear engineering	Face recognition has great demands in human recognition and recently it becomes one of the most important research areas of biometrics. In this paper, we present a novel layered face recognition method based on Fisher’s linear discriminant analysis. The basic aim is to decrease FAR by reducing the face dataset to small size by applying layered linear discriminant analysis. Although, the computational complexity at the time of recognition is much higher than conventional PCA and LDA due to the weights computation for small subspace at the time of recognition, but on the other hand the layered LDA provides significant performance gain especially on similar face database. Layered LDA is insensitive to large dataset and also small sample size and it provides 93% accuracy on BANCA face database. Experimental and simulation results show that the proposed scheme has encouraging results for a practical face recognition system.	biometrics;computation;computational complexity theory;facial recognition system;linear discriminant analysis;simulation	Muhammad Imran Razzak;Muhammad Khurram Khan;Khaled S. Alghathbar;Rubiyah Yousaf	2010	2010 10th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2010.252	speech recognition;computer science;machine learning;pattern recognition;linear discriminant analysis;statistics;principal component analysis	Vision	33.97475006316015	-56.30248668327159	143759
6eff57e634838efa313e6711f298af4ad406c9f0	rare: a new bottom-up saliency model	image processing;image processing brightness feature extraction;brightness;feature extraction;feature extraction computational modeling mathematical model quantization visualization humans image color analysis;human fixations bottom up visual saliency model locally contrasted features globally rare features sequential bottom up features extraction low level features luminance chrominance medium level features image orientations qualitative comparison quantitative comparison rare algorithm;rarity based method visual attention bottom up saliency iterative otsu quantization	In this paper, a new bottom-up visual saliency model is proposed. Based on the idea that locally contrasted and globally rare features are salient, this model will be called “RARE” in the following sections. It uses a sequential bottom-up features extraction where first low-level features as luminance and chrominance are computed and from those results medium-level features as image orientations are extracted. A qualitative and a quantitative comparison are achieved on a 120 images dataset. The RARE algorithm powerfully predicts human fixations compared with most of the freely available saliency models.	algorithm;bottom-up parsing;bottom-up proteomics;high- and low-level;top-down and bottom-up design	Nicolas Riche;Matei Mancas;Bernard Gosselin;Thierry Dutoit	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6466941	computer vision;image processing;feature extraction;computer science;machine learning;pattern recognition;brightness	Vision	38.15962968255072	-54.016871368129905	143789
970719f5b2ef54624ceca44fa461690c3430f24e	a target-based color space for sea target detection	linear and quadratic transformation;color space;sea target detection;color segmentation;particle swarm optimization;fuzzy c means;supervised clustering particle swarmoptimization fuzzy c means color space color segmentation linear and quadratic transformation seatarget detection;supervised clustering	Sea target detection is a vital application for military and navigation purposes. A new supervised clustering method based on the combination of the PSO and FCM techniques is presented for the sea target detection problem. The color components of the target and non-target pixels in the RGB color space are used as features to train the classification algorithm. The new classifier is presented in the form of a new color space which we call the Target-based Color Space (TCS); in fact the RGB color space is converted to this new space through a 3×3 matrix. The Particle Swarm Optimization (PSO) algorithm is then used to search for the optimum weights of the conversion matrix which results in a more discriminating clustering space between the target and non-target pixels. In other words, solving the optimization problem, minimization of the objective function of the FCM clustering technique in linear and quadratic transform domain (with a NP-hard problem in quadratic conversion), is done using the PSO algorithm. The main objective of this work is to demonstrate the efficiency of using just color features, as well as color space conversion in the classification domain. Experimental results show the efficiency of new method in finding sea targets in color images.	algorithm;benchmark (computing);cluster analysis;color space;data mining;differentiator;fuzzy cognitive map;iteration;mathematical optimization;nonlinear system;optimization problem;particle swarm optimization;pattern recognition;phase-shift oscillator;pixel;russell c. eberhart;statistical classification;support vector machine;swarm intelligence;velocity (software development);xslt/muenchian grouping	Saeed Mirghasemi;Hadi Sadoghi Yazdi;Mojtaba Lotfizad	2011	Applied Intelligence	10.1007/s10489-011-0307-y	computer vision;mathematical optimization;color quantization;computer science;machine learning;pattern recognition;color space;particle swarm optimization	Vision	35.47274245022748	-60.094147456749646	143856
ff9aa34357a4fe30e0dd93c6905f223fb7b90cb9	domain-specific human-inspired binarized statistical image features for iris recognition		Binarized statistical image features (BSIF) have been successfully used for texture analysis in many computer vision tasks, including iris recognition and biometric presentation attack detection. One important point is that all applications of BSIF in iris recognition have used the original BSIF filters, which were trained on image patches extracted from natural images. This paper tests the question of whether domain-specific BSIF can give better performance than the default BSIF. The second important point is in the selection of image patches to use in training for BSIF. Can image patches derived from eye-tracking experiments, in which humans perform an iris recognition task, give better performance than random patches? Our results say that (1) domain-specific BSIF features can outperform the default BSIF features, and (2) selecting image patches in a task-specific manner guided by human performance can out-perform selecting random patches. These results are important because BSIF is often regarded as a generic texture tool that does not need any domain adaptation, and human-task-guided selection of patches for training has never (to our knowledge) been done. This paper follows the reproducible research requirements, and the new iris-domain-specific BSIF filters, the patches used in filter training, the database used in testing and the source codes of the designed iris recognition method are made available along with this paper to facilitate applications of this concept.		Adam Czajka;Daniel M Moreira;Kevin W. Bowyer;Patrick J. Flynn	2018	CoRR		artificial intelligence;pattern recognition;feature (computer vision);task analysis;feature extraction;source code;domain adaptation;iris recognition;histogram;computer science;biometrics	Vision	25.410084262697595	-57.57394689126212	143864
83cc5f0e334548100f172a314c8794706a9013c0	approach toward extraction of sparse texture features and their application in robust two-level bare-hand detection		A practical two-level robust hand detection system is developed using the proposed sparse texture features and color–texture features. Traditional dense features such as a histogram of oriented gradients, Gabor, segmentation-based fractal texture analysis, and histogram features are dense in general, with high time-complexity, limiting their use in practical hand detection systems. However, if only the prominent edge or texture parameters of an image are processed, the time complexity of the system can be significantly reduced, along with an increase in performance. Performance of any practical system is most affected by the presence of spurious objects in the background. Proposed approaches use four efficient filtering techniques to extract salient regions of an image retaining significant object-related information, followed by extraction of texture features, as mentioned above. Therefore, in the first stage, 10 sparse variants of these existing texture features are extracted and assessed using five classification models, namely Naive Bayes, Real AdaBoost, Gentle AdaBoost, Modest AdaBoost, and SVM. In the second phase of the system, a two-level hand detection model is developed using the proposed texture and color–texture features using a sliding window-based SVM classification model. Experimental analysis shows that proposed sparse-based features are not only time efficient but also shows an improvement of 23.4% in the practical two-level detection system over the existing motion and skin filtering-based hand detection system.	sparse matrix	Songhita Misra;Rabul Hussain Laskar	2018	J. Electronic Imaging	10.1117/1.JEI.27.5.051209	histogram of oriented gradients;support vector machine;time complexity;artificial intelligence;naive bayes classifier;adaboost;pattern recognition;computer vision;sliding window protocol;computer science;filter (signal processing);histogram	Vision	34.53672507969759	-60.10383304278903	143937
9ce949e1ad45b8f59ff1c72329b7468fd5f1d005	a probabilistic substructure-based approach for graph classification	graph theory;face retrieval;gabor jets;neural nets;neural network based quantization;statistical graph matching;multivariate waid wolfowitz test;gabor filters;graph matching;neural gas;gabor filter;face recognition;statistical analysis;space use;feature extraction;robustness face recognition quantization data mining feature extraction face detection gabor filters frequency testing laboratories;non parametric statistics;vector quantizer;neural gas vector quantizer;frequency domain;nonparametric statistical test;statistical analysis face recognition feature extraction gabor filters graph theory image retrieval neural nets;distributional based approach;gabor filtering;nonparametric statistical test robust face recognition face retrieval neural network based quantization gabor jets statistical graph matching distributional based approach feature extraction gabor filtering neural gas vector quantizer multivariate waid wolfowitz test;robust face recognition;neural network;image retrieval	Graph classification is an important data mining task that has attracted considerable attention recently. This paper presents a probabilistic substructure-based approach for classifying graph-based data. More specifically, we use a frequent subgraph mining algorithm to extract substructure based descriptors and apply the maximum entropy principle to build a classification model from the frequent subgraphs. We perform extensive experiments to compare the performance of the proposed approach against existing feature vector methods using AdaBoost and support vector machine.	adaboost;algorithm;data mining;experiment;feature vector;principle of maximum entropy;structure mining;support vector machine	H. D. K. Moonesinghe;Hamed Valizadegan;Samah Jamal Fodeh;Pang-Ning Tan	2007	19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)	10.1109/ICTAI.2007.159	neural gas;nonparametric statistics;computer vision;feature extraction;computer science;machine learning;pattern recognition;mathematics;gabor wavelet;frequency domain;artificial neural network;matching	ML	35.19274510997369	-56.12195182752399	143978
293e83ed1a0d44cf5608b7f0d97af44192ab2a58	multi-instance learning with relational information of instances	kernel;text analysis learning artificial intelligence statistical analysis support vector machines;kernel support vector machines text categorization statistics australia image recognition text recognition drugs image retrieval object recognition;support vector machines;image categorization multiinstance learning relational information image text categorization support vector machines text categorization;bismuth;training;text analysis;feature space;accuracy;statistical analysis;feature extraction;multi instance;support vector machine;learning artificial intelligence;context;text categorization	Multi-instance learning (MIL) has many applications, including image and text categorization. One of the most effective approaches to MIL is by using Support Vector Machines with multi-instance kernels. In this paper we propose a multi-instance kernel, called MIR-Kernel, that takes into account the relational information of instances when computing similarities between bags. The relational information of instances are derived from the statistics of the distances between instances in feature space. The aim of MIR-Kernel is to efficiently capture the context in which instances occur within bags, so that it is able to better compute the similarities between bags. Experimental results on image and text categorization demonstrate the effectiveness of the proposed method compared to other methods.	categorization;document classification;feature vector;kernel (operating system);linux;support vector machine;text simplification;thresholding (image processing)	Gunawan Herman;Getian Ye;Yang Wang;Jie Xu;Bang Zhang	2009	2009 Workshop on Applications of Computer Vision (WACV)	10.1109/WACV.2009.5403078	support vector machine;text mining;computer science;machine learning;pattern recognition;data mining	ML	34.252963763698	-54.6535882055864	144020
dd6d59e853e9b88dbe61ed6afa3e0cefa22a8128	object recognition with näive bayes-nn via prototype generation		Näıve Bayes nearest neighbors (NBNN) is a variant of the classic KNN classifier that has proved to be very effective for object recognition and image classification tasks. Under NBNN an unseen image is classified by looking at the distance between the sets of visual descriptors of test and training images. Although NBNN is a very competitive pattern classification approach, it presents a major drawback: it requires of large storage and computational resources. NBNN’s requirements are even larger than those of the standard KNN because sets of raw descriptors must be stored and compared, therefore, efficiency improvements for NBNN are necessary. Prototype generation (PG) methods have proved to be helpful for reducing the storage and computational requirements of standard KNN. PG methods learn a reduced subset of prototypical instances to be used by KNN for classification. In this contribution we study the suitability of PG methods for enhancing the capabilities of NBNN. Throughout an extensive comparative study we show that PG methods can reduce dramatically the number of descriptors required by NBNN without significantly affecting its discriminative performance. In fact, we show that PG methods can improve the classification performance of NBNN by using much less visual descriptors. We compare the performance of NBNN to other state-of-the-art object recognition approaches and show the combination of PG and NBNN outperforms alternative techniques.	computation;computational resource;computer vision;k-nearest neighbors algorithm;naive bayes classifier;outline of object recognition;prototype;requirement;statistical classification;visual descriptor	Hugo Jair Escalante;Mauricio Sotomayor;Manuel Montes-y-Gómez;Adrián Pastor López-Monroy	2014		10.1007/978-3-319-07491-7_17	machine learning;artificial intelligence;naive bayes classifier;computer science;discriminative model;visual descriptors;cognitive neuroscience of visual object recognition;contextual image classification	Vision	26.708654933573797	-57.50620119634429	144370
fc4ce9b2edcf88618b0a7d11737a6afa8e232d71	poster abstract: predicting heating energy demand by computer vision		In many countries such as Austria the heating energy demand (HED) is an essential parameter of the energy certification of houses. In this paper, we present an approach in which the HED category for a single family house is—for the first time—determined from a standard photograph directly by means of computer vision and machine learning.	computer vision;machine learning	Miroslav Despotovic;Muntaha Sakeena;David Koch;Mario Döller;Matthias Zeppelzauer	2017	Computer Science - Research and Development	10.1007/s00450-017-0363-6	computer science;convolutional neural network;simulation;certification;computer vision;artificial intelligence	Vision	25.018458037106893	-59.53015004410295	144451
563e1c1821401eef5a6473524583951d3a0f641b	detecting small signs from large images		In the past decade, Convolutional Neural Networks (CNNs) have been demonstrated successful for object detections. However, the size of network input is limited by the amount of memory available on GPUs. Moreover, performance degrades when detecting small objects. To alleviate the memory usage and improve the performance of detecting small traffic signs, we proposed an approach for detecting small traffic signs from large images under real world conditions. In particular, large images are broken into small patches as input to a Small-Object-Sensitive-CNN (SOS-CNN) modified from a Single Shot Multibox Detector (SSD) framework with a VGG-16 network as the base network to produce patch-level object detection results. Scale invariance is achieved by applying the SOS-CNN on an image pyramid. Then, image-level object detection is obtained by projecting all the patch-level detection results to the image at the original scale. Experimental results on a real-world conditioned traffic sign dataset have demonstrated the effectiveness of the proposed method in terms of detection accuracy and recall, especially for those with small sizes.	convolutional neural network;graphics processing unit;minimum bounding box;neural networks;object detection;pyramid (image processing);sampling (signal processing);sensor;solid-state drive	Zibo Meng;Xiaochuan Fan;Xin Chen;Min Chen;Yan Tong	2017	2017 IEEE International Conference on Information Reuse and Integration (IRI)	10.1109/IRI.2017.57	memory management;machine learning;pattern recognition;pyramid (image processing);object-class detection;convolutional neural network;data mining;computer science;object detection;feature extraction;artificial intelligence;image resolution;computer vision;detector	Vision	29.587406158242597	-53.744665978732826	144461
4ad72250994883c01cd14b155f99b5aadb155ca7	a blanket binarization method for character string extraction		In this paper, a binarization method based on fractal dimension for character string extraction is proposed. In character extraction from a scene image, a major problem is how to deal with much different type of characters in a complex background. The proposed method can obtain multiple threshold values which are correspond to each character regions by detecting the stable intervals of fractal dimension FD. The stable interval is a relatively low and flat valley of the FD which indicates the binarized image has the stable connected regions, and therefore fine character regions have been appeared. The character regions may contain some noise and has conflictions between the regions derived with another threshold values. We call these character region as a ”Candidate Character Region Images”(CCRI), and will be processed by noise-reduction consists of two steps. After that, CCRI are integrated into one binarized image as output image through the contention resolution process. We show the performance of the proposed method by comparing Niblack’s method as a local method and Otsu’s method as a global method on the dataset provided at ICDAR 2003.	binary image;fractal dimension;image noise;international conference on document analysis and recognition;otsu's method;sensor;string (computer science)	Hiromi Yoshida;Naoki Tanaka	2011			arithmetic;artificial intelligence	Graphics	38.47102386661726	-65.81107468865055	144462
7705d375a68c28ff633b96839da3e8cddd3036ed	tiny hand gesture recognition without localization via a deep convolutional network		Visual hand-gesture recognition is being increasingly desired for human-computer interaction interfaces. In many applications, hands only occupy about 10% of the image, whereas the most of it contains background, human face, and human body. Spatial localization of the hands in such scenarios could be a challenging task and ground truth bounding boxes need to be provided for training, which is usually not accessible. However, the location of the hand is not a requirement when the criteria is just the recognition of a gesture to command a consumer electronics device, such as mobiles phones and TVs. In this paper, a deep convolutional neural network is proposed to directly classify hand gestures in images without any segmentation or detection stage that could discard the irrelevant not-hand areas. The designed hand-gesture recognition network can classify seven sorts of hand gestures in a user-independent manner and on real time, achieving an accuracy of 97.1% in the dataset with simple backgrounds and 85.3% in the dataset with complex backgrounds.	artificial neural network;convolutional neural network;gesture recognition;ground truth;human–computer interaction;real-time computing;relevance;requirement	Peijun Bao;Ana I. Maqueda;Carlos R. del-Blanco;Narciso N. García	2017	IEEE Transactions on Consumer Electronics	10.1109/TCE.2017.014971	computer vision;computer science;gesture recognition;convolutional neural network;artificial intelligence;ground truth;three-dimensional face recognition;feature extraction;artificial neural network;gesture	Vision	27.400274155826107	-59.12656730347389	144467
5b3d1e835848aa9a10aa782a7a56e46db2d0da50	robust single-label classification of facial attributes		Texture analysis is extensively used for extraction of facial features. In this paper, we investigate extraction of facial features related to attributes of gender, age, race and expression. We propose novel approaches for texture analysis to improve single-label classification of these facial attributes. The proposed methods are derived by applying Local Binary Pattern based approaches on polar raster sampled face images. We perform experiments on three state-of-the-art face databases, namely, Face95, FERET and CK+. Experimental results show that the proposed approach improves the performance of Local Binary Pattern and its variants for single-label classification of facial attributes.	database;experiment;feret (facial recognition technology);robustness (computer science)	Ahmed Abdulateef Mohammed;Atul Sajjanhar	2017	2017 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2017.8026324	pattern recognition;support vector machine;computer vision;artificial intelligence;robustness (computer science);computer science;local binary patterns;raster graphics;feret	Vision	35.4407958903358	-59.21031085175491	144586
1c404de2c85a1b58cf7048ca645de6d392358779	getting the look: clothing recognition and segmentation for automatic product suggestions in everyday photos	large scale retrieval;clothing segmentation;clothing suggestion;clothing detection;product recommendation	We present a scalable approach to automatically suggest relevant clothing products, given a single image without metadata. We formulate the problem as cross-scenario retrieval: the query is a real-world image, while the products from online shopping catalogs are usually presented in a clean environment. We divide our approach into two main stages: a) Starting from articulated pose estimation, we segment the person area and cluster promising image regions in order to detect the clothing classes present in the query image. b) We use image retrieval techniques to retrieve visually similar products from each of the detected classes. We achieve clothing detection performance comparable to the state-of-the-art on a very recent annotated dataset, while being more than 50 times faster. Finally, we present a large scale clothing suggestion scenario, where the product database contains over one million products.	autostereogram;emoticon;image retrieval;online shopping;scalability;world file	Yannis Kalantidis;Lyndon Kennedy;Li-Jia Li	2013		10.1145/2461466.2461485	data mining;multimedia	Vision	32.1868012860626	-52.327723055300886	144794
baabad0fbd0a980f8ad6d4b870ab20dd17b9b1cc	an effective color space for face recognition	cmu multipie face recognition rqcr dcs color space zrg lc c2 discriminant luminance component discriminant chrominance components weighted color space normalization technique wcsn frgc;color space;image color analysis face recognition color face databases correlation training;face recognition;image colour analysis face recognition;face recognition color space	The three color components specifying a color can be defined in various ways leading to significantly different classification abilities. Several effective color spaces including RQCr, DCS and ZRG have been proposed to achieve better face recognition performance. However, their performance is not consistent on different databases. What's more, the framework of effective color spaces has not been thoroughly studied yet. In this paper, we propose an effective color space LC\C2 based on a framework of effective color spaces. LC\C2 consists of one discriminant luminance component L and two discriminant chrominance components C\C2. To find the discriminant luminance component, 4 luminance components from existing effective color models are compared. After that, the weighted color space normalization technique (WCSN) is applied on the DCS color space to generate two complementary and discriminative chrominance components. Experiments conducted on three databases (FRGC, AR and CMU Multi-PIE) show that the proposed color space LC\C2 achieves the best face recognition performance consistently.	color space;database;discriminant;facial recognition system;graphics device interface	Ze Lu;Xudong Jiang;Alex ChiChung Kot	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472031	facial recognition system;color histogram;computer vision;color quantization;speech recognition;color normalization;computer science;pattern recognition;mathematics;color balance;color space	Vision	35.40445848285937	-58.165002427325135	144870
d9ef2299b770acb04d817b909c5728115e10216b	visual word pairs for automatic image annotation	weak geometrical information;histograms;co occurence;visual vocabulary;support vector machine svm image annotation visual vocabulary bag of visual words co occurence visual word pairs;support vector machine svm;automatic image annotation;training;standard benchmark dataset;vocabulary;text analysis;text analysis image representation;indexing terms;image annotation;data mining;visual word pairs;standard benchmark dataset visual word pairs automatic image annotation image representation weak geometrical information;visualization;shape;vocabulary support vector machines layout object detection image representation image databases visual databases labeling histograms performance evaluation;image representation;automatic annotation;clustering algorithms;bag of visual words	The bag-of-visual-words is a popular representation for images that has proven to be quite effective for automatic annotation. In this paper, we extend this representation in order to include weak geometrical information by using visual word pairs. We show on a standard benchmark dataset that this new image representation improves significantly the performances of an automatic annotation system.	automatic image annotation;bag-of-words model in computer vision;benchmark (computing);performance;visual word	Nicolas Hervé;Nozha Boujemaa	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202526	computer vision;visualization;index term;image retrieval;shape;computer science;machine learning;pattern recognition;histogram;bag-of-words model in computer vision;cluster analysis;automatic image annotation;information retrieval	Vision	36.445746975628694	-56.44831062625326	145115
c59aa61c90d5a27b688efe9825ed1885c69d5794	iterative copy-move forgery detection based on a new interest point detector	detectors;filtering;feature extraction detectors forgery computational efficiency filtering robustness transforms;duplicated region localization digital image forensics copy move detection interest point detection;forgery;feature extraction;transforms;robustness;public benchmark databases iterative copy move forgery detection interest point detector duplicated regions localize keypoint based methods block based methods filtering algorithm iterative improvement strategy keypoint density;computational efficiency;iterative methods image forensics image matching	In this paper, we propose a novel copy-move forgery detection scheme that can accurately localize duplicated regions with a reasonable computational cost. To this end, a new interest point detector is proposed utilizing the advantages of both block-based and traditional keypoint-based methods. The detected keypoints adaptively cover the entire image, even low contrast regions, based on a uniqueness metric. Moreover, a new filtering algorithm is employed, which can effectively prune the falsely matched regions. Considering the new interest point detector, an iterative improvement strategy is proposed. The whole procedure is iterated along with adjusting the keypoints density based on the achieved information. The experimental results demonstrate that the proposed scheme outperforms the state-of-the-art methods using two public benchmark databases.	algorithm;algorithmic efficiency;authentication;benchmark (computing);computation;database;image retrieval;interest point detection;iteration;iterative method;scale space;self-similarity;sensor	Mohsen Zandi;Ahmad Mahmoudi Aznaveh;Alireza Talebpour	2016	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2016.2585118	filter;computer vision;detector;feature extraction;computer science;theoretical computer science;data mining;mathematics;robustness	Vision	36.78453590739163	-61.33491261622021	145167
1997d4e2ebb511d7dc7ce96d746bc5a375b3fcec	preliminary face recognition grand challenge results	high resolution;image resolution;face recognition vendor test;image resolution face recognition grand challenge face recognition vendor test;face recognition;face recognition grand challenge;3d scanning;image resolution face recognition;data consistency;face recognition image recognition image resolution partitioning algorithms lighting control error analysis bioinformatics automatic testing algorithm design and analysis high performance computing	The goal of the face recognition grand challenge (FRGC) is to improve the performance of face recognition algorithms by an order of magnitude over the best results in face recognition vendor test (FRVT) 2002. The FRGC is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with a data corpus of 50,000 images. The data consists of 3D scans and high resolution still imagery taken under controlled and uncontrolled conditions. This paper presents preliminary results of the FRGC for all six experiments. The preliminary results indicate that significant progress has been made towards achieving the stated goals	3d film;algorithm;darpa grand challenge;experiment;face recognition grand challenge;face recognition vendor test;facial recognition system;grand challenges;image resolution;uncontrolled format string	P. Jonathon Phillips;Patrick J. Flynn;W. Todd Scruggs;Kevin W. Bowyer;William J. Worek	2006	7th International Conference on Automatic Face and Gesture Recognition (FGR06)	10.1109/FGR.2006.87	computer vision;face detection;speech recognition;computer science;three-dimensional face recognition;face recognition grand challenge;3d single-object recognition;computer graphics (images)	Vision	35.201315824156836	-64.66193901719149	145202
5070263c6a3a571cf5ed33d559e513dc130b30d4	a new system for automatic recognition of italian sign language		This work proposes a preliminary study of an automatic recognition system for the Italian Sign Language (Lingua Italiana dei Segni - LIS). Several other attempts have been made in the literature, but they are typically oriented to international languages. The system is composed of a feature extraction stage, and a sign recognition stage. Each sign is represeted by a single Hidden Markov Model, with parameters estimated through the resubstitution method. Then, starting from a set of features related to the position and the shape of head and hands, the Sequential Forward Selection technique has been applied to obtain feature vectors with the minimum dimension and the best recognition performance. Experiments have been performed using the cross-validation method on the Italian Sign Language Database A3LIS-147, maintaining the orthogonality between training and test sets. The obtained recognition accuracy averaged across all signers is 47.24%, which represents an encouraging result and demonstrates the effectiveness of the idea.		Marco Fagiani;Emanuele Principi;Stefano Squartini;Francesco Piazza	2012		10.1007/978-3-642-35467-0_8	computer science;artificial intelligence;pattern recognition;machine learning;hidden markov model;feature extraction;lingua franca;italian sign language;feature vector;orthogonality	NLP	31.213434351527543	-60.19252273708091	145233
f43dda7f896cc225185b414b92d191d927b3c31d	semantic classification of scenes and places with omnidirectional vision	transforms decision trees feature extraction image classification image recognition image representation image segmentation natural scenes support vector machines;image recognition;image segmentation;support vector machines;image classification;semantics image segmentation vegetation histograms kernel accuracy feature extraction;image representation;feature extraction;transforms;scale invariant feature transforms omnidirectional images semantic scene classification image region semantic place classification place recognition category floor vertical planar surfaces isolated objects heterogeneous visual feature extraction superpixel level randomized decision trees global image representation local densely extracted shape appearance representations dense sift support vector machine place categories omnidirectional vision hog histogram of oriented gradients;decision trees;natural scenes	This paper presents a novel approach for semantic classification of scenes and places with omnidirectional images. The objective of scene classification is to segment and classify different regions in the image whereas place recognition assigns a single category to the entire image. In scene classification image regions are classified into categories floor, vertical planar surfaces and isolated objects e.g. furniture. The semantic segmentation extracts multiple heterogeneous visual features at the superpixel-level that are labeled by randomized decision trees. The place recognition relies on a global image representation (GIST) and two local densely extracted shape and appearance representations(HOG, dense SIFT). A support vector machine predicts place categories such as room, corridor, doorway and open space from these visual features.	autostereogram;decision tree;map;random forest;randomized algorithm;support vector machine;vii	Luis-Felipe Posada;Krishna Kumar Narayanan;Frank Hoffmann;Torsten Bertram	2013	2013 European Conference on Mobile Robots	10.1109/ECMR.2013.6698829	image texture;support vector machine;computer vision;contextual image classification;feature detection;feature extraction;computer science;machine learning;decision tree;pattern recognition;mathematics;bag-of-words model in computer vision;image segmentation;automatic image annotation;feature	Vision	35.335043971974144	-53.93194325678546	145272
b472f91390781611d4e197564b0016d9643a5518	facial expression recognition using geometric and appearance features	texture differences;facial expression recognition;support vector machines;geometry;face recognition;internet;svm;geometric features;appearance features;gesture recognition;textures	A novel method using hybrid geometric and appearance features of the difference between the neutral and fully expressive facial expression images is proposed for facial expression recognition in this paper. The difference tends to emphasize the facial parts that are changed from the neutral to expressive face and eliminate in that way the identity of the facial image. The hybrid features include facial feature point displacements and local texture differences between the normalized neutral and expressive facial expression images. The proposed method achieved an average accuracy of 95% in the extended Cohn-Kanade database with a Support Vector Machine (SVM) classification method.	support vector machine	Jingying Chen;Dan Chen;Yujiao Gong;Meng Yu;Kun Zhang;Lizhe Wang	2012		10.1145/2382336.2382345	psychology;computer vision;speech recognition;pattern recognition;three-dimensional face recognition;face hallucination	Vision	32.468823066883616	-59.132606948689116	145286
ead7afd5ee175af8119fd63fd62c29a87c4afd21	openfabmap: an open source toolbox for appearance-based loop closure detection	thesaurus;research resource;researcher;organization;openfabmap application;search engine;science and technology;paper;integrated search;probability;idea;openfabmap an open source toolbox for appearance based loop closure detection;jst;technical term;institute;japan science and technology agency;interest points;sparkingarticle;expanding;database;codebook training;fab map algorithm;magazine;patent;technical trend;interest point feature tuning;chemical substance;information content;journal;professional;linking;public domain software;training data;j global;search;visualization;research and development;appearance based loop closure detection;robot vision;bobliography;ｊｇｌｏｂａｌ;openfabmap application open source toolbox appearance based loop closure detection high information content visual images robotic applications fast appearance based mapping seminal robotic mapping experiments open source implementation fab map algorithm source code codebook training interest point feature tuning quick algorithm customisation;comprehensive search;material;feature extraction;funding;facility;r d;robotic applications;simultaneous localization and mapping;jglobal;open source implementation;jdream;gene;ｊ ｇｌｏｂａｌ;robot vision image retrieval pose estimation public domain software;open source toolbox;imagination;source code;quick algorithm customisation;research project;related search;visual images;ｊｓｔ;article;seminal robotic mapping experiments;high information content;feature extraction visualization probability training data educational institutions simultaneous localization and mapping;open source;linkcenter;pose estimation;image retrieval;fast appearance based mapping	Appearance-based loop closure techniques, which leverage the high information content of visual images and can be used independently of pose, are now widely used in robotic applications. The current state-of-the-art in the field is Fast Appearance-Based Mapping (FAB-MAP) having been demonstrated in several seminal robotic mapping experiments. In this paper, we describe OpenFABMAP, a fully open source implementation of the original FAB-MAP algorithm. Beyond the benefits of full user access to the source code, OpenFABMAP provides a number of configurable options including rapid codebook training and interest point feature tuning. We demonstrate the performance of OpenFABMAP on a number of published datasets and demonstrate the advantages of quick algorithm customisation. We present results from OpenFABMAP's application in a highly varied range of robotics research scenarios.	algorithm;codebook;experiment;open-source software;personalization;robot;robotic mapping;robotics;self-information	Arren J. Glover;William P. Maddern;Michael Warren;Stephanie Reid;Michael Milford;Gordon Wyeth	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6224843	computer vision;simulation;image retrieval;computer science;engineering;artificial intelligence;data mining;statistics;research	Robotics	33.044005329099015	-54.148950115561384	145331
662904e722fec3dfb7122018a68c324d7d7a426d	overlapped text segmentation using markov random field and aggregation	document analysis;mrf;binary image;markov random field;machine learning;distance metric;text segmentation	Separating machine printed text and handwriting from overlapping text is a challenging problem in the document analysis field and no reliable algorithms have been developed thus far. In this paper, we propose a novel approach for separating handwriting from binary image of overlapped text. Instead of using fixed size training patches, we describe an aggregation method which uses shape context features to extract training samples automatically. We use a Markov Random Field (MRF) to model the overlapped text. The neighbor system is inherited from a coarsening procedure and the prior and likelihood of the MRF is learned based on a distance metric. Experimental results show that the proposed method can achieve 87.97% recall for handwriting and 91.44% recall for machine printed text.	algorithm;binary image;markov chain;markov random field;printing;shape context;text segmentation	Xujun Peng;Srirangaraj Setlur;Venu Govindaraju;Ramachandrula Sitaram	2010		10.1145/1815330.1815347	text segmentation;materials recovery facility;speech recognition;binary image;metric;computer science;machine learning;pattern recognition	AI	36.59799075871695	-64.77111694655517	145400
7dbd0e26e1a3ec461b506fdf87a2b2d386580e2e	hierarchical image saliency detection on extended cssd	image color analysis object detection indexes estimation computational modeling complexity theory merging;complexity theory;ecssd hierarchical image saliency detection extended cssd natural image final saliency value single scale information saliency map extended complex scene saliency dataset;region scale saliency detection;saliency detection;natural scenes image recognition image texture;indexes;computational modeling;region scale;estimation;image color analysis;merging;object detection	Complex structures commonly exist in natural images. When an image contains small-scale high-contrast patterns either in the background or foreground, saliency detection could be adversely affected, resulting erroneous and non-uniform saliency assignment. The issue forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. Different from varying patch sizes or downsizing images, we measure region-based scales. The final saliency values are inferred optimally combining all the saliency cues in different scales using hierarchical inference. Through our inference model, single-scale information is selected to obtain a saliency map. Our method improves detection quality on many images that cannot be handled well traditionally. We also construct an extended Complex Scene Saliency Dataset (ECSSD) to include complex but general natural images.	inference;layer (electronics);numerous;silo (dataset)	Jianping Shi;Qiong Yan;Li Xu;Jiaya Jia	2016	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2015.2465960	database index;computer vision;estimation;computer science;kadir–brady saliency detector;machine learning;pattern recognition;mathematics;computational model;statistics	Vision	38.121485338875864	-54.173300267548306	145404
5233651e7c6436ce63d24b8d74a03a34925d09b6	text detection and localization in complex scene images using constrained adaboost algorithm	databases;detectors;histograms;video sequence;complexity theory;image segmentation;colored noise;neural networks;neural nets;video signal processing;intelligent robots;heterogeneous feature set;training;text analysis;video sequences;cascade of boosted ensembles;layout;data mining;natural scene text detection text localization constrained adaboost algorithm gray scale scene image heterogeneous feature set neural network based localization text locating database video sequence;gray scale scene image;text locating database;feature complexity text detection and localization adaboost cascade of boosted ensembles feature selection feature combination;natural scene;text detection and localization;computational complexity;feature extraction;spatial databases;layout detectors image segmentation colored noise histograms intelligent robots neural networks robustness spatial databases video sequences;adaboost;feature complexity;neural network based localization;video signal processing image sequences learning artificial intelligence natural scenes neural nets object detection text analysis;text localization;robustness;text detection;feature selection;learning artificial intelligence;feature combination;constrained adaboost algorithm;natural scenes;object detection;neural network;image sequences	We have proposed a complete system for text detection and localization in gray scale scene images. A boosting framework integrating feature and weak classifier selection based on computational complexity is proposed to construct efficient text detectors. The proposed scheme uses a small set of heterogeneous features which are spatially combined to build a large set of features. A neural network based localizer learns necessary rules for localization. The evaluation is done on the challenging ICDAR 2003 robust reading and text locating database. The results are encouraging and our system can localize text of various font sizes and styles in complex background.	adaboost;artificial neural network;computational complexity theory;connected component (graph theory);database;feature selection;grayscale;international conference on document analysis and recognition;internationalization and localization;selection algorithm;sensor	Shehzad Muhammad Hanif;Lionel Prevost	2009	2009 10th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2009.172	adaboost;layout;computer vision;detector;colors of noise;feature extraction;computer science;machine learning;pattern recognition;histogram;image segmentation;computational complexity theory;artificial neural network;robustness	Vision	33.589546324716636	-55.06644523259328	145464
7e2544979b29cf3bf78adc86bceec25db4f57035	an infrared small target detecting algorithm based on human visual system	image edge detection object detection visual systems yttrium robustness standards indexes;standards;indexes;yttrium;image edge detection;infrared ir small target human visual system hvs improved difference of gabors idogb;robustness;visual systems;object detection gabor filters infrared detectors;idogb filter infrared small target detection multiscale detection ability raw ir images robust human visual system properties robust hvs properties ir small target detection field difference of gaussians filters difference of gabor filters false alarm rate;object detection	Infrared (IR) small target detection with high detection rate, low false alarm rate, and multiscale detection ability is a challenging task since raw IR images usually have low contrast and complex background. In recent years, robust human visual system (HVS) properties have been introduced into the IR small target detection field. However, existing algorithms based on HVS, such as difference of Gaussians (DoG) filters, are sensitive to not only real small targets but also background edges, which results in a high false alarm rate. In this letter, the difference of Gabor (DoGb) filters is proposed and improved (IDoGb), which is an extension of DoG but is sensitive to orientations and can better suppress the complex background edges, then achieves a lower false alarm rate. In addition, multiscale detection can be also achieved. Experimental results show that the IDoGb filter produces less false alarms at the same detection rate, while consuming only about 0.1 s for a single frame.		Jinhui Han;Yong Ma;Jun Huang;Xiaoguang Mei;Jiayi Ma	2016	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2016.2519144	database index;computer vision;speech recognition;computer science;yttrium;optics;robustness	Vision	37.37984991677644	-62.74319197240849	145507
6945b2a3187775b05cc6e6c1914e2e5481708971	scene text detection with a ssd and encoder-decoder network based method		In this work, we propose a simple yet powerful method that yields effective text detection in natural scenes. We present a Text Localization Neural Network, which detects text in scene images with one forward propagation and a standard non-maximum suppression subsequently. In order to eliminate few scene background mistaken by Text Localization Neural Network, we propose a Text Verification Model based on the encoder-decoder network. Thus, precision of text detection can be further improved by recognizing text in our candidate text regions. We have evaluated the proposed method for text detection on our own constructed horizontal text detection dataset. Compared with previous approaches, our method achieves a highest recall rate of 0.784 and competitive precision rate in text detection.		Cong Luo;Xue Gao	2018		10.1007/978-3-030-12177-8_3	encoder;artificial neural network;pattern recognition;computer science;artificial intelligence	Vision	28.94992878570408	-54.80224055967288	145574
5e316e09f9e9cb8a357044394026d58004dcd1a5	no-reference blur strength estimation based on spectral analysis of blurred images	cumulative distribution function;fourier transform;svm;blur estimation;no reference			Hanhoon Park	2015	IEICE Transactions	10.1587/transinf.2014EDL8175	fourier transform;support vector machine;computer vision;speech recognition;cumulative distribution function;machine learning;pattern recognition;statistics	Vision	37.16207145049795	-62.02907676509281	145648
4ca234a358668dbbe108e26108745bdb89cd980a	extraction of shell texture feature of coscinodiscus for classification based on wavelet and pca	wavelet analysis;principal component analysis wavelet transforms wavelet coefficients feature extraction wavelet analysis oceans electronic mail image analysis image converters artificial intelligence;shift invariant feature shell texture feature extraction coscinodiscus classification pca principal component analysis wavelet transform;oceans;electronic mail;image converters;image classification;texture features;classification;image texture;coscinodiscus;manganese;wavelet transforms;shift invariance;wavelet transform;principal component analysis pca;feature extraction;principal component analysis;classification algorithms;normalization;wavelet transforms feature extraction image texture pattern classification principal component analysis;pattern classification;shift invariant feature;shell texture feature extraction;artificial intelligence;image analysis;image classification normalization shift invariance wavelet transform principal component analysis pca;pca;shift invariant;wavelet coefficients;extraction method	Based on wavelet and principal component analysis(PCA), an effective shell texture feature of the coscinodiscus extraction method for classification is proposed in this paper.The feature extraction process involves a normalization of the given image with different sizes followed by shift invariant wavelet transform. The shift invariant feature is computed for subband of wavelet coefficients by PCA. The rate of recognition is calculated in the end. The experiments have proved the method is effective.	coefficient;experiment;feature extraction;principal component analysis;stationary wavelet transform	Li-Na Song;Guangrong Ji;Jing Chen	2009	2009 International Joint Conference on Artificial Intelligence	10.1109/JCAI.2009.75	computer vision;image analysis;speech recognition;computer science;machine learning;pattern recognition;stationary wavelet transform;wavelet transform;principal component analysis	Robotics	36.335186556878796	-61.88201636975881	145667
448aac9a3b9c490949602b091391bea16f013818	a hybrid approach to pedestrian clothing color attribute extraction	pedestrians feature extraction image classification image colour analysis;histograms;image color analysis clothing histograms color feature extraction joints semantics;color;processing pipeline;semantics;image classification;joints;pedestrians;low level features;mid level descriptors;image color analysis;image colour analysis;feature extraction;clothing;pedestrian clothing color attribute extraction;color classification;pedestrian dataset;pedestrian dataset pedestrian clothing color attribute extraction low level features mid level descriptors processing pipeline color classification	Clothing attributes, of which color plays an important role, are receiving more and more interests in machine vision researches and applications because of their uses and effectiveness in tasks like pedestrian analysis. However, color description is a challenging problem due to complex environments such as illumination variations. Most prior works describe color attributes using only low-level features or mid-level descriptors, which results in a marked drop of the discriminative power or photometric invariance. In this paper we introduce a new efficient joint representation that aims to overcome the shortcomings of using low-level features or mid-level descriptors alone and present a novel hybrid approach to pedestrian clothing color attribute extraction. As a necessary preprocessing step, a novel processing pipeline is also proposed. We evaluate our approach on the task of color classification on both the public dataset VIPeR and our own newly-built pedestrian dataset. Experimental results have demonstrated the effectiveness of our approach and have shown its great potential for further researches and applications.	high- and low-level;machine vision;open world;preprocessor	Mu Gao;Yuning Du;Haizhou Ai;Shihong Lao	2015	2015 14th IAPR International Conference on Machine Vision Applications (MVA)	10.1109/MVA.2015.7153138	computer vision;contextual image classification;feature extraction;computer science;clothing;machine learning;pattern recognition;histogram;semantics;statistics;computer graphics (images)	Vision	33.62468296835009	-53.03801117523728	145717
3ae3feb7d90da1010909183f9582da08bbabb273	face recognition techniques: a survey	illumination;security;face recognition;authentication	Nowadays research has expanded to extracting auxiliary information from various biometric techniques like fingerprints, face, iris, palm and voice . This information contains some major features like gender, age, beard, mustache, scars, height, hair, skin color, glasses, weight, facial marks and tattoos. All this information contributes strongly to identification of human. The major challenges that come across face recognition are to find age&gender of the person. This paper contributes a survey of various face recognition techniques for finding the age and gender. The existing techniques are discussed based on their performances. This paper also provides future directions for further research.	biometrics;facial recognition system;fingerprint;performance;scar (physics)	Raunak Dave;Ankit Vyas;Shubham Mojidra;Nikita P. Desai	2018	CoRR		psychology;computer vision;face detection;artificial intelligence;three-dimensional face recognition;face recognition grand challenge;social psychology	Vision	29.210401436894077	-61.247334003253904	145753
a294eeee5a28a7d8c50026426dd335eb7133ea12	logo recognition based on the dempster-shafer fusion of multiple classifiers	book chapter	The performance of different feature extraction and shape description methods in trademark image recognition systems have been studied by several researchers. However, the potential improvement in classification through feature fusion by ensemble-based methods has remained unattended. In this work, we evaluate the performance of an ensemble of three classifiers, each trained on different feature sets. Three promising shape description techniques, including Zernike moments, generic Fourier descriptors, and shape signature are used to extract informative features from logo images, and each set of features is fed into an individual classifier. In order to reduce recognition error, a powerful combination strategy based on the Dempster-Shafer theory is utilized to fuse the three classifiers trained on different sources of information. This combination strategy can effectively make use of diversity of base learners generated with different set of features. The recognition results of the individual classifiers are compared with those obtained from fusing the classifiers’ output, showing significant performance improvements of the proposed methodology.	computer vision;ensemble learning;experiment;feature extraction;information;logo;performance	Mohammad Ali Bagheri;Qigang Gao;Sergio Escalera	2013		10.1007/978-3-642-38457-8_1	computer vision;speech recognition;computer science;pattern recognition	AI	31.88768674621787	-62.87894580652179	145936
51ed4c92cab9336a2ac41fa8e0293c2f5f9bf3b6	a survey of face detection, extraction and recognition	neural networks;face recognition;pattern recognition;face detection;eigenface;elastic matching	The goal of this paper is to present a critical survey of existing literatures on human face recognition over the last 4–5 years. Interest and research activities in face recognition have increased significantly over the past few years, especially after the American airliner tragedy on September 11 in 2001. While this growth largely is driven by growing application demands, such as static matching of controlled photographs as in mug shots matching, credit card verification to surveillance video images, identification for law enforcement and authentication for banking and security system access, advances in signal analysis techniques, such as wavelets and neural networks, are also important catalysts. As the number of proposed techniques increases, survey and evaluation becomes important.	artificial neural network;authentication;cave creek disaster;closed-circuit television;face detection;facial recognition system;signal processing;wavelet	Yongzhong Lu;Jingli Zhou;Shengsheng Yu	2003	Computers and Artificial Intelligence		facial recognition system;computer vision;face detection;speech recognition;computer science;three-dimensional face recognition;face recognition grand challenge;eigenface;computer security;artificial neural network	Vision	30.881517329746003	-62.75353051123033	145941
001b628def69e33a66ff378a83f6ad38b298a9cf	a multi-stage fuzzy classifier for off-line handwritten chinese character recognition	character recognition pattern recognition support vector machines support vector machine classification educational institutions computer science fuzzy systems handwriting recognition statistical analysis signal processing;natural languages handwritten character recognition pattern classification fuzzy set theory;natural languages;fuzzy set theory;handwritten chinese character recognition;large scale;system integration;pattern classification;pattern recognition;fuzzy coarse classifier multistage fuzzy classifier off line handwritten character recognition chinese character recognition pattern recognition system integration;handwritten character recognition;fuzzy classifier	The decision boundaries in pattern recognition of large scale (PRLS) are usually complex and sometimes it is difficult to get ideal results with only a single classifier. In the paper, based on system integration idea, the method of fuzzy coarse classifying is given and a multi-stage fuzzy classifier is built. The recognition experiments on one level class of 3755 Chinese characters show that this method can largely improve the recognition accuracy. So it is an efficient method for solving PRLS.	algorithm;experiment;online and offline;optical character recognition;pattern recognition;system integration	Shaozhen Ye;Jiarui Chen	2005	2005 IEEE International Conference on Granular Computing	10.1109/GRC.2005.1547379	speech recognition;feature;intelligent character recognition;computer science;intelligent word recognition;machine learning;pattern recognition;fuzzy set;natural language;sketch recognition;system integration	Robotics	32.22080278246693	-66.05949475419277	145976
b3a92808155c209c044b724ec99f1ad648f7b4f4	colorpca: color principal feature extraction technique for color image reconstruction and recognition	eigenvalues and eigenfunctions;image recognition;principal component analysis eigenvalues and eigenfunctions feature extraction image colour analysis image recognition image reconstruction matrix algebra;eigen decomposition image feature extraction color image recognition color image reconstruction;matrix algebra;image colour analysis;feature extraction;image reconstruction;principal component analysis;eigenvectors colorpca color principal feature extraction technique color image reconstruction color image recognition color image space local topological information color image scatter matrix eigen decomposition;image color analysis color feature extraction image reconstruction principal component analysis face vectors	This paper introduces an effective mechanism to extract informative principal features from the color images and proposes a color principal feature extraction technique referred to as ColorPCA. ColorPCA performs in the color image space, extracting the principal features directly from the color images. As a result, the color and local topological information of pixels at each level of the color images can be effectively preserved. In extracting the most representative features, a color image scatter matrix is constructed and its eigenvectors are employed for color principal feature extraction. ColorPCA has only one parameter (i.e., the reduced dimension) to estimate and the projection axes can be effectively obtained using eigen-decomposition. Extensive color image reconstruction and recognition over the benchmark problems verified the effectiveness of the presented ColorPCA. Results show that ColorPCA can effectively reconstruct the color images. Image recognition also demonstrates that ColorPCA can deliver promising results compared with other state-of-the-art 1D and 2D principal feature extraction algorithms.	algorithm;benchmark (computing);color image;computer vision;eigen (c++ library);feature extraction;information;iterative reconstruction;pixel	Zhaoxing Zhang;Ming-Bo Zhao;Bing Jin Li;Peng Tang	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6706981	iterative reconstruction;demosaicing;color histogram;computer vision;feature detection;color quantization;hsl and hsv;color normalization;color image;image gradient;binary image;feature extraction;computer science;machine learning;pattern recognition;mathematics;color balance;feature;principal component analysis	Vision	36.87031325882124	-58.413596047547585	145979
740291d8b28ea61a638d8be98019040404c363db	age classification system with ica based local facial features	statistical independence;real time;projection method;face alignment;classification system;mutual information;facial features;face detection;classification accuracy	This paper proposes an automatic age classification system that distinguishes kid faces from adult faces in real time. The system consists of three parts: face detection, face alignment and normalization, and age classification. We use standard face detection and face alignment method to generate face samples by automatically locating, cropping and aligning faces from images. We use ICA to extract statistically independent basis images which contain local facial components. Therefore, an image can be represented as a linear combination of those basis images. Then we choose a subset of basis images based on mutual information between them and class labels. Finally, we perform classification by SVM. Our experiment results show that our method provides better classification accuracy than conventional 1D or 2D projecting method.	coefficient;database normalization;face detection;feature selection;independent computing architecture;linear discriminant analysis;mutual information;prototype;statistical classification	Hang Qi;Liqing Zhang	2009		10.1007/978-3-642-01510-6_86	independence;computer vision;face detection;object-class detection;computer science;machine learning;pattern recognition;projection method;mutual information;face hallucination;statistics	Vision	32.69963332728372	-59.67874361041999	146004
8a308c45726ded961cdbed5eb30c6de75c70d31f	selecting salient features from facial components for face recognition		A robust facial recognition system aims at achieving an optimum accuracy when matching and comparing faces. The system meets an accepted degree of precision when it selects distinctive and salient features from the feature space. This work proposes an approach to select salient features from facial components for identification and verification, disregard of the face configuration. The proposed method employs two local feature descriptors, Scale Invariant Feature Transform (SIFT) and Speed-Up Robust Features (SURF). The descriptors primarily rely on the gradient computation of the facial components to extracts local features from the forehead, eyes, nose, cheeks, mouth and chin. The study evaluates the proposed technique from two face datasets, SCface and CMU-PIE and achieves an excellent performance. The results corroborate that facial components contain rich features and choosing only the prominent features from the feature space can improve the accuracy of facial recognition.	facial recognition system	Andile M. Gumede;Serestina Viriri;Mandlenkosi Victor Gwetu	2017		10.1007/978-3-319-92753-4_6	computer vision;salient;feature vector;artificial intelligence;pattern recognition;scale-invariant feature transform;facial recognition system;chin;computer science;forehead	Vision	33.61381627169105	-58.317736526036676	146025
0b252d0226ce7dcbef4a40f8d378a6c4674a0fe6	text detection in natural images based on character classification	the and ridge image;image understanding;character classification;natural images;text detection;the and valley image	Text information in images is very important for image understanding. In this paper, a text location method based on character classification is proposed. The and-valley image (AVI) and the and-ridge image (ARI) are first extracted from the input image. Then character components are detected from the AVI and ARI respectively, and then these components are sent to a character classifier. Finally,text region can be generated by merging all the recognized components. This approach is robust to font style, font size, font color and the background complexity. It is demonstrated in the experiments that our method is efficient.		Yunxue Shao;Chunheng Wang;Baihua Xiao;Yonghui Zhang;Linbo Zhang;Long Ma	2010		10.1007/978-3-642-15696-0_68	computer vision;speech recognition;computer science;pattern recognition	Vision	36.207046366984386	-64.84214465778844	146459
5d14e3c41cd7931c7db2acb18206db2f34fa4009	improved face and head detection based on traditional middle eastern clothing		This paper is concerned with the detection of individuals in images who wear traditional Middle Eastern clothing. Traditional headwear for men includes a scarf known as the shemagh that often occludes the face or causes significant shadows. State-of-the-art face-detection systems do not perform well for these cases. To address this problem, we have developed a novel approach that detects a distinctive part of traditional headwear known as the igal. This is a band or cord, typically black, that rests on the shemagh to hold it in place. Our approach starts by applying multiscale SVM classification with a HoG descriptor to perform tentative detection. The proposed detections are then refined using a bag of visual words categorization system. Experimental results have shown significantly better performance for our technique over several face-detection systems. Our technique yielded an F1 score of 80% with a low false-positive rate, showing an improvement of 15% over the best face detector.		Abdulaziz Alorf;A. Lynn Abbott	2017		10.1007/978-3-319-59876-5_43	computer vision;support vector machine;bag-of-words model in computer vision;pattern recognition;artificial intelligence;computer science;face detection;categorization;clothing;f1 score	Vision	32.22185631739234	-56.72478990836572	146632
0d82ef3a3926a610a941d94d3b94c704e4276b8b	dorsal hand vein biometric using independent component analysis (ica)	false reject rate;biometrics access control;dimension reduction;image matching;imposter score dorsal hand vein pattern dimension reduction techniques ica genuine score;biomedical imaging;veins;euclidean distance;independent component analysis;false rejection rate dorsal hand vein biometric independent component analysis dorsal hand vein pattern biometric security system vein representation pixel value matching time dimension reduction reduction technique false acceptance rate;dimension reduction techniques;independent component analysis biometrics access control image matching image representation;vectors;image representation;feature extraction;veins feature extraction biomedical imaging euclidean distance vectors security conferences;secure system;imposter score;dorsal hand vein pattern;security;genuine score;ica;false accept rate;conferences	Lately, dorsal hand vein pattern is gaining popularity in biometric security system due to its uniqueness and stability. Though dorsal vein patterns are not complex, this does not reflect in its extraction and representation. Various existing methods consider vein pattern as straight lines or use some of its features like ending points and bifurcation points for its representation. However, this type of vein representation is not the ideal solution since important features may be lost. Consequently, using all the pixel values representing the vein pattern is a better alternative. But the processing and matching time is at stake when using all the pixel values of the considered image. This problem can be solved by using dimension reduction techniques. In this work, the vein patterns are represented using Independent Component Analysis (ICA). This representation and reduction technique is carefully explored by providing details of the vein matrices. The experiments are carried out on 100 individuals, and the efficiency of the system is tested by the computation of the false acceptance rate and the false rejection rate.	bifurcation theory;biometrics;computation;dimensionality reduction;experiment;independent computing architecture;independent component analysis;pixel;rejection sampling	Naushad Mamode Khan;N. M. Khan	2011	2011 International Conference for Internet Technology and Secured Transactions		computer vision;speech recognition;pattern recognition;mathematics	Vision	34.4917084101659	-60.79880243764426	146640
634184051dc2b720f788d60cd8d4ebcdf5ff6753	correlation-based feature analysis and multi-modality fusion framework for multimedia semantic retrieval	multi modality;feature correlation;nuswide lite data set correlation based feature analysis multimodality fusion framework multimedia semantic retrieval cfa mmf framework multimedia semantic concept retrieval cfa method feature space hidden coherent feature groups hcfg maximum spanning tree algorithm maxst algorithm correlation matrix feature pair correlations graph cut procedure intra group correlation inter group correlation;maximum spanning tree;fusion;information retrieval;training;semantics;matrix algebra;correlation methods;trees mathematics;fusion multimedia semantic retrieval feature correlation maximum spanning tree multi modality;trees mathematics correlation methods information retrieval matrix algebra multimedia computing semantic web;multimedia computing;visualization;feature extraction;multimedia semantic retrieval;multimedia communication;semantic web;correlation feature extraction semantics multimedia communication training algorithm design and analysis visualization;correlation;algorithm design and analysis	In this paper, we propose a Correlation based Feature Analysis (CFA) and Multi-Modality Fusion (CFA-MMF) framework for multimedia semantic concept retrieval. The CFA method is able to reduce the feature space and capture the correlation between features, separating the feature set into different feature groups, called Hidden Coherent Feature Groups (HCFGs), based on Maximum Spanning Tree (MaxST) algorithm. A correlation matrix is built upon feature pair correlations, and then a MaxST is constructed based on the correlation matrix. By performing a graph cut procedure on the MaxST, a set of feature groups are obtained, where the intra-group correlation is maximized and the inter-group correlation is minimized. Finally, one classifier is trained for each of the feature groups, and the generated scores from different classifiers are fused for the final retrieval. The proposed framework is effective because it reduces the dimensionality of the feature space. The experimental results on the NUSWIDE-Lite data set demonstrate the effectiveness of the proposed CFA-MMF framework.	adobe flash lite;algorithm;coherent;cut (graph theory);dimensionality reduction;feature model;feature vector;graph cuts in computer vision;incremental funding methodology;spanning tree	Hsin-Yu Ha;Yimin Yang;Fausto Fleites;Shu-Ching Chen	2013	2013 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2013.6607639	algorithm design;visualization;fusion;feature extraction;computer science;machine learning;kanade–lucas–tomasi feature tracker;semantic web;pattern recognition;semantics;k-nearest neighbors algorithm;correlation;feature;information retrieval;feature model;dimensionality reduction	Vision	35.496758160916414	-54.48543372806013	146809
2f8df9fb4d51ac5f5a81dc19ea713b94bee20c18	multi-layer local energy patterns for texture representation and classification		Motivated by the recent success of deep networks in providing effective and abstract image representations, in this paper, a multi-layer architecture called the multi-layer local energy patterns (ML-LEP) is proposed for texture representation and classification. The proposed approach follows a multi-layer convolutional neural network paradigm and is built upon the single-layer local energy pattern (LEP) approach, a statistical histogram-based method for texture representation. An important aspect of the proposed multi-layer method compared to other deep convolutional architectures is bypassing the computationally expensive learning stage using fixed filters. As such, the proposed training-free network circumvents the need for large data for learning system parameters. An extensive investigation is carried out to determine the merits of different nonlinear operators in the proposed architecture. For this purpose, different nonlinearities including an energy-based nonlinearity, the absolute operator as well as the rectifier functions are extensively investigated and compared against each other. Extensive experiments conducted on three challenging databases of KTH-TIPS, KTH-TIPS2-a and the UIUC indicate that the extension of the LEP method to the multi-layer LEP is effective and leads to better performance. Moreover, the proposed ML-LEP approach is compared to several other well-known descriptors in the field, achieving the best reported performance on the KTH-TIPS and the KTH-TIPS2-a databases despite being training-free.	analysis of algorithms;artificial neural network;convolutional neural network;database;experiment;layer (electronics);nonlinear system;programming paradigm;rectifier	Ahmad Amirolad;Shervin Rahimzadeh Arashloo;Mehdi Chehel Amirani	2016	The Visual Computer	10.1007/s00371-016-1220-5	computer vision;artificial intelligence;machine learning;data mining;mathematics;geometry;algorithm;statistics	Vision	24.70988168533675	-52.77784638877238	146939
260b5ff17188530c2cde0adcf67567241fd8d505	on matching interest regions using local descriptors - can an information theoretic approach help?		This paper shows that the common task of interest region matching using local descriptors can be improved using a new similarity measure. The similarity measure is motivated by the information theoretic image alignment that maximize mutual information between images. A property of the mutual information metric is that it does not only depend on how similar the signals are but also how complex they are. We present how similar logic can be applied to the standard SIFT descriptor. The results show improvement at almost no additional computational costs.	approximation algorithm;computation;information theory;local shared object;mutual information;scale-invariant feature transform;sensor;similarity measure;top-down and bottom-up design	Zoran Zivkovic;Ben J. A. Kröse	2005		10.5244/C.19.4	pattern recognition	ML	36.59150578137587	-56.259245164257976	147096
048f5a79dae733e2e3efcaaaf6c8365a297cf904	ant colony optimization inspired saliency detection using compressed video information	video databases ant colony optimization video information compression visual saliency detection algorithm spatiotemporal information spatiotemporal feature extraction heuristic matrix spatial temporal saliency map adaptive fusion method;image coding;ant colony optimization;feature extraction visualization ant colony optimization image coding attenuation media;attenuation;adaptive fusion aco compressed domain saliency spatiotemporal features;media;visualization;feature extraction;video coding ant colony optimisation data compression feature extraction graph theory image fusion image sequences matrix algebra spatiotemporal phenomena	A novel visual saliency detection algorithm using ant colony optimization and spatiotemporal information in compressed videos is proposed in this paper. Firstly, a graph is constructed for each frame in the video by dividing it into blocks and taking the block as nodes. We extract spatial and temporal features of each node directly from the compressed bitstreams to form the heuristic matrixes. Each heuristic matrix is used to steer the ants and the ants deposit pheromone on the graph. Then the pheromone is updated through attenuation and evaporation thus forming a spatial/temporal saliency map. Finally, an adaptive fusion method is used to merge the spatial and temporal saliency maps together. The proposed method has been extensively tested on several video databases with sequences in various scenes and experiment results show that it outperforms various state-of-the-art models in both quantitative evaluation scores and intuitive visual effects.	ant colony optimization algorithms;data compression;database;evaporation;greedy algorithm;heuristic;map;mathematical optimization;positive feedback;visual effects	Cuiwei Li;Qin Tu;Jun Xu;Ran Gao;Qiang Wang;Yongyu Chang	2015	2015 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2015.7457814	attenuation;computer vision;ant colony optimization algorithms;media;visualization;telecommunications;feature extraction;computer science;machine learning;pattern recognition	AI	38.712210067654254	-52.88966931488828	147178
52c4d41230708facca47970b3ad38eb02ad129d3	deep cross-layer activation features for visual recognition	neural networks;frequency domain analysis;semantics;feature extraction visualization semantics image analysis frequency domain analysis correlation neural networks;visualization;feature extraction;image analysis;frequency domain analysis visual recognition deep learning convolutional neural networks;correlation	Convolutional Neural Networks (CNNs), which have nowadays dominated image analysis tasks, constitute feed-forward methods that model increasingly complex data structures and patterns along the subsequent hidden layers of the network. However, the common practice of using the activation features from the last network layer inevitably leads to a visual recognition bottleneck. This is due to the fact that discriminative features for different objects of varying complexity do not need to be extracted from the same layer. To this end, a novel frequency domain analysis of the feature maps of the same as well as of different network layers is proposed. In this way, the proposed method exploits more efficiently the knowledge that is stored in the actual CNN and facilitates in identifying the most discriminative features for every individual object type. Experimental results in a large-scale real-world Closed-Circuit Television (CCTV) surveillance and the PASCAL VOC 2012 datasets demonstrate the efficiency of the proposed approach.	closed-circuit television;convolutional neural network;data structure;domain analysis;image analysis;map;object type (object-oriented programming);pascal	Georgios Th. Papadopoulos;Elpida Machairidou;Petros Daras	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532492	computer vision;image analysis;visualization;feature extraction;computer science;machine learning;pattern recognition;semantics;deep learning;correlation;frequency domain	Vision	26.435948379125154	-52.467536584462664	147246
6c5682f8ea2aa384878f91a561689a544dd0fec1	gestalt rule feature points	detectors;visualization feature extraction shape computer vision image edge detection computational modeling detectors;image representation computer vision feature extraction image matching;local feature detector cross domain image matching gestalt rules graph based ranking;computer vision;visualization;computational modeling;shape;image edge detection;feature extraction;cross domain image matching gestalt rule feature points video data online repository image data online repository visual variations visual scene photograph image editing tools computer vision algorithms image recognition reliable visual feature detection visual correspondences visual perception	As the large online repositories of image and video data has emerged and continued to grow in number, the visual variations in such repositories has also increased dramatically. For example, the visual scene of a photograph can be changed into different colors by image editing tools or depicted by multiple representations, such as a painting and a hand-drawn sketch. The large visual variations tend to cause ambiguities for the existing computer vision algorithms to recognize the visual analogies of these images and often limit the potential of related applications. In this paper, therefore, we propose a new approach for detecting reliable visual features from images, with a particular focus on improving the repeatability of the local features in those images containing the same semantic contents (e.g., a landmark) but in different visual styles (e.g., a photo and a painting). We proposed a novel method for establishing visual correspondences between images based on the Gestalt theory, a psychological study of how human visions organize the visual perception. Experiments demonstrated the outperformance of our approach over the state-of-the-art local features in various computer vision tasks, such as cross domain image matching and retrieval.	algorithm;color;computational problem;experiment;feature detection (computer vision);feature detection (web development);gaussian blur;gestalt psychology;graph cuts in computer vision;image editing;image registration;image retrieval;information;maximally stable extremal regions;real-time clock;real-time computing;repeatability;scale-invariant feature transform;scott continuity;sensor;shading;simulated annealing	I-Chao Shen;Wen-Huang Cheng	2015	IEEE Transactions on Multimedia	10.1109/TMM.2015.2405350	computer vision;detector;feature detection;visual word;image analysis;visualization;edge detection;image processing;feature extraction;shape;computer science;machine learning;pattern recognition;bag-of-words model in computer vision;human visual system model;computational model;automatic image annotation;feature	Vision	37.915473900810646	-54.2555067167075	147471
583c00f75c519be40aba0fb791806a72fd84f72a	violence detection in movies	blood detection video content analysis violent scene detection;image motion analysis;image segmentation;support vector machines;video signal processing;image classification;violent content violent scene detection bloody frame detection semantic complete video scene structure digital movie input video segmentation feature extraction support vector machine scene classification motion information face information;face recognition;feature extraction;video content analysis;motion pictures blood support vector machines feature extraction visualization face training;support vector machine;video signal processing entertainment face recognition feature extraction image classification image motion analysis image segmentation image sequences support vector machines;blood detection;entertainment;violent scene detection;image sequences	As violence in movies has harmful influence on children, in this paper, we propose an algorithm to detect violent scene in movies. Under our definition of violence, the task of violent scene detection is decomposed into action scene detection and bloody frame detection. While previous approaches addressed on shot level of video structure only, our approach works on more semantic-complete scene structure of video. The input video (digital movie) is first segmented into several scenes. Based on the filmmaking characteristics of action scene, some features of the scene are extracted to feed into the support vector machine for classification. Finally, the face, blood and motion information are integrated to determine whether the action scene has violent content. Experimental results show that the proposed approach works reasonably well in detecting most of the violent scenes. Compared with related work, our approach is computationally simple yet effective.	algorithm;best, worst and average case;collision detection;database;effective method;high- and low-level;sensor;support vector machine;undecidable problem	Liang-Hua Chen;Hsi-Wen Hsu;Li-Yun Wang;Chih-Wen Su	2011	2011 Eighth International Conference Computer Graphics, Imaging and Visualization	10.1109/CGIV.2011.14	computer vision;scene statistics;computer science;pattern recognition;multimedia	Vision	34.02123582616936	-52.933000200847744	147551
edf709d321b086a9ddc7e2e232cba8d0c421783d	spreading activation layers, visual saccades, and invariant representations for neural pattern recognition systems	neural computation;invariant recognition;machine vision;pattern recognition;spreading activation	This paper shows how a simple spreading activation network in the form of two-dimensional (2D) diffusion followed by local maximum detection can quickly perform a large number of early vision tasks, for example, feature extraction, feature clustering, feature-centroid determination, and boundary gap completion, all on multiple scales. The results of the spreading activation process can be used to facilitate 2D object learning and recognition from silhouettes by generating representations from bottom-up fixation cues which are invariant to translation, orientation, and scale. In addition, the proposed network suggests a possible approach to longrange apparent motion correspondence and multiscale object decomposition. The theory is described and implementation examples are presented.	pattern recognition;spreading activation	Michael Seibert;Allen M. Waxman	1989	Neural Networks	10.1016/0893-6080(89)90012-9	computer vision;feature;machine vision;computer science;machine learning;mathematics;spreading activation;3d single-object recognition;models of neural computation	Vision	37.7754668946587	-54.93176355783549	147622
6c52e974c3921170033430608185e4554345bdba	latent fingerprints segmentation based on rearranged fourier subbands	histograms;image segmentation;band pass filters;sorting;image segmentation feature extraction fingerprint identification fourier analysis frequency domain analysis;fingerprint recognition noise image segmentation fourier transforms sorting band pass filters histograms;fingerprint recognition;automatic segmentation algorithm latent fingerprints segmentation rearranged fourier subband latent fingerprint segmentation algorithm spatial frequency domain analysis overlapped block based fourier coefficient frequency subband orientation subband rfs latent fingerprint spectra ground truth comparison feature extraction latent matching nist sd27 latent database;fourier transforms;noise	In this work, we present a latent fingerprint segmentation algorithm based on spatial-frequency domain analysis. The algorithm arranges the overlapped block-based Fourier coefficients into groups of frequency and orientation subbands, called Rearranged Fourier Subband (RFS). The RFS reveals latent fingerprint spectra in only a limited number of subbands. The algorithm then boosts, sorts, and extracts, from complex background and noise, the latent fingerprint spectra in the RFS subbands. Several experiments are evaluated based on ground truth comparison, feature extraction, and latent matching on the NIST SD27 latent database. Our experimental results show that the proposed algorithm achieves better accuracy compared to those of the published automatic segmentation algorithms.	acoustic fingerprint;algorithm;coefficient;domain analysis;experiment;feature extraction;ground truth;remote file sharing	Phumpat Ruangsakul;Vutipong Areekul;Krisada Phromsuthirak;Arucha Rungchokanun	2015	2015 International Conference on Biometrics (ICB)	10.1109/ICB.2015.7139063	fourier transform;computer vision;speech recognition;computer science;sorting;noise;pattern recognition;histogram;mathematics;band-pass filter;image segmentation;scale-space segmentation;fingerprint recognition	Vision	37.08087216756138	-61.81698167940277	147721
28c24f16e20c83c747f2aca8232f2cb6614905f5	the role of face parts in gender recognition	combination of classifiers;classification rules	This paper evaluates the discriminant capabilities of face parts in gender recognition. Given the image of a face, a number of subimages containing the eyes, nose, mouth, chin, right eye, internal face (eyes, nose, mouth, chin), external face (hair, ears, contour) and the full face are extracted and represented as appearance-based data vectors. A greater number of face parts from two databases of face images (instead of only one) were considered with respect to previous related works, along with several classification rules. Experiments proved that single face parts offer enough information to allow discrimination between genders with recognition rates that can reach 86%, while classifiers based on the joint contribution of internal parts can achieve rates above 90%. The best result using the full face was similar to those reported in general papers of gender recognition (>95%). A high degree of correlation was found among classifiers as regards their capacity to measure the relevance of face parts, but results were strongly dependent on the composition of the database. Finally, an evaluation of the complementarity between discriminant information from pairs of face parts reveals a high potential to define effective combinations of classifiers.	complementarity theory;contour line;database;discriminant;ensemble learning;ensembles of classifiers;experiment;pattern recognition;relevance;statistical classification	Yasmina Andreu;Ramón Alberto Mollineda	2008		10.1007/978-3-540-69812-8_94	computer vision;speech recognition;computer science;pattern recognition;mathematics	Vision	31.76248073355068	-59.96548995915496	147741
340e50008f0dd8c76b8b4896ddce60e79d8ebfaf	a novel approach to video copy detection using audio fingerprints and pca	information sources;feature vector	Abstract   In Content-Based Copy detection(CBCD)literature, numerous state-of-the-art techniques are primarily focusing on visual content of video. Exploiting audio fingerprints for CBCD problem is necessary, because of following reasons: audio content constitutes an indispensable information source;transformations on audio content is limited compared to visual content. In this paper, a novel CBCD approach using audio features and PCA is proposed, which includes two stages:first, multiple feature vectors are computed by utilizing MFCC and four spectral descriptors;second, features are further processed using PCA, to provide compact feature description. The results of experiments tested on TRECVID-2007 dataset, demonstrate the e_ciency of proposed method against various transformations.	fingerprint;principal component analysis;video copy detection	R. Roopalakshmi;G. Ram Mohana Reddy	2011		10.1016/j.procs.2011.07.021	computer vision;speech recognition;feature vector;computer science;machine learning;multimedia	Vision	33.97782712649548	-59.675546350039575	147779
cf865bcfe0f918bfdcf70689b4394a23f11c012d	a new ear recognition approach for personal identification	barycenter of the triangle;longest path;reference point;canny edge detector;ear segmentation;pca;2d wavelet	Personal identification based on the ear structure is a new biometrics. In this paper, a new ear recognition method is proposed. For the purpose of segmentation, we apply the Canny edge detector to the ear image. Then the longest path in the edge image is extracted and selected as the outer boundary of the ear. By selecting the top, bottom, and left point of the detected boundary, we form a triangle with the selected points as its vertices. Further we calculate the barycenter of the triangle and select it as a reference point in all images. Then the ear region is extracted from the entire image using a predefined window centered at the reference point. For the recognition approach, we improve a previous method based on PCA by using 2D wavelet in three different directions and extracting three different coefficient matrices. Experimental results show the effectiveness of our proposed method.		Sepehr Attarchi;Masoud S. Nosrati;Karim Faez	2008		10.1007/978-3-540-87442-3_45	computer vision;speech recognition;longest path problem;computer science;pattern recognition;mathematics;canny edge detector;principal component analysis	Vision	35.80008828518999	-61.707178209927136	147795
9c7418b4dd1e7a31fe42c7368e6187ca4dccb511	rotation–covariant texture learning using steerable riesz wavelets	wavelet analysis;biological patents;biomedical journals;rotation covariance;support vector machines;text mining;europe pubmed central;texture classification;computer vision applications rotation covariant texture learning steerable riesz wavelets local scale organizations kernel support vector machines texture signatures optimal class wise discriminatory properties obtained signature visualization visual relevance learned concepts class wise signatures outex_tc_00010 outex_tc_00012 contrib_tc_00000 image orientation;citation search;digital signatures;serveur institutionnel;citation networks;image texture;computer vision;data visualisation;archive institutionnelle;wavelet analysis texture classification feature learning steerability rotation covariance illumination invariance;research articles;support vector machines training kernel wavelet transforms educational institutions organizations;abstracts;support vector machines computer vision data visualisation digital signatures image texture learning artificial intelligence;open access;life sciences;clinical guidelines;feature learning;archive ouverte unige;steerability;full text;learning artificial intelligence;cybertheses;illumination invariance;rest apis;institutional repository;orcids;europe pmc;biomedical research;bioinformatics;literature search	We propose a texture learning approach that exploits local organizations of scales and directions. First, linear combinations of Riesz wavelets are learned using kernel support vector machines. The resulting texture signatures are modeling optimal class-wise discriminatory properties. The visualization of the obtained signatures allows verifying the visual relevance of the learned concepts. Second, the local orientations of the signatures are optimized to maximize their responses, which is carried out analytically and can still be expressed as a linear combination of the initial steerable Riesz templates. The global process is iteratively repeated to obtain final rotation-covariant texture signatures. Rapid convergence of class-wise signatures is observed, which demonstrates that the instances are projected into a feature space that leverages the local organizations of scales and directions. Experimental evaluation reveals average classification accuracies in the range of 97% to 98% for the Outex_TC_00010, the Outex_TC_00012, and the Contrib_TC_00000 suites for even orders of the Riesz transform, and suggests high robustness to changes in images orientation and illumination. The proposed framework requires no arbitrary choices of scales and directions and is expected to perform well in a large range of computer vision applications.	antivirus software;clinical use template;computer vision;feature vector;imagery;mental orientation;organization administrative structures;projections and predictions;relevance;support vector machine;type signature;verification and validation;verifying specimen;wavelet	Adrien Depeursinge;Antonio Foncubierta-Rodríguez;Dimitri Van De Ville;Henning Müller	2014	IEEE Transactions on Image Processing	10.1109/TIP.2013.2295755	image texture;wavelet;support vector machine;feature learning;computer vision;digital signature;text mining;computer science;machine learning;data mining;algorithm;statistics	Vision	35.343330518164784	-54.91124833902365	147845
e94ad50b60653b28c878a96883d6a253473960b4	a new deep spatial transformer convolutional neural network for image saliency detection		In this paper we propose a novel deep spatial transformer convolutional neural network (Spatial Net) framework for the detection of salient and abnormal areas in images. The proposedmethod is general and has threemain parts: (1) context information in the image is captured by using convolutional neural networks (CNN) to automatically learn high-level features; (2) to better adapt the CNN model to the saliency task, we redesign the feature sub-network structure to output a 6-dimensional transformation matrix for affine transformation based on the spatial transformer network. Several local features are extracted, which can effectively capture edge pixels in the salient area, meanwhile embedded into the above model to reduce the impact of highlighting background regions; (3) finally, areas of interest are detected by means of the linear combination of global and local feature information. Experimental results demonstrate that Spatial Nets obtain superior detection performance over state-of-the-art algorithms on two popular datasets, requiring less memory and computation to achieve high performance.	.net framework;algorithm;artificial neural network;computation;convolutional neural network;database;embedded system;end-to-end encryption;end-to-end principle;high- and low-level;pixel;subnetwork;supervised learning;transformation matrix;transformer	Xinsheng Zhang;Teng Gao;Dongdong Gao	2018	Design Autom. for Emb. Sys.	10.1007/s10617-018-9209-0	pixel;real-time computing;convolutional neural network;linear combination;salience (neuroscience);transformer;computation;affine transformation;computer science;transformation matrix;pattern recognition;artificial intelligence	AI	26.669005404714774	-52.39848747531115	148061
6bacd4347f67ec60a69e24ed7cc0ac8073004e6f	kinship classification based on discriminative facial patches	discriminative facial patches;cornell family 101 dataset kinship classification discriminative facial patches social networks computer vision machine learning technology people relationships human centered image data kin relationships query face image facial landmark regions appearance features histogram of gradient hog scale invariant feature transform sift four patch local binary pattern fplbp high dimensional feature vector linear support vector machine svm polynomial kernel;kinship classification;feature extraction face support vector machines computer vision training conferences accuracy;feature extraction;transforms computer vision face recognition feature extraction image classification image retrieval learning artificial intelligence polynomials social networking online support vector machines;support vector machine;support vector machine kinship classification discriminative facial patches feature extraction	Recently there has been a large explosive growth of image data on social networks and how to use computer vision and machine learning technology to verify people relationships on these huge amount of human-centered image data remains a challenging issue. Remarkably, there have been few research attempts to analyze the possible human relationships on images, especially kin relationships. In this paper, we tackle a challenging and relatively new issue in kinship classification: determining the family that a query face image belongs to. To address this challenge, we propose a kinship classification method in three steps: (l)Discriminative patches are detected automatically in the facial landmark regions. (2) Appearance features, Histogram of Gradient (HOG), Scale-Invariant Feature Transform (SIFT) and Four-Patch Local Binary Pattern (FPLBP) are extracted from these patches respectively, and then we concatenate the features to create a high-dimensional feature vector. (3) Linear Support Vector Machine (SVM) with polynomial kernel is adopted to accomplish kinship classification task. Experimental evaluation results on Cornell Family 101 dataset demonstrate that our proposed method significantly outperforms the state-of-the-art kinship classification approaches.	computer vision;concatenation;facial recognition system;feature vector;gradient;machine learning;microsoft kin;polynomial kernel;scale-invariant feature transform;social network;support vector machine	Jie Dong;Xiang Ao;Songzhi Su;Shaozi Li	2014	2014 IEEE Visual Communications and Image Processing Conference	10.1109/VCIP.2014.7051528	support vector machine;computer vision;feature extraction;computer science;machine learning;linear classifier;pattern recognition;bag-of-words model in computer vision;feature;structured support vector machine	Vision	31.45299633066612	-56.89485162662121	148090
06e872e1de44e5bd4ec546401ff4f06d6826e032	deep learning for domain-specific action recognition in tennis		Recent progress in sports analytics has been driven by the availability of spatio-temporal and high level data. Video-based action recognition in sports can significantly contribute to these advances. Good progress has been made in the field of action recognition but its application to sports mainly focuses in detecting which sport is being played. In order for action recognition to be useful in sports analytics a finer-grained action classification is needed. For this reason we focus on the fine-grained action recognition in tennis and explore the capabilities of deep neural networks for this task. In our model, videos are represented as sequences of features, extracted using the well-known Inception neural network, trained on an independent dataset. Then a 3-layered LSTM network is trained for the classification. Our main contribution is the proposed neural network architecture that achieves competitive results in the challenging THETIS dataset, comprising videos of tennis actions.	artificial neural network;benchmark (computing);data (computing);deep learning;high-level programming language;human metabolome database;long short-term memory;network architecture;population;sensor;spatiotemporal database;whole earth 'lectronic link	Silvia Vinyes Mora;William J. Knottenbelt	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.27	architecture;computer vision;artificial neural network;deep learning;computer science;feature extraction;machine learning;analytics;artificial intelligence	Vision	27.253179633188974	-52.338928123675	148114
f5186838748a10808188dfa232c33fb056a8d628	kanji recognition in scene images without detection of text fields - robust against variation of viewpoint, contrast, and background texture	character recognition;data compression;geometry;image classification;image coding;image resolution;image retrieval;kanji character recognition;background texture;clipped region classification;coarse searching;geometric transformation;image scanning time;multicompression coarse-to-fine scanning;multiresolution image scanning;peak point detection;recognition dictionaries;scene image indexing;indexation	With the goal of indexing scene images, we propose a novel recognition method for Kanji characters captured in scene images. Our method scans multi-resolution images and classifies clipped regions with recognition dictionaries generated by learning a large amount of partial patterns of characters with large geometric transformation. The problem of scanning time, which tends to be unpractically long, is solved by using multi-compression coarse-to-fine scanning, and by detecting peak points after coarse searching. Despite the wrong results generated in the background, our method well supports image retrieval since it uses the regular spacing of characters. Experimental results show that this recognition method recognized characters at the rate of 82%. Precision was 84% and recall was 64% for image retrieval.	dictionary;image retrieval;sensor	Yoshinori Kusachi;Akira Suzuki;Naoki Ito;Kenichi Arakawa	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334163	data compression;image texture;computer vision;contextual image classification;feature detection;visual word;speech recognition;image resolution;binary image;image processing;image retrieval;computer science;digital image processing;pattern recognition;geometry;automatic image annotation	Vision	37.86355954272838	-63.650382690991634	148286
ab0e0bc97757c3e833c377935bb522693490b34b	shape matching using points co-occurrence pattern	databases;co occurrence pattern;dimension reduction;image matching;self similarity;shape recognition;shape context databases transform coding pattern matching skeleton shape measurement;shape measurement;transform coding;skeleton;computer vision;visual databases computer vision image matching image retrieval shape recognition;shape;shape matching;pattern matching;context;computational efficiency shape matching point co occurrence pattern computer vision sample contour point convexity concavity property shape feature shape contour spatial cooccurrence relation shape database mpeg 7 shape retrieval;dimension reduction shape matching co occurrence pattern self similarity;visual databases;image retrieval	Shape matching is a very critical problem in computer vision, and many smart features have been designed in recent literature for improving the similarity measure between pairs of shapes, and most of them consider either distribution of the sample contour points, or convexity/concavity property of the contour. In this paper, we design a novel shape feature to capture the Co-Occurrence Pattern (COP) of the points sampled from any given shape contour, and each pattern is described by \textbf{Self-Similarity} which investigates the spatial co-occurrence relation among all the sample points. We test our feature on three famous shape databases: MPEG-7 CE-Shape-1 part B, Tari1000, and Kimia99 data set for shape matching and retrieval. The experimental results show that the proposed descriptor achieves higher computational efficiency with no significant performance loss.	computation;computer vision;contour line;database;experiment;information;mpeg-7;self-similarity;shape analysis (digital geometry);shape context;similarity measure	Yu Zhou;Junwei Wang;Quan Zhou;Xiang Bai;Wenyu Liu	2011	2011 Sixth International Conference on Image and Graphics	10.1109/ICIG.2011.103	active shape model;computer vision;transform coding;self-similarity;image retrieval;shape;computer science;machine learning;pattern matching;pattern recognition;shape analysis;mathematics;skeleton;dimensionality reduction	Vision	38.606717223180134	-58.42789206998967	148416
507cf50318a4af17a34dfb39c8e7aa7063c0a713	a new recognition algorithm with high result reliability	neural nets;artificial neural networks pattern recognition feature extraction reliability theory bayes methods context;recognition algorithm recognition rate result reliability mscm;recognition rate recognition algorithm pattern recognition feature extraction mscm method multiple set compete method high result reliability algorithm;feature extraction;pattern recognition;pattern recognition feature extraction neural nets	High recognition rate is the aim of pattern recognition, which is difficult to obtain without ideal feature extraction. To solve this problem, a new recognition algorithm with high result reliability is proposed. The core contents of this algorithm are MSCM (multiple-set-compete method) and the algorithm of result reliability. In this paper, MSCM improving the recognition rate with nonideal feature extraction and the algorithm of result reliability are illustrated. A specific recognition case of 16 classes of sounds and the test flow of the specific case are given. The results show that the new recognition algorithm can improve the recognition rate greatly.	algorithm;feature extraction;pattern recognition	Xiaoyu Liu;Zhongyuan Yu;Yumin Liu;Houjian Kang;Jinhong Mu	2012	2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2012.6664313	feature;feature extraction;computer science;machine learning;pattern recognition;data mining;artificial neural network	Robotics	25.945985859409227	-58.378192261671	148546
0dfad781a2aba3c31f33d02260aa2f5592e8463a	off-line handwritten modi numerals recognition using chain code	modi handwritten recognition;correlation function;modi script;off line recognition;chain code method	In this paper a system for recognition of off-line handwritten Modi Numerals is presented. To extract the features of handwritten Modi numeral chain code feature extraction technique is used with non-overlapping blocking strategy. A correlation coefficient is used for Modi numeral recognition. Experimental results are evaluated using two strategies: different numeral image non-overlapping division and different sizes of data set. On experimentation the maximum recognition rate of 85.21% is achieved on a database of 30000 images. The recognition results shows better performance for 5X5 grids divisions.	blocking (computing);chain code;coefficient;feature extraction;online and offline	Manisha S. Deshmukh;Manoj P. Patil;Satish R. Kolhe	2015		10.1145/2791405.2791419	arithmetic;speech recognition;computer science;pattern recognition	AI	32.83318941500814	-64.63532208057667	148659
fb807b5ae37863531a49811de04ca36fd44996d8	robust facial representation for recognition		One of the main challenges in face recognition lies in robust representation of facial images in unconstrained real-world environment, where face appearances of a same person often vary significantly. This thesis investigates both holistic and local feature based representations, and develops several novel representation models in an effort to mitigate within-person variations and enhance discriminative power. The work first focuses on feature extraction of high-dimensional holistic representation based on intensities. Several linear and nonlinear dimensionality reduction methods are systematically compared. One of key findings is that linear PCA has comparable performances to the most recent nonlinear methods for extracting low-dimensional facial features. Extensive experiments are conducted and results are presented to support the findings, together with a quantitative measure of nonlinearity showing theoretical insights. Following these findings, a robust framework combining an automatic outlier detector and a nearest subspace classifier, is presented. The detector computes the corrupted regions of face images by measuring their reconstructive capabilities, while the classifier models face data by multiple linear subspaces. To remedy the inherent limitations of intensity based representation, a local feature method based on local binary pattern (LBP) is developed. It introduces a novel dissimilarity measure by computing the weighted distances between multiple resolution LBP features. The distance function can be formulated as a conditionally positive definite kernel, and hence is suitable for kernel-based algorithms such as support vector machine. Effectively, by fusing micro texture features and global spatial information, the proposed local feature representation achieves strong discriminative capability and high robustness to variations in expression, lighting condition and occlusion. Finally, a simple yet powerful local facial descriptor based on image gradients, termed the binary gradient correlation pattern (BGCP), is presented. The BGCP descriptor devises a set of binary strings by computing image gradients from multiple directions. It defines certain types of these binary strings as structural patterns, which resemble the fundamental textural information of images by detecting orientational micro edges. Theoretical analysis and experiments show that histogram statistics of structural BGCP patterns gain desirable properties of spatial locality, orientation and scale selectivity, and enable enhanced robustness and discriminative representation. In addition, binary strategy significantly simplifies the computational complexity. The capabilities can be further enhanced by applying the BGCP to a set of defined orientational image gradient magnitudes. Extensive experimental verifications show the efficiency of BGCP based representations, and the significant improvements in performance over the existing methods. To my loving parents ...		Weilin Huang	2013			computer vision;machine learning;pattern recognition;mathematics	Vision	35.652406596213815	-57.07137493535734	148844
936ad20c029631c55398f9c325057e56e51fe5cb	using mid-high level cues to detect salient object	object detection image colour analysis image segmentation;boundary cue saliency map multi scale segmentation background prior spatial color entropy;image color analysis visualization feature extraction entropy object detection image segmentation computational modeling;objectness evaluation model mid high level cues saliency object detection method complementary saliency maps multiscale segmentation cue background cue spatial color distribution cue	This paper proposes a novel saliency object detection method by using the mid-level and high-level visual cues. In the mid-level objectness evaluation, we generate three complementary saliency maps, such as the multi-scale segmentation cue, the background cue and the spatial color distribution cue. The first cue is used to highlight the objects via the local region segment. The second cue uses the background priors to detect the saliency information. The third cue is to capture the spatial color distribution. For the high-level visual cue, we propose an objectness evaluation model to distinguish the object and the background. All the saliency cues are finally combined to achieve the saliency detection. The experimental results show that the proposed method outperforms the state-of-the-art saliency object detection methods.	color;high- and low-level;high-level programming language;map;object detection	Hongliang Li;Yurui Xie;Bing Luo;Liangzhi Tang;Bing Zeng;King Ngi Ngan;Fanman Meng	2014	2014 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2014.6890148	computer vision;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	38.27497358260602	-54.22349704924507	149015
dd11490a29f109004c3afe7bec0704b08907e17a	clustering binary signature applied in content-based image retrieval		Image retrieval is an important problem on the multimedia systems. It is time-consuming to query directly on a large image database. So, the paper approaches to build an image retrieval system CBIR (Content-Based Image Retrieval) based on binary signature to retrieve effectively on the large data of images. Firstly, the paper presents the segmentation method based on low-level visual features including color and texture of image. On the basis of segmented image, the paper creates binary signature to describe location, color and shape of interest objects. In order to match similar images, the paper presents a similarity measure between the images based on binary signatures. On the basis of the similarity measure, the paper proposes the clustering binary signature method to quickly query similar images. At the same time it describes the splitting and group method to apply for clusters of binary signatures. From there, the paper gives the CBIR model to describe the process of similar image retrieval. To demonstrate the proposed method, the paper builds application and assesses experimental results on image databases including Corel, Corel Wang and ImageCLEF.	content-based image retrieval	Thanh The Van;Thanh Manh Le	2016		10.1007/978-3-319-31232-3_22	correlation clustering;visual word;cluster analysis	Vision	38.8136521849922	-60.71051002244642	149047
0aad18a693a0d1ff5cf6a7a7a69108ffdd6df536	tailoring non-homogeneous markov chain wavelet models for hyperspectral signature classification	selected works;hidden markov model;hidden markov models wavelet transforms hyperspectral imaging noise reduction training computational modeling;classification;model complexity nonhomogeneous hidden markov chain wavelet models hyperspectral signature classification semantic structural features improved nhmc model daubechies 1 wavelets;hyperspectral signal processing;bepress;image classification handwriting recognition hidden markov models hyperspectral imaging;hidden markov model classification hyperspectral signal processing wavelet;wavelet	We consider the application of non-homogeneous hidden Markov chain (NHMC) models to the problem of hyperspectral signature classification. It has been previously shown that the NHMC model enables the detection of several semantic structural features of hyperspectral signatures. However, there are some aspects of the spectral data that are not fully captured by the proposed NHMC models such as the relatively smooth but fluctuating regions and the fluctuation orientations. In order to address these limitations, we propose an improved NHMC model based on Daubechies-1 wavelets in conjunction with an increased the model complexity. Experimental results show that the revised approach outperforms existing approaches relevant in classification tasks.	antivirus software;hidden markov model;markov chain;quantum fluctuation;statistical classification;wavelet	Siwei Feng;Yuki Itoh;Mario Parente;Marco F. Duarte	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7026046	wavelet;computer vision;biological classification;computer science;machine learning;pattern recognition;mathematics;hidden markov model;statistics	Vision	35.873582544823044	-61.2572523995394	149164
3bc43718e47ea151f31f4e42708fe44d67932792	face recognition through hough transform for irises extraction and projection procedures for parts localization	face recognition;hough transform	In order to generate facial caricatures automatically, it is necessary to recognize facial parts in advance. Since it is difficult to realize the face recognition, the primal description of face has been provided manually so far also in our project PICASSO for facial caricaturing.	facial recognition system;hough transform	Yoshiaki Segawa;Hiroshi Sakai;Toshio Endoh;Kazuhito Murakami;Takashi Toriu;Hiroyasu Koshimizu	1996		10.1007/3-540-61532-6_53	hough transform;computer vision;speech recognition;computer science;pattern recognition	Vision	31.82420783480468	-58.525553368538304	149277
3e392f7eba3099e9582174dfcaa827ca0d1f2088	integrating low-level and semantic features for object consistent segmentation	feedback mechanism;semantic segmentation;object like regions	The aim of semantic segmentation is to assign each pixel a semantic label. Numerous methods for semantic segmentation have been proposed in recent years and most of them chose pixel or superpixel as the processing primitives. However, as the information contained in a pixel or a superpixel is not discriminative enough, the outputs of these algorithms are usually not object consistent. To tackle this problem, we introduce the concept of object-like regions as a new and higher level processing primitive. We first experimentally showed that using groundtruth segments as processing primitives can boost semantic segmentation accuracy, and then proposed a novel method to produce regions that resemble the groundtruth regions, which we named them as 'object-like regions'. We achieve this by integrating state of the art low-level segmentation algorithms with typical semantic segmentation algorithms through a novel semantic feature feedback mechanism. We present experimental results on the publicly available image understanding dataset MSRC21 and stanford background dataset, showing that the new method can achieve relatively good semantic segmentation results with far fewer processing primitives.	high- and low-level	Hao Fu;Guoping Qiu	2013	Neurocomputing	10.1016/j.neucom.2012.01.050	computer vision;semantic similarity;computer science;machine learning;segmentation-based object categorization;data mining;feedback;scale-space segmentation	Vision	30.037015224322523	-52.17049601788168	149341
0fe0be5e1400cd4c619c8ce0fa527f473c6b72d7	human-inspired order-based block feature in the hsi color space for image retrieval	histograms;image processing;image resolution;color space;histogram method human inspired order based block feature hsi color space image retrieval image processing image color descriptor image representation image intensity distribution human image classification behavior color feature representation method order based block color feature statistical value;image classification;color histogram;statistical analysis;image color analysis;image colour analysis;image representation;feature extraction;image retrieval histograms humans color information retrieval image classification image databases content based retrieval image processing indexing;multimedia communication;人工智能;humans;statistical analysis image classification image colour analysis image representation image retrieval;image retrieval	Color is one of the most important descriptor in image processing. Color histogram is the most commonly used color feature and has proved to be stable representation of an image, but it might be similar in different kinds of images because it describe the global intensity distribution of images. Inspired by human image classification behavior, in this paper, a new color feature representation method called Order-based Block Color Feature (OBCF) and its application in image retrieval is proposed. Firstly, RGB values of an image were transferred to HSI values. Secondly, each color channel (H, S and I) of an image is divided into M X N blocks and then statistical values are computed to characterize the block's color features. Thirdly, block features of the same statistical value in the same row are sorted in ascending order to form a row's features. Finally, all row features are concatenated to form an image's color feature. The experimental results show that the OBCF method provides high retrieval accuracy compared with color histogram method.	channel (digital image);color histogram;color space;computer vision;concatenation;content-based image retrieval;experiment;horizontal situation indicator;image processing;list of monochrome and rgb palettes;sorting	Hong Qin;Shoujue Wang;Huaxiang Lu;Xinliang Chen	2009	2009 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2009.5420542	demosaicing;color histogram;computer vision;contextual image classification;feature detection;color quantization;hsl and hsv;image resolution;color normalization;color depth;color image;image gradient;binary image;image processing;feature extraction;image retrieval;computer science;pattern recognition;high color;histogram;mathematics;color balance;color space;histogram equalization;feature;information retrieval	Robotics	37.68173317691302	-60.80943108084545	149379
f45f607667311bea426e88287992c845c103d64f	face perspective understanding using artificial neural network group-based tree	neural nets;face recognition trees mathematics neural nets;image understanding;trees mathematics;face recognition;complex system;secure system;image understanding face perspective understanding artificial neural network group based tree complex systems airport security system;artificial neural networks neural networks face recognition humans computer networks information systems electronic mail system testing laboratories airports;artificial neural network;neural network	Recent artificial neural network research has focused on simple models, but such models have not been very successful in describing complex systems (such as face perspective understanding). This paper presents the artificial neural network group-based tree (NGT) model, along with test results under laboratory conditions for face perspective understanding, suitable for use in an airport security system. We conclude that the NGT model offers significant improvement over conventional neural network trees for image understanding.	artificial neural network	Ming Zhang;John Fulcher	1996		10.1109/ICIP.1996.560534	nervous system network models;computer vision;computer science;artificial intelligence;machine learning;time delay neural network;deep learning;artificial neural network	NLP	30.293295572889807	-65.93330565887837	149658
c88384e4c1690d78f4c8730dd58b26f1b06928d6	image skin segmentation based on multi-agent learning bayesian and neural network	skin detector;bayesian method;qa75 electronic computers computer science;hf5546 5548 6 office management;neural network	Skin colour is considered to be a useful and discriminating spatial feature for many skin detection-related applications, but it is not sufficiently robust to address complex image environments because of light-changing conditions, skin-like colours and reflective glass or water. These factors can create major difficulties in face pixel-based skin detectors when the colour feature is used. Thus, this paper proposes a multi-agent learning method that combines the Bayesian method with a grouping histogram (GH) technique and the back-propagation neural network with a segment adjacent-nested (SAN) technique based on the YCbCr and RGB colour spaces, respectively, to improve skin detection performance. The findings from this study have shown that the proposed multi-agent learning for skin detector has produced significant true positive (TP) and true negative (TN) average rates (i.e. 98.44% and 99.86% respectively). In addition, it has achieved a significantly lower average rate for the false negative (FN) and false positive (FP) (i.e. only 1.56% and 0.14% respectively). The experimental results show that multi-agent learning in the skin detector is more efficient than other approaches. Keyword: Skin detector; Bayesian method; Neural network	acronis true image;artificial neural network;backpropagation;color space;multi-agent system;pixel;sensor;software propagation;twisted nematic field effect	A. A. Zaidan;N. N. Ahmad;H. Abdul Karim;M. Larbani;B. B. Zaidan;Aduwati Sali	2014	Eng. Appl. of AI	10.1016/j.engappai.2014.03.002	computer vision;bayesian probability;computer science;artificial intelligence;machine learning;artificial neural network	AI	24.92921041597869	-66.07336043400245	149750
8de316c88991b373c7f1622bc4b7dc13104ad32a	integrating rare minutiae in generic fingerprint matchers for forensics	least squares approximations database management systems digital forensics feature extraction fingerprint identification;feature extraction databases forensics fingerprint recognition manuals bifurcation image resolution;conferenceobject;rank identification accuracy generic fingerprint matcher forensics automated fingerprint identification system afis criminal database discriminatory feature feature extraction algorithm latent fingerprint crime scene partial minutiae pattern minutia feature reference point similarity score reference minutiae based matcher least square quantitative measure forensic fingerprint casework database guardia civil spanish law enforcement agency nist bozorth3 verifinger	Automated Fingerprint Identification Systems (AFIS) are commonly used by law enforcement agencies to narrow down the possible suspects from a criminal database. AFIS do not use all discriminatory features available in fingerprints but typically use only some types of features automatically extracted by a feature extraction algorithm. Latent fingerprints obtained from crime scenes are usually partial in nature which results to only very few number of reliable minutiae. Comparing a partial minutiae pattern to a full minutiae pattern is a difficult problem. Towards solving this challenge, we propose a method that exploits extended fingerprint features (unusual/rare minutiae) not commonly considered in typical minutiae-based matchers. The method we propose in this work can be combined with any existing minutiae-based matcher. We first compute a quantitative measure based on least squares between latent and tenprint minutiae points, with rare minutia feature as reference point. Then the similarity score of the reference minutiae-based matcher is modified based on the least square quantitative measure. The modified similarity score thus obtained incorporates the contribution of rare minutia features. We use a realistic forensic fingerprint casework database in our experiments which contains rare minutia features obtained from Guardia Civil, the Spanish law enforcement agency. Experiments are conducted using two reference minutiae-based matchers, namely: NIST-Bozorth3 and VeriFinger. We report a significant improvement in the rank identification accuracies when the reference minutiae matchers are augmented with our proposed algorithm based on rare minutia features.	acoustic fingerprint;algorithm;automated fingerprint identification;experiment;feature extraction;fingerprint recognition;image resolution;least squares;minutiae	Ram P. Krish;Julian Fiérrez;Daniel Ramos-Castro	2015	2015 IEEE International Workshop on Information Forensics and Security (WIFS)	10.1109/WIFS.2015.7368557	fingerprint verification competition;minutiae;engineering;data mining;internet privacy;computer security;integrated automated fingerprint identification system	Vision	30.656473203298646	-63.85491506159989	149805
622ed15bed384878c9ba1f366f601375d3c598b4	off-line preprocessing and verification of signatures	verification;image extraction;feature extraction;signature;simulated forgeries;preprocessing	This paper introduces an effective method for signature separation from nonhomogeneous noisy background. It also introduces a solution to the problem of simulated signature verification in off-line systems. Extraction of shape and density features and the effectiveness of using each and both of them are discussed in the light of experimental results.	electronic signature;preprocessor	Maan Ammar;Yuuji Yoshida;Teruo Fukumura	1988	IJPRAI	10.1142/S0218001488000376	verification;speech recognition;feature extraction;computer science;machine learning;pattern recognition;data mining;signature;preprocessor	Crypto	30.616148778437857	-63.71732741360306	149850
d7862f6913c46149ce33c0f58f5a4b02c23e6585	content-based image retrieval based on quantum-behaved particle swarm optimization algorithm		The performance of content-based image retrieval (CBIR) is usually limited since only single visual feature and single similarity measurement are used. In order to solve this problem, the color and texture visual features of an image are analyzed firstly. And then 12 kinds of similarity measurement are used to evaluate similarity between the image being checked and the images in the retrieval library. The CBIR problem is therefore transferred to an optimization problem with the precision ratio as its objective function. Quantum-behaved Particle Swarm Optimization (QPSO) algorithm is used to solve the CBIR optimization problem in order to find the optimal weight and the optimal combination of visual features and similarity measurements. Experimental results show that the proposed method based on QPSO algorithm has better performance on the retrieval effect.	algorithm;content-based image retrieval;particle swarm optimization;quantum	Wei Fang;Xiaobin Liu	2016		10.1007/978-3-319-41000-5_39	multi-swarm optimization;meta-optimization;metaheuristic	Vision	38.12600727886834	-62.217768099200235	149866
0451b917946dc13c82b63df2ebbe14a6344fab9b	efficient online signature authentication approach	databases;shape analysis	Signature authentication systems often have to focus their processing on acquired dynamic and/or static signatures descriptors to authenticate persons. This approach gives satisfactory results in ordinary cases but remains vulnerable against skilled forgeries. This is mainly because there is no relation between the signatory and his signature. We will show that the inclusion of the hand shape in the authentication process will considerably reduce the false acceptance rates of skilled forgeries and improve the authentication accuracy performances. A new online hand signature authentication approach based on both signature and hand shape descriptor is proposed. The signature acquisition is completely transparent, which allows a high level of security against fraudulent imitation attempts. Authentication performances are evaluated with extensive experiments. The obtained test results [equal error rate ðEERÞ 1⁄4 2%, genuine acceptance rate ðGARÞ 1⁄4 96%] confirm the efficiency of the proposed approach. © 2014 SPIE and IS&T [DOI: 10.1117/1.JEI.23.6.063009]	antivirus software;authentication;experiment;high-level programming language;performance;signature	Kaouther Saidani;Messaoud Mostefai;Abderraouf Bouziane;Youssef Chahir	2014	J. Electronic Imaging	10.1117/1.JEI.23.6.063009	computer vision;computer science;data mining;shape analysis;internet privacy;computer security	Security	31.925196407843192	-61.96823089273499	149990
793d558716c7606ee9c52b645329787c1db2aa1e	fast scene text localization by learning-based filtering and verification	verification;classification text detection coarse to fine feature extraction;coarse to fine strategy;text analysis filtering theory pattern classification;learning based filtering;training;text analysis;fast scene text localization;polynomial classifier fast scene text localization learning based filtering verification region filtering coarse to fine strategy multi orientation projection analysis;classification;polynomials;coarse to fine;region filtering;image edge detection;discrete cosine transforms;feature extraction;pixel;pattern classification;text detection;pixel polynomials feature extraction discrete cosine transforms training image edge detection conferences;spatial relationships;polynomial classifier;connected component;high speed;multi orientation projection analysis;filtering theory;natural scenes;conferences	This paper proposes a new method for fast text localization in natural scene images by combining learning-based region filtering and verification in a coarse-to-fine strategy. In each pyramid layer, a boosted region filter is used to extract candidate text regions, which are segmented into candidate text lines by multi-orientation projection analysis. A polynomial classifier with combined features is used to verify patches of candidate text lines for removing non-texts. The remaining text patches over all pyramid layers are grouped into text lines based on their spatial relationships. The text lines are further refined and partitioned into words by connected component analysis. Experimental results show that the proposed method provides competitive localization performance at high speed.	connected component (graph theory);connected-component labeling;international conference on document analysis and recognition;naive bayes classifier;polynomial;pyramid (geometry);pyramid (image processing)	Yi-Feng Pan;Cheng-Lin Liu;Xinwen Hou	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5651862	spatial relation;computer vision;text mining;verification;connected component;feature extraction;biological classification;computer science;machine learning;pattern recognition;mathematics;pixel;polynomial	Robotics	36.76390124717627	-64.39146567757251	150005
0e78adb1705958a9ebb1268c15cbbb601dd28a84	breaking the 99% barrier: optimisation of three-dimensional face recognition	3d image;3d face recognition;image registration parameters;classifier training;motion estimation;optimisation face recognition image classification image registration motion estimation;motion handling;verification rate;performance improvement;automatic outlier removal;three dimensional face recognition optimisation;three dimensional face recognition optimisation 3d face recognition real time image recognition false accept rate verification rate frgc v2 data performance improvement classifier training automatic outlier removal image registration parameters 3d image motion handling motion estimation;false accept rate;frgc v2 data;real time image recognition	This study presents optimisations to a three-dimensional (3D) face recognition method the authors published in 2011. The optimisations concern handling and estimation of motion from a single 3D image using the symmetry of the face, fine registration by selection of the maximum score for small variations of the registration parameters and efficient training using automatic outlier removal where only part of the classifier is retrained. The optimisations lead to a staggering performance improvement: the verification rate on Face Recognition Grand Challenge (FRGC) v2 data at false accept rate = 0.1% increases from 94.6 to 99.3% and the identification rate increases from 99 to 99.4%. Both are, to the authors' knowledge, the best scores ever published on the FRGC data. In addition, the registration time was reduced from about 2.5 to 0.2–0.6 s and the number of comparisons has increased from about 11 000 to more than 50 000 per second. For slightly decreased performance, even millions of comparisons can be realised. The fast registration means near real-time recognition with 2–5 images is possible. The optimisations are not specific for this method, but can be beneficial for other 3D face recognition methods as well.	facial recognition system;mathematical optimization;three-dimensional face recognition	Luuk J. Spreeuwers	2015	IET Biometrics	10.1049/iet-bmt.2014.0017	stereoscopy;computer vision;computer science;machine learning;pattern recognition;motion estimation;three-dimensional face recognition	Vision	33.3058678469937	-57.02769584647762	150382
88152d7c10ad20890ee5b72fcfcaf556b15c8dfc	verifying a user in a personal face space	viola jones method;pca user verification viola jones method face space;user verification;personal digital assistant;principal component analysis face recognition feature extraction image registration mobile computing notebook computers object detection;face space;viola jones based method;personal face space;principal component analysis personal face space user verification personal digital assistant face detection face registration viola jones based method face feature vectors;feature vector;face detection personal digital assistants principal component analysis face recognition feature extraction shape robustness facial features eyes nose;face recognition;feature extraction;face feature vectors;principal component analysis;image registration;notebook computers;face detection;mobile computing;pca;face registration;object detection	For user verification on a personal digital assistant (PDA), a fast and simple system is developed. In the enrollment phase, face detection and registration are done by a Viola-Jones based method, taking advantage of its accuracy and speed. The face feature vectors obtained this way are then used to build up a face space specific to the user by principal component analysis (PCA). Furthermore, the face variations caused by small registration shifts are also modeled, in order to better capture the variation in the face space, and simplify the enrollment. Current experiments show that this system is fast, efficient, and accurate	algorithm;experiment;face detection;face space;feature vector;image registration;jones calculus;personal digital assistant;principal component analysis	Qian Tao;Raymond N. J. Veldhuis	2006	2006 9th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2006.345479	computer vision;speech recognition;computer science;machine learning;pattern recognition;mobile computing;principal component analysis	Robotics	32.955336950705316	-60.663319103299685	150466
4930c5127064da2121639a3fe15f221c4a203a2d	histogram of oriented principal components for cross-view action recognition	three dimensional displays feature extraction histograms robustness image color analysis skeleton detectors;detectors;histograms;biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;skeleton;three dimensional displays feature extraction histograms robustness image color analysis spatiotemporal phenomena detectors;research articles;three dimensional displays;image color analysis;abstracts;feature extraction;open access;life sciences;pointcloud;clinical guidelines;spatiotemporal phenomena;robustness;view invariance;full text;rest apis;orcids;europe pmc;biomedical research;view invariance spatio temporal keypoint pointcloud;bioinformatics;literature search;spatio temporal keypoint	Existing techniques for 3D action recognition are sensitive to viewpoint variations because they extract features from depth images which are viewpoint dependent. In contrast, we directly process point clouds for cross-view action recognition from unknown and unseen views. We propose the histogram of oriented principal components (HOPC) descriptor that is robust to noise, viewpoint, scale and action speed variations. At a 3D point, HOPC is computed by projecting the three scaled eigenvectors of the pointcloud within its local spatio-temporal support volume onto the vertices of a regular dodecahedron. HOPC is also used for the detection of spatiotemporal keypoints (STK) in 3D pointcloud sequences so that view-invariant STK descriptors (or Local HOPC descriptors) at these key locations only are used for action recognition. We also propose a global descriptor computed from the normalized spatio-temporal distribution of STKs in 4-D, which we refer to as STK-D. We have evaluated the performance of our proposed descriptors against nine existing techniques on two cross-view and three single-view human action recognition datasets. The experimental results show that our techniques provide significant improvement over state-of-the-art methods.	dermatophilus congolensis:prthr:pt:exudate:ord:methylene blue stain;histogram;point cloud;published comment;regular dodecahedron;stk;spatiotemporal database;wavelet analysis	Hossein Rahmani;Arif Mahmood;Du Q. Huynh;Ajmal S. Mian	2016	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2016.2533389	computer vision;detector;text mining;feature extraction;computer science;data science;machine learning;data mining;histogram;skeleton;robustness	Vision	37.25711054142864	-52.10744384710564	150543
0f6d08491f9c7b2f9e586b012168c1544f3744cf	markovian mixture face recognition with discriminative face alignment	bottom up;stochastic process;probability;top down;markovian mixture face recognition;discriminative face alignment;face alignment;probability face recognition markov processes object detection;markov chain markovian mixture face recognition discriminative face alignment face detection stochastic process probability distribution;face recognition;shape;stochastic processes;feature extraction;probability distribution;face;markov processes;face detection;doped fiber amplifiers;object detection;face recognition face detection stochastic processes doped fiber amplifiers probability distribution humans testing active shape model;markov chain	A typical automatic face recognition system is composed of three parts: face detection, face alignment and face recognition. Conventionally, these three parts are processed in a bottom-up manner: face detection is performed first, then the results are passed to face alignment, and finally to face recognition. The bottom-up approach is one extreme of vision approaches. The other extreme approach is top-down. In this paper, we proposed a Markovian stochastic mixture approach for combining bottom-up and top-down face recognition: face recognition is performed from the results of face alignment in a bottom-up way, and face alignment is performed based on the results of face recognition in a top-down way. By modeling the mixture face recognition as a stochastic process, the recognized person is decided probabilistically according to the probability distribution coming from the stochastic face recognition, and the recognition problem becomes that ldquowho the most probable person is when the stochastic process of face recognition goes on for an infinite long durationrdquo. This problem is solved with the theory of Markov chains by properly modeling the stochastic process of face recognition as a Markov chain. As conventional face alignment is not suitable for this mixture approach, discriminative face alignment is proposed. And we also prove that the Markovian mixture face recognition results only depend on discriminative face alignment, not on conventional face alignment. Our approach can surprisingly outperform the face recognition performance with manual face localization, which is demonstrated by extensive experiments.	algorithm;alignment;bottom-up parsing;bottom-up proteomics;experiment;face detection;facial recognition system;job control (unix);markov chain;mixture model;probability;regulatory submission;stochastic process;top-down and bottom-up design	Ming Zhao;Tat-Seng Chua	2008	2008 8th IEEE International Conference on Automatic Face & Gesture Recognition	10.1109/AFGR.2008.4813443	computer vision;computer science;machine learning;pattern recognition;three-dimensional face recognition	Vision	36.05368448312592	-52.869327847483476	150670
5bfb5c5a6b05bda64b56372e137fdf42481ee236	a curvelet-based distance measure for seismic images		We introduce a new curvelet-based distance measure for post-migration seismic data. The measure exploits the highly directional content of seismic images. It calculates the sum of the squared chord distances between the histograms of the curvelet coefficients of two images, over all orientations and scales. In a retrieval task consisting of 918 retrieval instances, the proposed method successfully retrieves all images. This wasn't achieved by other methods in the literature. Furthermore, in comparison to the state-of-the-art, the proposed measure required 86% less computation time.	coefficient;computation;curvelet;time complexity	Yazeed Alaudah;Ghassan Al-Regib	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351597	computer vision;artificial intelligence;pattern recognition;curvelet;computer science	Vision	38.27045604977655	-60.94065132321163	150720
1c10b796ad5f72694c02518a25e5c0fb45a387f1	depth from monocular images using a semi-parallel deep neural network (spdnn) hybrid architecture		Computing pixel depth values provide a basis for understanding the 3D geometrical structure of an image. As it has been presented in recent research, using stereo images provides an accurate depth due to the advantage of having local correspondences; however, the processing time of these methods are still an open issue. To solve this problem, it has been suggested to use single images to compute the depth values but extracting depth from monocular images requires extracting a large number of cues from the global and local information in the image. This challenge has been studied for a decade and it is still an open problem. Recently the idea of using neural networks to solve this problem has attracted attention. In this paper, we tackle this challenge by employing a Deep Neural Network (DNN) equipped with semantic pixel-wise segmentation utilizing our recently published disparity post-processing method. Four models are trained in this study and they have been evaluated at 2 stages on KITTI dataset. The ground truth images in the first part of the experiment come from the benchmark and for the second part, the ground truth images are considered to be the disparity results from applying a state-of-art stereo matching method. The results of this evaluation demonstrate that using post-processing techniques to refine the target of the network increases the accuracy of depth estimation on individual mono images. The second evaluation shows that using segmentation data as the input can improve the depth estimation results to a point where performance is comparable with stereo depth matching.	deep learning	Shabab Bazrafkan;Hossein Javidnia;Joe Lemley;Peter Corcoran	2017	CoRR		computer vision;simulation;computer science;machine learning;pattern recognition	Robotics	28.836993277170624	-53.24814636372904	150847
3d61fb2301a3220ea32f680a1ccfbe3d6a04d1ff	high-resolution palmprint minutiae extraction based on gabor feature	palmprint recognition minutiae extraction minutiae reliability measurement adaboost algorithm gabor filter;gabor filter;minutiae extraction;palmprint recognition;minutiae reliability measurement;adaboost algo rithm	Extracting effective minutiae is difficult for high-resolution palmprint, because of the strong influence from principal lines, creases, and other noises. In this paper, a novel minutiae detection and reliability measurement method is proposed for high-resolution palmprint minutiae extraction. Firstly, we propose the Gabor Amplitude-Phase model for palmprint representation, which contains sufficient palmprint information and consists of the phase field and amplitude field. Because of the explicit meanings of minutiae in phase field, a minutiae descriptor is constructed to detect them directly. Also, to measure minutiae reliability and remove the unreliable ones, the Gabor Amplitude-Phase feature vector is designed. It can be used for describing the local area of a minutia redundantly. Then, the Adaboost algorithm is introduced in model training to select best features and corresponding weak classifiers for minutiae authenticity discriminant. Finally, the response value of weighted linear combination of selected weak classifiers is used for minutiae reliability measurement and unreliable ones removal. According to our analysis, the selected features are meaningful and useful for describing the minutiae area and measuring their reliability. Experimental results show that our proposed method is effective for minutiae extraction and can improve the matching performance.	adaboost;algorithm;convolution;discriminant;feature vector;fingerprint;gabor atom;gabor filter;image resolution;in-phase and quadrature components;minutiae;weak value	Jufu Feng;Chongjin Liu;Han Wang;Bing Sun	2014	Science China Information Sciences	10.1007/s11432-014-5125-5	computer vision;speech recognition;pattern recognition	Vision	35.41826841288194	-61.745047762407225	150912
7dbb1e62f9b714aa150cccb5bd512b5ddf53607e	sketch tokens: a learned mid-level representation for contour and object detection	pattern clustering;gradient histograms sketch tokens learned midlevel representation contour detection object detection local contour based representation midlevel features supervised midlevel information hand drawn contours human generated contour clustering random forest classifier top down task bottom up task pedestrian detection inria pascal low level features;trees mathematics feature extraction image classification image representation learning artificial intelligence object detection pattern clustering pedestrians;image classification;trees mathematics;pedestrians;image representation;feature extraction;learning artificial intelligence;object detection;image edge detection feature extraction image color analysis training object detection accuracy detectors	We propose a novel approach to both learning and detecting local contour-based representations for mid-level features. Our features, called sketch tokens, are learned using supervised mid-level information in the form of hand drawn contours in images. Patches of human generated contours are clustered to form sketch token classes and a random forest classifier is used for efficient detection in novel images. We demonstrate our approach on both top-down and bottom-up tasks. We show state-of-the-art results on the top-down task of contour detection while being over 200x faster than competing methods. We also achieve large improvements in detection accuracy for the bottom-up tasks of pedestrian and object detection as measured on INRIA and PASCAL, respectively. These gains are due to the complementary information provided by sketch tokens to low-level features such as gradient histograms.	bottom-up parsing;contour line;gradient;high- and low-level;object detection;pascal;random forest;sensor;sketch;top-down and bottom-up design	Joseph J. Lim;C. Lawrence Zitnick;Piotr Dollár	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.406	computer vision;contextual image classification;feature detection;object-class detection;feature extraction;computer science;viola–jones object detection framework;machine learning;pattern recognition	Vision	32.22330409965099	-54.54255521024035	151023
8fcfecd5285216b0ca0f25b3ee81f4a22c2cc4d7	adaptive deep pyramid matching for remote sensing scene classification		Convolutional neural networks (CNNs) have attracted increasing attention in the remote sensing community. Most CNNs only take the last fully-connected layers as features for the classification of remotely sensed images, discarding the other convolutional layer features which may also be helpful for classification purposes. In this paper, we propose a new adaptive deep pyramid matching (ADPM) model that takes advantage of the features from all of the convolutional layers for remote sensing image classification. To this end, the optimal fusing weights for different convolutional layers are learned from the data itself. In remotely sensed scenes, the objects of interest exhibit different scales in distinct scenes, and even a single scene may contain objects with different sizes. To address this issue, we select the CNN with spatial pyramid pooling (SPP-net) as the basic deep network, and further construct a multi-scale ADPM model to learn complementary information from multi-scale images. Our experiments have been conducted using two widely used remote sensing image databases, and the results show that the proposed method significantly improves the performance when compared to other state-of-the-art methods. Keywords—Convolutional neural network (CNN), adaptive deep pyramid matching (ADPM), convolutional features, multi-scale ensemble, remote-sensing scene classification.	artificial neural network;computer vision;convolutional neural network;database;experiment;image resolution;pyramid (geometry);self-propelled particles;super paper mario	Qingshan Liu;Renlong Hang;Huihui Song;Fuping Zhu;Javier Plaza;Antonio J. Plaza	2016	CoRR		computer vision;machine learning;pattern recognition	Vision	27.783301185416622	-52.14906880706233	151153
26b5a296e331238322db139d994b7a68a4f0c99f	image-based historical manuscript dating using contour and stroke fragments	contour fragment;stroke fragment;handwriting style;writer identification;historical manuscript dating	Historical manuscript dating has always been an important challenge for historians but since countless manuscripts have become digitally available recently, the pattern recognition community has started addressing the dating problem as well. In this paper, we present a family of local contour fragments (kCF) and stroke fragments (kSF) features and study their application to historical document dating. kCF are formed by a number of k primary contour fragments segmented from the connected component contours of handwritten texts and kSF are formed by a segment of length k of a stroke fragment graph. The kCF and kSF are described by scale and rotation invariant descriptors and encoded into trained codebooks inspired by classical bag of words model. We evaluate our methods on the Medieval Paleographical Scale (MPS) data set and perform dating by writer identification and classification. As far as dating by writer identification is concerned, we arrive at the conclusion that features which perform well for writer identification are not necessarily suitable for historical document dating. Experimental results of dating by classification demonstrate that a combination of kCF and kSF achieves optimal results, with a mean absolute error of 14.9 years when excluding writer duplicates in training and 7.9 years when including writer duplicates in training. & 2016 Elsevier Ltd. All rights reserved.	approximation error;bag-of-words model;codebook;connected component (graph theory);historical document;pattern recognition	Sheng He;Petros Samara;Jan Burgers;Lambert Schomaker	2016	Pattern Recognition	10.1016/j.patcog.2016.03.032	machine learning;historical document;artificial intelligence;bag-of-words model;connected component;mean absolute error;mathematics;pattern recognition;invariant (mathematics);graph	AI	34.181167471524255	-64.18616180769527	151178
47f32355b6b889c922f4f824ab84e3c56cd89127	online person identification and new person discovery using appearance features	kernel;video surveillance;support vector machines;video surveillance feature extraction feature selection image classification image recognition image sequences;hilbert space;video sequences new person discovery appearance based person identification system silhouette separation background separation feature extraction feature selection online person identification self adaptive kernel machine sakm algorithm recognition rates person classification video surveillance system;image color analysis;feature extraction;clustering algorithms;feature extraction support vector machines image color analysis hilbert space clustering algorithms kernel video surveillance	Person identification is an important but still challenging problem in video surveillance. This work designs a completely automatic appearance-based person identification system, which has the ability to achieve new person discovery and classification. The proposed system consists of three modules: background and silhouette separation; feature extraction and selection; and online person identification. The Self-Adaptive Kernel Machine (SAKM) algorithm is used to differentiate existing persons who can be classified from new persons who have to be learnt and added. A new video database with 22 persons is created in real-life environments. The experimental results show that the proposed system achieves satisfying recognition rates of over 90% on person classification with novelty identification.	algorithm;background subtraction;closed-circuit television;cluster analysis;co-occurrence matrix;feature extraction;feature model;filesystem-level encryption;kernel method;local binary patterns;naive bayes classifier;principal component analysis;real life;robert haralick;separation kernel	Yanyun Lu;Anthony Fleury;Jacques Boonaert;Stéphane Lecoeuche;Sebastien Ambellouis	2015	2015 IEEE International Conference on Evolving and Adaptive Intelligent Systems (EAIS)	10.1109/EAIS.2015.7368794	computer vision;feature extraction;computer science;machine learning;pattern recognition	Robotics	31.674641606516563	-58.17722427762878	151229
d03ee5c4329b980e2cc80fcc67dd7b9c36d6f6e2	developed global biotic cross pollination algorithm for cis		This paper focuses on the visual-based colour image segmentation with a global biotic cross pollination algorithm (GBCPA). The global biotic cross pollination algorithm segments the structurally challenging objects based on the colour, edge, entropy and edge information in the CIE L*a*b* colour space. The L*a*b* colour space is a colour-opponent space considered to approximate human vision. L* denotes the luminosity or brightness layer, chromaticity layer a* indicates colour falls along red-green axis and chromaticity layer b* indicates the blue-yellow axis. The FPO algorithm considering the global biotic cross pollination is proposed to improve the quality of the solution and computational speed. GBCPA is first introduced to find the locality of the solution. The performance of GBCPA is tested on a standard Berkeley segmentation dataset with 300 images. The dataset is illustrated under different evaluating strategies.	algorithm	K. Sasi Kala Rani;D. Rasi;S. N. Deepa	2018	IJBIDM	10.1504/IJBIDM.2017.10003631	pollination;chromaticity;computer science;brightness;image segmentation;algorithm;luminosity	EDA	37.68074795315947	-57.96281721860724	151395
6c7d835471d7992b0ecf99e13f2b60b94df427cb	puzzle based system for improving arabic handwriting recognition	paw;ocr;handwritten arabic;puzzle	Several researches have been done through the last years to improve the recognition rate of Arabic handwritten recognition systems. The use of different post-processing techniques for word selection methods such as voting and contextual information was the choice of many systems. In our previous works, we proposed a technique that uses SVM classifier to recognize Arabic handwritten based on two passes horizontal and vertical. In this work, we add a Puzzle algorithm as a post-processor to improve the recognition rate, especially for ambiguous characters. Our method uses a set of stages (filtering, segmentation, features extraction, classification, and posttreatment) and leads to a better classification rate. The approach is tested on Tunisian database IFN/ENIT for handwritten Arabic. It gives encouraging results and opens other perspectives in the domain of Arabic handwritten recognition.	algorithm;filter (signal processing);handwriting recognition;image segmentation;statistical classification;support vector machine;video post-processing	Zaiz Faouzi;Mohamed Chaouki Babahenini;Abdelhamid Djeffal	2016	Eng. Appl. of AI	10.1016/j.engappai.2016.09.005	natural language processing;speech recognition;intelligent word recognition;pattern recognition	AI	33.424806627926195	-65.77263432973992	151725
b67777e56da298c817493ed989e59c28d865b1bc	classifying many-class high-dimensional fingerprint datasets using random forest of oblique decision trees	fingerprint classification;scale invariant feature transform;random forest of oblique decision trees;bag of visual words	Classifying fingerprint images may require an important features extraction step. The scale-invariant feature transform which extracts local descriptors from images is robust to image scale, rotation and also to changes in illumination, noise, etc. It allows to represent an image in term of the comfortable bag-of-visual-words. This representation leads to a very large number of dimensions. In this case, random forest of oblique decision trees is very efficient for a small number of classes. However, in fingerprint classification, there are as many classes as individuals. A multi-class version of random forest of oblique decision trees is thus proposed. The numerical tests on seven real datasets (up to 5,000 dimensions and 389 classes) show that our proposal has very high accuracy and outperforms state-of-the-art algorithms.	adaboost;bag-of-words model in computer vision;c4.5 algorithm;decision tree;fingerprint;numerical analysis;oblique projection;on-die termination;on-line debugging tool;performance;radio frequency;random forest;scale-invariant feature transform;support vector machine	Thanh-Nghi Do;Philippe Lenca;Stéphane Lallich	2014	Vietnam Journal of Computer Science	10.1007/s40595-014-0024-7	random forest;computer vision;computer science;machine learning;pattern recognition;scale-invariant feature transform;bag-of-words model in computer vision	ML	33.87476551336757	-57.482722133772896	151836
9bd8fe15e5f6ac46e2815d1105f64aba1d8d76cb	a genetics-motivated unsupervised model for tri-subject kinship verification	databases;histograms;feature extraction genetics computational modeling histograms face databases hafnium;genetics;computational modeling;feature extraction;face;linear combination kinship verification tri subject genetics unsupervised model;hafnium	Given a child's and a couple's facial photos, tri-subject kinship verification aims to determine the existence of blood relation between the child and the couple. Different from existing methods which model the kinship inheritance process among three persons in separate stages and only use simple features, this work establishes a simple model inspired by genetics to measure tri-subject kinship similarity in one step. Meanwhile, high-dimensional features are incorporated into this simple model to seek for better performance. Experiment results demonstrate the effectiveness of our approach.	simple features;triangular function;unsupervised learning	Junkang Zhang;Si-Yu Xia;Hong Pan;A. K. Qin	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532893	face;feature extraction;computer science;artificial intelligence;machine learning;data mining;histogram;mathematics;hafnium;computational model;statistics	Robotics	30.973673484754936	-58.6658097617106	151893
6fe18ece5765aadf80226525ab64de7392582432	discriminating angry, happy and neutral facial expression: a comparison of computational models	facial expressions;book chapter;classification;conference papers meetings and proceedings;conference paper;dimensionality reduction;image analysis	Recognizing expressions are a key part of human social interac- tion, and processing of facial expression information is largely automatic for humans, but it is a non-trivial task for a computational system. The purpose of this work is to develop computational models capable of differentiating be- tween a range of human facial expressions. Raw face images are examples of high dimensional data, so here we use two dimensionality reduction techniques: Principal Component Analysis and Curvilinear Component Analysis. We also preprocess the images with a bank of Gabor filters, so that important features in the face images are identified. Subsequently the faces are classified using a Support Vector Machine. We show that it is possible to differentiate faces with a neutral expression from those with a happy expression and neutral expression from those of angry expressions and neutral expression with better accuracy. Moreover we can achieve this with data that has been massively re- duced in size: in the best case the original images are reduced to just 5 compo- nents with happy faces and 5 components with angry faces.	computation;computational model	Aruna Shenoy;Sue H. Anthony;Ray J. Frank;Neil Davey	2009		10.1007/978-3-642-03969-0_19	psychology;computer vision;artificial intelligence;communication	Vision	25.56805614853168	-63.633850404467005	151942
d141c31e3f261d7d5214f07886c1a29ac734d6fc	unsupervised video hashing via deep neural network	video hashing;unsupervised method;deep neural network;spatio-temporal feature	Hashing is a common solution for content-based multimedia retrieval by encoding high-dimensional feature vectors into short binary codes. Previous works mainly focus on image hashing problem. However, these methods can not be directly used for video hashing, as videos contain not only spatial structure within each frame, but also temporal correlation between successive frames. Several researchers proposed to handle this by encoding the extracted key frames, but these frame-based methods are time-consuming in real applications. Other researchers proposed to characterize the video by averaging the spatial features of frames and then the existing hashing methods can be adopted. Unfortunately, the sort of “video” features does not take the correlation between frames into consideration and may lead to the loss of the temporal information. Therefore, in this paper, we propose a novel unsupervised video hashing framework via deep neural network, which performs video hashing by incorporating the temporal structure as well as the conventional spatial structure. Specially, the spatial features of videos are obtained by utilizing convolutional neural network, and the temporal features are established via long-short term memory. After that, the time series pooling strategy is employed to obtain the single feature vector for each video. The obtained spatio-temporal feature can be applied to many existing unsupervised hashing methods. Experimental results on two real datasets indicate that by employing the spatio-temporal features, our hashing method significantly improves the performance of existing methods which only deploy the spatial features, and meanwhile obtains higher mean average precision compared with the state-of-the-art video hashing methods.	algorithm;artificial neural network;binary code;convolutional neural network;deep learning;feature vector;hash function;ibm notes;information retrieval;key frame;long short-term memory;machine learning;performance;time series;unified framework;unsupervised learning	Chao Ma;Yun Gu;Chen Gong;Jie Yang;Deying Feng	2018	Neural Processing Letters	10.1007/s11063-018-9812-x	artificial intelligence;binary code;convolutional neural network;machine learning;feature vector;artificial neural network;pooling;encoding (memory);hash function;pattern recognition;sort;mathematics	AI	26.735391684119005	-52.97990095832439	152058
aa7046a0f10bfe0b002886702741e6f4de3d484c	object fingerprints for content analysis with applications to street landmark localization	local system;web image search;object fingerprint;content analysis;multimedia data;street landmark localization;multimedia content analysis;problem solving	An object can be a basic unit for multimedia content analysis. Besides similarity among common objects, each object has its own unique characteristics which we cannot find in other surrounding objects in multimedia data. We call such unique characteristics object fingerprints. In this paper, we propose a novel approach to extract and match object fingerprints for multimedia content analysis. In particular, we focus on the problem of street landmark localization from images. Instead of modeling and matching a street landmark as a whole, our proposed approach extracts the landmark's object fingerprints in a given image and match to a new image or video in order to localize the landmark. We formulate matching the landmark's object fingerprints as a classification problem solved by a cascade of 1NN classifiers. We develop a street landmark localization system that combines salient region detection, segmentation, and object fingerprint extraction techniques for the purpose. To evaluate, we have compiled a novel dataset which consists of 15 U.S. street landmarks' images and videos. Our experiments on this dataset show superior performance to state-of-the-art recognition algorithms [20, 33]. The proposed approach can also be well generalized to other objects of interest and content analysis tasks. We demonstrate the feasibility through the application of our approach to refine web image search results and obtained encouraging results.	algorithm;compiler;experiment;fingerprint;image retrieval;language localisation	Wen Wu;Jie Yang	2008		10.1145/1459359.1459383	computer vision;content analysis;data mining;multimedia;local system	Vision	32.76077485426564	-52.50202185105785	152209
44db63b1a314da196327770021e0b2b6b8bdb6bc	fuzzy emotion recognition model for video sequences	face image color analysis skin emotion recognition computational modeling face detection pragmatics;video signal processing;image classification;emotion recognition;fuzzy set theory;face detection emotion recognition fuzzy inference system camshift;face recognition;principal component analysis;object tracking;fuzzy emotion recognition model automatic facial expression recognition bayes classifier face tracking cam shift principal component analysis adaptive thresholding skin detection boundary elliptical model emotion classification fuzzy based approach image subjectivity image analysis video sequence;video signal processing emotion recognition face recognition fuzzy set theory image classification image sequences object tracking principal component analysis;image sequences	Automatic facial expression recognition from video clips is a challenging task due to computational complexity, limitations of image analysis and subjectivity. This paper advocates a fuzzy based approach for emotion classification. On the other hand, several proposals have been put forward to enhance the pre-processing stage prior to the classification. This includes a combination of a boundary elliptical model for skin detection, adaptive thresholding, principal component analysis and use of cam-shift for face tracking. The performances of the developed system have been evaluated using TFEID and video clips and compared with Bayes' classifier.	computational complexity theory;emotion recognition;facial motion capture;image analysis;performance;preprocessor;principal component analysis;thresholding (image processing);video clip	Mourad Oussalah;S. Wang	2012	2012 3rd International Conference on Image Processing Theory, Tools and Applications (IPTA)	10.1109/IPTA.2012.6469574	facial recognition system;computer vision;contextual image classification;speech recognition;computer science;machine learning;video tracking;pattern recognition;three-dimensional face recognition;fuzzy set;principal component analysis	Vision	32.665117138224105	-57.02807649431114	152303
1d55e5b14648475ed43b496a5f9d71e490278622	a medical image retrieval framework in correlation enhanced visual concept feature space	visual concept structure descriptor;medical image retrieval;biomedical image dataset medical image retrieval framework correlation enhanced visual concept feature space statistical model probabilistic multiclass support vector machine image texture patches feature representation visual concept structure descriptor;probability;support vector machines;training;biomedical imaging;color histogram;feature space;statistical model;image texture;probabilistic multiclass support vector machine;multi dimensional;visualization;statistical analysis;biomedical image dataset;image color analysis;image representation;feature extraction;medical image processing;biomedical imaging image retrieval support vector machines image representation support vector machine classification medical diagnostic imaging information retrieval dicom picture archiving and communication systems image color analysis;spatial relationships;feature representation;support vector machine;probabilistic logic;support vector machines feature extraction image representation image retrieval image texture medical image processing probability statistical analysis;image texture patches;correlation enhanced visual concept feature space;medical image retrieval framework;quantization error;image retrieval	This paper presents a medical image retrieval framework that uses visual concepts in a feature space employing statistical models built using a probabilistic multi-class support vector machine (SVM). The images are represented using concepts that comprise color and texture patches from local image regions in a multi-dimensional feature space. A major limitation of concept feature representation is that the structural relationship or spatial ordering between concepts are ignored. We present a feature representation scheme as visual concept structure descriptor (VCSD) that overcomes this challenge and captures both the concept frequency similar to a color histogram and the local spatial relationships of the concepts. A probabilistic framework makes the descriptor robust against classification and quantization errors. Evaluation of the proposed image retrieval framework on a biomedical image dataset with different imaging modalities validates its benefits.	color histogram;feature vector;high- and low-level;image retrieval;semantic data model;statistical model;support vector machine	Md. Mahmudur Rahman;Sameer K. Antani;George R. Thoma	2009	2009 22nd IEEE International Symposium on Computer-Based Medical Systems	10.1109/CBMS.2009.5255392	medical imaging;support vector machine;computer vision;feature detection;visual word;image retrieval;computer science;machine learning;pattern recognition;feature;statistics	Vision	38.26451552286139	-61.905198983208734	152532
a57b92ed2d8aa5b41fe513c3e98cbf83b7141741	fisher's discriminant and relevant component analysis for static facial expression classification	emotion recognition;face recognition;feature extraction;image classification;statistical analysis;aam;cohn-kanade database;fld;fisher linear discriminant;gda;lda;rca;svm;active appearance model;anger;appearance parameters extraction;automatic classification;disgust;facial expression recognition framework;fear;joy;plug-&-play classifier;relevant component analysis;sadness;static facial expression classification;static images;surprise;universal emotional categories;support vector machines;hidden markov models;dimensionality reduction;face;shape	This paper addresses the issue of automatic classification of the six universal emotional categories (joy, surprise, fear, anger, disgust, sadness) in the case of static images. Appearance parameters are extracted by an active appearance model(AAM) representing the input for the classification step. We show how Relevant Component Analysis (RCA) in combination with Fisher's Linear Discriminant (FLD) provides a good “plug-&-play” classifier in the context of facial expression recognition framework. We test this method against several other classification techniques, including LDA, GDA and SVM, on the Cohn-Kanade database.	gnome-db;linear discriminant analysis;sadness	Matteo Sorci;Gianluca Antonini;Jean-Philippe Thiran	2007	2007 15th European Signal Processing Conference		psychology;computer vision;machine learning;pattern recognition	Vision	31.43077168818924	-58.80516245243682	152696
f2259e45a6fbe526d35ba94ad8c60beeb26a5d15	segmentation of color images for image data analysis	video streaming;distributed search trees;data analysis;service oriented architecture soa;b and b trees;hough transform;grid computing;color image	The work described here discusses the analysis of images in port settings. As cargo containers are of specific colors, the hue information is being exploited to segment, identify and compress such video streams. A novel scheme using multiple processes has been developed. The first process is obtaining the vector angle between the RGB components which is used to segment images based on the color. The second process is the Hough transformation that is carried out on the vector angle result as well as a Laplacian of the saturation image. Using the Hough transform result on both the vector angle result and the Laplacian, various broken segments are connected effectively without adding redundancy. This information can then be used to detect corners and perform compression.	color;hough transform;streaming media	Sanmati S. Kamath	2006		10.1145/1185448.1185619	hough transform;computer vision;computer science;theoretical computer science;computer graphics (images)	Vision	38.60041461995739	-61.659875350531784	152743
789b8fff223b0db0fe3babf46ea98b1d5197f0c0	automatic facial expression recognition based on pixel-pattern-based texture feature	gabor wavelet;pixel-pattern-based texture feature;automatic facial expression recognition;pie database;facial expression;real-time facial expression recognition;pattern map;face representation;dut database;pattern matching;facial texture information;cohn-kanade database;svm;adaboost	PCA, ICA, and Gabor wavelet are considered as the important and powerful face representation methods. In this article, we propose a new approach for face representation, which is called a pixel-pattern-based texture feature (PPBTF) and apply it to the realtime facial expression recognition. A gray scale image is transformed into a pattern map where edges and lines are used for characterizing the facial texture information. Based on the pattern map, a feature vector is comprised of the numbers of the pixels belonging to each pattern. We use the image basis functions obtained by principal component analysis as the templates for pattern matching. Adaboost and Support Vector Machine are adopted to classify facial expression. Extensive experiments on the Cohn-Kanade Database, PIE Database, and DUT Database illustrate that the PPBTF is quite effective and insensitive to illumination. The comparison with Gabor show the PPBTF is speedy. VC 2010 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 20, 253–260, 2010; View this article online at wileyonlinelibrary. com. DOI 10.1002/ima.20245	adaboost;basis function;device under test;experiment;feature vector;gabor wavelet;grayscale;independent computing architecture;john d. wiley;pattern matching;pixel;principal component analysis;real-time clock;support vector machine	Huchuan Lu;Yingjie Huang;Yen-Wei Chen	2010	Int. J. Imaging Systems and Technology	10.1002/ima.20245	adaboost;support vector machine;computer vision;speech recognition;computer science;machine learning;pattern recognition	Vision	34.744075448159045	-58.97615553031753	152795
1e762471dbce6154934271ca56b9d6614258f5ce	hierarchical dropped convolutional neural network for speed insensitive human action recognition		Human action recognition using skeleton data has lots of potential applications in content-based action retrieval and intelligent surveillance, with wide usage of depth sensors and robust skeleton estimation algorithms. Previous methods describe spatial temporal skeleton joints as a compact color image and then use Convolutional Neural Network (CNN) to extract more discriminative deep features. However, these methods ignore the effect of speed variation, which is a common phenomenon and can bring severe intra-varieties to same types of actions. To solve this problem, this paper presents a novel hierarchical dropped CNN architecture, which is constructed in two stages. Dropped CNN (d-CNN) is firstly developed to extract deep features from a probabilistic speed insensitive color image. This image expresses both spatial distributions and temporal evolutions of skeleton joints meanwhile avoids the effect of speed variations. To enhance the temporal discriminative power, we extend d-CNN to a hierarchical structure (h-CNN), where multiple scales of temporal information are encoded. Extensive experiments on benchmark MSRC-12 dataset and the largest NTU RGB+D dataset verify the effectiveness and robustness of the proposed method.	algorithm;benchmark (computing);color image;convolutional neural network;experiment;network interface device;sensor	Fanyang Meng;Hong Liu;Yongsheng Liang;Mengyuan Liu;Wei Liu	2018	2018 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2018.8486477	skeleton (computer programming);convolutional neural network;robustness (computer science);computer vision;rgb color model;feature extraction;pattern recognition;computer science;color image;architecture;artificial intelligence;phenomenon	Vision	28.450513321803406	-52.18876807616909	152884
1df6310ff10573e6a4413c31fb9d637acb3d288d	robust facial parts detection by using four directional features and relaxation matching	image database;template matching;high speed	Positions of facial parts, such as eyes, mouth, and nose, are required for constructing the user interface by face image recognition or for face pose estimation. We propose a facial parts detection method that is robust for face postures. The method uses four directional features and relaxation matching. Facial part templates based on four directional features learned from many face images correspond to varieties of input pattern. The initial probability is obtained by template matching of the four directional features. Positions of facial parts are detected by relaxation matching using the spring connection of each facial part. Since it corresponds to various face postures, the multi-direction integrated model is used. The proposal method is short computation time and useful for real-time system. Experiments show that facial parts are detectable to the HOIP multi-directional face image database. Detection rate was 96% for 45 ± horizontal and 30 ± vertical face postures.		Kenji Iwata;Hitoshi Hongo;Kazuhiko Yamamoto;Yoshinori Niwa	2003		10.1007/978-3-540-45226-3_121	computer vision;speech recognition;computer science;pattern recognition	Vision	33.18316451680533	-60.558191308728794	152944
271589e6fc321933973d260ddd154dacf880b5ff	invariant radon-wavelet packet signatures for pattern recognition	image recognition;radon transforms;radon transform;fourier transform;wavelet transforms fourier transforms image recognition radon transforms;wavelet packet;pattern recognition radon transform wavelet transform ridgelets wavelet packets fourier transform feature extraction;wavelet transforms;invariant pattern recognition;noisy environment radon wavelet transform packet signatures invariant pattern recognition shift invariant wavelet packet transform fourier transform rotation angles;wavelet transform;ridgelets;wavelet packet transform;feature extraction;fourier transforms;pattern recognition;wavelet packets;shift invariant;pattern recognition fourier transforms wavelet transforms wavelet packets feature extraction computer science operations research handwriting recognition noise level wavelet domain	A novel descriptor for invariant pattern recognition is proposed by using the Radon transform, the shift-invariant wavelet packet transform, and the Fourier transform. Experimental results show that the proposed descriptor achieves high recognition rates under different rotation angles and noise levels. It outperforms a previously developed method under the noisy environment	network packet;pattern recognition;wavelet packet decomposition	Guangyi Chen;Balázs Kégl	2006	2006 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2006.277475	fourier transform;computer vision;speech recognition;s transform;short-time fourier transform;continuous wavelet transform;pattern recognition;wavelet packet decomposition;wavelet transform	Vision	36.151778439890336	-62.1452400843674	152991
10f857eef7c11c7c6272a5d14e0e65bc259330d1	interesting region detection in aerial video using bayesian topic models	detectors;histograms;bottom up;video signal processing;bayes methods;vocabulary;semantics;observers;visualization;video signal processing bayes methods object detection vector quantisation;vectors;visualization histograms detectors semantics observers vectors vocabulary;vector quantizer;vector quantisation;probabilistic latent semantic analysis visual interesting region detection aerial video bayesian topic models plsa topic model bottom up information local image patches vector quantized sift descriptors image semantic content discovery real world scenes frame extraction topics discovery;object detection	Searching interesting regions in aerial video is a new and challenging problem. This paper presents an approach to detect visual interesting regions in aerial video using pLSA topic model. Traditional interesting region detection approaches just use bottom-up information, such as color, orientation and movement etc. Our proposed method can discover the semantic content of the whole image, the co-occurrence of local image patches via pLSA model, and consequently improve detection result significantly in real world scenes. First, we extract frames from aerial video as documents. Then we use vector quantized SIFT descriptors as words. Third, we discover topics (e.g. plants, roads, buildings) and the relation among them using pLSA model. Finally, we can detect interesting regions as we need according to calculated models. Experimental observations show the success of our approach on interesting region detection in aerial video.	aerial photography;aerial video;bottom-up proteomics;color;probabilistic latent semantic analysis;scale-invariant feature transform;topic model	Jiewei Wang;Yunhong Wang;Zhaoxiang Zhang	2011	The First Asian Conference on Pattern Recognition	10.1109/ACPR.2011.6166550	computer vision;speech recognition;computer science;pattern recognition	Vision	37.943578927194736	-53.37680088267626	153107
8eb80b0e1c19c1410d2436d4381005939624a4f2	iterative training of discriminative models for the generalized hough transform	optimal model generation;generalized hough transform;object localization;machine learning;discriminative training;shape modeling;discriminative model;optimization model	We present a discriminative approach to the Generalized Hough Transform (GHT) employing a novel fully-automated training procedure for the estimation of discriminative shape models. The technique aims at learning the shape and variability of the target object as well as further confusable structures (anti-shapes), visible in the training images. The integration of the learned target shape and anti-shapes into a single GHT model is implemented straightforwardly by positive and negative weights. These weights are learned by a discriminative training and utilized in the GHT voting procedure. In order to capture the shape and anti-shape information from a set of training images, the model is built from edge structures surrounding the correct and the most confusable locations. In an iterative procedure, the training set is gradually enhanced by images from the development set on which the localization failed. The proposed technique is shown to substantially improve the object localization capabilities on long-leg radiographs.	discriminative model;generalised hough transform;iterative method	Heike Ruppertshofen;Cristian Lorenz;Sarah Schmidt;Peter Beyerlein;Zein Salah;Georg Rose;Hauke Schramm	2010		10.1007/978-3-642-18421-5_3	computer vision;computer science;machine learning;pattern recognition;mathematics;discriminative model	Vision	35.43438814793085	-53.30509783626682	153144
47a390293cc8124ffbc3ad8fa0159010cde1b760	pollen classification based on geometrical, descriptors and colour features using decorrelation stretching method		Saving earth's biodiversity for future generations is an important global task, where automatic recognition of pollen species by means of computer vision represents a highly prioritized issue. This work focuses on analysis and classification stages. A combination of geometrical measures, Fourier descriptors of morphological details using Discrete Cosine Transform (DCT) in order to select their most significant values, and colour information over decorrelated stretched images are proposed as pollen grains discriminative features. A MultiLayer neural network was used as classifier applying scores fusion techniques. 17 tropical honey plant species have been classified achieving a mean of 96.49%  1.16 of success.	artificial neural network;computer vision;decorrelation;discrete cosine transform;pollen	Jaime Roberto Ticay-Rivas;Marcos del Pozo-Baños;Carlos Manuel Travieso-González;Jorge Arroyo-Hernández;Santiago T. Pérez;Jesús B. Alonso;Federico Mora-Mora	2011		10.1007/978-3-642-23960-1_41	computer vision;botany;communication	Vision	34.15972594813299	-61.47890842523684	153147
47f5e1aecb326e4f4dcca1d4e4462eb4e09c0302	fingerprint image super resolution using sparse representation with ridge pattern prior by classification coupled dictionaries			dictionary;fingerprint;sparse approximation;sparse matrix;super-resolution imaging	Weixin Bian;Shifei Ding	2017	IET Biometrics	10.1049/iet-bmt.2016.0097	ridge;pattern recognition;computer vision;superresolution;fingerprint;artificial intelligence;sparse approximation;computer science	Vision	29.419231668836485	-56.97519726056586	153273
0495cb430916e2657d4d42c9b695029f020a33d3	a nonlinear approach for face sketch synthesis and recognition	databases;face recognition pattern recognition geometry probes linear discriminant analysis databases nearest neighbor searches nonlinear distortion kernel laboratories;nearest neighbor searches;kernel;statistical analysis face recognition computational geometry;sketch recognition;pseudosketch synthesis;face sketch synthesis;geometry;computational geometry;nonlinear discriminate analysis face sketch synthesis face sketch recognition face recognition system photo based face recognition pseudosketch synthesis pseudosketch generation;probes;discriminant analysis;nonlinear distortion;nonlinear discriminate analysis;face recognition;face recognition system;statistical analysis;general methods;photo based face recognition;pattern recognition;linear discriminant analysis;pseudosketch generation;local linear embedding;face sketch recognition	Most face recognition systems focus on photo-based face recognition. In this paper, we present a face recognition system based on face sketches. The proposed system contains two elements: pseudo-sketch synthesis and sketch recognition. The pseudo-sketch generation method is based on local linear preserving of geometry between photo and sketch images, which is inspired by the idea of locally linear embedding. The nonlinear discriminate analysis is used to recognize the probe sketch from the synthesized pseudo-sketches. Experimental results on over 600 photo-sketch pairs show that the performance of the proposed method is encouraging.	arc diagram;facial recognition system;nonlinear dimensionality reduction;nonlinear system;sketch recognition	Qingshan Liu;Xiaoou Tang;Hongliang Jin;Hanqing Lu;Songde Ma	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.39	computer vision;nonlinear distortion;kernel;speech recognition;computer science;pattern recognition;geometry;linear discriminant analysis;sketch recognition	Vision	38.850177981569644	-57.09336561874131	153299
b21d61197b47724a71dd3e738d1d8bb797bcb1cb	cyclic histogram thresholding and multithresholding	optimal solution;image processing;periodic function approximation by gaussians;linear discriminate analysis;function approximation;pattern recognition;cyclic histogram thresholding and multithresholding;experimental evaluation;color image	The paper concerns the problem of thresholding of an integer domain of 1D cyclic histogram (periodic function) resulting in two or more consecutive regions (classes). An optimal solution is searched for in the terms of the statistical criterion well known in the pattern recognition area as Fisher's LDA (Linear Discriminant Analysis) and also successfully applied for image binarization by Otsu (1979). An effective (quadratic complexity) extension of the Otsu's method is also known, which segments the image by respective thresholding of the image intensity histogram into arbitrary number of classes. We propose one more extension of this approach for the case of the cyclic histograms. Similar problem can be brought by the optimal segmentation of color images based on their HSV histogram, and more general in all problems which try to approximate a given periodic function with a predefined number of Gaussians. The paper describes the theoretical basis and the experimental evaluation of the proposed approach.	approximation algorithm;cubic function;dynamic programming;fourier analysis;high-level synthesis;image segmentation;iterative method;linear discriminant analysis;otsu's method;pattern recognition;prospective search;quasiperiodicity;thresholding (image processing);wavelet	Dimo Dimov;Lasko Laskov	2009		10.1145/1731740.1731761	color histogram;computer vision;mathematical optimization;color normalization;color image;image processing;function approximation;computer science;histogram matching;otsu's method;pattern recognition;balanced histogram thresholding;region growing;thresholding;image segmentation;adaptive histogram equalization;histogram equalization;image histogram	Vision	36.0743190317282	-63.07415576321619	153326
