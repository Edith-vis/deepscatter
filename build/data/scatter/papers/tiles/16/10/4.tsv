id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
1ef27937275ba6d6b65883e82e35609a841ebc93	human action recognition with salient trajectories	kernel histogram;human action recognition;salient trajectory	Recognizing human actions in video sequences is attracting much attention, and this paper aims to deal with the problem of action recognition with salient trajectories. First, two kinds of trajectory saliency values, appearance and motion saliency, are calculated and combined to capture complementary information. Secondly, the combined saliency is utilized to prune redundant trajectories, and a compact and discriminative set of trajectories is obtained. Finally, kernel histograms are applied for the description of salient trajectories and human actions are classified by the bag-of-words approach. The proposed approach is validated on three public datasets including KTH, ADL, and UCF. Experimental results show that the method achieves superior results on the KTH and ADL datasets and comparable results with other state-of-the-art methods on the UCF dataset.		Yang Yi;Yikun Lin	2013	Signal Processing	10.1016/j.sigpro.2013.05.002	computer vision;machine learning;pattern recognition;mathematics	Vision	36.18284843325669	-50.60041956425828	48210
38ca137d0023431a16bef0f15a81aa5a69d8bd9c	detecting abnormal fish trajectories using clustered and labeled data	biology computing;pattern clustering;video signal processing;video signal processing biology computing feature extraction object detection pattern clustering principal component analysis;feature extraction;principal component analysis;outlier detection fish behavior clustered and labeled data feature selection abnormal trajectory;outlier detection abnormal fish trajectory detection clustered data labeled data unconstrained underwater videos abnormal trajectory detection pca principal component analysis feature clustering;object detection	We propose an approach for the analysis of fish trajectories in unconstrained underwater videos. Trajectories are classified into two classes: normal trajectories which contain the usual behavior of fish and abnormal trajectories which indicate the behaviors that are not as common as the normal class. The paper presents two innovations: 1) a novel approach to abnormal trajectory detection and 2) improved performance on video based abnormal trajectory analysis of fish in unconstrained conditions. First we extract a set of features from trajectories and apply PCA. We then perform clustering on a subset of features. Based on the clustering, outlier detection is applied to each cluster. Improved results are obtained which is significant considering the challenges of underwater environments, low video quality, and erratic movement of fish.	anomaly detection;cluster analysis;principal component analysis;sensor	Cigdem Beyan;Robert B. Fisher	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738303	feature extraction;computer science;machine learning;pattern recognition;data mining;principal component analysis	Robotics	38.195191173426856	-47.00373450664304	48478
f56211039a047048424294e23a1ca80b3dcaaff0	a progressively enhanced network for video satellite imagery superresolution		Deep convolutional neural networks (CNNs) have been extensively applied to image or video processing and analysis tasks. For single-image superresolution (SR) processing, previous CNN-based methods have led to significant improvements, when compared to the shallow learning-based methods. However, these CNN-based algorithms with simply direct or skip connections are not suitable for satellite imagery SR because of complex imaging conditions and unknown degradation process. More importantly, they ignore the extraction and utilization of the structural information in satellite images, which is very unfavorable for video satellite imagery SR with such characteristics as small ground targets, weak textures, and over-compression distortion. To this end, this letter proposes a novel progressively enhanced network for satellite image SR called PECNN, which is composed of a pretraining CNN-based network and an enhanced dense connection network. The pretraining part is used to extract the low-level feature maps and reconstructs a basic high-resolution image from the low-resolution input. In particular, we propose a transition unit to obtain the structural information from the base output. Then, the obtained structural information and the extracted low-level feature maps are transmitted to the enhanced network for further extraction to enforce the feature expression. Finally, a residual image with enhanced fine details obtained from the dense connection network is used to enrich the basic image for the ultimate SR output. Experiments on real-world Jilin-1 video satellite images and Kaggle Open Source Dataset show that the proposed PECNN outperforms the state-of-the-art methods both in visual effects and quantitative metrics. Code is available at https://github.com/kuihua/PECNN.	algorithm;artificial neural network;autostereogram;convolutional neural network;distortion;elegant degradation;high- and low-level;image resolution;map;super-resolution imaging;video processing;visual effects	Kui Jiang;Zhongyuan Wang;Peng Yi;Junjun Jiang	2018	IEEE Signal Processing Letters	10.1109/LSP.2018.2870536	convolutional neural network;mathematics;artificial intelligence;residual;pattern recognition;satellite imagery;feature extraction;video processing;distortion;superresolution;image resolution	Vision	26.45050498502586	-51.43509361258716	48558
1be0ce87bb5ba35fa2b45506ad997deef6d6a0a8	exmoves: classifier-based features for scalable action recognition		This paper introduces EXMOVES, learned exemplar-based features for efficient recognition of actions in videos. The entries in our descriptor are produced by evaluating a set of movement classifiers over spatial-temporal volumes of the input sequence. Each movement classifier is a simple exemplar-SVM trained on low-level features, i.e., an SVM learned using a single annotated positive space-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classification models trained on our global video descriptor yield action recognition accuracy approaching the stateof-the-art but at orders of magnitude lower cost, since at test-time no sliding window is necessary and linear models are efficient to train and test. This enables scalable action recognition, i.e., efficient classification of a large number of actions even in massive video databases. We show the generality of our approach by building our mid-level descriptors from two different low-level feature vectors. The accuracy and efficiency of the approach are demonstrated on several large-scale action recognition benchmarks.	benchmark (computing);database;feature vector;high- and low-level;linear classifier;linear model;scalability	Du Tran;Lorenzo Torresani	2013	CoRR		computer vision;computer science;machine learning;pattern recognition	Vision	29.222423281183588	-48.99319852778609	48755
0a2891043bf1e74bfbd43b8b4589c3dd172b57a6	learning-based hierarchical clustering regression for efficient unaligned face hallucination		In this paper, we propose an effective single image super-resolution method for unaligned face images, in which the learning-based hierarchical clustering regression approach is used to get better reconstruction model. The proposed face hallucination method can be divided into two parts: clustering and regression. In the clustering part, a dictionary is trained on the whole face image with tiny size, and the training images are clustered based on the Euclidean distance. Thus, the facial structural prior is fully utilized and the accurate result of clustering can be obtained. In the regression part, only one global dictionary in which atoms are taken as the anchors, will be trained in the entire training phase. Therefore, the time complexity can be effectively reduced. More importantly, the learned anchors are shared with all the clusters. For each cluster, the Euclidean distance is used to search the nearest neighbors for each anchor to form the subspace. Moreover, a regression model is learned to map the relationship between low-resolution features and high-resolution samples in every subspace. The core idea of our method is to utilize the same anchors but different samples for clusters to learn the local mapping more accurately, which can reduce training time and improve reconstruction quality. Experimental results show that the proposed method outperforms some state-of-the-art methods.		Shuyun Wang;Zongliang Gan;Feng Liu	2018	2018 10th International Conference on Wireless Communications and Signal Processing (WCSP)	10.1109/WCSP.2018.8555602	time complexity;regression analysis;iterative reconstruction;real-time computing;euclidean distance;computer science;cluster analysis;face hallucination;hierarchical clustering;subspace topology;artificial intelligence;pattern recognition	Vision	27.888862095139512	-46.09420553578215	48769
78e2abe987eb8e7af52fff44acca108faf107fbf	automatic player behavior analyses from baseball broadcast videos	pitch type recognition;player behavior analysis;hmm;event detection;baseball broadcast video;svm	In this paper, we present a baseball player behavior analysis system by combining pitch types and swing events. We use eight kinds of semantic scenes detected from baseball videos in our previous work. For the pitch types, we use the characteristic of the ball in a pitch scene to identify the ball trajectory, and then 39 features are extracted to feed into a trained SVM for classifying pitch types. For the swing events, we use moving objects in the batter region to determine whether a swing occurs. Then, the event following the swing is detected using an HMM, based on the after-swing scene sequence. Next, the experimental results show that both pitch type recognition and swing event detection have accuracy rates 91.5% and 91.1%. Finally, we analyze and summarize player behavior by combining pitch types and swing events.		Yin-Fu Huang;Zong-Xian Yang	2012		10.1007/978-3-642-35236-2_2	support vector machine;simulation;speech recognition;computer science;machine learning;multimedia;hidden markov model	HCI	38.55951083444506	-47.035922210702005	48839
7da473b75dc150259c6bfb457f540f8baa4f5ddd	semi supervised learning for human activity recognition using depth cameras	learning artificial intelligence cameras feature extraction image recognition;co training action recognition depth skeleton;msr dailyactivity 3d dataset semisupervised learning human activity recognition depth cameras 3d depth data rgb d cameras microsoft kinect high resolution real time depth depth based action recognition people activity surveillance labeled data semisupervision machine learning technique co training technique msr action 3d dataset;depth;skeleton;action recognition;co training;feature extraction three dimensional displays skeleton training cameras supervised learning sensors	"""Human action recognition is a very active research topic in computer vision and pattern recognition. Recently, it has shown a great potential for human action recognition using the 3D depth data captured by the promising RGB-D cameras, and particularly, the Microsoft Kinect which has made high resolution real-time depth cheaply available. Several features and descriptors have been proposed for depth based action recognition, and they have given high results when recognizing the actions, but one dilemma always exists, the labeled data given, which are manually set by humans. They are not enough to build the system, especially that the use of human action recognition is mainly for surveillance of people activities. In this paper, the paucity of labeled data is addressed, by the popular semi supervision machine learning technique """"co-training"""", which makes full use of unlabeled samples of two different independent views. Through the experiments on two popular datasets (MSR Action 3D, and MSR DailyActivity 3D), we demonstrate that our proposed framework outperforms the state of art. It improves the accuracy up to 83% in case of MSR Action 3D, and up to 80% MSR DailyActivity 3D, using the same number of labeled samples."""	activity recognition;co-training;computer vision;experiment;image resolution;kinect;machine learning;microsoft research;pattern recognition;real-time clock;semi-supervised learning;semiconductor industry;supervised learning	Moustafa F. Mabrouk;Nagia M. Ghanem;Mohamed A. Ismail	2015	2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2015.170	computer vision;computer science;machine learning;pattern recognition;skeleton	Vision	36.12794907987371	-47.878313869013056	49009
da7952d8e09d887e9bf1102f94c16177be330e2b	single-image rain removal using residual deep learning		Most outdoor vision systems can be influenced by rainy weather conditions. In this paper, we address a rain removal problem from a single image. Some existing de-raining methods suffer from hue change due to neglect of the information in low frequency layer. Others fail in assuming enough rainy image models. To solve them, we propose a residual deep network architecture called ResDerainNet. Based on the deep convolutional neural network (CNN), we learn the mapping relationship between rainy and residual images from data. Furthermore, for training, we synthesize rainy images considering various rain models. Specifically, we mainly focus on the composite models as well as orientations and scales of rain streaks. The experiments demonstrate that our proposed model is applicable to a variety of images. Compared with state-of-the-art methods, our proposed method achieves better results on both synthetic and real-world images.	artificial neural network;autostereogram;convolutional neural network;deep learning;experiment;network architecture;synthetic intelligence	Takuro Matsui;Takanori Fujisawa;Takuro Yamaguchi;Masaaki Ikehara	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451612	convolutional neural network;residual;deep learning;computer science;artificial intelligence;pattern recognition	Robotics	27.119242975650316	-50.71103736336098	49196
3eaa860f2735fce8b839237397455c13dfad1ed1	dynamic belief fusion for object detection	pascal voc 07 datasets dynamic belief fusion heterogeneous object detection methods dbf joint basic probability assignment dempster combination rule single fused detection score arl;detectors;bayes methods;detectors bayes methods object detection windows computational modeling trajectory;windows;computational modeling;trajectory;probability object detection;object detection	A novel approach for the fusion of heterogeneous object detection methods is proposed. In order to effectively integrate the outputs of multiple detectors, the level of ambiguity in each individual detection score is estimated using the precision/recall relationship of the corresponding detector. The main contribution of the proposed work is a novel fusion method, called Dynamic Belief Fusion (DBF), which dynamically assigns probabilities to hypotheses (target, non-target, intermediate state (target or non-target)) based on confidence levels in the detection results conditioned on the prior performance of individual detectors. In DBF, a joint basic probability assignment, optimally fusing information from all detectors, is determined by the Dempster's combination rule, and is easily reduced to a single fused detection score. Experiments on ARL and PASCAL VOC 07 datasets demonstrate that the detection accuracy of DBF is considerably greater than conventional fusion approaches as well as individual detectors used for the fusion.	authorization;baseline (configuration management);bayesian network;deep learning;information retrieval;object detection;pascal;sensor;universal instantiation	Hyungtae Lee;Heesung Kwon;Ryan M. Robinson;William D. Nothwang;Amar M. Marathe	2016	2016 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2016.7477574	computer vision;detector;speech recognition;computer science;trajectory;machine learning;pattern recognition;computational model;statistics	Vision	34.86498608643618	-47.0944789768804	49299
23ecc496eaa238ac884e6bae5763f6138a9c90a3	discriminative feature adaptation for cross-domain facial expression recognition	databases;feeds;training;feed database cross domain facial expression recognition face recognition face animation affective computing human computer interface testing face images training face images image capture unsupervised domain adaptation method discriminative feature adaptation dfa labelled face image set source domain unlabelled face images target domain feature space face image representation feature distributions facial expression database ck database jaffe database pics database;databases face face recognition training feature extraction target recognition feeds;face recognition;target recognition;feature extraction;face;visual databases emotion recognition face recognition feature extraction image capture image representation	Facial expression recognition is an important problem in many face-related tasks, such as face recognition, face animation, affective computing and human-computer interface. Existing methods mostly assume that testing and training face images are captured under the same condition and from the same population. Such assumption is, however, not valid in real-world applications, where face images could be taken from varying domains due to different cameras, illuminations, or populations. Motivated by recent progresses in domain adaptation, this paper proposes an unsupervised domain adaptation method, called discriminative feature adaptation (DFA), which requires for training a set of labelled face images in the source domain and some additional unlabelled face images in the target domain. It seeks for a feature space to represent face images from different domains such that two objectives are fulfilled: (i) mismatches between the feature distributions of these face images are minimized, and (ii) features are discriminative among these face images with respect to their facial expressions. Compared with existing methods, the proposed method can more effectively adapt discriminative features for recognizing facial expressions in various domains. Evaluation experiments have been done on four public facial expression databases: CK+, JAFFE, PICS, and FEED. The results demonstrate the superior performance of the proposed method over competing methods.	affective computing;database;domain adaptation;experiment;facial recognition system;feature vector;human–computer interaction;platform for internet content selection;population	Ronghang Zhu;Gaoli Sang;Qijun Zhao	2016	2016 International Conference on Biometrics (ICB)	10.1109/ICB.2016.7550085	facial recognition system;face;computer vision;face detection;speech recognition;object-class detection;feature extraction;computer science;pattern recognition;three-dimensional face recognition;geometry;face hallucination	Vision	27.39665327138587	-47.07487853173246	49395
d83583562f3536d5c13f8ca58925362f6878f5f4	enhance visual recognition under adverse conditions via deep networks		Visual recognition under adverse conditions is a very important and challenging problem of high practical value, due to the ubiquitous existence of quality distortions during image acquisition, transmission, or storage. While deep neural networks have been extensively exploited in the techniques of low-quality image restoration and high-quality image recognition tasks respectively, few studies have been done on the important problem of recognition from very low-quality images. This paper proposes a deep learning based framework for improving the performance of image and video recognition models under adverse conditions, using robust adverse pre-training or its aggressive variant. The robust adverse pre-training algorithms leverage the power of pre-training and generalizes conventional unsupervised pre-training and data augmentation methods. We further develop a transfer learning approach to cope with real-world datasets of unknown adverse conditions. The proposed framework is comprehensively evaluated on a number of image and video recognition benchmarks, and obtains significant performance improvements under various single or mixed adverse conditions. Our visualization and analysis further add to the explainability of results.		Ding Liu;Bowen Cheng;Zhangyang Wang;Haichao Zhang;Thomas S. Huang	2017	CoRR		image restoration;transfer of learning;artificial neural network;deep learning;visualization;artificial intelligence;pattern recognition;mathematics	AI	28.69197710595309	-50.34926129770991	49476
eb948160e1dc8574b80f85f5b19854ad5cb17422	real-time human action recognition using individual body part locations and local joints structure	body part locations;human action recognition;local joints structure;svms	In this paper, we present a novel approach for real-time human action recognition using local joints structure and body part locations. Individual body part locations are global features that ignore the local structure information of the human body joints, which is also essential for accurate action recognition. To cope with this problem, we propose local joints structure as a complement, and combine the two features for posture description in our method. We then perform classification using a combination of dynamic time warping, Fourier temporal pyramid representation and linear SVM. Experiments results on three action datasets show that the proposed representation outperforms many existing skeletal representations.	dynamic time warping;experiment;poor posture;pyramid (image processing);real-time clock;real-time transcription	Liqiang Du;Hong Chen;Shuli Mei;Qing Wang	2016		10.1145/3013971.3013974	support vector machine;computer vision;computer science;artificial intelligence;machine learning	Vision	36.47475391997155	-50.11556264205681	49480
1470eb583a4450854e6c1a3b42db317983cec2e1	monocular depth estimation using whole strip masking and reliability-based refinement		We propose a monocular depth estimation algorithm based on whole strip masking (WSM) and reliability-based refinement. First, we develop a convolutional neural network (CNN) tailored for the depth estimation. Specifically, we design a novel filter, called WSM, to exploit the tendency that a scene has similar depths in horizonal or vertical directions. The proposed CNN combines WSM upsampling blocks with a ResNet encoder. Second, we measure the reliability of an estimated depth, by appending additional layers to the main CNN. Using the reliability information, we perform conditional random field (CRF) optimization to refine the estimated depth map. Experimental results demonstrate that the proposed algorithm provides the state-of-the-art depth estimation performance.	4d reconstruction;algorithm;artificial neural network;conditional random field;convolutional neural network;depth map;encoder;focus stacking;master of science in information technology;mathematical optimization;refinement (computing);upsampling	Minhyeok Heo;Jaehan Lee;Kyung-Rae Kim;Han-Ul Kim;Chang-Su Kim	2018		10.1007/978-3-030-01225-0_3	encoder;artificial intelligence;convolutional neural network;computer vision;computer science;upsampling;conditional random field;masking (art);monocular;exploit;depth map	Vision	25.453252946583326	-51.92975083321065	49491
d614e536b8fb86fe0e41a569ede739ffb7292875	action recognition with low observational latency via part movement model		In this paper, we address the issue of recognizing human actions with low observational latency, which is vital for many applications such as virtual reality and interactive entertainment. Then our purpose is to achieve competitive action recognition performance from very short video clips. Such a task essentially is challenging because only very limited information is provided. To this end, we first develop a feature extraction method to exploit both motion (local flow) and appearance (local shape) features such that the information insufficiency can be effectively mitigated. Then we propose an action representation method named Part Movement Model (PMM), which explicitly captures the spatial-temporal structure of human actions and divides the actions into discriminative part movements. Consequently, the actions can be better represented and the competitive performance can be achieved although only short clips are used. Finally, we experimentally verify the effectiveness of the proposed method on three benchmark datasets. The results show that short clips of 6−7 frames (0.2−0.3 second video) are enough to achieve the recognition performance comparable to the baselines with high latency.	baseline (configuration management);benchmark (computing);computation;convergence insufficiency;experiment;feature extraction;graphics processing unit;hidden surface determination;image resolution;optical flow;video clip;virtual reality	Zhikang Liu;Zilei Wang	2016	Multimedia Tools and Applications	10.1007/s11042-016-4193-5	computer vision;simulation;telecommunications;computer science;artificial intelligence;machine learning;world wide web;computer security	Vision	34.07505553181836	-49.868201657701825	49699
f3734ac240806a13b9ea67393b716b1527bf0ec2	an improved locally linear embedding for sparse data sets	locally linear embedding;manifold learning;feature extraction;pattern recognition	Locally linear embedding is often invalid for sparse data sets because locally linear embedding simply takes the reconstruction weights obtained from the data space as the weights of the embedding space. This paper proposes an improved local linear embedding for sparse data sets. In the proposed method, the neighborhood correlation matrix presenting the position information of the points constructed from the embedding space is added to the correlation matrix in the original space, thus the reconstruction weights can be adjusted. As the reconstruction weights adjusted gradually, the position information of sparse points can also be changed continually and the local geometry of the data manifolds in the embedding space can be well preserved. Experimental results on both synthetic and real-world data show that the proposed approach is very robust against sparse data sets.	algorithm;arc diagram;dataspaces;iterative reconstruction;nonlinear dimensionality reduction;sparse matrix;supervised learning;synthetic intelligence	Ying Wen;Lianghua He	2010	2010 IEEE International Conference on Image Processing	10.1142/S0218001411008786	discrete mathematics;topology;feature extraction;computer science;machine learning;pattern recognition;sparse approximation;mathematics;nonlinear dimensionality reduction	Vision	26.612500776493626	-40.527549064545354	49721
71b0505c95f0b49a3de982ad015f3f67dcdbdf79	improving face recognition with domain adaptation		Nearly all recent face recognition algorithms have been evaluated on the Labeled Faces in the Wild (LFW) dataset and many of them achieved over 99% accuracy. However, the performance is still not enough for real-world applications. One problem is the data bias. The faces in LFW and other web-collected datasets come from celebrities. They are quite different from the faces of a normal person captured in the daily life. In other words, they are different in the face distribution. Replacing the training data with the same distribution is a simple solution. However, the photos of common people are much harder to collect because of the privacy concerns. So it is useful to develop a method that transfers the knowledge in the data of different face distribution to help improving the final performance. In this paper, we crawl a large face dataset whose distribution is different from LFW and show the improvement of LFW accuracy with a simple domain adaptation technique. To the best of our knowledge, it is the first time that domain adaptation is applied in the unconstrained face recognition problem with a million scale dataset. Besides, we incorporate face verification threshold into FaceNet triplet loss function explicitly. Finally, we achieve 99.33% on the LFW benchmark with only single CNN model and similar performance even without face alignment.	domain adaptation;facial recognition system	Ge Wen;Huaguan Chen;Deng Cai;Xiaofei He	2018	Neurocomputing	10.1016/j.neucom.2018.01.079	machine learning;artificial intelligence;large face;domain adaptation;facial recognition system;mathematics;pattern recognition;training set	Vision	27.179115886075333	-47.86352665902465	49737
9696ad8b164f5e10fcfe23aacf74bd6168aebb15	4dfab: a large scale 4d facial expression database for biometric applications		The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes.	3d computer graphics;biometrics;computer vision;database;experiment;facial recognition system;file spanning;image resolution;spontaneous order	Shiyang Cheng;Irene Kotsia;Maja Pantic;Stefanos P. Zafeiriou	2017	CoRR			Vision	34.429179943217925	-51.30602764030339	49826
a0dba71645574e46aad0548af202e43b0fe4770e	head pose estimation using spectral regression discriminant analysis	databases;regularization parameter head pose estimation face recognition spectral regression discriminant analysis subspace learning;regularization parameter;kernel;constraint optimization;subspace learning;spectral analysis face recognition kernel parameter estimation learning systems constraint optimization principal component analysis linear discriminant analysis clustering algorithms visual databases;probability density function;spectral regression discriminant analysis;data mining;head pose estimation;discriminant analysis;learning systems;face recognition;statistical analysis;estimation;principal component analysis;clustering algorithms;face;statistical analysis face recognition parameter estimation;parameter estimation;spectral analysis;constrained optimization problem;linear discriminant analysis;visual databases	In this paper, we investigate a recently proposed efficient subspace learning method, Spectral Regression Discriminant Analysis (SRDA), and its kernel version SRKDA for head pose estimation. One important unsolved issue of SRDA is how to automatically determine an appropriate regularization parameter. The parameter, which was empirically set in the existing work, has great impact on its performance. By formulating it as a constrained optimization problem, we present a method to estimate the optimal regularization parameter in SRDA and SRKDA. Our experiments on two databases illustrate that SRDA, especially SRKDA, is promising for head pose estimation. Moreover, our approach for estimating the regularization parameter is shown to be effective in head pose estimation and face recognition experiments.	3d pose estimation;constrained optimization;constraint (mathematics);database;experiment;facial recognition system;kernel (operating system);linear discriminant analysis;mathematical optimization;matrix regularization;optimization problem	Caifeng Shan;Wei Chen	2009	2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2009.5204261	face;estimation;probability density function;kernel;computer science;machine learning;pattern recognition;mathematics;cluster analysis;linear discriminant analysis;estimation theory;statistics;principal component analysis	Vision	25.260144157897802	-41.01608663035759	50195
b806b8a609d04364938634a50c831b45db7abbc7	efficient holistic feature basis learning for pedestrian detection		Pedestrian detection is an important research area in computer vision and Artificial Intelligence due to its potential applications in pedestrian safety, elderly monitor and care, surveillance, image retrieval and video compression. Many pedestrian detection systems have been proposed and it has been pointed out in state-of-the-art research that feature extraction is one of the significant factors in improving the performance of a pedestrian detector. Therefore, much work has focused on proposing novel feature extraction schemes to improve pedestrian detection. Moreover, most are end-to-end pedestrian detection systems, making it unclear about the contribution of classifiers in the detection pipeline. In this paper, we fill in some of this gap and focus on the classification process and propose feature basis learning for holistic high dimensional feature vectors that are common in pedestrian detection. We experimentally show that it is possible to obtain superior performance by our proposed feature basis ...	holism;pedestrian detection	Kyaw Kyaw Htike	2018	IJCVR	10.1504/IJCVR.2018.10008249	computer vision;artificial intelligence;pattern recognition;feature extraction;computer science;image retrieval;data compression;feature vector;feature learning;pedestrian detection	ML	33.82855409942132	-50.780582382199064	50262
5c4efee0effe8cb94e64ff3347d4f55ce2d5d37b	action recognition via sparse representation of characteristic frames	video signal processing;frame selection efficient action recognition sparse representation characteristic frames video sequence global descriptors local descriptors kth database;image representation;video signal processing gesture recognition image representation image sequences;gesture recognition;video sequences humans character recognition feature extraction vectors optical sensors conferences;image sequences	For achieving efficient action recognition, some recent works propose to select a smaller number of frames in a video sequence instead of the entire sequence of frames. In this study, we propose to represent a frame by a combination of local and global descriptors instead of the silhouette used in our previous approach aiming at frame selection. Action recognition is then executed on the basis of the selected frames. The experiment on KTH database shows that the selected frames by the proposed framework are, in the minimum number to achieve the best recognition rate, better than those by two compared selection ways.	optimization problem;sparse approximation;sparse matrix	Guoliang Lu;Mineichi Kudo;Jun Toyama	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		reference frame;computer vision;speech recognition;computer science;video tracking;pattern recognition;gesture recognition	Vision	37.13930323477323	-51.14074664839626	50295
22c994d0f286104be9a597a5d67764fe90c52135	combining extremal optimization with singular value decomposition for effective point matching	point matching;singular value decomposition;robust estimation;far from equilibrium dynamics;extremal optimization;combinatorial optimization	Feature point matching is a key step for most problems in computer vision. It is an ill-posed problem and suffers from combinatorial complexity which becomes even more critical with the increase in data and the presence of outliers. The work covered in this paper describes a new framework to solve this problem in order to achieve robust registration of two feature point sets assumed to be available. This framework combines the use of extremal optimization heuristic with a clever startup routine which exploits some properties of singular value decomposition. The role of the latter is to produce an interesting matching configuration whereas the role of the former is to refine the initial matching by generating hypothetical matches and outliers using a far-from-equilibrium based stochastic rule. Experiments on a wide range of real data have shown the effectiveness of the proposed method and its ability to achieve reliable feature point matching.	extremal optimization;singular value decomposition	Souham Meshoul;Mohamed Batouche	2003	IJPRAI	10.1142/S0218001403002782	extremal optimization;mathematical optimization;combinatorics;discrete mathematics;combinatorial optimization;computer science;artificial intelligence;3-dimensional matching;mathematics;singular value decomposition	Vision	35.48518390819557	-38.406417044592594	50482
15d653972d176963ef0ad2cc582d3b35ca542673	csvideonet: a real-time end-to-end learning framework for high-frame-rate video compressive sensing		"""This paper addresses the real-time encoding-decoding problem for high-frame-rate video compressive sensing (CS). Unlike prior works that perform reconstruction using iterative optimization-based approaches, we propose a noniterative model, named """"CSVideoNet"""", which directly learns the inverse mapping of CS and reconstructs the original input in a single forward propagation. To overcome the limitations of existing CS cameras, we propose a multi-rate CNN and a synthesizing RNN to improve the trade-o. between compression ratio (CR) and spatial-temporal resolution of the reconstructed videos. the experiment results demonstrate that CSVideoNet signi.cantly outperforms state-of-the-art approaches. Without any pre/post-processing, we achieve a 25dB Peak signal-to-noise ratio (PSNR) recovery quality at 100x CR, with a frame rate of 125 fps on a Titan X GPU. Due to the feedforward and high-data-concurrency natures of CSVideoNet, it can take advantage of GPU acceleration to achieve three orders of magnitude speed-up over conventional iterative-based approaches. We share the source code at https://github.com/PSCLab-ASU/CSVideoNet."""	artificial neural network;compressed sensing;concurrency (computer science);data compression ratio;deep learning;end-to-end principle;feed forward (control);feedforward neural network;graphics processing unit;high- and low-level;ibm notes;internet information services;iterative method;long short-term memory;mathematical optimization;object detection;peak signal-to-noise ratio;random neural network;real-time clock;real-time transcription;software propagation;sparse matrix;titan;video post-processing	Kai Xu;Fengbo Ren	2018	2018 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2018.00187	machine learning;computer vision;compressed sensing;artificial intelligence;iterative reconstruction;feature extraction;feed forward;motion estimation;source code;computer science;frame rate;image resolution	Vision	24.92008606732335	-51.54941594915817	50518
6494625d075dd8190efa1be6911656b86d0c4e9b	single image haze removal using a generative adversarial network		Single image haze removal is an under constrained problem due to lack of depth information. It is usually performed by estimating the transmission map directly or by using a prior. Other methods use predictive models to estimate the transmission map and perform guided dehazing. In this paper, we propose a conditional GAN, that can directly remove haze from an image, without explicitly estimating transmission map or haze relevant features. We find that, only one module, comprising of the generator and discriminator is enough. We replaced the classic U-Net with the Tiramisu model, yielding much higher parameter efficiency and performance. We also observe that performance during inference is dependent on the diversity of the dataset used for training. Experiments on synthetic and real world hazy images prove that our model performs competitively with the state of the art models.	algorithm;discriminator;predictive modelling;synthetic intelligence;visual comparison	O A Kadykova;Nagarajan Venkateswaran	2018	CoRR		artificial intelligence;adversarial system;machine learning;generative grammar;haze;pattern recognition;discriminator;inference;computer science	Vision	26.38489448587324	-50.28794660960643	50673
28afc068261c2259bc3e5c0bbb79d9e07fac1957	ingredient separation of natural images: a multiple transform domain method based on sparse coding strategy	natural images;posterior distribution;information processing;sparse coding;image modeling	'Sparse coding' is a ubiquitous strategy employed in the sensory information process system of mammals. Such strategy aims to find a representation of data in which the components of the representation are only rarely significantly active. This paper presents a multiple transform domain image model and demonstrates that it may be used to separate natural images into different ingredients based on the sparse coding strategy. In such model an overcomplete dictionary is constructed by combining different type of complete or over-complete systems that can respectively deal with different image ingredients. Based on a sparse prior restriction, decomposition coefficients are inferred by maximizing a posterior distribution. The resulting coefficients belonging to different systems correspond to different image ingredients. The proposed multiple transform domain image model provides a flexible framework for image ingredient separation which allows one to extract image structure of special interest.	neural coding;sparse	Xi Tan	2007		10.1007/978-3-540-74260-9_67	computer vision;information processing;k-svd;machine learning;pattern recognition;sparse approximation;mathematics;posterior probability;neural coding;statistics	Vision	26.866868885877317	-45.53079181051829	50687
1697753a213b7ecdd27bf4e43bfb45fdf2f249a1	learning full-range affinity for diffusion-based saliency detection	saliency detection;semi supervised learning;object detection graph theory image enhancement learning artificial intelligence;graph based diffusion;semi supervised learning saliency detection graph based diffusion affinity learning;ranking based diffusion diffusion based saliency detection salient object detection diffusion based techniques graph based diffusion scheme affinity learning based diffusion ald;affinity learning;image edge detection symmetric matrices computational modeling detectors image color analysis manifolds benchmark testing	In this paper we address the issue of enhancing salient object detection through diffusion-based techniques. For reliably diffusing the energy from labeled seeds, we propose a novel graph-based diffusion scheme called affinity learning-based diffusion (ALD), which is based on learning full-range affinity between two arbitrary graph nodes. The method differs from the previous existing work where implicit diffusion was formulated as a ranking problem on a graph. In the proposed method, the affinity learning is achieved in a unified graph-based semi-supervised manner, whose outcome is leveraged for global propagation. By properly selecting an affinity learning model, the proposed ALD outperforms the ranking-based diffusion in terms of accurately detecting salient objects and enhancing the correct salient objects under a range of background scenarios. By utilizing the ALD, we propose an enhanced saliency detector that outperforms 7 recent state-of-the-art saliency models on 3 benchmark datasets.	affinity analysis;atomic layer deposition;benchmark (computing);full-range speaker;object detection;semi-supervised learning;semiconductor industry;sensor;software propagation	Keren Fu;Irene Y. H. Gu;Jie Yang	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472012	semi-supervised learning;computer vision;computer science;machine learning;pattern recognition	Vision	26.97294048891328	-46.23705398469703	50694
1f41bb748ef6041631b7e282fc42db0b31d9c8ec	convex and network flow optimization for structured sparsity	matrix factorization;statistical machine learning;alternating direction method of multipliers;optimization technique;gradient method;natural images;convex optimization;proximal methods;science learning;structured sparsity;multi task learning;tree structure;background subtraction;polynomial time;higher dimensions;learning problems;image denoising;network flow;network flow optimization;sparse coding	We consider a class of learning problems regularized by a str uctu ed sparsity-inducing norm defined as the sum of l2or l∞-norms over groups of variables. Whereas much effort has bee n put in developing fast optimization techniques when the groups are disjoint or embedded in a hierarchy, we address here the case of general overlapping groups. To this end, we present two different strategies: On the one hand, we show that the proximal operat or associated with a sum of l∞norms can be computed exactly in polynomial time by solving a qu dratic min-cost flow problem , allowing the use of accelerated proximal gradient methods. On the other hand, we use proximal splitting techniques, and address an equivalent formulati on with non-overlapping groups, but in higher dimension and with additional constraints. We propo se efficient and scalable algorithms exploiting these two strategies, which are significantly fa ster than alternative approaches. We illustrate these methods with several problems such as CUR matrix f ctorization, multi-task learning of tree-structured dictionaries, background subtraction in video sequences, image denoising with wavelets, and topographic dictionary learning of natural i mage patches.	algorithm;background subtraction;computation;computer multitasking;dictionary;embedded system;experiment;fast fourier transform;flow network;machine learning;mathematical optimization;maxima and minima;minimum-cost flow problem;multi-task learning;noise reduction;polynomial;proximal gradient method;proximal gradient methods for learning;proximal operator;scalability;sparse matrix;time complexity;topography;wavelet	Julien Mairal;Rodolphe Jenatton;Guillaume Obozinski;Francis R. Bach	2011	Journal of Machine Learning Research		time complexity;multi-task learning;mathematical optimization;convex optimization;flow network;background subtraction;computer science;gradient method;theoretical computer science;machine learning;mathematics;tree structure;neural coding;matrix decomposition	ML	26.438291913250467	-39.78026099760306	50849
b26ecc924388dd8c82d17e64ed0f248037d6dbec	neural network image analysis for environmental protection	neural network;image analysis;environmental protection	In the following paper, new approaehes are described for the Classification and the Cluster analysis of multispectral Landsat TM datas using neural networks. First, the fundamental aspects of the neural networks used for this purpose and their advantages for nongaussian distributed density functions in feature Space are outlined. Furthermore, the explored network topologies and models are presented. For Classification, back propagation networks under supervised training are used at the pixel and texture level. For Cluster analysis, however, a generalized self-organizing Kohonen Map has been chosen. The resulting information can be visualized by directly displaying the neural activity mapped onto the RGB colour Space. Due to the topological ordering, the similarity of pixel colours identifies similar properties in the feature space.	artificial neural network;image analysis	Markus H. Gross;Frank Seibert	1991			multispectral image;topological sorting;artificial neural network;pixel;network topology;self-organizing map;backpropagation;feature vector;artificial intelligence;pattern recognition;computer science	ML	31.731087447254566	-44.46017911804189	50937
869df009324e80b8daf79319c19f68994177118a	detection and summarization of salient events in coastal environments	video signal processing computer animation marine engineering object detection object recognition video cameras;object recognition;boats cameras dictionaries vehicles training covariance matrix sea measurements;video signal processing;training;massachusetts salient event detection salient event summarization coastal environment monitoring video cameras video analysis maritime scenes background animation water reflections waves field of view boats motor vehicles people region of interest localization roi localization behavior subtraction roi validation feature covariance based object recognition video condensation great point nantucket;video cameras;dictionaries;marine engineering;vehicles;computer animation;cameras;object detection;covariance matrix;sea measurements;boats	The monitoring of coastal environments is of great interest to biologists and environmental protection organizations with video cameras being the dominant sensing modality. However, it is recognized that video analysis of maritime scenes is very challenging on account of background animation (water reflections, waves) and very large field of view. We propose a practical approach to the detection of three salient events, namely boats, motor vehicles and people appearing close to the shoreline, and their subsequent summarization. Our approach consists of three fundamental steps: region-of-interest (ROI) localization by means of behavior subtraction, ROI validation by means of feature-covariance-based object recognition, and event summarization by means of video condensation. The goal is to distill hours of video data down to a few short segments containing only salient events, thus allowing human operators to expeditiously study a coastal scene. We demonstrate the effectiveness of our approach on long videos taken at Great Point, Nantucket, Massachusetts.	algorithm;automatic summarization;dictionary;human–computer interaction;learning classifier system;modality (human–computer interaction);multi-core processor;nearest-neighbor interpolation;outline of object recognition;population;real-time computing;real-time transcription;reflection (computer graphics);region of interest;video content analysis;video processing	Daniel Cullen;Janusz Konrad;Thomas D. C. Little	2012	2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance	10.1109/AVSS.2012.35	computer vision;covariance matrix;simulation;computer science;automatic summarization;cognitive neuroscience of visual object recognition;computer animation;statistics;computer graphics (images)	Vision	39.08848958947674	-46.309191250857246	51082
267dca58c46d5d8b42d755d4b13f818f67b78675	image denoising with patch based pca: local versus global	image denoising	In recent years, overcomplete dictionaries combined with sparse learning techniques became extremely popular in computer vision. While their usefulness is undeniable, the improvement they provide in specific tasks of computer vision is still poorly understood. The aim of the present work is to demonstrate that for the task of image denoising, nearly state-of-the-art results can be achieved using orthogonal dictionaries only, provided that they are learned directly from the noisy image. To this end, we introduce three patchbased denoising algorithms which perform hard thresholding on the coefficients of the patches in image-specific orthogonal dictionaries. The algorithms differ by the methodology of learning the dictionary: local PCA, hierarchical PCA and global PCA. We carry out a comprehensive empirical evaluation of the performance of these algorithms in terms of accuracy and running times. The results reveal that, despite its simplicity, PCA-based denoising appears to be competitive with the state-of-the-art denoising algorithms, especially for large images and moderate signal-to-noise ratios.	algorithm;dictionary;inpainting;nl (complexity);noise reduction;texture synthesis	Charles-Alban Deledalle;Joseph Salmon;Arnak S. Dalalyan	2011		10.5244/C.25.25	computer vision;computer science;machine learning;pattern recognition;mathematics	Vision	28.44070714714688	-46.93159493195301	51184
c47bba0c1fd82e14661fa98076b90085cb8832a4	conditional random fields for urban scene classification with full waveform lidar data	urban;3d point cloud;conditional random fields;classification;full waveform lidar	We propose a context-based classification method for point clouds acquired by full waveform airborne laser scanners. As these devices provide a higher point density and additional information like echo width or type of return, an accurate distinction of several object classes is possible. However, especially in dense urban areas correct labelling is a challenging task. Therefore, we incorporate context knowledge by using Conditional Random Fields. Typical object structures are learned in a training step and improve the results of the point-based classification process. We validate our approach with two real-world datasets and by a comparison to Support Vector Machines and Markov Random Fields. This contribution was selected in a double blind review process to be published within the Lecture Notes in Computer Science series (Springer-Verlag, Heidelberg). Photogrammetric Image Analysis Volume Editors: Stilla U, Rottensteiner F, Mayer H, Jutzi B, Butenuth M LNCS Volume: 6952 Series Editors: Hutchison D, Kanade T, Kittler J, Kleinberg JM, Kobsa A, Mattern F, Mitchell JC, Naor M, Nierstrasz O, Pandu Rangan C, Steffen B, Sudan M, Terzopoulos D, Tygar D, Weikum G ISSN: 0302-9743 The article is accessible online through www.springerlink.com. 47 In: Stilla U et al (Eds) PIA11. International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences 38 (3/W22)	airborne ranger;conditional random field;friedrich kittler;image analysis;international standard serial number;kerry mitchell;lecture notes in computer science;markov chain;markov random field;photogrammetry;point cloud;pulse-width modulation;springer (tank);statistical classification;support vector machine;waveform	Joachim Niemeyer;Jan Dirk Wegner;Clément Mallet;Franz Rottensteiner;Uwe Soergel	2011		10.1007/978-3-642-24393-6_20	computer vision;computer science;data mining;remote sensing	Vision	35.509473168272585	-41.66175035618326	51199
925ae207db6a8f963a3d3d1de35ed9fdf9efe6f9	multi-feature adaptive classifiers for sar image segmentation	ensemble classification;image segmentation;neural networks;ensemble method;data fusion;feature extraction;sar image;neural network	We propose a multifeature scheme for terrain classification in SAR image analysis. Different neural classifiers, trained on different features of the same sample space, are combined by using a non-linear ensemble method. The feature extraction modules are chosen in order to discover the textural and contextual characteristics within the neighbourhood of each pixel. Comparisons with classical data fusion techniques and consensus schema are reported.	image segmentation	Michele Ceccarelli;Alfredo Petrosino	1997	Neurocomputing	10.1016/S0925-2312(96)00038-0	computer vision;feature extraction;computer science;machine learning;pattern recognition;sensor fusion;image segmentation;artificial neural network	Vision	31.677334703046583	-44.32132579639833	51586
a1a554113dd5d38b69c0906a42077602ca41f25a	leveliw: learning extreme verification latency with importance weighting		Nonstationary streaming data are characterized by changes in the underlying distribution between subsequent time steps. Learning in such environments becomes even more challenging when labeled data are available only at the initial time step, and the algorithm is provided unlabeled data thereafter, a scenario referred to as extreme verification latency. Our previously introduced COMPOSE framework works very well in such settings. COMPOSE is a semi-supervised approach that iteratively labels strategically chosen instances of the next time step using the instances it labeled in the previous time step. COMPOSE originally assumed a significant distribution overlap at consecutive time steps, allowing instances lying in the center of the feature space to be used as the most representative labeled instances from current time step to help label the new data at the next time step. Such an assumption is also inherent in importance weighting based domain adaptation, but only for a single time step with mismatched train and test data distributions. We explore importance weighting not for a single time step matching training / test distributions, but rather matching distributions between two consecutive time steps, and estimate the posterior distribution of the unlabeled data using importance weighted least squares probabilistic classifier. The estimated labels are then iteratively used as the training data for the next time step. We call this algorithm as LEVELIW, Learning Extreme VErification Latency with Importance Weighting. Our primary goal in doing so is to determine if and when importance weighting provides an advantage over COMPOSE's core support extraction, and whether it provides an alternate solution with reduced parameter sensitivity. Several datasets are used to compare the two approaches, which produced some unique insights.	algorithm;concept drift;domain adaptation;emoticon;feature vector;importance sampling;interrupt latency;least squares;optimization problem;sampling (signal processing);semi-supervised learning;semiconductor industry;stream (computing);streaming media;synthetic intelligence;test data	Muhammad Umer;Robi Polikar;Christopher Frederickson	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966061		ML	31.67791376798786	-47.77377041919583	51629
9ad36b013d58ae17dc190fabff7db0dc4ed315dd	label propagation on facial images using similarity and dissimilarity labelling constraints		In this paper, a novel multimedia data (specifically facial images) label propagation method is presented that is based on the inclusion of labelling constraints in the objective function of the MLPP-CLP state of the art algorithm. The proposed method can incorporate pairwise facial image similarity and dissimilarity constraints into the objective function of the aforementioned method. Experiments which have been conducted on facial image labelling in three stereoscopic movies, confirm the increased labelling accuracy of the proposed method.		Efstratios Kakaletsis;Olga Zoidi;Ioannis Tsingalis;Anastasios Tefas;Nikos Nikolaidis;Ioannis Pitas	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451066	computer vision;labelling;stereoscopy;pattern recognition;artificial intelligence;computer science;linear programming;pairwise comparison	Vision	26.92843041253924	-46.47329787432052	51814
26499aeae6e55adef4d7d7ad8def8a8c48f9048b	temporal matching kernel with explicit feature maps	multiple kernel;time consistent query expansion;event detection;explicit feature map;video synchronization;temporal match kernel	This paper proposes a framework for content-based video retrieval that addresses various tasks as particular event retrieval, copy detection or video synchronization. Given a video query, the method is able to efficiently retrieve, from a large collection, similar video events or near-duplicates with temporarily consistent excerpts. As a byproduct of the representation, it provides a precise temporal alignment of the query and the detected video excerpts.  Our method converts a series of frame descriptors into a single visual-temporal descriptor, called a temporal invariant match kernel. This representation takes into account the relative positions of the visual frames: the frame descriptors are jointly encoded with their timestamps. When matching two videos, the method produces a score function for all possible relative timestamps, which is maximized to obtain both the similarity score and the relative time offset.  Then, we propose two complementary contributions to further improve the detection and localization performance.The first is a novel query expansion method that takes advantage of the joint descriptor/timestamp representation to automatically align the first result set and produce an enriched temporal query. In contrast to other query expansion methods proposed for videos, it preserves the localization capability. Second, we improve the localization trade-off between quality and representation size by using several complementary temporal match kernels.  We evaluate our approach on benchmarks for particular event retrieval, copy detection and video synchronization. Our experiments show that our approach achieve excellent detection and localization results.	align (company);benchmark (computing);experiment;internationalization and localization;kernel (operating system);query expansion;result set;trusted timestamping	Sébastien Poullot;Shunsuke Tsukatani;Phuong Anh Nguyen;Hervé Jégou;Shin'ichi Satoh	2015		10.1145/2733373.2806228	computer vision;computer science;theoretical computer science;machine learning;pattern recognition;information retrieval	Vision	32.75651863749411	-51.38096495740364	51824
d22a270ec055216580164b4f297089845065ff23	gwcnn: a metric alignment layer for deep shape analysis		Deep neural networks provide a promising tool for incorporating semantic information in geometry processing applications. Unlike image and video processing, however, geometry processing requires handling unstructured geometric data, and thus data representation becomes an important challenge in this framework. Existing approaches tackle this challenge by converting point clouds, meshes, or polygon soups into regular representations using, e.g., multi-view images, volumetric grids or planar parameterizations. In each of these cases, geometric data representation is treated as a fixed pre-process that is largely disconnected from the machine learning tool. In contrast, we propose to optimize for the geometric representation during the network learning process using a novel metric alignment layer. Our approach maps unstructured geometric data to a regular domain by minimizing the metric distortion of the map using the regularized Gromov–Wasserstein objective. This objective is parameterized by the metric of the target domain and is differentiable; thus, it can be easily incorporated into a deep network framework. Furthermore, the objective aims to align the metrics of the input and output domains, promoting consistent output for similar shapes. We show the effectiveness of our layer within a deep network trained for shape classification, demonstrating state-of-the-art performance for nonrigid shapes.	algorithm;align (company);artificial neural network;convolution;data (computing);deep learning;definition;distortion;end-to-end principle;euclidean distance;feature learning;geometry processing;input/output;loss function;machine learning;map;planar (computer graphics);point cloud;polygon soup;preprocessor;shape analysis (digital geometry);video processing	Danielle Ezuz;Justin Solomon;Vladimir G. Kim;Mirela Ben-Chen	2017	Comput. Graph. Forum	10.1111/cgf.13244	computer vision;theoretical computer science;artificial intelligence;computer science;local feature size;computational geometry;shape analysis (digital geometry);geometric modeling;geometric primitive	ML	26.015765364206395	-49.24497799918481	52063
0621bc13813563ecb26e2897f746cf8f7f207651	human activity prediction based on sub-volume relationship descriptor	convolutional codes;training;computer vision;computational modeling;feature extraction;videos;activity recognition	In this paper, we address the problem of recognizing unfinished human activity from partially observed videos. Specifically, we propose a novel human activity descriptor, which can represent pairwise relationships among human activities in a compact manner using pre-trained Convolutional Neural Networks (CNNs) by capturing the discriminative sub-volume. The potentially important relationship among all pairwise sub-volumes, called key-volumes, is automatically captured using global and local motion activation and the ratio of the participant. The captured key-volumes without prior knowledge hold discriminative information related to the unfinished activity. The key-volume information is considered in the descriptor construction procedure. Training a CNN model for a particular purpose requires a lot of resources, such as large amount of labeled data and computing power, despite its representational power. Thus, we develop a method to utilize pre-trained CNN without any additional model training procedure. The low-level features can be extracted through existing CNN toolkits. For a real application, the proposed method may be more cost-effective while implementing a smart surveillance system to understand human activity. In our experiments, we compare the performances of the proposed method with other state-of-the-art human activity prediction methods for two public datasets; the results of the experiments show that the proposed method outperforms these competing methods.	activity recognition;computer vision;convolutional neural network;experiment;high- and low-level;information retrieval;interaction;list of toolkits;performance;smart tv;streaming media	Dong-Gyu Lee;Seong-Whan Lee	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899939	computer vision;convolutional code;feature extraction;computer science;machine learning;data mining;computational model;activity recognition	Vision	32.41972692093221	-49.46390339764968	52079
d6791b98353aa113d79f6fb96335aa6c7ea3b759	collaborative random faces-guided encoders for pose-invariant face representation learning		Learning discriminant face representation for pose-invariant face recognition has been identified as a critical issue in visual learning systems. The challenge lies in the drastic changes of facial appearances between the test face and the registered face. To that end, we propose a high-level feature learning framework called “collaborative random faces (RFs)-guided encoders” toward this problem. The contributions of this paper are three fold. First, we propose a novel supervised autoencoder that is able to capture the high-level identity feature despite of pose variations. Second, we enrich the identity features by replacing the target values of conventional autoencoders with random signals (RFs in this paper), which are unique for each subject under different poses. Third, we further improve the performance of the framework by incorporating deep convolutional neural network facial descriptors and linking discriminative identity features from different RFs for the augmented identity features. Finally, we conduct face identification experiments on Multi-PIE database, and face verification experiments on labeled faces in the wild and YouTube Face databases, where face recognition rate and verification accuracy with Receiver Operating Characteristic curves are rendered. In addition, discussions of model parameters and connections with the existing methods are provided. These experiments demonstrate that our learning system works fairly well on handling pose variations.	align (company);artificial neural network;autoencoder;biological neural networks;convolutional neural network;database;discriminant;encoder;encoder device component;experiment;face;facial recognition system;feature learning;fold (higher-order function);handling (psychology);high- and low-level;machine learning;one-to-many (data model);receiver operating characteristic;registration;sparse matrix;tracer;verification of theories;visual learning;anatomical layer	Ming Shao;Yizhe Zhang;Yun Fu	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2017.2648122	artificial intelligence;discriminative model;convolutional neural network;autoencoder;machine learning;receiver operating characteristic;facial recognition system;pattern recognition;feature learning;computer science;invariant (mathematics);visual learning	Vision	25.6275988133629	-49.67552891979413	52226
539923c8f2f4641f71056b71e5628d1b9b633835	mining actionlet ensemble for action recognition with depth cameras	image recognition;image motion analysis;depth camera;occlusion;tracked joints;intraclass variation;human object interaction;actionlet ensemble model;joints;image sensors;drntu engineering electrical and electronic engineering;conference paper;hidden markov models;feature extraction;human motion;joints humans hidden markov models cameras robustness noise feature extraction;human action recognition;object tracking;3d position;mocap system;mocap system actionlet ensemble mining human action recognition commodity depth sensor depth map depth camera 3d position tracked joints occlusion intraclass variation actionlet ensemble model human motion human object interaction;robustness;humans;commodity depth sensor;object tracking cameras image motion analysis image recognition image sensors;depth map;cameras;actionlet ensemble mining;noise	Human action recognition is an important yet challenging task. The recently developed commodity depth sensors open up new possibilities of dealing with this problem but also present some unique challenges. The depth maps captured by the depth cameras are very noisy and the 3D positions of the tracked joints may be completely wrong if serious occlusions occur, which increases the intra-class variations in the actions. In this paper, an actionlet ensemble model is learnt to represent each action and to capture the intra-class variance. In addition, novel features that are suitable for depth data are proposed. They are robust to noise, invariant to translational and temporal misalignments, and capable of characterizing both the human motion and the human-object interactions. The proposed approach is evaluated on two challenging action recognition datasets captured by commodity depth cameras, and another dataset captured by a MoCap system. The experimental evaluations show that the proposed approach achieves superior performance to the state of the art algorithms.	algorithm;depth map;depth perception;experiment;interaction;kinesiology;microsoft research;motion capture;neural ensemble;sensor	Jiang Wang;Zicheng Liu;Ying Wu;Junsong Yuan	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247813	computer vision;simulation;feature extraction;computer science;noise;machine learning;video tracking;image sensor;hidden markov model;robustness;depth map	Vision	36.13405242236589	-47.971625928921696	52254
0de89455c24c7bc2564f56efa5ba22baa1a3065d	spatio-temporal action localization and detection for human action recognition in big dataset	selective temporal segmentation;dense surf;interest points trajectory;spatio temporal action detection;action recognition;optical flow	Human action recognition is still attracting the computer vision research community due to its various applications. However, despite the variety of methods proposed to solve this problem, some issues still need to be addressed. In this paper, we present a human action detection and recognition process on large datasets based on Interest Points trajectories. In order to detect moving humans in moving field of views, a spatio-temporal action detection is performed basing on optical flow and dense speed-up-robust-features (SURF). Then, a video description based on a fusion process that combines motion, trajectory and visual descriptors is proposed. Features within each bounding box are extracted by exploiting the bag-of-words approach. Finally, a support-vector-machine is employed to classify the detected actions. Experimental results on the complex benchmark UCF101, KTH and HMDB51 datasets reveal that the proposed technique achieves better performances compared to some of the existing state-of-the-art action recognition approaches.	action potential;algorithmic efficiency;audio description;automatic number plate recognition;autoregressive integrated moving average;bag-of-words model;benchmark (computing);big data;computation;computer vision;data descriptor;end-to-end principle;kinesiology;minimum bounding box;optical flow;performance;preprocessor;speeded up robust features;support vector machine;visual descriptor	Sameh Megrhi;Marwa Jmal;Wided Souidène;Azeddine Beghdadi	2016	J. Visual Communication and Image Representation	10.1016/j.jvcir.2016.10.016	computer vision;computer science;artificial intelligence;data mining;optical flow	Vision	36.113009388955234	-50.427737647370186	52255
31e57fa83ac60c03d884774d2b515813493977b9	face alignment with cascaded semi-parametric deep greedy neural forests		Face alignment is an active topic in computer vision, consisting in aligning a shape model on the face. To this end, most modern approaches refine the shape in a cascaded manner, starting from an initial guess. Those shape updates can either be applied in the feature point space (i.e. explicit updates) or in a low-dimensional, parametric space. In this paper, we propose a semi-parametric cascade that first aligns a parametric shape, then captures more finegrained deformations of an explicit shape. For the purpose of learning shape updates at each cascade stage, we introduce a deep greedy neural forest (GNF) model, which is an improved version of deep neural forest (NF). GNF appears as an ideal regressor for face alignment, as it combines differentiability, high expressivity and fast evaluation runtime. The proposed framework is very fast and achieves high accuracies on multiple challenging benchmarks, including small, medium and large pose experiments.	baseline (configuration management);benchmark (computing);computer vision;dimensionality reduction;experiment;greedy algorithm;greibach normal form;high- and low-level;kerrison predictor;parametric model;semiconductor industry;sparse matrix;top-down and bottom-up design	Arnaud Dapogny;Kevin Bailly;Séverine Dubuisson	2018	Pattern Recognition Letters	10.1016/j.patrec.2017.12.010	computer vision;artificial intelligence;machine learning;pattern recognition;mathematics;statistics	Vision	25.935249750238224	-48.44730912274546	52322
f4d97941daa58acb591e99530e83361891ed31fa	tree-based structural twin support tensor clustering with square loss function		Most of the real-life applications involving images, videos etc. deals with matrix data (second order tensor space). Tensor based clustering models can be utilized for identifying patterns in matrix data as they take advantage of structural information in multi-dimensional framework and reduce computational overheads as well. Despite such numerous advantages, tensor clustering has still remained relatively unexplored research area. In this paper, we propose a novel clustering technique, termed as Treebased Structural Least Squares Twin Support Tensor Clustering (Tree-SLSTWSTC), that builds a cluster model as a binary tree, where each node comprises of proposed Structural Least Squares Twin Support Tensor Machine (S-LSTWSTM) classifier that considers the structural risk minimization of data alongside a symmetrical L2-norm loss function. The proposed approach results in time-efficient learning. Initialization framework based on tensor (k{-})means has been proposed and implemented in order to overcome the instability disseminated by random initialization. To validate the efficacy of the proposed framework, computational experiments have been performed with relevant tensor based models on face recognition and optical digit recognition datasets.	loss function	Reshma Rastogi;Sweta Sharma	2017		10.1007/978-3-319-69900-4_4	binary tree;tensor;computer science;pattern recognition;machine learning;artificial intelligence;cluster analysis;initialization;unsupervised learning;instability;structural risk minimization;least squares	Vision	24.67851022070664	-42.120995159530636	52517
dcf4f6e27fba24c0c451059b9b8276039f7ea7b3	topology representing networks for intrinsic dimensionality estimation	traitement signal;learning algorithm;analisis forma;algorithme apprentissage;signal processing;pattern recognition;pattern analysis;reconnaissance forme;topology preservation;reconocimiento patron;algoritmo optimo;algorithme optimal;optimal algorithm;algoritmo aprendizaje;procesamiento senal;analyse forme	In this paper we compare two methods for intrinsic dimen-sionality (ID) estimation based on optimally topology preserving maps (OTPMs). The rst one is a direct approach, where the intrinsic di-mensionality is estimated directly from the OTPM. We argue that this approach suuers from both practical and theoretical pitfalls. The second is a new approach which combines OTPMs with an eecient local principal component analysis (PCA). Exploiting the OTPM, local PCA can be shown to have only linear time complexity w.r.t. the dimensionality of the input space (in contrast to the prohibitive cubic complexity of the conventional approach), and hence the method becomes applicable even for very high dimensional input spaces as frequently encountered in computer vision. A local ID estimate is then obtained as the local number of signiicant eigenvalues. In addition to ID estimation the local subspaces as revealed by our local PCA can be directly used for further data processing tasks including classiication and regression. The workability of the new approach for ID estimation and subspace auto-association is demonstrated on a sequence of 64 64 pixel images (4096-dimensional input space).	algorithm;approximation;computational complexity theory;computer vision;cubic function;data modeling;hyper basis function network;intrinsic dimension;map;pixel;principal component analysis;time complexity;visual learning	Jörg Bruske;Gerald Sommer	1997		10.1007/BFb0020219	computer science;artificial intelligence;machine learning;signal processing;mathematics;algorithm	ML	29.12209809070422	-38.958901158613976	52863
8a5099b2ae6912b4df22534a1b3065e147c38b9c	face hallucination with tiny unaligned images by transformative discriminative neural networks		Conventional face hallucination methods rely heavily on accurate alignment of low-resolution (LR) faces before upsampling them. Misalignment often leads to deficient results and unnatural artifacts for large upscaling factors. However, due to the diverse range of poses and different facial expressions, aligning an LR input image, in particular when it is tiny, is severely difficult. To overcome this challenge, here we present an end-to-end transformative discriminative neural network (TDN) devised for super-resolving unaligned and very small face images with an extreme upscaling factor of 8. Our method employs an upsampling network where we embed spatial transformation layers to allow local receptive fields to line-up with similar spatial supports. Furthermore, we incorporate a class-specific loss in our objective through a successive discriminative network to improve the alignment and upsampling performance with semantic information. Extensive experiments on large face datasets show that the proposed method significantly outperforms the state-of-the-art.	align (company);artificial neural network;end-to-end principle;experiment;face hallucination;lr parser;upsampling	Xin Yu;Fatih Murat Porikli	2017			machine learning;discriminative model;upsampling;computer science;small face;artificial neural network;receptive field;transformative learning;computer vision;facial expression;face hallucination;pattern recognition;artificial intelligence	AI	25.56638268407238	-49.86849397255768	53028
677b3e2d3d506b53241880360f2a23012e55646a	indexicality and dynamic attention control in qualitative recogniton of assembly actions	attentional control	Visual recognition of physical actions requires temporal segmentation and identification of action types. Action concepts are analyzed into attention, context, and change. Temporal segmentation is defined as a context switch detected by a switching of attention. Actions are identified by detecting “indexical” features which can be quickly calculated from visual features and directly point to action concepts. Validity of the indexicality depends on the attention and the context. These are maintained by three types of attention control: spatial, temporal and hierarchical. They are combined by a mechanism called “attention stack”, which extends at important points and winds up elsewhere. An action recognizer built upon the framework successfully recognized human assembly action sequences in real time and output qualitative descriptions of the tasks.		Yasuo Kuniyoshi;Hirochika Inoue	1992		10.1007/3-540-55426-2_101	attentional control;computer science	Robotics	38.13655611156139	-47.82038679600255	53124
2f52b969750bf1687d0a0d9dd6fa96dc8e3f440f	collaborative sources identification in mixed signals via hierarchical sparse modeling	instruments;convex programming;pitch independent method collaborative source identification mixed signal detection hierarchical sparse modeling convex optimization problem collaborative filtering c hilasso instrument identification speaker identification texture separation audio signals harmonic sounds;signal detection;collaboration;convex optimization;power spectrum;discrete cosine transform;source identification;hamming distance;automatic detection;collaborative filtering;discrete cosine transforms;feature extraction;signal representation;dictionaries;pattern recognition;signal representation convex programming filtering theory signal detection;collaboration dictionaries instruments encoding feature extraction hamming distance discrete cosine transforms;sparse representation;encoding;filtering theory	A collaborative framework for detecting the different sources in mixed signals is presented in this paper. The approach is based on C-HiLasso, a convex collaborative hierarchical sparse model, and proceeds as follows. First, we build a structured dictionary for mixed signals by concatenating a set of sub-dictionaries, each one of them learned to sparsely model one of a set of possible classes. Then, the coding of the mixed signal is performed by efficiently solving a convex optimization problem that combines standard sparsity with group and collaborative sparsity. The present sources are identified by looking at the sub-dictionaries automatically selected in the coding. The collaborative filtering in C-HiLasso takes advantage of the temporal/spatial redundancy in the mixed signals, letting collections of samples collaborate in identifying the classes, while allowing individual samples to have different internal sparse representations. This collaboration is critical to further stabilize the sparse representation of signals, in particular the class/sub-dictionary selection. The internal sparsity inside the sub-dictionaries, as naturally incorporated by the hierarchical aspects of C-HiLasso, is critical to make the model consistent with the essence of the sub-dictionaries that have been trained for sparse representation of each individual class. We present applications from speaker and instrument identification and texture separation. In the case of audio signals, we use sparse modeling to describe the short-term power spectrum envelopes of harmonic sounds. The proposed pitch independent method automatically detects the number of sources on a recording.	collaborative filtering;concatenation;convex optimization;dictionary;mathematical optimization;mixed-signal integrated circuit;optimization problem;sensor;sparse approximation;sparse matrix;spectral density	Pablo Sprechmann;Ignacio Ramírez;Pablo Cancela;Guillermo Sapiro	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947683	computer vision;convex optimization;hamming distance;speech recognition;feature extraction;computer science;collaborative filtering;machine learning;discrete cosine transform;pattern recognition;sparse approximation;mathematics;spectral density;encoding;detection theory;collaboration	ML	31.551918480010617	-40.09037188001245	53128
42d583c48e6f97ce990e5426c9c65cac269b271b	local integrity constraints for structure detection and segmentation in high-resolution earth observation images	pairwise learning object retrieval and segmentation interactive learning;training image segmentation optimization shape object detection image color analysis standards;image segmentation feature extraction image resolution;local integrity constraints semantic structure extraction local structural integrity high resolution earth observation images structure segmentation structure detection	Considering the idea that objects in images have a higher local structural integrity than the background they lie into, we propose a method that learns a supervised distance characterizing the membership of a pair of elements to the target structure. We test our ideas by applying them to the task of extracting semantic structures in high resolution Earth Observation images. The results show that the method works well in many situations when there is no training dataset. The limits of the method are also discussed.	data integrity;image resolution;structural integrity and failure	Pierre Blanchart;Marin Ferecatu	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7350823	image texture;computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Robotics	31.37241354027487	-44.845379315116034	53468
267595dd40cd109c93e67874a1cf49ce79871f3a	a compromise principle in deep monocular depth estimation		Monocular depth estimation, which plays a key role in understanding 3D scene geometry, is fundamentally an illposed problem. Existing methods based on deep convolutional neural networks (DCNNs) have examined this problem by learning convolutional networks to estimate continuous depth maps from monocular images. However, we find that training a network to predict a high spatial resolution continuous depth map often suffers from poor local solutions. In this paper, we hypothesize that achieving a compromise between spatial and depth resolutions can improve network training. Based on this “compromise principle”, we propose a regression-classification cascaded network (RCCN), which consists of a regression branch predicting a low spatial resolution continuous depth map and a classification branch predicting a high spatial resolution discrete depth map. The two branches form a cascaded structure allowing the main classification branch to benefit from the auxiliary regression branch. By leveraging large-scale raw training datasets and some data augmentation strategies, our network achieves competitive or state-of-the-art results on three challenging benchmarks, including NYU Depth V2 [1], KITTI [2], and Make3D [3].	artificial neural network;branch predictor;convolutional neural network;depth map;discrete fourier transform;experiment;model-driven engineering	Huan Fu;Mingming Gong;Chaohui Wang;Dacheng Tao	2017	CoRR		convolutional neural network;computer science;pattern recognition;computer vision;image resolution;monocular;compromise;artificial intelligence;depth map	Vision	24.8997772391608	-51.307848135219395	53498
378ee2be23ed38453ae8e00854b0a11e2ed4bb63	structured output svm for remote sensing image classification	remote sensing image;support vector machines;kernel methods;kernel function;learning support;land use;remote sensing data;loss function;visual inspection;multispectral images;non structural;urban area;kernel method;structured output learning;land use classification;high spatial resolution	In the recent years, kernel methods have revealed very powerful tools in many application domains in general and in remote sensing image classification in particular. The special characteristics of remote sensing images (high dimension, few labeled samples and different noise sources) are efficiently dealt with kernel machines. In this paper, we propose the use of structured output learning to improve remote sensing image classification based on kernels. structured output learning is concerned with the design of machine learning algorithms that not only implement input-output mapping, but also take into account the relations between output labels, thus generalizing unstructured kernel methods. We analyze the framework and introduce it to the remote sensing community. Output similarity is here encoded into SVM classifiers by modifying the model loss function and the kernel function either independently or jointly. Experiments on a very high resolution (VHR) image classification problem shows promising results and opens a wide field of research with structured output kernel methods.	algorithm;computer vision;image resolution;kernel method;loss function;machine learning;structured prediction;structured support vector machine	Devis Tuia;Jordi Muñoz-Marí;Mikhail F. Kanevski;Gustavo Camps-Valls	2009	2009 IEEE International Workshop on Machine Learning for Signal Processing	10.1007/s11265-010-0483-8	computer vision;kernel method;computer science;machine learning;pattern recognition;one-class classification	ML	30.698244742111573	-43.568501536655575	53564
0f7fdd7f98ee5fbfa7d293e0f1fa399b7a4ec13a	two-granularity tracking: mediating trajectory and detection graphs for tracking under occlusions	detection tracklet classification;detection graph;detection tracklet graph;grouping affinity;corresponding detection tracklets;detection tracklets;two-granularity tracking;point trajectory;simultaneous detection tracklet classification;mediating trajectory;non-accidental grouping alignment;inaccurate detection	We propose a tracking framework that mediates grouping cues from two levels of tracking granularities, detection tracklets and point trajectories, for segmenting objects in crowded scenes. Detection tracklets capture objects when they are mostly visible. They may be sparse in time, may miss partially occluded or deformed objects, or contain false positives. Point trajectories are dense in space and time. Their affinities integrate long range motion and 3D disparity information, useful for segmentation. Affinities may leak though across similarly moving objects, since they lack model knowledge. We establish one trajectory and one detection tracklet graph, encoding grouping affinities in each space and associations across. Two-granularity tracking is cast as simultaneous detection tracklet classification and clustering (cl) in the joint space of tracklets and trajectories. We solve cl by explicitly mediating contradictory affinities in the two graphs: Detection tracklet classification modifies trajectory affinities to reflect object specific dis-associations. Non-accidental grouping alignment between detection tracklets and trajectory clusters boosts or rejects corresponding detection tracklets, changing accordingly their classification. We show our model can track objects through sparse, inaccurate detections and persistent partial occlusions. It adapts to the changing visibility masks of the targets, in contrast to detection based bounding box trackers, by effectively switching between the two granularities according to object occlusions, deformations and background clutter.	affinity analysis;biclustering;binocular disparity;cluster analysis;clutter;consortium;error-tolerant design;limbo;minimum bounding box;robotics;sensor;sparse matrix	Katerina Fragkiadaki;Weiyu Zhang;Geng Zhang;Jianbo Shi	2012		10.1007/978-3-642-33715-4_40	computer vision;machine learning;mathematics	Vision	35.767333760205695	-47.232341200816954	54038
587807fabc45e5112cdcd9132c261b0c10afee7d	domain adaptation with hidden markov random fields	geophysical image processing;domain adaptation;manifolds;reflectivity;hyperspectral images;training;multitemporal sequences;image classification;graph matching;hidden markov random fields;hidden markov models;vector quantization;remote sensing;graph matching hidden markov random fields domain adaptation reflectance spectra multitemporal sequences hyperspectral images;hidden markov random fields multitemporal classification domain adaptation graph matching;clustering algorithms;remote sensing geophysical image processing hidden markov models hyperspectral imaging image classification image sequences reflectivity;hyperspectral imaging;manifolds training clustering algorithms hidden markov models hyperspectral imaging vector quantization;reflectance spectra;image sequences;multitemporal classification	In this paper, we propose a method to match multitemporal sequences of hyperspectral images using Hidden Markov Random Fields. Based on the matching of the data manifold, the algorithm matches the reflectance spectra of the classes, thus allowing the reuse of labeled examples acquired on one image to classify the other. This allows valorization of spectra collected in situ to other acquisitions than the one they were acquired for, without user supervision, prior knowledge of the class reflectance in the new domain or global information about atmospheric conditions.	algorithm;domain adaptation;markov chain;markov random field	Jan-Pieter Jacobs;Guy Thoonen;Devis Tuia;Gustavo Camps-Valls;Birgen Haest;Paul Scheunders	2013	2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS	10.1109/IGARSS.2013.6723485	computer vision;contextual image classification;manifold;computer science;hyperspectral imaging;machine learning;pattern recognition;mathematics;reflectivity;cluster analysis;vector quantization;hidden markov model;matching;remote sensing	Vision	31.815220326069067	-42.639896726857266	54076
6636eb51eec759e5b517d17ba64725938f6cfbf3	spin discriminant analysis (sda) - using a one-dimensional classifier for high dimensional classification problems	spline;empirical study;probability;high dimensionality;support vector machines;gaussian processes;testing;linear discriminate analysis;learning automata;discriminant analysis;ear;statistical analysis;learning automata pattern classification;binary classification high dimensional classification one dimensional classifier spin discriminant analysis covariance matrices support vector machines;covariance matrices;pattern classification;linear discriminant analysis gaussian processes covariance matrix statistical analysis probability support vector machines support vector machine classification spline testing ear;high dimensional classification;support vector machine classification;binary classification;support vector machine;linear discriminant analysis;covariance matrix;one dimensional classifier;spin discriminant analysis	In this paper we discuss how to use a one-dimensional classifier for solving high dimensional classification problems, We propose Spin Discriminant Analysis (SDA), which enables us to construct a family of new classifiers. We prove that SDA is equivalent to ridged Linear Discriminant Analysis (LDA) when two classes are Gaussians with common covariance matrices. Moreovel; we prove that classification based on Parzen's window is a special case of SDA. In addition to theoretical investigations, we conduct extensive empirical studies, implementing SDA using Support Vector Machines (SVMs) as its one-dimensional classifiers. This SVM-based SDA implementation is named SpinSVM. Our experiments show that SpinSVM outperforms traditional high dimensional classifiers like SVMs, Classification Using Spline (CUS), classification-based Parzen 's window, and LDA on most standard and synthetic datasets we tested.	experiment;linear discriminant analysis;spline (mathematics);support vector machine;synthetic intelligence	Huaxin You;Hong Hua;Narendra Ahuja;Edward Y. Chang	2001		10.1109/CVPR.2001.990635	support vector machine;computer science;machine learning;pattern recognition;mathematics;linear discriminant analysis;statistics	ML	31.0004991029584	-38.89876381509354	54244
36acbc8ef9bcca46ff9b18aa2dc972d9e0a13bf8	a method for human action recognition	large data sets;three dimensional;human tracking;motion recognition;articulated motion;action recognition;human activity	This paper deals with the problem of classification of human activities from video. Our approach uses motion features only that are computed very efficiently, and subsequently projected into a lower dimensional space where matching is performed. Each action is represented as a manifold in this lower dimensional space and matching is done by comparing these manifolds. To demonstrate the effectiveness of this approach, it was used on a large data set of similar actions, each performed by many different actors. Classification results were very accurate and show that this approach is robust to challenges such as variations in performers’ physical attributes, color of clothing, and style of motion. An important result of this paper is that the recovery of the three-dimensional properties of a moving person, or even the two-dimensional tracking of the person’s limbs need not precede action recognition.		Osama Masoud;Nikolaos Papanikolopoulos	2003	Image Vision Comput.	10.1016/S0262-8856(03)00068-4	three-dimensional space;computer vision;simulation;artificial intelligence;mathematics	Vision	36.70914551988701	-49.55156091713525	54348
eaf4accc17ac5075fcf773fc407385b69a77859a	an analysis of linear subspace approaches for computer vision and pattern recognition	singular value;singular value decomposition;first order perturbation;indexing terms;computer vision;matrix perturbation;multiple eigenvalue;first order;face recognition;linear subspaces;factorization method;principal component analysis;homography;pattern recognition;multiple eigenvalue singular value;structure from motion;measurement noise	Linear subspace analysis (LSA) has become rather ubiquitous in a wide range of problems arising in pattern recognition and computer vision. The essence of these approaches is that certain structures are intrinsically (or approximately) low dimensional: for example, the factorization approach to the problem of structure from motion (SFM) and principal component analysis (PCA) based approach to face recognition. In LSA, the singular value decomposition (SVD) is usually the basic mathematical tool. However, analysis of the performance, in the presence of noise, has been lacking. We present such an analysis here. First, the “denoising capacity” of the SVD is analysed. Specifically, given a rank-r matrix, corrupted by noise—how much noise remains in the rank-r projected version of that corrupted matrix? Second, we study the “learning capacity” of the LSA-based recognition system in a noise-corrupted environment. Specifically, LSA systems that attempt to capture a data class as belonging to a rank-r column space will be affected by noise in both the training samples (measurement noise will mean the learning samples will not produce the “true subspace”) and the test sample (which will also have measurement noise on top of the ideal clean sample belonging to the “true subspace”). These results should help one to predict aspects of performance and to design more optimal systems in computer vision, particularly in tasks, such as SFM and face recognition. Our analysis agrees with certain observed phenomenon, and these observations, together with our simulations, verify the correctness of our theory.	computer vision;correctness (computer science);facial recognition system;noise reduction;pattern recognition;principal component analysis;simulation;singular value decomposition;structure from motion	Pei Chen;David Suter	2006	International Journal of Computer Vision	10.1007/s11263-006-6659-9	facial recognition system;computer vision;structure from motion;speech recognition;index term;homography;computer science;machine learning;first-order logic;mathematics;singular value decomposition;singular value;statistics;principal component analysis	Vision	30.500987938512417	-40.53870771625779	54417
41f4df98eadfdf6e0b933bfd65021b93a9c74e68	qualitative organization of collections of shapes via quartet analysis	organization;clustering;shape collections	We present a method for organizing a heterogeneous collection of 3D shapes for overview and exploration. Instead of relying on quantitative distances, which may become unreliable between dissimilar shapes, we introduce a qualitative analysis which utilizes multiple distance measures but only in cases where the measures can be reliably compared. Our analysis is based on the notion of quartets, each defined by two pairs of shapes, where the shapes in each pair are close to each other, but far apart from the shapes in the other pair. Combining the information from many quartets computed across a shape collection using several distance measures, we create a hierarchical structure we call categorization tree of the shape collection. This tree satisfies the topological (qualitative) constraints imposed by the quartets creating an effective organization of the shapes. We present categorization trees computed on various collections of shapes and compare them to ground truth data from human categorization. We further introduce the concept of degree of separation chart for every shape in the collection and show the effectiveness of using it for interactive shapes exploration.	categorization;ground truth;organizing (structure);six degrees of separation	Shi-Sheng Huang;Ariel Shamir;Chao-Hui Shen;Hao Zhang;Alla Sheffer;Shi-Min Hu;Daniel Cohen-Or	2013	ACM Trans. Graph.	10.1145/2461912.2461954	computer science;organization;artificial intelligence;machine learning;data mining;shape analysis;mathematics;cluster analysis	Graphics	34.59245070518502	-41.53604694623444	54566
ccb7264be1f556bc724042249ebae020b5a1d075	structural generative descriptions for time series classification	time series representation statistical structural pattern recognition structural generative descriptions sgds time series classification;time series analysis time domain analysis pattern recognition estimation vectors wavelet transforms equations;time series data mining pattern classification probability statistical analysis;journal article;structural time series representation framework structural generative descriptions time series classification data mining application structural pattern recognition paradigm statistical pattern recognition paradigm probability domain data representation stage statistical classification algorithms time series data dependency	In this paper, we formulate a novel time series representation framework that captures the inherent data dependency of time series and that can be easily incorporated into existing statistical classification algorithms. The impact of the proposed data representation stage in the solution to the generic underlying problem of time series classification is investigated. The proposed framework, which we call structural generative descriptions moves the structural time series representation to the probability domain, and hence is able to combine statistical and structural pattern recognition paradigms in a novel fashion. Two algorithm instantiations based on the proposed framework are developed. The algorithms are tested and compared using different publicly available real-world benchmark data. Results reported in this paper show the potential of the proposed representation framework, which in the experiments investigated, performs better or comparable to state-of-the-art time series description techniques.	algorithm;anomaly detection;benchmark (computing);cluster analysis;data (computing);data dependency;data mining;description;discriminative model;distortion;experiment;extraction;foundations;generic drugs;image segmentation;inductor device component;mathematical optimization;sparse matrix;statistical classification;stochastic process;structural pattern;syntactic pattern recognition;synthetic intelligence;time series;venue (sound system);density;emotional dependency;statistical cluster	Edgar S. García-Treviño;Javier A. Barria	2014	IEEE Transactions on Cybernetics	10.1109/TCYB.2014.2322310	computer science;machine learning;pattern recognition;data mining	Vision	35.247744858195915	-47.72945080650863	54993
c8df58a01d40db8918180e0e860f92eb861d072b	unsupervised transfer learning for target detection from hyperspectral images	representation;hyperspectral images;segmentation;journal article;nonlinear dimensionality reduction;model;transfer learning;space;target detection	Target detection has been of great interest in hyperspectral image analysis. Feature extraction from target samples and counterpart backgrounds consist the key to the problem. Traditional target detection methods depend on comparatively fixed feature for all the pixels under observation. For example, RX employs the same distance measurement for all the pixels. However, the best separation results usually come from certain targets and backgrounds. Theoretically, they are the purest targets and backgrounds pixels, or the constructive endmembers in the subspace model. So using those most representative pixels’ feature to train a concentrated subspace is expected to enhance the separability between targets and backgrounds. Meanwhile, applying the discriminative information from these training data to the large testing data which are not in the same feature space and with different data distributions is a challenge. Here, the idea of transfer learning from interactive annotation technique in video is employed. Based on the transfer learning frame, several points are taken into consideration and the proposed method is named as an unsupervised transfer learning based target detection (UTLD) method. Firstly, the extreme target and background pixels are generated from robust outlier detection, providing the input for target samples and background samples in transfer learning. Secondly, pixels are calculated from	anomaly detection;feature extraction;feature vector;image analysis;linear separability;pixel;unsupervised learning	Bo Du;Liangpei Zhang;Dacheng Tao;Dengyi Zhang	2013	Neurocomputing	10.1016/j.neucom.2012.08.056	computer vision;transfer of learning;computer science;machine learning;space;pattern recognition;mathematics;nonlinear dimensionality reduction;segmentation;representation	AI	29.57389974763722	-43.46881392583316	55170
79890a7d61b082947e1300f60231336a53cc285c	deep joint rain detection and removal from a single image		In this paper, we address a rain removal problem from a single image, even in the presence of heavy rain and rain streak accumulation. Our core ideas lie in our new rain image model and new deep learning architecture. We add a binary map that provides rain streak locations to an existing model, which comprises a rain streak layer and a background layer. We create a model consisting of a component representing rain streak accumulation (where individual streaks cannot be seen, and thus visually similar to mist or fog), and another component representing various shapes and directions of overlapping rain streaks, which usually happen in heavy rain. Based on the model, we develop a multi-task deep learning architecture that learns the binary rain streak map, the appearance of rain streaks, and the clean background, which is our ultimate output. The additional binary map is critically beneficial, since its loss function can provide additional strong information to the network. To handle rain streak accumulation (again, a phenomenon visually similar to mist or fog) and various shapes and directions of overlapping rain streaks, we propose a recurrent rain detection and removal network that removes rain streaks and clears up the rain accumulation iteratively and progressively. In each recurrence of our method, a new contextualized dilated network is developed to exploit regional contextual information and to produce better representations for rain detection. The evaluation on real images, particularly on heavy rain, shows the effectiveness of our models and architecture.	autostereogram;computer multitasking;deep learning;distance fog;embedded system;loss function;simulation;tree accumulation	Wenhan Yang;Robby T. Tan;Jiashi Feng;Jiaying Liu;Zongming Guo;Shuicheng Yan	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.183	computer vision;architecture;mist;image restoration;real image;artificial intelligence;atmospheric model;feature extraction;streak;computer science;phenomenon	Vision	26.164387194682934	-51.16446441461185	55458
1e526cfb03bcf4092dc05db00e392da735aac24d	viewpoint invariant subject retrieval via soft clothing biometrics	databases;biometrics access control;surveillance;training;clothing biometrics access control training databases surveillance image color analysis shape;shape;image color analysis;verbal query description viewpoint invariant subject retrieval soft clothing biometrics subject reidentification surveillance video clothing analysis image recognition single viewpoint data images;clothing;video surveillance biometrics access control clothing image recognition image retrieval	As much information as possible should be used when identifying subjects in surveillance video due to the poor quality and resolution. So far, little attention has been paid to exploiting clothing as it has been considered unlikely to be a potential cue to identity. Clothing analysis could not only potentially improve recognition, but could also aid in subject re-identification. Further, we show here how clothing can aid recognition when there is a large change in viewpoint. Our study offers some important insights into the capability of clothing information in more realistic scenarios. We show how recognition can benefit from clothing analysis when the viewpoint changes with partial occlusion, unlike other approaches addressing soft biometrics from single viewpoint data images. This research presents how soft clothing biometrics can be used to achieve viewpoint invariant subject retrieval, given a verbal query description of the subject observed from a different viewpoint. We investigate the influence of the most correlated clothing traits when extracted from multiple viewpoints, and how they can lead to increased performance.	closed-circuit television;image resolution;soft biometrics;viewpoint	Emad Sami Jaha;Mark S. Nixon	2015	2015 International Conference on Biometrics (ICB)	10.1109/ICB.2015.7139078	computer vision;shape;clothing;geometry;multimedia	Vision	33.783889467554715	-51.65726031089027	55552
a3ed080262f130051d2a02e846f5d227a440b294	contextnet: exploring context and detail for semantic segmentation in real-time		Modern deep learning architectures produce highly accurate results on many challenging semantic segmentation datasets. State-ofthe-art methods are, however, not directly transferable to real-time applications or embedded devices, since näıve adaptation of such systems to reduce computational cost (speed, memory and energy) causes a significant drop in accuracy. We propose ContextNet, a new deep neural network architecture which builds on factorized convolution, network compression and pyramid representation to produce competitive semantic segmentation in real-time with low memory requirement. ContextNet combines a deep network branch at low resolution that captures global context information efficiently with a shallow branch that focuses on high-resolution segmentation details. We analyse our network in a thorough ablation study and present results on the Cityscapes dataset, achieving 66.1% accuracy at 18.3 frames per second at full (1024×2048) resolution (41.9 fps with pipelined computations for streamed data).	algorithmic efficiency;artificial neural network;autonomous system (internet);autostereogram;color depth;computation;convolution;deep learning;embedded system;image resolution;network architecture;pyramid (image processing);real-time clock;real-time transcription;streaming media	Rudra P. K. Poudel;Ujwal Bonde;Stephan Liwicki;Christopher Zach	2018			architecture;pyramid (image processing);computer science;computer vision;machine learning;artificial intelligence;computation;artificial neural network;deep learning;frame rate;convolution;segmentation	Vision	25.30570151711275	-51.75538508937293	56032
67a2f699fb742c4bf0483e76e3ac40b619569bbe	a robust semi-semantic approach for visual localization in urban environment		This paper provides a new contribution to the problem of vision-based place recognition introducing a novel appearance and viewpoint invariant approach that guarantees robustness with respect to perceptual aliasing and kidnapping. Most of the state-of-the-art strategies rely on low level visual features and ignore the semantical structure of the scene. Thus, even small changes in the appearance of the scene (e.g., illumination conditions) cause a significant performance drop. In contrast to previous work, we propose a new strategy to model the scene by preserving its geometrical and the semantical structure and, at the same time, achieving an improved appearance invariance through a robust visual representation. In particular, to manage the perceptual aliasing problem, we introduce a covisibility graph, that connects semantical entities of the scene preserving their geometrical relations. The method relies on high level patches consisting of dense and robust descriptors that are extracted by a Convolutional Neural Network (CNN). Through the graph structure, we are able to efficiently retrieve candidate locations and to synthesize virtual locations (i.e., artificial intermediate views between two keyframes) to improve the viewpoint invariance. The proposed approach has been compared with state-of-the-art approaches in different challenging scenarios taken from public datasets.	aliasing;autostereogram;cluster analysis;convolutional neural network;entity;global illumination;graph (discrete mathematics);high-level programming language;key frame;machine vision;robustness (computer science);semiconductor industry;sensor	Silvia Cascianelli;Gabriele Costante;Enrico Bellocchio;Paolo Valigi;Mario Luca Fravolini;Thomas Alessandro Ciarfuglia	2016	2016 IEEE International Smart Cities Conference (ISC2)	10.1109/ISC2.2016.7580799	computer vision;simulation;machine learning;mathematics	Vision	31.278421881263167	-51.935860331455864	56066
c10855323ee9affd548b2d8c366b0c499f2e189f	a fast discriminant approach to active object recognition and pose estimation	comparable recognition performance;next viewpoint;aircraft model;current belief state;active object recognition;fast discriminant approach;lower computational cost;active bayesian object recognition;pose estimation;expected discriminability;information theoretic approach;viewpoint selection;difficult database;probability;estimation theory;object recognition	This paper presents a new criterion for viewpoint selection in the context of active Bayesian object recognition and pose estimation. Recognition is performed by probabilistically fusing successive observations with the current belief state of the system. Based on the current belief state, the next viewpoint is chosen to maximize the expected discriminability of the current competing hypotheses. Experiments on a difficult database of aircraft models show that this approach achieves comparable recognition performance to the widely used information theoretic approaches at a much lower computational cost.	3d pose estimation;active object;algorithmic efficiency;computation;discriminant;experiment;information theory;instance-based learning;mutual information;online and offline;outline of object recognition	Catherine Laporte;Rupert Brooks;Tal Arbel	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334476	computer vision;pose;3d pose estimation;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;probability;3d single-object recognition;estimation theory;statistics	Robotics	36.20839058975436	-45.27375289844231	56290
1757fc87d1dcc3280ee23bcf8730f4e8a47f7e3e	a spectral approach to learning structural variations in graphs	generic model;shape analysis;principal component analysis;pattern recognition;adjacency matrix;deformable model;eigenvectors;covariance matrix;approaches to learning	This paper investigates the use of graph-spectral methods for learning the modes of structural variation in sets of graphs. Our approach is as follows. First, we vectorise the adjacency matrices of the graphs. Using a graph-matching method we establish correspondences between the components of the vectors. Using the correspondences we cluster the graphs using a Gaussian mixture model. For each cluster we compute the mean and covariance matrix for the vectorised adjacency matrices. We allow the graphs to undergo structural deformation by linearly perturbing the mean adjacency matrix in the direction of the modes of the covariance matrix. We demonstrate the method on sets of corner Delaunay graphs for 3D objects viewed from varying directions.		Bin Luo;Richard C. Wilson;Edwin R. Hancock	2003		10.1007/3-540-36592-3_39	estimation of covariance matrices;adjacency list;computer vision;covariance matrix;eigenvalues and eigenvectors;two-graph;computer science;machine learning;shape analysis;integer matrix;adjacency matrix;principal component analysis	Vision	33.41989596184148	-38.98392717585702	56417
704f05b476d500703d285deabae365bb95da7ca1	skeleton-based gesture recognition using several fully connected layers with path signature features and temporal transformer module		The skeleton based gesture recognition is gaining more popularity due to its wide possible applications. The key issues are how to extract discriminative features and how to design the classification model. In this paper, we first leverage a robust feature descriptor, path signature (PS), and propose three PS features to explicitly represent the spatial and temporal motion characteristics, i.e., spatial PS (S PS), temporal PS (T PS) and temporal spatial PS (T S PS). Considering the significance of fine hand movements in the gesture, we propose an ”attention on hand” (AOH) principle to define joint pairs for the S PS and select single joint for the T PS. In addition, the dyadic method is employed to extract the T PS and T S PS features that encode global and local temporal dynamics in the motion. Secondly, without the recurrent strategy, the classification model still faces challenges on temporal variation among different sequences. We propose a new temporal transformer module (TTM) that can match the sequence key frames by learning the temporal shifting parameter for each input. This is a learning-based module that can be included into standard neural network architecture. Finally, we design a multi-stream fully connected layer based network to treat spatial and temporal features separately and fused them together for the final result. We have tested our method on three benchmark gesture datasets, i.e., ChaLearn 2016, ChaLearn 2013 and MSRC-12. Experimental results demonstrate that we achieve the state-of-the-art performance on skeleton-based gesture recognition with high computational efficiency.	artificial neural network;benchmark (computing);dyadic transformation;encode;gesture recognition;key frame;network architecture;the third manifesto;transformer;visual descriptor	Chenyang Li;Xin Zhang;Lufan Liao;Lianwen Jin;Weixin Yang	2018	CoRR		skeleton (computer programming);architecture;machine learning;discriminative model;artificial intelligence;pattern recognition;gesture recognition;artificial neural network;computer science;gesture	Vision	28.41972085444877	-52.08024260493495	56420
504e55316ab4f0d40cd9056095822b01f646fbe1	geometric feature-based facial emotion recognition using two-stage fuzzy reasoning model	facial action measurement;geometric feature extraction;facial emotion recognition;fuzzy reasoning model;action unit detection	Facial Emotion recognition is a significant requirement in machine vision society. In this sense, this paper utilizes geometric facial features and calculates displacement of feature points between expressive and neutral frames and finally applies a two-stage fuzzy reasoning model for facial emotion recognition and classification. The prototypical emotion sequence according to the Facial Action Coding System (FACS) is formed analyzing small, medium and large displacement. Furthermore geometric displacements are fuzzified and mapped onto an Action Units (AUs) by employing first-stage fuzzy reasoning model and later AUs are fuzzified and mapped onto an Emotion space by employing second-stage fuzzy relational model. The overall performance of the proposed system is evaluated on the extended Cohn-Kanade (CK+) database for classifying basic emotions like surprise, sadness, fear, anger, and happiness. The experimental results on the task of facial emotion analysis and emotion recognition are shown to outperform other existing methods available in the literature.	emotion recognition	Md. Nazrul Islam;Chu Kiong Loo	2014		10.1007/978-3-319-12640-1_42	computer vision;artificial intelligence	Vision	36.7002299725478	-50.26965839880252	56494
937ef5d97dbada96d40fd3577aadcd8938854a6c	human activity recognition based on motion projection profile features in surveillance videos using support vector machines and gaussian mixture models		Human Activity Recognition (HAR) is an active research area in computer vision and pattern recognition. The area of human activity recognition, attention consistently focuses on changes in the scene of a subject with reference to time, since motion information can sensibly depict the activity. This paper depicts a novel framework for activity recognition based on Motion Projection Profile (MPP) features of the difference image, representing various levels of a person’s interaction. The motion projection profile features consist of the measure of moving pixel of each row, column and diagonal (left and right) of the difference image and they give adequate motion information to recognize the instantaneous posture of the person. The experiments are carried out using UT-Interaction dataset (Set 1 and Set 2), considering six activities viz (handshake, hug, kick, point, punch, push) and the extracted features are modeled by Support Vector Machines (SVM) with RBF kernel and Gaussian Mixture Models (GMM) for recognizing human activities. In the experimental results, GMM exhibit effectiveness of the proposed method with an overall accuracy rate of 93.01 % and 90.81 % for Set 1 and 2 respectively, this outperforms the SVM classifier.	activity recognition;mixture model;support vector machine	J. Arunnehru;M. Kalaiselvi Geetha	2015		10.1007/978-3-319-22915-7_38	computer vision;machine learning;pattern recognition	Vision	36.3845400440068	-50.150157266506355	56503
d5136336032b6ed3b8f7979a5ec4f1b86dc5bbcb	activity recognition using eigen-joints based on hmm	principal component analysis dynamic programming eigenvalues and eigenfunctions geometry hidden markov models image classification;hmm activity recognition eigen joint;hmm;yttrium skeleton;eigen joint;pca activity recognition eigenjoint hidden markov model hmm 3d skeleton data kinect sensor dynamic time wrapping dtw euclidean geometry distance joint classification principal component analysis;activity recognition	In this paper, we present an approach for activity recognition by using 3D skeleton data obtained with a Kinect sensor. Primarily, we use the simplified dynamic time wrapping (DTW) and calculate Euclidean geometry distance to obtain the probable activities from the trained data. Afterwards, for each activity, we define a modified activity feature descriptor using the interrelation of correlated joints in each frame. Before classification, we employ normalization to avoid non-uniformity in coordinates, and then Principal Component Analysis (PCA) is applied to deduce redundancy and decrease the dimensionality. As the result Eigen-joints for each activity are obtained. Finally we classify the joints into multiple actions using Hidden Markov Model (HMM). The experimental result on benchmark dataset shows that the accuracy approximates that of the state-of-the-art.	activity recognition;benchmark (computing);circuit complexity;eigen (c++ library);euclidean distance;hidden markov model;kinect;markov chain;principal component analysis;visual descriptor;wrapping (graphics)	Hao Xu;Yong-Cheol Lee;Chil-Woo Lee	2015	2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2015.7358958	speech recognition;computer science;machine learning;pattern recognition;activity recognition	Robotics	36.71446279522599	-51.78723514059549	56623
dfd56fdca36aa34a6b3e260108b44c3e946cd6aa	discriminant quaternion local binary pattern embedding for person re-identification through prototype formation and color categorization	quaternion local binary pattern;person re identification;color categorization;discrimination;prototype formation	Re-identifying objects is one of the fundamental elements for visual surveillance, in the sense that images of the same object at different time or places should be assigned with the same label. In this work, we propose a new embedding scheme for person re-identification under nonoverlapping target cameras. Inspired by the prototype approach derived from cognition field, we propose to use prototype images as a reference set to achieve a discriminative representation of a person's appearance. To enhance the discrimination between different persons, we learn a linear subspace in a training phase during which person correspondences are assumed to be known. The robustness of the algorithm against results that are counterintuitive to a human operator is improved by proposing the Color Categorization procedure. By doing so, our method becomes very flexible when tracing a person in a camera network even under large illumination changes. The proposed framework was tested on VIPeR, the most challenging dataset for person re-identification. Results confirm that our method outperforms the state of the art techniques.	binary pattern (image generation);categorization;discriminant;prototype	C. Chahla;Hichem Snoussi;Fahed Abdallah;Fadi Dornaika	2017	Eng. Appl. of AI	10.1016/j.engappai.2016.11.004	computer vision;discrimination;machine learning;pattern recognition	AI	34.39732793933101	-51.0493803343275	56760
3431b4602c1d5d39823efc181c5850b779f7f5f7	recognition of repetitive movement patterns - the case of football analysis	spatio temporal analysis;text;football analysis;trajectory;pattern recognition;article	Analyzing sports like football is interesting not only for the sports team itself, but also for the public and the media. Both have recognized that using more detailed analyses of the teams’ behavior increases their attractiveness and also their performance. For this reason, the games and the individual players are recorded using specially developed tracking systems. The tracking solution usually comes with elementary analysis software allowing for basic statistical information extraction. Going beyond these simple statistics is a challenging task. However, it is worthwhile when it provides a better view into the tactics of team or the typical movements of an individual player. In this paper an approach for the recognition of movement patterns as an advanced analysis method is presented, which uses the players’ trajectories as input data. Besides individual movement patterns it is also able to detect patterns in relation to group movements. A detailed description is followed by a discussion of the approach, where different experiments on real trajectory datasets, even from other contexts than football, show the method’s benefits and features.	experiment;information extraction;tracking system;trajectory optimization	Udo Feuerhake	2016	ISPRS Int. J. Geo-Information	10.3390/ijgi5110208	simulation;geography;artificial intelligence;multimedia	ML	38.76240250175961	-46.1184564800094	56821
0b46c972cd5d84359aa4dde6b722bb89c151a6a2	probabilistic dyadic data analysis with local and global consistency	modeling technique;probability density;probability density function;latent dirichlet allocation;data analysis;euclidean space;social network analysis;graph laplacian;probabilistic latent semantic analysis;laplace beltrami operator	Dyadic data arises in many real world applications such as social network analysis and information retrieval. In order to discover the underlying or hidden structure in the dyadic data, many topic modeling techniques were proposed. The typical algorithms include Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA). The probability density functions obtained by both of these two algorithms are supported on the Euclidean space. However, many previous studies have shown naturally occurring data may reside on or close to an underlying submanifold. We introduce a probabilistic framework for modeling both the topical and geometrical structure of the dyadic data that explicitly takes into account the local manifold structure. Specifically, the local manifold structure is modeled by a graph. The graph Laplacian, analogous to the Laplace-Beltrami operator on manifolds, is applied to smooth the probability density functions. As a result, the obtained probabilistic distributions are concentrated around the data manifold. Experimental results on real data sets demonstrate the effectiveness of the proposed approach.	algorithm;dyadic transformation;global optimization;information retrieval;laplacian matrix;latent dirichlet allocation;probabilistic latent semantic analysis;social network analysis;topic model	Deng Cai;Xuanhui Wang;Xiaofei He	2009		10.1145/1553374.1553388	latent dirichlet allocation;probability density function;discrete mathematics;machine learning;pattern recognition;mathematics;probabilistic latent semantic analysis;statistics	ML	27.84965068632904	-39.267519318247636	56943
0359935f5fe14eb4cb8af9486291f760495de7dd	modelling activity global temporal dependencies using time delayed probabilistic graphical model	conventional logging;member and geographic activities;stage structure;learning algorithm;real time;delay effects;global optimisation;layout;noise robustness;junctions;time delay;probabilistic graphical model;learning systems;graphical models;computer science;content addressable storage;camera network;delay effects graphical models cameras optimization methods learning systems layout noise robustness road vehicles content addressable storage computer science;cameras;road vehicles;real time systems;optimization methods	We present a novel approach for detecting global behaviour anomalies in multiple disjoint cameras by learning time delayed dependencies between activities cross camera views. Specifically, we propose to model multi-camera activities using a Time Delayed Probabilistic Graphical Model (TD-PGM) with different nodes representing activities in different semantically decomposed regions from different camera views, and the directed links between nodes encoding causal relationships between the activities. A novel two-stage structure learning algorithm is formulated to learn globally optimised time-delayed dependencies. A new cumulative abnormality score is also introduced to replace the conventional log-likelihood score for gaining significantly more robust and reliable real-time anomaly detection. The effectiveness of the proposed approach is validated using a camera network installed at a busy underground station.	algorithm;anomaly detection;broadcast delay;causality;experiment;graphical model;real-time clock;sensor	Chen Change Loy;Tao Xiang;Shaogang Gong	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459156	layout;computer vision;real-time computing;simulation;computer science;machine learning;graphical model	Vision	37.09310804269468	-45.67276355994875	57166
3d19ca3eca2aef14bd6a7c50d085fafe03ef26a7	a classification method of multispectral images which is based on fuzzy svm	remote sensing image;multispectral imaging support vector machines support vector machine classification robust stability remote sensing computer science software engineering pattern recognition software algorithms image classification;kernel;support vector machines;remote sensing image classification;training;image classification;satisfiability;pattern recognition algorithm;classification;fuzzy set theory;support vector machines fuzzy set theory geophysical signal processing image classification remote sensing spectral analysis;accuracy;distance measurement;fuzzy;geophysical signal processing;remote sensing;multispectral images;classification algorithms;pattern recognition;remote sensing image classification multispectral image classification method fuzzy svm algorithm support vector machine pattern recognition algorithm statistical robustness;support vector machine classification;svm;support vector machine;spectral analysis;fuzzy svm algorithm;multispectral image classification method;statistical robustness;classification multispectral images fuzzy svm	Support vector machine (SVM) is more popular in recent years in the field of pattern recognition algorithms. SVM algorithm has good validity of the calculation, statistical robustness and stability. SVM has gradually become an important tool in the field of remote sensing image classification. Because of the similarity between the different classes in the multispectral images, the result of the classification which is gotten by using the SVM directly is usually not satisfying. In this paper, we proposed a method based on fuzzy SVM (FSVM) to classify the multispectral images. The result of the experiment shows that the accuracy of this method is higher compared with the method which used the SVM directly.	algorithm;computer vision;experiment;multispectral image;pattern recognition;support vector machine	Huai-bin Wang;Jing-hua Ma	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.784	statistical classification;support vector machine;computer vision;computer science;machine learning;pattern recognition;ranking svm	Robotics	30.493823103546035	-42.58103709369103	57191
47443aab680387f6f19e1af2c3b82e61c499bd1a	real-time hand posture and gesture-based touchless automotive user interface using deep learning		In this study, a vision based in-car entertainment user interface is presented. The user interface is designed using a hand posture and gesture recognition algorithm in deep learning framework. The hand posture recognition algorithm is formulated using the convolutional neural network to perform the fundamental tasks in the user interface. The hand gesture recognition algorithm is formulated using the long-term recurrent convolutional neural network to intuitively interact with the touchless automotive user interface in a detailed manner. In the recurrent deep learning framework, typically, the gesture frames are taken from a uniformly sampled image sequence. In this work, the recurrent structure is enhanced using a reduced number of input frames captured from the image sequence. The reduced input frames or key frames represent the action present in the video sequence. Sparse dictionary learning provide reliable key frame extraction from video sequences. However, sparse dictionary learning is computationally expensive, and are individually optimized for every video sequence. In this paper, we propose to approximate sparse dictionary learning using a non-linear regression framework. The multilayer perceptron is utilized to model the non-linear regression framework. The optimal neural network architecture is identified after a detailed evaluation. We evaluate the proposed recognition methods on public datasets. The proposed methods yield a recognition accuracy of 92% and 90% for pose and gestures, respectively. The combined hand posture and gesture recognition takes 82ms which is a reasonable for real time implementation.	analysis of algorithms;approximation algorithm;artificial neural network;convolutional neural network;deep learning;gesture recognition;key frame;machine learning;multilayer perceptron;network architecture;nonlinear system;poor posture;pose (computer vision);real-time clock;real-time transcription;sparse dictionary learning;sparse matrix;user interface	Vijay John;M. Umetsu;Ali Boyali;Seiichi Mita;Masayuki Imanishi;Norio Sanma;S. Shibata	2017	2017 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2017.7995825	convolutional neural network;gesture recognition;feature extraction;key frame;artificial neural network;deep learning;multilayer perceptron;machine learning;user interface;computer science;artificial intelligence	Vision	28.261675186122865	-47.70712524669052	57199
036bae2260aa62c72628954b67bacae0c849f035	auto-encoder-based shared mid-level visual dictionary learning for scene classification using very high resolution remote sensing images	scene classification;machine learning technique;remote sensing geophysical image processing image classification image representation image resolution learning artificial intelligence;autoencoder based shared midlevel visual dictionary learning;scene classification scheme;image representation;rich semantic information;vhr remote sensing images;mid level visual elements;mid level visual dictionary;auto encoder based method;very high resolution remote sensing images;image representation autoencoder based shared midlevel visual dictionary learning scene classification very high resolution remote sensing images vhr remote sensing images auto encoder based method mid level visual dictionary mid level visual elements machine learning technique rich semantic information image clutters occlusions scene classification scheme;image clutters;occlusions	Effective representation and classification of scenes using very high resolution (VHR) remote sensing images cover a wide range of applications. Although robust low-level image features have been proven to be effective for scene classification, they are not semantically meaningful and thus have difficulty to deal with challenging visual recognition tasks. In this study, the authors propose a new and effective auto-encoder-based method to learn a shared mid-level visual dictionary. This dictionary serves as a shared and universal basis to discover mid-level visual elements. On the one hand, the mid-level visual dictionary learnt using machine learning technique is more discriminative and contains rich semantic information, compared with the traditional low-level visual words. On the other hand, the mid-level visual dictionary is more robust to occlusions and image clutters. In the authors’ sceneclassification scheme, they use discriminative mid-level visual elements, rather than individual pixels or low-level image features, to represent images. This new image representation is able to capture much of the high-level meaning and contents of the image, facilitating challenging remote sensing image scene-classification tasks. Comprehensive evaluations on a challenging VHR remote sensing images data set and comparisons with state-of-the-art approaches demonstrate the effectiveness and superiority of their study.	autoencoder;data dictionary;encoder;high- and low-level;image resolution;machine learning;pixel	Gong Cheng;Peicheng Zhou;Junwei Han;Lei Guo;Jungong Han	2015	IET Computer Vision	10.1049/iet-cvi.2014.0270	computer vision;computer science;machine learning;pattern recognition	Vision	28.72252386819489	-48.81982389426586	57316
404ac62d19a2677ad0266041882244d943e7033e	application of a hybrid model based on a convolutional auto-encoder and convolutional neural network in object-oriented remote sensing classification		Variation in the format and classification requirements for remote sensing data makes establishing a standard remote sensing sample dataset difficult. As a result, few remote sensing deep neural network models have been widely accepted. We propose a hybrid deep neural network model based on a convolutional auto-encoder and a complementary convolutional neural network to solve this problem. The convolutional auto-encoder supports feature extraction and data dimension reduction of remote sensing data. The extracted features are input into the convolutional neural network and subsequently classified. Experimental results show that in the proposed model, the classification accuracy increases from 0.916 to 0.944, compared to a traditional convolutional neural network model; furthermore, the number of training runs is reduced from 40,000 to 22,000, and the number of labelled samples can be reduced by more than half, all while ensuring a classification accuracy of no less than 0.9, which suggests the effectiveness and feasibility of the proposed model.	artificial neural network;autoencoder;computer vision;convolutional neural network;data redundancy;deep learning;dimensionality reduction;encoder;feature extraction;map;network model;network planning and design;requirement	Wei Cui;Qi Zhou;Zhendong Zheng	2018	Algorithms	10.3390/a11010009	machine learning;artificial intelligence;convolutional neural network;autoencoder;mathematics;feature extraction;remote sensing;dimensionality reduction;artificial neural network;object-oriented programming	AI	24.784246938251755	-51.020607581751804	57383
3e8693abac1d026552afe6ce2448a0213b931db4	ensembles of landmark multidimensional scalings	landmark multidimensional scaling;unsupervised learning;embedding;ground control points;multidimensional systems scalability computer science information retrieval image retrieval costs control systems unsupervised learning extraterrestrial measurements geometry;complexity theory;landmark mds ensemble;image retrieval affine transforms data handling grid computing;similar image retrieval;indexing terms;multidimensional scaling mds;data mining;noise measurement;ground control point;distance measurement;dimensionality reduction;noise level;affine transformation;affine transforms;multidimensional scaling;similar image retrieval landmark multidimensional scaling landmark points landmark mds ensemble ground control points affine transformations noisy grid;landmark points;affine transformations;synthetic data;data handling;unsupervised learning dimensionality reduction embedding multidimensional scaling mds;noisy grid;grid computing;high performance;dimensional reduction;noise;coordinate system;image retrieval	Landmark multidimensional scaling (LMDS) uses a subset of data (landmark points) to solve classical MDS, where the scalability is increased but the approximation is noise-sensitive. In this paper we present an ensemble of LMDSs, referred to as landmark MDS ensemble (LMDSE), where we use a portion of the input in a piecewise manner to solve classical MDS, combining individual LMDS solutions which operate on different partitions of the input. Ground control points (GCPs) that are shared by partitions considered in the ensemble, allow us to align individual LMDS solutions in a common coordinate system through affine transformations. LMDSE solution is determined by averaging aligned LMDS solutions. We show that LMDSE is less noise-sensitive while maintaining the scalability as well as the speed of LMDS. Experiments on synthetic data (noisy grid) and real-world data (similar image retrieval) confirm the high performance of the proposed LMDSE.	align (company);approximation;image retrieval;image scaling;landmark point;multidimensional scaling;scalability;synthetic data	Seunghak Lee;Seungjin Choi	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959917	unsupervised learning;computer vision;image retrieval;computer science;theoretical computer science;machine learning;data mining;affine transformation;mathematics	DB	28.11273796973792	-40.14838222341035	57509
e4cf6a559b4002b650dde376005a64b064963b19	memory based online learning of deep representations from video streams		We present a novel online unsupervised method for face identity learning from video streams. The method exploits deep face descriptors together with a memory based learning mechanism that takes advantage of the temporal coherence of visual data. Specifically, we introduce a discriminative descriptor matching solution based on Reverse Nearest Neighbour and a forgetting strategy that detect redundant descriptors and discard them appropriately while time progresses. It is shown that the proposed learning procedure is asymptotically stable and can be effectively used in relevant applications like multiple face identification and tracking from unconstrained video streams. Experimental results show that the proposed method achieves comparable results in the task of multiple face tracking and better performance in face identification with offline approaches exploiting future information. Code will be publicly available.		Federico Pernici;Federico Bartoli;Matteo Bruni;Alberto Del Bimbo	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00247	streams;computer vision;pattern recognition;machine learning;facial motion capture;discriminative model;visualization;artificial intelligence;video tracking;computer science;forgetting;exploit	Vision	31.023799055893257	-50.45378160333832	57740
413bd84c4dbc3d5d1481d58a03e5890e1474935e	siamese instance search for tracking		In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-the-art tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the most similar patch by a learned matching function. The strength of the matching function comes from being extensively trained generically, i.e., without any data of the target, using a Siamese deep neural network, which we design for tracking. Once learned, the matching function is used as is, without any adapting, to track previously unseen targets. It turns out that the learned matching function is so powerful that a simple tracker built upon it, coined Siamese INstance search Tracker, SINT, which only uses the original observation of the target from the first frame, suffices to reach state-of-the-art performance. Further, we show the proposed tracker even allows for target re-identification after the target was absent for a complete video shot.	artificial neural network;benchmark (computing);deep learning;orfeo toolbox	Ran Tao;Efstratios Gavves;Arnold W. M. Smeulders	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.158	computer vision;simulation;machine learning	Vision	31.871680853730826	-49.53284092380037	57849
10534d3164a7a31896190fefb6b99a31697d3dab	a supervised non-linear dimensionality reduction approach for manifold learning	non linear dimensionality reduction;discriminant analysis;face recognition;supervised manifold learning	In this paper we introduce a novel supervised manifold learning technique called Supervised Laplacian Eigenmaps (S-LE), which makes use of class label information to guide the procedure of non-linear dimensionality reduction by adopting the large margin concept. The graph Laplacian is split into two components: within-class graph and between-class graph to better characterize the discriminant property of the data. Our approach has two important characteristics: (i) it adaptively estimates the local neighborhood surrounding each sample based on data density and similarity and (ii) the objective function simultaneously maximizes the local margin between heterogeneous samples and pushes the homogeneous samples closer to each other. Our approach has been tested on several challenging face databases and it has been conveniently compared with other linear and non-linear techniques, demonstrating its superiority. Although we have concentrated in this paper on the face recognition problem, the proposed approach could also be applied to other category of objects characterized by large variations in their appearance (such as hand or body pose, for instance). & 2011 Elsevier Ltd. All rights reserved.	algorithm;areal density (computer storage);class diagram;computation;database;discriminant;facial recognition system;feature selection;laplacian matrix;loss function;nonlinear dimensionality reduction;nonlinear system;online and offline;optimization problem;point of view (computer hardware company);programming paradigm;sample rate conversion;sparse approximation;sparse matrix;statistical classification;supervised learning;time complexity	Bogdan Raducanu;Fadi Dornaika	2012	Pattern Recognition	10.1016/j.patcog.2011.12.006	facial recognition system;computer vision;computer science;machine learning;pattern recognition;mathematics;linear discriminant analysis;manifold alignment	AI	25.567721822358898	-42.60951901414599	57996
7c66e7f357553fd4b362d00ff377bffb9197410e	gaussian process domain experts for modeling of facial affect		Most of existing models for facial behavior analysis rely on generic classifiers, which fail to generalize well to previously unseen data. This is because of inherent differences in source (training) and target (test) data, mainly caused by variation in subjects’ facial morphology, camera views, and so on. All of these account for different contexts in which target and source data are recorded, and thus, may adversely affect the performance of the models learned solely from source data. In this paper, we exploit the notion of domain adaptation and propose a data efficient approach to adapt already learned classifiers to new unseen contexts. Specifically, we build upon the probabilistic framework of Gaussian processes (GPs), and introduce domain-specific GP experts (e.g., for each subject). The model adaptation is facilitated in a probabilistic fashion, by conditioning the target expert on the predictions from multiple source experts. We further exploit the predictive variance of each expert to define an optimal weighting during inference. We evaluate the proposed model on three publicly available data sets for multi-class (MultiPIE) and multi-label (DISFA, FERA2015) facial expression analysis by performing adaptation of two contextual factors: “where” (view) and “who” (subject). In our experiments, the proposed approach consistently outperforms: 1) both source and target classifiers, while using a small number of target examples during the adaptation and 2) related state-of-the-art approaches for supervised domain adaptation.	acclimatization;conditioning (psychology);domain adaptation;experiment;face;facial recognition system;galaxy morphological classification;gaussian process;inference;multi-label classification;normal statistical distribution;numerous;probabilistic turing machine;sample variance;source data	Stefanos Eleftheriadis;Ognjen Rudovic;Marc Peter Deisenroth;Maja Pantic	2017	IEEE Transactions on Image Processing	10.1109/TIP.2017.2721114	source data;computer vision;artificial intelligence;data modeling;probabilistic logic;pattern recognition;small number;inference;data set;machine learning;computer science;gaussian process;weighting	Vision	24.83067158080682	-47.13911573482844	58271
6ce6da7a6b2d55fac604d986595ba6979580393b	cross domain knowledge transfer for person re-identification		Person Re-Identification (re-id) is a challenging task in computer vision, especially when there are limited training data from multiple camera views. In this paper, we propose a deep learning based person re-identification method by transferring knowledge of mid-level attribute features and high-level classification features. Building on the idea that identity classification, attribute recognition and reidentification share the same mid-level semantic representations, they can be trained sequentially by fine-tuning one based on another. In our framework, we train identity classification and attribute recognition tasks from deep Convolutional Neural Network (dCNN) to learn person information. The information can be transferred to the person re-id task and improves its accuracy by a large margin. Furthermore, a Long Short Term Memory(LSTM) based Recurrent Neural Network (RNN) component is extended by a spacial gate. This component is used in the re-id model to pay attention to certain spacial parts in each recurrent unit. Experimental results show that our method achieves 78.3% of rank-1 recognition accuracy on the CUHK03 benchmark.	benchmark (computing);computer vision;convolutional neural network;deep learning;high- and low-level;long short-term memory;random neural network;recurrent neural network;unified framework	Qiqi Xiao;Kelei Cao;Haonan Chen;Fangyue Peng;Chi Zhang	2016	CoRR		computer vision;computer science;machine learning;pattern recognition	AI	25.663285917537774	-49.523975858699366	58317
527ea1e24b848e9bd27b786323d11052c096a029	translation non-negative matrix factorization with fast optimization	nenmf non negative matrix factorization nmf translation transformation;optimisation face recognition image reconstruction image representation matrix decomposition;translation nonnegative matrix factorization face image datasets nenmf multiplicative update rule based algorithm image reconstruction sparse representation data mining pattern recognition optimization tnmf translation nmf;face recognition face matrix decomposition accuracy training sparse matrices optimization	Non-negative matrix factorization (NMF) reconstructs the original samples in a lower dimensional space and has been widely used in pattern recognition and data mining because it usually yields sparse representation. Since NMF leads to unsatisfactory reconstruction for the datasets that contain translations of large magnitude, it is required to develop translation NMF (TNMF) to first remove the translation and then conduct a decomposition. However, existing multiplicative update rule based algorithm for TNMF is not efficient enough. In this paper, we reformulate TNMF and show that it can be efficiently solved by using the state-of-the-art solvers such as NeNMF. Experimental results on face image datasets confirm both efficiency and effectiveness of the reformulated TNMF.	algorithm;data mining;loss function;mathematical optimization;non-negative matrix factorization;optimization problem;pattern recognition;sparse approximation;sparse matrix;statistical machine translation	Yuanyuan Wang;Naiyang Guan;Bin Mao;Xuhui Huang;Zhigang Luo	2014	2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2014.6974365	sparse matrix;theoretical computer science;machine learning;pattern recognition;mathematics	Vision	25.7455517741331	-41.244293145950245	58410
771fe2789db5657bbc2c3a3c81f48a77f34cfddb	improved classification approach for use with large-scale scene images in the hadoop cluster environment				Jianfang Cao;Yanfei Li;Yan Wang;Hongyan Cui;Yun Tian	2018	J. Electronic Imaging	10.1117/1.JEI.27.6.063027	computer vision;artificial intelligence;computer science	Vision	38.214360167499784	-40.00532078497135	58472
1d3733e1a4b9d3e5b95fb7cc135a0819cba90c22	linear subspace methods in face recognition	qa 75 electronic computers computer science;ta engineering general civil engineering general	Despite over 30 years of research, face recognition is still one of the most difficult problems in the field of Computer Vision. The challenge comes from many factors affecting the performance of a face recognition system: noisy input, training data collection, speed-accuracy trade-off, variations in expression, illumination, pose, or ageing. Although relatively successful attempts have been made for special cases, such as frontal faces, no satisfactory methods exist that work under completely unconstrained conditions. This thesis proposes solutions to three important problems: lack of training data, speed-accuracy requirement, and unconstrained environments. The problem of lacking training data has been solved in the worst case: single sample per person. Whitened Principal Component Analysis is proposed as a simple but effective solution. Whitened PCA works well under this scenario because of two reasons. On one hand, PCA has the potential to extract discriminating information as covariance matrix has characterised all the inherent differences in the training data. On the other hand, whitening process can exclude the trained variation retained by the PCA which is harmful to the recognition process. Whitened PCA performs consistently well on multiple face datasets.		Hieu Nguyen	2011			computer science;artificial intelligence;machine learning;data mining	Vision	27.48138888869094	-45.11387547004117	58481
3e517f89e451707eb16a63a52e6243524082b3e3	end-to-end photo-sketch generation via fully convolutional representation learning	neural nets;face verification;sketch photo generation	Sketch-based face recognition is an interesting task in vision and multimedia research, yet it is quite challenging due to the great difference between face photos and sketches. In this paper, we propose a novel approach for photo-sketch generation, aiming to automatically transform face photos into detail-preserving personal sketches. Unlike the traditional models synthesizing sketches based on a dictionary of exemplars, we develop a fully convolutional network to learn the end-to-end photo-sketch mapping. Our approach takes whole face photos as inputs and directly generates the corresponding sketch images with efficient inference and learning, in which the architecture is stacked by only convolutional kernels of very small sizes. To well capture the person identity during the photo-sketch transformation, we define our optimization objective in the form of joint generative discriminative minimization. In particular, a discriminative regularization term is incorporated into the photo-sketch generation, enhancing the discriminability of the generated person sketches against other individuals. Extensive experiments on several standard benchmarks suggest that our approach outperforms other state-of-the-arts in both photo sketch generation and face sketch verification.	dictionary;discriminative model;end-to-end principle;experiment;facial recognition system;feature learning;machine learning;mathematical optimization;sketch	Liliang Zhang;Liang Lin;Xian Wu;Shengyong Ding;Lei Zhang	2015		10.1145/2671188.2749321	computer vision;speech recognition;computer science;machine learning;pattern recognition;artificial neural network	AI	25.612979715547173	-49.2471114722169	58505
fea9d1d7f7ecb5a67fe946029f636e768c980017	norm discriminant eigenspace transform for pattern classification		Most of the supervised dimensionality reduction (DR) methods design interclass scatter as the separability between the class means, which may force to assume unimodal Gaussian likelihoods and their projection space trends toward the class means. This paper presents a novel DR approach, norm discriminant eigenspace transform (NDET), in which average norms ( ${l_{2}}$ ) of classes have been utilized to characterize the interclass separability and the within-class distance characterizes the intraclass compactness. NDET is intended to accommodate data distributions that may be multimodal and non-Gaussian. We derive an upper bound for NDET, and a specific solution space to attain this bound. Existence of the specific solution is very unwonted, thereby we have considered the solution space of upper bound to achieve better reduction of dimensionality and discrimination of classes. Also, a nonlinear version of NDET (kernel NDET) is developed to model nonlinear relationships between the features. We show, experimentally (on synthetic data) that NDET effectively overcomes the limitations, which arise due to unimodal and data distribution assumptions of the traditional algorithms. Extensive empirical studies are made; and the proposed method is compared with closely related state-of-the-art schemes on UCI machine learning repository and face recognition data sets, to establish its novelty.		K. Ramachandra Murthy;Ashish Ghosh	2017	IEEE Transactions on Cybernetics	10.1109/TCYB.2017.2771530	kernel (linear algebra);mathematical optimization;mathematics;curse of dimensionality;dimensionality reduction;compact space;linear programming;eigenvalues and eigenvectors;synthetic data;upper and lower bounds	ML	24.956702954809455	-40.304065312333904	58840
74c19438c78a136677a7cb9004c53684a4ae56ff	resound: towards action recognition without representation bias		While large datasets have proven to be a key enabler for progress in computer vision, they can have biases that lead to erroneous conclusions. The notion of the representation bias of a dataset is proposed to combat this problem. It captures the fact that representations other than the ground-truth representation can achieve good performance on any given dataset. When this is the case, the dataset is said not to be well calibrated. Dataset calibration is shown to be a necessary condition for the standard state-of-the-art evaluation practice to converge to the ground-truth representation. A procedure, RESOUND, is proposed to quantify and minimize representation bias. Its application to the problem of action recognition shows that current datasets are biased towards static representations (objects, scenes and people). Two versions of RESOUND are studied. An Explicit RESOUND procedure is proposed to assemble new datasets by sampling existing datasets. An implicit RESOUND procedure is used to guide the creation of a new dataset, Diving48, of over 18,000 video clips of competitive diving actions, spanning 48 fine-grained dive classes. Experimental evaluation confirms the effectiveness of RESOUND to reduce the static biases of current datasets.	algorithm;calibration (statistics);computer vision;converge;experiment;file spanning;mathematical optimization;sampling (signal processing);universal instantiation;video clip	Yingwei Li;Yi Li;Nuno Vasconcelos	2018		10.1007/978-3-030-01231-1_32	clips;machine learning;artificial intelligence;sampling (statistics);computer science	Vision	31.931634071093978	-49.07294456782897	58851
e94c05eecc78008c12090e24e13526dd26d0a644	detecting threat behaviours	semantics;endnotes;computational modeling;trajectory;mobile communication;zinc;pubications;security;cameras	This paper addresses the complex problem of recognising threat situations from videos streamed by surveillance cameras. A behaviour recognition approach is proposed, which is based on a semantic recognition of the event. Low-level tracking information is transformed into high-level semantic descriptions mainly by analysis of the tracked object speed and direction. Semantic terms combined with automatically learned activity zones of the observed scene allow delivering behaviour events indicating the mobile activity. Behaviours of interest are modelled and recognised in the semantic domain. The approach has been applied on different public datasets, namely CAVIAR and ARENA. Both datasets contain instances of people attacked (with physical aggression). Successful results have been obtained when compared to other state of the art algorithms.	algorithm;closed-circuit television;database;high- and low-level;object detection;sensor;streaming media;threat (computer);threat model	Jose Luis Patino;James M. Ferryman	2016	2016 13th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2016.7738072	computer vision;simulation;computer science;trajectory;machine learning;zinc;semantics	Vision	39.13090100178095	-45.64384992808324	58985
9b1e2ac0051a0afe28b9dacd1113242058c93da4	a generative semi-supervised model for multi-view learning when some views are label-free	audio visual systems;image classification;accuracy training standards vectors data models covariance matrices educational institutions;multiview learning lip video views digit recognition standard mfa problem em algorithm mixtures of factors analyzers generative semi supervised mixture model multiview classification;learning artificial intelligence audio visual systems expectation maximisation algorithm image classification;learning artificial intelligence;expectation maximization multi view learning semi supervised learning mixture of factors analyzers;expectation maximisation algorithm	We consider multi-view classification for the challenging scenario where, for some views, there are no labeled training examples. Several discriminative approaches have been recently proposed for special instances of this problem. Here, alternatively, we propose a generative semi-supervised mixture model across all views which, via marginalization, flexibly performs exact class inference, given any subset of available views. The proposed model is an extension of semi-supervised mixtures to a multi-view setting, as well as a semi-supervised extension of mixtures of factors analyzers (MFA)[1]. A novel EM algorithm with a computationally efficient E-step is derived for learning our multi-view model. Specialization of this formulation to the standard MFA problem also gives a reduced complexity E-step, compared to the original EM algorithm proposed for MFA. Our multi-view method is experimentally demonstrated on digit recognition using audio and lip video views, achieving competitive results with alternative, discriminative approaches.	algorithmic efficiency;expectation–maximization algorithm;experiment;free viewpoint television;mixture model;partial template specialization;semi-supervised learning;semiconductor industry;universal quantification;view model	Gaole Jin;Raviv Raich;David J. Miller	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638269	computer vision;contextual image classification;speech recognition;computer science;machine learning;pattern recognition;statistics	Vision	24.728891221697776	-45.162511185833814	59054
8eef728728385507d1b09bbe350088c4eb4e3508	bidirectional representation for face recognition across pose	sparse representation	Conventional representation methods try to express the test sample as a weighting sum of training samples and exploit the deviation between the test sample and the weighting sum of the training samples from each class (also referred to as deviation between the test sample and each class) to classify the test sample. In particular, the methods assign the test sample to the class that has the smallest deviation among all the classes. This paper analyzes the relationship between face images under different poses and, for the first time, devises a bidirectional representation method-based pattern classification (BRBPC) method for face recognition across pose. BRBPC includes the following three steps: the first step uses the procedure of conventional representation methods to express the test sample and calculates the deviation between the test sample and each class. The second step first expresses the training sample of a class as a weighting sum of the test sample and the training samples from all the other classes and then obtains the corresponding deviation (referred to as complementary deviation). The third step uses the score-level fusion to integrate the scores, that is, deviations generated from the first and second steps for final classification. The experimental results show that BRBPC classifies more accurately than conventional representation methods.	facial recognition system	Jinrong Cui	2012	Neural Computing and Applications	10.1007/s00521-012-1093-0	machine learning;pattern recognition;mathematics;statistics	AI	26.391712853553024	-43.62837842349312	59205
52276e1c3059c873cd4f60c9ca5f56d424bd1ace	hierarchical dictionary learning for invariant classification	sparse representation theory;hierarchy;dictionaries signal processing algorithms feature extraction machine learning testing robustness additive noise biological system modeling robust stability vectors;image processing;classification sparse models dictionary learning hierarchy log polar invariance;log polar mapping;learning;cortical space;training;supervised classification;transformations;biological system modeling;additive noise;signal representation encoding signal classification;testing;sparse models;indexing terms;classification;hierarchical dictionary learning;learning machines;sparse features invariant extraction;hierarchies;invariance;log polar space;robust stability;vectors;machine learning;supervised classification hierarchical dictionary learning invariant classification sparse representation theory sparse features invariant extraction sparse coding cortical space log polar space;feature extraction;signal processing;signal representation;dictionaries;signal classification;classification algorithms;algorithms;dictionary learning;robustness;log polar;invariant classification;signal processing algorithms;sparse representation;encoding;sparse coding;off the shelf;preprint	Sparse representation theory has been increasingly used in the fields of signal processing and machine learning. The standard sparse models are not invariant to spatial transformations such as image rotations, and the representation is very sensitive even under small such distortions. Most studies addressing this problem proposed algorithms which either use transformed data as part of the training set, or are invariant or robust only under minor transformations. In this paper we suggest a framework which extracts sparse features invariant under significant rotations and scalings. The algorithm is based on a hierarchical architecture of dictionary learning for sparse coding in a cortical (log-polar) space. The proposed model is tested in supervised classification applications and proved to be robust under transformed data.	algorithm;dictionary;distortion;machine learning;neural coding;signal processing;sparse matrix;test set	Leah Bar;Guillermo Sapiro	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495916	image processing;k-svd;computer science;theoretical computer science;machine learning;signal processing;pattern recognition;sparse approximation;hierarchy	Vision	29.274735334718564	-42.46271373974518	59263
66d6fbac5e48aa4704ad7bbb1691e46abdd7a5de	on combining compressed sensing and sparse representations for object tracking		The tracking algorithm of compressed sensing takes advantage of the objective's background information, but lacks the feedback mechanism towards the results. The 11 sparse tracking algorithm adapts to the changes in the objectives' appearances but at the cost of losing their background information. To enhance the effectiveness and robustness of the algorithm in coping with such distractions as occlusion and illumination variation, this paper proposes a tracking framework with the 11 sparse representation being the detector and compressed sensing algorithm the tracker, and establishes a complementary classifier model. A second-order model updating strategy has therefore been proposed to preserve the most representative templates in the 11 sparse representations. It is concluded that this tracking algorithm is better than the prevalent 8 ones with a respective precision plot of 77.15ï¾¿%, 72.33ï¾¿% and 81.13ï¾¿% and a respective success plot of 77.67ï¾¿%, 74.01ï¾¿%, 81.51ï¾¿% in terms of the overall, occlusion and illumination variation.		Hang Sun;Jing Li;Bo Du;Dacheng Tao	2016		10.1007/978-3-319-48890-5_4	computer vision;machine learning;pattern recognition	Robotics	33.73011689577626	-47.22780489688215	59349
a46bb6a28bf471c40592be32220efa3c08425e49	weakly supervised learning of indoor geometry by dual warping		A major element of depth perception and 3D understanding is the ability to predict the 3D layout of a scene and its contained objects for a novel pose. Indoor environments are particularly suitable for novel view prediction, since the set of objects in such environments is relatively restricted. In this work we address the task of 3D prediction especially for indoor scenes by leveraging only weak supervision. In the literature 3D scene prediction is usually solved via a 3D voxel grid. However, such methods are limited to estimating rather coarse 3D voxel grids, since predicting entire voxel spaces has large computational costs. Hence, our method operates in image-space rather than in voxel space, and the task of 3D estimation essentially becomes a depth image completion problem. We propose a novel approach to easily generate training data containing depth maps with realistic occlusions, and subsequently train a network for completing those occluded regions. Using multiple publicly available datasets we benchmark our method against existing approaches and are able to obtain superior performance. We further demonstrate the flexibility of our method by presenting results for new view synthesis of RGB-D images.	benchmark (computing);depth map;depth perception;supervised learning;view synthesis;voxel space	Pulak Purkait;Ujwal Bonde;Christopher Zach	2018	2018 International Conference on 3D Vision (3DV)	10.1109/3DV.2018.00089	supervised learning;view synthesis;grid;task analysis;voxel;artificial intelligence;pattern recognition;computer science;rgb color model;image warping;depth perception	Vision	28.83455799038014	-49.4960632213477	59626
1f833d80883c112ca854aca12de671fce2162efe	compact signature-based compressed video matching using dominant color profiles (dcp)	g740 computer vision;compressed video video matching dc image video similarity dominant color profile;color image color analysis feature extraction gray scale quantization signal histograms transform coding;video retrieval computational complexity data compression feature extraction image colour analysis image matching image sequences video coding;g720 knowledge representation;computation complexity compact signature based compressed video matching dominant color profiles dcp compressed video shot matching dominant color sequence spike sequence human retinal representation integer values real time processing maximum matching video set retrieval mpeg compressed videos dc image color feature extraction visual aspects i frame standard datasets	This paper presents a novel technique for efficient and generic matching of compressed video shots, through compact signatures extracted directly without decompression. The compact signature is based on the Dominant Color Profile (DCP), a sequence of dominant colors extracted and arranged as a sequence of spikes in analogy to the human retinal representation of a scene. The proposed signature represents a given video shot with ~490 integer values, facilitating for real time processing to retrieve a maximum set of matching videos. The technique is able to work directly on MPEG compressed videos, without full decompression, as it utilizes the DC-image as a base for extracting color features. The DC-image has a highly reduced size, while retaining most of visual aspects, and provides high performance compared to the full I-frame. The experiments and results on various standard datasets show the promising performance, both the accuracy and the efficient computation complexity, of the proposed technique.	color;computation;data compression;experiment;intra-frame coding;moving picture experts group;real-time computing;type signature	Saddam Bekhet;Amr Ahmed	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.674	video compression picture types;rgb color model;computer vision;color depth;computer science;video tracking;pattern recognition;multiview video coding;computer graphics (images)	Vision	37.912206111653475	-52.00017016573678	59820
afb929a49988fa4ec08aa9a004f5b1c5d0d4e082	perceptual 3d pose distance estimation by boosting relational geometric features		Traditional pose similarity functions based on joint coordinates or rotations often do not conform to human perception. We propose a new perceptual pose distance: Relational Geometric Distance that accumulates the differences over a set of features that reflects the geometric relations between different body parts. An extensive relational geometric feature pool that contains a large number of potential features is defined, and the features effective for pose similarity estimation are selected using a set of labeled data by Adaboost. The extensive feature pool guarantees that a wide diversity of features is considered, and the boosting ensures that the selected features are optimized when used jointly. Finally, the selected features form a pose distance function that can be used for novel poses. Experiments show that ourmethod outperforms others in emulating human perception in pose similarity. Our method can also adapt to specific motion types and capture the features that are important for pose similarity of a certain motion type. Copyright # 2009 John Wiley & Sons, Ltd.	adaboost;emulator;john d. wiley	Cheng Chen;Yueting Zhuang;Jun Xiao;Zhang Liang	2009	Journal of Visualization and Computer Animation	10.1002/cav.297	computer vision;pose;3d pose estimation;machine learning;pattern recognition	Visualization	34.64580190262566	-51.39205319233217	60043
119c655d0029e454e2aa732e0117966424f069fa	in-vehicle camera traffic sign detection and recognition	recognition systems;regression tracking;keywords adaboost algorithm;in vehicle;real time traffics;detection accuracy;real time traffic;mean shift;journal article;object detectors;practical realizations;image pairs;traffic sign recognition;time use;on the fly;feature representation;confidence weighted mean shift;false positive;theoretical foundation;multi class;simboost	In this paper, we discuss theoretical foundations and a practical realization of a real-time traffic sign detection, tracking and recognition system operating on board of a vehicle. In the proposed framework, a generic detector refinement procedure based on mean shift clustering is introduced. This technique is shown to improve the detection accuracy and reduce the number of false positives for a broad class of object detectors for which a soft response’s confidence can be sensibly estimated. The track of an already established candidate is maintained over time using an instance-specific tracking function that encodes the relationship between a unique feature representation of the target object and the affine distortions it is subject to. We show that this function can be learned on-the-fly via regression from random transformations applied to the image of the object in known pose. Secondly, we demonstrate its capability of reconstructing the full-face view of a sign from substantial view angles. In the recognition stage, a concept of class similarity measure learned from image pairs is discussed and its realization using SimBoost, a novel version of AdaBoost algorithm, is analyzed. Suitability of the proposed method for solving multi-class traffic sign classification problems is shown experimentally for different feature representations of an image. Overall performance of our system is evaluated based on a prototype C++ implementation. Illustrative output generated by this demo application is provided as a supplementary material attached to this paper.	adaboost;algorithm;c++;cluster analysis;distortion;experiment;high- and low-level;mean shift;object detection;prototype;real life;real-time clock;refinement (computing);sensor;similarity measure;synthetic intelligence;visual descriptor	Andrzej Ruta;Fatih Murat Porikli;Shintaro Watanabe;Yongmin Li	2009	Machine Vision and Applications	10.1007/s00138-009-0231-x	computer vision;simulation;mean-shift;type i and type ii errors;computer science;artificial intelligence;machine learning;traffic sign recognition;statistics	Vision	35.29630301286767	-51.253126515445985	60175
f49d62339de9e9cb7e75d0282c438c1a8996dd4c	video human motion recognition using knowledge-based hybrid method	image recognition;image motion analysis;human computer interaction;video signal processing;hidden markov model;training;data mining;noniterative method video human motion recognition knowledge based hybrid method video data training data machine learning hidden markov model baum welch algorithm;computer vision;baum welch;motion capture;assisted living;training data;noniterative method;hidden markov models;hybrid method;machine learning;3d motion capture;video human motion recognition;three dimensional displays;feature extraction;human motion;video signal processing hidden markov models image motion analysis image recognition knowledge based systems learning artificial intelligence;three dimensional displays humans feature extraction hidden markov models training computer vision data mining;knowledge based hybrid method;humans;learning artificial intelligence;video data;baum welch algorithm;knowledge based systems;human computer interaction video human motion recognition 3d motion capture hidden markov models;knowledge base	Human motion recognition in video data has several interesting applications in fields such as gaming, senior/assisted living environments, and surveillance. In these scenarios, we might have to consider adding new motion classes (i.e. new types of human motions to be recognized) as well as new training data (say, for handling different type of subjects). Hence, both accuracy of classification and training time for the machine learning algorithms become important performance parameters in these cases. In this paper, we propose a Knowledge Based Hybrid (KBH) method that can compute the probabilities for Hidden Markov Models (HMMs) associated with different human motion classes. This computation is facilitated by appropriately mixing features from two different media types (3D motion capture and 2D video). We conducted a variety of experiments comparing the proposed KBH for HMMs and the traditional Baum-Welch algorithms. With the advantage of computing the HMMs parameters in a non-iterative manner, the KBH method outperforms the Baum-Welch algorithm both in terms of accuracy as well as reduced training time.	baum–welch algorithm;computation;experiment;feature data;hidden markov model;iterative method;kinesiology;knowledge base;lempel–ziv–welch;machine learning;markov chain;maxima and minima;motion capture;norm (social);speech recognition;welch's method	Myunghoon Suk;Ashok Ramadass;Yohan Jin;B. Prabhakaran	2010	2010 IEEE International Symposium on Multimedia	10.1109/ISM.2010.19	computer vision;computer science;baum–welch algorithm;machine learning;pattern recognition;hidden markov model	Vision	37.58152667662862	-46.53291684646481	60343
3c08c5ea2e5459b2a816e0bd443a912fe517eaf9	improved robust tensor principal component analysis via low-rank core matrix		Robust principal component analysis (RPCA) has been widely used for many data analysis problems in matrix data. Robust tensor principal component analysis (RTPCA) aims to extract the low rank and sparse components of multidimensional data, which is a generation of RPCA. The current RTPCA methods are directly based on tensor singular value decomposition (t-SVD), which is a new tensor decomposition method similar to singular value decomposition (SVD) in matrices. These methods focus on utilizing different sparse constraints for real applications and make less analysis for tensor nuclear norm (TNN) defined in t-SVD. However, we find low-rank structure still exists in the core tensor and existing methods can not fully extract the low-rank structure of tensor data. To further exploit the low-rank structures in multiway data, we extract low-rank component for the core matrix whose entries are from the diagonal elements of the core tensor. Based on this idea, we have defined a new TNN that extends TNN with core matrix and propose a creative algorithm to deal with RTPCA problems. The results of numerical experiments show that the proposed method outperforms state-of-the-art methods in terms of both accuracy and computational complexity.		Yipeng Liu;Longxi Chen;Ce Zhu	2018	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2018.2873142	tensor;mathematical optimization;sparse matrix;computer science;principal component analysis;robust principal component analysis;matrix (mathematics);matrix decomposition;singular value decomposition;matrix norm	ML	27.517752982958598	-39.379542771542376	60389
6464c95a078a4417c061ffb639404b99702cadba	understanding dyadic interactions applying proxemic theory on videosurveillance trajectories	video surveillance;surveillance;social sciences;image classification;labelled training set dyadic interactions proxemic theory video surveillance trajectories social behaviour understanding collective people behaviour understanding open spaces sociological theories human behaviour classification human behaviour interpretation behavioural rules social involvement proxemic analysis proxemic states sequences people pairs symbol sequence elastic measure interaction classification mutual interactions most frequent dyads interactions online classification;computer vision;video surveillance behavioural sciences image classification social sciences;computational modeling;trajectory;time series analysis;feature extraction;trajectory time series analysis surveillance computational modeling computer vision feature extraction context;behavioural sciences;context	Understanding social and collective people behaviour in open spaces is one of the frontier of modern video surveillance. Many sociological theories, and proxemics in particular, have been proved their validity as a support for classifying and interpreting human behaviour. Proxemics suggest some simple but effective behavioural rules, useful to understand what people are doing and their social involvement with other individuals. In this paper we propose to extend the proxemics analysis along the time and provide a solution for analysing sequences of proxemic states computed between trajectories of people pairs (dyads). Trajectories, computed from videosurveillance videos, are first analysed and converted to a sequence of symbols according to proxemic theory. Then an elastic measure for comparing those sequences is introduced. Finally, interactions are classified both in an off-line unsupervised way and in an on-line fashion. Results on videosurveillance data, demonstrate that sequences of proxemic states can be effective in characterizing mutual interactions and experiments in capturing the most frequent dyads interactions and on-line classifying them when a labelled training set is available are proposed.	closed-circuit television;computer vision;dyadic transformation;experiment;interaction;online and offline;string (computer science);test set;theory;tracking system;unsupervised learning	Simone Calderara;Rita Cucchiara	2012	2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2012.6239351	computer vision;contextual image classification;feature extraction;behavioural sciences;computer science;artificial intelligence;trajectory;machine learning;time series;computational model	Vision	37.72940601091488	-47.40679901372293	60498
2859668b5c303f8ea74ee100ef226f390f601b53	a new feature selection algorithm for multispectral and polarimetric vehicle images	geophysical image processing;remote sensing feature selection multispectral data object detection polarimetric data;image recognition;object detection remote sensing classification algorithms vehicle detection object recognition target recognition application software land vehicles displays polarization;vehicles feature extraction geophysical image processing image recognition remote sensing spectral analysis target tracking;training;search algorithm;suboptimal search algorithms feature selection algorithm multispectral vehicle images polarimetric vehicle images automatic target recognition remote sensing generalized steepest ascent feature selection technique prior steepest ascent algorithm classical sequential forward floating selection algorithm;feature extraction;remote sensing;polarimetric data;pixel;classification algorithms;automatic target recognition;multispectral data;pattern recognition;feature selection;vehicles;target tracking;spectral analysis;object detection	Multispectral and polarimetric data have been shown to provide detailed information useful for automatic target recognition applications. A major limitation of using these data in remote sensing is that they often consist of a large number of features with an inadequate number of samples. To reduce the number of features, we thus present a new generalized steepest ascent feature selection technique that selects only a small subset of important features to use for classification. Our proposed algorithm improves upon the prior steepest ascent algorithm by selecting a better starting search point and performing a more thorough search. It is guaranteed to provide solutions that equal or exceed those of the classical sequential forward floating selection algorithm. Initial results for one multispectral and polarimetric data set show that our algorithm yields better classification results than other suboptimal search algorithms.	automatic target recognition;feature selection;gradient descent;multispectral image;polarimetry;search algorithm;selection algorithm;times ascent	Songyot Nakariyakul	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414581	computer vision;feature extraction;computer science;machine learning;pattern recognition;feature selection;pixel;automatic target recognition;search algorithm	Robotics	32.86397980008352	-43.48199441571621	60558
bb72de68c28fb7c3a34f650bd746db97d88c5b02	k-way tree classification based on semi-greedy structure applied to multisource remote sensing images	remote sensing image;geophysical image processing;vegetation mapping;classification tree analysis remote sensing space technology boolean functions hyperspectral sensors hyperspectral imaging error analysis computer science computer networks modis;pacrim ii campaign k way tree classification semi greedy structure multisource remote sensing images generalized positive boolean function minimum classification error positive labeled samples negative labeled samples d dimensional hyperplane statistical ratios ktsg learning modules modis airborne simulator aster airborne simulator master hyperspectral images airborne synthetic aperture radar images land cover classification;bottom up;boolean functions;top down;supervised classification;land cover classification;image classification;greedy algorithms;radiometry;remote sensing by radar;positive boolean function;vegetation mapping airborne radar boolean functions geophysical image processing greedy algorithms image classification radiometry remote sensing by radar;airborne radar;minimum classification error;classification accuracy;hyperspectral image;synthetic aperture radar	In this paper we present a new supervised classification method, referred to as the k-way tree semi-greedy (KTSG) classifier, for the classification of multisource remote sensing images. The generalized positive Boolean function (GPBF) classifier scheme is recently proposed based on minimum classification error (MCE) criteria to improve classification performance. It makes use of MCE criteria to apply positive and negative samples as training parameters. Unfortunately, the classification performance of GPBF is limited when the number of classes increases. This is occurred in training phase by the unbalanced numbers of positive and negative samples caused by the use of a large number of classes. The proposed KTSG overcomes this drawback by modifying the scheme from the perception of pattern-node based semi-greedy (bottom-up scheme used in GPBF) to the conception of region-based semi-greedy (also known as the top-down scheme in KTSG). It is organized by a k-way tree in which every node is composed of a set of k-dimensional positive and negative labeled samples as represented as a percentage, i.e. the corresponding ratio of number of a specific (positive) class samples to the total number of the other (negative) classes. It iteratively divides the d-dimensional hyperplane into 2d subspaces according to the centroids of the labeled (training) samples of all classes. The statistical ratios between different classes are then compared as a basis for stopping the new subspace separation and identifying which subspace belongs to which class. By delivering both positive and negative samples of different classes to KTSG learning modules, KTSG outperforms GPBF and traditional classifiers in terms of classification accuracies. The effectiveness of the proposed KTSG is evaluated by fusing MODIS/ASTER airborne simulator (MASTER) hyperspectral images and airborne synthetic aperture radar (AIRSAR) images for land cover classification during the Pacrim II campaign.	airborne ranger;aperture (software);greedy algorithm;k-ary tree;linuxmce;machine learning;semiconductor industry;simulation;supervised learning;synthetic data;tinymce;top-down and bottom-up design;unbalanced circuit	Yang-Lang Chang;Zhi-Ming Chen;Jyh-Perng Fang;Wei-Lieh Hsu;Wen-Yew Liang;Tung-Ju Hsieh;Hsuan Ren;Kunshan Chen	2009	2009 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2009.5417939	computer vision;computer science;machine learning;classification rule;top-down and bottom-up design;soft independent modelling of class analogies;one-class classification;physics;remote sensing	Vision	30.51644284884377	-42.86705652604201	61066
4dfefcc717ff4928ae374ed30225437962a15f07	scene object recognition for mobile robots through semantic knowledge and probabilistic graphical models	object recognition;expert systems;probabilistic graphical models;mobile robotics;semantic knowledge;autonomous agents	Scene object recognition is an essential requirement for intelligent mobile robots. In addition to geometric or appearance features, modern recognition systems strive to incorporate contextual information, normally modelled through Probabilistic Graphical Models (PGMs) or Semantic Knowledge (SK). However, these approaches, separately, show some weaknesses that limit their application, e.g., the exponential complexity of the probabilistic inference over PGMs or the inability of SK to handle uncertainty. This paper presents a hybrid PGM-SK system for object recognition that integrates both techniques reducing their individual limitations and gaining in probabilistic inference efficiency, performance robustness, uncertainty handling, and providing coherent results according to domain knowledge codified by a human expert. We support this claim with an extensive experimental evaluation according to both recognition success and time requirements in real scenarios from two datasets (NYU2 and UMA-offices). The yielded figures support the suitability of the hybrid PGM-SK recognition system, and its applicability to mobile robotic agents.	approximation algorithm;baseline (configuration management);category theory;coherence (physics);computer monitor;conditional random field;floating-point unit;graphical model;high- and low-level;holism;hybrid system;iterated conditional modes;iterated function;knowledge representation and reasoning;mobile robot;outline of object recognition;reduction (complexity);requirement;robotics;robustness (computer science);ski combinator calculus;semantic reasoner;time complexity;user-managed access	José-Raúl Ruiz-Sarmiento;Cipriano Galindo;Javier González	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.07.033	semantic memory;computer science;artificial intelligence;autonomous agent;cognitive neuroscience of visual object recognition;machine learning;data mining;graphical model;expert system	AI	30.573492286271993	-48.45311995050552	61314
878f90055f639711f29dd566ab341697a13f9a1b	improving dimensionality reduction with spectral gradient descent	dimensionalidad;iterative method;optimisation;optimizacion;learning;analisis datos;metodo descenso;gradient method;dimensionality;matriz simetrica;power method;fonction objectif;symmetric matrix;metodo iterativo;eigenvalue;aprendizaje;objective function;methode gradient;data analysis;apprentissage;metodo gradiente;gradient descent;dimensionnalite;methode iterative;valor propio;funcion objetivo;analyse donnee;matrice symetrique;valeur propre;optimization;descent method;local minima;dimensional reduction;methode descente	We introduce spectral gradient descent, a way of improving iterative dimensionality reduction techniques. The method uses information contained in the leading eigenvalues of a data affinity matrix to modify the steps taken during a gradient-based optimization procedure. We show that the approach is able to speed up the optimization and to help dimensionality reduction methods find better local minima of their objective functions. We also provide an interpretation of our approach in terms of the power method for finding the leading eigenvalues of a symmetric matrix and verify the usefulness of the approach in some simple experiments.	affinity analysis;common flash memory interface;conjugate gradient method;contain (action);dimensionality reduction;experiment;gradient descent;immunostimulating conjugate (antigen);iterative method;large;line search;machine learning;mathematical optimization;maxima and minima;newton;power iteration;processor affinity;rate of convergence;silo (dataset);spectral method;speedup;pegvisomant	Roland Memisevic;Geoffrey E. Hinton	2005	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2005.06.034	gradient descent;mathematical optimization;curse of dimensionality;power iteration;eigenvalues and eigenvectors;gradient method;maxima and minima;calculus;mathematics;geometry;iterative method;data analysis;symmetric matrix	ML	24.685879426654537	-38.228914591426125	61335
674fcd61f195e0255069b32ac0ffda2e93fafa75	unsupervised manifold learning based on multiple feature spaces	non linear dimensionality reduction;manifold learning;shape retrieval;image retrieval	Manifold learning is a well-known dimensionality reduction scheme which can detect intrinsic low-dimensional structures in non-linear high-dimensional data. It has been recently widely employed in data analysis, pattern recognition, and machine learning applications. Isomap is one of the most promising manifold learning algorithms, which extends metric multi-dimensional scaling by using approximate geodesic distance. However, when Isomap is conducted on real-world applications, it may have some difficulties in dealing with noisy data. Although many applications represent a special sample by multiple feature vectors in different spaces, Isomap employs samples in unique observation space. In this paper, two extended versions of Isomap to multiple feature spaces problem, namely fusion of dissimilarities and fusion of geodesic distances, are presented. We have employed the advantages of several spaces and depicted the Euclidean distance on learned manifold that is more compatible to the semantic distance. To show the effectiveness and validity of the proposed method, some experiments have been carried out on the application of shape analysis on MPEG7 CE Part B and Fish data sets.	approximation algorithm;automatic image annotation;distance (graph theory);euclidean distance;experiment;isomap;mpeg-7;machine learning;multidimensional scaling;nonlinear dimensionality reduction;nonlinear system;outline of object recognition;pattern recognition;sensor;shape analysis (digital geometry);signal-to-noise ratio;supervised learning;unsupervised learning	Mohammad Ali Zare Chahooki;Nasrollah Moghaddam Charkari	2014	Machine Vision and Applications	10.1007/s00138-014-0604-7	computer vision;image retrieval;computer science;machine learning;pattern recognition;mathematics;nonlinear dimensionality reduction;dimensionality reduction;manifold alignment	AI	25.098886070515025	-41.56356385813275	61410
620cacd0ce49550def7d08f86166bd28d947ed0e	local-global background modeling for anomaly detection in hyperspectral images	local global algorithm local global background modeling anomaly detection hyperspectral image statistical background modeling;background modeling;gaussian mixture;image processing;hyperspectral images;degree of freedom;anomaly detection;local global algorithm;statistical background modeling;false alarm rate;unsupervised anomaly detection;data mining;statistical analysis;estimation;statistical analysis image processing;pixel;clustering algorithms;robustness;hyperspectral imaging detection algorithms hyperspectral sensors remote sensing training data design methodology detectors greedy algorithms vegetation mapping soil;local global background modeling;hyperspectral imaging;hyperspectral image;hyperspectral images background modeling unsupervised anomaly detection	In this paper, we address the problem of unsupervised detection of anomalies in hyperspectral images. Our proposed method is based on a novel statistical background modeling approach that combines local and global approaches. The local-global background model has the ability to adapt to all nuances of the background process like local approaches but avoids over-fitting due to a too high number of degrees of freedom, which produces a high false alarm rate. This is done by constraining the local background models to be interrelated. The results strongly prove the effectiveness of the proposed algorithm. We experimentally show that our localglobal algorithm performs better than several other global or local anomaly detection techniques, such as the well known RX or its Gaussian Mixture version (GMRX).	algorithm;anomaly detection;background process;consistency model;experiment;overfitting;unsupervised learning;whole earth 'lectronic link	Eyal Madar;Oleg Kuybeda;David Malah;Meir Barzohar	2009	2009 First Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing	10.1109/WHISPERS.2009.5289036	computer vision;geography;machine learning;remote sensing	AI	30.931043950972622	-43.17055808775265	61467
993b2883923b82ce71588acd189cd8dae9c9de5d	entropy-based sparse trajectories prediction enhanced by matrix factorization		Existing moving object’s trajectory prediction algorithms suffer from the data sparsity problem, which affects the accuracy of the trajectory prediction. Aiming to the problem, we present an Entropy-based Sparse Trajectories Prediction method enhanced by Matrix Factorization (ESTP-MF). Firstly, we do trajectory synthesis based on trajectory entropy and put synthesized trajectories into the trajectory space. It can resolve the sparse problem of trajectory data and make the new trajectory space more reliable. Secondly, under the new trajectory space, we introduce matrix factorization into Markov models to improve the sparse trajectory prediction. It uses matrix factorization to infer transition probabilities of the missing regions in terms of corresponding existing elements in the transition probability matrix. It aims to further solve the problem of data sparsity. Experiments with a real trajectory dataset show that ESTP-MF generally improves prediction accuracy by as much as 6% and 4% compared to the SubSyn algorithm and STP-EE algorithm respectively. key words: trajectory prediction, data sparsity, entropy estimation, matrix factorization	algorithm;entropy estimation;markov chain;markov model;sparse matrix;stochastic matrix	Lei Zhang;Qingfu Fan;Wen Li;Zhizhen Liang;Guoxing Zhang;Tongyang Luo	2017	IEICE Transactions		pattern recognition;incomplete lu factorization;computer science;artificial intelligence;sparse matrix;matrix decomposition;sparse approximation;entropy estimation;incomplete cholesky factorization	ML	28.26836551902166	-40.742298881013525	61591
41860b45915c7674a9e0706115435d91aec970a3	a structural approach to person re-identification problem		Although it has been studied extensively during past decades, object tracking is still a difficult problem due to many challenges. Several improvements have been done, but more and more complex scenes (dense crowd, complex interactions) need more sophisticated approaches. Particularly long-term tracking is an interesting problem that allow to track objects even after it may become longtime occluded or it leave/re-enter the field-of-view. In this case the major challenges are significantly changes in appearance, scale and so on. At the heart of the solution of long-term tracking is the re-identification technique, that allows to identify an object coming back visible after an occlusion or re-entering on the scene. This paper proposes an approach for pedestrian re-identification based on structural representation of people. The experimental evaluation is carried out on two public data sets (ETHZ and CAVIAR4REID datasets) and they show promising results compared to others state-of-the-art approaches.		Amal Mahboubi;Luc Brun;Donatello Conte	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8545640	structural approach;task analysis;computer vision;feature extraction;artificial intelligence;video tracking;parameter identification problem;pattern recognition;computer science	Vision	33.19139922642174	-50.71069041619076	61779
3b75c8c3087f999556a7c9219a766883d59cde6c	face detection with information-based maximum discrimination	databases;likelihood ratio;probability;neural networks;learning artificial intelligence markov processes face recognition;learning techniques;discrete markov processes;information based discrimination;information based visual learning training set discrete markov processes face detection probability models likelihood ratio detection process maximum discrimination;computer networks;computer vision;training set;detection process;face recognition;maximum discrimination;visual learning;probability models;face detection neural networks markov processes computer vision statistics probability computer networks databases pattern recognition knowledge based systems;markov process;statistics;pattern recognition;information based;markov processes;learning artificial intelligence;face detection;knowledge based systems	In this paper we present a visual learning technique that maximizes the discrimination between positive and negative examples in a training set. We demonstrate our technique in the context of face detection with complex background without color or motion information, which has proven to be a challenging problem. We use a family of discrete Markov processes to model the face and background patterns and estimate the probability models using the data statistics. Then, we convert the learning process into an optimization, selecting the Markov process that optimizes the information-based discrimination between the two classes. The detection process is carried out by computing the likelihood ratio using the probability model obtained from the learning procedure. We show that because of the discrete nature of these models, the detection process is, by almost two orders of magnitude, less computationally expensive than neural network approaches. However, no improvement in terms of correct-answer/false-alarm tradeoo is achieved.	analysis of algorithms;artificial neural network;face detection;mathematical optimization;test set;visual learning	Antonio Colmenarez;Thomas S. Huang	1997		10.1109/CVPR.1997.609415	facial recognition system;computer vision;computer science;knowledge-based systems;machine learning;pattern recognition;markov process;artificial neural network;statistics	Vision	32.8395154934625	-41.50877262960432	61813
9713e87734046cb7a2d040d71fa35cf93472ea80	granular-based dense crowd density estimation		Dense crowd density estimation is one of the fundamental tasks in crowd analysis. While tremendous progress has been made to understand crowd scenes along with the rise of Convolutional Neural Networks (CNNs), research work on dense crowd density estimation is still an ongoing process. In this paper, we propose a novel approach to learn discriminative crowd features from granules, that conforms to the outline between crowd and background (i.e. non-crowd) regions, for density estimation. It shows that by studying the inner statistics of granules for density estimation, this approach is adaptive to arbitrary distribution of crowd (i.e. scene independent). Multiple features fusion is proposed to learn discriminative crowd features from granules. This is to be used as description of the crowd where a direct mapping between the features and crowd density is learned. Extensive experiments on public benchmark datasets demonstrate the effectiveness of our novel approach for scene independent dense crowd density estimation.	benchmark (computing);convolutional neural network;crowdsourcing;discriminative model;experiment;ground truth;human error;kernel density estimation	Ven Jyn Kok;Chee Seng Chan	2017	Multimedia Tools and Applications	10.1007/s11042-017-5418-y	pattern recognition;artificial intelligence;discriminative model;convolutional neural network;computer science;crowd analysis;density estimation	Vision	30.33470646507997	-50.36723249891127	61916
b730908bc1f80b711c031f3ea459e4de09a3d324	active orientation models for face alignment in-the-wild	principal component analysis computational complexity face recognition optimisation;face alignment active orientation models active appearance models;active appearance model shape deformable models face robustness principal component analysis;matlab code active orientation models face alignment in the wild aoms generative models facial shape active appearance models aams generic face alignment unconstrained conditions image gradient orientations optimization frameworks aam learning kernel principal component analysis model fitting computational cost computational complexity project out inverse compositional algorithm	We present Active Orientation Models (AOMs), generative models of facial shape and appearance, which extend the well-known paradigm of Active Appearance Models (AAMs) for the case of generic face alignment under unconstrained conditions. Robustness stems from the fact that the proposed AOMs employ a statistically robust appearance model based on the principal components of image gradient orientations. We show that when incorporated within standard optimization frameworks for AAM learning and fitting, this kernel Principal Component Analysis results in robust algorithms for model fitting. At the same time, the resulting optimization problems maintain the same computational cost. As a result, the main similarity of AOMs with AAMs is the computational complexity. In particular, the project-out version of AOMs is as computationally efficient as the standard project-out inverse compositional algorithm, which is admittedly one of the fastest algorithms for fitting AAMs. We verify experimentally that: 1) AOMs generalize well to unseen variations and 2) outperform all other state-of-the-art AAM methods considered by a large margin. This performance improvement brings AOMs at least in par with other contemporary methods for face alignment. Finally, we provide MATLAB code at http://ibug.doc.ic.ac.uk/resources.	active appearance model;algorithmic efficiency;associate-o-matic;automatic acoustic management;channel length modulation;computation;computational complexity theory;curve fitting;experiment;fastest;gauss–newton algorithm;image gradient;kernel principal component analysis;matlab;mathematical optimization;maxima and minima;newton;optimization problem;programming paradigm;test set;whole earth 'lectronic link	Georgios Tzimiropoulos;Joan Alabort-i-Medina;Stefanos P. Zafeiriou;Maja Pantic	2014	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2014.2361018	computer vision;computer science;machine learning;pattern recognition	Vision	34.54702410953237	-42.75520508807221	61959
13636bbe75556f069eeccd6a84329d9fc4a057fd	label consistent fisher vectors for supervised feature aggregation	vectors kernel principal component analysis matrix decomposition training accuracy feature extraction;feature aggregation supervised information fisher kernel image classification;matrix algebra image classification image representation image retrieval learning artificial intelligence;discriminative label comparison matrix label consistent fisher vectors supervised feature aggregation image representation method image classification image retrieval fisher kernel training process	In this paper, we present a simple and efficient way to add supervised information into Fisher vectors, which has become a popular image representation method for image classification and retrieval purposes in recent years. The basic idea of our approach is to improve the Fisher kernel in the training process by adding a discriminative label comparison matrix to it. The resulting new representations, which we call Label Consistent Fisher Vectors (LCFV), can be solved for both over determined and underdetermined cases. We show that LCFV has better classification performance than traditional Fisher vectors on three public datasets.	computer vision;feature vector;fisher kernel;fisher–yates shuffle	Quan Wang;Xin Shen;Meng Wang;Kim L. Boyer	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.617	computer vision;kernel fisher discriminant analysis;machine learning;pattern recognition;mathematics;feature;fisher kernel	Vision	26.10677971582151	-43.32045262211886	62077
877c5a3ad915c2bbb5595252d08163f34ce58957	lcr-net: localization-classification-regression for human pose		We propose an end-to-end architecture for joint 2D and 3D human pose estimation in natural images. Key to our approach is the generation and scoring of a number of pose proposals per image, which allows us to predict 2D and 3D pose of multiple people simultaneously. Hence, our approach does not require an approximate localization of the humans for initialization. Our architecture, named LCR-Net, contains 3 main components: 1) the pose proposal generator that suggests potential poses at different locations in the image, 2) a classifier that scores the different pose proposals, and 3) a regressor that refines pose proposals both in 2D and 3D. All three stages share the convolutional feature layers and are trained jointly. The final pose estimation is obtained by integrating over neighboring pose hypotheses, which is shown to improve over a standard non maximum suppression algorithm. Our approach significantly outperforms the state of the art in 3D pose estimation on Human3.6M, a controlled environment. Moreover, it shows promising results on real images for both single and multi-person subsets of the MPII 2D pose benchmark.	3d pose estimation;approximation algorithm;benchmark (computing);end-to-end principle;zero suppression	Grégory Rogez;Philippe Weinzaepfel;Cordelia Schmid	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.134	architecture;computer vision;initialization;pattern recognition;artificial intelligence;articulated body pose estimation;real image;3d pose estimation;computer science;pose	Vision	30.22929344363138	-50.80096021507604	62118
26e9776d51794b1540d5fceff2c026ceab5a2c23	active post-refined multimodality video semantic concept detection with tensor representation	semantic concept detection;learning algorithm;high dimensionality;dimension reduction;active learning;video analysis;higher order;support tensor machines stm;hosvd;temporal associated cooccurrence tac;temporal dependency;tensorshot;contextual correlation;euclidean space;multi modality video semantic concept detection;semantic analysis	In this paper, we resolve the problem of multi-modality video representation and semantic concept detection. Interaction and integration of multi-modality media types such as visual, audio and textual data in video are essential to video semantic analysis. Traditionally, videos are represented as vectors in the Euclidean space. Many learning algorithms are then taken to these vectors in a high dimensional space for dimension reduction, classification, clustering and so on. However, the multiple modalities in video not only have their own properties, but also have correlations among them; whereas the simple vector representation weakens the power of these relatively independent modalities and even ignores their relations to some extent. In this paper, we introduce a higher-order tensor framework for video analysis, in which we represent image, video and text three modalities in video shots as data points by the 3rd-order tensor called tensorshots. We propose a novel dimension reduction method that explicitly considers the manifold structure of the tensor space from multimodal media data which is temporal associated co-occurrence and then detect video semantic concepts through powerful classifiers which take tensor as input. Our algorithm preserves the intrinsic structure of the submanifold where tensorshots are sampled, and is also able to map out-of-sample data points directly. Moreover we apply an active learning based contextual and temporal post-refining strategy to enhance detection accuracy. Experiment results show that our method improves the performance of video semantic concept detection.	active learning (machine learning);algorithm;cluster analysis;data point;dimensionality reduction;machine learning;modality (human–computer interaction);multimodal interaction;statistical classification;text corpus;video content analysis	Yanan Liu;Fei Wu;Yueting Zhuang;Jun Xiao	2008		10.1145/1459359.1459372	computer vision;higher-order logic;computer science;euclidean space;machine learning;video tracking;pattern recognition;active learning;dimensionality reduction	Vision	25.359211247598733	-45.86199558275862	62131
1a83a63c645f717950b65d22fe53be208e6de71d	rotational invariant dimensionality reduction algorithms	rotational invariant ri subspace learning dimensionality reduction image classification image feature extraction;dimensionality reduction image classification image feature extraction rotational invariant ri subspace learning;robustness algorithm design and analysis feature extraction principal component analysis learning systems measurement optimization	"""A common intrinsic limitation of the traditional subspace learning methods is the sensitivity to the outliers and the image variations of the object since they use the <inline-formula> <tex-math notation=""""LaTeX"""">$\boldsymbol {L_{2}} $ </tex-math></inline-formula> norm as the metric. In this paper, a series of methods based on the <inline-formula> <tex-math notation=""""LaTeX"""">$\boldsymbol {L_{2,1}} $ </tex-math></inline-formula>-norm are proposed for linear dimensionality reduction. Since the <inline-formula> <tex-math notation=""""LaTeX"""">$\boldsymbol {L_{2,1}} $ </tex-math></inline-formula>-norm based objective function is robust to the image variations, the proposed algorithms can perform robust image feature extraction for classification. We use different ideas to design different algorithms and obtain a unified rotational invariant (RI) dimensionality reduction framework, which extends the well-known graph embedding algorithm framework to a more generalized form. We provide the comprehensive analyses to show the essential properties of the proposed algorithm framework. This paper indicates that the optimization problems have global optimal solutions when all the orthogonal projections of the data space are computed and used. Experimental results on popular image datasets indicate that the proposed RI dimensionality reduction algorithms can obtain competitive performance compared with the previous <inline-formula> <tex-math notation=""""LaTeX"""">$\boldsymbol {L_{2}}$ </tex-math></inline-formula> norm based subspace learning algorithms."""	algorithm;anterior descending branch of left coronary artery;cd74 gene;computational complexity theory;convergence (action);data point;dataspaces;dimensionality reduction;essence;feature (computer vision);feature extraction;graph - visual representation;graph embedding;local-density approximation;mfa message structure;machine learning;mathematical optimization;optimization problem;principal component analysis;projections and predictions;published database;rs-232;solutions;t-norm;taxicab geometry;whole earth 'lectronic link	Yong Xu;Jian Yang;Linlin Shen;Ronald M Summers	2017	IEEE Transactions on Cybernetics	10.1109/TCYB.2016.2578642	diffusion map;computer vision;feature extraction;machine learning;pattern recognition;mathematics;k-nearest neighbors algorithm;dimensionality reduction	Vision	24.974752466944707	-41.46541477025902	62177
528c03761682f73eed7d736c19551856fe92b1e1	uncovering interactions and interactors: joint estimation of head, body orientation and f-formations from surveillance videos	joint inference framework interaction uncovering head body orientation estimation f formations surveillance videos distant social scene cocktail party surveillance cameras coupled head body pose learning temporal consistency occlusions low resolution data handling;head videos surveillance foot target tracking;video surveillance cameras image resolution learning artificial intelligence pose estimation	We present a novel approach for jointly estimating targets' head, body orientations and conversational groups called F-formations from a distant social scene (e.g., a cocktail party captured by surveillance cameras). Differing from related works that have (i) coupled head and body pose learning by exploiting the limited range of orientations that the two can jointly take, or (ii) determined F-formations based on the mutual head (but not body) orientations of interactors, we present a unified framework to jointly infer both (i) and (ii). Apart from exploiting spatial and orientation relationships, we also integrate cues pertaining to temporal consistency and occlusions, which are beneficial while handling low-resolution data under surveillance settings. Efficacy of the joint inference framework reflects via increased head, body pose and F-formation estimation accuracy over the state-of-the-art, as confirmed by extensive experiments on two social datasets.	algorithm;closed-circuit television;experiment;interaction;multimodal interaction;sensor;unified framework;wearable computer	Elisa Ricci;Jagannadan Varadarajan;Subramanian Ramanathan;Samuel Rota Bulò;Narendra Ahuja;Oswald Lanz	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.529	computer vision;simulation	Vision	34.59481248791185	-49.12843366834718	62193
02e0886d7f7fab397ace91b6d9abf8e547c34737	probabilistic situation recognition for vehicular traffic scenarios	lasers;sequences;temporal context;uncertain systems;probability;state sequence probabilistic situation recognition vehicular traffic scenario dynamic environment temporal context ambiguous interpretation hidden markov model;intelligent robots;uncertainty;hidden markov model;road traffic;hidden markov models traffic control vehicle dynamics uncertainty predictive models state estimation robotics and automation intelligent robots intelligent vehicles remotely operated vehicles;automated highways;traffic control;probabilistic situation recognition;remotely operated vehicles;probabilistic approach;data mining;state estimation;ambiguous interpretation;dynamic environment;computational modeling;hidden markov models;intelligent vehicles;predictive models;vehicles;state sequence;robotics and automation;vehicle dynamics;vehicular traffic scenario;uncertain systems automated highways hidden markov models probability road traffic road vehicles sequences;data models;road vehicles	To act intelligently in dynamic environments, a system must understand the current situation it is involved in at any given time. This requires dealing with temporal context, handling multiple and ambiguous interpretations, and accounting for various sources of uncertainty. In this paper we propose a probabilistic approach to modeling and recognizing situations. We define a situation as a distribution over sequences of states that have some meaningful interpretation. Each situation is characterized by an individual hidden Markov model that describes the corresponding distribution. In particular, we consider typical traffic scenarios and describe how our framework can be used to model and track different situations while they are evolving. The approach was evaluated experimentally in vehicular traffic scenarios using real and simulated data. The results show that our system is able to recognize and track multiple situation instances in parallel and make sensible decisions between competing hypotheses. Additionally, we show that our models can be used for predicting the position of the tracked vehicles.	experiment;hidden markov model;interpretation (logic);markov chain	Daniel Meyer-Delius;Christian Plagemann;Wolfram Burgard	2009	2009 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2009.5152838	remotely operated underwater vehicle;data modeling;vehicle dynamics;simulation;uncertainty;laser;computer science;engineering;machine learning;probability;data mining;sequence;predictive modelling;computational model;hidden markov model;statistics	Robotics	38.513945354535934	-41.63180475594061	62313
392785674d00a737e122a79ff8f215609a6ee093	homotopy-based semi-supervised hidden markov tree for texture analysis	unlabeled data;hidden markov tree;parametric statistics;image texture expectation maximisation algorithm hidden markov models;yield estimation;homotopy method;image texture;fixed point;texture analysis;hidden markov models;expectation maximization algorithm homotopy based semi supervised hidden markov tree texture analysis source allocation problem;homotopy based semi supervised hidden markov tree;hidden markov models parameter estimation semisupervised learning parametric statistics yield estimation context modeling equations performance analysis gaussian distribution;performance analysis;expectation maximization algorithm;source allocation problem;parameter estimation;context modeling;gaussian distribution;semisupervised learning;expectation maximisation algorithm	A semi-supervised hidden Markov tree (HMT) model is developed for texture analysis, incorporating both labeled and unlabeled data for training; the optimal balance between labeled and unlabeled data is estimated via the homotopy method. In traditional EM-based semi-supervised modeling, this balance is dictated by the relative size of labeled and unlabeled data, often leading to poor performance. Semi-supervised modeling may be viewed as a source allocation problem between labeled and unlabeled data, controlled by a parameter lambda isin [0,1], where lambda = 0 and 1 correspond to the purely supervised HMT model and purely unsupervised HMT-based clustering, respectively. We consider the homotopy method to track a path of fixed points starting from lambda = 0, with the optimal source allocation identified as a critical transition point where the solution is unsupported by the initial labeled data. Experimental results on real textures demonstrate the superiority of this method compared to the EM-based semi-supervised HMT training	cluster analysis;depth perception;hidden markov model;markov chain;semi-supervised learning;semiconductor industry	Nilanjan Dasgupta;Shihao Ji;Lawrence Carin	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660288	normal distribution;image texture;expectation–maximization algorithm;computer science;machine learning;pattern recognition;mathematics;fixed point;context model;estimation theory;parametric statistics;hidden markov model;statistics	Robotics	32.43512139875239	-39.41446998067267	62358
b58ad98e2457e75c09f6786b7807742402e057fa	frontal view recognition in multiview video sequences	databases;person frontal face images;idiap database demonstration;facial expression recognition;support vector machines;research outputs;training;image classification;video cameras face recognition image classification image sequences matrix decomposition support vector machines;research publications;video sequences cameras face recognition image recognition magnetic heads matrix decomposition informatics support vector machines support vector machine classification telematics;face recognition;matrix decomposition;video cameras;non negative matrix factorization;image sequence;idiap database demonstration frontal view recognition multiview video sequences image sequences cameras person frontal face images facial expression recognition techniques discriminant nonnegative matrix factorization algorithm support vector machines image classification;frontal view recognition;face;support vector machine;discriminant nonnegative matrix factorization algorithm;facial expression recognition techniques;multiview video sequences;cameras;image sequences	In this paper, a novel method is proposed as a solution to the problem of frontal view recognition from multiview image sequences. Our aim is to correctly identify the view that corresponds to the camera placed in front of a person, or the camera whose view is closer to a frontal one. By doing so, frontal face images of the person can be acquired, in order to be used in face or facial expression recognition techniques that require frontal faces to achieve a satisfactory result. The proposed method firstly employs the Discriminant Non-Negative Matrix Factorization (DNMF) algorithm on the input images acquired from every camera. The output of the algorithm is then used as an input to a Support Vector Machines (SVMs) system that classifies the head poses acquired from the cameras to two classes that correspond to the frontal or non frontal pose. Experiments conducted on the IDIAP database demonstrate that the proposed method achieves an accuracy of 98.6% in frontal view recognition.	algorithm;discriminant;non-negative matrix factorization;support vector machine	Irene Kotsia;Nikos Nikolaidis;Ioannis Pitas	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202593	facial recognition system;support vector machine;computer vision;computer science;machine learning;pattern recognition	Vision	36.736489688344356	-47.97417666980609	62465
4118565a011070d821ce07961e1fba31f6c3e9fb	a revisit to human action recognition from depth sequences: guided svm-sampling for joint selection	skeleton cameras hidden markov models training support vector machines computational modeling three dimensional displays;support vector machines;training;ambiguity metrics human action recognition performance depth sequences guided svm sampling joint selection skeleton joint locations joint space sampling computational complexity training stage guided joint sampling strategy svm classifiers classifier selection confidence metrics;skeleton;computational modeling;hidden markov models;three dimensional displays;support vector machines bone computational complexity image classification image sampling learning artificial intelligence;cameras	This paper revisits the problem of human action recognition from skeleton joint locations, and analyses the tradeoff of sampling the joint space with respect to the recognition performance and computational complexity. The provided insights led to the design of a new algorithm for automatically selecting the most appropriate set of joints for each action. During the training stage, the approach applies a guided joint sampling strategy for learning different SVM classifiers, selecting the classifier that maximizes confidence and ambiguity metrics. Experimental results on three action datasets show that pre-selecting the most varying skeleton joints for each action dramatically reduces the computational complexity while keeping competitive recognition rates.	algorithm;computation;computational complexity theory;decision theory;gibbs sampling;sampling (signal processing)	Michel Antunes;Djamila Aouada;Björn E. Ottersten	2016	2016 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2016.7477582	support vector machine;computer vision;computer science;machine learning;pattern recognition;computational model;skeleton;hidden markov model	Vision	36.142206580363215	-44.23359505660533	62683
9a73b9bb17202e140e19619ed323b461dda3e10f	the 2-codeword screening test for lasso problems	optimisation;optimal test 2 codeword screening test lasso screening tests sparse representation signal representation machine learning region test dome test correlation screening test uncertainty measure;signal representation;learning artificial intelligence;machine learning optimization algorithms;signal representation learning artificial intelligence optimisation;uncertainty measurement uncertainty dictionaries optimization signal processing educational institutions correlation	Solving a lasso problem is a practical approach for acquiring a sparse representation of a signal with respect to a given dictionary. Driven by the demand for sparse representations over large-scale data in machine learning and statistics, we explore lasso screening tests. These enhance solution efficiency via the elimination of codewords absent in the optimal solution prior to detailed computation. On basis of the concept of a region test and the recently introduced dome test, we propose the 2-codeword test, which uses two codewords together in a correlation screening test. In addition to the rejection rate as the performance measure, we introduce an innovative way to access the performance of a screening test, called the uncertainty measure, via a comparison with the optimal test.	code word;computation;dictionary;lasso;machine learning;rejection sampling;sparse approximation;sparse matrix	Hao Wu;Peter J. Ramadge	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638270	computer science;machine learning;pattern recognition;mathematics;statistics	Robotics	30.30086276857327	-41.63763424847481	62755
a8531c25e7d4b4e57ce8c6a3d3c6ca0fd4bf9d75	face recognition using unlabeled data		Face recognition systems can normally attain good accuracy when they are provided with a large set of training examples. However, when a large training set is not available, their performance is commonly poor. In this work we describe a method for face recognition that achieves good results when only a very small training set is available (one image per person). The method is based on augmenting the original training set with previously unlabeled data (that is, face images for which the identity of the person is not known). Initially, we apply the well-known eigenfaces technique to reduce the dimensionality of the image space, then we perform an iterative process, classifying all the unlabeled data with an ensemble of classifiers built from the current training set, and appending to the training set the previously unlabeled examples that are believed to be correctly classified with a high confidence level, according to the ensemble.		Carmen Martínez;Olac Fuentes	2003	Computación y Sistemas		art;performance art;cartography	Vision	27.970966358792012	-48.87771152948946	62815
c71fdc02cc7639ef73a7b20bb7112e0303c39311	genesis: a ga-based fuzzy segmentation algorithm for remote sensing images	genetic algorithm ga based image segmentation;sequential object extraction;hyperspectral images;fuzzy output svm;spectral spatial classification	This paper proposes an object-based classification scheme for handling remotely sensed images. The method combines the results of a supervised pixel-based classifier with spatial information extracted from image segmentation. First, pixel-wise classification is implemented by a fuzzy output SVM classifier using spectral and textural features of pixels. This classification results to a set of fuzzy membership maps. Operating on this transformed space, a Genetic Sequential Image Segmentation (GeneSIS) algorithm is next developed to partition the image into homogeneous regions. GeneSIS follows a sequential object extraction approach, whereby at each iteration a single object is extracted by invoking a GA-based object extraction algorithm. This module evaluates the fuzzy content of candidate regions, and through an effective fitness function design provides objects with optimal balance between three fuzzy components: coverage, consistency and smoothness. The final classification map is obtained automatically via segmentation, since each segment is extracted with its own class label. The validity of the proposed method is shown on the land cover classification of three different remote sensing images, with varying number of spectral bands (multispectral/hyperspectral), different spatial resolutions and ground truth cover types. The accuracy results of our approach are favorably compared with the ones obtained by other segmentation-based classification techniques.	algorithm;software release life cycle	Stelios K. Mylonas;Dimitris G. Stavrakoudis;Ioannis B. Theocharis	2013	Knowl.-Based Syst.	10.1016/j.knosys.2013.07.018	computer vision;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Mobile	31.680177730367046	-44.47690874058809	62849
2816b19ac55593efc0aaa18028ca419fc0225f06	computational intelligence methods in forward-looking explosive hazard detection		This chapter discusses several methods for forward-looking (FL) explosive hazard detection (EHD) using FL infrared (FLIR) and FL ground penetrating radar (FLGPR). The challenge in detecting explosive hazards with FL sensors is that there are multiple types of targets buried at different depths in a highly-cluttered environment. A wide array of target and clutter signatures exist, which makes detection algorithm design difficult. Recent work in this application has focused on fusion methods, including fusion of multiple modalities of sensors (e.g., GPR and IR), fusion of multiple frequency sub-band images in FLGPR, and feature-level fusion using multiple kernel and iECO learning. For this chapter, we will demonstrate several types of EHD techniques, including kernel methods such as support vector machines (SVMs), multiple kernel learning MKL, and feature learning methods, including deep learners and iECO learning. We demonstrate the performance of several algorithms using FLGPR and FLIR data collected at a US Army test site. The summary of this work is that deep belief networks and evolutionary approaches to feature learning were shown to be very effective both for FLGPR and FLIR based EHD.	algorithm design;antivirus software;baseline (configuration management);bayesian network;benchmark (computing);clutter;computation;computational intelligence;deep belief network;deep learning;feature learning;kernel method;kriging;local binary patterns;math kernel library;multiple kernel learning;preprocessor;rejection sampling;sensor;support vector machine	Timothy C. Havens;Derek Anderson;Kevin E. Stone;John Becker;Anthony J. Pinar	2016		10.1007/978-3-319-26450-9_2	computer vision;support vector machine;deep belief network;deep learning;multiple kernel learning;kernel method;artificial intelligence;computer science;feature learning;sensor fusion;computational intelligence	ML	32.48481598853921	-42.72898839327638	63026
ba27a41b5bef0ec4b5dbb36cce822c8b58df4484	graphbpt: an efficient hierarchical data structure for image representation and probabilistic inference		This paper presents GraphBPT, a tool for hierarchical representation of images based on binary partition trees. It relies on a new BPT construction algorithm that have interesting tuning properties. Besides, access to image pixels from the tree is achieved efficiently with data compression techniques, and a textual representation of BPT is also provided for interoperability. Finally, we illustrate how the proposed tool takes benefit from probabilistic inference techniques by empowering the BPT with its equivalent factor graph. The relevance of GraphBPT is illustrated in the context of image segmentation.	algorithm;algorithmic efficiency;computation;computer vision;data compression;data structure;directed acyclic graph;factor graph;feature learning;image segmentation;interoperability;machine learning;outline of object recognition;pixel;recursion;relevance	Abdullah Al-Dujaili;François Merciol;Sébastien Lefèvre	2015		10.1007/978-3-319-18720-4_26	machine learning;pattern recognition	Vision	28.614467841795097	-48.76230405795114	63453
70eb53ccabf653cd837e0a25578683ff83b31e26	silhouette analysis for human action recognition based on supervised temporal t-sne and incremental learning	manifolds;manifolds feature extraction learning systems probability distribution euclidean distance training mathematical model;training;stochastic neighbor embedding;manifold learning;euclidean distance;human action recognition method incremental learning methods pattern manifold linear projection manifold oriented stochastic neighbor projection intrinsic action structure linear representations locality preserving projection linear embedding low dimensional embedding temporal information class label information st tsne supervised temporal t sne supervised temporal t stochastic neighbor embedding human silhouette sequences;learning systems;incremental learning;stochastic processes image motion analysis image recognition image representation image sequences learning artificial intelligence;feature extraction;probability distribution;期刊论文;human action recognition;mathematical model;incremental learning human action recognition manifold learning stochastic neighbor embedding	This paper develops a human action recognition method for human silhouette sequences based on supervised temporal t-stochastic neighbor embedding (ST-tSNE) and incremental learning. Inspired by the SNE and its variants, ST-tSNE is proposed to learn the underlying relationship between action frames in a manifold, where the class label information and temporal information are introduced to well represent those frames from the same action class. As to the incremental learning, an important step for action recognition, we introduce three methods to perform the low-dimensional embedding of new data. Two of them are motivated by local methods, locally linear embedding and locality preserving projection. Those two techniques are proposed to learn explicit linear representations following the local neighbor relationship, and their effectiveness is investigated for preserving the intrinsic action structure. The rest one is based on manifold-oriented stochastic neighbor projection to find a linear projection from high-dimensional to low-dimensional space capturing the underlying pattern manifold. Extensive experimental results and comparisons with the state-of-the-art methods demonstrate the effectiveness and robustness of the proposed ST-tSNE and incremental learning methods in the human action silhouette analysis.	arc diagram;body position;epilepsy, temporal lobe;frame (physical object);increment;leigh disease;locality of reference;nonlinear dimensionality reduction;poor posture;silhouette (clustering);t-distributed stochastic neighbor embedding;manifold	Jian Cheng;Haijun Liu;Feng Wang;Hongsheng Li;Ce Zhu	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2441634	probability distribution;computer vision;manifold;feature extraction;computer science;machine learning;pattern recognition;mathematical model;euclidean distance;mathematics;nonlinear dimensionality reduction;statistics	Vision	36.463981736077216	-48.591573000575984	63865
93a8f4a8619671734b6c1d47d68381c69105a959	a visual framework for interaction detection in soccer matches	ball recognition;player detection;trajectory analysis;interaction detection	In the last decade, soccer video analysis has received a lot of attention from the scientific community. This increasing interest is motivated by the possible applications over a wide spectrum of topics: indexing, summarization, video enhancement, team and players statistics, tactics analysis, referee support, etc. The application of computer vision methodologies in the soccer context requires many problems to be faced: ball and players have to be detected in the images in any light and weather condition, they have to be localized in the field, tracked over time and finally their interactions have to be detected and analyzed. The latter task is fundamental, especially for statistic and referee decision support purposes, but, unfortunately, it has not received adequate attention from the scientific community and a lot of research remains to be done. In this paper a multicamera system is presented to detect the ball player interactions during soccer matches. The proposed method extracts, by triangulation from multiple cameras, the 3D ball and player trajectories and, by estimating the trajectory intersections, detects the ball-player interactions. An inference process is then introduced to determine the player kicking the ball and to estimate the interaction frame. The system was tested during several matches of the Italian first division football championship and experimental results demonstrated that the proposed method is robust and accurate.		Marco Leo;Nicola Mosca;Paolo Spagnolo;Pier Luigi Mazzeo;Tiziana D'Orazio;Arcangelo Distante	2010	IJPRAI	10.1142/S0218001410008081	computer vision;simulation;multimedia	Vision	38.977192182905824	-45.905656430172975	63969
d608507410a3dc3029bf9ca0666b076ddd354c63	isnn: impact sound neural network for audio-visual object classification		3D object geometry reconstruction remains a challenge when working with transparent, occluded, or highly reflective surfaces. While recent methods classify shape features using raw audio, we present a multimodal neural network optimized for estimating an object’s geometry and material. Our networks use spectrograms of recorded and synthesized object impact sounds and voxelized shape estimates to extend the capabilities of vision-based reconstruction. We evaluate our method on multiple datasets of both recorded and synthesized sounds. We further present an interactive application for real-time scene reconstruction in which a user can strike objects, producing sound that can instantly classify and segment the struck object, even if the object is transparent or visually occluded. Fig. 1: Our Impact Sound Neural Network Audio (ISNN-A) uses as input a spectrogram of sound created by a real or synthetic object being struck. Our audio-visual network (ISNN-AV) combines ISNN-A with VoxNet to produce state-of-the-art object classification accuracy. 2 A. Sterling, J. Wilson, S. Lowe, and M. C. Lin	3d reconstruction from multiple images;artificial neural network;baseline (configuration management);computer graphics (computer science);local interconnect network;multimodal interaction;overfitting;real-time locating system;sound card;spectrogram;synthetic intelligence	Auston Sterling;Justin Wilson;Sam Lowe;Ming C. Lin	2018		10.1007/978-3-030-01267-0_34	computer vision;machine learning;artificial neural network;artificial intelligence;computer science;spectrogram;raw audio format	Vision	27.012850067201104	-50.85017985889352	64062
bae579402e1f27102e174a25307ebbbd34f639af	multiple path exploration for graph matching	graph matching;gnccp;path following;singular point;multiple smooth curves	The graph matching problem is a hot topic in machine vision. Although a myriad of matching algorithms have been proposed during decades of investigation, it is still a challenging issue because of the combinatorial nature. As one of the outstanding graph matching algorithms, the graduated nonconvexity and concavity procedure follows the path following algorithm. The main drawback of this approach lies that there may exist singular points which violate the smoothness of the solution path and thus harm the accuracy of matching. Addressing this problem, we develop a novel algorithm to bypass this pitfall to improve the matching accuracy. We design an effective method of singular point discovering by checking the smoothness of the path and subsequently explore multiple smooth curves at detected points for better matching results. For evaluation, we make comparisons between our approach and several outstanding matching algorithms on three popular benchmarks, and the results reveal the advantage of our approach.	algorithm;benchmark (computing);concave function;effective method;experiment;machine vision;matching (graph theory);technological singularity	Ran Chen;Congyan Lang;Tao Wang	2017	Machine Vision and Applications	10.1007/s00138-017-0847-1	mathematics;matching (graph theory);machine vision;smoothness;singular point of a curve;mathematical optimization;effective method;machine learning;optimal matching;artificial intelligence;3-dimensional matching	AI	35.50368822219435	-38.39181478799374	64146
63fd865bd28a8f00efe9a76b93d031553e76ed4f	human gait recognition based on deterministic learning through multiple views fusion	gait recognition;deterministic learning;gait variability;multiple views fusion	Gait characteristics extracted from one single camera are limited and not comprehensive enough to develop a robust recognition system. This paper proposes a robust gait recognition method using multiple views fusion and deterministic learning. First, a multiple-views fusion strategy is introduced, in which gaits collected under different views are synthesized as a kind of synthesized silhouette images. Second, the synthesized silhouettes are characterized with four kinds of time-varying gait features, including three width features of the silhouette and one silhouette area feature. Third, gait variability underlying different individuals’ time-varying gait features is effectively modeled by using deterministic learning algorithm. This kind of variability reflects the change of synthesized silhouettes while preserving temporal dynamics information of human walking. Gait patterns are represented as the gait variability underlying time-varying gait features and a rapid recognition scheme is presented in published gait databases. Experimental results show that encouraging recognition accuracy can be achieved. Keyword: Gait recognition; deterministic learning; multiple views fusion; gait variability c © 2016 Elsevier Ltd. All rights reserved.	algorithm;database;gait analysis;heart rate variability;pattern recognition;performance;radial basis function network;spatial variability;speech synthesis	Muqing Deng;Cong Wang;Qingfeng Chen	2016	Pattern Recognition Letters	10.1016/j.patrec.2016.04.004	computer vision;pattern recognition	Vision	35.85148529754139	-51.85422738669137	64480
75259a613285bdb339556ae30897cb7e628209fa	unsupervised domain adaptation for zero-shot learning	semantics visualization encoding adaptation models prototypes feature extraction birds;video signal processing feature extraction unsupervised learning;unsupervised domain adaptation regularised sparse coding zsl method naive knowledge transfer projection function visual feature vector semantic representation space transfer learning zero shot learning	Zero-shot learning (ZSL) can be considered as a special case of transfer learning where the source and target domains have different tasks/label spaces and the target domain is unlabelled, providing little guidance for the knowledge transfer. A ZSL method typically assumes that the two domains share a common semantic representation space, where a visual feature vector extracted from an image/video can be projected/embedded using a projection function. Existing approaches learn the projection function from the source domain and apply it without adaptation to the target domain. They are thus based on naive knowledge transfer and the learned projections are prone to the domain shift problem. In this paper a novel ZSL method is proposed based on unsupervised domain adaptation. Specifically, we formulate a novel regularised sparse coding framework which uses the target domain class labels' projections in the semantic space to regularise the learned target domain projection thus effectively overcoming the projection domain shift problem. Extensive experiments on four object and action recognition benchmark datasets show that the proposed ZSL method significantly outperforms the state-of-the-arts.	apply;benchmark (computing);domain adaptation;embedded system;experiment;feature vector;neural coding;sparse matrix;unsupervised learning	Elyor Kodirov;Tao Xiang;Zhen-Yong Fu;Shaogang Gong	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.282	computer vision;computer science;machine learning;pattern recognition	Vision	25.355153042323554	-46.82784166270226	64525
09bc1f1148b481bf3fd05bc3344179a15f44ef25	nonorthogonal projections for feature extraction in pattern recognition	nonlinear mapping;probability density function;nonparametric;dimensionality reduction feature extraction nonlinear mapping nonparametric pattern recognition;feature space;dimensionality reduction;feature extraction;pattern recognition;diseases;speech recognition;extraterrestrial measurements;biotechnology;dimensional reduction;character recognition;medical diagnosis;multidimensional systems	It is known that R linearly separable classes of multidimensional pattern vectors can always be represented in a feature space of at most R dimensions. An approach is developed which can frequently be used to find a nonorthogonal transformation to project the patterns into a feature space of considerably lower dimensionality. Examples involving classification of handwritten and printed digits are used to illustrate the technique.	feature extraction;feature vector;linear separability;map projection;pattern recognition;printing	Thomas W. Calvert	1970	IEEE Transactions on Computers	10.1109/T-C.1970.222943	nonparametric statistics;computer vision;probability density function;feature vector;multidimensional systems;feature extraction;computer science;machine learning;medical diagnosis;pattern recognition;mathematics;dimensionality reduction	Vision	28.583268455413187	-41.8325387946209	64546
16d1fc5f326cda67fc9a9e5982cf2316614f1457	from stochastic grammar to bayes network: probabilistic parsing of complex activity	grammar detectors probabilistic logic silicon compounds visualization production stochastic processes;event prediction;stochastic grammar;stochastic grammar activity parsing action prediction event prediction temporal segmentation sequential interval network hsmm;human action continual prediction bayes network probabilistic parsing sub activity action composition high level activity classification complex activity temporal structure string length limited stochastic context free grammar sequential interval network sin variable nodes visual detection primitive action posterior probability message passing exact inference test sequence temporal segmentation vision tasks human robot interaction domain;temporal segmentation;stochastic processes bayes methods belief networks computer vision context free grammars image classification message passing probability;hsmm;action prediction;sequential interval network;activity parsing	We propose a probabilistic method for parsing a temporal sequence such as a complex activity defined as composition of sub-activities/actions. The temporal structure of the high-level activity is represented by a string-length limited stochastic context-free grammar. Given the grammar, a Bayes network, which we term Sequential Interval Network (SIN), is generated where the variable nodes correspond to the start and end times of component actions. The network integrates information about the duration of each primitive action, visual detection results for each primitive action, and the activity's temporal structure. At any moment in time during the activity, message passing is used to perform exact inference yielding the posterior probabilities of the start and end times for each different activity/action. We provide demonstrations of this framework being applied to vision tasks such as action prediction, classification of the high-level activities or temporal segmentation of a test sequence, the method is also applicable in Human Robot Interaction domain where continual prediction of human action is needed.	bayesian network;context-free language;experiment;grammar induction;high- and low-level;high-level programming language;human–robot interaction;message passing;parsing;probabilistic turing machine;stochastic context-free grammar;stochastic grammar;time series	Nam N. Vo;Aaron F. Bobick	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.338	natural language processing;computer science;high-speed multimedia radio;machine learning;pattern recognition;stochastic grammar	Vision	37.28135800694899	-41.524867631399225	64635
23319f5b1fbc43a089f81a321d36eed4c252118d	an optimization method of fusing multiple decisions in object detection		Object detection is widely employed in a large number of areas, such as human detection, medical image processing, etc. However, it is insufficient to use only a learning algorithm to detect objects and more techniques or models, such as a probability based approach, a part model, a segmentation model, are combined with the learning algorithm to accomplish the detection task. To this end, a fusion approach is required to balance the decisions making by multiple models. This paper proposes an optimization methodology that fuses a set of confidence outputs estimated by multiple models. Various experiments are executed and demonstrate that the proposed fusion method has a relative better performance than that of the system constituted by a single model.	object detection;program optimization	Zhu Teng;Baopeng Zhang	2014		10.1007/978-3-319-13186-3_4	computer vision;machine learning;pattern recognition	Vision	34.13489440138938	-44.06584247886388	64793
29ed8b8c96f3b7135219fc5901b400a6dd711b82	large-scale video event classification using dynamic temporal pyramid matching of visual semantics	trecvid multimedia event detection large scale video event classification dynamic temporal pyramid matching visual semantics visual content linear temporal pyramid semantic description hadoop map reduction;video signal processing;image matching;image classification;semantics pyramid event video temporal;video signal processing image classification image matching	Video event classification and retrieval has recently emerged as a challenging research topic. In addition to the variation in appearance of visual content and the large scale of the collections to be analyzed, this domain presents new and unique challenges in the modeling of the explicit temporal structure and implicit temporal trends of content within the video events. In this study, we present a technique for video event classification that captures temporal information over semantics using a scalable and efficient modeling scheme. An architecture for partitioning videos into a linear temporal pyramid, using segments of equal length and segments determined by the patterns of the underlying data, is applied over a rich underlying semantic description at the frame level using a taxonomy of nearly 1000 concepts containing 500,000 training images. Forward model selection with data bagging is used to prune the space of temporal features and data for efficiency. The system is implemented in the Hadoop Map-Reduce environment for arbitrary scalability. Our method is applied to the TRECVID Multimedia Event Detection 2012 task. Results demonstrate a significant boost in performance of over 50%, in terms of mean average precision, compared to common max or average pooling, and 17.7% compared to more complex pooling strategies that ignore temporal content.	apache hadoop;information retrieval;mapreduce;model selection;scalability;taxonomy (general)	Noel C. F. Codella;Gang Hua;Liangliang Cao;Michele Merler;Leiguang Gong;Matthew L. Hill;John R. Smith	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738592	computer vision;contextual image classification;computer science;machine learning;pattern recognition;information retrieval	Vision	32.63519364550488	-50.09119762474842	64794
dbb37e95325dac0ea6da519252aa933bb9faa2cd	multi-dimensional sparse models	multilinearity synthesis sparse model analysis sparse model dictionary learning md signal restoration;dictionaries two dimensional displays computational modeling analytical models tensile stress adaptation models data models	Traditional synthesis/analysis sparse representation models signals in a one dimensional (1D) way, in which a multidimensional (MD) signal is converted into a 1D vector. 1D modeling cannot sufficiently handle MD signals of high dimensionality in limited computational resources and memory usage, as breaking the data structure and inherently ignores the diversity of MD signals (tensors). We utilize the multilinearity of tensors to establish the redundant basis of the space of multi linear maps with the sparsity constraint, and further propose MD synthesis/analysis sparse models to effectively and efficiently represent MD signals in their original form. The dimensional features of MD signals are captured by a series of dictionaries simultaneously and collaboratively. The corresponding dictionary learning algorithms and unified MD signal restoration formulations are proposed. The effectiveness of the proposed models and dictionary learning algorithms is demonstrated through experiments on MD signals denoising, image super-resolution and texture classification. Experiments show that the proposed MD models outperform state-of-the-art 1D models in terms of signal representation quality, computational overhead, and memory storage. Moreover, our proposed MD sparse models generalize the 1D sparse models and are flexible and adaptive to both homogeneous and inhomogeneous properties of MD signals.	algorithm;circuit restoration;computational resource;cryptographic hash function;data structure;dictionary [publication type];experiment;machine learning;map;molecular dynamics;noise reduction;numerous;overhead (computing);sparse approximation;sparse matrix;super-resolution imaging	Na Qi;Yunhui Shi;Xiaoyan Sun;Jingdong Wang;Baocai Yin;Junbin Gao	2018	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2017.2663423	overhead (computing);tensor;computer science;artificial intelligence;curse of dimensionality;pattern recognition;data modeling;machine learning;data structure;sparse approximation;homogeneous	Vision	28.746869543830723	-40.36016449983294	64823
dacd1696b3280aba81043cacf5d997822d4f84ca	complementary visual tracking	sbpmc;target tracking visualization robustness training adaptation models vectors;training;incremental pca;principal component analysis object tracking particle filtering numerical methods;visualization;vectors;particle filter;principal component analysis;visual cues;object tracking;cvt complementary visual tracking tracking algorithm region tracker pca object tracker particle filter robust tracking framework inplane rotation;robustness;sbpmc visual tracking object tracking superpixel incremental pca multi state particle filter;target tracking;visual tracking;adaptation models;superpixel;multi state particle filter;particle filtering numerical methods	In this paper, we propose a tracking algorithm which combines two complementary trackers together to supervise each other in dealing with different tracking problems. We design a region tracker based on high-level structure information (incremental PCA), and an object tracker based on mid-level visual cues, and adopt multi-state particle filter to integrate them into a robust tracking framework. While region tracker is more robust to scaling and in-plane rotation, object tracker is more competent in dealing with out-of-plane rotation and deformation. Experiment shows that these two tracker supervise each other against different challenges, and our Complementary Visual Tracking (CVT) framework can resist scaling, deformation, in-plane rotation and out-of-plane rotation simultaneously.	algorithm;bittorrent tracker;high- and low-level;image scaling;level structure;particle filter	Shu Wang;Huchuan Lu;Guang Yang	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116555	computer vision;simulation;visualization;particle filter;sensory cue;tracking system;eye tracking;computer science;machine learning;video tracking;control theory;statistics;robustness;principal component analysis	Vision	35.61470456506724	-47.576938202812514	64922
a7e09a8b258108565ce6945ead13a604de8171d0	enriching a motion database by analogous combination of partial human motions	learning model;motion synthesis;motion segmentation;human motion;human body;character animation;collision avoidance;dimensional reduction;animated character;new combination	We have synthesized new human body motions from existing motion data, by dividing the body of an animated character into several parts, such as upper and lower body, and partitioning the motion of the character into corresponding partial motions. By combining different partial motions, we can generate new motion sequences. We select the most natural-looking combinations by analyzing the similarity of partial motions, using techniques such as motion segmentation, dimensionality reduction, and clustering. These new combinations can dramatically increase the size of a motion database, allowing more score in selecting motions to meet constraints, such as collision avoidance. We verify the naturalness and physical plausibility of the new motions using an SVM learning model and by analysis of static and dynamic balance.	brownian motion;cluster analysis;constraint (mathematics);dimensionality reduction;forsyth–edwards notation;motion capture;motion compensation;plausibility structure;real-time locating system	Won-Seob Jang;Won-Kyu Lee;In-Kwon Lee;Jehee Lee	2007	The Visual Computer	10.1007/s00371-007-0200-1	character animation;computer vision;human body;simulation;computer science;motion estimation;computer graphics (images)	Graphics	36.68916060438641	-50.17641129071532	64957
44b300c95edba8ffe596c5d1d151c89f8a6acf7e	can motion segmentation improve patch-based object recognition?	motion segmentation object recognition;nearest neighbor searches;object recognition;clutter;image motion analysis;image segmentation;video signal processing;information retrieval;training;motion segmentation computer vision visualization training object recognition nearest neighbor searches clutter;computer vision;visualization;motion segmentation;object category retrieval motion segmentation patch based object recognition patch based methods video data;video data;object category retrieval;patch based methods;video signal processing image motion analysis image segmentation information retrieval object recognition;patch based object recognition	Patch-based methods, which constitute the state of the art in object recognition, are often applied to video data, where motion information provides a valuable clue for separating objects of interest from the background. We show that such motion-based segmentation improves the robustness of patch-based recognition with respect to clutter. Our approach, which employs segmentation information to rule out incorrect correspondences between training and test views, is demonstrated empirically to distinctly outperform baselines operating on unsegmented images. Relative improvements reach 50% for the recognition of specific objects, and 33% for object category retrieval.	benchmark (computing);category theory;clutter;experiment;outline of object recognition;sparse matrix;structure from motion	Adrian Ulges;Thomas M. Breuel	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.745	computer vision;visualization;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;clutter;3d single-object recognition;image segmentation	Vision	33.07354271243681	-50.977457917557516	65079
4f44cd8e9f71d01e39b3dcc17710260d5f2dbe33	material classification of hyperspectral images using unsupervised fuzzy clustering methods	spectral analysis fuzzy set theory image classification pattern clustering;casi image material classification hyperspectral images unsupervised fuzzy clustering method image pixel mapping reference spectra pixel clusters fuzzy c means clustering fuzzy relational clustering object identification light radiance;unsupervised fuzzy clustering method;object recognition;pattern clustering;fuzzy relational clustering;material classification;light radiance;hyperspectral images;casi image;fuzzy relation;image classification;hyperspectral sensors;hyperspectral imaging distance measurement hyperspectral sensors materials pixel clustering algorithms object recognition;materials;fuzzy set theory;distance measurement;fuzzy clustering;reference spectra;pixel;clustering algorithms;image pixel mapping;pixel clusters;fuzzy relational clustering material classification hyperspectral imaging unsupervised fuzzy clustering fcm;spectral analysis;hyperspectral imaging;fuzzy c means clustering;fcm;hyperspectral image;unsupervised fuzzy clustering;object identification	This paper presents a novel approach in classifying materials in Hyperspectral images. In particular, unlike other similar approaches in which every pixel in the image is mapped to one of the reference spectra, the proposed methods use the data itself to create clusters of pixels with the same material. This is done by using unsupervised fuzzy clustering methods. Here, two fuzzy clustering approaches have been addressed: Fuzzy C-Means clustering (FCM) and fuzzy relational clustering (FRC). The proposed methods can also solve the problem of identifying the objects for which the radiance of light makes it barely hard to identify them as a single object e.g., a pitched roof. The proposed methods have been applied on the CASI image and the results show that they can successfully classify the materials in the image.	cluster analysis;frame rate control;fuzzy clustering;fuzzy cognitive map;pixel	Soudeh Kasiri Bidhendi;Abbas Sarraf Shirazi;Narges Fotoohi;Mohammad Mehdi Ebadzadeh	2007	2007 Third International IEEE Conference on Signal-Image Technologies and Internet-Based System	10.1109/SITIS.2007.113	correlation clustering;computer vision;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;hyperspectral imaging;machine learning;pattern recognition;mathematics;cluster analysis;conceptual clustering	Vision	31.34364510280456	-44.932267646314244	65295
10b63692ac0077611b7956149615428d4ce46add	robust pca via principal component pursuit: a review for a comparative evaluation in video surveillance	robust principal component analysis;foreground detection;principal component pursuit	Foreground detection is the first step in video surveillance system to detect moving objects. Recent research on#R##N#subspace estimation by sparse representation and rank minimization represents a nice framework to separate moving#R##N#objects from the background. Robust Principal Component Analysis (RPCA) solved via Principal Component Pursuit#R##N#decomposes a data matrix A in two components such that A = L + S , where L is a low-rank matrix and S is a sparse#R##N#noise matrix. The background sequence is then modeled by a low-rank subspace that can gradually change over#R##N#time, while the moving foreground objects constitute the correlated sparse outliers. To date, many efforts have been#R##N#made to develop Principal Component Pursuit (PCP) methods with reduced computational cost that perform visually#R##N#well in foreground detection. However, no current algorithm seems to emerge and to be able to simultaneously#R##N#address all the key challenges that accompany real-world videos. This is due, in part, to the absence of a rigorous#R##N#quantitative evaluation with synthetic and realistic large-scale dataset with accurate ground truth providing a balanced#R##N#coverage of the range of challenges present in the real world. In this context, this work aims to initiate a rigorous#R##N#and comprehensive review of RPCA-PCP based methods for testing and ranking existing algorithms for foreground#R##N#detection. For this, we first review the recent developments in the field of RPCA solved via Principal Component#R##N#Pursuit. Furthermore, we investigate how these methods are solved and if incremental algorithms and real-time#R##N#implementations can be achieved for foreground detection. Finally, experimental results on the Background Models#R##N#Challenge (BMC) dataset which contains different synthetic and real datasets show the comparative performance of#R##N#these recent methods.	closed-circuit television	Thierry Bouwmans;El-hadi Zahzah	2014	Computer Vision and Image Understanding	10.1016/j.cviu.2013.11.009	computer vision;machine learning;pattern recognition;statistics	Vision	28.812088647840195	-38.85020463736985	65383
5e754b72731497a970ec4b73b5a9ce1dcddebc0a	compressed domain features extraction for shot characterization	feature extraction	In this work, we propose a system for shot comparison directly working on the MPEG-1 stream in the compressed domain, extracting both color, texture and motion features considering all frames with a reasonable computational cost, and results comparable to those obtained on uncompressed keyframes. In particular a summary descriptor for each Group Of Pictures (GOP) is computed and employed for shot characterization and comparison. The Mallows distance allows to match different length clips in a unified framework.	algorithmic efficiency;group of pictures;key frame;mpeg-1;unified framework	Costantino Grana;Roberto Vezzani;Daniele Borghesani;Rita Cucchiara	2007			clips;information retrieval;computer vision;feature extraction;artificial intelligence;computer science;uncompressed video;group of pictures	Vision	38.38733327107069	-52.03920092663452	65392
8bcc400da06e3f619c7814f8656886212549da28	linear versus nonlinear pca for the classification of hyperspectral data based on the extended morphological profiles	geophysical image processing;classification algorithm;support vector machines;neural nets;feature extraction nonlinear pca hyperspectral data classification extended morphological profiles remotely sensed data principal component analysis autoassociative neural network;remote sensing feature extraction geophysical image processing geophysical techniques image classification neural nets principal component analysis;nonlinear principal component analysis nlpca classification extended morphological profiles emps neural networks nns;nonlinear principal component analysis;image classification;classification;information content;principal component analysis hyperspectral imaging accuracy feature extraction support vector machines;nonlinear principal component analysis nlpca;accuracy;remote sensing data;feature extraction;principal component analysis;remote sensing;hyperspectral data;ground truth;neural networks nns;support vector machine;hyperspectral imaging;classification accuracy;hyperspectral image;geophysical techniques;spectral resolution;neural network;extended morphological profiles emps	Morphological profiles (MPs) have been proposed in recent literature as aiding tools to achieve better results for classification of remotely sensed data. MPs are in general built using features containing most of the information content of the data, such as the components derived from principal component analysis (PCA). Recently, nonlinear PCA (NLPCA), performed by autoassociative neural network, has emerged as a good unsupervised technique to fit the information content of hyperspectral data into few components. The aim of this letter is to investigate the classification accuracies obtained using extended MPs built from the features of NPCA. A comparison of the two approaches has been validated on two different data sets having different spatial and spectral resolutions/coverages, over the same ground truth, and also using two different classification algorithms. The results show that NLPCA permits one to obtain better classification accuracies than using linear PCA.	algorithm;artificial neural network;autoassociative memory;coverage data;ground truth;nonlinear system;principal component analysis;self-information;unsupervised learning	Giorgio Licciardi;Prashanth Reddy Marpu;Jocelyn Chanussot;Jon Atli Benediktsson	2012	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2011.2172185	support vector machine;computer vision;computer science;machine learning;pattern recognition;artificial neural network	ML	31.645230470111255	-43.803458659338204	65433
9d84856650f1f85753dfcbcb31abc26963c80d82	motion indexing of video	databases;motion analysis;moving object;hand drawn queries;representation;moving objects motion indexing surveillance video segmentation tracking trajectories compressed video representation multiresolution database hand drawn queries imprecise searches video footage;tracking system;image segmentation;image resolution;image databases;video signal processing;motion indexing;surveillance;video compression;database;motion estimation;segmentation;layout;surveillance video;trajectories;trajectory;optical tracking;indexing;image representation;indexing video compression surveillance motion analysis layout trajectory databases system testing;indexation;success rate;multiresolution;system testing;video footage;moving objects;imprecise searches;image resolution indexing video signal processing image segmentation visual databases motion estimation surveillance optical tracking image representation;tracking;compressed video;visual databases	A system has been developed to analyze and index surveillance videos based on the motions of objects in the scene. A segmentation and tracking system extracts trajectories from compressed video, which are represented in a multiresolution manner and stored in a database. Hand-drawn queries can be submitted to the system for imprecise searches. The system was tested on real video footage with numerous moving objects; the success rates for tracking and recall in most cases exceeded 70 percent , a promising result. 1 Introduct ion A valuable tool in the management of visual records is the ability to automatically “describe” and index the content of video sequences in a meaningful manner. Such a facility would allow recovery of desired video segments or objects from a very large database of image sequences. The efficient use of stock film archives and identification of specific activities in surveillance videos are usually cited as potential applications. A parallel goal to creating such a database is the use of compressed video in the indexing and searching functions. More specifically, the elements of the compressed sequence themselves should serve as search keys. The concept of compression is thus extended from only producing an efficient representation, to also providing a meaningful one. This idea is embodied in the term “content based video” [l]. While vast energies have been expended in the development of video databases, relatively little of it has focused on using motion to describe object activity. Most relevant are the works of Dmitrova and Golshani[2] and Ioka and Kurokawa [3], who used macroblock tracing and clustering to derive trajectories, and some form of dynamic programming to determine similarity between trajectories. Ardizzone [4] used frame-wide optical flow to specify more global motion characteristics. Several attempts have also been made to use more statistical descriptions of motion [5] [7]. We present a fusion and extension of several of the ideas outlined above in order to more effectively use the temporal dimension of video for classification and indexing. The development of a representation technique driven by hierarchical and “meaningful” representations, as well as a desire to combine compression and representation, has inspired the use of motion of objects to index a compressed video database. In this paper we apply this concept to a street surveillance application. A segmentation and tracking program analyzes compressed video of a scene and extracts the trajectories of moving objects, represented as two dimensional curves parameterized by time. The coarse-scale components of these trajectories are stored as keys in an index. A user who wishes to find an object moving in a particular way draws a trajectory, which is then matched against those in the index. In what follows, the system will be presented. The testing strategy is then described, and the efficacy of the various system parts evaluated on a database of sever a1 hundred objects.	archive;cluster analysis;data compression;database;dynamic programming;macroblock;multiresolution analysis;optical flow;tracking system	Emile Sahouria;Avideh Zakhor	1997		10.1109/ICIP.1997.638824	computer vision;computer science;trajectory;video tracking;multimedia;computer graphics (images)	Vision	39.12705914117492	-51.093431620017945	65468
8c5a55569583abddc258f9435934d1c5d3df0061	adagio: fast data-aware near-isometric linear embeddings	nonlinear distortion;principal component analysis;parameter estimation;signal processing algorithms;algorithm design and analysis	Many important applications, including signal reconstruction, parameter estimation, and signal processing in a compressed domain, rely on a low-dimensional representation of the dataset that preserves all pairwise distances between the data points and leverages the inherent geometric structure that is typically present. Recently Hedge, Sankaranarayanan, Yin and Baraniuk [19] proposed the first data-aware near-isometric linear embedding which achieves the best of both worlds. However, their method NuMax does not scale to large-scale datasets. Our main contribution is a simple, data-aware, near-isometric linear dimensionality reduction method which significantly outperforms a state-of-the-art method [19] with respect to scalability while achieving high quality near-isometries. Furthermore, our method comes with strong worst-case theoretical guarantees that allow us to guarantee the quality of the obtained nearisometry. We verify experimentally the efficiency of our method on numerous real-world datasets, where we find that our method (9 hours) on medium scale datasets with 60 000 datapoints in 784 dimensions. Finally, we use our method as a preprocessing step to increase the computational efficiency of a classification application and for speeding up approximate nearest neighbor queries.	approximation algorithm;arc diagram;best, worst and average case;data point;dimensionality reduction;display resolution;estimation theory;experiment;isometric projection;preprocessor;scalability;signal processing;signal reconstruction	Jaroslaw Blasiok;Charalampos E. Tsourakakis	2016	2016 IEEE 16th International Conference on Data Mining (ICDM)	10.1109/ICDM.2016.0014	algorithm design;mathematical optimization;nonlinear distortion;computer science;machine learning;mathematics;estimation theory;statistics;principal component analysis	DB	27.946076468212944	-38.664192909672146	65528
b2c9824709a5b050bb0642495632fc2573c5c4d5	toward robust and fast two-dimensional linear discriminant analysis		This paper presents an approach toward robust and fast Two-Dimensional Linear Discriminant Analysis (2DLDA). 2DLDA is an extension of Linear Discriminant Analysis (LDA) for 2-dimensional objects such as images. Linear transformation matrices are iteratively calculated based on the eigenvectors of asymmetric matrices in 2DLDA. However, repeated calculation of eigenvectors of asymmetric matrices may lead to unstable performance. We propose to use simultaneous diagonalization of scatter matrices so that eigenvectors can be stably calculated. Furthermore, for fast calculation, we propose to use approximate decomposition of a scatter matrix based on its several leading eigenvectors. Preliminary experiments are conducted to investigate the effectiveness of our approach. Results are encouraging, and indicate that our approach can achieve comparative performance with the original 2DLDA with reduced computation time.	linear discriminant analysis;robustness (computer science)	Tetsuya Yoshida;Yuu Yamada	2013		10.1007/978-3-319-02750-0_13	mathematical optimization;statistics	ML	29.645846635658266	-41.158775733683996	65612
3fd74bfa63723db536c66f1f2c653d6986127361	feature space mapping for sensor fusion	feature space;sensor fusion	Abstract#R##N##R##N#In the context of a random process scene environment model, a method is presented for fusing data from multiple sensors into a simplified, ordered space for performing electronic vision tasks. The method is based on a new discriminating measure called the tie statistic that is introduced to quantify sensor/feature performance and to provide a mapping from sensor/feature measurement space to a simplified and ordered decision space. The mapping process uses the tie statistic to measure the closeness of an unknown sample probability density function (pdf) to a known pdf for a decision class. Theorems presented in this article relate the tie statistic to minimum probability of error decision making and to the well known Kolmogorov-Smirnov distance. As examples of the sensor/feature fusion method, the tie mapping process is applied to the object location (cueing) and the texture recognition problems.	feature vector;space mapping	G. M. Flachs;J. B. Jordan;C. L. Beer;D. R. Scott;J. J. Carlson	1990	J. Field Robotics	10.1002/rob.4620070306	computer vision;feature vector;computer science;machine learning;pattern recognition;mathematics;sensor fusion	Robotics	36.459379764569235	-39.52770741258315	65632
4bce5998386051f4fb14f6e8d52740bfb5763b53	space-time pose representation for 3d human action recognition	3d human action;temporal modeling;activity recognition	3D human action recognition is an important current challenge at the heart of many research areas lying to the modeling of the spatio-temporal information. In this paper, we propose representing human actions using spatio-temporal motion trajectories. In the proposed approach, each trajectory consists of one motion channel corresponding to the evolution of the 3D position of all joint coordinates within frames of action sequence. Action recognition is achieved through a shape trajectory representation that is learnt by a K-NN classifier, which takes benefit from Riemannian geometry in an open curve shape space. Experiments on the MSR Action 3D and UTKinect human action datasets show that, in comparison to state-of-the-art methods, the proposed approach obtains promising results that show the potential of our approach.	applicative programming language;k-nearest neighbors algorithm	Maxime Devanne;Hazem Wannous;Stefano Berretti;Pietro Pala;Mohamed Daoudi;Alberto Del Bimbo	2013		10.1007/978-3-642-41190-8_49	computer vision;computer science;artificial intelligence;mathematics;activity recognition	Vision	36.457955670549964	-50.071728212282544	65835
5ebc8f3acf69d6c0773c33e598ea6aed48612956	optimization on clustering method of the liquid drop fingerprint	regression analysis computational complexity eigenvalues and eigenfunctions feature extraction fingerprint identification pattern clustering;liquids clustering methods fingerprint recognition linear regression optimization feature extraction accuracy;iterative dynamic clustering method liquid drop fingerprint time complexity multiple linear regression eigenvector dimensions feature extraction waveform analysis method hierarchical clustering method liquid drop fingerprint recognition ratio computational complexity;dynamic clustering method liquid drop fingerprint multiple linear regression characteristic value	In order to effectively reduce the time complexity of clustering algorithm, a new method based on multiple linear regression is put forward to reduce the eigenvector dimensions of the liquid drop fingerprint. After feature extraction with waveform analysis method applied on 38 kinds of liquid samples, optimization is carried out to decrease the 10 characteristic values to 8 values, which is then used in subsequent hierarchical clustering and dynamic clustering. Based on the first dynamic clustering results, comprehensive analysis is applied and dynamic clustering method is used once more. Experimental results show that the recognition ratio of the liquid drop fingerprint can be ensured, together with the reduced computational complexity and excellent clustering accuracy. Compared with hierarchical clustering method, the iterative dynamic clustering method is more effective in liquid identification, with its accuracy up to 100% among selected samples.	algorithm;audio signal processing;cluster analysis;computational complexity theory;feature extraction;fingerprint;hierarchical clustering;iterative method;mathematical optimization;time complexity	Qing Song;Mingyang Qiao;Shihui Zhang	2014	2014 10th International Conference on Natural Computation (ICNC)	10.1109/ICNC.2014.6975923	correlation clustering;machine learning;pattern recognition;data mining;mathematics;cluster analysis	EDA	29.592144950526226	-41.282716297472284	66083
5b67d61b1c64a86b099ed5280dfef85a12e139df	a generic diffusion kernel for semi-supervised learning	graph based semi supervised learning;kernel methods;taylor expansion;semi supervised learning;similarity;kernel method;generating function;diffusion kernel;theoretical foundation;eigenvectors	In this paper, we present a generic diffusion kernel for graph-based semi-supervised learning, whose kernel matrix is generated with a Taylor expansion on the generating function of diffusion similarity matrix. The generic diffusion kernel subsumes common known diffusion kernels, and provides a kernel framework for semi-supervised learning. Specifically, we first present the definition of diffusion similarity matrix, and lay the theoretical foundation for our approach. Then we derive the 2- and  d -diffusion kernels, and naturally extend them to the generic diffusion kernel. Further we prove that small eigenvalues of the generic diffusion kernel correspond to smooth eigenvectors over the graph. This property is critical for the construction of generic diffusion kernels. Experiments on simulated and benchmark databases demonstrate that the generic diffusion kernel is sound and effective.	kernel (operating system);semi-supervised learning;semiconductor industry;supervised learning	Lei Jia;Shizhong Liao	2008		10.1007/978-3-540-87732-5_81	semi-supervised learning;diffusion map;kernel method;mathematical optimization;string kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;graph kernel;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother	ML	25.164185763845655	-39.612750079659826	66331
09c1c3a7693d5790a2be0cab506debf26069bc95	image2mass: estimating the mass of an object from its image		Successful robotic manipulation of real-world objects requires an understanding of the physical properties of these objects. We propose a model for estimating one such physical property, mass, from an object’s image. We collect a large dataset of online product information containing images, sizes, and weights. We compare several baseline models for the image-to-mass problem that were trained on this dataset. We also characterize human performance on the problem. Finally, we present a model that takes into account an estimate of the 3D shape of the object. This model performs significantly better than these baselines and compares favorably to the performance of humans. All models are tested on a held-out set of product data, as well as a relatively small dataset that we captured with a scale and a digital camera.	artificial neural network;baseline (configuration management);digital camera;human reliability;humans;network model;robot;simulation	Trevor Scott Standley;Ozan Sener;Dawn Chen;Silvio Savarese	2017				AI	28.152697890285918	-48.50530903894688	66508
3f9f5a8966c035dc179a60c042b160aee2bf8f53	deep second-order siamese network for pedestrian re-identification		Typical pedestrian re-identification system consists of feature extraction and similarity learning modules. The learning methods involved in the two modules are usually designed separately, which makes them sub-optimal to each other, let alone to the re-identification target. In this paper, we propose a deep second-order siamese network for pedestrian re-identification which is composed of a deep convolutional neural network and a second-order similarity model. The deep convolutional network learns comprehensive features automatically from the data. The similarity model exploits second-order information, thus more suitable for re-identification setting than traditional metric learning methods. The two models are jointly trained over one unified large margin objective and the consistent convergence is guaranteed. Moreover, our deep model can be trained effectively with a small pedestrian re-identification dataset, through an irrelevant pre-training and relevant fine-tuning process. Experimental results on two public datasets illustrate the superior performance of our model over other state-of-theart methods.	artificial neural network;benchmark (computing);convolutional neural network;encode;feature extraction;feature learning;mathematical optimization;relevance;similarity learning;similarity measure	Xuesong Deng;Bingpeng Ma;Hong Chang;Shiguang Shan;Xilin Chen	2016		10.1007/978-3-319-54184-6_20	artificial intelligence;similarity learning;pattern recognition;convolutional neural network;feature extraction;pedestrian;computer science;exploit;convergence (routing)	AI	26.63954738775401	-49.19728712459772	66562
e2fe04d9a3bd3aa3cbd9c281170b4f49119a88fd	detecting kangaroos in the wild: the first step towards automated animal surveillance	complexity theory;animal;population tracking;surveillance biology computing computer vision;kangaroo;dpm;dpm object detection animal kangaroo population tracking;activity analysis kangaroos automated animal surveillance computer vision image dataset national parks queensland detection accuracy multi pose approach deformable part model dpm population tracking;object detection	Recent studies in computer vision have provided new solutions to real-world problems. In this paper, we focus on using computer vision methods to assist in the study of kangaroos in the wild. In order to investigate the feasibility, we built a kangaroo image dataset from collected data from several national parks across the State of Queensland. To achieve reasonable detection accuracy, we explored a multi-pose approach and proposed a framework based on the state-of-the-art Deformable Part Model (DPM). Experiments show that the proposed framework outperformed the state-of-the-art methods on the proposed dataset. Also, the proposed vision tools are able to help our field biologists in studying kangaroo related problems such as population tracking for activity analysis.	computer vision;experiment;sensor	Teng Zhang;Arnold Wiliem;Graham Hemsony;Brian C. Lovell	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178313	computer vision;simulation	Vision	37.741080589581706	-45.18087616145038	66568
ee5d5664dff8e52c463317b00899a2388df9e3bc	3dcontextnet: k-d tree guided hierarchical learning of point clouds using local contextual cues		3D data such as point clouds and meshes are becoming more and more available. The goal of this paper is to obtain 3D object and scene classification and semantic segmentation. Because point clouds have irregular formats, most of the existing methods convert the 3D data into multiple 2D projection images or 3D voxel grids. These representations are suited as input of conventional CNNs but they either ignore the underlying 3D geometrical structure or are constrained by data sparsity and computational complexity. Therefore, recent methods encode the coordinates of each point cloud to certain high dimensional features to cover the 3D space. However, by their design, these models are not able to sufficiently capture the local patterns. In this paper, we propose a method that directly uses point clouds as input and exploits the implicit space partition of k-d tree structure to learn the local contextual information and aggregate features at different scales hierarchically. Extensive experiments on challenging benchmarks show that our proposed model properly captures the local patterns to provide discriminative point set features. For the task of 3D scene semantic segmentation, our method outperforms the state-of-the-art on the challenging Stanford Large-Scale 3D Indoor Spaces Dataset(S3DIS) by a large margin.	aggregate data;computation;computational complexity theory;deep learning;deep web;encode;experiment;gist;point cloud;spaces;sparse matrix;tree structure;voxel	Wei Zeng;Theo Gevers	2017	CoRR		voxel;point cloud;discriminative model;tree structure;computational complexity theory;k-d tree;polygon mesh;artificial intelligence;pattern recognition;computer science	Vision	28.764300749201574	-48.919718687808725	66659
a4638ffd11d25e0f3a69576785231303d5e409d3	extracting hierarchical spatial and temporal features for human action recognition	hierarchical feature extraction;dual-channel model;subspace network;spatial and temporal representation;action recognition	Human action recognition is a challenging computer vision task and many efforts have been made to improve the performance. Most previous work has concentrated on the hand-crafted features or spatial-temporal features learned from multiple contiguous frames. In this paper, we present a dual-channel model to decouple the spatial and temporal feature extraction. More specifically, we propose to capture the complementary static form information from single frame and dynamic motion information from multi-frame differences in two separate channels. In both channels we use two stacked classical subspace networks to learn hierarchical representations, which are subsequently fused for action recognition. Our model is trained and evaluated on three typical benchmarks: KTH, UCF and Hollywood2 datasets. The experimental results illustrate that our approach achieves comparable performances to the state-of-the-art methods. In addition, both feature analysis and control experiments are also carried out to demonstrate the effectiveness of the proposed approach for feature extraction and thereby action recognition.	channel (communications);computer vision;experiment;feature extraction;feature learning;greedy algorithm;high- and low-level;multi-channel memory architecture;network architecture;neural coding;performance;unsupervised learning;video tracking	Keting Zhang;Liqing Zhang	2017	Multimedia Tools and Applications	10.1007/s11042-017-5179-7	artificial intelligence;computer vision;computer science;machine learning;feature extraction;feature (machine learning);pattern recognition;pattern recognition (psychology);subspace topology;communication channel	Vision	28.522075503981956	-51.76579798639696	66787
6bafd74029dec23134e4460b70245ff49ea1edc9	active learning based on locally linear reconstruction	experimental design;nearest neighbor searches;transductive learning;manifolds;convex programming;best approximation;predictive value;reconstruction;active learning;reconstruction active learning experimental design local structure;convex optimization;convex functions;optimization problem;local structure;optimization manifolds algorithm design and analysis pattern analysis nearest neighbor searches convex functions;analysis pattern;convex function;data structures;optimization problem locally linear reconstruction representative point selection optimum experimental design parameter estimate global euclidean structure local manifold structure i optimal design data points active learning algorithm data space local structure local reconstruction coefficient transductive learning algorithm sequential optimization convex optimization;optimal design;optimization;pattern analysis;nearest neighbor search;parameter estimation;learning artificial intelligence;algorithm design;algorithm design and analysis;learning artificial intelligence convex programming data structures	We consider the active learning problem, which aims to select the most representative points. Out of many existing active learning techniques, optimum experimental design (OED) has received considerable attention recently. The typical OED criteria minimize the variance of the parameter estimates or predicted value. However, these methods see only global euclidean structure, while the local manifold structure is ignored. For example, I-optimal design selects those data points such that other data points can be best approximated by linear combinations of all the selected points. In this paper, we propose a novel active learning algorithm which takes into account the local structure of the data space. That is, each data point should be approximated by the linear combination of only its neighbors. Given the local reconstruction coefficients for every data point and the coordinates of the selected points, a transductive learning algorithm called Locally Linear Reconstruction (LLR) is proposed to reconstruct every other point. The most representative points are thus defined as those whose coordinates can be used to best reconstruct the whole data set. The sequential and convex optimization schemes are also introduced to solve the optimization problem. The experimental results have demonstrated the effectiveness of our proposed method.	active learning (machine learning);analysis of algorithms;approximation algorithm;coefficient;computer vision;convex optimization;data point;dataspaces;design of experiments;digit structure;estimated;facial recognition system;geographic coordinate system;lucas–lehmer–riesel test;mathematical optimization;nearest neighbor search;optimal design;optimization problem;population parameter;radiotherapy systems, linear accelerator;sample variance;single linkage cluster analysis;sparse approximation;sparse matrix;statistical classification;subgroup;synthetic intelligence;tracer;transduction (machine learning);manifold	Lijun Zhang;Chun Chen;Jiajun Bu;Deng Cai;Xiaofei He;Thomas S. Huang	2011	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2011.20	convex function;algorithm design;mathematical optimization;convex optimization;computer science;machine learning;pattern recognition;mathematics;active learning	Vision	26.2905509935854	-39.758132425545554	66843
0dd92d7a55bbab7c5d12ae3858a6baeb99b65e67	modeling and propagating cnns in a tree structure for visual tracking		We present an online visual tracking algorithm by managing multiple target appearance models in a tree structure. The proposed algorithm employs Convolutional Neural Networks (CNNs) to represent target appearances, where multiple CNNs collaborate to estimate target states and determine the desirable paths for online model updates in the tree. By maintaining multiple CNNs in diverse branches of tree structure, it is convenient to deal with multi-modality in target appearances and preserve model reliability through smooth updates along tree paths. Since multiple CNNs share all parameters in convolutional layers, it takes advantage of multiple models with little extra cost by saving memory space and avoiding redundant network evaluations. The final target state is estimated by sampling target candidates around the state in the previous frame and identifying the best sample in terms of a weighted average score from a set of active CNNs. Our algorithm illustrates outstanding performance compared to the state-of-the-art techniques in challenging datasets such as online tracking benchmark and visual object tracking challenge.	algorithm;benchmark (computing);computation;convolutional neural network;dspace;modality (human–computer interaction);orfeo toolbox;sampling (signal processing);tree structure;video tracking	Hyeonseob Nam;Mooyeol Baek;Bohyung Han	2016	CoRR		computer vision;theoretical computer science;machine learning;mathematics	Vision	38.91022436895655	-40.83923549313803	66936
19d90fc71377dbb9d7f6ec5c61e9c0cf9fb8784e	laplacian pca and its applications	image sampling;image coding;laplacian pca;laplace equations principal component analysis scattering noise robustness signal processing algorithms face recognition clustering algorithms kernel linear discriminant analysis active shape model;coding length;data processing;manifold learning;image sampling laplacian pca data processing principal component analysis manifold learning manifold unfolding nonlinear dimensionality reduction linear transformation face recognition coding length;laplace equations;face recognition;nonlinear dimensionality reduction;principal component analysis;linear transformation;kernel pca;data reduction;learning artificial intelligence;dimensional reduction;principal component analysis data reduction image coding image sampling laplace equations learning artificial intelligence;manifold unfolding	Dimensionality reduction plays a fundamental role in data processing, for which principal component analysis (PCA) is widely used. In this paper, we develop the Laplacian PCA (LPCA) algorithm which is the extension of PCA to a more general form by locally optimizing the weighted scatter. In addition to the simplicity of PCA, the benefits brought by LPCA are twofold: the strong robustness against noise and the weak metric-dependence on sample spaces. The LPCA algorithm is based on the global alignment of locally Gaussian or linear subspaces via an alignment technique borrowed from manifold learning. Based on the coding length of local samples, the weights can be determined to capture the local principal structure of data. We also give the exemplary application of LPCA to manifold learning. Manifold unfolding (non-linear dimensionality reduction) can be performed by the alignment of tangential maps which are linear transformations of tangent coordinates approximated by LPCA. The superiority of LPCA to PCA and kernel PCA is verified by the experiments on face recognition (FRGC version 2 face database) and manifold (Scherk surface) unfolding.	anomaly detection;approximation algorithm;code;computation;distortion;experiment;facial recognition system;information theory;isomap;kernel method;kernel principal component analysis;laplacian matrix;matlab;manifold regularization;map;nonlinear dimensionality reduction;nonlinear system;tandy 1000;unfolding (dsp implementation);x/open transport interface	Deli Zhao;Zhouchen Lin;Xiaoou Tang	2007	2007 IEEE 11th International Conference on Computer Vision	10.1109/ICCV.2007.4409096	facial recognition system;computer vision;data processing;computer science;machine learning;pattern recognition;mathematics;nonlinear dimensionality reduction	Vision	27.058611460243263	-40.74874909749606	67099
1f8f797f5394f98a9ee4afa55c4412e4a061a264	multiple scale canonical correlation analysis networks for two-view object recognition		With the rapid development of representation learning, deep learning has been proved to be an effective technique to extract high level features. Many variants have been reported including convolutional neural network (CNN), principle component analysis networks (PCANet) and canonical correlation analysis networks (CCANet). The representative CCANet utilizes CCA to learn two-view multi-stage filter banks and achieves significant superiority to PCANet for object recognition. However, CCANet tends to only use the output feature of the last convolutional stage, which ignores the previous different scale features. To surmount this problem, in this paper, we present a novel method dubbed multiple scale canonical correlation analysis networks (MS-CCANet). Specifically, the MS-CCANet learns more discriminative information by stacking multi-scale features of all the convolutional stages together. Extensive experiments are conducted on ETH-80 dataset to verify the effectiveness of MS-CCANet. The results demonstrate that the proposed MS-CCANet outperforms the state-of-art methods including PCANet and CCANet.	outline of object recognition	Xinghao Yang;Weifeng Liu	2017		10.1007/978-3-319-70087-8_35	discriminative model;machine learning;convolutional neural network;pattern recognition;principal component analysis;canonical correlation;deep learning;artificial intelligence;feature learning;computer science;cognitive neuroscience of visual object recognition	Vision	26.102122180204407	-51.00554887800548	67110
269f061e8dba09579457ede4124dc678e880eec8	l1-norm-based principal component analysis with adaptive regularization	trace lasso;dimensionality reduction;principal component analysis;l1 norm;l2 norm	Recently, some L1-norm-based principal component analysis algorithms with sparsity have been proposed for robust dimensionality reduction and processing multivariate data. The L1-norm regularization used in these methods encounters stability problems when there are various correlation structures among data. In order to overcome the drawback, in this paper, we propose a novel L1-norm-based principal component analysis with adaptive regularization (PCA-L1/AR) which can consider sparsity and correlation simultaneously. PCA-L1/AR is adaptive to the correlation structure of the training samples and can benefit both from L2-norm and L1-norm. An iterative procedure for solving PCA-L1/AR is also proposed. The experiment results on some data sets demonstrate the effectiveness of the proposed method. We propose a L1-norm-based principal component analysis with adaptive regularization.We use trace Lasso to regularize the projection vectors.Our mode can simultaneously consider the sparsity and correlation.	matrix regularization;principal component analysis;taxicab geometry	Gui-Fu Lu;Jian Zou;Yong Wang;Zhongqun Wang	2016	Pattern Recognition	10.1016/j.patcog.2016.07.014	norm;mathematical optimization;sparse pca;taxicab geometry;computer science;machine learning;pattern recognition;mathematics;statistics;dimensionality reduction;principal component analysis	Vision	26.159930585228697	-38.15011652926467	67150
9c16a974756ca43b2bf628358ddb08210857f464	efficient calculation of the complete optimal classification set	feature extraction;image classification;matrix inversion;basis pursuit classification;complete optimal classification set;feature selection;parameter locus;rank-1 inverse matrix updates;structure selection	Feature and structure selection is an important part of many classification problems, in previous papers, an approach called basis pursuit classification has been proposed which poses feature selection as a regularization problem using a 1-norm to measure parameter complexity. In addition, a complete optimal parameter set, here called the locus, can be calculated which contains every optimal collection of sparse features as a function of the regularization parameter. This paper considers how to iteratively calculate the parameter locus using a set of rank-1 inverse matrix updates. The algorithm is tested on both artificial and real data and it is shown that the computational cost is reduced from a cubed to a squared problem in the number of features.	algorithm;basis pursuit;computation;computational complexity theory;feature selection;locus;matrix regularization;sparse matrix	Martin Brown;Nicholas Costen;Shigeru Akamatsu	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334183	mathematical optimization;contextual image classification;basis pursuit;feature extraction;computer science;machine learning;linear classifier;pattern recognition;mathematics;feature selection;feature	Vision	24.66788184220988	-40.063861159897364	67305
c5a6118d2aaa3e74a0d9dae5b33899cd66f7869a	modality and component aware feature fusion for rgb-d scene classification		While convolutional neural networks (CNN) have been excellent for object recognition, the greater spatial variability in scene images typically meant that the standard full-image CNN features are suboptimal for scene classification. In this paper, we investigate a framework allowing greater spatial flexibility, in which the Fisher vector (FV) encoded distribution of local CNN features, obtained from a multitude of region proposals per image, is considered instead. The CNN features are computed from an augmented pixel-wise representation comprising multiple modalities of RGB, HHA and surface normals, as extracted from RGB-D data. More significantly, we make two postulates: (1) component sparsity - that only a small variety of region proposals and their corresponding FV GMM components contribute to scene discriminability, and (2) modal non-sparsity - within these discriminative components, all modalities have important contribution. In our framework, these are implemented through regularization terms applying group lasso to GMM components and exclusive group lasso across modalities. By learning and combining regressors for both proposal-based FV features and global CNN features, we were able to achieve state-of-the-art scene classification performance on the SUNRGBD Dataset and NYU Depth Dataset V2.	artificial neural network;convolutional neural network;farmville;feature vector;google map maker;lasso;matrix regularization;modal logic;modality (human–computer interaction);multimodal interaction;normal (geometry);outline of object recognition;pixel;sparse matrix;spatial variability	Anran Wang;Jianfei Cai;Jiwen Lu;Tat-Jen Cham	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.645	computer vision;speech recognition;machine learning;pattern recognition;mathematics	Vision	26.578312735468753	-46.84915236270439	67321
10f5e85c51290f3539d908d9fbbea4b4b5d77584	rain removal via residual generation cascading		Single-image rain removal has always been challenging, due to its inherent ill-posed nature. In this paper, we propose a novel rain removal pipeline. This pipeline features two processing pathways: 1) a bottom-up pathway which iteratively performs down sampling and subtraction operations, yielding a Laplacian pyramid (LP) with decreasing scales, to bypass rain details; and 2) a top-down pathway which is built on image series obtained by the above bottom-up process and generate a series of image components based on conditional generative adversarial nets (CGAN), aiming at removing rain streaks while preserving image details. Generated components are further fused via scale-space aggregation, and descriptive image intrinsic information contained in different scales is interacted to jointly generate high quality de-rained image. Extensive experiments on synthetic and real-world rainy image datasets both demonstrate the effectiveness of the proposed rain streaks removal approach.	algorithm;autostereogram;bottom-up parsing;bottom-up proteomics;display resolution;experiment;gene regulatory network;laplacian matrix;nonlinear system;sampling (signal processing);scale space;synthetic data;synthetic intelligence;top-down and bottom-up design;well-posed problem	Qiaobo Chen;Xu Yi;Bingbing Ni;Zan Shen;Xiaokang Yang	2017	2017 IEEE Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2017.8305092	computer vision;residual;artificial intelligence;computer science;subtraction;laplacian pyramid;decimation;image series	Vision	25.890994960024184	-51.27371184286362	67376
88b4c9832099e9ea72f90d922150edbff2f86132	single- and two-person action recognition based on silhouette shape and optical point descriptors		In this paper, we present action descriptors that are capable of performing single- and two-person simultaneous action recognition. In order to exploit the shape information of action silhouettes, we detect junction points and geometric patterns at the silhouette boundary. The motion information is exploited by using optical flow points. We compute centroid distance signatures to construct the junction points and optical flow-based action descriptors. By taking advantage of the distinct poses, we extract key frames and construct geometric pattern action descriptor, which is based on histograms of the geometric patterns classes obtained by a distance-based classification method. In order to exploit the shape and motion information simultaneously, we follow the information fusion strategy and construct a joint action descriptor by combining geometric patterns and optical flow descriptors. We evaluate the performance of these descriptors on the two widely used action datasets, i.e., Weizmann dataset (single-person actions) and SBU Kinect interaction dataset, clean and noisy versions (two-person actions). The experimental outcomes demonstrate the ability of the individual descriptors to give satisfactory performance on average. It is found that the joint action descriptor shows the best performance among the proposed descriptors due to its high discriminative power and also outperforms state-of-the-art approaches.		Shujah Islam;Tehreem Qasim;Muhammad Yasir;Naeem Bhatti;Hasan Mahmood;Muhammad Yousuf Irfan Zia	2018	Signal, Image and Video Processing	10.1007/s11760-017-1228-y	silhouette;discriminative model;artificial intelligence;computer vision;centroid;pattern recognition;histogram;mathematics;optical flow	Vision	35.42487987322341	-50.55167314948509	67541
b4984e3ab2dc7a897c93ce48263c4d90f8ee7194	an end-to-end compression framework based on convolutional neural networks	data compression	Traditional image coding standards (such as JPEG and JPEG2000) make the decoded image suffer from many blocking artifacts or noises since the use of big quantization steps. To overcome this problem, we proposed an end-to-end compression framework based on two CNNs, as shown in Figure 1, which produce a compact representation for encoding using a third party coding standard and reconstruct the decoded image, respectively. To make two CNNs effectively collaborate, we develop a unified end-to-end learning framework to simultaneously learn CrCNN and ReCNN such that the compact representation obtained by CrCNN preserves the structural information of the image, which facilitates to accurately reconstruct the decoded image using ReCNN and also makes the proposed compression framework compatible with existing image coding standards.	algorithm;artificial neural network;blocking (computing);convolutional neural network;deep learning;display resolution;encoder;end-to-end principle;image compression;intermediate representation;jpeg 2000;mathematical optimization;video post-processing	Feng Jiang;Wen Tao;Shaohui Liu;Jie Ren;Xun Guo;Debin Zhao	2017	2017 Data Compression Conference (DCC)	10.1109/DCC.2017.54	data compression;computer vision;image compression;computer science;theoretical computer science;machine learning;mathematics;statistics	Vision	24.634935652963236	-51.054971727289185	67664
4ae6c0051713282b1c34c551188d300e3dfd5da5	rsgan: face swapping and editing using face and hair representation in latent spaces		"""This abstract introduces a generative neural network for face swapping and editing face images. We refer to this network as """"region-separative generative adversarial network (RSGAN)"""". In existing deep generative models such as Variational autoencoder (VAE) and Generative adversarial network (GAN), training data must represent what the generative models synthesize. For example, image inpainting is achieved by training images with and without holes. However, it is difficult or even impossible to prepare a dataset which includes face images both before and after face swapping because faces of real people cannot be swapped without surgical operations. We tackle this problem by training the network so that it synthesizes synthesize a natural face image from an arbitrary pair of face and hair appearances. In addition to face swapping, the proposed network can be applied to other editing applications, such as visual attribute editing and random face parts synthesis."""	artificial neural network;autoencoder;inpainting;paging;variational principle	Ryota Natsume;Tatsuya Yatagawa;Shigeo Morishima	2018		10.1145/3230744.3230818	machine learning;generative grammar;autoencoder;inpainting;computer vision;artificial intelligence;artificial neural network;computer science;image editing;swap (computer programming);training set	Vision	24.89670950147685	-49.60994919897014	67688
062040cc663aa7169c438ea59736e30deb137f13	multitask low-rank affinity graph for image segmentation and image annotation	image segmentation;image annotation;multitask;low rank	This article investigates a low-rank representation--based graph, which can used in graph-based vision tasks including image segmentation and image annotation. It naturally fuses multiple types of image features in a framework named multitask low-rank affinity pursuit. Given the image patches described with multiple types of features, we aim at inferring a unified affinity matrix that implicitly encodes the relations among these patches. This is achieved by seeking the sparsity-consistent low-rank affinities from the joint decompositions of multiple feature matrices into pairs of sparse and low-rank matrices, the latter of which is expressed as the production of the image feature matrix and its corresponding image affinity matrix. The inference process is formulated as a minimization problem and solved efficiently with the augmented Lagrange multiplier method. Considering image patches as vertices, a graph can be built based on the resulted affinity matrix. Compared to previous methods, which are usually based on a single type of feature, the proposed method seamlessly integrates multiple types of features to jointly produce the affinity matrix in a single inference step. The proposed method is applied to graph-based image segmentation and graph-based image annotation. Experiments on benchmark datasets well validate the superiority of using multiple features over single feature and also the superiority of our method over conventional methods for feature fusion.	affinity analysis;automatic image annotation;benchmark (computing);computer multitasking;contour line;convex optimization;feature (computer vision);image segmentation;lagrange multiplier;mathematical optimization;optimization problem;processor affinity;sparse matrix	Teng Li;Bin Cheng;Bingbing Ni;Guangchan Liu;Shuicheng Yan	2016	ACM TIST	10.1145/2856058	computer vision;feature detection;human multitasking;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation;automatic image annotation;feature	Vision	27.00061719599074	-45.86638861304808	67779
fded99a29bfa03d02709dd23f49b04ce3840af51	every smile is unique: landmark-guided diverse smile generation		Each smile is unique: one person surely smiles in different ways (e.g. closing/opening the eyes or mouth). Given one input image of a neutral face, can we generate multiple smile videos with distinctive characteristics? To tackle this one-to-many video generation problem, we propose a novel deep learning architecture named Conditional Multi-Mode Network (CMM-Net). To better encode the dynamics of facial expressions, CMM-Net explicitly exploits facial landmarks for generating smile sequences. Specifically, a variational auto-encoder is used to learn a facial landmark embedding. This single embedding is then exploited by a conditional recurrent network which generates a landmark embedding sequence conditioned on a specific expression (e.g. spontaneous smile). Next, the generated landmark embeddings are fed into a multi-mode recurrent landmark generator, producing a set of landmark sequences still associated to the given smile class but clearly distinct from each other. Finally, these landmark sequences are translated into face videos. Our experimental results demonstrate the effectiveness of our CMM-Net in generating realistic videos of multiple smile expressions.		Wei Wang;Xavier Alameda-Pineda;Dan Xu;Elisa Ricci;Nicu Sebe	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00740	machine learning;architecture;deep learning;pattern recognition;landmark;computer science;expression (mathematics);facial expression;embedding;artificial intelligence	Vision	25.851189196651635	-50.051923864588375	67934
f539b94e8ffdb3c12f52a0ff9a09fd725a3532df	efficient pedestrian detection in the low resolution via sparse representation with sparse support regression		We propose a novel pedestrian detection approach in the extreme Low-Resolution (LR) images via sparse representation. Pedestrian detection in the extreme LR images is very important for some specific applications such as abnormal event detection and video forensics from surveillance videos. Although the pedestrian detection in High-Resolution (HR) images has achieved remarkable progress, it is still a challenging task in the LR images, because the discriminative information in the HR images usually disappear in the LR ones. It makes the precision of the detectors in the LR images decrease by a large margin. Most of the traditional methods enlarge the LR image by the linear interpolation methods. However, it can not preserve the high frequency information very well, which is very important for the detectors. For solving this problem, we reconstruct the LR image in the high resolution by sparse representation. In our model, the LR and HR dictionaries are established respectively in the training stage, and the representative coefficients mapping relations are determined. Moreover, for improving the speed of feature extraction, the feature reconstruction in the LR images is converted to the sparse linear combination between the coefficients and the response of the atoms in HR dictionary by the LR-HR mapping, no matter how complex the feature extraction is. Experiments on the four challenging datasets: Caltech, INRIA, ETH and TUD-Brussels, demonstrate that our proposed method outperforms the state-of-the-art approaches and is much efficient with more than 10 times speedup.	pedestrian detection;sparse approximation	Wenhua Fang;Jun Chen;Ruimin Hu	2017		10.1007/978-3-319-57529-2_25	discriminative model;artificial intelligence;computer science;machine learning;linear interpolation;linear combination;feature extraction;object detection;speedup;sparse approximation;pedestrian detection;pattern recognition	AI	29.18388917260152	-47.85239539127631	68024
32ef759ecfb768a966d0775c7b8f27ff6c2440da	face recognition using partial least squares components	small sample size;dimension reduction;partial least square;partial least squares;feature vector;face recognition;principal component analysis	The paper considers partial least squares (PLS) as a new dimension reduction technique for the feature vector to overcome the small sample size problem in face recognition. Principal component analysis (PCA), a conventional dimension reduction method, selects the components with maximum variability, irrespective of the class information. So PCA does not necessarily extract features that are important for the discrimination of classes. PLS, on the other hand, constructs the components so that the correlation between the class variable and themselves is maximized. Therefore PLS components are more predictive than PCA components in classi6cation. The experimental results on Manchester and ORL databases show that PLS is to be preferred over PCA when classi6cation is the goal and dimension reduction is needed. ? 2003 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	database;dimensionality reduction;facial recognition system;feature vector;partial least squares regression;pattern recognition;principal component analysis;return loss;spatial variability	Jangsun Baek;Min-Soo Kim	2004	Pattern Recognition	10.1016/j.patcog.2003.10.014	facial recognition system;non-linear iterative partial least squares;feature vector;computer science;machine learning;pattern recognition;mathematics;partial least squares regression;statistics;dimensionality reduction;principal component analysis	AI	26.21202415237664	-42.63920151458126	68226
43dce79cf815b5c7068b1678f6200dabf8f5de31	deep multi-frame face hallucination for face identification.		Face verification problem has been successfully approached in many recent papers [12, 11, 9, 8]. Unfortunately, recognition quality decreases significantly in the presence of different degradation factors inherent to real world data [5], for example surveillance videos, where faces can be very small in size, blurred and affected with compression artifacts. Here we present face hallucination approach based on using several sequential video frames: our neural network restores the central frame of each input sequence additionally taking into account a number of adjacent frames, warped to align with the central one. We show that our face hallucination approach can improve face identification quality compared to the single-frame baseline and to the multi-frame approach without adjacent frame warping.	align (company);artificial neural network;baseline (configuration management);compression artifact;elegant degradation;face hallucination	Evgeniya Ustinova;Victor S. Lempitsky	2017	CoRR		artificial neural network;computer vision;image warping;face hallucination;computer science;compression artifact;artificial intelligence	Vision	29.499560188163358	-50.66200815887649	68243
1b0ea143623d282ef9ec53d62e4dff5f65e639c2	fusing two convolutional neural networks for high-resolution scene classification		This paper presents a novel deep convolutional feature fusion (ConvFF) approach for high-resolution scene classification, characterizing the well-known deep convolutional neural network (ConvNet) approach. The proposed ConvFF approach starts by generating an initial feature representation of the original scenes under exploration from two deep ConvNets pre-trained on two different large amount of labeled data. After the pre-training phase, we fine tune the two deep ConvNets consisting of mainly objects and scenes respectively in a supervised manner using the target training images. Then we propose to fuse the extracted two types of convolutional features provided by the last fully-connected (FC) layer, respectively. Finally, the fused convolutional features are fed as input to a SVM classifier for classification. The proposed method is evaluated by using two challenging high-resolution scene datasets. Experimental results show that the proposed method can effectively extract complementary features of the scenes and capture local spatial patterns, consistently outperforming several state-of-the-art methods.	artificial neural network;convolutional neural network;image resolution;scene graph;supervised learning;whole earth 'lectronic link	Xiaoyong Bian;Chen Chen;Yuxia Sheng;Yan Xu;Qian Du	2017	2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2017.8127688	artificial intelligence;support vector machine;convolutional neural network;computer vision;labeled data;computer science	Vision	27.444396980523422	-52.05546020972957	68523
e182a90347bf84946440584a979aa57f5c1cad4c	a note on patch-based low-rank minimization for fast image denoising		Patch-based sparse representation and low-rank approximation for image processing attract much attention in recent years. The minimization of the matrix rank coupled with the Frobenius norm data fidelity can be solved by the hard thresholding filter with principle component analysis (PCA) or singular value decomposition (SVD). Based on this idea, we propose a patch-based low-rank minimization method for image denoising, which learns compact dictionaries from similarpatches with PCA or SVD, and applies simple hard thresholding filtersto shrink the representation coefficients. Compared to recent patchbased sparse representation methods, experiments demonst rate that the proposed method is not only rather rapid, but also effective for a variety of natural images, especially for texture parts in images.	coefficient;dictionary;experiment;image processing;low-rank approximation;matrix multiplication;noise reduction;principal component analysis;singular value decomposition;sparse approximation;sparse matrix;the matrix;thresholding (image processing)	Haijuan Hu;Jacques Froment;Quansheng Liu	2018	J. Visual Communication and Image Representation	10.1016/j.jvcir.2017.11.013	artificial intelligence;pattern recognition;mathematics;grayscale;image processing;rank (linear algebra);matrix norm;singular value decomposition;thresholding;mathematical optimization;principal component analysis;sparse approximation	ML	26.810011287409814	-40.79053646418256	68548
1f1a86f19cfad674f2db57f05748ae59217d2b6f	spatial context driven manifold learning for hyperspectral image classification	context dependency;manifold learning;presentation;hyperspectral classification	Manifold learning techniques have been demonstrated to be successful in representing spectral signatures in hyperspectral images, which consist of spectral features with very subtle differences and often spatially induced disjoint classes whose neighborhood relations are difficult to capture using traditional graph based embedding techniques. Robust parameter estimation is a challenge in traditional kernel functions that compute neighborhood graphs e.g finding the optimal number of nearest neighbors. We address these challenges by proposing spatial context driven manifold learning methods. Empirically, the study reveals that use of spatial contextual information has a bearing on the structure of the graph Laplacian that in turn links image pixel observations to their manifold spaces. Further experimental results demonstrate an improvement in the classification performance compared to traditional manifold learning methods.	computer vision;estimation theory;laplacian matrix;nonlinear dimensionality reduction;pixel;software testing controversies;type signature	Yongzhi Zhang;Hsiuhan Lexie Yang;Dalton Lunga;S. Prasad;Melba M. Crawford	2014	2014 6th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)	10.1109/WHISPERS.2014.8077527	computer science;data science;machine learning;data mining	AI	28.345461283313476	-43.66018858247974	68560
d9a3efe3a6563d54ab470e319cab0a88c6bbc82c	face recognition using margin-enhanced classifier in graph-based space	face recognition	In this paper, we develop a face recognition system with the derived subspace learning method, i.e. classifier-concerning subspace, where not only the discriminant structure of data can be preserved but also the classification ability can be explicitly considered by introducing the Mahalanobis distance metric in the subspace. Most of graph-based subspace learning methods find a subspace with the preservation of certain geometric and discriminant structure of data but not explicitly include the classification information from the classifier. Via the distance metric, which is constrained by k-NN classification rule, the pairwise distance relation can be locally adjusted and thus the projected data in the classifier-concerning subspace are more suitable for k-NN classifier. In addition, an iterative procedure is derived to get rid of the overfitting problem. Experimental results show that the proposed system can yield the promising recognition results under various lighting, pose and expression conditions.	discriminant;facial recognition system;iterative method;k-nearest neighbors algorithm;overfitting;pose (computer vision);statistical classification	Ju-Chin Chen;Shang-You Shi;Jenn-Jier James Lien	2010			computer vision;artificial intelligence;face detection;pattern recognition;computer science;facial recognition system;three-dimensional face recognition;3d single-object recognition;classifier (linguistics);graph	ML	26.521860521250712	-43.415809387974846	68991
2bcec23ac1486f4106a3aa588b6589e9299aba70	an uncertain future: forecasting from static images using variational autoencoders		In a given scene, humans can easily predict a set of immediate future events that might happen. However, pixel-level anticipation in computer vision is difficult because machine learning struggles with the ambiguity of predicting the future. In this paper, we focus on predicting the dense trajectory of pixels in a scene — what will move in the scene, where it will travel, and how it will deform over the course of one second. We propose a conditional variational autoencoder as a solution to this problem. In this framework, direct inference from the image shapes the distribution of possible trajectories while latent variables encode information that is not available in the image. We show that our method predicts events in a variety of scenes and can produce multiple different predictions for an ambiguous future. We also find that our method learns a representation that is applicable to semantic vision tasks. Our algorithm is trained on thousands of diverse, realistic videos and requires absolutely no human labeling—relying only on labels produced by pixel tracking.	algorithm;autoencoder;computer vision;encode;latent variable;machine learning;pixel;variational principle	Jacob Walker;Carl Doersch;Abhinav Gupta;Martial Hebert	2016		10.1007/978-3-319-46478-7_51	computer vision;computer science;artificial intelligence;machine learning;pattern recognition	Vision	27.26825020295974	-51.01591125149791	69039
e420a7499216d00ed7b4c394aec5f26e315b180c	pedestrian detection using multi-objective optimization		Pedestrian detection on urban video sequences challenges classification systems because of the presence of cluttered backgrounds which drop their performances. This article proposes a Multi-Objective Optimization (MOO) technique reducing this limitation. It trains a pool of cascades of boosted classifiers using different positive datasets. A Pareto Front is obtained from the locally non-dominated operational points of the Receptive Objective Curve (ROC) of those classifiers. Using information about the dynamic of the scene, different pairs of operational points from the Pareto Front are employed to improve the performance of the system. Results on a real sequences outperform traditional detector systems.	multi-objective optimization;pedestrian detection;program optimization	Pablo Negri	2015		10.1007/978-3-319-25751-8_93	pattern recognition;artificial intelligence;computer vision;computer science;detector;multi-objective optimization;pedestrian detection	EDA	33.403821557131906	-45.44207544621772	69201
7d677e3263608ab90d5664e312f5ebb2d74cd246	a multistage approach to cooperatively coevolving feature construction and object detection	estensibilidad;multistage;extraction forme;multietage;classification;coevolution;detection objet;detector proximidad;extraccion forma;feature construction;feature extraction;poliescalonado;pattern recognition;algorithme evolutionniste;algoritmo evolucionista;extensibilite;scalability;reconnaissance forme;extraction caracteristique;evolutionary algorithm;reconocimiento patron;coevolucion;pattern extraction;clasificacion;cooperative coevolution;proximity detector;object detection;detecteur proximite	In previous work, we showed how cooperative coevolution could be used to evolve both the feature construction stage and the classification stage of an object detection algorithm. Evolving both stages simultaneously allows highly accurate solutions to be created while needing only a fraction of the number of features extracting as in generic approaches. Scalability issues in the previous system have motivated the introduction of a multi-stage approach which has been shown in the literature to provide large reductions in computational requirements. In this work we show how using the idea of coevolutionary feature extraction in conjunction with this multi-stage approach can reduce the computational requirements by at least two orders of magnitude, allowing the impressive performance gains of this technique to be readily applied to many real world problems.	algorithm;central processing unit;computation;cooperative coevolution;cooperative multitasking;feature extraction;feature vector;image processing;multistage amplifier;object detection;requirement;run time (program lifecycle phase);scalability;sensor	Mark E. Roberts;Ela Claridge	2005		10.1007/978-3-540-32003-6_40	scalability;simulation;feature extraction;biological classification;coevolution;computer science;artificial intelligence;machine learning;evolutionary algorithm;algorithm	AI	34.45458625801893	-38.20197203796807	69205
e7a4e1fda9cf8e03e8d4ec96b6a1591914b8e1c6	real- time pedestrian detection in crowded scenes using deep omega-shape features		Region-based Fully ConvNet (R-FCN) designed for general object detection is difficult to be directly applied for pedestrian detection, due to being with large human pose and scale changes, and even with partial occlusion in surveillance scenarios. This paper presents a real time pedestrian detection method with partial occlusion handling, which builds on the framework of Region-based Fully ConvNet. We introduce a deep Omega-shape feature learning and multi-paths detection to make our detector being robust to human pose and scale changes. A novel predicted boxes fusion strategy is proposed to reduce the number of false negatives caused by partial occlusion in crowded environment. Our end-to-end approach achieved 95.35% mAP on the Caltech dataset and 97.43% on Bronze dataset at a test-time speed of 86ms second per image.	convolutional neural network;end-to-end principle;feature learning;object detection;omega;pedestrian detection	Yuting Xu;Xue Zhou;Pengfei Liu;Hongbing Xu	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461956	feature extraction;object detection;artificial intelligence;pattern recognition;feature learning;pedestrian detection;computer science	Vision	29.925943054663236	-51.82043271018731	69219
08ca2f8639af604c196a3626b9dcef5ac0cf3f67	a robust local sparse tracker with global consistency constraint	partial and spatial information;local sparse model;visual object tracking;global consistency;template update	In the field of visual object tracking, partial occlusion and the variation of illumination, pose and background are the core problems to be handled. More and more visual tracking methods tend to exploit part or local features to deal with the above problems. However, single local features may lead to overfitting and drifting problem, as will cause the failure of tracking task. In this paper, we propose a novel tracking method by exploiting the partial and spatial information with a global regulation on the stabilization of local features. With the local features and the global constraint, the problems of occlusion and variation can be well solved and a stable performance can be obtained without overfitting. In the first stage, overlapped patches are used to hold the local features and each patch is reconstructed with all the template patches. The reconstruction coefficients are obtained by solving the ?1 regularized least square problem. In the second stage, a global constraint is added to find the final result. The constraint is achieved by restraining the difference in contributions of each patch. Additionally, we employ occlusion information to improve the template update strategy. The experiment results on several widely used benchmark datasets demonstrate that our method is effective and outperforms the state-of-the-art trackers. HighlightsThe first contribution is proposing a novel local sparse model with global constraint, which can reduce the drifting problems.The second contribution is giving a two-stage algorithm to exploit the partial and spatial information without overfitting problems.The third contribution is raising exploiting occlusion information to guide the template update, which can improve the template update strategy.	sparse matrix	Xinhua You;Xin Li;Zhenyu He;Xuewan Zhang	2015	Signal Processing	10.1016/j.sigpro.2014.09.019	machine learning;pattern recognition;data mining;mathematics	Vision	33.536350738123105	-47.17305984420013	69249
87e4178f71990818a3c125a41db91749bba17cc2	structured time series analysis for human action segmentation and recognition	kernel;manifolds;computer vision machine learning;motion segmentation three dimensional displays kernel time series analysis manifolds heuristic algorithms hidden markov models;computer vision;multivariate time series;motion segmentation;hidden markov models;saptio temporal alignment;machine learning;time series analysis;three dimensional displays;heuristic algorithms;action recognition;online temporal segmentation;transfer learning module structured time series analysis human action segmentation human action recognition structure learning continuous monocular motion sequence multivariate time series joint trajectories space kernelized temporal cut change point detection hilbert space realtime segmentation high action segmentation spatio temporal manifold framework spatio temporal alignment algorithm dynamic manifold warping action sequences human motion capture data 3d depth sensor data;transfer learning;time series hilbert spaces image motion analysis image recognition image segmentation image sequences learning artificial intelligence	We address the problem of structure learning of human motion in order to recognize actions from a continuous monocular motion sequence of an arbitrary person from an arbitrary viewpoint. Human motion sequences are represented by multivariate time series in the joint-trajectories space. Under this structured time series framework, we first propose Kernelized Temporal Cut (KTC), an extension of previous works on change-point detection by incorporating Hilbert space embedding of distributions, to handle the nonparametric and high dimensionality issues of human motions. Experimental results demonstrate the effectiveness of our approach, which yields realtime segmentation, and produces high action segmentation accuracy. Second, a spatio-temporal manifold framework is proposed to model the latent structure of time series data. Then an efficient spatio-temporal alignment algorithm Dynamic Manifold Warping (DMW) is proposed for multivariate time series to calculate motion similarity between action sequences (segments). Furthermore, by combining the temporal segmentation algorithm and the alignment algorithm, online human action recognition can be performed by associating a few labeled examples from motion capture data. The results on human motion capture data and 3D depth sensor data demonstrate the effectiveness of the proposed approach in automatically segmenting and recognizing motion sequences, and its ability to handle noisy and partially occluded data, in the transfer learning module.	alignment;cardiomyoplasty;embedding;hilbert space;kernel method;kinesiology;manifold regularization;motion capture;published comment;qr code;range imaging;segmentation action;sixty nine;structured-light 3d scanner;time series;tracer;algorithm;biologic segmentation;videocassette	Dian Gong;Gérard G. Medioni;Xuemei Zhao	2014	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2013.244	computer vision;kernel;manifold;transfer of learning;computer science;machine learning;time series;pattern recognition;mathematics;hidden markov model	ML	36.77914601836976	-48.42022299274849	69304
fd29891cea25c6614061c57b7850d31d616ec855	a semi-supervised incremental learning framework for sports video view classification	video signal processing image classification image sequences learning artificial intelligence sport;video signal processing;semisupervised incremental learning;negative model pool;image sequence analysis;online optimization;optimal model;image classification;video sequences;sports video view classification;sports video;learning systems;positive model pool;incremental learning;online optimization semisupervised incremental learning sports video view classification optimal model video sequences positive model pool negative model pool;video sequences hidden markov models asia indium tin oxide games dynamic programming layout learning systems image edge detection robustness;sport;learning artificial intelligence;optimization model;image sequences	Sports videos have special characteristics such as well-defined video structure, specialized sports syntax, and typically having some canonical view types. In this paper, we propose a semi-supervised incremental learning framework for sports video view classification. Baseball is selected as an example to explain the main ideas. In order to obtain an optimal model based on a small number of pre-labeled training samples, the semi-supervised incremental learning framework explores the local distributed properties of the video sequences and sufficiently utilizes the information of a positive model pool and a negative model pool. After each round of online optimization process for the under-investigating video, a locally-optimized positive model and a set of negative models are added into the positive model pool and the negative model pool according to some heuristic criteria, respectively. Experiments results on real sports video data show that the proposed system is effective and promising	heuristic;mathematical optimization;online optimization;semi-supervised learning;semiconductor industry	Jun Wu;Bo Zhang;Xian-Sheng Hua;Jianwei Zhang	2006	2006 12th International Multi-Media Modelling Conference	10.1109/MMMC.2006.1651302	computer vision;computer science;sport;machine learning;pattern recognition;multimedia	Vision	26.27885901613955	-45.485573457655825	69344
6b33efb9b0cac1b49b4158c3fe7819008a121913	efficient algorithms for subwindow search in object detection and localization	pascal voc 2006 data set;worst case complexity;tree searching;sliding window search;object localization;efficient subwindow search;computational complexity;object detection;maximum subarray search;linear-time kadane algorithm;branch-and-bound method;sliding window;histograms;linear time;feature extraction;computer vision;confidence level	Recently, a simple yet powerful branch-and-bound method called Efficient Subwindow Search (ESS) was developed to speed up sliding window search in object detection. A major drawback of ESS is that its computational complexity varies widely from O(n2) to O(n4) for n × n matrices. Our experimental experience shows that the ESS's performance is highly related to the optimal confidence levels which indicate the probability of the object's presence. In particular, when the object is not in the image, the optimal subwindow scores low and ESS may take a large amount of iterations to converge to the optimal solution and so perform very slow. Addressing this problem, we present two significantly faster methods based on the linear-time Kadane's Algorithm for 1D maximum subarray search. The first algorithm is a novel, computationally superior branch-and-bound method where the worst case complexity is reduced to O(n3). Experiments on the PASCAL VOC 2006 data set demonstrate that this method is significantly and consistently faster (approximately 30 times faster on average) than the original ESS. Our second algorithm is an approximate algorithm based on alternating search, whose computational complexity is typically O(n2). Experiments shows that (on average) it is 30 times faster again than our first algorithm, or 900 times faster than ESS. It is thus well-suited for real time object detection.	approximation algorithm;best, worst and average case;branch and bound;computational complexity theory;converge;iteration;maxima and minima;maximum subarray problem;nonlinear system;object detection;structured prediction;time complexity;worst-case complexity	Senjian An;Patrick Peursum;Wanquan Liu;Svetha Venkatesh	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206822	sliding window protocol;time complexity;computer vision;mathematical optimization;confidence interval;feature extraction;computer science;theoretical computer science;machine learning;worst-case complexity;histogram;computational complexity theory	Vision	31.4759575256502	-48.274595106175376	69608
43476cbf2a109f8381b398e7a1ddd794b29a9a16	a practical transfer learning algorithm for face verification	minimisation;convex programming;bayes methods;image classification;face verification;bayes methods face joints algorithm design and analysis computational modeling testing vectors;face recognition;minimisation bayes methods convex programming expectation maximisation algorithm face recognition image classification learning artificial intelligence;face verification transfer learning;transfer learning;kullback leibler divergence prior face verification facial images labeled training data family album photo organization software principled transfer learning approach source domain data target domain data generative bayesian model kl divergence based regularizer prior robust likelihood function em algorithm convex analysis equivalent structured rank minimization problem feature transform invariance;learning artificial intelligence;expectation maximisation algorithm	Face verification involves determining whether a pair of facial images belongs to the same or different subjects. This problem can prove to be quite challenging in many important applications where labeled training data is scarce, e.g., family album photo organization software. Herein we propose a principled transfer learning approach for merging plentiful source-domain data with limited samples from some target domain of interest to create a classifier that ideally performs nearly as well as if rich target-domain data were present. Based upon a surprisingly simple generative Bayesian model, our approach combines a KL-divergence based regularizer/prior with a robust likelihood function leading to a scalable implementation via the EM algorithm. As justification for our design choices, we later use principles from convex analysis to recast our algorithm as an equivalent structured rank minimization problem leading to a number of interesting insights related to solution structure and feature-transform invariance. These insights help to both explain the effectiveness of our algorithm as well as elucidate a wide variety of related Bayesian approaches. Experimental testing with challenging datasets validate the utility of the proposed algorithm.	bayesian network;convex analysis;expectation–maximization algorithm;kullback–leibler divergence;scalability	Xudong Cao;David P. Wipf;Fang Wen;Genquan Duan;Jian Sun	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.398	facial recognition system;computer vision;minimisation;contextual image classification;convex optimization;transfer of learning;computer science;machine learning;pattern recognition;data mining;statistics	Vision	25.28375411407502	-42.23787164155901	69635
d38b4eb545a2d956fae3950b2d087192798acaad	frontal-standing pose based person identification using kinect		In this paper we propose a person identification methodology from frontal standing posture using only skeleton information obtained from Kinect. In the first stage, features related to the physical characteristic of a person are calculated for every frame and then noisy frames are removed based on these features using unsupervised learning based approach. We have also proposed 6 new angle and area related features along with the physical build of a person for the supervised learning based identification. Experimental results indicate that the proposed algorithm is able to achieve 96% recognition accuracy and outperforms all the stat-of-the-art methods suggested by Sinha et al. and Preis et al.	kinect	Kingshuk Chakravarty;Tanushyam Chattopadhyay	2014		10.1007/978-3-319-07230-2_21	human–computer interaction;supervised learning;skeleton (computer programming);support vector machine;computer science;unsupervised learning;structural risk minimization;artificial intelligence;pattern recognition	Robotics	36.46758778325985	-48.81114272972234	69728
abeda55a7be0bbe25a25139fb9a3d823215d7536	understanding human-centric images: from geometry to fashion	informatica;human centric imaging;conditional random fields;doctoral thesis;info eu repo semantics doctoralthesis;feature descriptors;generative models;convolutional neural networks;semantic segmentation;fashion;human pose estimation;info eu repo semantics publishedversion	Understanding humans from photographs has always been a fundamental goal of computer vision. Early works focused on simple tasks such as detecting the location of individuals by means of bounding boxes. As the field progressed, harder and more higher level tasks have been undertaken. For example, from human detection came the 2D and 3D human pose estimation in which the task consisted of identifying the location in the image or space of all different body parts, e.g., head, torso, knees, arms, etc. Human attributes also became a great source of interest as they allow recognizing individuals and other properties such as gender or age. Later, the attention turned to the recognition of the action being performed. This, in general, relies on the previous works on pose estimation and attribute classification. Currently, even higher level tasks are being conducted such as predicting the motivations of human behaviour or identifying the fashionability of an individual from a photograph. In this thesis we have developed a hierarchy of tools that cover all these range of problems, from low level feature point descriptors to high level fashion-aware conditional random fields models, all with the objective of understanding humans from monocular RGB images. In order to build these high level models it is paramount to have a battery of robust and reliable low and mid level cues. Along these lines, we have proposed two low-level keypoint descriptors: one based on the theory of the heat diffusion on images, and the other that uses a convolutional neural network to learn discriminative image patch representations. We also introduce distinct low-level generative models for representing human pose: in particular we present a discrete model based on a directed acyclic graph and a continuous model that consists of poses clustered on a Riemannian manifold. As mid level cues we propose two 3D human pose estimation algorithms: one that estimates the 3D pose given a noisy 2D estimation, and an approach that simultaneously estimates both the 2D and 3D pose. Finally, we formulate higher level models built upon low and mid level cues for understanding humans from single images. Concretely, we focus on two different tasks in the context of fashion: semantic segmentation of clothing, and predicting the fashionability from images with metadata to ultimately provide fashion advice to the user. In summary, to robustly extract knowledge from images with the presence of humans it is necessary to build high level models that integrate low and mid level cues. In general, using and understanding strong features is critical for obtaining reliable performance. The main contribution of this thesis is in proposing a variety of low, mid and high level algorithms for human-centric images that can be integrated into higher level models for comprehending humans from photographs, as well as tackling novel fashionoriented problems.	3d pose estimation;algorithm;artificial neural network;autostereogram;coat of arms;computer vision;conditional random field;convolutional neural network;directed acyclic graph;generative model;high- and low-level;high-level programming language;human-based computation;minimum bounding box;sensor	Edgar Simo-Serra	2015	CoRR		computer vision;simulation;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;convolutional neural network;conditional random field;statistics	Vision	33.918171547640846	-49.9684898897277	69943
67f96b790373349d444dc4256c6925cedbbcf4cf	quantifying contextual information for object detection	minimisation;object detection context modeling layout solid modeling histograms labeling computer science robustness terminology roads;i lids dataset contextual information quantification object detection ambiguity minimisation context modelling framework polar geometric histogram descriptor context representation context risk function maximum margin context discriminant context inference method context confidence function pascal voc2005 dataset;contextual information;context model;object detection minimisation;modelling framework;object detection	Context is critical for minimising ambiguity in object detection. In this work, a novel context modelling framework is proposed without the need of any prior scene segmentation or context annotation. This is achieved by exploring a new polar geometric histogram descriptor for context representation. In order to quantify context, we formulate a new context risk function and a maximum margin context (MMC) model to solve the minimization problem of the risk function. Crucially, the usefulness and goodness of contextual information is evaluated directly and explicitly through a discriminant context inference method and a context confidence function, so that only reliable contextual information that is relevant to object detection is utilised. Experiments on PASCAL VOC2005 and i-LIDS datasets demonstrate that the proposed context modelling approach improves object detection significantly and outperforms a state-of-the-art alternative context model.	data mining;discriminant;expectation–maximization algorithm;image segmentation;linux intrusion detection system;loss function;memory management controller;object detection;pascal;quadratic programming;sensor;while	Wei-Shi Zheng;Shaogang Gong;Tao Xiang	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459344	minimisation;computer science;machine learning;pattern recognition;data mining;mathematics;context model;statistics	Vision	31.07416415795525	-47.18203879511645	70143
925d7f0dea38cea19f00dcbe0e0119ecfc8e6595	evolutionary feature learning for 3-d object recognition		3-D object recognition is a challenging task for many applications including autonomous robot navigation and scene understanding. Accurate recognition relies on the selection/learning of discriminative features that are in turn used to uniquely characterize the objects. This paper proposes a novel evolutionary feature learning (EFL) technique for 3-D object recognition. The proposed novel automatic feature learning approach can operate directly on 3-D raw data, alleviating the need for data pre-processing, human expertise and/or defining a large set of parameters. EFL offers smart search strategy to learn the best features in a huge feature space to achieve superior recognition performance. The proposed technique has been extensively evaluated for the task of 3-D object recognition on four popular data sets including Washington RGB-D (low resolution 3-D Video), CIN 2D3D, Willow 2D3D and ETH-80 object data set. Reported experimental results and evaluation against existing state-of-the-art methods (e.g., unsupervised dictionary learning and deep networks) show that the proposed EFL consistently achieves superior performance on all these data sets.	autonomous robot;data pre-processing;dictionary;enlightenment foundation libraries;feature learning;feature vector;image resolution;machine learning;outline of object recognition;preprocessor;robotic mapping;willow	Syed Afaq Ali Shah;Mohammed Bennamoun;Farid Boussaid;Lyndon While	2018	IEEE Access	10.1109/ACCESS.2017.2783331	discriminative model;distributed computing;raw data;machine learning;computer science;feature vector;feature extraction;autonomous robot;evolutionary algorithm;data set;feature learning;artificial intelligence	AI	27.036052164230785	-48.746372893285255	70190
97858e5774dbf07c49325f98f9ac0e0c00b02fff	multivariate scale mixtures for joint sparse regularization in multi-task learning		In this paper we address the problem of learning shared sparse representation across several tasks. Assuming that the tasks share a common set of relevant features across all tasks is highly restrictive. This acts as a motivation to look for a generalized model which will be able to learn any correlation structure present between the tasks. We propose a generalized scale mixture distribution, the Multivariate Power Exponential Scale Mixture (M-PESM), as a joint sparsity promoting prior and derive a unified framework which consists of many of the popular Multitask Learning algorithms. Our proposed unified model also has the ability to learn any present correlation structure between tasks which leads to a more robust framework.	algorithm;computer multitasking;multi-task learning;sparse approximation;sparse matrix;unified framework;unified model	Ritwik Giri;Bhaskar D. Rao	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952662	artificial intelligence;pattern recognition;sparse matrix;unified model;sparse approximation;computer science;regularization (mathematics);mixture distribution;multi-task learning;machine learning;multivariate statistics	Vision	24.86929423229224	-43.93928072704656	70269
9fc3f5dbf41daa2f1ea7d106d6331b502ae0e222	rgbd object recognition and visual texture classification for indoor semantic mapping	rgbd object recognition;object recognition;3d information;carotte;image databases;semantic map;neural nets;sensors;material recognition approach;mobile robot;texture classification;image database;semantics;image classification;mobile robots;color information;three dimensional;image texture;visual texture classification;robots three dimensional displays object recognition image color analysis cameras sensors semantics;semantic mapping;robot vision;object detection approach;machine learning;three dimensional displays;image color analysis;image colour analysis;indoor environment;robots;french exploration mapping contest;color depth camera;texture information;learning artificial intelligence;image databases rgbd object recognition visual texture classification indoor semantic mapping environment mobile robot semantic map french exploration mapping contest carotte object detection approach color depth camera 3d information color information texture information neural network material recognition approach machine learning;indoor semantic mapping environment;cameras;object detection;neural network;visual databases cameras image classification image colour analysis image texture learning artificial intelligence mobile robots neural nets object detection object recognition robot vision;visual databases	We present a mobile robot whose goal is to autonomously explore an unknown indoor environment and to build a semantic map containing high-level information similar to those extracted by humans. This information includes the rooms, their connectivity, the objects they contain and the material of the walls and ground. This robot was developed in order to participate in a French exploration and mapping contest called CAROTTE whose goal is to produce easily interpretable maps of an unknown environment. In particular we present our object detection approach based on a color+depth camera that fuse 3D, color and texture information through a neural network for robust object recognition. We also present the material recognition approach based on machine learning applied to vision. We demonstrate the performances of these modules on image databases and provide examples on the full system working in real environments.	artificial neural network;color depth;database;high- and low-level;machine learning;map;mobile robot;object detection;outline of object recognition;performance;semantic mapping (statistics)	David Filliat;Emmanuel Battesti;Stéphane Bazeille;Guillaume Duceux;Alexander Gepperth;Lotfi Harrath;Islem Jebari;Rafael Pereira;Adriana Tapus;Cedric Meyer;Sio-Hoi Ieng;Ryad B. Benosman;Eddy Cizeron;Jean-Charles Mamanna;Benoit Pothier	2012	2012 IEEE International Conference on Technologies for Practical Robot Applications (TePRA)	10.1109/TePRA.2012.6215666	computer vision;computer science;machine learning;communication	Robotics	35.77574749153923	-45.74051082612856	70395
a61c2947f461310218708fe46e7115683db0dabe	detection, tracking and pose estimation of people in challenging real-world scenes		In this thesis, we consider three challenging and longstanding problems in computer vision: people detection, people tracking and articulated pose estimation. Generic solutions to these problems are essential building blocks for understanding images containing people, an exciting and challenging task with numerous applications in automotive safety, robotic navigation, human-computer interaction, and automatic image indexing and retrieval. Indeed, human actions, intentions and emotions can often be inferred from accurate estimates of human body poses and their movement over time. However, untill recently, accurate estimation of body poses has been possible only in controlled laboratory conditions, typically requiring multiple cameras and specialized motion capture equipment. In order to address this shortcoming, we propose algorithms capable of automatically finding people in uncontrolled outdoor environments, tracking them over time and estimating their body configurations. In the process, we also tackle several important technical challenges, including the large appearance variability of humans, the full and partial occlusions that frequently occur in typical street scenes, and ambiguities in 2D to 3D lifting and data association.  #R##N##R##N#Humans appear in images wearing a large variety of clothing, in a large number of possible body poses and visible from various viewpoints. Jointly, these factors create very complex appearance patterns that are hard to model and detect well. In order to deal with the large appearance variability, we propose an approach based on the pictorial structures paradigm in which we represent the human body as a flexible configuration of rigid body parts and model the appearance of each body part using local image descriptors and discriminative classifiers. We demonstrate the generality of our approach by successfully applying it to various human detection and pose estimation problems.#R##N##R##N#One of the goals of this work is to demonstrate the advantages of a tight coupling of people detection, pose estimation and tracking. Tracking of people in uncontrolled conditions is difficult not only due to appearance variability, but also to frequent full and partial occlusions, which often happen when multiple people are present in the scene. Presence of multiple people also severely complicates data association between frames of the sequence. In order to address this challenge, we propose a tracking-by-detection framework that combines evidence from single-frame detections over several subsequent frames using a dynamical model of body articulations. We demonstrate the effectiveness of our tracking-by-detection approach by applying it to the problem of monocular 3D pose estimation of people in uncontrolled street environments.	3d pose estimation	Mykhaylo Andriluka	2011			computer vision;simulation;geography;articulated body pose estimation;communication	Vision	33.69358140427984	-50.171800824540455	70608
1a29066c85f23d9e74be573726e31c3da4f6b07d	sparse representation based undersampled face recognition with shared prototypeauxiliary dictionaries		The sparse representation with auxiliary dictionary based face recognition methods have achieved significant performance in recent years. The prevailing auxiliary dictionary based methods use training dictionary and auxiliary dictionary to separate facial samples’ prototype dictionary and the intra-class variation component respectively. While in undersampled cases, training dictionary usually contains large intra-class variations, the prototype component cannot be fully separated. For this limitation, a  sparse representation based classification with shared prototype–auxiliary dictionaries  (SRSPA) method is proposed. In SRSPA, a shared prototype dictionary is exploited to specifically separating the prototype component. In addition, a novel dictionary learning method is proposed, which fully considers the separation ability of prototype dictionary and auxiliary dictionary. Experiments on various data sets verify efficacy of the proposed SRSPA especially in undersampled cases.		Xiao Ma;Wenjing Zhuang;Yuelong Li;Jufu Feng	2017	Neurocomputing	10.1016/j.neucom.2017.01.082	speech recognition;k-svd;computer science;machine learning;pattern recognition	Vision	28.466554268249933	-46.5202936253793	70636
501c38b45982d6255702656b03790ed988f8ab28	extracting gestural motion trajectories	feature extraction;computer vision;graph matching;skin;affine transformations;motion estimation;head;image segmentation	This paper is concerned with the extraction of spatiotemporal patterns in video sequences with focus on trajectories of gestural motions associated with American Sign Language. An algorithm is described to extract the motion trajectories of salient features such as human palms from an image sequence. First, motion segmentation of the image sequence is generated based on a multiscale segmentation of the frames and attributed graph matching of regions across frames. This produces region correspondences and their affine transformations. Second, colors of the moving regions are used to determine skin regions. Third, the head and palm regions are identified based on the shape and size of skin regions in motion. Finally, affine transformations defining a region’s motion between successive frames are concatenated to construct the region’s motion trajectory. Experimental results showing the extracted motion trajectories are presented.	algorithm;attributed graph grammar;color;concatenation;geometric analysis;image segmentation;kinesiology;matching (graph theory);skin (computing);spatiotemporal pattern;transformation matrix	Ming-Hsuan Yang;Narendra Ahuja	1998			computer vision;speech recognition;feature extraction;computer science;motion estimation;affine transformation;skin;image segmentation;motion field;head;matching	Vision	38.66044181594689	-49.375791727291194	70832
c2894f2389f89a6da74f37d2e00a6dfa1123594a	face pose estimation based on kernelized maximum separability		Face pose estimation from 2D images is an important topic in the field of computer vision. However, the distribution of face images, under pose variations, is highly nonlinear and complex. We deal with this problem based on the following understanding: (1) the essence of face pose estimation is to assign a face image to a pose template, and the variance of pose templates should be as large as possible; (2) kernel trick is a good auxiliary tool for describing nonlinear distribution. In this paper, we propose a face pose estimation method based on maximum separability of pose templates and incorporate the kernel technique to help model the nonlinear distribution of face patterns under pose variations. The proposed method involves a kernel subspace projection phase and the nearest neighbor classification. Experimental results on the CMU PIE face database and the UMIST database show that the proposed method can achieve high pose estimation performance.	3d pose estimation;kernel method;linear separability	Xiao-Zhang Liu;Yu-Wei Li	2018	Soft Comput.	10.1007/s00500-018-3498-x	machine learning;kernel (linear algebra);kernel method;artificial intelligence;subspace topology;pose;computer science;nonlinear system;k-nearest neighbors algorithm	Robotics	26.775068004269862	-43.33113814082474	70901
80dc5eb64bdf61500287672e6d7a9039511c60dd	multimodal deep learning for robust recognizing maritime imagery in the visible and infrared spectrums		The robust recognition of objects is an essential element of many maritime video surveillance systems. This paper builds on recent advances in convolutional neural networks (CNN) and proposes a new visible-infrared spectrum architecture for ship recognition. Our architecture is composed of two separate CNN processing streams, which will be consecutively combined with a merge network. This merge allows the classification to be performed and provide a rich semantic information such as appearance. It also allows to remedy some problems related to the quality of the visible images due to the weather conditions (rain, fog, etc.) and very complex maritime environment (foam, etc.). Using this architecture, we are able to achieve an average recognition accuracy of 87%.	deep learning;multimodal interaction	Kheireddine Aziz;Frédéric Bouchara	2018		10.1007/978-3-319-93000-8_27	architecture;artificial intelligence;streams;convolutional neural network;computer vision;infrared;computer science;merge (version control);deep learning;sensor fusion	Vision	28.331606671485254	-51.44562541473489	71042
38d37b613396dbec80c77c9d30e2f400c849c51b	feature selection based on maximizing separability in gauss mixture model and its application to image classification	full search;gaussian processes;dimension reduction;gaussian processes image classification clustering algorithms feature extraction principal component analysis entropy karhunen loeve transforms vectors information systems laboratories;image classification;trees mathematics image classification gaussian processes feature extraction computational complexity;trees mathematics;mixture model;computational complexity;feature extraction;tree structure;subspace clustering;feature selection;tree structured classifier gauss mixture model image classification feature selection algorithm backward elimination method full search classifier subspace clustering dimension reduction algorithm;exhaustive search	We propose a feature selection algorithm suitable for classification problems. Our algorithm tries to find a subset of features, which maximizes separability between Gaussian clusters. To reduce the complexity of exhaustive searching the best feature set, we follow a backward elimination method. Our feature selection algorithm can be applied to a full search classifier to obtain a single global subspace. However, one global subspace may not alone capture local behavior well. We realize multiple subspace clustering by applying our dimension reduction algorithm to a tree structured classifier. Experimental results show that the resulting classifier not only removes irrelevant features but also improves classification performance.	brute-force search;cluster analysis;clustering high-dimensional data;computer vision;dimensionality reduction;feature selection;linear separability;mixture model;relevance;selection algorithm;stepwise regression	Sangho Yoon;Robert M. Gray	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1530276	contextual image classification;feature extraction;computer science;machine learning;linear classifier;pattern recognition;brute-force search;mixture model;data mining;gaussian process;mathematics;tree structure;computational complexity theory;feature selection;feature;dimensionality reduction	ML	28.07457898950012	-42.818512906536746	71104
7f61a644aeb375972b1a4c7bde3e9c5268ffa549	adaptive local spatiotemporal features from rgb-d data for one-shot learning gesture recognition	one shot learning;spatiotemporal feature;adaptive;motion region of interest;optical flow;gesture recognition	Noise and constant empirical motion constraints affect the extraction of distinctive spatiotemporal features from one or a few samples per gesture class. To tackle these problems, an adaptive local spatiotemporal feature (ALSTF) using fused RGB-D data is proposed. First, motion regions of interest (MRoIs) are adaptively extracted using grayscale and depth velocity variance information to greatly reduce the impact of noise. Then, corners are used as keypoints if their depth, and velocities of grayscale and of depth meet several adaptive local constraints in each MRoI. With further filtering of noise, an accurate and sufficient number of keypoints is obtained within the desired moving body parts (MBPs). Finally, four kinds of multiple descriptors are calculated and combined in extended gradient and motion spaces to represent the appearance and motion features of gestures. The experimental results on the ChaLearn gesture, CAD-60 and MSRDailyActivity3D datasets demonstrate that the proposed feature achieves higher performance compared with published state-of-the-art approaches under the one-shot learning setting and comparable accuracy under the leave-one-out cross validation.	body dysmorphic disorders;body part;chronic granulomatous disease;cross reactions;cross-validation (statistics);data rate units;extraction;flow;gesture recognition;gradient;grayscale color map;harris affine region detector;human–robot interaction;local area networks;motion estimation;one-shot learning;real-time locating system;region of interest;sample variance;scientific publication;silo (dataset);sparse matrix;velocity (software development);zbtb20 gene;sensor (device);triangulation	Jia Lin;Xiaogang Ruan;Naigong Yu;Yee-Hong Yang	2016		10.3390/s16122171	computer vision;speech recognition;computer science;adaptive behavior;pattern recognition;gesture recognition;optical flow	Vision	35.058441709717584	-49.48217935898166	71231
13df978c16a1d85042afd6fe6dc69ecf23004c03	learning regularized, query-dependent bilinear similarities for large scale image retrieval	quadratic programming;web image dataset regularized bilinear similarity learning query dependent bilinear similarity measure large scale image retrieval query adaptation indexing retrieval method angular regularization constraint quadratic programming problem qp problem smo type algorithm public datasets;conference_paper;angular regularization;training image retrieval indexing support vector machines euclidean distance;indexing;angular regularization bilinear similarities image retrieval;bilinear similarities;quadratic programming image retrieval indexing learning artificial intelligence;learning artificial intelligence;image retrieval	An effective way to improve the quality of image retrieval is by employing a query-dependent similarity measure. However, implementing this in a large scale system is non-trivial because we want neither hurting the efficiency nor relying on too many training samples. In this paper, we introduce a query-dependent bilinear similarity measure to address the first issue. Based on our bilinear similarity model, query adaptation can be achieved by simply applying any existing efficient indexing/retrieval method to a transformed version (surrogate) of a query. To address the issue of limited training samples, we further propose a novel angular regularization constraint for learning the similarity measure. The learning is formulated as a Quadratic Programming (QP) problem and can be solved efficiently by a SMO-type algorithm. Experiments on two public datasets and our 1-million web-image dataset validate that our proposed method can consistently bring improvements and the whole solution is practical in large scale applications.	algorithm;angularjs;approximation algorithm;bilinear filtering;bilinear transform;image retrieval;matrix regularization;overfitting;quadratic programming;similarity measure	Zhanghui Kuang;Jian Sun;Kwan-Yee Kenneth Wong	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2013.69	search engine indexing;image retrieval;computer science;machine learning;pattern recognition;mathematics;quadratic programming;information retrieval	Vision	25.681210084644366	-41.63217310787832	71370
1f6336857aee87400bf5027906e99f168de327e9	null space discriminant locality preserving projections for face recognition	locality preserving projection;eigenvalue problem;small sample size;small sample size problem;facial feature extraction;locality preserving projections;face recognition;synthetic data;null space discriminant locality preserving projections	In this paper, we propose a null space discriminant locality preserving projections (NDLPP) method for facial feature extraction and recognition. Based on locality preserving projections (LPP) and discriminant locality preserving projections (DLPP) methods, NDLPP comes into the characteristics of DLPP that encodes both the geometrical and discriminant structure of the data manifold, and addresses synthetic data and ORL, Yale, and FERET face databases are performed to test and evaluate the proposed algorithm. The results demonstrate the effectiveness of NDLPP. & 2008 Elsevier B.V. All rights reserved.	algorithm;database;dimensionality reduction;discriminant;feret (facial recognition technology);facial recognition system;feature extraction;format-preserving encryption;genetic algorithm;kernel (linear algebra);laplacian matrix;locality of reference;loss function;optimization problem;return loss;synthetic data	Liping Yang;Weiguo Gong;Xiaohua Gu;Weihong Li;Yixiong Liang	2008	Neurocomputing	10.1016/j.neucom.2008.03.009	facial recognition system;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;statistics;synthetic data	AI	25.58712242688027	-41.260743419126605	71429
7113907629a6f4f2f033bc3d1e1716dd9715aa9e	an efficient semi-supervised classifier based on block-polynomial mapping	semi supervised block polynomial mapping classification manifold regularization;kernel;manifolds;training;semi supervised image classification semi supervised classifier block polynomial mapping image feature learning khatri rao product local discriminative information traditional kernel mapping manifold regularization framework;polynomials;manganese;learning systems;polynomials kernel manifolds training manganese signal processing algorithms learning systems;期刊论文;polynomial matrices computational complexity feature extraction image classification learning artificial intelligence;signal processing algorithms	In this paper, we propose a block-polynomial mapping for image feature learning, which can be efficiently represented by the matrix Khatri-Rao product. The block-polynomial mapping not only captures the local discriminative information within the image structure, but is also much more efficient than the traditional kernel mapping. Moreover, we embed the proposed mapping into the manifold regularization framework for semi-supervised image classification. Experimental results demonstrate that, while maintaining a comparable classification accuracy, the proposed algorithm performs much more efficient than the state-of-the-art methods.	algorithm;computer vision;feature (computer vision);feature learning;kernel (operating system);machine learning;manifold regularization;matrix regularization;nonlinear system;polynomial;semi-supervised learning;semiconductor industry;the matrix	Di Wang;Xiaoqin Zhang;Mingyu Fan;Xiuzi Ye	2015	IEEE Signal Processing Letters	10.1109/LSP.2015.2433917	mathematical optimization;kernel;manganese;machine learning;linear classifier;pattern recognition;mathematics;polynomial kernel	Vision	28.237673123039734	-44.00614541473767	71480
0e1034985c21d77bf3b85b8aa8b685ad3c0b554e	local descriptions for human action recognition from 3d reconstruction data	kinect;action recognition;view invariance;3d flow;3d reconstruction	In this paper, a view-invariant approach to human action recognition using 3D reconstruction data is proposed. Initially, a set of calibrated Kinect sensors are employed for producing a 3D reconstruction of the performing subjects. Subsequently, a 3D flow field is estimated for every captured frame. For performing action recognition, the `Bag-of-Words' methodology is followed, where Spatio-Temporal Interest Points (STIPs) are detected in the 4D space (xyz-coordinates plus time). A novel local-level 3D flow descriptor is introduced, which among others incorporates spatial and surface information in the flow representation and efficiently handles the problem of defining 3D orientation at every STIP location. Additionally, typical 3D shape descriptors of the literature are used for producing a more complete representation. Experimental results as well as comparative evaluation using datasets from the Huawei/3DLife 3D human reconstruction and action recognition Grand Challenge demonstrate the efficiency of the proposed approach.	3d reconstruction;bag-of-words model;kinect;sensor;shape analysis (digital geometry)	Georgios Th. Papadopoulos;Petros Daras	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025569	3d reconstruction;computer vision;simulation;computer science;artificial intelligence;mathematics	Vision	36.685979957283514	-49.902244439321585	71779
444a252270be53f88aab401825e248f2ab94b5c2	a sift-svm method for detecting cars in uav images	automobiles;support vector machines;sensors;support vector machine svm;support vector machine classifier sift svm method cars detection method uav images unmanned aerial vehicles civilian remote sensing cars automatic detection feature extraction process scalar invariant feature transform;training;image classification;feature extraction process;accuracy;support vector machine classifier;cars detection method;unmanned aerial vehicle uav;image color analysis;car detection;feature extraction;remote sensing;transforms automobiles autonomous aerial vehicles feature extraction image classification object detection remote sensing support vector machines traffic engineering computing;uav images;civilian remote sensing;transforms;scale invariant feature transform sift;traffic engineering computing;unmanned aerial vehicle uav car detection feature extraction scale invariant feature transform sift support vector machine svm;unmanned aerial vehicles;sift svm method;autonomous aerial vehicles;feature extraction support vector machines image color analysis training accuracy transforms sensors;cars automatic detection;object detection;scalar invariant feature transform	In the last years, the advent of unmanned aerial vehicles (UAVs) for civilian remote sensing purposes has generated a lot of interest because of the various new applications they can offer. One of them is represented by the automatic detection and counting of cars. In this paper, we propose a novel car detection method. It starts with a feature extraction process based on scalar invariant feature transform (SIFT) thanks to which a set of keypoints is identified in the considered image and opportunely described. Successively, the process discriminates between keypoints assigned to cars and those associated with all remaining objects by means of a support vector machine (SVM) classifier. Experimental results have been conducted on a real UAV scene. They show how the proposed method allows providing interesting detection performances.	aerial photography;feature extraction;performance;sensor;statistical classification;support vector machine;unmanned aerial vehicle;vector graphics	Thomas Moranduzzo;Farid Melgani	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6352585	support vector machine;computer vision;contextual image classification;feature extraction;computer science;sensor;machine learning;pattern recognition;accuracy and precision;remote sensing	Robotics	32.785364204629644	-43.93179807985826	71797
8cb8d352eb5120b1f0972ba5d1dfb3000a0e173b	edge-aware integration model for semantic labeling of rare classes		Rare class objects in natural images often convey semanti-cally important information than background for scene understanding, but they are often overlooked during image parsing due to their low occurrence frequency and limited spatial coverage. In this work, we present a superpixel-based and rare class-oriented scene labeling framework (sRCSL), which seamlessly incorporates edge features into an integration of global and local CNN models. A dual-mode coarse-to-fine superpixel representation is developed for accurate yet efficient labeling, where coarse and fine superpixels are applied to background and rare classes respectively. Furthermore, saliency detection is incorporated by the combination of probabilistic belief maps from local and global inference. Experimental results demonstrate promising performance of the proposed framework on the SIFTflow dataset both qualitatively and quantitatively for semantic labeling, especially for rare classes.	map;parsing	Liangjiang Yu;Guoliang Fan	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8297130	probabilistic logic;salience (neuroscience);image segmentation;parsing;semantics;artificial intelligence;pattern recognition;inference;computer science	Vision	27.142265488523968	-51.93769829759141	71860
45e135167a763f3dd49cfd90b088699378e56790	hierarchically learned view-invariant representations for cross-view action recognition		Recognizing human actions from varied views is challenging due to huge appearance variations in different views. The key to this problem is to learn discriminant view-invariant representations generalizing well across views. In this paper, we address this problem by learning view-invariant representations hierarchically using a novel method, referred to as Joint Sparse Representation and Distribution Adaptation (JSRDA). To obtain robust and informative feature representations, we first incorporate a sample-affinity matrix into the marginalized stacked denoising Autoencoder (mSDA) to obtain shared features, which are then combined with the private features. In order to make the feature representations of videos across views transferable, we then learn a transferable dictionary pair simultaneously from pairs of videos taken at different views to encourage each action video across views to have the same sparse representation. However, the distribution difference across views still exists because a unified subspace where the sparse representations of one action across views are the same may not exist when the view difference is large. Therefore, we propose a novel unsupervised distribution adaptation method that learns a set of projections that project the source and target views data into respective low-dimensional subspaces where the marginal and conditional distribution differences are reduced simultaneously. Therefore, the finally learned feature representation is view-invariant and robust for substantial distribution difference across views even the view difference is large. Experimental results on four multiview datasets show that our approach outperforms the state-ofthe-art approaches.	affinity analysis;autoencoder;dictionary;discriminant;experiment;feature learning;information;machine learning;marginal model;noise reduction;sparse approximation;sparse matrix;unified framework	Yang Liu;Zhaoyang Lu;Jing Li;Tao Yang	2018	CoRR	10.1109/TCSVT.2018.2868123	linear subspace;autoencoder;pattern recognition;artificial intelligence;sparse approximation;generalization;computer science;invariant (mathematics);subspace topology;matrix (mathematics);conditional probability distribution	Vision	25.35774193264684	-44.81453150730331	72006
e9015264b4cfd52db50bc5e9d5ae2f0997089880	an energy model approach to people counting for abnormal crowd behavior detection	people counting;image potential energy model;abnormal events;intelligent surveillance	Abnormal crowd behavior detection plays an important role in surveillance applications. We propose a camera parameter independent and perspective distortion invariant approach to detect two types of abnormal crowd behavior. The two typical abnormal activities are people gathering and running. Since people counting is necessary for detecting the abnormal crowd behavior, we present an potential energy-based model to estimate the number of people in public scenes. Building histograms on the X- and Y-axes, respectively, we can obtain probability distribution of the foreground object and then define crowd entropy. We define the Crowd Distribution Index by combining the people counting results with crowd entropy to represent the spatial distribution of crowd. We set a threshold on Crowd Distribution Index to detect people gathering. To detect people running, the kinetic energy is determined by computation of optical flow and Crowd Distribution Index. With a threshold, kinetic energy can be used to detect people running. To test the performance of our algorithm, videos of different scenes and different crowd densities are used in the experiments. Without camera calibration and training data, our method can robustly detect abnormal behaviors with low computation load.	people counter	Guogang Xiong;Jun Cheng;Xinyu Wu;Yen-Lun Chen;Yongsheng Ou;Yangsheng Xu	2012	Neurocomputing	10.1016/j.neucom.2011.12.007	computer vision;simulation;computer security	Vision	38.86707790972804	-45.7242381723051	72151
4e1c84aa032bdf5733937676f9528b943159d7ec	learning spread-out local feature descriptors		We propose a simple, yet powerful regularization technique that can be used to significantly improve both the pairwise and triplet losses in learning local feature descriptors. The idea is that in order to fully utilize the expressive power of the descriptor space, good local feature descriptors should be sufficiently “spread-out” over the space. In this work, we propose a regularization term to maximize the spread in feature descriptor inspired by the property of uniform distribution. We show that the proposed regularization with triplet loss outperforms existing Euclidean distance based descriptor learning techniques by a large margin. As an extension, the proposed regularization technique can also be used to improve image-level deep feature embedding.	euclidean distance;expressive power (computer science);feature model;feature vector;matrix regularization;prototype;triplet state;visual descriptor	Xu Zhang;Felix X. Yu;Sanjiv Kumar;Shih-Fu Chang	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.492	euclidean distance;expressive power;artificial intelligence;robustness (computer science);regularization (mathematics);pattern recognition;embedding;computer science;pairwise comparison;uniform distribution (continuous)	Vision	24.64258799370316	-47.578478261502525	72234
784f98604d9775e56d2951c26392c914785cf537	a landmark selection method for l-isomap based on greedy algorithm and its application	eigenvalues and eigenfunctions;silicon;telecommunication traffic computational complexity data reduction greedy algorithms internet matrix algebra;manifolds;approximation algorithms;greedy algorithms;会议论文;low dimensional manifold structure landmark selection method l isomap greedy algorithm isometric feature mapping nonlinear dimensionality reduction method computational complexity embedding computation physical data sets synthetic data sets internet traffic matrix data low dimensional features;internet;internet manifolds silicon greedy algorithms automation approximation algorithms eigenvalues and eigenfunctions;automation	Isometric feature mapping (Isomap) is a widely-used nonlinear dimensionality reduction method, but it suffers from high computational complexity. L-Isomap is a variant of Isomap which is faster than Isomap. In this algorithm, a subset of points are chosen out of the total data points as landmark points so as to simplify the embedding computation. In this paper, we propose a novel landmark selection method for L-Isomap based on a greedy algorithm. Experiments performed on synthetic and physical data sets validate the effectiveness of the proposed method. Internet traffic matrix has been an effective model to analyzing the Internet. However, the Internet traffic matrix data usually possesses high dimensionality. In this paper, we apply the improved L-Isomap to the real Internet traffic matrix data to investigate its low-dimensional features. The experiment results show that the Internet traffic matrix has a small intrinsic dimension and there indeed exists a low-dimensional manifold structure.	computation;computational complexity theory;data point;experiment;greedy algorithm;internet;intrinsic dimension;isomap;isometric projection;landmark point;nonlinear dimensionality reduction;nonlinear system;synthetic data;synthetic intelligence	Hao Shi;Baoqun Yin;Xiaofeng Zhang;Yu Kang;Yingke Lei	2015	2015 54th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2015.7403383	mathematical optimization;greedy algorithm;the internet;manifold;theoretical computer science;automation;machine learning;mathematics;silicon;approximation algorithm	ML	27.56378147370001	-39.50632556733938	72236
9dfd1e9daea4c54a05b06df905bf8ee1faccaa72	new l2, 1-norm relaxation of multi-way graph cut for clustering		The clustering methods have absorbed even-increasing attention in machine learning and computer vision communities in recent years. Exploring manifold information in multi-way graph cut clustering, such as ratio cut clustering, has shown its promising performance. However, traditional multi-way ratio cut clustering method is NP-hard and thus the spectral solution may deviate from the optimal one. In this paper, we propose a new relaxed multi-way graph cut clustering method, where 2,1-norm distance instead of squared distance is utilized to preserve the solution having much more clearer cluster structures. Furthermore, the resulting solution is constrained with normalization to obtain more sparse representation, which can encourage the solution to contain more discrete values with many zeros. For the objective function, it is very difficult to optimize due to minimizing the ratio of two non-smooth items. To address this problem, we transform the objective function into a quadratic problem on the Stiefel manifold (QPSM), and introduce a novel yet efficient iterative algorithm to solve it. Experimental results on several benchmark datasets show that our method significantly outperforms several state-of-the-art clustering approaches.	algorithm;benchmark (computing);cluster analysis;clustering coefficient;cut (graph theory);experiment;graph cuts in computer vision;iterative method;linear programming relaxation;loss function;machine learning;manifold regularization;mathematical optimization;maximum cut;np-hardness;optimization problem;quadratic equation;sparse approximation;sparse matrix	Xu Yang;Cheng Deng;Xianglong Liu;Feiping Nie	2018				AI	25.975359765910778	-40.14588303377176	72438
19183c83397a2ca2ef4e74d3b87810e73d32201d	predicting driver maneuvers by learning holistic features	vehicles cameras sensors vehicle dynamics radar tracking foot trajectory;driver analysis and assistance systems driver maneuver prediction holistic feature learning driver maneuver recognition sensor array driver head driver hand driver foot gestures latent dynamic discriminative framework driver activity recognition driver activity prediction naturalistic on road dataset;traffic engineering computing driver information systems gesture recognition learning artificial intelligence sensor arrays	In this work, we propose a framework for the recognition and prediction of driver maneuvers by considering holistic cues. With an array of sensors, driver's head, hand, and foot gestures are being captured in a synchronized manner together with lane, surrounding agents, and vehicle parameters. An emphasis is put on real-time algorithms. The cues are processed and fused using a latent-dynamic discriminative framework. As a case study, driver activity recognition and prediction in overtaking situations is performed using a naturalistic, on-road dataset. A consequence of this work would be in development of more effective driver analysis and assistance systems.	activity recognition;algorithm;holism;real-time clock;real-time computing;sensor	Eshed Ohn-Bar;Ashish Tawari;Sujitha Martin;Mohan Manubhai Trivedi	2014	2014 IEEE Intelligent Vehicles Symposium Proceedings	10.1109/IVS.2014.6856612	computer vision;simulation;engineering;communication	Robotics	37.64549617845908	-43.34922682278939	72556
ebe07afa5bfad16dbfbebeb37dc36dee4f706e4e	probabilistic modeling of real-world scenes in a virtual environment		In this paper we present an approach for the automated creation of real-world scenes in an virtual environment. Here we focus on human-robot interaction and collaboration in the industrial domain, with corresponding virtual object classes and inter-class constellations. As the basis for the sample generation process, we probabilistically model essential discrete and continuous object parameters, by adapting a generic mixed joint density function to distinct scene classes, in order to capture the specific interand intra-class dependencies. To provide a convenient way to assert these object interactions, we use a Bayesian Network for the representation of the density function, where dependencies can directly be modeled by the network layout. For the conditioned and uncertain descriptions of object translations, we use hierarchical Gaussian Mixture Models as geometrical sampling primitives in the 3D space. In our paper, we show how the combination of a Bayesian Network with these sampling primitives can directly be used for the automated collision avoidance of objects, during the sampling process. For the illustration of the applicability and usefulness of our approach, we instantiate the generic and abstract concept using an example with reduced complexity.	bayesian network;computer vision;human–robot interaction;mixture model;sampling (signal processing);synthetic data;virtual reality	Frank Dittrich;Stephan Irgenfried;Heinz Wörn	2015		10.5220/0005313301650173	mixture model;virtual image;computer vision;computer science;artificial intelligence;probabilistic logic;sampling (statistics);synthetic data;machine learning;virtual machine;graphical model;bayesian network	Vision	36.975447934235824	-41.56581221548127	72591
ae9ebfba1c8ffb80ec675595438680020bdf759d	fusion of supervised and unsupervised learning for improved classification of hyperspectral images	voting rules;hyperspectral images;markov random field;fuzzy c means;markov fisher selector;support vector machine	In this paper, we introduce a novel framework for improved classification of hyperspectral images based on the combination of supervised and unsupervised learning paradigms. In particular, we propose to fuse the capabilities of the support vector machine classifier and the fuzzy C-means clustering algorithm. While the former is used to generate a spectral-based classification map, the latter is adopted to provide an ensemble of clustering maps. To reduce the computation complexity, the most representative spectral channels identified by the Markov Fisher Selector algorithm are used during the clustering process. Then, these maps are successively labeled via a pairwise relabeling procedure with respect to the pixel-based classification map using voting rules. To generate the final classification result, we propose to aggregate the obtained set of spectro-spatial maps through different fusion methods based on voting rules and Markov Random Field theory. Experimental results obtained on two hyperspectral images acquired by the reflective optics system imaging spectrometer and the airborne visible/infrared imaging spectrometer, respectively; confirm the promising capabilities of the proposed framework.	supervised learning;unsupervised learning	Naif Alajlan;Yakoub Bazi;Farid Melgani;Ronald R. Yager	2012	Inf. Sci.	10.1016/j.ins.2012.06.031	support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics	AI	30.677368310868772	-44.419289240561824	72924
7865d011f39fcac7c48e53c2b1ff726f6e6b56ea	active humanoid vision and object classification	humanoid robot;object recognition;kernel;image resolution;support vector machines;real time;training;humanoid robots robot vision systems object recognition image recognition gabor filters cameras computer vision humans image resolution three dimensional displays;image classification;object learning;robot vision;humanoid robots;feature extraction;pixel;robots;robot vision humanoid robots image classification learning artificial intelligence object recognition;humanoid robot active humanoid vision object classification object learning object recognition;object classification;learning artificial intelligence;active humanoid vision	In this paper we study object learning and recognition on a humanoid robot with foveated vision. The developed approach is view-based and can learn viewpoint-independent representations for object recognition. The training data is collected statistically and in an interactive way where a human instructor freely shows the object from a number of different viewpoints. The proposed system was fully implemented and runs in real-time, which is essential for meaningful interaction with a humanoid robot.	computer vision;humanoid robot;outline of object recognition;real-time computing;real-time transcription	Ales Ude	2009	2009 24th International Symposium on Computer and Information Sciences	10.1109/ISCIS.2009.5291811	computer vision;computer science;humanoid robot;artificial intelligence;machine learning;3d single-object recognition	Robotics	36.35614979512063	-47.04551311172646	72975
bfcba38d563a4a75f69f892a9638f464049723b9	depth prediction without the sensors: leveraging structure for unsupervised learning from monocular videos		Learning to predict scene depth from RGB inputs is a challenging task both for indoor and outdoor robot navigation. In this work we address unsupervised learning of scene depth and robot ego-motion where supervision is provided by monocular videos, as cameras are the cheapest, least restrictive and most ubiquitous sensor for robotics. Previous work in unsupervised image-to-depth learning has established strong baselines in the domain. We propose a novel approach which produces higher quality results, is able to model moving objects and is shown to transfer across data domains, e.g. from outdoors to indoor scenes. The main idea is to introduce geometric structure in the learning process, by modeling the scene and the individual objects; camera ego-motion and object motions are learned from monocular videos as input. Furthermore an online refinement method is introduced to adapt learning on the fly to unknown domains. The proposed approach outperforms all state-of-the-art approaches, including those that handle motion e.g. through learned flow. Our results are comparable in quality to the ones which used stereo as supervision and significantly improve depth prediction on scenes and datasets which contain a lot of object motion. The approach is of practical relevance, as it allows transfer across environments, by transferring models trained on data collected for robot navigation in urban scenes to indoor navigation settings. The code associated with this paper can be found at https://sites.google.com/	algorithm;baseline (configuration management);benchmark (computing);motion estimation;on the fly;refinement (computing);relevance;robotic mapping;robotics;sensor;unsupervised learning	Vincent Casser;Sören Pirk;Reza Mahjourian;Anelia Angelova	2018	CoRR		artificial intelligence;data domain;machine learning;on the fly;robot;unsupervised learning;computer science;monocular;rgb color model;robotics	Robotics	28.246017973055768	-49.533752446567874	73076
eb0dfd6a62766afd144250fa5b23015f8379d765	object based fusion of polarimetric sar and hyperspectral imaging for land use classification	sar signalverarbeitung;landoberflache	In this paper, we propose an object-based fusion approach for the joint use of polarimetric synthetic aperture radar (PolSAR) and hyperspectral data. The proposed approach extracts information from both datasets based on an object-level, which is used here for land use classification. The achieved classification result infers that the proposed methodology improves the classification performance of both hyperspectral and PolSAR data and can properly gather complementary information of the two kinds of dataset. The fusion approach also considers that only limited training samples are available, which is often the case in remote sensing.	object-based language;polarimetry;synthetic data	Jingliang Hu;Pedram Ghamisi;Andreas Schmitt;Xiao xiang Zhu	2016	2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)	10.1109/WHISPERS.2016.8071752	computer vision;geography;data mining;remote sensing	Vision	30.870281473326763	-44.68096425245911	73086
7653348a68e20f1a85ddcb56fdb6623ab0927ca0	detection of global and local motion changes in human crowds	histograms;training;video sequences;trajectory histograms cameras training video sequences computational modeling adaptive optics;computational modeling;trajectory;crowd simulation human crowd analysis change detection ideo surveillance;cameras;adaptive optics	Crowds arise in a variety of situations, such as public concerts and sporting matches. In typical conditions, the crowd moves in an orderly manner, but panic situations may lead to catastrophic results. We propose a computer vision method to identify motion pattern changes in human crowds that can be related to an unusual event. The proposed approach can identify global changes, by evaluating 2D motion histograms in time, and also local effects, by identifying clusters that present similar spatial locations and velocity vectors. The method is tested both on publicly available data sets involving crowded scenarios and on synthetic data produced by a crowd simulation algorithm, which allows the creation of controlled environments with known motion patterns that are particularly suitable for multicamera scenarios.	algorithm;cache coherence;computer vision;crowd simulation;global change;machine learning;synthetic data;synthetic intelligence;temporal logic;velocity (software development);web coverage service	Igor R. de Almeida;Vinícius Jurinic Cassol;Norman I. Badler;Soraia Raupp Musse;Cláudio Rosito Jung	2017	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2016.2596199	computer vision;simulation;trajectory;crowd simulation;histogram;adaptive optics;computational model;statistics;computer graphics (images)	Vision	39.04359235123391	-47.091030692640906	73119
9de25c2f6640d407dea65a691504965526d6d3b8	sparse representation based hyperspectral imagery classification via expanded dictionary	decision rule sparse representation hyperspectral imagery classification expanded dictionary pattern classification pattern recognition 1d dyadic wavelet transform wavelet features spectral signatures;dictionaries abstracts hyperspectral imaging indexes programming support vector machines accuracy;dyadic wavelet transform hyperspectral imagery classification sparse representation;wavelet transforms feature extraction hyperspectral imaging image classification image representation learning artificial intelligence	Recently, pattern classification and recognition based on sparse representation have seen a surge of interest in many applications. In this article, we present a method of sparse representation based hyperspectral imagery classification via expanded dictionary. The original spectral signatures in hyperspectral imagery are transformed with 1-D dyadic wavelet transform. Then these wavelet features are combined with the original spectral signatures to form an expanded dictionary. Finally, linear programming is employed to calculate the sparse solution on such a dictionary which was further substituted into related decision rule. Results of experiment on real hyperspectral imagery validate the effectiveness of our method.	algorithm;dictionary;dyadic transformation;linear programming;sparse approximation;sparse matrix;statistical classification;type signature;wavelet transform	Lin He;Weitong Ruan;Yuanqing Li	2012	2012 4th Workshop on Hyperspectral Image and Signal Processing (WHISPERS)	10.1109/WHISPERS.2012.6874300	computer vision;computer science;machine learning;pattern recognition	Vision	29.868082891030774	-42.86696444147067	73202
9ebb7f51ccd51b13e872a1407a1a0cbe80437e97	classification of casi-3 hyperspectral image by subspace method	geophysical image processing;casi 3;data compression;sensors;training;image classification;casi 3 hyperspectral data subspace methods;iterative methods;training data;accuracy;karhunen loeve transforms;subspace methods;terrain mapping data compression geophysical image processing image classification iterative methods karhunen loeve transforms learning artificial intelligence principal component analysis;principal component analysis;hyperspectral data;subspace method;terrain mapping;learning artificial intelligence;hyperspectral imaging;hyperspectral remote sensing data casi 3 hyperspectral image classification supervised subspace learning classification method spectral band land cover classification class featuring information compression method feature subspace karhunen loeve transform principal component analysis iterative learning technology averaged learning subspace method compact airborne spectrographic imager 3 data set grass species mapping grass health monitoring;hyperspectral image;training hyperspectral imaging accuracy sensors training data	This study presents a supervised subspace learning classification method which can be applied directly to the original set of spectral bands of hyperspectral data for land cover classification purpose. The CLAss-Featuring Information Compression (CLAFIC) method is used to generate the appropriate feature subspace for each class on the training data set by Karhunen-Loève transform (also known as the principal component analysis). Then, using the iterative learning technology of averaged learning subspace methods (ALSM) to rotate the subspaces slowly for optimizes the subspaces to get better classification accuracy. We carried out experiments with 68 spectral bands Compact Airborne Spectrographic Imager-3 (CASI-3) data set. Experimental results show that Subspace method is a valid and effective alternative to other pattern recognition approaches for the mapping grass species and monitoring grass health using hyperspectral remote sensing data. Moreover, it is worth noting that the ALSMs are easily applied (i.e. they only request to set two parameters and can be directly applied to hyperspectral data) and they can entirely identify the training samples in a finite number of steps.	experiment;iterative method;pattern recognition;principal component analysis;test set	Buho Hoshino;Hasi Bagan;Akihiro Nakazawa;Masami Kaneko;Masaki Kawai;Tetsuo Yabuki	2011	2011 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2011.6049232	data compression;computer vision;training set;contextual image classification;computer science;sensor;hyperspectral imaging;machine learning;pattern recognition;accuracy and precision;iterative method;statistics;principal component analysis	Vision	29.997986863719152	-43.07247750779609	73270
3dd2a632e16f93eada6878dbad260622527d3551	robust underwater fish classification based on data augmentation by adding noises in random local regions		Underwater fish classification is in great demand, but the unrestricted natural environment makes it a challenging task. The monitor placed underwater gets a lot of low-quality and hard-to-mark marine fish images. These images suffer from various illumination, complex background etc. At the same time, there are many high-quality and easy-to-mark marine fish pictures on the Internet. In this paper, we propose an effective data augmentation approach for improving the classification accuracy of low-quality marine fish images. In our method, unlike the existing global image method, random local regions are proposed for simulating local occlusion and fuzziness in various underwater environment. In addition, four types of noise are incorporated for augmenting training data set. Experimental results demonstrate that our approach can significantly enhance the classification performance of low-quality marine fish images under various challenging conditions when using high-quality marine fish images as training sets.		Guanqun Wei;Zhiqiang Wei;Lei Huang;Jie Nie;Huanhuan Chang	2018		10.1007/978-3-030-00767-6_47	the internet;convolutional neural network;computer vision;artificial intelligence;computer science;pattern recognition;underwater;training set;underwater vision	ML	28.75451251135229	-50.03433226052364	73700
ffd71815bd5b04ac4c007695a8df67eed1f85a97	graph-based transform coding with application to image compression		In this paper, we propose a new graph-based coding framework and illustrate its application to image compression. Our approach relies on the careful design of a graph that optimizes the overall rate-distortion performance through an effective graph-based transform. We introduce a novel graph estimation algorithm, which uncovers the connectivities between the graph signal values by taking into consideration the coding of both the signal and the graph topology in rate-distortion terms. In particular, we introduce a novel coding solution for the graph by treating the edge weights as another graph signal that lies on the dual graph. Then, the cost of the graph description is introduced in the optimization problem by minimizing the sparsity of the coefficients of its graph Fourier transform (GFT) on the dual graph. In this way, we obtain a convex optimization problem whose solution defines an efficient transform coding strategy. The proposed technique is a general framework that can be applied to different types of signals, and we show two possible application fields, namely natural image coding and piecewise smooth image coding. The experimental results show that the proposed method outperforms classical fixed transforms such as DCT, and, in the case of depth map coding, the obtained results are even comparable to the state-of-the-art graph-based coding method, that are specifically designed for depth map images.		Giulia Fracastoro;Dorina Thanou;Pascal Frossard	2017	CoRR		mathematical optimization;fourier transform;image compression;artificial intelligence;discrete cosine transform;transform coding;mathematics;topological graph theory;pattern recognition;dual graph;convex optimization;optimization problem	ML	32.642505498652035	-38.191132273674484	73833
4cc1b0666cfa522922bad8f90bed5648712e7200	from videos to verbs: mining videos for activities using a cascade of dynamical systems	pattern clustering;video streaming;single video stream;video sequences streaming media clustering algorithms automation educational institutions indexing surveillance airports pattern analysis hidden markov models;generic model;video signal processing;video sequence clustering;video mining;dynamic system;activities extraction;distance metric;video signal processing image sequences pattern clustering;dynamical systems;single video stream video mining dynamical systems video sequence clustering activities extraction;image sequences	Clustering video sequences in order to infer and extract activities from a single video stream is an extremely important problem and has significant potential in video indexing, surveillance, activity discovery and event recognition. Clustering a video sequence into activities requires one to simultaneously recognize activity boundaries (activity consistent subsequences) and cluster these activity subsequences. In order to do this, we build a generative model for activities (in video) using a cascade of dynamical systems and show that this model is able to capture and represent a diverse class of activities. We then derive algorithms to learn the model parameters from a video stream and also show how a single video sequence may be clustered into different clusters where each cluster represents an activity. We also propose a novel technique to build affine, view, rate invariance of the activity into the distance metric for clustering. Experiments show that the clusters found by the algorithm correspond to semantically meaningful activities.	algorithm;cluster analysis;dynamical system;generative model;streaming media	Pavan K. Turaga;Ashok Veeraraghavan;Rama Chellappa	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383170	computer vision;data stream clustering;dynamical systems theory;metric;computer science;dynamical system;video tracking;data mining;mathematics;block-matching algorithm;multimedia;cluster analysis;motion compensation	Vision	37.925887684707384	-48.16198409638893	74041
fa2fcc3071793667cf96f87053f9bcf48a7fc181	multi-cue integration for multi-camera tracking	unsupervised learning;video surveillance;tracking fusion integration multi camera;multicamera tracking;supervised learning;fusion;training;multicue integration;integration;unsupervised learning method;training data;large camera networks;accuracy;disjoint views;cue integration;score fusion methods;large camera networks multicue integration multicamera tracking target tracking disjoint views matching model unsupervised learning method score fusion methods;video surveillance object detection target tracking unsupervised learning;multi camera;target tracking;camera network;cameras target tracking accuracy supervised learning training data unsupervised learning training;cameras;matching model;tracking;object detection	For target tracking across multiple cameras with disjoint views, previous works usually employed multiple cues and focused on learning a better matching model of each cue, separately. However, none of them had discussed how to integrate these cues to improve performance, to our best knowledge. In this paper, we look into the multi-cue integration problem and propose an unsupervised learning method since a complicated training phase is not always viable. In the experiments, we evaluate several types of score fusion methods and show that our approach learns well and can be applied to large camera networks more easily.	experiment;match moving;oracle fusion middleware;romp;supervised learning;unsupervised learning	Kuan-Wen Chen;Yi-Ping Hung	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.44	unsupervised learning;computer vision;training set;fusion;computer science;machine learning;pattern recognition;accuracy and precision;tracking;supervised learning	Vision	36.39256948704265	-47.31240479537373	74279
0155040ba024640c9516ffebfbe9e45d4c20f1e8	using decision trees to recognize visual events	institutional repositories;event recognition;video surveillance;decision tree;fedora;random tree;vital;activity pattern;visual surveillance;randomized decision trees;graph model;vtls;automated visual surveillance system;ils;activity recognition	This paper presents a classifier-based approach to recognize events in video surveillance sequences. The aim of this work is to propose a generic event recognition system that can be used without relying on a long-term tracking procedure. It is composed of three stages. The first one aims at defining and building a set of relevant features from the foreground objects. Second, a clustering tree-based method is used to handle the features and aggregate them locally in a set of coarse to fine activity patterns. Finally, events are modeled as a sequence of structured patterns with an ensemble of randomized trees. In particular, we want this classifier to discover the temporal and causal correlations between the most discriminative patterns. Our system is tested on simulated events and in a real world context with the CAVIAR video sequences dataset. Preliminary results demonstrate the effectiveness of the proposed framework for event recognition in automated visual surveillance applications. We also prove that more flexible algorithms (i.e. deterministic classifiers) rather than probabilistic graph models are conceivable for video events analysis.	aggregate data;causal filter;closed-circuit television;cluster analysis;decision tree;randomized algorithm;simulation	Cedric J Simon;Jérôme Meessen;Christophe De Vleeschouwer	2008		10.1145/1463542.1463550	computer science;machine learning;decision tree;pattern recognition;data mining;activity recognition	Vision	34.7010933217087	-46.562274748399616	74324
4a2525d9e5afc91b0f6690eb6049756b1641c4fc	discriminative subspace learning with sparse representation view-based model for robust visual tracking	discriminative subspace learning;selection;spectral regression;regression;object tracking;sparse representation;article	In this paper, we propose a robust tracking algorithm to handle drifting problem. This algorithm consists of two parts: the first part is the G&D part that combines Generative model and Discriminative model for tracking, and the second part is the View-Based model for target appearance that corrects the result of the G&D part if necessary. In G&D part, we use the Maximum Margin Projection (MMP) to construct a graph model to preserve both local geometrical and discriminant structures of the data manifold in low dimensions. Therefore, such discriminative subspace combined with traditional generative subspace can benefit from both models. In addition, we address the problem of learning maximum margin projection under the Spectral Regression (SR) which results in significant savings in computational time. To further solve the drift, an online learned sparsely represented view-based model of the target is complementary to the G&D part. When the result of G&D part is unreliable, the view-based model can rectify the result in order to avoid drifting. Experimental results on several challenging video sequences demonstrate the effectiveness and robustness of our approach. HighlightsA tracker combines the generative and the discriminative model.Introduce maximum margin projection into object tracking.Transform MMP function to be solved under the Spectral Regression framework.Learning an incremental view-based model of the target with sparse representation.Experiments show our tracker to be more robust and stable than state-of-the-art methods.	sparse approximation;sparse matrix;video tracking	Yuan Xie;Wensheng Zhang;Yanyun Qu;Yinghua Zhang	2014	Pattern Recognition	10.1016/j.patcog.2013.07.010	selection;computer vision;regression;computer science;machine learning;video tracking;pattern recognition;sparse approximation;mathematics	Vision	33.378647781467365	-47.291390778040736	74449
54980e35ad4f6182fc8d7399caa749143c31045a	metric learning for sequences in relational lvq	metric learning;relational lvq;dissimilarity data;sequential data	Metric learning constitutes a well-investigated field for vectorial data with successful applications, e.g. in computer vision, information retrieval, or bioinformatics. One particularly promising approach is offered by low-rank metric adaptation integrated into modern variants of learning vector quantization (LVQ). This technique is scalable with respect to both, data dimensionality and the number of data points, and it can be accompanied by strong guarantees of learning theory. Recent extensions of LVQ to general (dis-)similarity data have paved the way towards LVQ classifiers for non-vectorial, possibly discrete, structured objects such as sequences, which are addressed by classical alignment in bioinformatics applications. In this context, the choice of metric parameters plays a crucial role for the result, just as it does in the vectorial setting. In this contribution, we propose a metric learning scheme which allows for an autonomous learning of parameters (such as the underlying scoring matrix in sequence alignments) according to a given discriminative task in relational LVQ. Besides facilitating the often crucial and problematic choice of the scoring parameters in applications, this extension offers an increased interpretability of the results by pointing out structural invariances for the given task.	autonomous robot;bioinformatics;ca gen;computer vision;data point;information retrieval;learning vector quantization;position weight matrix;scalability;sequence alignment;statistical learning theory;taiwan fellowship editor	Bassam Mokbel;Benjamin Paaßen;Frank-Michael Schleif;Barbara Hammer	2015	Neurocomputing	10.1016/j.neucom.2014.11.082	machine learning;pattern recognition;data mining;mathematics;statistics	ML	24.882642382673595	-41.01697710208844	74539
18f51e9bdc1abdb6f1601c5c0692d6c150421a48	synthetic data for text localisation in natural images		In this paper we introduce a new method for text detection in natural images. The method comprises two contributions: First, a fast and scalable engine to generate synthetic images of text in clutter. This engine overlays synthetic text to existing background images in a natural way, accounting for the local 3D scene geometry. Second, we use the synthetic images to train a Fully-Convolutional Regression Network (FCRN) which efficiently performs text detection and bounding-box regression at all locations and multiple scales in an image. We discuss the relation of FCRN to the recently-introduced YOLO detector, as well as other end-to-end object detection systems based on deep learning. The resulting detection network significantly out performs current methods for text detection in natural images, achieving an F-measure of 84.2% on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per second on a GPU.	benchmark (computing);clutter;deep learning;eclipse;end-to-end principle;expectation propagation;graphics processing unit;international conference on document analysis and recognition;minimum bounding box;object detection;scalability;synthetic data;synthetic intelligence	Ankush Gupta;Andrea Vedaldi;Andrew Zisserman	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.254	computer vision;computer science;machine learning;pattern recognition;data mining	Vision	30.577527059162367	-50.425943812060375	74565
dde429015f1912edcce91acab451368cdba76ae5	facial feature extraction method based on coefficients of variances	coefficient of variation;high dimensionality;small sample size;singular value decomposition;image database;null space;linear discriminate analysis;facial feature extraction;face recognition;statistical analysis;linear feature extraction;feature extraction;principal component analysis;statistical pattern recognition;scattering matrix;coefficient of variance;gram schmidt orthogonalizing procedure	Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are two popular feature extraction techniques in statistical pattern recognition field. Due to small sample size problem LDA cannot be directly applied to appearance-based face recognition tasks. As a consequence, a lot of LDA-based facial feature extraction techniques are proposed to deal with the problem one after the other. Nullspace Method is one of the most effective methods among them. The Nullspace Method tries to find a set of discriminant vectors which maximize the between-class scatter in the null space of the within-class scatter matrix. The calculation of its discriminant vectors will involve performing singular value decomposition on a high-dimensional matrix. It is generally memory- and time-consuming. Borrowing the key idea in Nullspace method and the concept of coefficient of variance in statistical analysis we present a novel facial feature extraction method, i.e., Discriminant based on Coefficient of Variance (DCV) in this paper. Experimental results performed on the FERET and AR face image databases demonstrate that DCV is a promising technique in comparison with Eigenfaces, Nullspace Method, and other state-of-the-art facial feature extraction methods.	coefficient;database;eigenface;feret (facial recognition technology);facial recognition system;feature extraction;kernel (linear algebra);linear discriminant analysis;pattern recognition;principal component analysis;singular value decomposition	Fengxi Song;David Zhang;Cai-Kou Chen;Jingyu Yang	2007	Journal of Computer Science and Technology	10.1007/s11390-007-9070-2	facial recognition system;computer science;machine learning;pattern recognition;coefficient of variation;statistics	AI	25.119085834562835	-40.593842611030134	74692
1da71294d2898acd1c78d92682e3e29c724437ff	novel pattern recognition-based methods for re-identification in biometric context	recognition-based method;biometric context;novel pattern	The object re-identification problem is to authenticate an object across multiple disjoint fields of view. Towards that end, once the object has been detected and initialized at one location, one seeks to match it with a feasible set of candidates detected at other locations and over time. Detection and seeding for process initialization do not presume known identity and that one allows for reidentification of objects and/or faces whose identity might remain unknown. Standard pattern recognition methods can be exploited, including Bayesian analysis, in order to cope with the dynamic nature of the problem. In the context of biometrics, advanced video (data stream) surveillance applications (including sensor networks) for airport and subway surveillance require the ability to reidentify an individual. Faceand gaitbased recognition are especially promising biometrics for re-identification, since they can operate at a distance and do not require detailed and/or high resolution images of the subject and/or its biometric traits. Two core aspects this special issue addresses are: (1) registration to establish correspondences between parts of a pair of images; and (2) invariant template representations suitable to compare the corresponding parts. The invariance requirement comes from the inherent variability in the data capture process with respect to both sensors and subjects, e.g., PIE (pose, illumination, and expression). The registration requirement facilitates linking corresponding parts into meaningful assemblies that can be then authenticated. The methods proposed are expected to take advantage among others of (a) recognition and tracking are complementary to each other; (b) temporal reasoning and spatial layout of the different cameras can be used for pruning the set of candidate matches; and (c) the brightness transfer function between different cameras can be used to track individuals over multiple non-overlapping cameras. Further help to handle image variability comes from on-line evidence accumulation characteristic of closed-loop control, e.g., explore and exploit using sequential importance sampling (SIS). Learning and adaptation using both labeled and unlabeled data using statistical learning, in general, and semi-supervised learning, in particular, provide further help with re-identification. One particular learning strategy of interest is co-training, where only a small amount of labeled data is required to learn and the use of unlabeled data improves performance over time. The solution to the re-identification problem would make a significant contribution to a wide range of biometric applications. Reidentification of a user identity is of paramount importance for high security applications, when single and static verification are not adequate. The methods proposed for re-identification should be as unobtrusive as possible and require minimal interaction from the user. Re-identification has the potential to promote dynamic multibiometric environments, including the progressive addition of new modules for better recognition performance. The addition	authentication;bayesian network;biometrics;co-training;control theory;emoticon;feasible region;image resolution;importance sampling;machine learning;online and offline;particle filter;pattern recognition;sampling (signal processing);semi-supervised learning;semiconductor industry;sensor;spatial variability;supervised learning;transfer function;tree accumulation	Mislav Grgic;Michele Nappi;Harry Wechsler	2012	Pattern Recognition Letters	10.1016/j.patrec.2012.08.001	pattern recognition	Vision	31.910677135294815	-48.791845047935944	74817
6ae3862634f5c0b7faef4d69c0138a66552aa7de	multilinear regression for embedded feature selection with application to fmri analysis		Embedded feature selection is effective when both predictionrnand interpretation are needed. The Lasso and its extensionsrnare standard methods for selecting a subset of features whilernoptimizing a prediction function. In this paper, we are interestedrnin embedded feature selection for multidimensionalrndata, wherein (1) there is no need to reshape the multidimensionalrndata into vectors and (2) structural information fromrnmultiple dimensions are taken into account. Our main contributionrnis a new method called Regularized multilinear regressionrnand selection (Remurs) for automatically selecting a subsetrnof features while optimizing prediction for multidimensionalrndata. Both nuclear norm and the `1-norm are carefullyrnincorporated to derive a multi-block optimization algorithmrnwith proved convergence. In particular, Remurs is motivatedrnby fMRI analysis where the data are multidimensional and itrnis important to find the connections of raw brain voxels withrnfunctional activities. Experiments on synthetic and real datarnshow the advantages of Remurs compared to Lasso, ElasticrnNet, and their multilinear extensions.	algorithm;coherence (physics);elastic map;elastic net regularization;embedded system;experiment;feature selection;lasso;mathematical optimization;synthetic data;synthetic intelligence;voxel	Xiaonan Song;Haiping Lu	2017			machine learning;lasso (statistics);artificial intelligence;voxel;multilinear map;computer science;feature selection;elastic net regularization;multilinear principal component analysis;matrix norm;pattern recognition;convergence (routing)	ML	26.89061757672912	-40.03151460213886	74827
984ecfbda7249e67eca8d9b1697e81f80e2e483d	visual object categorization with new keypoint-based adaboost features	automobiles;image classification;learning (artificial intelligence);object detection;traffic engineering computing;video signal processing;boolean response;keypoint-based adaboost features;lateral-viewed cars;pedestrian database;vehicle video detection;visual object categorization;pattern recognition;real time	We present promising results for visual object categorization, obtained with adaBoost using new original “keypoints-based features”. These weak-classifiers produce a boolean response based on presence or absence in the tested image of a “keypoint” (a kind of SURF interest point) with a descriptor sufficiently similar (i.e. within a given distance) to a reference descriptor characterizing the feature. A first experiment was conducted on a public image dataset containing lateral-viewed cars, yielding 95% recall with 95% precision on test set. Preliminary tests on a small subset of a pedestrians database also gives promising 97% recall with 92 % precision, which shows the generality of our new family of features. Moreover, analysis of the positions of adaBoost-selected keypoints show that they correspond to a specific part of the object category (such as “wheel” or “side skirt” in the case of lateral-cars) and thus have a “semantic” meaning. We also made a first test on video for detecting vehicles from adaBoost-selected keypoints filtered in real-time from all detected keypoints.	adaboost;categorization;lateral computing;lateral thinking;mathematical optimization;performance;real-time clock;sensor;speeded up robust features;test set	Taoufik Bdiri;Fabien Moutarde;Bruno Steux	2009	2009 IEEE Intelligent Vehicles Symposium		computer vision;contextual image classification;probability density function;feature extraction;computer science;machine learning;pattern recognition;software testing;boosting	Vision	32.847004707491564	-51.68475161498211	75013
e4a01851d4b01b0fe5524259bfd7c7f6268d0494	robust estimation of human posture using incremental learnable self-organizing map	unsupervised learning;learning process;estimation theory;variable density self organizing map;robust estimator;human posture;incremental learnable self organizing map;artificial neural networks conferences joints impedance matching;motion estimation;joints;motion capture;artificial neural networks;variable density self organizing map robust posture estimation human posture incremental learnable self organizing map vision based motion capture system;self organising feature maps;robust posture estimation;human body;impedance matching;unsupervised learning estimation theory motion estimation pose estimation self organising feature maps;robustness;self organized map;humans;vision based motion capture system;conferences;pose estimation	We propose an approach to improve the accuracy of estimating feature points of human body on a vision-based motion capture system (MCS) by using the Variable-Density Self-Organizing Map (VDSOM). The VDSOM is a kind of Self-Organizing Map (SOM) and has an ability to learn training samples incrementally. We let VDSOM learn 3-D feature points of human body when the MCS succeeded in estimating them correctly. On the other hand, one or more 3-D feature point could not be estimated correctly, we use the VDSOM for the other purpose. The SOM including VDSOM has an ability to recall a part of weight vector which have learned in the learning process. We use this ability to recall correct patterns and complement such incorrect feature points by replacing such incorrect feature points with them.	motion capture;organizing (structure);poor posture;self-organizing map	Atsushi Shimada;Madoka Kanouchi;Daisaku Arita;Rin-ichiro Taniguchi	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633912	unsupervised learning;robust statistics;computer vision;impedance matching;motion capture;human body;pose;computer science;artificial intelligence;machine learning;motion estimation;estimation theory;feature;artificial neural network;statistics;robustness	Vision	35.39069495760873	-43.341765893810084	75195
43ae7d15a9e6e68ac86b8b183c23c9730ae762c5	improving clustering by learning a bi-stochastic data similarity matrix	optimal solution;cluster algorithm;stochastic matrix;euclidean distance;general methods;adjacency matrix;stochastic matrices;k means clustering;normalized cut;kullback leibler	An idealized clustering algorithm seeks to learn a cluster-adjacency matrix such that, if two data points belong to the same cluster, the corresponding entry would be 1; otherwise, the entry would be 0. This integer (1/0) constraint makes it difficult to find the optimal solution. We propose a relaxation on the cluster-adjacency matrix, by deriving a bi-stochastic matrix from a data similarity (e.g., kernel) matrix according to the Bregman divergence. Our general method is named the Bregmanian Bi-Stochastication (BBS) algorithm. We focus on two popular choices of the Bregman divergence: the Euclidean distance and the Kullback–Leibler (KL) divergence. Interestingly, the BBS algorithm using the KL divergence is equivalent to the Sinkhorn–Knopp (SK) algorithm for deriving bi-stochastic matrices. We show that the BBS algorithm using the Euclidean distance is closely related to the relaxed k-means clustering and can often produce noticeably superior clustering results to the SK algorithm (and other algorithms such as Normalized Cut), through extensive experiments on public data sets.	adjacency matrix;algorithm;bregman divergence;cluster analysis;data point;division by zero;euclidean distance;experiment;information privacy;k-means clustering;kullback–leibler divergence;linear programming relaxation;ski combinator calculus;similarity measure;stochastic matrix	Fei Wang;Ping Li;Arnd Christian König;Muting Wan	2011	Knowledge and Information Systems	10.1007/s10115-011-0433-1	cuthill–mckee algorithm;mathematical optimization;combinatorics;distance matrix;eight-point algorithm;k-medians clustering;fuzzy clustering;canopy clustering algorithm;machine learning;euclidean distance;stochastic matrix;mathematics;euclidean distance matrix;cluster analysis;kullback–leibler divergence;k-medoids;spectral clustering;adjacency matrix;statistics;k-means clustering	ML	26.971893062403286	-38.7930447727534	75254
0d332e51593918b38c596002889588c4316abc80	cross- view gait recognition using non-linear view transformations of spatiotemporal features		This paper presents a novel cross-view gait recognition technique based on the spatiotemporal characteristics of human motion. We propose a deep fully-connected neural network with unsupervised learning which transfers the gait descriptors from multiple views to the single canonical view. The proposed non-linear network learns a single model for all videos captured from different viewpoints and finds a shared high-level virtual path to map them on a single canonical view. Therefore, the model does not require any labels or viewpoint information in the learning phase. The network is learned only once using the spatiotemporal motion features of the gait sequences from several viewpoints, later it is used to construct the cross-view gait descriptors for the gallery and the probe sets. The descriptors are classified using simple linear support vector machine. Experiments carried out on the benchmark cross-view gait dataset, CASIA-B, and comparisons with the state-of-the-art demonstrate that the proposed method outperforms the existing cross-view gait recognition algorithms.		Muhammad Hassan Khan;Muhammad Shahid Farid;Maryiam Zahoor;Marcin Grzegorzek	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451629	unsupervised learning;gait;artificial intelligence;support vector machine;artificial neural network;pattern recognition;nonlinear system;solid modeling;computer science	Vision	28.693181149728066	-51.52803600137482	75362
2fe7105ef8e61330a3ddc7f7b35955ca62fc1ab3	unifying identification and context learning for person recognition		Despite the great success of face recognition techniques, recognizing persons under unconstrained settings remains challenging. Issues like profile views, unfavorable lighting, and occlusions can cause substantial difficulties. Previous works have attempted to tackle this problem by exploiting the context, e.g. clothes and social relations. While showing promising improvement, they are usually limited in two important aspects, relying on simple heuristics to combine different cues and separating the construction of context from people identities. In this work, we aim to move beyond such limitations and propose a new framework to leverage context for person recognition. In particular, we propose a Region Attention Network, which is learned to adaptively combine visual cues with instance-dependent weights. We also develop a unified formulation, where the social contexts are learned along with the reasoning of people identities. These models substantially improve the robustness when working with the complex contextual relations in unconstrained environments. On two large datasets, PIPA [27] and Cast In Movies (CIM), a new dataset proposed in this work, our method consistently achieves state-of-the-art performance under multiple evaluation policies.		Qingqiu Huang;Yu Xiong;Dahua Lin	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00236	artificial intelligence;robustness (computer science);task analysis;machine learning;sensory cue;social relation;visualization;facial recognition system;computer science;context model;heuristics	Vision	28.51107726496467	-47.95935501718483	75386
b5e87fa2b809b435b7030628e6af4d0624da722d	spectral image classification from optimal coded-aperture compressive measurements	geophysical image processing;sparsity classification coded aperture coded aperture snapshot spectral imaging cassi hyperspectral imagery principal component analysis pca restricted isometry property rip;image coding;image classification;apertures vectors training sensors image coding dictionaries principal component analysis;image coding feature extraction geophysical image processing image classification;feature extraction;traditional aperture codes spectral image classification optimal coded aperture compressive measurements hyperspectral imaging sensors high dimensional data objects discrimination feature discrimination coded aperture snapshot spectral imaging cassi system compressive spectral image data coded focal plane array measurements cassi compressive measurements classification method test pixel sparsity constrained optimization problem optical system cassi sensing matrix restricted isometry property	Traditional hyperspectral imaging sensors acquire high-dimensional data that are used for the discrimination of objects and features in a scene. Recently, a novel architecture known as the coded-aperture snapshot spectral imaging (CASSI) system has been developed for the acquisition of compressive spectral image data with just a few coded focal plane array measurements. This paper focuses on developing a classification approach with hyperspectral images directly from CASSI compressive measurements, without first reconstructing the full data cube. The proposed classification method uses the compressive measurements to find the sparse vector representation of the test pixel in a given training dictionary. The estimated sparse vector is obtained by solving a sparsity-constrained optimization problem and is then used to directly determine the class of the unknown pixel. The performance of the proposed classifier is improved by taking optimal CASSI compressive measurements obtained when optimal coded apertures are used in the optical system. The set of optimal coded apertures is designed such that the CASSI sensing matrix satisfies a restricted isometry property with high probability. Several simulations illustrate the performance of the proposed classifier using optimal coded apertures and the gain in the classification accuracy obtained over using traditional aperture codes in CASSI.	code;coded aperture;collective knowledge (ck);column (database);constrained optimization;constraint (mathematics);data cube;dictionary;focal (programming language);machine learning;mathematical optimization;new general catalogue;optimization problem;pixel;restricted isometry property;sensor;simulation;snapshot (computer storage);sparse approximation;sparse matrix;staring array;support vector machine;the matrix;with high probability	Ana B. Ramirez;Henry Arguello;Gonzalo R. Arce;Brian M. Sadler	2014	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2013.2272378	computer vision;contextual image classification;feature extraction;machine learning;pattern recognition;mathematics;remote sensing	Vision	30.13721111679585	-43.331582632953534	75474
19be4580df2e76b70a39af6e749bf189e1ca3975	adversarial binary coding for efficient person re-identification		Person re-identification (ReID) aims at matching persons across different views/scenes. In addition to accuracy, the matching efficiency has received more and more attention because of demanding applications using large-scale data. Several binary coding based methods have been proposed for efficient ReID, which either learn projections to map high-dimensional features to compact binary codes, or directly adopt deep neural networks by simply inserting an additional fully-connected layer with tanh-like activations. However, the former approach requires time-consuming hand-crafted feature extraction and complicated (discrete) optimizations; the latter lacks the necessary discriminative information greatly due to the straightforward activation functions. In this paper, we propose a simple yet effective framework for efficient ReID inspired by the recent advances in adversarial learning. Specifically, instead of learning explicit projections or adding fully-connected mapping layers, the proposed Adversarial Binary Coding (ABC) framework guides the extraction of binary codes implicitly and effectively. The discriminability of the extracted codes is further enhanced by equipping the ABC with a deep triplet network for the ReID task. More importantly, the ABC and triplet network are simultaneously optimized in an end-to-end manner. Extensive experiments on three large-scale ReID benchmarks demonstrate the superiority of our approach over the state-of-the-art methods.	activation function;algorithmic efficiency;artificial neural network;binary code;binary number;deep learning;discriminator;embedded system;end-to-end principle;experiment;feature extraction;randomness extractor;triplet state;while	Zheng Liu;Jie Qin;Annan Li;Yunhong Wang;Luc Van Gool	2018	CoRR		binary code;machine learning;adversarial system;pattern recognition;artificial intelligence;discriminative model;artificial neural network;computer science;feature extraction	AI	25.158728068127957	-49.8223506409169	75594
d88c4e9cef7bb982bdf0cecd95eaf48b8b43a9a0	online cnn-based multiple object tracking with enhanced model updates and identity association		Abstract Online multiple objects tracking (MOT) is a challenging problem due to occlusions and interactions among targets. An online MOT method with enhanced model updates and identity association is presented to handle the error drift and the identity switch problems in this work. The proposed MOT system consists of multiple single CNN(Convolutional Neural Networks)-based object trackers, where the shared CONV layers are fixed and used to extract the appearance representation while target-specific FC layers are updated online to distinguish the target from background. Two model updates are developed to build an accurate tracker. When a target is visible and with smooth movement, we perform the incremental update based on its recent appearance. When a target experiences error drifting due to occlusion, we conduct the refresh update to clear all previous memory of the target. Moreover, we introduce an enhanced online ID assignment scheme based on multi-level features to confirm the trajectory of each target. Experimental results demonstrate that the proposed online MOT method outperforms other existing online methods against the MOT17 and MOT16 benchmark datasets and achieves the best performance in terms of ID association.		Weihao Gan;Shuo Wang;Xuejing Lei;Ming-Sui Lee;C.-C. Jay Kuo	2018	Sig. Proc.: Image Comm.	10.1016/j.image.2018.05.008	convolutional neural network;computer vision;artificial intelligence;video tracking;machine learning;computer science;bittorrent tracker;trajectory	Vision	33.688075811481845	-48.135053787271076	75770
fedc150a322390a10e99c2fa9bfbb590c8c176b2	semi-supervised dimension reduction with kernel sliced inverse regression		This study is an attempt to draw on research of semi-supervised dimension reduction. Many real world problems can be formulated as semi-supervised problems since the data labeling is much more challenging to obtain than the unlabeled data. Dimension reduction benefits the computation performance and is usually applied in the problem with high dimensional data. This paper proposes a semi-supervised dimension reduction achieved with the kernel sliced inverse regression (KSIR). The prior information is applied to estimate the statistical parameters in the KSIR formula. The semi-supervised KSIR performs comparably to other established methods but much more efficient.	dimensionality reduction;kernel (operating system);semiconductor industry;sliced inverse regression	Chiao-Ching Huang;Kuan-Ying Su	2014		10.1007/978-3-319-13987-6_17	principal component regression;sliced inverse regression	ML	24.75475080688107	-38.16782217168218	76024
7d5add846a6c8e75f5187857254460d63e4c9ac5	a surf-based spatio-temporal feature for feature-fusion-based action recognition	surf-based spatio-temporal feature;kth dataset;proposed spatio-temporal feature;action recognition performance;youtube datasets;motion feature;holistic appearance feature;local spatio-temporal feature;youtube dataset;novel spatio-temporal feature;feature-fusion-based action recognition	In this paper, we propose a novel spatio-temporal feature which is useful for feature-fusion-based action recognition with Multiple Kernel Learning (MKL). The proposed spatio-temporal feature is based on moving SURF interest points grouped by Delaunay triangulation and on their motion over time. Since this local spatio-temporal feature has different characteristics from holistic appearance features and motion features, it can boost action recognition performance for both controlled videos such as the KTH dataset and uncontrolled videos such as Youtube datasets, by combining it with visual and motion features with MKL. In the experiments, we evaluate our method using KTH dataset, and Youtube dataset. As a result, we obtain 94.5% as a classification rate for in KTH dataset which is almost equivalent to state-of-art, and 80.4% for Youtube dataset which outperforms state-of-the-art greatly.	delaunay triangulation;experiment;holism;kernel (operating system);math kernel library;motion compensation;multiple kernel learning;speeded up robust features;uncontrolled format string;video clip	Akitsugu Noguchi;Keiji Yanai	2010		10.1007/978-3-642-35749-7_12	computer vision;computer science;pattern recognition;data mining	Vision	35.80105365441772	-50.79152432339874	76110
6e972ff2b8baaba1d655d2832ae171d48ce58ded	recognizing human activity in still images by integrating group-based contextual cues	group based cue;focal subspace measurement;context;fusion rbm;conference proceeding;activity recognition	Images with wider angles usually capture more persons in wider scenes, and recognizing individuals' activities in these images based on existing contextual cues usually meet difficulties. We instead construct a novel group-based cue to utilize the context carried by suitable surrounding persons. We propose a global-local cue integration model (GLCIM) to find a suitable group of local cues extracted from individuals and form a corresponding global cue. A fusion restricted Boltzmann machine, a focal subspace measurement and a cue integration algorithm based on entropy are proposed to enable the GLCIM to integrate most of the relevant local cues and least of the irrelevant ones into the group. Our experiments demonstrate how integrating group-based cues improves the activity recognition accuracies in detail and show that all of the key parts of GLCIM make positive contributions to the increases of the accuracies.	activity recognition;algorithm;contextual inquiry;experiment;focal (programming language);relevance;restricted boltzmann machine	Zheng Zhou;Kan Li;Xiangjian He	2015		10.1145/2733373.2806300	computer vision;speech recognition;computer science;activity recognition	Vision	34.43060660398414	-50.71655998694714	76123
0e9eb3a501f6178b8b536e97a466f29d04d8730d	multivariate data mapping based on dendritic lattice associative memories		We describe a dendritic lattice hetero-associative memory (DLHAM) that performs multivariate numerical data mapping with respect to a set of prototype data vectors selected by diverse objective or subjective criteria. The memory is a feedforward four-layer dendritic neural network based on lattice algebra operations that computes the nearest match between input and prototype data vectors. Our approach shows the inherent capability of n-dimensional vector association to realize coarse or fine data mapping that is computationally simple. Specifically, we apply the DLHAM in a two stage algorithm to the quantization and transfer of Red-Green-Blue (RGB) color coded images. Input color pixels are first quantized and then the resulting representative colors are mapped to another set of palette colors by hetero-association. Examples and quantization error are included to show the DLHAM performance.	algorithm;artificial neural network;color depth;color mapping;color space;computation;content-addressable memory;feedforward neural network;iteration;level of measurement;numerical analysis;palette (computing);pixel;prototype;quantization (signal processing);taxicab geometry	Gonzalo Urcid;Rocio Morales-Salgado;Gerhard X. Ritter	2017	2017 IEEE Latin American Conference on Computational Intelligence (LA-CCI)	10.1109/LA-CCI.2017.8285687	pixel;data mapping;quantization (signal processing);lattice (order);artificial neural network;associative property;multivariate statistics;rgb color model;mathematics;artificial intelligence;pattern recognition	Robotics	34.05328445284823	-39.64373958094406	76283
bb030eaf7c25953369ee111dc1555f4f85409bb4	scenarios: a new representation for complex scene understanding		The ability for computational agents to reason about the high-level content of real world scene images is important for many applications. Existing attempts at addressing the problem of complex scene understanding lack representational power, efficiency, and the ability to create robust metaknowledge about scenes. In this paper, we introduce scenarios as a new way of representing scenes. The scenario is a simple, low-dimensional, data-driven representation consisting of sets of frequently co-occurring objects and is useful for a wide range of scene understanding tasks. We learn scenarios from data using a novel matrix factorization method which we integrate into a new neural network architecture, the ScenarioNet. Using ScenarioNet, we can recover semantic information about real world scene images at three levels of granularity: 1) scene categories, 2) scenarios, and 3) objects. Training a single ScenarioNet model enables us to perform scene classification, scenario recognition, multi-object recognition, content-based scene image retrieval, and content-based image comparison. In addition to solving many tasks in a single, unified framework, ScenarioNet is more computationally efficient than other CNNs because it requires significantly fewer parameters while achieving similar performance on benchmark tasks and is more interpretable because it produces explanations when making decisions. We validate the utility of scenarios and ScenarioNet on a diverse set of scene understanding tasks on several benchmark datasets.	algorithmic efficiency;artificial neural network;benchmark (computing);computation;experiment;high- and low-level;image retrieval;network architecture;object-based language;outline of object recognition;statistical classification;unified framework	Zachary A. Daniels;Dimitris N. Metaxas	2018	CoRR		artificial intelligence;machine learning;computer science;artificial neural network;granularity;architecture;image retrieval;matrix decomposition	Vision	27.25024386924202	-49.167105478810846	76333
15862b309fb40c994429e0b981727db1b886288a	multi-approach satellite images fusion based on blind sources separation	decision support;neural networks;blind source separation;image interpretation;remote sensing;intelligent information retrieval;data imperfection;satellite image;artificial intelligence;learning and adaptation;case based reasoning;images fusion;land cover detection	The development of satellite image acquisition tools helped improving the extraction of information about natural scenes. In the proposed approach, we try to minimize imperfections accompanying the image interpretation process and to maximize useful information extracted from these images through the use of blind source separation (BSS) and fusion methods. In order to extract maximum information from multi-sensor images, we propose to use three algorithms of BSS that are FAST- ICA2D, JADE2D, and SOBI2D. Then by employing various fusion methods such as the probability, possibility, and evidence methods we can minimize both imprecision and uncertainty. In this paper, we propose a hybrid approach based on five main steps. The first step is to apply the three BSS algorithms to the satellites images; it results in obtaining a set of image sources representing each a facet of the land cover. A second step is to choose the image having the maximum of kurtosis and negentropy. After the BSS evaluation, we proceed to the training step using neural networks. The goal of this step is to provide learning regions which are useful for the fusion step. The next step consists in choosing the best adapted fusion method for the selected source images through a case-based reasoning (CBR) module. If the CBR module does not contain a case similar to the one we are seeking, we proceed to apply the three fusion methods. The evaluation of fusion methods is a necessary step for the learning process of our CBR.		Wadii Boulila;Imed Riadh Farah	2011	Int. J. Image Graphics	10.1142/S0219467811004020	case-based reasoning;computer vision;decision support system;computer science;artificial intelligence;machine learning;blind signal separation;artificial neural network	Graphics	33.06544119813948	-44.1519793757142	76675
6197e7aad7aeebfcca8601162986252e07c7a1de	classification of hyperspectral images using subspace projection feature space	support vector machines;training;support vector machines feature extraction geophysical image processing hyperspectral imaging image classification;accuracy;feature extraction;subspace based classification method feature reduction fr hyperspectral image classification maximum likelihood classifier mlc;training support vector machines accuracy hyperspectral imaging feature extraction;hyperspectral imaging;support vector machine classifier hyperspectral image classification subspace projection feature space feature reduction technique subspace based classifier high dimensional space characteristics high hyperspectral image dimension subspace based classification approach	A concern in hyperspectral image classification is the high number of required training samples. When traditional classifiers are applied, feature reduction (FR) techniques are the most common approaches to deal with this problem. Subspace-based classifiers, which are developed based on high-dimensional space characteristics, are another way to handle the high dimension of hyperspectral images. In this letter, a novel subspace-based classification approach is proposed and compared with basic and improved subspace-based classifiers. The proposed classifier is also compared with traditional classifiers that are accompanied by an FR technique and the well-known support vector machine classifier. Experimental results prove the efficiency of the proposed method, especially when a limited number of training samples are available. Furthermore, the proposed method has a very high level of automation and simplicity, as it has no parameters to be set.	computation;computer vision;feature vector;high-level programming language;multi-level cell;point of view (computer hardware company);preprocessor;support vector machine;whole earth 'lectronic link	Reza Aghaee;Mehdi Mokhtarzade	2015	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2015.2424911	random subspace method;support vector machine;computer vision;feature extraction;computer science;hyperspectral imaging;machine learning;linear classifier;pattern recognition;mathematics;accuracy and precision;remote sensing	Vision	30.31225078133119	-43.112466461109044	76684
46e0703044811c941f0b5418139f89d46b360aa3	random forest classifier for zero-shot learning based on relative attribute		For the zero-shot image classification with relative attributes (RAs), the traditional method requires that not only all seen and unseen images obey Gaussian distribution, but also the classifications on testing samples are made by maximum likelihood estimation. We therefore propose a novel zero-shot image classifier called random forest based on relative attribute. First, based on the ordered and unordered pairs of images from the seen classes, the idea of ranking support vector machine is used to learn ranking functions for attributes. Then, according to the relative relationship between seen and unseen classes, the RA ranking-score model per attribute for each unseen image is built, where the appropriate seen classes are automatically selected to participate in the modeling process. In the third step, the random forest classifier is trained based on the RA ranking scores of attributes for all seen and unseen images. Finally, the class labels of testing images can be predicted via the trained RF. Experiments on Outdoor Scene Recognition, Pub Fig, and Shoes data sets show that our proposed method is superior to several state-of-the-art methods in terms of classification capability for zero-shot learning problems.	arabic numeral 0;business architecture;class;classification;computer vision;high- and low-level;information;physical object;problem-based learning;radio frequency;random forest;rheumatoid arthritis;shoes;support vector machine;traffic collision avoidance system	Yuhu Cheng;Xue Qiao;Xuesong Wang;Qiang Yu	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2017.2677441	quadratic classifier;machine learning;pattern recognition;data mining	Vision	30.293405255472564	-47.69969211683876	76747
b54a5c263068a58834c1214f89f7f8ad35d616de	parts-based face super-resolution via non-negative matrix factorization		Display Omitted We propose a novel parts-based face super-resolution method using non-negative matrix factorization.We propose a global face hallucination method using non-negative matrix factorization and canonical correlation analysis.We propose a residue compensation method based on High-dimensional Coupled non-negative matrix factorization. Face super-resolution refers to inferring the high-resolution face image from its low-resolution one. In this paper, we propose a parts-based face hallucination framework which consists of global face reconstruction and residue compensation. In the first phase, correlation-constrained non-negative matrix factorization (CCNMF) algorithm combines non-negative matrix factorization and canonical correlation analysis to hallucinate the global high-resolution face. In the second phase, the High-dimensional Coupled NMF (HCNMF) algorithm is used to compensate the error residue in hallucinated images. The proposed CCNMF algorithm can generate global face more similar to the ground truth face by learning a parts-based local representation of facial images; while the HCNMF can learn the relation between high-resolution residue and low-resolution residue to better preserve high frequency details. The experimental results validate the effectiveness of our method.	non-negative matrix factorization;super-resolution imaging	Xiaofeng Wang;Hefei Ling;Xin Xu	2014	Computers & Electrical Engineering	10.1016/j.compeleceng.2014.04.016	computer vision;machine learning;pattern recognition;mathematics	Vision	28.590665241190724	-45.50064492619272	76760
a1cda8e30ce35445e4f51b47ab65b775f75c9f18	normalized face image generation with perceptron generative adversarial networks		This paper presents a deep neural architecture for synthesizing the frontal and neutral facial expression image of a subject given a query face image with arbitrary expression. This is achieved by introducing a combination of feature space perceptual loss, pixel-level loss, adversarial loss, symmetry loss, and identity-preserving loss. We leverage both the frontal and neutral face distributions and pre-trained discriminative deep perceptron models to guide the identity-preserving inference of the normalized views from expressive profiles. Unlike previous generative methods that utilize their intermediate features for the recognition tasks, the resulting expression- and pose-disentangledface image has potential for several downstream applications, such as facial expression or face recognition, and attribute estimation. We show that our approach produces photorealistic and coherent results, which assist the deep metric learning-based facial expression recognition (FER) to achieve promising results on two well-known FER datasets.	coherence (physics);downstream (software development);facial recognition system;feature vector;generative adversarial networks;glossary of computer graphics;interpolation;perceptron;pixel	Xiaofeng Liu;B. V. K. Vijaya Kumar;Yubin Ge;Chao Yang;Jane You;Ping Jia	2018	2018 IEEE 4th International Conference on Identity, Security, and Behavior Analysis (ISBA)	10.1109/ISBA.2018.8311462	architecture;perceptron;discriminative model;feature vector;decoding methods;hidden markov model;facial recognition system;facial expression;artificial intelligence;computer science;pattern recognition	Vision	25.216495346481267	-50.03323105682876	76806
84383e2fff99804b92d75d166b952628e26e8cfd	real-time online tracking via a convolution-based complementary model		Several object tracking convolution networks have been proposed in recent years. Despite their favorable performance, the balancing of tracking accuracy and efficiency remains challenging. In this paper, we propose a real-time online tracking method based on complementary tracking models: the convolution-based discriminative model (CDM) that can predict the center location of an object and the convolution-based generative model (CGM) that estimates the scale of the target. In the CDM model, we leverage a simple convolution operation to model the correlation between the apparent features (gradient and color features) of the object and its background. Then, the center location is predicted by maximizing the response value of the convolution. In the CGM model, a two-layer convolution network is proposed to learn geometric structural information, and the target scale is estimated by selecting the best candidate extracted from the foreground of the target through the observation model. Moreover, online updating and the fast Fourier transform are adopted for fast learning and detection. Despite its surprisingly lightweight structure, the proposed tracker performs favorably against the state-of-the-art methods in terms of efficiency, accuracy, and robustness on the CVPR2013 tracking benchmark data set.	benchmark (computing);conceptual schema;convolution;discriminative model;fast fourier transform;generative model;gradient;online algorithm;real-time clock;real-time transcription	Xu Qi;Wang Huabin;Zhou Jian;Tao Liang	2018	IEEE Access	10.1109/ACCESS.2018.2841030	fast fourier transform;discriminative model;robustness (computer science);video tracking;generative model;feature extraction;distributed computing;convolution;computer science;artificial intelligence;pattern recognition	Vision	30.990038496292915	-49.882750538683325	76844
77037a22c9b8169930d74d2ce6f50f1a999c1221	robust face recognition with kernelized locality-sensitive group sparsity representation		In this paper, a novel joint sparse representation method is proposed for robust face recognition. We embed both group sparsity and kernelized locality-sensitive constraints into the framework of sparse representation. The group sparsity constraint is designed to utilize the grouped structure information in the training data. The local similarity between test and training data is measured in the kernel space instead of the Euclidian space. As a result, the embedded nonlinear information can be effectively captured, leading to a more discriminative representation. We show that, by integrating the kernelized local-sensitivity constraint and the group sparsity constraint, the embedded structure information can be better explored, and significant performance improvement can be achieved. On the one hand, experiments on the ORL, AR, extended Yale B, and LFW data sets verify the superiority of our method. On the other hand, experiments on two unconstrained data sets, the LFW and the IJB-A, show that the utilization of sparsity can improve recognition performance, especially on the data sets with large pose variation.	dictionary [publication type];embedded system;embedding;experiment;facial recognition system;kernel method;locality of reference;locality-sensitive hashing;nonlinear system;rps6kb2 gene;return loss;sparse approximation;sparse matrix;test data;user space	Shoubiao Tan;Xi Sun;Wentao Chan;Lei Qu;Ling Shao	2017	IEEE Transactions on Image Processing	10.1109/TIP.2017.2716180	euclidean space;facial recognition system;robustness (computer science);iterative reconstruction;kernel (linear algebra);discriminative model;pattern recognition;machine learning;mathematics;sparse approximation;data set;artificial intelligence	Vision	25.710777072141838	-42.89815929897104	76846
b95bb11b66f0440cc7538ae457ecc8282c83e50b	deep-see: joint object detection, tracking and recognition with application to visually impaired navigational assistance	convolutional neural networks;object detection;tracking and recognition;visually impaired users;wearable assistive device	In this paper, we introduce the so-called DEEP-SEE framework that jointly exploits computer vision algorithms and deep convolutional neural networks (CNNs) to detect, track and recognize in real time objects encountered during navigation in the outdoor environment. A first feature concerns an object detection technique designed to localize both static and dynamic objects without any a priori knowledge about their position, type or shape. The methodological core of the proposed approach relies on a novel object tracking method based on two convolutional neural networks trained offline. The key principle consists of alternating between tracking using motion information and predicting the object location in time based on visual similarity. The validation of the tracking technique is performed on standard benchmark VOT datasets, and shows that the proposed approach returns state-of-the-art results while minimizing the computational complexity. Then, the DEEP-SEE framework is integrated into a novel assistive device, designed to improve cognition of VI people and to increase their safety when navigating in crowded urban scenes. The validation of our assistive device is performed on a video dataset with 30 elements acquired with the help of VI users. The proposed system shows high accuracy (>90%) and robustness (>90%) scores regardless on the scene dynamics.	acoustic cryptanalysis;algorithm;artificial neural network;assistive technology;audio feedback;behavior;benchmark (computing);bone tissue;cautionary warning;cognition;computational complexity theory;computer vision;conflict (psychology);convolutional neural network;detectors;estimated;experiment;f1 score;facial recognition system;haptic device component;headphones;interpretation (logic);line code;manuscripts;navigation;object detection;online and offline;performance;physical object;prototype;real life;self-help devices;sensor;sensorineural hearing loss (disorder);silo (dataset);situated;source-to-source compiler;tracking system;visually impaired persons;disease transmission;message	Ruxandra Tapu;Bogdan Cosmin Mocanu;Titus B. Zaharia	2017		10.3390/s17112473	convolutional neural network;robustness (computer science);video tracking;computational complexity theory;a priori and a posteriori;object detection;computer vision;exploit;artificial intelligence;computer science	Vision	32.291011358652256	-50.865506457408735	77285
e4b921ac6e5859e348d66688e70c00889fe448ed	unsupervised feature selection with local structure learning		Conventional graph-based unsupervised feature selection approaches carry out the feature selection requiring two stages: first, constructing the data similarity matrix and next performing feature selection. In this way, the similarity matrix is invariably kept unchanged, totally separated from the process of feature selection and the performance of feature selection highly depends on the initially constructed similarity matrix. In order to address this problem, a novel unsupervised feature selection method is proposed in this paper where constructing similarity matrix and performing feature selection are together incorporated into a coherent model. Besides, the constructed similarity matrix has $k$ connected components ($k$ is the number of data clusters). At last, five state-of-the-art unsupervised feature selection methods are compared to validate the effectiveness of the proposed method.	coherence (physics);connected component (graph theory);feature selection;similarity measure;unsupervised learning	Sheng Yang;Feiping Nie;Xuelong Li	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451101	optical imaging;feature extraction;artificial intelligence;sparse matrix;feature selection;pattern recognition;data structure;connected component;graph;computer science;similarity matrix	Robotics	28.62344308004887	-43.09586758368864	77445
e6004e86e34a274d73fb100f09238bf90fca7904	multilevel cloud detection in remote sensing images based on deep learning		Cloud detection is one of the important tasks for remote sensing image processing. In this paper, a novel multilevel cloud detection method based on deep learning is proposed for remote sensing images. First, the simple linear iterative clustering (SLIC) method is improved to segment the image into good quality superpixels. Then, a deep convolutional neural network (CNN) with two branches is designed to extract the multiscale features from each superpixel and predict the superpixel as one of three classes including thick cloud, thin cloud, and noncloud. Finally, the predictions of all the superpixels in the image yield the cloud detection result. In the proposed cloud detection framework, the improved SLIC method can obtain accurate cloud boundaries by optimizing initial cluster centers, designing dynamic distance measure, and expanding search space. Moreover, different from traditional cloud detection methods that cannot achieve multilevel detection of cloud, the designed deep CNN model can not only detect cloud but also distinguish thin cloud from thick cloud. Experimental results indicate that the proposed method can detect cloud with higher accuracy and robustness than compared methods.	artificial neural network;biological neural networks;class;cluster analysis;convolutional neural network;deep learning;image processing;iterative method;robustness (computer science);statistical cluster	Fengying Xie;Mengyun Shi;Zhenwei Shi;Jihao Yin;Danpei Zhao	2017	IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing	10.1109/JSTARS.2017.2686488	computer vision;remote sensing	Mobile	32.643093969488284	-45.11064753927344	77520
220f8088f2fc1ddd9df1a0b583d3d01cb929ee8d	roml: a robust feature correspondence approach for matching objects in a set of images	object matching;sparsity;feature correspondence;low rank	Feature-based object matching is a fundamental problem for many applications in computer vision, such as object recognition, 3D reconstruction, tracking, and motion segmentation. In this work, we consider simultaneously matching object instances in a set of images, where both inlier and outlier features are extracted. The task is to identify the inlier features and establish their consistent correspondences across the image set. This is a challenging combinatorial problem, and the problem complexity grows exponentially with the image number. To this end, we propose a novel framework, termed Robust Object Matching using Low-rank constraint (ROML), to address this problem. ROML optimizes simultaneously a partial permutation matrix (PPM) for each image, and feature correspondences are established by the obtained PPMs. Two of our key contributions are summarized as follows. (1) We formulate the problem as rank and sparsity minimization for PPM optimization, and treat simultaneous optimization of multiple PPMs as a regularized consensus problem in the context of distributed optimization. (2) We use the alternating direction method of multipliers method to solve the thus formulated ROML problem, in which a subproblem associated with a single PPM optimization appears to be a difficult integer quadratic program (IQP). We prove that under wildly applicable conditions, this IQP is equivalent to a linear sum assignment problem, which can be efficiently solved to an exact solution. Extensive experiments on rigid/non-rigid object matching, matching instances of a common object category, and common object localization show the efficacy of our proposed method.	3d reconstruction;approximation algorithm;assignment problem;augmented lagrangian method;computation;computer vision;consensus (computer science);convex optimization;experiment;hungarian algorithm;instance (computer science);mathematical optimization;online algorithm;outline of object recognition;perl package manager;power iteration;qr decomposition;quadratic programming;rate of convergence;scalability;singular value decomposition;sparse matrix;time complexity;unsupervised learning	Kui Jia;Tsung-Han Chan;Zinan Zeng;Shenghua Gao;Gang Wang;Tianzhu Zhang;Yi Ma	2015	International Journal of Computer Vision	10.1007/s11263-015-0858-1	computer vision;mathematical optimization;3-dimensional matching;machine learning;pattern recognition;mathematics;sparsity-of-effects principle;statistics	Vision	33.632341570713784	-40.97539904715611	77573
055a45d1ad77cb21e0ee3a1b532a9fbeaaa16dd4	hnsr: highway networks based deep convolutional neural networks model for single image super-resolution		Convolutional neural networks (CNNs) have been widely used in computer vision community. Single image super-resolution (SISR) is a classic computer vision problem, which aims to output a high-resolution image from a low-resolution one. In recent years, CNNs-based SISR methods emerged and achieved a performance leap. In this paper, we present a highly accurate deep CNNs model for SISR. Inspired by the ideas in highway networks, we propose a highway unit and cascade highway units to ensemble our model. Furthermore, we employ structural similarity index (SSIM) as a part of loss function to enhance the accuracy of trained deep CNNs model. Experimental results show that our proposed model outperforms other state-of-the-art methods.	artificial neural network;computer vision;convolutional neural network;image resolution;loss function;neural networks;structural similarity;super-resolution imaging	Ke Li;Bahetiyaer Bare;Bo Yan;Bailan Feng;Chunfeng Yao	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462007	convolutional neural network;structural similarity;superresolution;convolution;pattern recognition;machine learning;artificial intelligence;computer science;image resolution	Vision	24.7130606116639	-51.942039660212366	77877
0485927affa841c0c4396647e27044e3471c662a	dense spatial translation network		Neural networks are widely used in autonomous driving and driver assistance systems tasks. Limited by hardware, these networks are restricted by their capacity and capability. To deal with this limitation, an application dedicated unit which exploits prior knowledge on beneficial steps may reduce the required network complexity. We introduce a neuralnetwork-integrable unit, Dense Spatial Translation Network (DSTN), that compensates for complex intra-class variations in spatial appearance. For example, considering Traffic Sign Recognition (TSR), the design of the same traffic sign in different countries may be different. This efficient unit is explicitly designed for this rectification task and thus replaces the demand to substantially increase the network capacity. It samples input feature maps which are augmented by intra-class variations, and produces output feature maps compensating for these variations. This clearly simplifies the subsequent classification tasks. Also, the DSTN is light-weighted, and is suitable for end-to-end training. It is easily integrated into any existing network structure. We evaluate the performance of the unit based on TSR and number recognition. Results show significant improvement after integrating this unit into a neural network.	architecture design and assessment system;autonomous car;autonomous robot;computational resource;convolutional neural network;dual scan;embedded system;end-to-end principle;gradient;login;machine learning;map;neural networks;pixel;rectifier;sampling (signal processing);traffic sign recognition	Weimeng Zhu;Jan Siegemund;Anton Kummert	2018	2018 IEEE International Conference on Vehicular Electronics and Safety (ICVES)	10.1109/ICVES.2018.8519518		Vision	28.530078823616844	-50.525321130664096	77883
4fc7d7776ee67c2cc1bc15babc4cde2994b601c1	spatially aware dictionary learning and coding for fossil pollen identification		We propose a robust approach for performing automatic species-level recognition of fossil pollen grains in microscopy images that exploits both global shape and local texture characteristics in a patch-based matching methodology. We introduce a novel criteria for selecting meaningful and discriminative exemplar patches. We optimize this function during training using a greedy submodular function optimization framework that gives a near-optimal solution with bounded approximation error. We use these selected exemplars as a dictionary basis and propose a spatially-aware sparse coding method to match testing images for identification while maintaining global shape correspondence. To accelerate the coding process for fast matching, we introduce a relaxed form that uses spatiallyaware soft-thresholding during coding. Finally, we carry out an experimental study that demonstrates the effectiveness and efficiency of our exemplar selection and classification mechanisms, achieving 86.13% accuracy on a difficult fine-grained species classification task distinguishing three types of fossil spruce pollen 1.	approximation error;dictionary;experiment;fossil;greedy algorithm;machine learning;mathematical optimization;neural coding;pollen;sparse matrix;submodular set function;thresholding (image processing)	Shu Kong;Surangi Punyasena;Charless C. Fowlkes	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2016.165	computer vision;machine learning;pattern recognition	Vision	26.346294267420507	-45.16171535488606	77886
4faf736de2b798afed30c3f35c53b88ae36745e3	hyperspectral anomaly detection using compressed columnwise robust principal component analysis		This paper proposes a compressed columnwise robust principal component analysis (CCRPCA) method for hyperspectral anomaly detection. The CCRPCA improves the regular RPCA by using the Hadamard random projection and constraining the columnwise structure of sparse anomaly matrix. The Hadamard random projection reduces the computational cost of the hyperspectral data, and the columnwise sparse structure alleviates negative effects from the anomalies on the columns of the background. The sparse anomaly matrix and the background matrix are estimated by optimizing a convex program, and the anomalies are estimated from nonzero columns of the compressed sparse matrix. Preliminary experiment result from the San Diego dataset shows that the CCRPCA outperforms four state-of-the-art detection methods in both the receiver operating characteristic curve and the area under curve.	anomaly detection;column (database);computational complexity theory;convex optimization;random projection;receiver operating characteristic;robust principal component analysis;sparse matrix	Weiwei Sun;Gang Yang;Jialin Li;Dianfa Zhang	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518817	anomaly detection;computer vision;sparse matrix;robust principal component analysis;principal component analysis;hadamard transform;computer science;matrix decomposition;matrix (mathematics);pattern recognition;random projection;artificial intelligence	Vision	26.063144380632703	-39.85767704491255	78082
fe5db567054320d5226c86f4f4a04f99a846254e	3d tracking of multiple people using their 2d face locations	face detection	In this paper, we address tracking of multiple people in complex 3D scenes, using multiple calibrated and synchronized far-field recordings. Our approach utilizes the faces detected in every camera view. Faces of the same person seen from the different cameras are associated by first finding all possible associations and then choosing the best option by means of a 3D stochastic tracker. The performance of the proposed system is evaluated by using the outputs of two grossly different 2D face detectors as input to our 3D algorithm. The multi-camera videos employed come from the CLEAR evaluation campaign. Even though the two 2D face detectors have very different performance, the 3D tracking performance of our system remains practically unchanged.	advanced intelligent tape;algorithm;integrated project support environment;sensor;testbed;tracking system	Nikos Katsarakis;Aristodemos Pnevmatikakis;Michael C. Nechyba	2007		10.1007/978-0-387-74161-1_40	computer vision;face detection;simulation;computer science	Vision	38.66669895483717	-43.58572727544036	78165
ab51dc9de802842b0f46b24b8ea85ff73cdd907d	deep learning hyperspectral image classification using multiple class-based denoising autoencoders, mixed pixel training augmentation, and morphological operations		Herein, we present a system for hyperspectral image segmentation that utilizes multiple class-based denoising autoen-coders which are efficiently trained. Moreover, we present a novel hyperspectral data augmentation method for labelled HSI data using linear mixtures of pixels from each class, which helps the system with edge pixels which are almost always mixed pixels. Finally, we utilize a deep neural network and morphological hole-filling to provide robust image classification. Results run on the Salinas dataset verify the high performance of the proposed algorithm,	algorithm;artificial neural network;autoencoder;computer vision;convolutional neural network;deep learning;horizontal situation indicator;image segmentation;mathematical morphology;noise reduction;pixel	John E. Ball;Pan Wei	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8519368	artificial neural network;deep learning;computer science;pattern recognition;computer vision;pixel;noise reduction;contextual image classification;hyperspectral imaging;image segmentation;feature extraction;artificial intelligence	Vision	30.072774169710797	-45.02872138356366	78305
c8b59d7fb8105c05b453097b7c8168a101ebc489	supervised linear manifold learning feature extraction for hyperspectral image classification	hyperspectral imaging manifolds feature extraction principal component analysis training image classification;hyperspectral image classification feature extraction dimensionality reduction neighborhood preserving embedding manifold learning;supervised neighborhood preserving embedding linear manifold learning feature extraction method hyperspectral image classification point k nearest neighbors prior class label information multiple manifolds aviris hyperspectral data set;learning artificial intelligence feature extraction hyperspectral imaging image classification	A supervised neighborhood preserving embedding (SNPE) linear manifold learning feature extraction method for hyperspectral image classification is presented in this paper. A point's k nearest neighbors is found by using new distance which is proposed according to prior class-label information. The new distance makes intra-class more tightly and inter-class more separately. SNPE overcomes the single manifold assumption of NPE. Data sets lay on (or near) multiple manifolds can be processed. Experimental results on AVIRIS hyperspectral data set demonstrate the effectiveness of our method.	computer vision;feature extraction;k-nearest neighbors algorithm;nonlinear dimensionality reduction;norton power eraser;semi-supervised learning	Jinhuan Wen;Weidong Yan;Wei Lin	2014	2014 IEEE Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2014.6947289	computer vision;feature extraction;machine learning;pattern recognition;k-nearest neighbors algorithm;feature;dimensionality reduction	Vision	28.602780909625128	-43.72468786483939	78368
79d6d077a543a7b789d42dd10c5e102bbe2cbf2c	manifold convex hull (mach): satisfying a need for spd	kernel;manifolds;training;geometry;convex hull classification;computer vision;computational modeling;optimization;spd manifold	In this paper, we extend the nearest convex hull classifier to Symmetric Positive Definite (SPD) manifolds. SPD manifold features have been shown to have excellent performance in various image/video classification tasks. Unfortunately, SPD manifolds naturally possess non-Euclidean geometry, so existing Euclidean machineries such as the nearest convex hull classifier cannot be used directly. To that end, we propose a novel mathematical framework, named Manifold Convex Hull (MACH), that extends the nearest convex hull classifier to SPD Manifolds. The superior performance of our nearest convex hull framework on SPD manifolds is demonstrated in several computer vision applications including object recognition, pedestrian detection and texture classification.	computer vision;convex hull;euclidean distance;outline of object recognition;pedestrian detection	Kun Zhao;Arnold Wiliem;Shaokang Chen;Brian C. Lovell	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532357	mathematical optimization;combinatorics;kernel;convex polytope;topology;convex combination;orthogonal convex hull;manifold;convex hull;mathematics;convex set;computational model	Vision	27.413550127789318	-43.944078682723124	78484
0daf6feabb072b9b01b16c71eb77ec399893ff74	predict brain mr image registration via sparse learning of appearance and transformation	deformable image registration;transformation prediction;brain mr image registration;correspondence detection;sparsity learning	We propose a new approach to register the subject image with the template by leveraging a set of intermediate images that are pre-aligned to the template. We argue that, if points in the subject and the intermediate images share similar local appearances, they may have common correspondence in the template. In this way, we learn the sparse representation of a certain subject point to reveal several similar candidate points in the intermediate images. Each selected intermediate candidate can bridge the correspondence from the subject point to the template space, thus predicting the transformation associated with the subject point at the confidence level that relates to the learned sparse coefficient. Following this strategy, we first predict transformations at selected key points, and retain multiple predictions on each key point, instead of allowing only a single correspondence. Then, by utilizing all key points and their predictions with varying confidences, we adaptively reconstruct the dense transformation field that warps the subject to the template. We further embed the prediction-reconstruction protocol above into a multi-resolution hierarchy. In the final, we refine our estimated transformation field via existing registration method in effective manners. We apply our method to registering brain MR images, and conclude that the proposed framework is competent to improve registration performances substantially.		Qian Wang;Minjeong Kim;Yonghong Shi;Guorong Wu;Dinggang Shen	2015	Medical image analysis	10.1016/j.media.2014.10.007	computer vision;computer science;machine learning;pattern recognition	Vision	29.90891253473804	-47.10912763155762	78544
f6dabb4d91bf7389f3af219d486d4e67cec18c17	vector projection for face recognition	combined projection length;image vector;face identification;vector projection classification	The vector projection length can measure the similarity of two face image vectors.Two projection lengths are combined to make the final classification for face recognition.The projection length between a test image and a training image is the main similarity metric.The projection length between a test image and a class mean is secondary but useful.High dimensionality face images can be directly used to compute their similarities. In this paper, a novel approach for face recognition is proposed by using vector projection length to formulate the pattern recognition problem. Face images of a single-object class are more similar than those of different-object classes. The projection length of a test image vector on the direction of a training image vector can measure the similarity of the two images. But the decision cannot be made by only a training image which is the most similar to the test one, the mean image vector of each class also contributes to the final classification. Thus, the decision of the proposed vector projection classification (VPC) algorithm is ruled in favor of the maximum combination projection length. To address the partial occlusion problem in face recognition, we propose a local vector projection classification (LVPC) algorithm. The experimental results show that the proposed VPC and LVPC approaches are efficient and outperform some existing approaches.	facial recognition system	Changhui Hu;Mengjun Ye;Yijun Du;Xiaobo Lu	2014	Computers & Electrical Engineering	10.1016/j.compeleceng.2014.08.010	computer vision;oblique projection;projection;machine learning;dykstra's projection algorithm;pattern recognition;mathematics;projection	HCI	26.602698354974404	-43.62562556196121	78769
eb14da9d0f01eadcda90e5f0aad8426ff519f335	analysis of focuses of attention distribution for a novel face recognition system	face recognition	In this paper we propose an automated approach to recognize human faces based on the analysis of the distribution of the focuses of attention (FOAs) that reproduces the ability of the humans in the interpretation of visual scenes. The analysis of the FOAs (distribution and position), carried out by an efficient and source light independent visual attention module, allows us to integrate the face features (e.g., eyes, nose, mouth shape) and the holistic features (the relations between the various parts of the face). Moreover, a remarkable approach has been developed for skin recognition based on the shifting of the Hue plane in the HSL color	holism;telecommunications link	Concetto Spampinato;Massimiliano Nicotra;Armando Travaglianti	2008			facial recognition system;face detection;computer science;three-dimensional face recognition	Vision	34.17643545739544	-51.054389104835714	78908
ed88fb0726841b0c6d7575073c784fb40ec1ee1f	human action recognition using global point feature histograms and action shapes	data stream;space time;three dimensional;suffix tree;statistical analysis;pattern matching;action recognition;indoor environment;global features;action segmentation;point cloud	This article investigates the recognition of human actions from 3D point clouds that encode the motions of people acting in sensor-distributed indoor environments. Data streams are time-sequences of silhouettes extracted from cameras in the environment. From the 2D silhouette contours we generate space-time streams by continuously aligning and stacking the contours along the time axis as third spatial dimension. The space-time stream of an observation sequence is segmented into parts corresponding to subactions using a pattern matching technique based on suffix trees and interval scheduling. Then, the segmented space-time shapes are processed by treating the shapes as 3D point clouds and estimating global point feature histograms for them. The resultant models are clustered using statistical analysis, and our experimental results indicate that the presented methods robustly derive different action classes. This holds despite large intra-class variance in the recorded datasets due to performances from different persons at different time intervals. keywords: action recognition, point cloud, global features, action segmentation	action potential;algorithm;apache axis;categorization;cognition;encode;experiment;feature extraction;feature vector;focus stacking;information;interval scheduling;pattern matching;performance;point cloud;resultant;scheduling (computing);silhouette edge;stationary process;suffix tree;vertical blanking interval	Radu Bogdan Rusu;Jan Bandouch;Franziska Meier;Irfan A. Essa;Michael Beetz	2009	Advanced Robotics	10.1163/016918609X12518783330243	three-dimensional space;computer vision;computer science;machine learning;pattern matching;space time;pattern recognition;point cloud;mathematics	Vision	37.72185235033731	-49.54764024481907	78920
b3f1502f2abbfb45b8875aaf078dcb144bc91719	cooperative features extraction in visual sensor networks: a game-theoretic approach	game theory;multi view object recognition;nash bargaining solution;visual sensor networks	Visual Sensor Networks consist of several camera nodes that perform analysis tasks, such as object recognition. In many cases camera nodes have overlapping fields of view. Such overlap is typically leveraged in two different ways: (i) to improve the accuracy/quality of the visual analysis task by exploiting multi-view information or (ii) to reduce the consumed energy by applying temporal scheduling techniques among the multiple cameras. In this work, we propose a game theoretic framework based Nash Bargaining Solution to bridge the gap between the two aforementioned approaches. The key tenet of the proposed framework is for cameras to reduce the consumed energy in the analysis process by exploiting the redundancy in the reciprocal fields of view. Experimental results confirm that the proposed scheme is able to improve the network lifetime, with a negligible loss in terms of visual analysis accuracy.	game theory;nash equilibrium;outline of object recognition;redundancy (engineering);scheduling (computing)	Alessandro Enrico Cesare Redondi;Luca Baroffio;Matteo Cesana;Marco Tagliasacchi	2015		10.1145/2789116.2789124	bargaining problem;game theory;computer vision;simulation;computer science;machine learning;visual sensor network	Robotics	33.122231488651316	-49.542314952619826	79023
25d0a395876b77185f736fb61aeed7d2c4d8196c	pattern classification using a linear associative memory	image processing;learning;procesamiento imagen;classification;transformacion hough;traitement image;algorithme;aprendizaje;algorithm;apprentissage;automatic recognition;pattern matching;pattern classification;pattern recognition;associative memory;hough transform;transformation hough;concordance forme;reconnaissance forme;reconocimiento patron;clasificacion;reconocimiento automatico;reconnaissance automatique;algoritmo	"""-Pattern classification is a very important image processing task. A typical pattern classification algorithm can be broken into two parts; first, the pattern features are extracted and, second, these features are compared with a stored set of reference features until a match is found. In the second part, usually one of the several clustering algorithms or similarity measures is applied. In this paper, a new application of linear associative memory (LAM) to pattern classification problems is introduced. Here, the clustering algorithms or similarity measures are replaced by a LAM matrix multiplication. With a LAM, the reference features need not be separately stored. Since the second part of most classification algorithms is similar, a LAM standardizes the many clustering algorithms and also allows for a standard digital hardware implementation. Computer simulations on regular textures using a feature extraction algorithm achieved a high percentage of successful classification. In addition, this classification is independent of topological transformations. Pattern classification Feature extraction Associative memory Topological transformation Hough transform Associative encoding Iterative encoding l. I N T R O D U C T I O N Pattern classification problems have been studied extensively and for different classes of patterns many classification algorithms have been proposed, m Most classification algorithms, however, have some common features. Figure 1 shows a block diagram of a typical classification algorithm. As the first algorithm step, pattern features used in the classification are extracted. This extraction step usually varies from algorithm to algorithm. To provide a best fit for the class of patterns of interest, the way the primitives are chosen and extracted will differ. The second classification algorithm step is usually common to most algorithms. Here, once the pattern features are extracted, they are then compared with a set of stored reference features. These reference features, assumed to be a priori known, are established during a training phase. The process of matching the unknown features to one of the reference ones is carried out by one of the several available clustering or similarity measure algorithms."""" 31 Even though the second, classification algorithm phase, is somewhat uniform, because of the large number of implementation details, this standardization is lost. Furthermore, the implementation complexity depends on the method chosen. A dynamic clustering algorithm is time-consuming and is difficult to implement. A simple similarity measure algorithm while it is less time-consuming, in some situations it does not perform well. Additionally, no matter which method is chosen, the reference features have to be stored. It is desirable to have available a standard process that would incorporate both the reference feature storage and the decision making process. An alternate way of matching the unknown features to the reference ones, is by mapping a feature vector into an identifying code vector, If this memory mapping is possible, then classification is accomplished. The required mapping matrix can be trained so that it will map known input and output vectors. This mapping requirement is best fulfilled by a linear associative memory (LAM) matrix. The LAM is a learning algorithm that is trained to map desired inputs into desired outputs. Additionally, because of the properties of LAM, this mapping is optimum in the least-square error sense, and it can tolerate some errors introduced by the feature extraction algorithm. The organization of this paper is as follows. In Section 2, the background on the LAM is presented"""	algorithm;bidirectional associative memory;cluster analysis;content-addressable memory;curve fitting;diagram;digital electronics;feature extraction;feature vector;hough transform;image processing;input/output;iterative method;matrix multiplication;reference implementation;similarity measure;simulation;statistical classification	G. Eichmann;Takis Kasparis	1989	Pattern Recognition	10.1016/0031-3203(89)90009-5	hough transform;computer vision;image processing;biological classification;computer science;pattern matching;pattern recognition;algorithm	ML	34.50756016880025	-40.06588221483165	79231
0b852d3b10e244fdb1699281b03a53e9116f247b	recursive ica		Independent Component Analysis (ICA) is a popular method for extracting independent features from visual data. However, as a fundamentally linear technique, there is always nonlinear residual redundancy that is not captured by ICA. Hence there have been many attempts to try to create a hierarchical version of ICA, but so far none of the approaches have a natural way to apply them more than once. Here we show that there is a relatively simple technique that transforms the absolute values of the outputs of a previous application of ICA into a normal distribution, to which ICA maybe applied again. This results in a recursive ICA algorithm that may be applied any number of times in order to extract higher order structure from previous layers.	algorithm;independent computing architecture;independent component analysis;nonlinear system;recursion (computer science)	Honghao Shan;Lingyun Zhang;Garrison W. Cottrell	2006			speech recognition;machine learning;pattern recognition;mathematics	ML	27.381311269443398	-41.712843237208766	79615
0ad344673a12c718637c851758484d7889125347	bregman pooling: feature-space local pooling for image classification	feature pooling;bregman divergence;image classification;co clustering;image representation	In this paper, we propose a novel feature-space local pooling method for the commonly adopted architecture of image classification. While existing methods partition the feature space based on visual appearance to obtain pooling bins, learning more accurate space partitioning that takes semantics into account boosts performance even for a smaller number of bins. To this end, we propose partitioning the feature space over clusters of visual prototypes common to semantically similar images (i.e., images belonging to the same category). The clusters are obtained by Bregman co-clustering applied offline on a subset of training data. Therefore, being aware of the semantic context of the input image, our features have higher discriminative power than do those pooled from appearance-based partitioning. Testing on four datasets (Caltech-101, Caltech-256, 15 Scenes, and 17 Flowers) belonging to three different classification tasks showed that the proposed method outperforms methods in previous works on local pooling in the feature space for less feature dimensionality. Moreover, when implemented within a spatial pyramid, our method achieves comparable results on three of the datasets used.	biclustering;bregman divergence;caltech 101;cluster analysis;computer vision;experiment;feature vector;multiple kernel learning;online and offline;space partitioning	Alameen Najjar;Takahiro Ogawa;Miki Haseyama	2015	International Journal of Multimedia Information Retrieval	10.1007/s13735-015-0086-z	contextual image classification;computer science;bioinformatics;machine learning;pattern recognition;data mining;biclustering;bregman divergence	Vision	27.022300519185023	-46.852005438424015	80015
05bb60661b58aaf4da21621b75bfd5c87ae482ba	video-based person re-identification with adaptive multi-part features learning		Video-based person re-identification plays a significant role in the video surveillance, which can automatically judge whether two non-overlapping video sequences of the pedestrian belong to the same class or not. However, many factors make it challenging, such as different viewpoints and illumination among different cameras, the occlusion, etc. Aiming at increasing the robustness to the occlusion, this paper extracts multi-part appearance features and the feature weight of each part is learned according to its importance. Besides, in order to fully utilize the information included in the video sequences, this paper combines the appearance features and spatial-temporal features of pedestrian by learning several independent metric kernels and fusing the learned metric distances. Extensive experiments on two public benchmark datasets, i.e., the iLIDS-VID and PRID-2011 datasets, demonstrate the effectiveness of the proposed method.		Jingjing Wu;Jianguo Jiang;Meibin Qi;Hao Liu;Meng Wang	2018		10.1007/978-3-030-00776-8_11	robustness (computer science);computer vision;artificial intelligence;viewpoints;computer science;pattern recognition	Vision	34.382320797630676	-50.684028287423786	80041
4eaedf3625b2cc99796b718e04df573cc7b2f96b	hierarchical spatial model for 2d range data based room categorization	layer construction hierarchical spatial model 2d range data service robots mobile robot 2d ground plan like laser range data based room categorization compositional hierarchical representation abstraction layer multicategory set affinity measure part selection;mobile robots laser ranging;dictionaries simultaneous localization and mapping histograms data models buildings	The next generation service robots are expected to co-exist with humans in their homes. Such a mobile robot requires an efficient representation of space, which should be compact and expressive, for effective operation in real-world environments. In this paper we present a novel approach for 2D ground-plan-like laser-range-data-based room categorization that builds on a compositional hierarchical representation of space, and show how an additional abstraction layer, whose parts are formed by merging partial views of the environment followed by graph extraction, can achieve improved categorization performance. A new algorithm is presented that finds a dictionary of exemplar elements from a multi-category set, based on the affinity measure defined among pairs of elements. This algorithm is used for part selection in new layer construction. Room categorization experiments have been performed on a challenging publicly available dataset, which has been extended in this work. State-of-the-art results were obtained by achieving the most balanced performance over all categories.	abstraction layer;algorithm;categorization;combinatorial optimization;dictionary;experiment;mathematical optimization;mobile robot;processor affinity;sensor	Peter Ursic;Ale&#x0161; Leonardis;Danijel Skocaj;Matej Kristan	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487650	computer science;artificial intelligence;machine learning;data mining	Robotics	35.709354927443144	-45.78087373152252	80285
287237596601288cfc9ff332fcf3fa2df8d3240f	object- and space-based visual attention: an integrated framework for autonomous robots	bayes methods;sequential monte carlo implementation;bayesian methods;object based visual attention;autonomous robots;visualization;computational modeling;robot vision;image color analysis;robots;space based visual attention;mathematical model;sequential monte carlo implementation object based visual attention space based visual attention autonomous robots bayesian model;robot vision bayes methods monte carlo methods;visual attention;autonomous robot;monte carlo methods;cameras;robots visualization bayesian methods computational modeling image color analysis mathematical model cameras;bayesian model;sequential monte carlo	This paper argues that the object- and space-based modes of visual attention can be naturally integrated in a common mathematical framework. In an earlier work we have proposed a mathematical model of visual attention for robotic system exploiting the knowledge of visual attention mechanism of the primates. This paper investigates on the validity of the proposed model for robotic systems through experimentation on a real robot. The paper sheds light on a number of real world issues involved with the design of visual attention system for physically embodied robots and explains how the proposed Bayesian model of visual attention addresses these issues. The object- and space-based modes of visual attention are naturally integrated in the model and is reflected in the sequential Monte Carlo implementation of the model on a real robot.	autonomous robot;interoperable object reference;mathematical model;monte carlo method;object-based language;statistical model;top-down and bottom-up design	Momotaz Begum;George K. I. Mann;Ray G. Gosine;Fakhri Karray	2008	2008 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2008.4650651	robot;computer vision;simulation;visualization;particle filter;bayesian probability;computer science;artificial intelligence;mathematical model;human visual system model;bayesian inference;computational model;statistics;monte carlo method	Robotics	37.220577512364414	-41.47744932968662	80312
1baaf293b91414203700f49e303bc87c1c15db38	hand segmentation for gesture recognition in ego-vision	hand segmentation;ego vision;exemplar svm;random forest;gesture recognition	Portable devices for first-person camera views will play a central role in future interactive systems. One necessary step for feasible human-computer guided activities is gesture recognition, preceded by a reliable hand segmentation from egocentric vision. In this work we provide a novel hand segmentation algorithm based on Random Forest superpixel classification that integrates light, time and space consistency. We also propose a gesture recognition method based Exemplar SVMs since it requires a only small set of positive sampels, hence it is well suitable for the egocentric video applications. Furthermore, this method is enhanced by using segmented images instead of full frames during test phase. Experimental results show that our hand segmentation algorithm outperforms the state-of-the-art approaches and improves the gesture recognition accuracy on both the publicly available EDSH dataset and our dataset designed for cultural heritage applications.	algorithm;ego;gesture recognition;random forest	Giuseppe Serra;Marco Camurri;Lorenzo Baraldi;Michela Benedetti;Rita Cucchiara	2013		10.1145/2505483.2505490	computer vision;speech recognition;computer science;gesture recognition;scale-space segmentation;communication	Vision	36.153846170081074	-48.70605217761072	80322
deec883de61822c88b6fa782836e7f04529740e1	null space based image recognition using incremental eigendecomposition	discriminative projections;incremental learning;discriminative common vector;subespaces	An incremental approach to the discriminative common vector (DCV) method for image recognition is considered. Discriminative projections are tackled in the particular context in which new training data becomes available and learned subspaces may need continuous updating. Starting from incremental eigendecomposition of scatter matrices, an efficient updating rule based on projections and orthogonalization is given. The corresponding algorithm has been empirically assessed and compared to its batch counterpart. The same good properties and performance results of the original method are kept but with a dramatic decrease in the computation needed.	computer vision;kernel (linear algebra)	Katerine Díaz-Chito;Francesc J. Ferri;Wladimiro Díaz Villanueva	2011		10.1007/978-3-642-21257-4_39	mathematical optimization;machine learning;pattern recognition;mathematics	Vision	26.165287291251815	-42.33739701656298	80331
da4574b07e2a6002178251d5d60bd55ffc9f1c0f	robust neural approach for the estimation of the essential parameters in computer vision	computer vision	The essential parameters approach is a well-known technique in computer vision for recovering the motion and scene parameters from a sequence of images. This approach has long been considered as suboptimal because of the underestimation of the effect of noise and outliers. This paper re-evaluates this method because it is correctly classified as a structured Total Least Squares problem and proposes very robust linear neurons for its solution. Then, a novel neural network, CASEDEL EXIN, which exploits these neurons together with the case deletion diagnostics, is introduced. It is not only very robust (outlier rejection), but is also able to identify the outliers. This fact can be exploited, for example, to refine the image segmentation.	computer vision	Giansalvo Cirrincione;Maurizio Cirrincione	1999	International Journal on Artificial Intelligence Tools	10.1142/S021821309900018X	computer vision;mathematical optimization;ransac;simulation;computer science;machine learning	Vision	34.853428904763874	-45.47091575160258	80419
44484d2866f222bbb9b6b0870890f9eea1ffb2d0	human reidentification with transferred metric learning	different candidate set;visual similarity;human reidentification;metric learning;weighted maximum margin metric;visual feature;viper dataset;different visual metrics;whole training set;training sample;large training set;candidate set	Human reidentification is to match persons observed in nonoverlapping camera views with visual features for inter-camera tracking. The ambiguity increases with the number of candidates to be distinguished. Simple temporal reasoning can simplify the problem by pruning the candidate set to be matched. Existing approaches adopt a fixed metric for matching all the subjects. Our approach is motivated by the insight that different visual metrics should be optimally learned for different candidate sets. We tackle this problem under a transfer learning framework. Given a large training set, the training samples are selected and reweighted according to their visual similarities with the query sample and its candidate set. A weighted maximum margin metric is online learned and transferred from a generic metric to a candidate-set-specific metric. The whole online reweighting and learning process takes less than two seconds per candidate set. Experiments on the VIPeR dataset and our dataset show that the proposed transferred metric learning significantly outperforms directly matching visual features or using a single generic metric learned from the whole training set.	adaptive grammar;experiment;match moving;pose (computer vision);test set	Wei Li;Rui Zhao;Xiaogang Wang	2012		10.1007/978-3-642-37331-2_3	computer vision;machine learning;data mining;mathematics;statistics	Vision	32.62059603393789	-48.257262936063114	80510
874b190e22043aa54ee92ab9bc1e2594f13cf436	fast spatiotemporal mach filter for action recognition	spatiotemporal mach filter;3d normalized correlation;action recognition;integral video	Human action recognition has been an active field of research in computer vision community for the last decade. The spatiotemporal MACH (maximum average correlation height) filter approach has proved to be a very efficient method to solve the problem. It captures the intra-class variability and produces a very high response at the spatiotemporal location $$(x,y,t)$$ where the action is present in a video. Its computation cost is significantly lower than any other action recognition approach. However, faster algorithm is always needed to perform a computer vision task in real-time. Therefore, we propose a very efficient algorithm for normalized spatiotemporal MACH filtering for action recognition. It is based on the computations performed both in the frequency domain as well as the spatiotemporal domain exploiting integral video. We compare its speed with that of the relevant traditional algorithms and show that our approach drastically outperforms all of them.	algorithm;analysis of algorithms;c++;color;computation;computational complexity theory;computer vision;experiment;matlab;mach;motion capture;motion estimation;programming language;real-time clock;real-time transcription;spatial variability;video	Javed Ahmed;Sadaf Abbasi;M. Zakir Shaikh	2013	Machine Vision and Applications	10.1007/s00138-013-0484-2	computer vision;simulation;computer science;machine learning	Vision	37.865312493365984	-50.769590857360264	80542
c78acea70807c9d4c10176c0e8a7566f76da3394	using double regularization to improve the effectiveness and robustness of fisher discriminant analysis as a projection technique		Fisher Linear Discriminant Analysis (LDA) is a widely-used projection technique. Its application includes face recognition and speaker recognition. The kernel version of LDA (KDA) has also been developed, which generalizes LDA by introducing a kernel. LDA and KDA consists of a within-class scatter matrix and a between-class scatter matrix. The original formulations of LDA and KDA involve the inversion of the within-class scatter matrix, which may have singularity problem. A simple way to prevent singularity is adding a regularization term to the within-class scatter matrix. The resulting LDA and KDA are called Regularized LDA (RLDA) and Regularized KDA (RKDA). In this paper, we experimentally investigate how this regularization term will influence the performance of LDA and KDA. In addition, we introduce an extra regularization term to the between-class scatter matrix, and the resulting LDA and KDA are then called Doubly Regularized LDA (D-RLDA) and Doubly Regularized KDA (D-RKDA). We then apply LDA, KDA, RLDA, RKDA, D-RLDA and D-RKDA as a feature projection technique to two audio signal classification tasks. Gaussian Supervector (GSV) is used as the feature vector and linear Support Vector Machine (SVM) is used as the classifier. Experimental results show that, RLDA, D-RLDA, RKDA and D- RKDA are more effective than the conventional LDA and KDA. Besides, D-RLDA and D-RKDA are more robust than RLDA and RKDA.	experiment;facial recognition system;feature vector;linear discriminant analysis;matrix regularization;regularized meshless method;speaker recognition;statistical classification;support vector machine;technological singularity	Yuechi Jiang;F. H. Frank Leung	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489508	artificial intelligence;kernel (linear algebra);scatter matrix;support vector machine;speaker recognition;feature vector;computer science;pattern recognition;linear discriminant analysis;regularization (mathematics);singularity	AI	24.920773473818247	-40.72400860411488	80706
b5180cc436b65cc3c7e647849422207501a24a6a	vehicle logo super-resolution by canonical correlation analysis	image recognition;image resolution;image resolution vehicles correlation training signal resolution principal component analysis interpolation;image matching;correlation methods;hog feature space vehicle make recognition law enforcement surveillance canonical correlation analysis cca based method vehicle logo super resolution high resolution logos gamma transformations low resolution image matching histogram of oriented gradients;canonical correlation analysis;road vehicles correlation methods image matching image recognition image resolution;super resolution;road vehicles;subspace learning super resolution vehicle make recognition	Recognition of a vehicle make is of interest in the fields of law enforcement and surveillance. In this paper, we develop a canonical correlation analysis (CCA) based method for vehicle logo super-resolution to facilitate the recognition of the vehicle make. From a limited number of high-resolution logos, we populate the training dataset for each make using gamma transformations. Given a vehicle logo from low-resolution source (i.e., surveillance or traffic camera recordings), the learned models yield super-resolved results. By matching the low-resolution image and the generated high-resolution images, we select the final output that is closest to the low-resolution image in the histogram of oriented gradients (HOG) feature space. Experimental results show that our approach outperforms the state-of-the-art super-resolution methods in qualitative and quantitative measures. Furthermore, the super-resolved logos help to improve the accuracy in the subsequent recognition tasks significantly.	feature vector;gradient;histogram of oriented gradients;image resolution;logo;population;super-resolution imaging	Le An;Ninad Thakoor;Bir Bhanu	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467338	computer vision;canonical correlation;speech recognition;image resolution;computer science;pattern recognition;superresolution	Vision	35.031173734621674	-51.69245336527207	80786
fc068f7f8a3b2921ec4f3246e9b6c6015165df9a	beyond part models: person retrieval with refined part pooling (and a strong convolutional baseline)		Employing part-level features offers fine-grained information for pedestrian image description.A prerequisite of part discovery is that each part should be well located. Instead of using external resources like pose estimator, we consider content consistency within each part for precise part location. Specifically, we target at learning discriminative part-informed features for person retrieval and make two contributions. (i) A network named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. With a uniform partition strategy, PCB achieves competitive results with the state-of-the-art methods, proving itself as a strong convolutional baseline for person retrieval. (ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs outliers in each part, which are in fact more similar to other parts. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1 accuracy, surpassing the state of the art by a large margin. Code is available at: https://github.com/syfafterzy/PCB_RPP	baseline (configuration management);column-oriented dbms;convolutional neural network;refinement (computing)	Yifan Sun;Liang Zheng;Yi Yang;Qi Tian;Shengjin Wang	2018		10.1007/978-3-030-01225-0_30	pattern recognition;artificial intelligence;estimator;machine learning;discriminative model;computer science;outlier;partition (number theory);pooling	Vision	30.090735228481762	-51.243465131127444	81094
fca592e5ad206539572c431c0dd3d5f72a11e083	age estimation with expression changes using multiple aging subspaces	eigenvalues and eigenfunctions;graph theory;emotion recognition;matrix algebra;computer vision;estimation aging training data databases correlation training vectors;matrix algebra computer vision eigenvalues and eigenfunctions emotion recognition graph theory;eigenvector matrix image based human age estimation multiple aging subspaces computer vision biometrics weighted random subspace method cross expression age estimation explicit expression change graph based weighted combination method	Image-based human age estimation has become one of the interesting but challenging problems in computer vision and biometrics. It is even harder when the faces have different expressions. In this paper, we propose a weighted random subspace method to solve the relatively new problem: cross-expression age estimation. The proposed method does not depend on the learning of correlation between different expressions, and thus could work in the situation when the expression-correlation does not exist in the training data. We also explore the use of data from multiple datasets to further improve the estimation performance. Experiments on two aging datasets with explicit expression changes demonstrate that the proposed approach gives superior performance over the state-of-the-art method.	biometrics;cosmo-rs;computer vision;correlation does not imply causation;face (geometry);linear discriminant analysis;random subspace method;reed–solomon error correction;test data	Chao Zhang;Guodong Guo	2013	2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)	10.1109/BTAS.2013.6712720	computer vision;graph theory;theoretical computer science;machine learning;pattern recognition;mathematics	Vision	25.891458653700454	-43.66069452724744	81095
645fac7ceffc405d67d73b783defce36d1b6252b	analyzing hyperspectral and hypertemporal data by decoupling feature redundancy and feature relevance	geophysical image processing;hypertemporal earth observation data;unsupervised clustering;support vector machine svm classification;support vector machines;feature forward selection;feature clustering;support vector machines feature selection geophysical image processing hyperspectral imaging image classification learning artificial intelligence remote sensing;supervised classification;image classification;hypertemporal classification;hyperspectral imaging accuracy redundancy support vector machines modis;hyperspectral classification;feature redundancy decoupling;accuracy;dimensionality reduction;redundancy;feature extraction;remote sensing;feature redundancy decoupling hyperspectral earth observation data hypertemporal earth observation data unsupervised clustering support vector machine feature forward selection hyperspectral classification hypertemporal classification;hyperspectral data;modis;feature selection;support vector machine;learning artificial intelligence;support vector machine svm classification dimensionality reduction feature clustering feature extraction hyperspectral data hypertemporal data supervised classification;hyperspectral imaging;hypertemporal data;hyperspectral earth observation data	The high information redundancy in hyperspectral and hypertemporal Earth observation data can limit the performance of supervised learning algorithms. Traditional sequential feature selection approaches start the search on the full set of correlated features, which is a computationally expensive task and impedes the search and discovery of spectral or temporal segments relevant for classification or regression tasks. We therefore propose to decouple the reduction of redundancy from the ranking of features. This is achieved by: 1) an unsupervised clustering of spectrally or temporally correlating neighboring features; 2) the definition of cluster representatives; and 3) the determination of the representatives' relevance by an support vector machine-based feature forward selection. Exemplified by two data sets for solving both, a hyperspectral and a hypertemporal classification problem, we show that our approach leads to well-interpretable spectral and temporal clusters, with comparable accuracies to more processing extensive traditional sequential feature selection.	algorithm;analysis of algorithms;cluster analysis;coupling (computer programming);feature selection;feature vector;machine learning;open-source software;redundancy (information theory);relevance;sensor;stepwise regression;supervised learning;support vector machine	Matthias Held;Andreas Rabe;Cornelius Senf;Sebastian van der Linden;Patrick Hostert	2015	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2014.2371242	support vector machine;minimum redundancy feature selection;computer science;machine learning;pattern recognition;data mining;feature selection	ML	30.516336659130555	-43.75456838939669	81151
b1b925a2a0085f25a41a20dec4471fc8bc9867d6	feature selection based on high dimensional model representation for hyperspectral images	feature selection hyperspectral imaging image classification image representation;kernel;computational modeling feature extraction hyperspectral imaging training kernel correlation computational efficiency;training;computational modeling;feature extraction;dimensionality reduction feature selection high dimensional model representation hyperspectral image classification;feature selection algorithm high dimensional model representation hyperspectral image analysis classification task spectral features spectral bands classification performance degradation supervised classification hughes effects curse of dimensionality dimensionality reduction problem hyperspectral datasets;correlation;hyperspectral imaging;computational efficiency	In hyperspectral image analysis, the classification task has generally been addressed jointly with dimensionality reduction due to both the high correlation between the spectral features and the noise present in spectral bands, which might significantly degrade classification performance. In supervised classification, limited training instances in proportion with the number of spectral features have negative impacts on the classification accuracy, which is known as Hughes effects or curse of dimensionality in the literature. In this paper, we focus on dimensionality reduction problem, and propose a novel feature-selection algorithm, which is based on the method called high dimensional model representation. The proposed algorithm is tested on some toy examples and hyperspectral datasets in comparison with conventional feature-selection algorithms in terms of classification accuracy, stability of the selected features and computational time. The results show that the proposed approach provides both high classification accuracy and robust features with a satisfactory computational time.	bands;computation;curse of dimensionality;dimensionality reduction;feature selection;genetic selection;image analysis;machine learning;selection algorithm;supervised learning;time complexity	G&#x00FC;l&#x015F;en Ta&#x015F;k&#x0131;n;Gülsen Taskin Kaya;Lorenzo Bruzzone	2017	IEEE Transactions on Image Processing	10.1109/TIP.2017.2687128	computer vision;kernel;feature extraction;computer science;hyperspectral imaging;machine learning;pattern recognition;mathematics;computational model;correlation;dimensionality reduction	Vision	29.593489761776212	-43.686353405666495	81304
c9d88634330ad6589d735555a66456c30be1a0dc	one-step time-dependent future video frame prediction with a convolutional encoder-decoder neural network		There is an inherent need for autonomous cars, drones, and other robots to have a notion of how their environment behaves and to anticipate changes in the near future. In this work, we focus on anticipating future appearance given the current frame of a video. Existing work focuses on either predicting the future appearance as the next frame of a video, or predicting future motion as optical flow or motion trajectories starting from a single video frame. This work stretches the ability of CNNs (Convolutional Neural Networks) to predict an anticipation of appearance at an arbitrarily given future time, not necessarily the next video frame. We condition our predicted future appearance on a continuous time variable that allows us to anticipate future frames at a given temporal distance, directly from the input video frame. We show that CNNs can learn an intrinsic representation of typical appearance changes over time and successfully generate realistic predictions at a deliberate time difference in the near future.	artificial neural network;autonomous car;autostereogram;baseline (configuration management);convolutional neural network;display resolution;encoder;iterative method;mean squared error;neural networks;optical flow;robot;video	Vedran Vukotic;Silvia-Laura Pintea;Christian Raymond;Guillaume Gravier;Jan C. van Gemert	2017		10.1007/978-3-319-68560-1_13	computer vision;machine learning;computer science;encoder;convolutional neural network;artificial intelligence;video tracking;artificial neural network;optical flow	AI	27.31028248194969	-51.18760480987398	81321
0dc8a83050a05f05ea7748836cab09aabd4dc1c5	human action description based on temporal pyramid histograms	temporal pyramid histograms;action description;action recognition	In this paper, we present an approach to action description based on temporal pyramid histograms. Bag of features is a widely used action recognition framework based on local features, for example spatio-temporal feature points. Although it outperforms other approaches on several public datasets, sequencing information is ignored. Instead of only calculating the occurrence of code words, we also encode their temporal layout in this work. The proposed temporal pyramid histograms descriptor is a set of histogram atoms generated from the original video clip and its subsequences. To classify actions based on the temporal pyramid histograms descriptor, we design a function to calculate the weights of the histogram atoms according to the corresponding sequence lengths. We test the descriptor using nearest neighbour for classification. Experimental results show that, in comparison to the state-of-the-art, our description approach improves action recognition accuracy.	code word;concatenation;data structure alignment;encode;experience;experiment;feature extraction;optimization problem;pyramid (geometry);video clip	Yingying Liu;Arcot Sowmya	2014		10.5220/0004825206290636	computer vision;machine learning;pattern recognition;mathematics	Vision	37.29000231850415	-51.29712304472466	81485
14b34ff6e911a5b10ce33305598e98b2eec3f868	a multiscale spectral method for learning number of clusters	spectral methods;clustering algorithms eigenvalues and eigenfunctions robustness laplace equations euclidean distance machine learning algorithms;pattern clustering eigenvalues and eigenfunctions graph theory learning artificial intelligence matrix algebra;multiscale analysis;multiscale analysis clustering spectral methods;clustering;block diagonal matrix form multiscale spectral method cluster number learning graph laplacian eigenvalue scale parameter eigengap distance matrix	We propose a novel multiscale, spectral algorithm for estimating the number of clusters in a data set. Our algorithm computes the eigenvalues of the graph Laplacian iteratively for a large range of values of the scale parameter, and estimates the number of clusters from the maximal eigengap. Thus variation of the scale parameter, which usually confuses the clustering problem, is used to infer the number of clusters in a robust and efficient way. Commute distances are used to transform the distance matrix into a block-diagonal form, allowing the algorithm to succeed on irregularly shaped clusters, and the algorithm is applied to test data sets (both simulated and real-world) for method validation.	algorithm;algorithmic efficiency;cluster analysis;distance matrix;laplacian matrix;magnetoencephalography;maximal set;simulation;spectral method;test data;unbalanced circuit	Anna V. Little;Alicia K Byrd	2015	2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2015.119	correlation clustering;mathematical optimization;determining the number of clusters in a data set;combinatorics;discrete mathematics;laplacian matrix;k-medians clustering;computer science;machine learning;mathematics;cluster analysis;spectral clustering;spectral method	Vision	28.183691568671893	-38.43691383116245	81697
471690f8ee8a2b85bc59c146e712d26193869210	using gaussian processes for human tracking and action classification	dynamic model;articulated body tracking;human tracking;dimensionality reduction;particle filter;human body;action classification;gaussian process	We present an approach for tracking human body parts and classification of human actions. We introduce Gaussian Processing Annealed Particle Filter Tracker (GPAPF), which is an extension of the annealed particle filter tracker and uses Gaussian Process Dynamical Model (GPDM) in order to reduce the dimensionality of the problem, increase the tracker's stability and learn the motion models. Motion of human body is described by concatenation of low dimensional manifolds which characterize different motion types. The trajectories in the latent space provide low dimensional representations of sequences of body poses performed during motion. Our approach uses these trajectories in order to classify human actions. The approach was checked on HumanEva data set as well as on our own one. The results and the comparison to other methods are presented.	gaussian process	Leonid M. Raskin;Ehud Rivlin;Michael Rudzsky	2007		10.1007/978-3-540-76858-6_4	computer vision;human body;simulation;particle filter;computer science;machine learning;gaussian process;mathematics;statistics;dimensionality reduction	Vision	36.348174457149995	-48.65913387945358	81764
04c630b9978396044d6a37fdc6a86ac7081b31ab	large-margin predictive latent subspace learning for multiview data analysis	large margin learning;regression large margin predictive latent subspace learning multiview data analysis salient multiview data representations image classification image retrieval image annotation support vector machines view level analysis statistical method supervising side information multiview latent subspace markov network latent subspace mn data likelihood maximization contrastive divergence method hotel review datasets predictive latent subspace representations;view level analysis;supervising side information;support vector machines;support vector machines data analysis image classification image representation image retrieval learning artificial intelligence markov processes regression analysis;multiview data analysis;multiview latent subspace markov network;image classification;statistical method;hotel review datasets;predictive latent subspace representations;image annotation;classification;latent subspace model;learning systems;contrastive divergence method;data analysis;salient multiview data representations;regression;learning systems image retrieval classification;image representation;regression analysis;large margin predictive latent subspace learning;markov processes;learning artificial intelligence;image retrieval and annotation;latent subspace mn;data likelihood maximization;image retrieval and annotation latent subspace model large margin learning classification regression;image retrieval	Learning salient representations of multiview data is an essential step in many applications such as image classification, retrieval, and annotation. Standard predictive methods, such as support vector machines, often directly use all the features available without taking into consideration the presence of distinct views and the resultant view dependencies, coherence, and complementarity that offer key insights to the semantics of the data, and are therefore offering weak performance and are incapable of supporting view-level analysis. This paper presents a statistical method to learn a predictive subspace representation underlying multiple views, leveraging both multiview dependencies and availability of supervising side-information. Our approach is based on a multiview latent subspace Markov network (MN) which fulfills a weak conditional independence assumption that multiview observations and response variables are conditionally independent given a set of latent variables. To learn the latent subspace MN, we develop a large-margin approach which jointly maximizes data likelihood and minimizes a prediction loss on training data. Learning and inference are efficiently done with a contrastive divergence method. Finally, we extensively evaluate the large-margin latent MN on real image and hotel review datasets for classification, regression, image annotation, and retrieval. Our results demonstrate that the large-margin approach can achieve significant improvements in terms of prediction performance and discovering predictive latent subspace representations.	automatic image annotation;complementarity theory;computer vision;hearing loss, high-frequency;inference;latent variable;markov chain;markov random field;restricted boltzmann machine;resultant;statistical classification;support vector machine	Ning Chen;Jun Zhu;Fuchun Sun;Eric P. Xing	2012	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2012.64	latent class model;support vector machine;contextual image classification;regression;biological classification;image retrieval;computer science;machine learning;pattern recognition;data mining;markov process;data analysis;regression analysis;statistics	ML	24.651433435142813	-45.409111759283114	81790
f03483254f91c2c77fb58ebcee6bd4c0dc5ea14f	visual event detection using multi-dimensional concept dynamics	detectors;temporal processes;keywords dynamic pattern;stochastic temporal process;tellurium;semantics;event detection;layout;multimedia systems;multidimensional concept dynamics;computer vision;visual event detection;conference paper;multi dimensional;training data;stochastic processes;semantic concept space;airplanes;pattern matching;feature extraction;video clip visual event detection multidimensional concept dynamics stochastic temporal process semantic concept space;random processes;pattern classification;temporal processing;switches;computer simulation;multidimensional systems;object detection;concept space;event detection stochastic processes airplanes layout computer vision tellurium detectors switches training data object detection;stochastic processes multidimensional systems pattern classification pattern matching;video clip;image retrieval	A novel framework is introduced for visual event detection. Visual events are viewed as stochastic temporal processes in the semantic concept space. In this concept-centered approach to visual event modeling, the dynamic pattern of an event is modeled through the collective evolution patterns of the individual semantic concepts in the course of the visual event. Video clips containing different events are classified by employing information about how well their dynamics in the direction of each semantic concept matches those of a given event. Results indicate that such a data-driven statistical approach is in fact effective in detecting different visual events such as exiting car, riot, and airplane flying	sensor	Shahram Ebadollahi;Lexing Xie;Shih-Fu Chang;John R. Smith	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262691	layout;stochastic process;computer vision;training set;detector;multidimensional systems;feature extraction;network switch;computer science;complex event processing;theoretical computer science;machine learning;pattern matching;tellurium;programming language;statistics	Robotics	38.36370261017747	-47.709954100786426	81894
762d84cf48b57a029cc2742ec263be3970c84416	accelerated analysis of occlusion	accelerated analysis;occlusion;tree pruning;ambiguity;image analysis	Instead of relying on heuristics for the automatic analysis of occlusion, this paper introduces provably correct algorithms for simple classes of synthetic pictures. This work is an initial part of research towards efficient picture analysing algorithms that have the same rigour as, for example, parsing algorithms for formal languages. Systematic analysis of a synthetic picture composed of known object images involves a search tree, pruning of which is shown to substantially accelerate the search for feasible interpretations. Quantitative experimental results are reportedfor noise-free and noisy synthetic pictures. Keywords: image analysis, occlusion, ambiguity, tree pruning		Martin C. Cooper	1988	Image Vision Comput.	10.1016/0262-8856(88)90039-X	computer science;theoretical computer science;machine learning;mathematics;algorithm	Vision	36.42827293950191	-38.82277013643289	82098
f78fba6a8e22ea0325cf3b886dcdf8277c393a97	salient object detection via high-to-low hierarchical context aggregation		Recent progress on salient object detection mainly aims at exploiting how to effectively integrate convolutional side-output features in convolutional neural networks (CNN). Based on this, most of the existing state-of-the-art saliency detectors design complex network structures to fuse the side-output features of the backbone feature extraction networks. However, should the fusion strategies be more and more complex for accurate salient object detection? In this paper, we observe that the contexts of a natural image can be well expressed by a high-to-low self-learning of side-output convolutional features. As we know, the contexts of an image usually refer to the global structures, and the top layers of CNN usually learn to convey global information. On the other hand, it is difficult for the intermediate side-output features to express contextual information. Here, we design an hourglass network with intermediate supervision to learn contextual features in a high-to-low manner. The learned hierarchical contexts are aggregated to generate the hybrid contextual expression for an input image. At last, the hybrid contextual features can be used for accurate saliency estimation. We extensively evaluate our method on six challenging saliency datasets, and our simple method achieves state-of-the-art performance under various evaluation metrics. Code will be released upon paper acceptance.		Yun Liu;Yu Qiu;Le Zhang;Jiawang Bian;Guang-Yu Nie;Ming-Ming Cheng	2018	CoRR		machine learning;convolutional neural network;pattern recognition;salient;artificial intelligence;object detection;salience (neuroscience);complex network;feature extraction;computer science	Vision	27.033566752196045	-51.96472097245593	82140
c4f1fcd0a5cdaad8b920ee8188a8557b6086c1a4	the ignorant led by the blind: a hybrid human–machine vision system for fine-grained categorization	object recognition;fine grained categorization;parts;pose mixture models;interactive;birds;attributes;human in the loop;information gain;crowdsourcing;deformable part models	We present a visual recognition system for fine-grained visual categorization. The system is composed of a human and a machine working together and combines the complementary strengths of computer vision algorithms and (non-expert) human users. The human users provide two heterogeneous forms of information object part clicks and answers to multiple choice questions. The machine intelligently selects the most informative question to pose to the user in order to identify the object class as quickly as possible. By leveraging computer vision and analyzing the user responses, the overall amount of human effort required, measured in seconds, is minimized. Our formalism shows how to incorporate many different types of computer vision algorithms into a human-in-the-loop framework, including standard multiclass methods, part-based methods, and localized multiclass and attribute methods. We explore our ideas by building a field guide for bird identification. The experimental results demonstrate the strength of combining ignorant humans with poor-sighted machines the hybrid system achieves quick and accurate bird identification on a dataset containing 200 bird species.	algorithm;categorization;computer vision;field guide;hybrid system;information;machine vision;semantics (computer science)	Steve Branson;Grant Van Horn;Catherine Wah;Pietro Perona;Serge J. Belongie	2014	International Journal of Computer Vision	10.1007/s11263-014-0698-4	computer vision;computer science;cognitive neuroscience of visual object recognition;machine learning;mathematics;kullback–leibler divergence;interactivity;crowdsourcing;statistics	Vision	35.283065740292116	-45.1253526160918	82210
59402ba305384589e44cecdafa755623afb42f5d	hyperspectral image classification using spectral-spatial lstms		In this paper, we propose a hyperspectral image (HSI) classification method using spectral-spatial long short term memory (LSTM) networks. Specifically, for each pixel, we feed its spectral values in different channels into Spectral LSTM one by one to learn the spectral feature. Meanwhile, we firstly use principle component analysis (PCA) to extract the first principle component from a HSI, and then select local image patches centered at each pixel from it. After that, we feed the row vectors of each image patch into Spatial LSTM one by one to learn the spatial feature for the center pixel. In the classification stage, the spectral and spatial features of each pixel are fed into softmax classifiers respectively to derive two different results, and a decision fusion strategy is further used to obtain a joint spectral-spatial results. Experiments are conducted on two widely used HSIs, and the results show that our method can achieve higher performance than other state-of-the-art methods.		Feng Zhou;Renlong Hang;Qingshan Liu;Xiao-Tong Yuan	2017		10.1007/978-981-10-7299-4_48	pixel;long short term memory;first principle;deep learning;principal component analysis;contextual image classification;hyperspectral imaging;artificial intelligence;computer science;pattern recognition;softmax function	Vision	29.815809119528954	-44.80836669734374	82234
37474926cbc7b5c99978b2ea7081450405ced723	discriminative extended canonical correlation analysis for pattern set matching	angles;vectors;principal;matching;set	In this paper we address the problem of matching sets of vectors embedded in the same input space. We propose an approach which is motivated by canonical correlation analysis (CCA), a statistical technique which has proven successful in a wide variety of pattern recognition problems. Like CCA when applied to the matching of sets, our extended canonical correlation analysis (E-CCA) aims to extract the most similar modes of variability within two sets. Our first major contribution is the formulation of a principled framework for robust inference of such modes from data in the presence of uncertainty associated with noise and sampling randomness. E-CCA retains the efficiency and closed form computability of CCA, but unlike it, does not possess free parameters which cannot be inferred directly from data (inherent data dimensionality, and the number of canonical correlations used for set similarity computation). Our second major contribution is to show that in contrast to CCA, E-CCA is readily adapted to match sets in a discriminative learning scheme which we call discriminative extended canonical correlation analysis (DE-CCA). Theoretical contributions of this paper are followed by an empirical evaluation of its premises on the task of face recognition from sets of rasterized appearance images. The results demonstrate that our approach, E-CCA, already outperforms both CCA and its quasi-discriminative counterpart constrained CCA (C-CCA), for all values of their free parameters. An even greater improvement is achieved with the discriminative variant, DE-CCA.	computability;computation;embedded system;facial recognition system;pattern recognition;randomness;rasterisation;sampling (signal processing);spatial variability	Ognjen Arandjelovic	2013	Machine Learning	10.1007/s10994-013-5380-5	matching;set;machine learning;pattern recognition;data mining;mathematics;angle;principal;statistics	ML	29.031326936389544	-38.21121962160253	82262
d108d360aea6e9adaf633a7300a3ef7d73ab3970	optimal linear projections for enhancing desired data statistics	empirical study;dimension reduction;gene cluster;stiefel manifold;numerical optimization;objective function;statistical properties;stochastic optimization;statistical analysis;grassmann manifold;high dimensional data;pattern recognition;image analysis;optimal algorithm	Problems involving high-dimensional data, including pattern recognition, image analysis, and gene clustering, often require a preliminary step of dimension reduction before or during statistical analysis. If one restricts to a linear technique for dimension reduction, the remaining issue is how to choose the projection. This choice can be dictated by desire to maximize certain statistical properties, including variance, kurtosis, sparseness, and entropy, of the projected data. Motivations for such criteria comes from empirical studies involving natural images. We present a geometric framework for finding projections that are optimal for obtaining desired statistical properties. Our approach is to define an objective function on spaces of orthogonal linear projections – Stiefel and Grassmann manifolds, and to use stochastic gradient techniques to optimize that function. This construction uses the geometries of these manifolds to perform the optimization. Experimental results are presented to demonstrate these ideas for natural and facial images.	algorithm;basis (linear algebra);cluster analysis;dimensionality reduction;entropy (information theory);gradient;image analysis;loss function;mathematical optimization;neural coding;optimization problem;pattern recognition;statistical manifold;stochastic optimization	Evgenia Rubinshtein;Anuj Srivastava	2010	Statistics and Computing	10.1007/s11222-009-9120-4	mathematical optimization;stiefel manifold;combinatorics;statistical theory;image analysis;gene cluster;grassmannian;stochastic optimization;machine learning;mathematics;empirical research;statistics;dimensionality reduction;clustering high-dimensional data	ML	27.601920897174743	-38.67338456949922	82358
c0a0f020d8e303a20da5528a09647edb2745c344	an adversarial hierarchical hidden markov model for human pose modeling and generation		We propose a hierarchical extension to hidden Markov model (HMM) under the Bayesian framework to overcome its limited model capacity. The model parameters are treated as random variables whose distributions are governed by hyperparameters. Therefore the variation in data can be modeled at both instance level and distribution level. We derive a novel learning method for estimating the parameters and hyperparameters of our model based on adversarial learning framework, which has shown promising results in generating photorealistic images and videos. We demonstrate the benefit of the proposed method on human motion capture data through comparison with both state-of-the-art methods and the same model that is learned by maximizing likelihood. The first experiment on reconstruction shows the model’s capability of generalizing to novel testing data. The second experiment on synthesis shows the model’s capability of generating realistic and diverse data.		Rui Zhao;Qiang Ji	2018			artificial intelligence;machine learning;adversarial system;hierarchical hidden markov model;computer science	AI	24.91383332355873	-48.01373754438005	82396
cf98c333c8d7d5870c1ce5538bb0c3de3de16657	panoptic segmentation		We propose and study a novel panoptic segmentation (PS) task. Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we first propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. Second, we are working to introduce panoptic segmentation tracks at upcoming recognition challenges. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.	coherence (physics);computer vision;image segmentation;parsing;pixel	Alexander Kirillov;Kaiming He;Ross B. Girshick;Carsten Rother;Piotr Dollár	2018	CoRR		pattern recognition;artificial intelligence;pixel;computer science;image segmentation;parsing;segmentation	Vision	28.628597009915033	-51.17260554615623	82468
67dda1bf4fbd44125ce325cb7e980ae5e3555f98	lstm-based early recognition of motion patterns	image sequences image classification image motion analysis;training tracking motion segmentation accuracy pattern recognition erbium training data;microsoft kinect lstm based early recognition motion patterns motion template early recognition long short term memory frame by frame classification motion sequences	In this paper a method for Early Recognition (ER) of Motion Templates (MTs) is presented. We define ER as an algorithm to provide recognition results before a motion sequence is completed. In our experiments we apply Long Short-Term Memory (LSTM) and optimize the training for the task of recognizing the motion template as early as possible. The evaluation has shown that the recognition accuracy for a frame-by-frame classification the LSTM achieves a recognition accuracy of 88% if no training data of the person him/herself is included, and 92% if the training data also contains motion sequences of the person. Furthermore, the average earliness - the number of time frames it takes before the LSTM correctly classifies a motion pattern - is around 24.77 frames, which is less than a second with the used tracking technology, i.e., the Microsoft Kinect.	algorithm;erdős–rényi model;experiment;kinect;long short-term memory	Markus Weber;Marcus Liwicki;Didier Stricker;Christopher Schoelzel;Seiichi Uchida	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.611	computer vision;structure from motion;speech recognition;pattern recognition	Vision	37.48845138067966	-49.46711548248135	82691
27bdbca55015bd86833aa799d94196fc9aefc0a8	similarity measure between two gestures using triplets	image classification;vectors;time warped distance similarity measure gesture recognition triplets two sequence based time warped distance relational distributions hog frame difference chalearn gesture challenge dataset rgbd sequences anchor sequence dynamic time warping process warp vectors distance pattern vectors frame wise distances nearest neighbor classification;vectors computational modeling hidden markov models gesture recognition mathematical model assistive technology equations;vectors gesture recognition image classification;gesture recognition	One of the dominant approaches to gesture recognition, especially when we have one or few samples per class, is to compute the time-warped distance between the two sequences and perform nearest-neighbor classification. In this work, we show that we get much better results if instead we consider the similarity of the pattern of frame-wise distances of these two sequences with a third (anchor) sequence from the modelbase. We refer to these distance pattern vectors as the warp vectors. If these warp vectors are similar, then so are the sequences, if not, they are dissimilar. At the algorithmic core we have two dynamic time warping processes, one to compute the warp vectors with the anchor sequences and the other to compare these warp vectors. We select the anchor sequence to be the one that minimizes the overall distance, i.e. the sequence with respect to which these two sequences are the most similar. We present results on a large dataset of 1500 RGBD sequences spanning 150 gesture classes, such as traffic signals, sign language, and every day actions, extracted from the ChaLearn Gesture Challenge dataset. We experimented with three different feature types: difference of frames, HOG and relational distributions. We found that there were improvements of 5%, 15%, and 7%, respectively, at 20% false alarm rate, over traditional two-sequence based timewarped distance.	cluster analysis;dynamic time warping;file spanning;gesture recognition;histogram of oriented gradients;one-shot learning;similarity measure;triplet state	RaviKiran Krishnan;Sudeep Sarkar	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2013.81	computer vision;contextual image classification;speech recognition;computer science;pattern recognition;gesture recognition;mathematics	Vision	36.606723426641736	-51.75290167287017	82890
82265670f78bb20d23447405589ea92b66e9b5a7	retraining maximum likelihood classifiers using a low-rank model	geophysical image processing;tropical forest;forestry;constrained low rank modeling;tropical forest maximum likelihood classifier retraining data distribution constrained low rank modeling cloud detection quickbird worldview 2 images tree cover mapping;training;image classification;maximum likelihood estimation;data distribution;data model;vegetation;maximum likelihood classifier retraining;training data;cloud detection;vectors;worldview 2 images;tree cover mapping;clouds;training data clouds vegetation training covariance matrix data models vectors;quickbird;maximum likelihood estimation clouds forestry geophysical image processing image classification;maximum likelihood classifier;covariance matrix;data models	In this paper we propose a method for retraining a maximum likelihood classifier such that it may be applied to cases when the data distribution of the test data is different from the training data distributions. The proposed approach for retraining the classifier to the test data distribution is based on a constrained low-rank modeling of the unknown parameters, and may be designed such that the class structure is (to a larger degree) maintained after retraining. The proposed methodology is evaluated on two different applications; (1) cloud detection in Quickbird andWorldView-2 images and (2) tree cover mapping of tropical forest. The results show that the retrained classifiers clearly outperform their non-retrained counterpart.	test data	Arnt-Børre Salberg	2011	2011 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2011.6048958	data modeling;training set;covariance matrix;contextual image classification;data model;computer science;machine learning;pattern recognition;maximum likelihood;vegetation;statistics;remote sensing	Robotics	30.984506838288	-43.38843166093295	82956
301af5526eab0237bf481e0571768ecc5604219a	linear-time online action detection from 3d skeletal data using bags of gesturelets	histograms;feature extraction skeleton three dimensional displays kinematics histograms indexes video sequences;video sequences;kinematics;skeleton;indexes;three dimensional displays;feature extraction;video signal processing gesture recognition image classification image segmentation image sequences object detection real time systems vectors;real time applications linear time online action detection 3d skeletal data bags of gesturelets sliding window action recognition presegmented video sequence classifier	Sliding window is one direct way to extend a successful recognition system to handle the more challenging detection problem. While action recognition decides only whether or not an action is present in a pre-segmented video sequence, action detection identifies the time interval where the action occurred in an unsegmented video stream. Sliding window approaches can however be slow as they maximize a classifier score over all possible sub-intervals. Even though new schemes utilize dynamic programming to speed up the search for the optimal sub-interval, they require offline processing on the whole video sequence. In this paper, we propose a novel approach for online action detection based on 3D skeleton sequences extracted from depth data. It identifies the sub-interval with the maximum classifier score in linear time. Furthermore, it is suitable for real-time applications with low latency.	binary classification;dynamic programming;online and offline;partial template specialization;real-time clock;real-time computing;statistical classification;streaming media;time complexity	Moustafa Meshry;Mohamed E. Hussein;Marwan Torki	2016	2016 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2016.7477587	database index;computer vision;kinematics;speech recognition;feature extraction;computer science;video tracking;pattern recognition;histogram;skeleton	Vision	38.63006368402355	-49.374383887678	83011
ba78bae54ff4c46bfd47c34cd92e7602eab16790	bayesian body schema estimation using tactile information obtained through coordinated random movements	probabilistic generative model;bayesian nonparametrics;general movements;body schema;body map	This paper describes a computational model, called the Dirichlet process Gaussian mixture model with latent joints (DPGMM-LJ), that can find latent tree structure embedded in data distribution in an unsupervised manner. By combining DPGMM-LJ and a pre-existing body map formation method, we propose a method that enables an agent having multi-link body structure to discover its kinematic structure, i.e., body schema, from tactile information alone. The DPGMM-LJ is a probabilistic model based on Bayesian nonparametrics and an extension of Dirichlet process Gaussian mixture model (DPGMM). In a simulation experiment, we used a simple fetus model that had five body parts and performed structured random movements in a womb-like environment. It was shown that the method could estimate the number of body parts and kinematic structures without any pre-existing knowledge in many cases. Another experiment showed that the degree of motor coordination in random movements affects the result of body schema formation strongly. It is confirmed that the accuracy rate for body schema estimation had the highest value 84.6% when the ratio of motor coordination was 0.9 in our setting. These results suggest that kinematic structure can be estimated from tactile information obtained by a fetus moving randomly in a womb without any visual information even though its accuracy was not so high. They also suggest that a certain degree of motor coordination in random movements and the sufficient dimension of state space that represents the body map are important to estimate body schema correctly.	biological system;computation;computational model;database schema;experiment;hidden markov model;intelligent agent;lennard-jones potential;lightweight java;multimodal interaction;point-to-point protocol;randomness;simulation;tree structure	Tomohiro Mimura;Yoshinobu Hagiwara;Tadahiro Taniguchi;Tetsunari Inamura	2017	Advanced Robotics	10.1080/01691864.2016.1270854	computer vision;artificial intelligence;machine learning;mathematics;body schema	Robotics	36.96719603895523	-40.80677418806817	83109
80cb432a7fe5ef0fb5fc5fbd7bda2e2430f250f3	spatial and temporal pyramid-based real-time gesture recognition		This paper proposes a novel method for real-time gesture recognition. Aiming at improving the effectiveness and accuracy of HGR, spatial pyramid is applied to linguistically segment gesture sequence into linguistic units and a temporal pyramid is proposed to get a time-related histogram for each single gesture. Those two pyramids can help to extract more comprehensive information of human gestures from RGB and depth video. A two-layered HGR is further exploited to further reduce the computation complexity. The proposed method obtains high accuracy and low computation complexity performance on the ChaLearn Gesture Dataset, comprising more than 50, 000 gesture sequences recorded.	computation;gesture recognition;real-time locating system	Feng Jiang;Jie Ren;Changhoon Lee;Wuzhen Shi;Shaohui Liu;Debin Zhao	2016	Journal of Real-Time Image Processing	10.1007/s11554-016-0620-0	computer vision;speech recognition;computer science	Vision	36.45532437949565	-50.81765198172196	83179
29d3ff2a4f2769a217be99ab76e6a73b191c3011	border-sensitive learning in kernelized learning vector quantization	support vector machine;challenging task;class discrimination;class border;topologically equivalent data space;hebbian learning;kernelized learning vector quantization;vector quantization model;vectorial data;classification approach;feature mapping space	support vector machine;challenging task;class discrimination;class border;topologically equivalent data space;hebbian learning;kernelized learning vector quantization;vector quantization model;vectorial data;classification approach;feature mapping space	kernel method;learning vector quantization	Marika Kaden;Martin Riedel;Marc Strickert;Wieland Hermann;Thomas Villmann	2013		10.1007/978-3-642-38679-4_35	learning vector quantization;artificial intelligence;machine learning;pattern recognition;mathematics	ML	28.845128048745664	-41.89266394536372	83323
1fb9ca6c052deb15c8f1d83ef874e2336407b99a	face recognition using overcomplete independent component analysis	technology;computer science artificial intelligence;independent component analysis;separation;face recognition;science technology;computer science	Most current face recognition algorithms find a set of basis functions in a subspace by training the input data. However, in many applications, the training data is limited or only a few training data are available. In the case, these classic algorithms degrade rapidly. The overcomplete independent component analysis (overcomplete ICA) can separate out more source signals than the input data. In this paper, we use the overcomplete ICA for face recognition with the limited training data. The experimental results show that the overcomplete ICA can im- prove efficiently the recognition rate.	facial recognition system;independent component analysis	Jian Cheng;Hanqing Lu;Yen-Wei Chen;Xiang-Yan Zeng	2003		10.1007/978-3-540-45224-9_194	speech recognition;computer science;machine learning;pattern recognition	Vision	27.604904087551194	-42.117729003787396	83425
db6017d7f86188484f7927c3844f27088cb396ba	kinematic pose rectification for performance analysis and retrieval in sports		The automated extraction of kinematic parameters from athletes in video footage allows for direct training feedback and continuous quantitative assessment of an athlete's performance. Recent developments in the field of deep learning enable the measurement of kinematic coefficients directly from human pose estimates. However, the detection quality decreases while errors and noise increase with the complexity of the scene. In aquatic training scenarios, for instance, continuous pose estimation suffers from several orthogonal errors like switched joint predictions between the left and right sides of the body. In this paper, we analyze different error modes and present a rectification pipeline for improving the pose predictions using merely joint coordinates. We show experimentally that joint rectification equally improves the detection of key-poses, which are essential for a continuous qualitative performance assessment and pose retrieval, as well as posture visualization for quantitative training feedback.		Dan Zecha;Moritz Einfalt;Christian Eggert	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2018.00232	rectification;computer vision;deep learning;pattern recognition;artificial intelligence;visualization;pose;computer science;kinematics	Vision	38.42367194912244	-43.51445785960081	83444
f866ca0a099f641579fa57f4f345da31a9d87e6d	joint object and pose recognition using homeomorphic manifold analysis	object recognition;kinect;machine learning multi modal visual learning;vision	Object recognition is a key precursory challenge in the fields of object manipulation and robotic/AI visual reasoning in general. Recognizing object categories, particular instances of objects and viewpoints/poses of objects are three critical subproblems robots must solve in order to accurately grasp/manipulate objects and reason about their environments. Multi-view images of the same object lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g. visual/depth descriptor spaces). These object manifolds share the same topology despite being geometrically different. Each object manifold can be represented as a deformed version of a unified manifold. The object manifolds can thus be parametrized by its homeomorphic mapping/reconstruction from the unified manifold. In this work, we construct a manifold descriptor from this mapping between homeomorphic manifolds and use it to jointly solve the three challenging recognition sub-problems. We extensively experiment on a challenging multi-modal (i.e. RGBD) dataset and other object pose datasets and achieve state-of-the-art results.	modal logic;outline of object recognition;robot	Haopeng Zhang;Tarek El-Gaaly;Ahmed M. Elgammal;Zhiguo Jiang	2013			vision;computer vision;method;object model;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;3d single-object recognition	AI	30.689973565391803	-47.483501094479834	83485
13b821450da7dd4c4d923bb232ca42bf41f0936c	consistent image analogies using semi-supervised learning	nearest neighbor searches;kernel;image segmentation;image processing;image resolution;supervised learning;markov processes image processing learning artificial intelligence;inference engines;semi supervised learning;markov random field;computer vision;conference paper;keywords artificial intelligence;hidden markov models;filtering algorithms;image edge detection;feature extraction;pixel;pattern recognition;facilities;empirical evaluations;inference algorithms;consistent image analogies;markov processes;learning artificial intelligence;image quilting consistent image analogies semi supervised learning semi supervised component markov random field;semi supervised component;image quilting;a priori;global;large scale systems;semisupervised learning image resolution pixel inference algorithms filters nearest neighbor searches markov random fields euclidean distance image restoration noise figure	In this paper we study the following problem: given two source images A and Apsila, and a target image B, can we learn to synthesize a new image Bpsila which relates to B in the same way that Apsila relates to A? We propose an algorithm which a) uses a semi-supervised component to exploit the fact that the target image B is available apriori, b) uses inference on a Markov random field (MRF) to ensure global consistency, and c) uses image quilting to ensure local consistency. Our algorithm can also deal with the case when A is only partially labeled, that is, only small parts of Apsila are available for training. Empirical evaluation shows that our algorithm consistently produces visually pleasing results, outperforming the state of the art.	apriori algorithm;emoticon;image analogy;local consistency;markov chain;markov random field;semi-supervised learning;semiconductor industry;supervised learning	Li Cheng;S. V. N. Vishwanathan;Xinhua Zhang	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587364	computer vision;kernel;a priori and a posteriori;image resolution;image processing;feature extraction;computer science;machine learning;pattern recognition;image segmentation;markov process;supervised learning;inference engine;pixel	Vision	32.788129409300254	-42.305227344437434	83520
74e38dfeb5abc7ddf077abc01de90f4d0a49c142	omni-directional feature learning for person re-identification		Person re-identification (PReID) has received increasing attention due to it is an important part in intelligent surveillance. Recently, many state-of-the-art methods on PReID are part-based deep models. Most of them focus on learning the part feature representation of person body in horizontal direction. However, the feature representation of body in vertical direction is usually ignored. Besides, the spatial information between these part features and the different feature channels is not considered. In this study, we introduce a multi-branches deep model for PReID. Specifically, the model consists of five branches. Among the five branches, two of them learn the local feature with spatial information from horizontal or vertical orientations, respectively. The other one aims to learn spatial information between the different feature channels generated by the last convolution layer. The remains of two other branches are identification and triplet sub-networks, in which the discriminative global feature and a corresponding measurement can be learned simultaneously. All the five branches can improve the representation learning. We conduct extensive comparative experiments on three PReID benchmarks including CUHK03, Market-1501 and DukeMTMC-reID. The proposed deep framework outperforms many state-of-the-art in most cases. Keywords— person re-identification, deep learning, triplet model, identification model, GRU.		Di Wu;Hong-wei Yang;De-shuang Huang	2018	CoRR			AI	30.21277048500803	-51.24211283504957	83650
57dc7aaa7d14dce33e542d06e2a500da02cc7a1b	multi-view latent space learning based on local discriminant embedding		In many computer vision systems, one object can be described by multi-view data. Compared with individual view, multi-view data can contain complete and complementary information of the problem. But when views capture information which is uniquely but not complete enough to give an uniform learning performance, multi-view data may degrade the learning performance and it is therefore not an ideal solution to simply concatenate multiple views into single view. In this paper, we proposed an multi-view latent space learning algorithm which assume that multi-view data is extracted from the same latent space via distinct transformation. Under this assumption, our algorithm can have a good performance even though views are not complete and the space obtained can contain the valuable information of each view as well as get the underlying connections between multi-view data. Due to the local discriminant embedding of the input space, this multi-view latent space is more suitable for classification or recognition problems. The proposed algorithm is evaluated on two tasks: indoor scene classification and abnormal objects classification on MIT scene 67, Abnormal Objects database respectively. Extensive experiments show that the algorithm we proposed achieves comparable improvements when compared with many other outstanding methods.	algorithm;computer vision;concatenation;discriminant;embedded system;experiment;mathematical optimization;scene graph;statistical classification	Yue Zhao;Xinge You;Yantao Wei;Shi Yin;Dacheng Tao;Yiu-ming Cheung	2016	2016 7th International Conference on Cloud Computing and Big Data (CCBD)	10.1109/CCBD.2016.052	computer science;concatenation;kernel (linear algebra);algorithm design;feature extraction;machine learning;discriminant;embedding;artificial intelligence;pattern recognition	Vision	26.40361681027646	-46.64977815335755	83826
c8df887e4e874dca57e9b11ee5982817fde45790	querying geo-tagged videos for vision applications using spatial metadata	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	In this paper, we propose a novel geospatial image and video filtering tool (GIFT) to select the most relevant input images and videos for computer vision applications with geo-tagged mobile videos. GIFT tightly couples mobile media content and their geospatial metadata for fine granularity video manipulation in the spatial and temporal domain and intelligently indexes field of views (FOVs) to deal with large volumes of data. To demonstrate the effectiveness of GIFT, we introduce an end-to-end application that utilizes mobile videos to achieve persistent target tracking over large space and time. Our experimental results show promising performance of vision applications with GIFT in terms of lower communication load, improved efficiency, accuracy, and scalability when compared with baseline approaches which do not fully utilize geospatial metadata.	baseline (configuration management);computer vision;end-to-end principle;mobile media;scalability	Yinghao Cai;Ying Lu;Seon Ho Kim;Luciano Nocera;Cyrus Shahabi	2017	EURASIP J. Image and Video Processing	10.1186/s13640-017-0165-6	scalability;geospatial analysis;computer vision;artificial intelligence;range query (data structures);geospatial metadata;metadata;granularity;computer science;biometrics;mobile media	Vision	35.604101411091975	-42.322708004393604	83889
7e8e7d4901c1c95f6890cee58bf5759e17f3d2e4	deep residual network with subclass discriminant analysis for crowd behavior recognition		In this work, we extract rich representations of crowd behavior from video using a fine-tuned deep convolutional neural residual network. Using spatial partitioning trees we create subclasses within the feature maps from each of the crowd behavior attributes (classes). Features from these subclasses are then regularized using an eigen modeling scheme. This enables to model the variance appearing from the intra-subclass information. Low dimensional discriminative features are extracted after using the total subclass scatter information. Dynamic time warping is used on the cosine distance measure to find the similarity measure between videos. A 1-nearest neighbor (NN) classifier is used to find the respective crowd behavior attribute classes from the normal videos. Experimental results on large crowd behavior video database show the superior performance of our proposed framework as compared to the baseline and current state-of-the-art methodologies for the crowd behavior recognition task.		Bappaditya Mandal;Jiri Fajtl;Vasileios Argyriou;Dorothy Monekosso;Paolo Remagnino	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451190	discriminative model;crowd psychology;feature extraction;similarity measure;data modeling;artificial intelligence;dynamic time warping;classifier (linguistics);linear discriminant analysis;pattern recognition;computer science	Vision	32.346347569716286	-48.66831582177964	83933
7156a8fc3e47c6a215896f5d6daa2eeaa4990556	stereoscopic video description for human action recognition	erbium;monocular algorithm stereoscopic video description human action recognition scene geometry information stereo disparity video interest point manipulation monocular low level feature descriptor action recognition database unconstrained stereoscopic 3d videos hollywood movies depth aware approach;visual databases feature extraction geometry object recognition stereo image processing video signal processing;vectors;three dimensional displays;image color analysis;stereo image processing;pattern recognition;stereo image processing three dimensional displays cameras image color analysis pattern recognition vectors erbium;cameras	In this paper, a stereoscopic video description method is proposed that indirectly incorporates scene geometry information derived from stereo disparity, through the manipulation of video interest points. This approach is flexible and able to cooperate with any monocular low-level feature descriptor. The method is evaluated on the problem of recognizing complex human actions in natural settings, using a publicly available action recognition database of unconstrained stereoscopic 3D videos, coming from Hollywood movies. It is compared both against competing depth-aware approaches and a state-of-the-art monocular algorithm. Experimental results denote that the proposed approach outperforms them and achieves state-of-the-art performance.	algorithm;audio description;binocular disparity;high- and low-level;hollywood;interest point detection;performance;requirement;stereoscopic video game;stereoscopy;visual descriptor	Ioannis Mademlis;Alexandros Iosifidis;Anastasios Tefas;Nikos Nikolaidis;Ioannis Pitas	2014	2014 IEEE Symposium on Computational Intelligence for Multimedia, Signal and Vision Processing (CIMSIVP)	10.1109/CIMSIVP.2014.7013263	stereo cameras;computer vision;computer science;multimedia;computer graphics (images)	Vision	36.89270959078369	-51.17974626812231	83964
b85afc049a83bd42b68099cc07e9bc6a0eb60239	scalable spectral clustering with cosine similarity		We propose a unified scalable computing framework for three versions of spectral clustering - Normalized Cut (Shi and Malik, 2000), the Ng-Jordan-Weiss (NJW) algorithm (2001), and Diffusion Maps (Coifman and Lafon, 2006), in the setting of cosine similarity. We assume that the input data is either sparse (e.g., as a document-term frequency matrix) or of only a few hundred dimensions (e.g., for small images or data obtained through PCA). We show that in such cases, spectral clustering can be implemented solely based on efficient operations on the data matrix such as elementwise manipulation, matrix-vector multiplication and low-rank SVD, thus entirely avoiding the weight matrix. Our algorithm is simple to implement, fast to run, accurate and robust to outliers. We demonstrate its superior performance through extensive experiments which compare our scalable algorithm with the plain implementation on several benchmark data sets.		Guangliang Chen	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8546193	artificial intelligence;spectral clustering;principal component analysis;sparse matrix;pattern recognition;diffusion map;cosine similarity;singular value decomposition;matrix decomposition;cluster analysis;computer science	ML	25.47649022224157	-38.86891722609896	84059
d8b59d507ec2170614ade1d1ef5ad542e1968451	coarse-to-fine trained multi-scale convolutional neural networks for image classification	automobiles;training laplace equations airplanes automobiles birds atmospheric modeling marine vehicles;training;laplace equations;birds;marine vehicles;airplanes;cifar 100 dataset coarse to fine trained multiscale convolutional neural networks image classification feature learning translation invariance spatial convolution mechanism pooling mechanism scale variation problem multiscale cnn training depth decreasing multicolumn structure input images scale invariant feature learning spatial frequency perception classification error reduction model averaging technique performance improvement cifar io dataset;neural nets feature extraction image classification learning artificial intelligence;atmospheric modeling	Convolutional Neural Networks (CNNs) have become forceful models in feature learning and image classification. They achieve translation invariance by spatial convolution and pooling mechanisms, while their ability in scale invariance is limited. To tackle the problem of scale variation in image classification, this work proposed a multi-scale CNN model with depth-decreasing multi-column structure. Input images were decomposed into multiple scales and at each scale image, a CNN column was instantiated with its depth decreasing from fine to coarse scale for model simplification. Scale-invariant features were learned by weights shared across all scales and pooled among adjacent scales. Particularly, a coarse-to-fine pre-training method imitating the human's development of spatial frequency perception was proposed to train this multi-scale CNN, which accelerated the training process and reduced the classification error. In addition, model averaging technique was used to combine models obtained during pre-training and further improve the performance. With these methods, our model achieved classification errors of 15.38% on CIFAR-10 dataset and 41.29% on CIFAR-100 dataset, i.e. 1.05% and 2.97% reduction compared with single-scale CNN model.	artificial neural network;computer vision;convolution;convolutional neural network;curve fitting;dropout (neural networks);feature learning;imagenet;laplacian matrix;level of detail;neural network software;teaching method	Haobin Dou;Xihong Wu	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280542	computer vision;atmospheric model;computer science;artificial intelligence;machine learning	Vision	24.833400796203314	-50.63179639289113	84061
954dfbbab2a26ca2ff822aebd98d3d8cd0c0f53c	multi-view pairwise relationship learning for sketch based 3d shape retrieval		Recent progress in sketch-based 3D shape retrieval creates a novel and user-friendly way to explore massive 3D shapes on the Internet. However, current methods on this topic rely on designing invariant features for both sketches and 3D shapes, or complex matching strategies. Therefore, they suffer from problems like arbitrary drawings and inconsistent viewpoints. To tackle this problem, we propose a probabilistic framework based on Multi-View Pairwise Relationship (MVPR) learning. Our framework includes multiple views of 3D shapes as the intermediate layer between sketches and 3D shapes, and transforms the original retrieval problem into the form of inferring pairwise relationship between sketches and views. We accomplish pairwise relationship inference by a novel MVPR net, which can automatically predict and merge the pairwise relationships between a sketch and multiple views, thus freeing us from exhaustively selecting the best view of 3D shapes. We also propose to learn robust features for sketches and views via fine-tuning pre-trained networks. Extensive experiments on a large dataset demonstrate that the proposed method can outperform state-of-the-art methods significantly.	experiment;usability	Hanhui Li;Hefeng Wu;Xiangjian He;Shujin Lin;Ruomei Wang;Xiaonan Luo	2017	2017 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2017.8019464	computer science;the internet;artificial intelligence;computer vision;pattern recognition;probabilistic logic;semantics;machine learning;invariant (mathematics);inference;sketch;pairwise comparison;solid modeling	Vision	29.89322760676479	-48.155115762291516	84106
78e1c7cf730f487507ddcc1595a73198cfe41c98	metric learning with two-dimensional smoothness for visual analysis	image application;2dsml;learning artificial intelligence correlation methods image representation;two dimensional smooth metric learning;discretized laplacian penalty;euclidean distance;correlation methods;visualization;laplace equations;smoothing methods;spatial correlation;vectors;image representation;regularized metric learning framework;laplace equations vectors visualization face smoothing methods euclidean distance;visual analysis;face identification;pairwise side information;image analysis;face;image dataset;learning artificial intelligence;image dataset visual analysis pairwise side information content based image retrieval face identification image analysis image representation image pixel spatial correlation regularized metric learning framework two dimensional smooth metric learning 2dsml discretized laplacian penalty metric learning algorithm image application;content based image retrieval;image pixel;metric learning algorithm	In recent years, metric learning methods based on pairwise side information have attracted considerable interests, and lots of efforts have been devoted to utilize these methods for visual analysis like content based image retrieval and face identification. When applied to image analysis, these methods merely look on an n1 × n2 image as a vector in Rn1×n2 space and the pixels of the image are considered as independent. They fail to consider the fact that an image represented in the plane is intrinsically a matrix, and pixels spatially close to each other may probably be correlated. Even though we have n1 × n2 pixels per image, this spatial correlation suggests the real number of freedom is far less. In this paper, we introduce a regularized metric learning framework, Two-Dimensional Smooth Metric Learning (2DSML), which uses a discretized Laplacian penalty to restrict the coefficients to be two-dimensional smooth. Many existing metric learning algorithms can fit into this framework and learn a spatially smooth metric which is better for image applications than their original version. Recognition, clustering and retrieval can be then performed based on the learned metric. Experimental results on benchmark image datasets demonstrate the effectiveness of our method.	algorithm;benchmark (computing);cluster analysis;coefficient;content-based image retrieval;discretization;image analysis;machine learning;pixel	Xinlei Chen;Zifei Tong;Haifeng Liu;Deng Cai	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247970	face;computer vision;spatial correlation;image analysis;visualization;computer science;intrinsic metric;machine learning;pattern recognition;euclidean distance;mathematics;geometry	Vision	27.103789686422463	-41.45199456047248	84149
80525f4a963dc6657b45ae4e61ade0a81f2b323f	jointly detecting infants' multiple facial action units expressed during spontaneous face-to-face communication	action unit recognition;video signal processing emotion recognition face recognition feature extraction image texture psychology shape recognition;structural svm spontaneous facial expressions action unit recognition infants face to face communication;structural svm;gold support vector machines feature extraction face mathematical model vectors equations;infant parent interaction infants multiple facial action units face to face communication spontaneous facial action units emotion mediated interaction emotion development facial feature tracking facial feature extraction face shape face texture subject independent structural output model;spontaneous facial expressions;infants face to face communication	Automatic detection of spontaneous facial Action Units (AUs) in video has many applications including understanding infants' emotion-mediated interactions and development. The target AUs for detection are those essential to positive and negative emotion (i.e., AU 6, AU 12, and AU 20). Tracking and extraction of facial features is especially challenging in infants. Face shape and texture markedly differ from that in adults, jaw contour often is occult, sudden changes in pose and expression are common, and AU often occur in complex combinations. We investigate the association among AUs central to positive and negative emotion and propose a methodology for jointly detecting positively correlated facial AUs of infants during spontaneous interactions with their parents. We apply a subject-independent structural output model to (1) recognize combinations of AUs simultaneously, and (2) model the dependencies between AUs. Using this approach, we improved the reliability of automatic detection of AU 12 and AU 20 in a total 90-minute video of infant-parent interaction of 12 infants.	interaction;sensor;spontaneous order	Nazanin Zaker;Mohammad H. Mahoor;Daniel S. Messinger;Jeffrey F. Cohn	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025271	computer vision;face detection;speech recognition;three-dimensional face recognition;face hallucination	Vision	35.062561268375504	-47.41876162639823	84206
4e4a4359c7dd25af7e2ef0910928cd9faa5d0cfb	end-to-end 3d face reconstruction with deep neural networks		Monocular 3D facial shape reconstruction from a single 2D facial image has been an active research area due to its wide applications. Inspired by the success of deep neural networks (DNN), we propose a DNN-based approach for End-to-End 3D FAce Reconstruction (UH-E2FAR) from a single 2D image. Different from recent works that reconstruct and refine the 3D face in an iterative manner using both an RGB image and an initial 3D facial shape rendering, our DNN model is end-to-end, and thus the complicated 3D rendering process can be avoided. Moreover, we integrate in the DNN architecture two components, namely a multi-task loss function and a fusion convolutional neural network (CNN) to improve facial expression reconstruction. With the multi-task loss function, 3D face reconstruction is divided into neutral 3D facial shape reconstruction and expressive 3D facial shape reconstruction. The neutral 3D facial shape is class-specific. Therefore, higher layer features are useful. In comparison, the expressive 3D facial shape favors lower or intermediate layer features. With the fusion-CNN, features from different intermediate layers are fused and transformed for predicting the 3D expressive facial shape. Through extensive experiments, we demonstrate the superiority of our end-to-end framework in improving the accuracy of 3D face reconstruction.	3d rendering;artificial neural network;computer multitasking;convolutional neural network;database;deep learning;end-to-end principle;experiment;facial recognition system;iterative and incremental development;iterative method;level of detail;loss function;multi-task learning	Pengfei Dou;Shishir K. Shah;Ioannis A. Kakadiaris	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.164	computer vision;convolutional neural network;artificial intelligence;iterative reconstruction;rendering (computer graphics);architecture;3d rendering;computer science;artificial neural network;pattern recognition;facial expression;face hallucination	Vision	25.252075282264126	-49.95856763951728	84249
9d4290c0eb9a93e3bc7f0dacb8ce20c0d632a671	modeling and synthesis of kinship patterns of facial expressions		Analysis of kinship from facial images or videos is an important problem. Prior machine learning and computer vision studies approach kinship analysis as a verification or recognition task. In this paper, for the first time in the literature, we propose a kinship synthesis framework, which generates smile and disgust videos of (probable) children from the expression videos (smile and disgust) of parents. While the appearance of a child’s expression is learned using a convolutional encoder-decoder network, another neural network models the dynamics of the corresponding expression. The expression video of the estimated child is synthesized by the combined use of appearance and dynamics models. In order to validate our results, we perform kinship verification experiments using videos of real parents and estimated children generated by our framework. The results show that generated videos of children achieve higher correct verification rates than those of real children. Our results also indicate that the use of generated videos together with the real ones in the training of kinship verification models, increases the accuracy, suggesting that such videos can be used as a synthetic dataset. Furthermore, we evaluate the expression similarity between input and output frames, and show that the proposed method can fairly retain the expression of input faces while transforming the facial identity.	computer vision;convolutional code;encoder;experiment;facial recognition system;generative adversarial networks;input/output;machine learning;network architecture;synthetic data;synthetic intelligence;test set;video	Itir Onal Ertugrul;László A. Jeni;Hamdi Dibeklioglu	2018	Image Vision Comput.	10.1016/j.imavis.2018.09.012	artificial intelligence;artificial neural network;pattern recognition;disgust;mathematics;facial expression;input/output;kinship	Vision	25.442579901857034	-50.12274605539211	84286
2a5ebb376421e0568bcc4d6ebc76ed631c19b24c	topology dictionary for 3d video understanding	event recognition;graph theory;topology;learning process;image recognition;three dimensional displays topology dictionaries shape video sequences solid modeling markov processes;topology based shape descriptor dictionary;3d video understanding;topology matching;semantic description 3d video dictionary reeb graph topology matching markov model editing summarization;video signal processing;shape descriptor;video signal processing graph theory image recognition image sequences learning artificial intelligence markov processes;video sequences;prior knowledge;3d video sequences;training sequences;three dimensional;editing;model complexity;summarization;markov model;3d model;3d video progressive summarization;shape;reeb graphs;topology change states;three dimensional displays;solid modeling;dictionaries;semantic description;markov process;dictionary;3d video content;reeb graph;markov motion graph;3d video progressive summarization 3d video understanding data sets 3d video content topology based shape descriptor dictionary pattern extraction training sequences reeb graphs markov motion graph topology change states 3d video sequences content based description learning process content based event recognition;markov processes;learning artificial intelligence;data sets;3d video;pattern extraction;content based description;image sequences;content based event recognition	This paper presents a novel approach that achieves 3D video understanding. 3D video consists of a stream of 3D models of subjects in motion. The acquisition of long sequences requires large storage space (2 GB for 1 min). Moreover, it is tedious to browse data sets and extract meaningful information. We propose the topology dictionary to encode and describe 3D video content. The model consists of a topology-based shape descriptor dictionary which can be generated from either extracted patterns or training sequences. The model relies on 1) topology description and classification using Reeb graphs, and 2) a Markov motion graph to represent topology change states. We show that the use of Reeb graphs as the high-level topology descriptor is relevant. It allows the dictionary to automatically model complex sequences, whereas other strategies would require prior knowledge on the shape and topology of the captured subjects. Our approach serves to encode 3D video sequences, and can be applied for content-based description and summarization of 3D video sequences. Furthermore, topology class labeling during a learning process enables the system to perform content-based event recognition. Experiments were carried out on various 3D videos. We showcase an application for 3D video progressive summarization using the topology dictionary.	3d film;3d modeling;anatomy, regional;automatic summarization;bibo stability;biologic preservation;browsing;cluster analysis;data dictionary;data structure;dictionary [publication type];digital video;encode (action);extraction;graph - visual representation;high- and low-level;interaction;matching;markov chain;maxima and minima;network topology;physical object;progressive enhancement;yoga;statistical cluster	Tony Miu Tung;Takashi Matsuyama	2012	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2011.258	computer vision;computer science;graph theory;machine learning;pattern recognition;mathematics;markov process;statistics	Vision	37.556727771930376	-48.565981670872965	84292
849319c637c56a7b61ef4cd69446df767f002332	a new unsupervised hyperspectral band selection method based on multiobjective optimization		Unsupervised band selection methods usually assume specific optimization objectives, which may include band or spatial relationship. However, since one objective could only represent parts of hyperspectral characteristics, it is difficult to determine which objective is the most appropriate. In this letter, we propose a new multiobjective optimization-based band selection method, which is able to simultaneously optimize several objectives. The hyperspectral band selection is transformed into a combinational optimization problem, where each band is represented by a binary code. More importantly, to overcome the problem of unique solution selection in traditional multiobjective methods, we develop a new incorporated rank-based solution set concentration approach in the process of Tchebycheff decomposition. The performance of our method is evaluated under the application of hyperspectral imagery classification. Three recently proposed band selection methods are compared.	binary code;combinational logic;mathematical optimization;multi-objective optimization;optimization problem;program optimization;selection (genetic algorithm);unsupervised learning	Xia Xu;Zhenwei Shi;Bin Pan	2017	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2753237	mathematics;binary code;machine learning;artificial intelligence;hyperspectral imaging;solution set;mathematical optimization;linear programming;multi-objective optimization;optimization problem	EDA	24.923973757059688	-44.941941634238695	84311
b08635a3295c2fffd7e759b517ef4f91015d35db	consistent sparse representation for video-based face recognition		This paper presents a novel method named Consist Sparse Representation (CSR) to solve the problem of video-based face recognition. We treat face images from each set as an ensemble. For each probe set, our goal is that the non-zero elements of the coefficient matrix can ideally focus on the gallery examples from a few/one subject(s). To obtain the sparse representation of a probe set, we simultaneously consider group-sparsity of gallery sets and probe sets. A new matrix norm (i.e. (l_{F,0})-mixed norm) is designed to describe the number of gallery sets selected to represent the probe set. The coefficient matrix is obtained by minimizing the (l_{F,0})-mixed norm which directly counts the number of gallery sets used to represent the probe set. It could better characterize the relations among classes than previous methods based on sparse representation. Meanwhile, a special alternating optimization strategy based on the idea of introducing auxiliary variables is adopted to solve the discontinuous optimization problem. We conduct extensive experiments on Honda, COX and some image set databases. The results demonstrate that our method is more competitive than those state-of-the-art video-based face recognition methods.	facial recognition system;sparse approximation	Xiuping Liu;Aihong Shen;Jie Zhang;Junjie Cao;Yanfang Zhou	2016		10.1007/978-3-319-54187-7_27	pattern recognition;artificial intelligence;coefficient matrix;facial recognition system;computer science;sparse approximation;matrix norm;optimization problem	Vision	24.91050785827732	-43.23341321135542	84388
29e1a58721cafd8abd5b3a0d327d41d438736345	joint object recognition and pose estimation using a nonlinear view-invariant latent generative model	topology;object recognition;kernel;manifolds topology object recognition computational modeling kernel robots;manifolds;computational modeling;object recognition category recognition inference approaches latent space supervised embedding approach object manifold unified manifold computer vision pose estimation nonlinear view invariant latent generative model;robots;pose estimation computer vision inference mechanisms object recognition	Object recognition and pose estimation are two fundamental problems in the field of computer vision. Recognizing objects and their poses/viewpoints are critical components of ample vision and robotic systems. Multiple viewpoints of an object lie on an intrinsic low-dimensional manifold in the input space (i.e. descriptor space). Different objects captured from the same set of viewpoints have manifolds with a common topology. In this paper we utilize this common topology between object manifolds by learning a low-dimensional latent space which non-linearly maps between a common unified manifold and the object manifold in the input space. Using a supervised embedding approach, the latent space is computed and used to jointly infer the category and pose of objects. We empirically validate our model by using multiple inference approaches and testing on multiple challenging datasets. We compare our results with the state-of-the-art and present our increased category recognition and pose estimation accuracy.	3d pose estimation;coefficient;computer vision;generative model;map;nonlinear system;outline of object recognition;robot	Amr Bakry;Tarek El-Gaaly;Mohamed Elhoseiny;Ahmed M. Elgammal	2016	2016 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2016.7477655	robot;computer vision;kernel;pose;3d pose estimation;manifold;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;articulated body pose estimation;mathematics;3d single-object recognition;computational model	Vision	30.63612041240785	-47.20895643256803	84453
8880157cc17258ec0ccbdb7d04ec07ce75bc54a6	classification of remotely sensed images using decimal coded morphological profiles		In this paper, we propose a novel method for pixel classification of remotely sensed images. The proposed method exploits the spatial information of image pixels using morphological profiles produced by structuring elements of different sizes and shapes. Morphological profiles produced by multiple structuring elements are combined into a single feature by decimal coding. The advantage of proposed feature is that it can effectively utilize the potential of multiple morphological profiles without increasing the complexity of feature space. The proposed approachwas tested on remotely sensed images with known ground truths, and performance was improved up to 27% in the overall accuracy results over existing techniques.	feature vector;mike lesser;morphological pattern;pixel	Muhammad Hasnat Khurshid;Muhammad Faisal Khan	2016	Signal, Image and Video Processing	10.1007/s11760-015-0851-8	computer vision;pattern recognition	Vision	31.324104182948783	-44.76892557793792	84522
11fa30ccbf62a64f650844b9cc39797e5faa82d5	a spatial and temporal features mixture model with body parts for video-based person re-identification		The video-based person re-identification is to recognize a person under different cameras, which is a crucial task applied in visual surveillance system. Most previous methods mainly focused on the feature of full body in the frame. In this paper we propose a novel Spatial and Temporal Features Mixture Model (STFMM) based on convolutional neural network (CNN) and recurrent neural network (RNN), in which the human body is split into N parts in horizontal direction so that we can obtain more specific features. The proposed method skillfully integrates features of each part to achieve more expressive representation of each person. We first split the video sequence into N part sequences which include the information of head, waist, legs and so on. Then the features are extracted by STFMM whose 2N inputs are obtained from the developed Siamese network, and these features are combined into a discriminative representation for one person. Experiments are conducted on the iLIDS-VID and PRID-2011 datasets. The results demonstrate that our approach outperforms existing methods for video-based person re-identification. It achieves a rank-1 CMC accuracy of 74% on the iLIDS-VID dataset, exceeding the the most recently developed method ASTPN by 12%. For the cross-data testing, our method achieves a rank-1 CMC accuracy of 48% exceeding the ASTPN method by 18%, which shows that our model has significant stability.	artificial neural network;convolutional neural network;deep learning;mixture model;network architecture;pixel;random neural network;recurrent neural network	Jie Liu;Cheng Sun;Xiang Xu;Baomin Xu;Shuangyuan Yu	2018	CoRR		computer science;pattern recognition;mixture model;artificial intelligence	Vision	30.811554377749125	-51.04333849883857	84610
c784d4918ad33f4dd2991155ea583b4789ba3c11	bimodal vein recognition based on task-specific transfer learning		Both gender and identity recognition task with hand vein information is solved based on the proposed cross-selected-domain transfer learning model. State-of-the-art recognition results demonstrate the effectiveness of the proposed model for pattern recognition task, and the capability to avoid over-fitting of fine-tuning DCNN with small-scaled database. key words: gender recognition, vein recognition, transfer learning	overfitting;pattern recognition	Guoqing Wang;Zaiyu Pan	2017	IEICE Transactions		artificial intelligence;transfer of learning;pattern recognition;computer science;speech recognition	Vision	25.82181671573838	-49.7264767602473	84679
566c22f9eaba1f279a2e8912168fd8159f30f09a	robust kernel approximation for classification		This paper investigates a robust kernel approximation scheme for support vector machine classification with indefinite kernels. It aims to tackle the issue that the indefinite kernel is contaminated by noises and outliers, i.e. a noisy observation of the true positive definite (PD) kernel. The traditional algorithms recovery the PD kernel from the observation with the small Gaussian noises, however, such way is not robust to noises and outliers that do not follow a Gaussian distribution. In this paper, we assume that the error is subject to a Gaussian-Laplacian distribution to simultaneously dense and sparse/abnormal noises and outliers. The derived optimization problem including the kernel learning and the dual SVM classification can be solved by an alternate iterative algorithm. Experiments on various benchmark data sets show the robustness of the proposed method when compared with other state-of-the-art kernel modification based methods.	adobe photoshop;algorithm;approximation;benchmark (computing);iterative method;kernel (operating system);mathematical optimization;optimization problem;semiconductor industry;semidefinite programming;sparse matrix;support vector machine	Fanghui Liu;Xiaolin Huang;Cheng Peng;Jie Yang;Nikola K. Kasabov	2017		10.1007/978-3-319-70087-8_31	kernel embedding of distributions;kernel (statistics);polynomial kernel;support vector machine;kernel principal component analysis;artificial intelligence;pattern recognition;kernel smoother;computer science;radial basis function kernel;variable kernel density estimation	ML	25.876385233677752	-38.12520651839846	84988
8236a77ffe3ca24817ec4a4ada0024e108570887	multiple label prediction for image annotation with multiple kernel correlation models	image sampling;multiple linear regression;kernel canonical correlation analysis;kernel;image segmentation;generalized eigen value problem;training;linear regression;multiple kernel learning;text analysis;layout;image annotation;data mining;image caption reconstruction;text keywords;image representation;canonical correlation analysis;image reconstruction;kernel multiple linear regression model;text analysis image reconstruction regression analysis;linear transformation;predictive models;image analysis;regression analysis;semantic space;computer science;multiple kernel correlation models;correlation;multiple label prediction;multiple kernel learning multiple label prediction image annotation multiple kernel correlation models text keywords kernel multiple linear regression model image caption reconstruction linear transformation canonical correlation analysis generalized eigen value problem;kernel predictive models linear regression image retrieval image analysis layout image reconstruction image representation image sampling computer science;image retrieval	Image annotation is a challenging task that allows to correlate text keywords with an image. In this paper we address the problem of image annotation using Kernel Multiple Linear Regression model. Multiple Linear Regression (MLR) model reconstructs image caption from an image by performing a linear transformation of an image into some semantic space, and then recovers the caption by performing another linear transformation from the semantic space into the label space. The model is trained so that model parameters minimize the error of reconstruction directly. This model is related to Canonical Correlation Analysis (CCA) which maps both images and caption into the semantic space to minimize the distance of mapping in the semantic space. Kernel trick is then used for the MLR resulting in Kernel Multiple Linear Regression model. The solution to KMLR is a solution to the generalized eigen-value problem, related to KCCA (Kernel Canonical Correlation Analysis). We then extend Kernel Multiple Linear Regression and Kernel Canonical Correlation analysis models to multiple kernel setting, to allow various representations of images and captions. We present results for image annotation using Multiple Kernel Learning CCA and MLR on Oliva and Torralba (2001) scene recognition that show kernel selection behaviour.	automatic image annotation;benchmark (computing);eigen (c++ library);experiment;graph kernel;kernel (operating system);kernel method;learning to rank;map;multiple kernel learning;n-gram;standard test image	Oksana Yakhnenko;Vasant Honavar	2009	2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2009.5204274	kernel;principal component regression;computer vision;kernel method;kernel fisher discriminant analysis;string kernel;image analysis;kernel embedding of distributions;radial basis function kernel;image retrieval;kernel principal component analysis;computer science;linear regression;machine learning;pattern recognition;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother	Vision	25.570404384785952	-44.77271106114972	84990
6c7a42b4f43b3a2f9b250f5803b697857b1444ac	multiple feature fusion for face recognition	databases;image coding;training;image fusion;dictionaries databases training image reconstruction face image edge detection face recognition;face recognition;image edge detection;dictionary learning framework multiple feature fusion face recognition fr human face characteristics sparse coding intrinsic problems feature encoding training set jointly encoding process;feature extraction;image reconstruction;learning artificial intelligence face recognition feature extraction image coding image fusion;dictionaries;face;learning artificial intelligence	Recent studies show face recognition (FR) with additional features achieves better performance than that with single one. Different features can represent different characteristics of human faces, and utilizing different features effectively will have positive effect on FR. Meanwhile, the advances of sparse coding enable researchers to develop various recognition methods to cooperate with multiple features. However, even if these methods achieve very encouraging performances, there still exist some intrinsic problems. Firstly, these methods directly encode the multiple features over the original training set, by which way some redundant, noisy and trivial information are incorporated and the recognition performance can be compromised. Moreover, when the training data increase in number, the jointly-encoding process can be very time-consuming. Thirdly, these methods ignore some semantic relationships among the features, which can boost the FR performance. Thus, coarsely utilizing all the features not only adds extra computation burden, but also prevent further improvement. To address these issues, we propose to fuse the multiple features into a more preferable presentation, which is more compact and more discriminative for better FR performance. As well, we take advantage of the dictionary learning framework to derive an effective recognition scheme. We evaluate our model by comparing it with other state-of-the-art approaches, and the experimental results demonstrate the effectiveness of our approach.	coefficient;computation;data compression;data dictionary;dictionary;encode;facial recognition system;machine learning;neural coding;performance;sparse matrix;test set	Shu Kong;Xikui Wang;Donghui Wang;Fei Wu	2013	2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)	10.1109/FG.2013.6553718	computer vision;feature;feature extraction;computer science;machine learning;pattern recognition;three-dimensional face recognition;feature	Vision	28.313902421646095	-46.441981222447424	85138
4b41034c39197ed02d676fe3d6f42d7d5c9d0d15	recovery of corrupted multiple kernels for clustering		Kernel-based methods, such as kernel k-means and kernel PCA, have been widely used in machine learning tasks. The performance of these methods critically depends on the selection of kernel functions; however, the challenge is that we usually do not know what kind of kernels is suitable for the given data and task in advance; this leads to research on multiple kernel learning, i.e. we learn a consensus kernel from multiple candidate kernels. Existing multiple kernel learning methods have difficulty in dealing with noises. In this paper, we propose a novel method for learning a robust yet lowrank kernel for clustering tasks. We observe that the noises of each kernel have specific structures, so we can make full use of them to clean multiple input kernels and then aggregate them into a robust, low-rank consensus kernel. The underlying optimization problem is hard to solve and we will show that it can be solved via alternating minimization, whose convergence is theoretically guaranteed. Experimental results on several benchmark data sets further demonstrate the effectiveness of our method.	aggregate data;algorithm;benchmark (computing);cluster analysis;iterative method;k-means clustering;kernel (operating system);kernel principal component analysis;machine learning;mathematical optimization;multiple kernel learning;optimization problem;scalability;separation kernel;sparse matrix	Peng Zhou;Liang Du;Lei Shi;Hanmo Wang;Yi-Dong Shen	2015			kernel method;mathematical optimization;string kernel;kernel embedding of distributions;radial basis function kernel;machine learning;pattern recognition;graph kernel;mathematics;tree kernel;polynomial kernel;kernel smoother	ML	25.848370029693605	-38.06033560958791	85191
898b7c2d540af532e2bde0075d0b1fb61c4e1851	modified self-organizing feature map neural network with semi-supervision for change detection in remotely sensed images	change detection;fuzzy set;semi supervised learning;selforganizing feature map	Problem of change detection of remotely sensed images using insufficient labeled patterns is the main topic of present work. Here, semisupervised learning is integrated with an unsupervised context-sensitive change detection technique based on modified self-organizing feature map (MSOFM) network. In this method, training of theMSOFMis performed iteratively using unlabeled patterns along with a few labeled patterns. A method has been suggested to select unlabeled patterns for training. To check the effectiveness of the proposed methodology, experiments are carried out on two multitemporal remotely sensed images. Results are found to be encouraging.	artificial neural network;organizing (structure);self-organizing map	Susmita Ghosh;Moumita Roy	2011		10.1007/978-3-642-21786-9_18	semi-supervised learning;computer science;machine learning;pattern recognition;fuzzy set;change detection	ML	31.761652544661285	-44.39811390736422	85228
39675124e4fe1be08f42bdd2e1e237e5a87839ba	adversarial collaboration: joint unsupervised learning of depth, camera motion, optical flow and motion segmentation		We address the unsupervised learning of several interconnected problems in lowlevel vision: single view depth prediction, camera motion estimation, optical flow and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled and, consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other by exploiting known geometric constraints. In order to model geometric constraints, we introduce Adversarial Collaboration, a framework that facilitates competition and collaboration between neural networks. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. Adversarial Collaboration works much like expectation-maximization but with neural networks that act as adversaries, competing to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state of the art results amongst unsupervised methods.	artificial neural network;expectation–maximization algorithm;google moderator;motion estimation;optical flow;pixel;unsupervised learning	Anurag Ranjan;Varun Jampani;Kihwan Kim;Deqing Sun;Jonas Wulff;Michael J. Black	2018	CoRR		market segmentation;artificial intelligence;unsupervised learning;motion estimation;pixel;pattern recognition;artificial neural network;adversarial collaboration;computer science;optical flow;segmentation	Vision	27.796371487790424	-51.84223906118769	85251
c21553b39ba0894d8e53843663d738673ad8e52b	online anomaly detection in videos by clustering dynamic exemplars	pattern clustering;video surveillance;video signal processing;false alarm online anomaly detection dynamic exemplar clustering nonparametric hierarchical event model cluster extraction data structure abnormal event detection crowd surveillance video detection rate;data structures;feature extraction;clustering anomaly detection hierarchical model;videos surveillance computational modeling conferences computer vision legged locomotion pattern recognition;video surveillance data structures feature extraction pattern clustering video signal processing	We propose a non-parametric hierarchical event model to perform online anomaly detection in videos. A dynamic exemplar set is first used to represent observed event samples which updates itself every time when a new sample comes in. Upon this set, clusters are extracted to summarize the exemplars, offering a compact yet informative data structure for past event samples. Abnormal events are detected by both considering their dissimilarity with the model and low frequency. Experiments on real world crowd surveillance videos demonstrate the effectiveness and robustness of the proposed algorithm which shows reliable detection rates and low false alarms.	algorithm;anomaly detection;cluster analysis;data structure;event (computing);information	Jie Feng;Chao Zhang;Pengwei Hao	2012	2012 19th IEEE International Conference on Image Processing	10.1109/ICIP.2012.6467555	computer vision;data structure;feature extraction;computer science;machine learning;pattern recognition;data mining	Robotics	38.38660760926222	-46.77409654254357	85514
c528927b004308eeed4b03a4ff224d76214cbc6f	regularized embedded multiple kernel dimensionality reduction for mine signal processing		Traditional multiple kernel dimensionality reduction models are generally based on graph embedding and manifold assumption. But such assumption might be invalid for some high-dimensional or sparse data due to the curse of dimensionality, which has a negative influence on the performance of multiple kernel learning. In addition, some models might be ill-posed if the rank of matrices in their objective functions was not high enough. To address these issues, we extend the traditional graph embedding framework and propose a novel regularized embedded multiple kernel dimensionality reduction method. Different from the conventional convex relaxation technique, the proposed algorithm directly takes advantage of a binary search and an alternative optimization scheme to obtain optimal solutions efficiently. The experimental results demonstrate the effectiveness of the proposed method for supervised, unsupervised, and semisupervised scenarios.	binary search algorithm;class;coefficient;convex optimization;curse of dimensionality;data point;dimensionality reduction;etoposide/ifosfamide/mesna/mitoxantrone;expectation–maximization algorithm;graph - visual representation;graph embedding;kernel (operating system);linear programming relaxation;math kernel library;mathematical optimization;multiple kernel learning;relaxation techniques;semi-supervised learning;signal processing;solutions;sparse matrix;supervised learning;unsupervised learning;well-posed problem;gamma-endorphin generating enzyme;interest;manifold	Shuang Li;Bing Liu	2016		10.1155/2016/4920670	mathematical optimization;kernel embedding of distributions;machine learning;pattern recognition;mathematics;dimensionality reduction	ML	25.439862013807595	-38.28178762827548	85681
55cb0faa6cb4fb66d749d923de10c18fa4b163fe	dynamic vision sensors for human activity recognition		Unlike conventional cameras which capture video at a fixed frame rate, Dynamic Vision Sensors (DVS) record only changes in pixel intensity values. The output of DVS is simply a stream of discrete ON/OFF events based on the polarity of change in its pixel values. DVS has many attractive features such as low power consumption, high temporal resolution, high dynamic range and less storage requirements. All these make DVS a very promising camera for potential applications in wearable platforms where power consumption is a major concern. In this paper we explore the feasibility of using DVS for Human Activity Recognition (HAR). We propose to use the various slices (such as x - y, x - t and y - t) of the DVS video as a feature map for HAR and denote them as Motion Maps. We show that fusing motion maps with Motion Boundary Histogram (MBH) gives good performance on the benchmark DVS dataset as well as on a real DVS gesture dataset collected by us. Interestingly, the performance of DVS is comparable to that of conventional videos although DVS captures only sparse motion information.		Stefanie Anna Baby;Bimal Vinod;Chaitanya Chinni;Kaushik Mitra	2017	2017 4th IAPR Asian Conference on Pattern Recognition (ACPR)	10.1109/ACPR.2017.136	pattern recognition;pixel;feature extraction;wearable computer;artificial intelligence;computer vision;computer science;frame rate;high dynamic range;temporal resolution;activity recognition;histogram	Vision	35.66541876823925	-49.98353586501759	86006
46175c7bf3365a556c2f409849c57e1fd1a301af	multivariate decision trees through margin maximization principle and topological organization of clusters	institutional repositories;fedora;vital;vtls;ils	This thesis explores two different ways of inducing multivariate decision tree classiﬁers, in order to take into account the correlation between the attributes when deﬁning the splits in the tree nodes. The ﬁrst part of the thesis proposes to use the margin maximization principle in order to create efﬁcient multivariate splits at each node of an ensemble of decision trees. The objective of this method, called support vector trees forest (SVTF), is to assess the performance of multivariate trees forest on multicategory and/or high dimensional classification problems. A new decision scheme is also presented as a substitute to the common averaging schemes (i.e. like majority voting), which is used to infer a unique decision vote from the outputs of every tree. In our proposal, the trees decisions are weighted according to the conﬁdence of each tree towards its own output decision. To do so, fuzzy logic principles are exploited, allowing to compute a conﬁdence score in each node based on where the new data sample fall from the decision boundaries in the SVM features space. The second part of the thesis focuses on visual events from video surveillance sequences, which is a specific case of topologically structured data. In this context, our goal is to conceive an automated method, ﬂexible and accurate, for recognizing the visual events. This is done by deﬁning a three stages system. The ﬁrst one aims at deﬁning and building a set of relevant features describing the shape and movements of the foreground objects in the scene. To this aim, we introduce new motion descriptors based on space-time volumes. Second, an unsupervised learning-based method is used to cluster the objects, thereby deﬁning a set of coarse to ﬁne local patterns of features, representing primitive events in the video sequences. Finally, events are modeled as a spatio-temporal organization of patterns based on an ensemble of randomized trees. In particular, we want this classiﬁer to discover the temporal and causal correlations between the most discriminative patterns.	decision tree;expectation–maximization algorithm	Cedric J Simon	2010			machine learning;pattern recognition;data mining;mathematics	ML	34.610833993234074	-46.46296718012513	86060
163b2d3461ca4c1350875c43a4927839079150a9	generalized lasso based approximation of sparse coding for visual recognition		Sparse coding, a method of explaining sensory data with as few dictionary bases as possible, has attracted much attention in computer vision. For visual object category recognition, `1 regularized sparse coding is combined with the spatial pyramid representation to obtain state-of-the-art performance. However, because of its iterative optimization, applying sparse coding onto every local feature descriptor extracted from an image database can become a major bottleneck. To overcome this computational challenge, this paper presents “Generalized Lasso based Approximation of Sparse coding” (GLAS). By representing the distribution of sparse coefficients with slice transform, we fit a piece-wise linear mapping function with the generalized lasso. We also propose an efficient post-refinement procedure to perform mutual inhibition between bases which is essential for an overcomplete setting. The experiments show that GLAS obtains a comparable performance to `1 regularized sparse coding, yet achieves a significant speed up demonstrating its effectiveness for large-scale visual recognition problems.	approximation algorithm;code;coefficient;computation;computer vision;dictionary;experiment;iterative method;lasso;mathematical optimization;neural coding;pyramid (image processing);refinement (computing);scale-invariant feature transform;self-similarity;sparse approximation;sparse matrix;speedup;visual descriptor	Nobuyuki Morioka;Shin'ichi Satoh	2011			speech recognition;k-svd;computer science;machine learning;pattern recognition;sparse approximation;statistics	Vision	26.49647462589245	-46.773409186948825	86084
6c58e3a8209fef0e28ca2219726c15ea5f284f4f	temporally subsampled detection for accurate and efficient face tracking and diarization	detectors;motion pictures;image color analysis;face;tv;face detection;tracking	Face diarization, i.e. face tracking and clustering within video documents, is useful and important for video indexing and fast browsing but it is also a difficult and time consuming task. In this paper, we address the tracking aspect and propose a novel algorithm with two main contributions. First, we propose an approach that leverages state-of-the-art deformable part-based model (DPM) face detector with a multi-cue discriminant tracking-by-detection framework that relies on automatically learned long-term time-interval sensitive association costs specific to each document type. Secondly to improve performance, we propose an explicit false alarm removal step at the track level to efficiently filter out wrong detections (and resulting tracks). Altogether, the method is able to skip frames, i.e. process only 3 to 4 frames per second - thus cutting down computational cost - while performing better than state-of-the-art methods as evaluated on three public benchmarks from different context including a movie and broadcast data.	addressing mode;algorithm;algorithmic efficiency;cluster analysis;computation;discriminant;jumbo frame;sensor;speaker diarisation;supervised learning;tracking system	Nam Le;Alexandre Heili;Di Wu;Jean-Marc Odobez	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899896	face;computer vision;detector;face detection;speech recognition;computer science;geometry;tracking;multimedia	Vision	33.65904847883563	-50.01529155121315	86149
17b5ab898ac692eb088c66c5048d5324e2071043	encoding based saliency detection for videos and images	videos image color analysis encoding image coding histograms training detectors;video coding image motion analysis object detection;motion cues encoding based saliency detection video saliency detection method human activity recognition activity detection algorithms gestalt principle figure ground segregation joint feature distribution approximation salient object detection ground truth eye gaze estimation activity annotation estimation appearance cues	We present a novel video saliency detection method to support human activity recognition and weakly supervised training of activity detection algorithms. Recent research has emphasized the need for analyzing salient information in videos to minimize dataset bias or to supervise weakly labeled training of activity detectors. In contrast to previous methods we do not rely on training information given by either eye-gaze or annotation data, but propose a fully unsupervised algorithm to find salient regions within videos. In general, we enforce the Gestalt principle of figure-ground segregation for both appearance and motion cues. We introduce an encoding approach that allows for efficient computation of saliency by approximating joint feature distributions. We evaluate our approach on several datasets, including challenging scenarios with cluttered background and camera motion, as well as salient object detection in images. Overall, we demonstrate favorable performance compared to state-of-the-art methods in estimating both ground-truth eye-gaze and activity annotations.	activity recognition;activity tracker;approximation algorithm;computation;eye tracking;gestalt psychology;ground truth;image;map;object detection;sensor;video	Thomas Mauthner;Horst Possegger;Georg Waltner;Horst Bischof	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298864	computer vision;object-class detection;computer science;machine learning;pattern recognition	Vision	34.74041793982067	-48.1200105884659	86217
a6840267e169382d3c5ab2645f6da60dd842d6ac	person re-identification via unsupervised transfer of learned visual representations		Person re-identification is an open and challenging problem in computer vision. Most of the existing literature has focused on designing solutions which advantage of a supervised phase either to learn an end-to-end solution based on deep learning architectures or to find the optimal metric between hand-crafted features. Such solutions require a significant manual labor to annotate a large number of matching pairs to obtain good generalization performance. As a result, an extremely poor scalability is achieved. To address such a problem, we propose a deep learning scheme which leverages on the large quantity of labeled data that may be available in a source domain - different from the re-identification one- to learn a robust visual representation. This is then exploited in an unsupervised transfer learning scheme to better handle the difficulties in the target re-identification domain. The transferred sparse representation obtained via dictionary learning is used to perform the re-identification. Results on two benchmark datasets have shown that our approach performs on par or even better than state-of-the-art approaches.	benchmark (computing);caller id;computer vision;deep learning;dictionary;end-to-end principle;machine learning;performance;scalability;sparse approximation;sparse matrix;supervised learning;unsupervised learning	Niki Martinel;Matteo Dunnhofer;Gian Luca Foresti;Christian Micheloni	2017		10.1145/3131885.3131923	transfer of learning;labeled data;computer science;computer vision;artificial intelligence;scalability;deep learning;workflow;sparse approximation;machine learning	AI	26.98513862950858	-48.946185946254346	86234
360a590703542f2ba345b432416398b6dad9e3fb	multimodal person reidentification using rgb-d cameras	clothing image color analysis sensors histograms torso feature extraction joints;rgb d sensors multi modal person re identification clothing appearance anthropometric measures;image recognition image colour analysis image fusion;anthropometric measures multimodal person reidentification rgb d cameras clothing appearance descriptors dissimilarity based framework score level fusion kinect sensors	Person reidentification consists of recognizing individuals across different sensors of a camera network. Whereas clothing appearance cues are widely used, other modalities could be exploited as additional information sources, like anthropometric measures and gait. In this paper, we investigate whether the reidentification accuracy of clothing appearance descriptors can be improved by fusing them with anthropometric measures extracted from depth data, using RGB-D sensors, in unconstrained settings. We also propose a dissimilarity-based framework for building and fusing the multimodal descriptors of pedestrian images for reidentification tasks, as an alternative to the widely used score-level fusion. The experimental evaluation is carried out on two data sets including RGB-D data, one of which is a novel publicly available data set that we acquired using Kinect sensors. The fusion with anthropometric measures increases the first-rank recognition rate of clothing appearance descriptors up to 20%, whereas our fusion approach reduces the processing cost of the matching phase.	anthropometry;discriminant;experiment;facial recognition system;identification (psychology);kinect;magnetic circular dichroism;modal logic;multimodal interaction;sensor	Federico Pala;Riccardo Satta;Giorgio Fumera;Fabio Roli	2016	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2015.2424056	computer vision;speech recognition;pattern recognition	Vision	35.80462968801249	-49.02910890891176	86283
2fa01ff3ed835480c6ba14d8db6134350829109b	single image intrinsic decomposition without a single intrinsic image		Intrinsic image decomposition—decomposing a natural image into a set of images corresponding to different physical causes—is one of the key and fundamental problems of computer vision. Previous intrinsic decomposition approaches either address the problem in a fully supervised manner, or require multiple images of the same scene as input. These approaches are less desirable in practice, as ground truth intrinsic images are extremely difficult to acquire, and requirement of multiple images pose severe limitation on applicable scenarios. In this paper, we propose to bring the best of both worlds. We present a two stream convolutional neural network framework that is capable of learning the decomposition effectively in the absence of any ground truth intrinsic images, and can be easily extended to a (semi-)supervised setup. At inference time, our model can be easily reduced to a single stream module that performs intrinsic decomposition on a single input image. We demonstrate the effectiveness of our framework through extensive experimental study on both synthetic and real-world datasets, showing superior performance over previous approaches in both single-image and multi-image settings. Notably, our approach outperforms previous state-of-the-art single image methods while using only 50% of ground truth supervision.	artificial neural network;autostereogram;computer vision;convolutional neural network;experiment;ground truth;qr decomposition;synthetic intelligence	Wei-Chiu Ma;Hang Chu;Bolei Zhou;Raquel Urtasun;Antonio Torralba	2018		10.1007/978-3-030-01264-9_13	artificial intelligence;machine learning;instrumental and intrinsic value;convolutional neural network;unsupervised learning;computer science;inference;ground truth	Vision	27.34692817454031	-49.51104860117426	86745
0bbacab0db207434b3137d820a3aa10d6a4e8b58	robust face recognition based on kfda-lle and svm techniques	face recognition;kernel fisher discriminant analysis (kfda);locally linear embedding;manifold learning;support vector machines	Locally Linear Embedding (LLE) is a recently proposed algorithm for non-linear dimensionality reduction and manifold learning. However, it may not be optimal for classification problem. In this paper, an improved version of LLE, namely KFDA-LLE, is proposed using kernel Fisher discriminant analysis (KFDA) method, combined with SVM classifier for face recognition task. Firstly, the input training samples are projected into the low-dimensional space by LLE. Then KFDA is introduced for finding the optimal projection direction. Finally, SVM classifier is used for face recognition. Experimental results on face database demonstrate that the extended LLE method is more efficient and robust.	facial recognition system	Guoqiang Wang;ChunLing Gao	2011		10.1007/978-3-642-23321-0_91	speech recognition;computer science;machine learning;pattern recognition	Vision	25.464471048765724	-41.98145910396244	86901
c038beaa228aeec174e5bd52460f0de75e9cccbe	temporal segment networks for action recognition in videos		Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the instantiation of TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on four challenging action recognition benchmarks: HMDB51 (71.0%), UCF101 (94.9%), THUMOS14 (80.1%), and ActivityNet v1.2 (89.6%). Using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91.0 %) while running at 340 FPS. Furthermore, based on the temporal segment networks, we won the video classification track at the ActivityNet challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and the proposed good practices.	algorithmic efficiency;computation;computer vision;epilepsy, temporal lobe;floating point systems;numerous;overfitting;sampling (signal processing);sampling - surgical action;sparse matrix;test set;time-sensitive networking;universal instantiation;won;videocassette	Limin Wang;Yuanjun Xiong;Zhe Wang;Yu Qiao;Dahua Lin;Xiaoou Tang;Luc Van Gool	2018	IEEE transactions on pattern analysis and machine intelligence	10.1109/TPAMI.2018.2868668	artificial intelligence;pattern recognition;machine learning;pooling;rgb color model;computer science;sampling (statistics)	Vision	26.614791279296572	-51.39688220452997	86908
2dee521717cc21a8aeb6f1a30d14305819313740	robust interactive image segmentation via graph-based manifold ranking	interactive image segmentation graph structure graph edge weights manifold ranking relevance inference;manifold ra;graph structure;graph edge weights;期刊论文;interactive image segmentation	Interactive image segmentation aims at classifying the image pixels into foreground and background classes given some foreground and background markers. In this paper, we propose a novel framework for interactive image segmentation that builds upon graph-based manifold ranking model, a graph-based semi-supervised learning technique which can learn very smooth functions with respect to the intrinsic structure revealed by the input data. The final segmentation results are improved by overcoming two core problems of graph construction in traditional models: graph structure and graph edge weights. The user provided scribbles are treated as the must-link and must-not-link constraints. Then we model the graph as an approximatively k-regular sparse graph by integrating these constraints and our extended neighboring spatial relationships into graph structure modeling. The content and labels driven locally adaptive kernel parameter is proposed to tackle the insufficiency of previous models which usually employ a unified kernel parameter. After the graph construction, a novel three-stage strategy is proposed to get the final segmentation results. Due to the sparsity and extended neighboring relationships of our constructed graph and usage of superpixels, our model can provide nearly real-time, user scribble insensitive segmentations which are two core demands in interactive image segmentation. Last but not least, our framework is very easy to be extended to multi-label segmentation, and for some less complicated scenarios, it can even get the segmented object through single line interaction. Experimental results and comparisons with other state-of-the-art methods demonstrate that our framework can efficiently and accurately extract foreground objects from background.	algorithmic efficiency;color;computation;constrained clustering;convergence insufficiency;data point;experiment;image segmentation;instability;interaction;local consistency;mathematical optimization;microsoft edge;multi-label classification;parallel computing;pixel;real-time clock;semi-supervised learning;semiconductor industry;sparse graph code;sparse matrix;supervised learning;texture mapping	Hong Li;Wen Wu;Enhua Wu	2015	Computational Visual Media	10.1007/s41095-015-0024-2	computer vision;null graph;machine learning;pattern recognition;graph;strength of a graph	Vision	27.39838765770911	-46.012719765601275	86912
914f54e74817a4eebcbd32edc61a73d2c5e9cfe8	applying space state models in human action recognition: a comparative study	orientation discrimination;generic model;action recognition;comparative study;conditional random field;optical flow	This paper presents comparative results of applying different architectures of generative classifiers (HMM, FHMM, CHMM, Multi-Stream HMM, Parallel HMM ) and discriminative classifier as Conditional Random Fields (CRFs) in human action sequence recognition. The models are fed with histogram of very informative features such as contours evolution and optical-flow. Motion orientation discrimination has been obtained tiling the bounding box of the subject and extracting features from each tile. We run our experiments on two well-know databases, KTH's database and Weizmann's. The results show that both type of models reach similar score, being the generative model better when used with optical flow features and being the discriminative one better when uses with shape-context features.		Maria Ángeles Mendoza;Nicolas Pérez de la Blanca	2008		10.1007/978-3-540-70517-8_6	computer vision;speech recognition;computer science;machine learning;comparative research;pattern recognition;optical flow;conditional random field	Vision	35.89812231177251	-48.79574774423334	87281
52f9960de41d0ee0314ba0ec1529ea10e930b98f	image-based video retrieval using deep feature		In this paper, we focus on retrieving video by an image querying. Current approaches involve extracting hand- craft features from each key-frame of videos, which is memory cost. We propose to use deep feature deriving from deep neural network to tackle this issue. Specifically, deep feature is employed to detect shots consisting of similar key-frames and represent them by different aggregation strategies, which can avoid saving redundant key-frames of videos. In addition, to discount the contribution of background, we propose a two-way localization approach, which searches the best matched regions between query and video key-frames. Then, the updated similarity built upon the best matched regions is utilized to re-rank initial retrieval results for further refinement. Experimental results over the public CNN2h dataset demonstrate the effectiveness of the proposed approach.	artificial neural network;deep learning;farmville;key frame;memory footprint;refinement (computing);video clip	Mao Wang;Yuewei Ming;Qiang Liu;Jianping Yin	2017	2017 IEEE International Conference on Smart Computing (SMARTCOMP)	10.1109/SMARTCOMP.2017.7947017	visualization;image retrieval;feature extraction;artificial neural network;machine learning;computer vision;artificial intelligence;computer science	Robotics	32.286110813657125	-51.456533167898904	87456
e0abd0814b224873d8bc0cb6d83550654a1092a6	heat kernel analysis of syntactic structures		We consider two different data sets of syntactic parameters and we discuss how to detect relations between parameters through a heat kernel method developed by Belkin–Niyogi, which produces low dimensional representations of the data, based on Laplace eigenfunctions, that preserve neighborhood information. We analyze the different connectivity and clustering structures that arise in the two datasets, and the regions of maximal variance in the two-parameter space of the Belkin–Niyogi construction, which identify preferable choices of independent variables. We compute clustering coefficients and their variance.	cluster analysis;coefficient;kernel method;maximal set;partha niyogi	Andrew Ortegaray;Robert C. Berwick;Matilde Marcolli	2018	CoRR		machine learning;artificial intelligence;heat kernel;eigenfunction;cluster analysis;variables;syntax;computer science;laplace transform;data set	ML	27.85887755668405	-38.824988543263345	87610
68d51d2583377cfa2f5b473c6e2bb5570b21c52d	robust line drawing understanding incorporating efficient closed symbols extraction		This paper presents an efficient extraction method of closed loops as primitive symbols of line drawings, and an robust line drawing understanding system incorporating this extraction method. A graph search technique is used for the symbols extraction method to provide efficient model prediction and verification facilities, and quantitative consideration on the computational cost is shown. The line drawing understanding system can efficiently extract models from fairly complicated line drawings preserving the computational cost fairly small.	algorithmic efficiency;computation;graph traversal;line drawing algorithm	Shin'ichi Satoh;Hiroshi Mo;Masao Sakauchi	1994			computer vision;artificial intelligence;mathematics;graph	AI	36.514549469374394	-38.89357936705567	87672
4907a834b176dd9053de35f531e1d87f202cbd31	flexible edge arrangement templates for object detection	image training;categorical object detection;semi supervised learning approach;edge detection;interest points;car datasets flexible edge arrangement templates feature representation categorical object detection image training semi supervised learning approach feature selection regression problem lasso method holistic patch method;object detection image edge detection shape computer vision object recognition detectors semisupervised learning lighting statistical analysis layout;semi supervised learning;holistic patch method;regression analysis edge detection feature extraction image representation learning artificial intelligence object detection;image representation;feature extraction;feature selection;regression analysis;feature representation;learning artificial intelligence;regression problem;object detection;car datasets;flexible edge arrangement templates;lasso method	We present a novel feature representation for categorical object detection. Unlike previous approaches that have concentrated on generic interest-point detectors, we construct object-specific features directly from the training images. Our feature is represented by a collection of Flexible Edge Arrangement Templates (FEATs). We propose a two-stage semi-supervised learning approach to feature selection. A subset of frequent templates are first selected from a large template pool. In the second stage, we formulate feature selection as a regression problem and use LASSO method to find the most discriminative templates from the preselected ones. FEATs adaptively capture the image structure and naturally accommodate local shape variations. We show that this feature can be complemented by the traditional holistic patch method, thus achieving both efficiency and accuracy. We evaluate our method on three well-known car datasets, showing performance competitive with existing methods.	feature selection;holism;object detection;semi-supervised learning;semiconductor industry;sensor;shape context;supervised learning	Yan Li;Yanghai Tsin;Yakup Genc;Takeo Kanade	2008	2008 IEEE Workshop on Applications of Computer Vision	10.1109/WACV.2008.4544002	computer vision;edge detection;feature extraction;computer science;machine learning;pattern recognition;feature selection;feature;regression analysis	Vision	27.675515318164823	-46.9656241906545	87685
42a8d099a5bb9de5588fccb306533e968977acdc	support tucker machines	iterative method;image recognition;tensors affine transforms covariance matrices image recognition iterative methods matrix decomposition matrix multiplication pattern classification support vector machines;support tucker machines;support vector machines;research outputs;tensile stress covariance matrix optimization matrix decomposition support vector machines minimization videos;gait recognition;research publications;feature space;tensor based framework;iterative methods;tensor decomposition;matrices;matrix decomposition;covariance matrices;action recognition;affine transformation;action recognition support tucker machines two class classification problem tensor based framework tucker tensor decomposition multiplication matrices covariance matrix affine transformations feature space iterative method support vector machine gait recognition;affine transforms;tucker tensor decomposition;pattern classification;matrix multiplication;affine transformations;two class classification problem;support vector machine;classification accuracy;multiplication;covariance matrix;tensors	In this paper we address the two-class classification problem within the tensor-based framework, by formulating the Support Tucker Machines (STuMs). More precisely, in the proposed STuMs the weights parameters are regarded to be a tensor, calculated according to the Tucker tensor decomposition as the multiplication of a core tensor with a set of matrices, one along each mode. We further extend the proposed STuMs to the Σ/Σw STuMs, in order to fully exploit the information offered by the total or the within-class covariance matrix and whiten the data, thus providing in-variance to affine transformations in the feature space. We formulate the two above mentioned problems in such a way that they can be solved in an iterative manner, where at each iteration the parameters corresponding to the projections along a single tensor mode are estimated by solving a typical Support Vector Machine-type problem. The superiority of the proposed methods in terms of classification accuracy is illustrated on the problems of gait and action recognition.	feature vector;iteration;mathematical optimization;support vector machine;tucker decomposition	Irene Kotsia;Ioannis Patras	2011	CVPR 2011	10.1109/CVPR.2011.5995663	support vector machine;mathematical optimization;discrete mathematics;tensor;cartesian tensor;machine learning;tensor;affine transformation;mathematics;iterative method	Vision	25.718758819145542	-41.499385237790634	87837
8c0a47c61143ceb5bbabef403923e4bf92fb854d	improved strategies for hpe employing learning-by-synthesis approaches		The first contribution of this paper is the presentation of a synthetic video database where the groundtruth of 2D facial landmarks and 3D head poses is available to be used for training and evaluating Head Pose Estimation (HPE) methods. The database is publicly available and contains videos of users performing guided and natural movements. The second and main contribution is the submission of a hybrid method for HPE based on Pose from Ortography and Scaling by Iterations (POSIT). The 2D landmark detection is performed using Random Cascaded-Regression Copse (R-CR-C). For the training stage we use, state of the art labeled databases. Learning-by-synthesis approach has been also used to augment the size of the database employing the synthetic database. HPE accuracy is tested by using two literature 3D head models. The tracking method proposed has been compared with state of the art methods using Supervised Descent Regressors (SDR) in terms of accuracy, achieving an improvement of 60%.	2.5d;3d modeling;database;etsi satellite digital radio;graphics processing unit;iteration;simulation;stochastic gradient descent;supervised learning;synthetic data;synthetic intelligence;titan;tracking system	Andoni Larumbe;Mikel Ariz;Jose Javier Bengoechea;Ruben Segura;Rafael Cabeza;Arantxa Villanueva	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.182	artificial intelligence;pattern recognition;pose;solid modeling;computer science	Vision	30.5469297621129	-49.63686859287515	87917
3480d1c0b7f7f5073d8019efebe146bcb6d6439f	cascade error-correction mechanism for human pose estimation in videos		This paper aims to estimate constantly changing human poses in videos. Traditional methods fail to locate wrists accurately, which is a tremendously challenging task. We propose a three-stage framework for human pose estimation, emphasizing on the improvement of wrist location accuracy. The first stage applies the pictorial structure model to localize the positions of all joints in each frame and calculate the posterior edge distribution probability of wrists. In the second stage, a visual tracking based method is fused into the posterior edge distribution probability of wrists to obtain the wrist location. Instead of directly predicting the wrist location, the third stage designs a novel cascade error-correction mechanism (CECM) to correct the predicted results. In addition, a skin-based proposal and multifarious reinitializing modes are also involved in CECM. Experiments are conducted on the two public datasets, and results demonstrate the superiority of the proposed algorithm compared to state-of-the-art methods.	3d pose estimation	Huibing Dai;Lihuo He;Xinbo Gao;Zhaoqi Guo;Wen Lu	2017		10.1007/978-3-319-67777-4_25	probability distribution;cascade;error detection and correction;pose;computer vision;artificial intelligence;eye tracking;computer science	Vision	34.58322723527154	-48.81658567487192	87983
f186c0d21173e5727d4557c8990d7d0d0251ade4	superpixel appearance and motion descriptors for action recognition	histograms feature extraction trajectory image color analysis visualization optical imaging kernel;human action recognition superpixel appearance motion descriptors action recognition video representation superpixel segmentation computer vision atomic regions superpixel based histograms of oriented gradients motion boundary histograms optical flow histograms superpixel based descriptors;video signal processing computer vision image motion analysis image representation image segmentation	This paper introduces a novel video representation based on superpixel segmentation and appearance and motion descriptors. Superpixel represents a very useful preprocessing step for a wide range of computer vision applications, as they group pixels into perceptually meaningful atomic regions which can be used for recognizing complex motion patterns. We construct a novel video representation in terms of superpixel-based histograms of oriented gradients (HOG), histograms of optical flow (HOF) and motion boundary histograms (MBH) descriptors, and integrate such representations with a bag-of-features (BoF) model for classification. The proposed approach is evaluated in the context of action classification on a challenging benchmark dataset: UCF Sports dataset and it achieves 87.9% generalization accuracy. The experimental results demonstrate the advantage of superpixel-based descriptors compared to other approaches for human action recognition.	benchmark (computing);computer vision;experiment;gradient;image segmentation;optical flow;pixel;preprocessor	Xuan Dong;Ah Chung Tsoi;Sio-Long Lo	2014	2014 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2014.6889575	computer vision;machine learning;pattern recognition	Vision	36.776453206922206	-51.198825706417104	88063
4bc15b3c4e4522ce500b39c2b040cebdaf29d11e	dimensionality reduction via compressive sensing	signal r compressive sensing;supervised learning;dimensionality reduction method;natural images;sparse models;semi supervised;journal article;wavelet basis;dimensionality reduction;dimensionality reduction algorithms;compressive sensing;random measurement;sparse data;un supervised learning;sparse representation;pca;keywords compressive sensing	0167-8655/$ see front matter 2012 Elsevier B.V. A doi:10.1016/j.patrec.2012.02.007 q This work is partially supported by Charles S Research Grant OPA 4818. 1 NICTA is funded by the Australian Government as re of Broadband, Communications and the Digital Econom Council through the ICT Centre of Excellence program. ⇑ Corresponding author. E-mail addresses: jbgao@csu.edu.au (J. Gao), q Tiberio.Caetano@nicta.com.au (T.S. Caetano). Compressive sensing is an emerging field predicated upon the fact that, if a signal has a sparse representation in some basis, then it can be almost exactly reconstructed from very few random measurements. Many signals and natural images, for example under the wavelet basis, have very sparse representations, thus those signals and images can be recovered from a small amount of measurements with very high accuracy. This paper is concerned with the dimensionality reduction problem based on the compressive assumptions. We propose novel unsupervised and semi-supervised dimensionality reduction algorithms by exploiting sparse data representations. The experiments show that the proposed approaches outperform state-of-the-art dimensionality reduction methods. 2012 Elsevier B.V. All rights reserved.	algorithm;basis (linear algebra);compressed sensing;data dependency;experiment;generative topographic map;isomap;nonlinear dimensionality reduction;nonlinear system;opa;semi-supervised learning;semiconductor industry;simplicial complex;sparse approximation;sparse matrix;the australian;unsupervised learning;wavelet	Junbin Gao;Qinfeng Shi;Tibério S. Caetano	2012	Pattern Recognition Letters	10.1016/j.patrec.2012.02.007	computer vision;sparse matrix;computer science;machine learning;pattern recognition;sparse approximation;mathematics;supervised learning;compressed sensing;dimensionality reduction;principal component analysis	AI	25.97183197381768	-39.902046777829035	88113
0183f5dd416f1ec7beb1c6bfec5d9e14d1d79912	multi-view subspace clustering via relaxed l1-norm of tensor multi-rank		In this paper, we address the multi-view subspace clustering problem. Our method utilize the circulant algebra for tensor, which is constructed by stacking the subspace representation matrices of different views and then shifting, to explore the high order correlations underlying multi-view data. By introducing a recently proposed tensor factorization, namely tensor-Singular Value Decomposition (t-SVD) [16], we can impose a new type of low-rank tensor constraint on the shifted tensor to capture the complementary information from multiple views. Different from traditional unfolding based tensor norm, this low-rank tensor constraint has optimality properties similar to that of matrix rank derived from SVD, so that complementary information among views can be explored more efficiently and thoroughly. The established model, called t-SVD based Multi-view Subspace Clustering (t-SVD-MSC), falls into the applicable scope of augmented Lagrangian method, and its minimization problem can be efficiently solved with theoretical convergence guarantee and relatively low computational complexity. Extensive experimental testing on eight challenging image dataset shows that the proposed method has achieved highly competent objective performance compared to several state-of-the-art multi-view clustering methods.	algorithm;augmented lagrangian method;circulant matrix;cluster analysis;clustering high-dimensional data;coefficient;computational complexity theory;focus stacking;mathematical optimization;singular value decomposition;taxicab geometry;unfolding (dsp implementation)	Yuan Xie;Dacheng Tao;Wensheng Zhang;Lei Zhang	2016	CoRR			Vision	27.87533819010112	-39.109417124247834	88287
f6d9057c6c1c675ff73f307d041e129b2e77f11c	an efficient approach for multi-view human action recognition based on bag-of-key-poses	multi view action recognition;action recognition;human action recognition;muhavi dataset;computer science and informatics;bag of key poses;key pose	This paper presents a novel multi-view human action recognition approach based on a bag-of-key-poses. In the case of multi-view scenarios, it is especially difficult to perform accurate action recognition that still runs at an admissible recognition speed. The presented method aims to fill this gap by combining a silhouette-based pose representation with a simple, yet effective multi-view learning approach based on Model Fusion. Action classification is performed through efficient sequence matching and by the comparison of successive key poses which are evaluated on both feature similarity and match relevance. Experimentation on the MuHAVi dataset shows that the method outperforms currently available recognition rates and is exceptionally robust to actorvariance. Temporal evaluation confirms the method’s suitability for realtime recognition.	linear algebra;real-time transcription;relevance	Alexandros André Chaaraoui;Pau Climent-Pérez;Francisco Flórez-Revuelta	2012		10.1007/978-3-642-34014-7_3	computer vision;feature;computer science;machine learning;pattern recognition	Vision	36.00598605011252	-50.62807739579972	88457
c2c2a8614ebb00448f4cb3667724efdc9596702c	pose aided deep convolutional neural networks for face alignment		Recently, deep convolutional neural networks have been widely used and achieved state-of-the-art performance in face recognition tasks such as face verification, face detection and face alignment. However, face alignment remains a challenging problem due to large pose variation and the lack of data. Although researchers have designed various network architecture to handle this problem, pose information was rarely used explicitly. In this paper, we propose Pose Aided Convolutional Neural Networks (PACN) which uses different networks for faces with different poses. We first train a CNN to do pose classification and a base CNN, then different networks are finetuned from the base CNN for faces of different pose. Since there wouldn’t be many images for each pose, we propose a data augmentation strategy which augment the data without affecting the pose. Experiment results show that the proposed PACN achieves better or comparable results than the state-of-the-art methods.	convolutional neural network	Shuying Liu;Jiani Hu;Weihong Deng	2016		10.1007/978-3-319-46654-5_7	computer vision;machine learning;pattern recognition	Vision	29.109558135548383	-50.56071436565014	88692
e45a024a68d19aac2e212c382e133510ddf3b435	bearing defect classification based on individual wavelet local fisher discriminant analysis with particle swarm optimization	wavelet analysis;kernel;support vector machines;training;kernel feature extraction support vector machines training accuracy wavelet analysis fault diagnosis;pattern recognition local fisher discriminant analysis bearing defect classification wavelet kernel feature extraction dimensional reduction;support vector machines fault diagnosis feature extraction machine bearings mechanical engineering computing particle swarm optimisation pattern classification;accuracy;feature extraction;institutional repository research archive oaister;fault diagnosis;decision fusion mechanism bearing defect classification individual wavelet local fisher discriminant analysis feature extraction dimensionality reduction wavelet kernel local fisher discriminant analysis particle swarm optimization algorithm pso algorithm single global transformation multiclass task binary classification tasks one against one strategy oao strategy individual pso wklfda i pso wklfda support vector machine classifier svm classifier	In order to enhance the performance of bearing defect classification, feature extraction and dimensionality reduction have become important. In order to extract the effective features, wavelet kernel local fisher discriminant analysis (WKLFDA) is first proposed; herein, a new wavelet kernel function is proposed to construct the kernel function of LFDA. In order to automatically select the parameters of WKLFDA, a particle swarm optimization (PSO) algorithm is employed, yielding a new PSO-WKLFDA. When compared with the other state-of-the-art methods, the proposed PSO-WKLFDA yields better performance. However, the use of a single global transformation of PSO-WKLFDA for the multiclass task does not provide excellent classification accuracy due to the fact that the projected data still significantly overlap with each other in the projected subspace. In order to enhance the performance of bearing defect classification, a novel method is then proposed by transforming the multiclass task into all possible binary classification tasks using a one-against-one (OAO) strategy. Then, individual PSO-WKLFDA (I-PSO-WKLFDA) is used for extracting effective features of each binary class. The extracted effective features of each binary class are input to a support vector machine (SVM) classifier. Finally, a decision fusion mechanism is employed to merge the classification results from each SVM classifier to identify the bearing condition. Simulation results using synthetic data and experimental results using different bearing fault types show that the proposed method is well suited and effective for bearing defect classification.	algorithm;binary classification;dimensionality reduction;domain analysis;feature extraction;linear discriminant analysis;mathematical optimization;multimodal interaction;netware loadable module;oddworld: abe's oddysee;particle swarm optimization;pattern recognition;phase-shift oscillator;simulation;software bug;support vector machine;synthetic data;wavelet	Mien Van;Hee-Jun Kang	2016	IEEE Transactions on Industrial Informatics	10.1109/TII.2015.2500098	wavelet;support vector machine;kernel method;kernel;feature extraction;computer science;engineering;machine learning;linear classifier;pattern recognition;data mining;accuracy and precision;linear discriminant analysis;structured support vector machine;statistics	ML	28.842779474205585	-42.79588233858958	88744
044e01ae3b5ba9f51ec43b43af6c6a74a9e293b0	look wider to match image patches with convolutional neural networks	computer architecture cost function convolution training neural networks benchmark testing visualization;cnn stereo matching pooling	When a human matches two images, the viewer has a natural tendency to view the wide area around the target pixel to obtain clues of right correspondence. However, designing a matching cost function that works on a large window in the same way is difficult. The cost function is typically not intelligent enough to discard the information irrelevant to the target pixel, resulting in undesirable artifacts. In this letter, we propose a novel convolutional neural network (CNN) module to learn a stereo matching cost with a large-sized window. Unlike conventional pooling layers with strides, the proposed per-pixel pyramid-pooling layer can cover a large area without a loss of resolution and detail. Therefore, the learned matching cost function can successfully utilize the information from a large area without introducing the fattening effect. The proposed method is robust despite the presence of weak textures, depth discontinuity, illumination, and exposure difference. The proposed method achieves near-peak performance on the Middlebury benchmark.	artifact (software development);artificial neural network;benchmark (computing);binocular disparity;computer stereo vision;convolutional neural network;glossary of computer graphics;illumination (image);loss function;microsoft windows;pixel;pyramid (geometry);reflections of signals on conducting lines;relevance	Haesol Park;Kyoung Mu Lee	2017	IEEE Signal Processing Letters	10.1109/LSP.2016.2637355	pattern recognition;convolutional neural network;pixel;computer vision;pooling;artificial neural network;artificial intelligence;discontinuity (linguistics);benchmark (computing);computer science	Vision	28.023887079508736	-50.14841373492379	88875
6d1e846f6c3fb3aa8bb9d392e563145a840247b4	a comparative analysis of kernel subspace target detectors for hyperspectral imagery	hyperspectral imagery;signal image and speech processing;capteur imagerie hyperspectral;evaluation performance;sensor hiperespectral de formacion de imagenes;comparative analysis;performance evaluation;learning;methode noyau;evaluacion prestacion;kernel function;filtro adaptado;metodo subespacio;hyperspectral imaging sensor;carta de datos;methode sous espace;algorithme;aprendizaje;algorithm;apprentissage;imagineria hiperespectral;quantum information technology spintronics;mappage;metodo nucleo;funcion nucleo;fonction noyau;subspace method;kernel method;mapping;matched filter;filtre adapte;imagerie hyperspectrale;algoritmo	Several linear and nonlinear detection algorithms that are based on spectral matched (subspace) filters are compared. Nonlinear (kernel) versions of these spectral matched detectors are also given and their performance is compared with linear versions. Several well-known matched detectors such as matched subspace detector, orthogonal subspace detector, spectral matched filter, and adaptive subspace detector are extended to their corresponding kernel versions by using the idea of kernel-based learning theory. In kernel-based detection algorithms the data is assumed to be implicitly mapped into a high-dimensional kernel feature space by a nonlinear mapping, which is associated with a kernel function. The expression for each detection algorithm is then derived in the feature space, which is kernelized in terms of the kernel functions in order to avoid explicit computation in the high-dimensional feature space. Experimental results based on simulated toy examples and real hyperspectral imagery show that the kernel versions of these detectors outperform the conventional linear detectors.	algorithm;computation;feature vector;kernel (operating system);kernel method;matched filter;nonlinear system;sensor	Heesung Kwon;Nasser M. Nasrabadi	2007	EURASIP J. Adv. Sig. Proc.	10.1155/2007/29250	kernel;principal component regression;qualitative comparative analysis;kernel method;speech recognition;kernel embedding of distributions;radial basis function kernel;kernel adaptive filter;kernel principal component analysis;computer science;artificial intelligence;hyperspectral imaging;machine learning;mathematics;matched filter;kernel;variable kernel density estimation;polynomial kernel;kernel smoother	ML	30.632069380020145	-41.14011238564534	88880
24f67d6065b8e87c64a2db1d7c4b2534d63d0084	road sign classification using laplace kernel classifier	traffic signs;decision tree;image processing;kernel density estimation;decision making process;road sign recognition;kernel density estimate;expectation maximization algorithm	Driver support systems of intelligent vehicles will predict potentially dangerous situations in heavy traffic, help with navigation and vehicle guidance and interact with a human driver. Important information necessary for traffic situation understanding is presented by road signs. A new kernel rule has been developed for road sign classification using the Laplace probability density. Smoothing parameters of the Laplace kernel are optimized by the pseudo-likelihood cross-validation method. To maximize the pseudo-likelihood function, an ExpectationMaximization algorithm is used. The algorithm has been tested on a dataset with more than 4 900 noisy images. A comparison to other classification methods is also given.	academy;computation;cross-validation (statistics);expectation–maximization algorithm;experiment;image segmentation;kernel (operating system);multimodal interaction;rejection sampling;simple features;smoothing	Pavel Paclík;Jana Novovicová;Pavel Pudil;Petr Somol	2000	Pattern Recognition Letters	10.1016/S0167-8655(00)00078-7	kernel density estimation;kernel embedding of distributions;radial basis function kernel;image processing;computer science;machine learning;pattern recognition;data mining;mathematics;variable kernel density estimation;statistics	Vision	32.939354109447095	-40.45865215588287	89256
3e0cc4fdd7a4ab9a8ea19ca3429c7ea1668a6e8d	logistic regression for feature selection and soft classification of remote sensing data	logistic regression lr;remote sensing image;soft classification feature selection linear discriminant analysis lda logistic regression lr;image classification;linear discriminate analysis;logistic regression;remote sensing feature extraction image classification regression analysis;remote sensing data processing;remote sensing data;feature extraction;remote sensing;linear discriminant analysis logistic regression feature selection soft classification remote sensing data processing hyperspectral image classification;feature selection;regression analysis;classification accuracy;logistics remote sensing hyperspectral sensors hyperspectral imaging linear discriminant analysis feature extraction data processing predictive models information resources robustness;hyperspectral image classification;linear discriminant analysis;hyperspectral image;soft classification;linear discriminant analysis lda	Feature selection is a key task in remote sensing data processing, particularly in case of classification from hyperspectral images. A logistic regression (LR) model may be used to predict the probabilities of the classes on the basis of the input features, after ranking them according to their relative importance. In this letter, the LR model is applied for both the feature selection and the classification of remotely sensed images, where more informative soft classifications are produced naturally. The results indicate that, with fewer restrictive assumptions, the LR model is able to reduce the features substantially without any significant decrease in the classification accuracy of both the soft and hard classifications	feature selection;information;lr parser;logistic regression	Qi Cheng;Pramod K. Varshney;Reet K. Tiwari	2006	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2006.877949	contextual image classification;feature extraction;computer science;machine learning;pattern recognition;data mining;logistic regression;linear discriminant analysis;feature selection;regression analysis	ML	31.030559580221787	-43.00464056708783	89339
ee668d03754a1dd7ce4f36ff68dab45e8186a7ae	sequential knowledge-driven scene recognition model	image sampling;frequent attentional gaze shifts;image recognition;image motion analysis;visual scene sampling;probability;image segmentation;temporal nature;segmentation techniques sequential knowledge driven scene recognition model eye movements human visual behavior temporal nature visual scene sampling frequent attentional gaze shifts saccades fixations informative regions visual scene prior knowledge similar scenes scene categories probabilistic combination hypothetical objects prototypical regions image region successive recognition process conditional probabilities scene category informative value sequential spatial information gathering algorithm eye movement saccade;probabilistic combination;layout image segmentation partitioning algorithms image motion analysis humans image sampling image recognition predictive models robustness shape;conditional probabilities;prior knowledge;layout;fixations;segmentation techniques;image region;shape;sequential knowledge driven scene recognition model;prototypical regions;information value;successive recognition process;eye movements;informative value;probability image recognition image sampling image segmentation digital simulation;eye movement;scene category;sequential spatial information gathering algorithm;visual scene;scene categories;robustness;predictive models;humans;saccades;eye movement saccade;conditional probability;similar scenes;human visual behavior;spatial information;digital simulation;hypothetical objects;partitioning algorithms;informative regions	Eye movements are an important aspect of human visual behavior The temporal and space-variant nature of sampling a visual scene requires frequent attentional gaze shifts, saccades, to &ate onto diferent parts of an image. Experimental evidence suggests that fiations are ofen directed towards the most informative regions in the visual scene. We develop a model and its simulation that can select such regions based on prior knowledge of similar scenes. Having representations of scene categories as a probabilistic combination of hypothetical objects, i.e., prototypical regions with certain properties, it is possible to assess the likely contribution of each image region to the successive recognition process. Using conditional probabilities for each region given the scene category, the model can then predict its informative value and initiate a sequentia I spatial information-gathering algorithm analogous to an eye movement saccade to a neweation. This algorithm establishes the most likely scene category for a given image.	algorithm;information;sampling (signal processing);simulation	Dimitri A. Chernyak;Lawrence W. Stark	2001		10.1109/CVPR.2001.990986	computer vision;conditional probability;computer science;machine learning;pattern recognition;mathematics;statistics;eye movement	Vision	39.00029561164218	-49.957803675631546	89482
31503c70bf0ee69a5ec1e59b6e0d6ad4b3b413a0	a generic approach for systematic analysis of sports videos	sports video analysis;assessment and evaluation;genre categorization;dissimilarity measure;event detection;sports video;domain knowledge;large scale;structure prediction;conditional random field;k nearest neighbor;bag of visual words;classification accuracy;generic framework;probabilistic latent semantic analysis;view classification	Various innovative and original works have been applied and proposed in the field of sports video analysis. However, individual works have focused on sophisticated methodologies with particular sport types and there has been a lack of scalable and holistic frameworks in this field. This article proposes a solution and presents a systematic and generic approach which is experimented on a relatively large-scale sports consortia. The system aims at the event detection scenario of an input video with an orderly sequential process. Initially, domain knowledge-independent local descriptors are extracted homogeneously from the input video sequence. Then the video representation is created by adopting a bag-of-visual-words (BoW) model. The video’s genre is first identified by applying the k-nearest neighbor (k-NN) classifiers on the initially obtained video representation, and various dissimilarity measures are assessed and evaluated analytically. Subsequently, an unsupervised probabilistic latent semantic analysis (PLSA)-based approach is employed at the same histogram-based video representation, characterizing each frame of video sequence into one of four view groups, namely closed-up-view, mid-view, long-view, and outer-field-view. Finally, a hidden conditional random field (HCRF) structured prediction model is utilized for interesting event detection. From experimental results, k-NN classifier using KL-divergence measurement demonstrates the best accuracy at 82.16% for genre categorization. Supervised SVM and unsupervised PLSA have average classification accuracies at 82.86% and 68.13%, respectively. The HCRF model achieves 92.31% accuracy using the unsupervised PLSA based label input, which is comparable with the supervised SVM based input at an accuracy of 93.08%. In general, such a systematic approach can be widely applied in processing massive videos generically.	bag-of-words model in computer vision;categorization;conditional random field;holism;k-nearest neighbors algorithm;kullback–leibler divergence;probabilistic latent semantic analysis;qp state machine frameworks;scalability;semi-supervised learning;structured prediction;unsupervised learning;video content analysis	Ning Zhang;Ling-yu Duan;Lingfang Li;Qingming Huang;Jun Du;Wen Gao;Ling Guan	2012	ACM TIST	10.1145/2168752.2168760	computer science;machine learning;pattern recognition;data mining;bag-of-words model in computer vision;probabilistic latent semantic analysis;k-nearest neighbors algorithm;conditional random field;domain knowledge	ML	34.51476045190576	-49.63977029688488	89557
b41d14ca81ab76f8528277e65bd2f57078796779	a discriminative deep model for pedestrian detection with occlusion handling	cuhk occlusion dataset occlusion handling part detectors probabilistic pedestrian detection framework deformable part based model part visibility probabilities discriminative deep model visibility relationship learning public datasets caltech eth daimler;detectors;probability;discriminative deep model;support vector machines;visibility relationship learning;training;daimler;cuhk occlusion dataset;probabilistic pedestrian detection framework;deformable models;caltech;deformable models detectors training estimation correlation support vector machines probabilistic logic;probability object detection pedestrians;part visibility probabilities;pedestrians;estimation;occlusion handling;deformable part based model;correlation;probabilistic logic;eth;object detection;public datasets;part detectors	Part-based models have demonstrated their merit in object detection. However, there is a key issue to be solved on how to integrate the inaccurate scores of part detectors when there are occlusions or large deformations. To handle the imperfectness of part detectors, this paper presents a probabilistic pedestrian detection framework. In this framework, a deformable part-based model is used to obtain the scores of part detectors and the visibilities of parts are modeled as hidden variables. Unlike previous occlusion handling approaches that assume independence among visibility probabilities of parts or manually define rules for the visibility relationship, a discriminative deep model is used in this paper for learning the visibility relationship among overlapping parts at multiple layers. Experimental results on three public datasets (Caltech, ETH and Daimler) and a new CUHK occlusion dataset1 specially designed for the evaluation of occlusion handling approaches show the effectiveness of the proposed approach.	hidden variable theory;object detection;part-based models;pedestrian detection;sensor	Wanli Ouyang;Xiaogang Wang	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6248062	support vector machine;computer vision;estimation;detector;simulation;machine learning;probability;mathematics;probabilistic logic;correlation;statistics	Vision	35.15763329542962	-47.15646163921164	89591
b13c73dc24fa6157c9d5f9276dad7b5af53bf756	efficient spatiotemporal-attention-driven shot matching	saliency map;shot similarity;focus of attention;motion saliency;visual attention	As human attention is an effective mechanism for information prioritizing and selecting, it provides a practical approach for intelligent shot similarity matching. In this paper, we propose an attention-driven video interpretation method using an efficient spatiotemporal attention detection framework. The motion attention detection in most existing methods is unstable and computationally expensive. Avoiding calculating motion explicitly, the proposed framework generates motion saliency using the rank deficiency of grayscale gradient tensors. To address an ill-posed weight determination problem, an adaptive fusion method is proposed for motion and spatial saliency integration by highlighting the more reliable saliency maps. An attention-drive matching strategy is proposed by converting attention values to importance factors, which subsequently boost the attended regions in region-based shot matching. A global feature-based matching strategy is also included the attention-driven strategy, to address cases where visual attention detection is less applicable. Experiment results demonstrate the advantages of the proposed method in similarity matching.	analysis of algorithms;angular defect;control theory;estimation theory;gradient;grayscale;map;well-posed problem	Shan Li;Moon-Chuen Lee	2007		10.1145/1291233.1291275	computer vision;machine learning;pattern recognition	Vision	37.52042339272978	-51.55076840442491	89720
9fe4ce0f6a9f685eef0269d4264b12bd544956d7	real-time estimation of hand gestures based on manifold learning from monocular videos	manifold learning;locality preserving projections;gesture recognition	Object pose estimation by manifold learning has become a hot research area recently. In this paper, we propose an efficient method that can recover pose and viewpoints for numerous hand gestures from monocular videos based on Locality Preserving Projections. We first select some hand dynamic gestures as primitive hand motions and set a 3D-2D mapping table to relate 3D joint angles of sampling static pose with their projective silhouettes from arbitrary viewpoints. Then the embedding space and explicit mapping function are learnt for every primitive motion. In order to make classification and prediction among those embedding spaces, a Subspace Filtering Algorithm is also proposed which can recognize and recover numerous hand dynamic gestures by the combination of primitive gestures. At last, by using skin color cues and oriented k-Dops, multi-hands can be labeled and tracked separately and accurately. Extensive experimental results demonstrate qualitatively and quantitatively that 3D pose recovery of hands can be achieved by our method robustly and efficiently.	algorithm;associative entity;format-preserving encryption;locality of reference;nonlinear dimensionality reduction;real-time transcription;sampling (signal processing);signal subspace;skin (computing)	Yali Wang;Zhongxuan Luo;JunCheng Liu;Xin Fan;Haojie Li;Yunzhen Wu	2013	Multimedia Tools and Applications	10.1007/s11042-013-1524-7	computer vision;computer science;machine learning;gesture recognition;nonlinear dimensionality reduction	Vision	36.47906162313646	-49.25939151703105	89774
25885e9292957feb89dcb4a30e77218ffe7b9868	analyzing the affect of a group of people using multi-modal framework		Millions of images on the web enable us to explore images from social events such as a family party, thus it is of interest to understand and model the affect exhibited by a group of people in images. But analysis of the affect expressed by multiple people is challenging due to varied indoor and outdoor settings, and interactions taking place between various numbers of people. A few existing works on Group-level Emotion Recognition (GER) have investigated on face-level information. Due to the challenging environments, face may not provide enough information to GER. Relatively few studies have investigated multi-modal GER. Therefore, we propose a novel multi-modal approach based on a new feature description for understanding emotional state of a group of people in an image. In this paper, we firstly exploit three kinds of rich information containing face, upperbody and scene in a group-level image. Furthermore, in order to integrate multiple person’s information in a group-level image, we propose an information aggregation method to generate three features for face, upperbody and scene, respectively. We fuse face, upperbody and scene information for robustness of GER against the challenging environments. Intensive experiments are performed on two challenging group-level emotion databases to investigate the role of face, upperbody and scene as well as multi-modal framework. Experimental results demonstrate that our framework achieves very promising performance for GER.	bottom-up proteomics;database;encode;emotion recognition;experiment;face detection;interaction;modal logic;top-down and bottom-up design	Xiaohua Huang;Abhinav Dhall;Xin Liu;Guoying Zhao;Jingang Shi;Roland Goecke;Matti Pietikäinen	2016	CoRR		computer vision	Vision	34.09004686467916	-50.610858630072705	89877
37c481a7e669bedc31c468857252e6b38bda1f9e	learning support order for manipulation in clutter	manipulators;inference mechanisms;robot vision;image colour analysis;clutter image segmentation robots grasping three dimensional displays object detection visualization;object object interaction manipulation support order learning surrounding objects support relationship cluttered environment photometric property geometric property inferred support relationship robotic applications object manipulation grasping picking from bin rgbd dataset red green blue depth;learning artificial intelligence;object detection;robot vision image colour analysis inference mechanisms learning artificial intelligence manipulators object detection	Understanding positional semantics of the environment plays an important role in manipulating an object in clutter. The interaction with surrounding objects in the environment must be considered in order to perform the task without causing the objects fall or get damaged. In this paper, we learn the semantics in terms of support relationship among different objects in a cluttered environment by utilizing various photometric and geometric properties of the scene. To manipulate an object of interest, we use the inferred support relationship to derive a sequence in which its surrounding objects should be removed while causing minimal damage to the environment. We believe, this work can push the boundary of robotic applications in grasping, object manipulation and picking-from-bin, towards objects of generic shape and size and scenarios with physical contact and overlap. We have created an RGBD dataset that consists of various objects used in day-to-day life present in clutter. We explore many different settings involving different kind of object-object interaction. We successfully learn support relationships and predict support order in these settings.	clutter;data structure;robot;tree traversal	Swagatika Panda;A. H. Abdul Hafez;C. V. Jawahar	2013	2013 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2013.6696444	computer vision;deep-sky object;computer science;artificial intelligence;object-oriented design	Robotics	35.936067692693534	-46.05219065952793	89931
84cde9b52c77f2a1684f1be7bc61d2677fbd66fe	classification of hyperspectral images with self organizing map		The aim of this paper is to demonstrate the feasibility of using Self Organizing Map for hyperspectral image classification. Self organizing maps are widely used for dimension reduction, clustering techniques and classification. This paper describes a modified SOM algorithm for learning process, and its use for hyperspectral classification.	algorithm;cluster analysis;computer vision;dimensionality reduction;organizing (structure);self-organizing map	Stéphane May	2013	2013 5th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)	10.1109/WHISPERS.2013.8080616	self-organizing map;dimensionality reduction;artificial intelligence;hyperspectral imaging;network topology;machine learning;cluster analysis;computer science;pattern recognition;contextual image classification	Robotics	30.904176967074562	-43.95723991879362	90053
afba76d0fe40e1be381182aec822431e20de8153	projection-optimal local fisher discriminant analysis for feature extraction	dimensionality reduction;feature extraction;local fisher discriminant analysis;linear discriminant analysis	In this paper, a novel dimensionality reduction algorithm called projection-optimal local Fisher discriminant analysis (PoLFDA) is proposed in order to address the multimodal problem. Novel weight matrices defined on the projected space can represent the intraclass compactness and the interclass separability. Based on the novel weighted matrices, the local between-class scatter matrix and the local within-class scatter matrix are defined such that the local structure can be preserved. In order to enhance the discriminant ability, we impose an orthogonal constraint on the objective function, which can be regarded as a trace ratio problem. In general, a trace ratio problem does not have a closed-form solution; however, it can be solved using some efficient iterative algorithms. Therefore, we optimize the projection matrix by solving the trace ratio problem iteratively. Experiments on toy data, face, and handwritten digit data sets are conducted to evaluate the performance of PoLFDA; the results and comparisons verify the effectiveness of the proposed method.	algorithm;database;dimensionality reduction;facial recognition system;feature extraction;graph embedding;iterative method;linear discriminant analysis;linear separability;loss function;mathematical optimization;multimodal interaction;numerical analysis;optimization problem;similarity measure	Zhan Wang;Qiuqi Ruan;Gaoyun An	2014	Neural Computing and Applications	10.1007/s00521-014-1768-9	kernel fisher discriminant analysis;feature extraction;computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis;multiple discriminant analysis;statistics;dimensionality reduction	AI	25.894052651377013	-41.09883262835751	90119
ea176146ff0f0639095c6fd47cd19056c96cd951	robust face recognition via double low-rank matrix recovery for feature extraction	minimisation;occlusion robust face recognition double low rank matrix recovery feature extraction low rank representation lrr model multiple subspace structure low rank subspace face image row space information column space information augmented lagrangian multiplier alm optimization problem nuclear norm minimization illumination;会议论文;matrix algebra;minimisation face recognition feature extraction image representation matrix algebra;face recognition;face recognition low rank representation sparse representation robust pca feature extraction;image representation;feature extraction	Feature extraction is one of the most fundamental problems in face recognition tasks. In this paper, motivated by low-rank representation (LRR) model on exploring the multiple subspace structures of observation data, we propose a double low-rank matrix recovery method to learn low-rank subspaces from face images, where it takes into account the recovery of row space and column space information simultaneously. Applying Augmented Lagrangian Multiplier (ALM), the optimization problem on minimization of nuclear norm is resolved efficiently. By evaluating on public face databases, experimental results show that our proposed method works much better than existing face recognition methods based on feature extraction. It is more robust to outliers, varying illumination and occlusion.	augmented lagrangian method;database;facial recognition system;feature extraction;hidden surface determination;lagrange multiplier;mathematical optimization;optimization problem	Ming Yin;Shuting Cai;Junbin Gao	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738777	facial recognition system;computer vision;minimisation;feature extraction;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	26.777588207244698	-41.97421936370625	90160
5903d30708e6883389f6b0c3aa13546896a4e566	cur from a sparse optimization viewpoint	statistical machine learning;convex optimization;randomized algorithm;data structure	The CUR decomposition provides an approximation of a matrix X that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of X. In this regard, it appears to be similar to many sparse PCA methods. However, CUR takes a randomized a lgorithmic approach, whereas most sparse PCA methods are framed as conv ex optimization problems. In this paper, we try to understand CUR from a spars e optimization viewpoint. We show that CUR is implicitly optimizing a spars e regression objective and, furthermore, cannot be directly cast as a sparse PC A method. We also observe that the sparsity attained by CUR possesses an inter esting structure, which leads us to formulate a sparse PCA method that achieves a CURlike sparsity.	approximation algorithm;column (database);computational resource;manifold regularization;mathematical optimization;principal component analysis;randomized algorithm;randomness;sparse pca;sparse approximation;sparse matrix;viewpoint	Jacob Bien;Ya Xu;Michael W. Mahoney	2010			mathematical optimization;sparse pca;convex optimization;data structure;computer science;machine learning;pattern recognition;sparse approximation;mathematics;randomized algorithm;programming language;algorithm	ML	26.340210458321657	-38.92534123013219	90372
87e3aea2141e225ad79cc2e6bc9dbba3a144e20a	face recognition with contiguous occlusion using linear regression and level set method	occlusion;linear regression;face recognition;level set method	Partial occlusions in face images pose a great challenge for most existing face recognition approaches. Although algorithms based on sparse representation and linear regression have demonstrated promising results about handling occlusion, the performance strongly depends on the way partition scheme is performed. In the present paper, we propose a novel method for face recognition against contiguous occlusion without using partition scheme. The general idea is to eliminate the impact of occlusions on the linear regression-based classification (LRC) method. In this approach, we first analyze that error image derived from the LRC is a better choice than original image for identifying occluded regions. Inspired by the level set methods that can provide smooth and closed contours as segmentation results which fit for the assumption of spatially continuity about occlusion, we present how to effectively use the spatial continuity of corrupted pixels to determine the occluded regions. By incorporating the idea of level set based image segmentation into the LRC, the proposed approach is capable of reliably determining the occluded regions and removing them from LRC framework. Extensive experiments on several publicly available databases (Extended Yale B, outdoor and AR) show the efficacy of the proposed approach against different types of occlusion.	facial recognition system;hidden surface determination	Xiao Luan;Bin Fang;Linghui Liu;Lifang Zhou	2013	Neurocomputing	10.1016/j.neucom.2013.06.014	facial recognition system;computer vision;computer science;linear regression;machine learning;pattern recognition;mathematics;level set method	Vision	32.91198575683406	-46.68463532110562	90456
ded41c9b027c8a7f4800e61b7cfb793edaeb2817	dyan: a dynamical atoms network for video prediction		The ability to anticipate the future is essential when making real time critical decisions, provides valuable information to understand dynamic natural scenes, and can help unsupervised video representation learning. State-of-art video prediction is based on LSTM recursive networks and/or generative adversarial network learning. These are complex architectures that need to learn large numbers of parameters, are potentially hard to train, slow to run, and may produce blurry predictions. In this paper, we introduce DYAN, a novel network with very few parameters and easy to train, which produces accurate, high quality frame predictions, significantly faster than previous approaches. DYAN owes its good qualities to its encoder and decoder, which are designed following concepts from systems identification theory and exploit the dynamicsbased invariants of the data. Extensive experiments using several standard video datasets show that DYAN is superior generating frames and that it generalizes well across domains.	activity recognition;display resolution;dynamical system;encoder;experiment;feature learning;invariant (computer science);long short-term memory;machine learning;recursion;sparse matrix;unsupervised learning;video processing	WenQian Liu;Abhishek Sharma;Octavia I. Camps;Mario Sznaier	2018	CoRR		machine learning;generative grammar;encoder;artificial intelligence;recursion;computer science;feature learning;exploit;invariant (mathematics)	AI	26.53579238462985	-51.3950292328541	90606
1121873326ab0c9f324b004aa0970a31d4f83eb8	robust facial landmark detection via a fully-convolutional local-global context network		While fully-convolutional neural networks are very strong at modeling local features, they fail to aggregate global context due to their constrained receptive field. Modern methods typically address the lack of global context by introducing cascades, pooling, or by fitting a statistical model. In this work, we propose a new approach that introduces global context into a fully-convolutional neural network directly. The key concept is an implicit kernel convolution within the network. The kernel convolution blurs the output of a local-context subnet, which is then refined by a global-context subnet using dilated convolutions. The kernel convolution is crucial for the convergence of the network because it smoothens the gradients and reduces overfitting. In a postprocessing step, a simple PCA-based 2D shape model is fitted to the network output in order to filter outliers. Our experiments demonstrate the effectiveness of our approach, outperforming several state-of-the-art methods in facial landmark detection.		Daniel Merget	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00088	subnet;overfitting;pooling;artificial intelligence;artificial neural network;pattern recognition;convolution;computer science;statistical model;context model;convergence (routing)	Vision	25.299421506211395	-49.10192275782592	90683
c146aa6d56233ce700032f1cb179700778557601	3d morphable models as spatial transformer networks		In this paper, we show how a 3D Morphable Model (i.e. a statistical model of the 3D shape of a class of objects such as faces) can be used to spatially transform input data as a module (a 3DMM-STN) within a convolutional neural network. This is an extension of the original spatial transformer network in that we are able to interpret and normalise 3D pose changes and self-occlusions. The trained localisation part of the network is independently useful since it learns to fit a 3D morphable model to a single image. We show that the localiser can be trained using only simple geometric loss functions on a relatively small dataset yet is able to perform robust normalisation on highly uncontrolled images including occlusion, self-occlusion and large pose changes.	algorithm;artificial neural network;autostereogram;convolutional neural network;fits;graphics processing unit;ground truth;loss function;multiview video coding;norm (social);statistical model;super-twisted nematic display;titan;transformer;uncontrolled format string;unsupervised learning	Anil Bas;Patrik Huber;William A. P. Smith;Muhammad Awais;Josef Kittler	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.110	computer vision;convolutional neural network;machine learning;artificial intelligence;transformer;computer science;statistical model;pattern recognition;solid modeling	Vision	26.635715541961734	-50.376182138827204	90752
3f1e9e377648dda8c0428965de35c9aa850f89e2	unsupervised image-to-image translation using domain-specific variational information bound		Unsupervised image-to-image translation is a class of computer vision problems which aims at modeling conditional distribution of images in the target domain, given a set of unpaired images in the source and target domains. An image in the source domain might have multiple representations in the target domain. Therefore, ambiguity in modeling of the conditional distribution arises, specially when the images in the source and target domains come from different modalities. Current approaches mostly rely on simplifying assumptions to map both domains into a shared-latent space. Consequently, they are only able to model the domaininvariant information between the two modalities. These approaches usually fail to model domain-specific information which has no representation in the target domain. In this work, we propose an unsupervised image-to-image translation framework which maximizes a domain-specific variational information bound and learns the target domain-invariant representation of the two domain. The proposed framework makes it possible to map a single source image into multiple images in the target domain, utilizing several target domain-specific codes sampled randomly from the prior distribution, or extracted from reference images.		Hadi Kazemi;Sobhan Soleymani;Fariborz Taherkhani;Seyed Mehdi Iranmanesh;Nasser M. Nasrabadi	2018			computer science;machine learning;modalities;ambiguity;pattern recognition;prior probability;artificial intelligence;conditional probability distribution;image translation	ML	24.767866891417714	-48.08859269882161	90888
099435e86e8929b12a82ea4757ebebab30250dc0	coloring of dt-mri fiber traces using laplacian eigenmaps	nuclear magnetic resonance imaging;dimensionalidad;euclidean theory;medical and health sciences;imagineria rmn;systeme nerveux central;visualizacion;espace euclidien;color space;medicin och halsovetenskap;laplacian;dimensionality;hombre;espacio euclidiano;percepcion;encefalo;sistema nervioso central;visualization;laplacien;laplaciano;visualisation;encephale;dimensionnalite;human;theorie euclidienne;euclidean space;imagerie rmn;encephalon;espace chromatique;espacio cromatico;perception;dimensional reduction;similarity measure;human brain;central nervous system;teoria euclidiana;homme	We propose a novel post processing method for visualization of fiber traces from DT-MRI data. Using a recently proposed non-linear dimensionality reduction technique, Laplacian eigenmaps [3], we create a mapping from a set of fiber traces to a low dimensional Euclidean space. Laplacian eigenmaps constructs this mapping so that similar traces are mapped to similar points, given a custom made pairwise similarity measure for fiber traces. We demonstrate that when the low-dimensional space is the RGB color space, this can be used to visualize fiber traces in a way which enhances the perception of fiber bundles and connectivity in the human brain.	cluster analysis;color space;digital footprint;laplacian matrix;nonlinear dimensionality reduction;nonlinear system;organizing (structure);similarity measure;tracing (software)	Anders Brun;Hae-Jeong Park;Hans Knutsson;Carl-Fredrik Westin	2003		10.1007/978-3-540-45210-2_47	computer vision;visualization;topology;computer science;mathematics;geometry	Visualization	29.777764055786562	-39.600225774701855	91041
9bc1abc2abfb21616a5c4b1d979f2b1b73d876e2	efficient stereo matching leveraging deep local and context information		Stereo matching is a challenging problem with respect to weak texture, discontinuities, illumination difference and occlusions. Therefore, a deep learning framework is presented in this paper, which focuses on the first and last stage of typical stereo methods: the matching cost computation and the disparity refinement. For matching cost computation, two patch-based network architectures are exploited to allow the trade-off between speed and accuracy, both of which leverage multi-size and multi-layer pooling unit with no strides to learn cross-scale feature representations. For disparity refinement, unlike traditional handcrafted refinement algorithms, we incorporate the initial optimal and sub-optimal disparity maps before outlier detection. Furthermore, diverse base learners are encouraged to focus on specific replacement tasks, corresponding to the smooth regions and details. Experiments on different datasets demonstrate the effectiveness of our approach, which is able to obtain sub-pixel accuracy and restore occlusions to a great extent. Specifically, our accurate framework attains near-peak accuracy both in non-occluded and occluded region and our fast framework achieves competitive performance against the fast algorithms on Middlebury benchmark.	algorithm;anomaly detection;benchmark (computing);binocular disparity;computation;computer stereo vision;deep learning;experiment;layer (electronics);map;pixel;refinement (computing);time complexity	Xiaoqing Ye;Jiamao Li;Han Wang;Hexiao Huang;Xiaolin Zhang	2017	IEEE Access	10.1109/ACCESS.2017.2754318	computer science;anomaly detection;artificial neural network;network architecture;deep learning;computation;computer vision;pooling;classification of discontinuities;artificial intelligence;pattern recognition	Vision	25.45676190678186	-51.140196863275655	91071
1a286b7c6d004ecc2b6dd02b22c0955dec20cc1b	semi-supervised regression with temporal image sequences	graph laplacian prior;graph theory;time of day estimation;kernel;atmospheric measurements;manifolds;training;mnist digits;scene estimation;indexing terms;regression analysis graph theory image sequences learning artificial intelligence;semi supervised learning;time of day;laplace equations;artificial neural networks;temporal image sequences;principal component analysis;image sequence;time of day estimation semi supervised regression temporal image sequences graph laplacian prior mnist digits;regression analysis;training laplace equations principal component analysis atmospheric measurements kernel manifolds artificial neural networks;graph laplacian;learning artificial intelligence;semi supervised regression;scene estimation semi supervised learning;image sequences	We consider a semi-supervised regression setting where we have temporal sequences of partially labeled data, under the assumption that the labels should vary slowly along a sequence, but that nearby points in input space may have drastically different labels. The setting ismotivated by problems such as determining the time of the day or the level of air visibility given an image of a landscape, which is hard because the time or visibility label is related in a complex way with the pixel values. We propose a regression framework regularized with a graph Laplacian prior, where the graph is given by the sequential information. We show this outperforms graphs learned in an unsupervised way for detecting the rotation of MNIST digits and estimating the time of day an image is captured, and provides modest improvement in the challenging visibility problem.	laplacian matrix;mnist database;pixel;semi-supervised learning;semiconductor industry;sensor;unsupervised learning;visibility (geometry)	Ling Xie;Miguel Á. Carreira-Perpiñán;Shawn D. Newsam	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5652612	computer vision;kernel;index term;laplacian matrix;manifold;computer science;graph theory;machine learning;pattern recognition;mathematics;regression analysis;principal component analysis	Vision	34.141066613192535	-44.05440112369283	91151
d102844a89009780b1a82c102d789d65ea359e06	neural network fusion of color, depth and location for object instance recognition on a mobile robot	indoor scene understanding;mobile robotics;instance recognition;semantic mapping;rgb d camera	The development of mobile robots for domestic assistance requires solving problems integrating ideas from different fields of research like computer vision, robotic manipulation, localization and mapping. Semantic mapping, that is, the enrichment a map with high-level information like room and object identities, is an example of such a complex robotic task. Solving this task requires taking into account hard software and hardware constraints brought by the context of autonomous mobile robots, where short processing times and low energy consumption are mandatory. We present a light-weight scene segmentation and object instance recognition algorithm using an RGB-D camera and demonstrate it in a semantic mapping experiment. Our method uses a feed-forward neural network to fuse texture, color and depth information. Running at 3 Hz on a single laptop computer, our algorithm achieves a recognition rate of 97% in a controlled environment, and 87% in the adversarial conditions of a real robotic task. Our results demonstrate that state of the art recognition rates on a database does not guarantee performance in a real world experiment. We also show the benefit in these conditions of fusing several recognition decisions and data from different sources. The database we compiled for the purpose of this study is publicly available.	algorithm;artificial neural network;autonomous robot;color depth;compiler;computer vision;feedforward neural network;gene ontology term enrichment;high- and low-level;internationalization and localization;laptop;mobile robot;semantic mapper;semantic mapping (statistics)	Louis-Charles Caron;David Filliat;Alexander Gepperth	2014		10.1007/978-3-319-16199-0_55	computer vision;simulation;computer science;machine learning	Robotics	29.76869251455591	-49.23892289803167	91156
d434e3fe31d823883d9559686eed48dcf0fdf059	multi-view human activity recognition based on silhouette and uniform rotation invariant local binary patterns		This paper addresses the problem of silhouette-based human activity recognition. Most of the previous work on silhouette based human activity recognition focus on recognition from a single view and ignores the issue of view invariance. In this paper, a system framework has been presented to recognize a view invariant human activity recognition approach that uses both contour-based pose features from silhouettes and uniform rotation local binary patterns for view invariant activity representation. The framework is composed of three consecutive modules: (1) detecting and locating people by background subtraction, (2) combined scale invariant contour-based pose features from silhouettes and uniform rotation invariant local binary patterns (LBP) are extracted, and (3) finally classifying activities of people by Multiclass Support vector machine (SVM) classifier. The rotation invariant nature of uniform LBP provides view invariant recognition of multi-view human activities. We have tested our approach successfully in the indoor and outdoor environment results on four multi-view datasets namely: our own view point dataset, VideoWeb Multi-view dataset [28], i3DPost multi-view dataset [29], and WVU multi-view human action recognition dataset [30]. The experimental results show that the proposed method of multi-view human activity recognition is robust, flexible and efficient.	activity recognition;background subtraction;belief propagation;feature extraction;local binary patterns;multiclass classification;sensor;support vector machine	Alok Kumar Singh Kushwaha;Subodh Srivastava;Rajeev Srivastava	2016	Multimedia Systems	10.1007/s00530-016-0505-x	computer vision;machine learning;pattern recognition	Vision	35.53746149287254	-50.519565311789975	91161
410ad524c2d9b0f833e2aee87a35dc2efc9c8b01	abnormal crowd behavior detection and localization using maximum sub-sequence search	video surveillance;anomaly detection;localization	This paper presents a novel framework for anomaly event detection and localization in crowded scenes. We propose an anomaly detector that extends the Bayes classifier from multi-class to one-class classification to characterize normal events. We also propose a localization scheme for anomaly localization as a maximum subsequence problem in a video sequence. The maximum subsequence algorithm locates an anomaly event by discovering the optimal collection of successive patches with spatial proximity over time without prior knowledge of the size, start and end of the anomaly event. Our localization scheme can locate multiple occurrences of abnormal events in spite of noise. Experimental results on the well-established UCSD dataset show that the proposed framework significantly outperforms state-of-the-art methods up to 53.55% localization rate. This study concludes that the localization framework plays an important role in abnormal event detection.	algorithm;anomaly detection;internationalization and localization;model-based testing;one-class classification;video post-processing;whole earth 'lectronic link	Kai-Wen Cheng;Yie-Tarng Chen;Wen-Hsien Fang	2013		10.1145/2510650.2510655	machine learning;pattern recognition;data mining;mathematics	AI	37.327783489494614	-46.53756271289981	91180
f0f9e696d7a2af3010a2d6394d993b1c24cae481	polarimetric sar image classification based on selective ensemble learning of sparse representation	matching pursuit algorithms;san francisco test area polarimetric sar image classification sparse representation sr based selective ensemble learning method structured dictionary pre set threshold polsar image data;training;image classification;scattering;polsar image classification selective ensemble learning sparse representation;genetic algorithms training scattering image classification dictionaries matching pursuit algorithms algorithm design and analysis;dictionaries;genetic algorithms;synthetic aperture radar geophysical image processing image classification radar polarimetry remote sensing by radar;algorithm design and analysis	This paper presents a sparse representation (SR)-based selective ensemble learning method for Polarimetric SAR image classification. Sparse representation uses the least dictionary atoms which come from a structured dictionary to represent the data, however, different training samples will lead to different options of the selected atoms and the corresponding coefficients, which will lead different subsequent classification results. Ensemble learning can be adopted to solve the issue, which uses several different learners to acquire a set of results that are integrated to get the final result. But, the result of each learner isn't all good. Therefore, the paper introduces selective ensemble learning which excludes the learners whose weights are smaller than the pre-set threshold. Experiments are conducted on the PolSAR image data of San Francisco test area to verify the performance of the proposed method.	coefficient;computer vision;data dictionary;ensemble learning;experiment;polarimetry;sparse approximation;sparse matrix	Cuijuan Han;Xiao Wang	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7730295	algorithm design;computer vision;contextual image classification;genetic algorithm;k-svd;computer science;machine learning;pattern recognition;scattering;physics	Vision	30.27777969284478	-43.615927579688105	91271
72544455ebb04b9836431cc695f2f902393a4dca	feature extraction for human action classification using adaptive key frame interval	histograms;history;time domain analysis;weizmann dataset feature extraction human action classification adaptive key frame interval spatial domain time domain key frame detection action cycle window length local orientation histogram key pose energy image motion history image kpei mhi;video signal processing feature extraction image classification object detection;abstracts;feature extraction;decision support systems time domain analysis history lifting equipment abstracts feature extraction histograms;decision support systems;lifting equipment	Human actions in video have the variation in both spatial and time domains which cause the difficulty for action classification. According to the nature of articulated body, an amount of movement from point-to-point is not constant, which can be illustrated as a bell-shape. In this paper, key frames are detected for specifying a starting and ending point for an action cycle. The time between key frames determines the window length for feature extraction in time domain. Since the cycles are varying, the key frame interval is varying and adaptive to performer and action. A local orientation histogram of Key Pose Energy Image (KPEI) and Motion History Image (MHI) is constructed during the period. The experimental results on WEIZMANN dataset demonstrate that the feature within the adaptive key frame interval can effectively classify actions.	action potential;feature extraction;feature vector;item unique identification;key frame;point-to-point protocol	Kanokphan Lertniphonphan;Supavadee Aramvith;Thanarat H. Chalidabhongse	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041766	computer vision;speech recognition;computer science;pattern recognition;feature	Vision	37.88079569812373	-49.5945581952094	91797
03d1d0a665e358863ff4de9ee7d78f64edd7e756	“who are you?” - learning person specific classifiers from video	kernel;tv;image classification;face recognition;face detection;web pages;data mining;motion pictures;tracking;detectors;feature extraction;labeling	"""We investigate the problem of automatically labelling faces of characters in TV or movie material with their names, using only weak supervision from automatically-aligned subtitle and script text. Our previous work (Everingham et al. [8]) demonstrated promising results on the task, but the coverage of the method (proportion of video labelled) and generalization was limited by a restriction to frontal faces and nearest neighbour classification. In this paper we build on that method, extending the coverage greatly by the detection and recognition of characters in profile views. In addition, we make the following contributions: (i) seamless tracking, integration and recognition of profile and frontal detections, and (ii) a character specific multiple kernel classifier which is able to learn the features best able to discriminate between the characters. We report results on seven episodes of the TV series """"Buffy the Vampire Slayer"""", demonstrating significantly increased coverage and performance with respect to previous methods on this material."""	javaserver faces;seamless3d;sensor;slayer	Josef Sivic;Mark Everingham;Andrew Zisserman	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206513	facial recognition system;computer vision;detector;contextual image classification;face detection;kernel;speech recognition;feature extraction;computer science;machine learning;pattern recognition;tracking	Vision	32.7036225395835	-51.83847444543372	91935
57cf6509377136530db3e72e4fd907665a6a4831	learning a combined model of visual saliency for fixation prediction	para boosting learner visual saliency model fixation prediction saliency detection accuracy late fusion strategies semantic analysis multi modal biometrics confidence scores;support vector machines;biological system modeling;semantics;visualization;computational modeling predictive models biological system modeling visualization feature extraction support vector machines semantics;computational modeling;feature extraction;learning artificial intelligence image fusion;predictive models;scene understanding saliency bottom up attention regions of interest eye movements score level fusion para boosting learner	A large number of saliency models, each based on a different hypothesis, have been proposed over the past 20 years. In practice, while subscribing to one hypothesis or computational principle makes a model that performs well on some types of images, it hinders the general performance of a model on arbitrary images and large-scale data sets. One natural approach to improve overall saliency detection accuracy would then be fusing different types of models. In this paper, inspired by the success of late-fusion strategies in semantic analysis and multi-modal biometrics, we propose to fuse the state-of-the-art saliency models at the score level in a para-boosting learning fashion. First, saliency maps generated by several models are used as confidence scores. Then, these scores are fed into our para-boosting learner (i.e., support vector machine, adaptive boosting, or probability density estimator) to generate the final saliency map. In order to explore the strength of para-boosting learners, traditional transformation-based fusion strategies, such as Sum, Min, and Max, are also explored and compared in this paper. To further reduce the computation cost of fusing too many models, only a few of them are considered in the next step. Experimental results show that score-level fusion outperforms each individual model and can further reduce the performance gap between the current models and the human inter-observer model.	adaboost;area striata structure;biometrics;computation;fuse device component;generalization (psychology);gradient boosting;inspiration function;map;max;modal logic;numerous;overfitting;predictive modelling;support vector machine	Jingwei Wang;Ali Borji;C.-C. Jay Kuo;Laurent Itti	2016	IEEE Transactions on Image Processing	10.1109/TIP.2016.2522380	support vector machine;computer vision;visualization;feature extraction;computer science;machine learning;pattern recognition;semantics;predictive modelling;computational model	ML	26.73289834845896	-48.19161737867911	91990
45f7995eb953876590d22ed41b9c1b7ece4fbbae	class-specific feature selection with local geometric structure and discriminative information based on sparse similar samples	feature extraction support vector machines remote sensing accuracy computational modeling object oriented modeling information retrieval;remote sensing artificial satellites feature selection geophysical image processing image resolution;support vector machines;information retrieval;accuracy;computational modeling;supervised feature selection class based features object oriented image analysis remote sensing;feature extraction;remote sensing;object oriented modeling;discriminative information very high resolution vhr remote sensing images class specific feature selection method based on sparse similar sample cfs4 geometrical structure sparsity regularization problem vhr satellite image	It is necessary while quite challenging to select features strongly relevant to a thematic class, i.e., class-specific features, from very high resolution (VHR) remote sensing images. To meet this challenge, a class-specific feature selection method based on sparse similar samples (CFS4) is proposed. Specifically, CFS4 incorporates the local geometrical structure and discriminative information of the data into a sparsity regularization problem. The experimental results on VHR satellite images well validate the effectiveness and practicability of the proposed method.	feature selection;image resolution;matrix regularization;sparse matrix	Xi Chen;Yanfeng Gu	2015	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2015.2402205	support vector machine;computer vision;feature extraction;computer science;machine learning;pattern recognition;accuracy and precision;computational model;feature;remote sensing	AI	31.079937282644256	-43.800813238107416	92275
4a923b3454ba1266738911a10c2fe1eed303fc3d	modelling of interactions for the recognition of activities in groups of people		Abstract In this research study we adopt a probabilistic modelling of interactions in groups of people, using video sequences, leading to the recognition of their activities. Firstly, we model short smooth streams of localised movement. Afterwards, we partition the scene in regions of distinct movement, by using maximum a posteriori estimation, by fitting Gaussian Mixture Models (GMM) to the movement statistics. Interactions between moving regions are modelled using the Kullback–Leibler (KL) divergence between pairs of statistical representations of moving regions. Such interactions are considered with respect to the relative movement, moving region location and relative size, as well as to the dynamics of the movement and location inter-dependencies, respectively. The proposed methodology is assessed on two different data sets showing different categories of human interactions and group activities.	closed-circuit television;deep learning;estimation theory;expectation–maximization algorithm;human–computer interaction;interaction;kernel density estimation;kullback–leibler divergence;real life;stationary process;streaming media;timeline	Kyle Stephens;Adrian G. Bors	2018	Digital Signal Processing	10.1016/j.dsp.2018.03.021	divergence;streams;artificial intelligence;mixture model;pattern recognition;mathematics;probabilistic logic;partition (number theory);maximum a posteriori estimation;social group;data set	HCI	37.6534384027922	-48.659985235867154	92392
f6071eb167ac70099d357976c0a065ad685193b8	an svm ensemble approach combining spectral, structural, and semantic features for the classification of high-resolution remotely sensed imagery	geophysical image processing;high resolution;image resolution;support vector machines;image classification;support vector machines feature extraction geophysical image processing geophysical techniques hyperspectral imaging image classification image resolution remote sensing;classification;multifeature;worldview 2 classification feature extraction high resolution morphological multifeature object based semantic support vector machines svms;accuracy;probabilistic models svm ensemble approach semantic features structural features spectral features high resolution remotely sensed image classification spectral domains spatial domains spatial information spectral information multifeature model support vector machine gray level cooccurrence matrix differential morphological profiles urban complexity index object based semantic approach probabilistic fusion approach vector stacking hyperspectral digital imagery collection experiment dc mall data set worldview 2 data set semantic based postprocessing;vectors;feature extraction;remote sensing;semantic;support vector machines hyperspectral imaging accuracy feature extraction spatial resolution vectors;worldview 2;hyperspectral imaging;morphological;support vector machines svms;object based;geophysical techniques;spatial resolution	In recent years, the resolution of remotely sensed imagery has become increasingly high in both the spectral and spatial domains, which simultaneously provides more plentiful spectral and spatial information. Accordingly, the accurate interpretation of high-resolution imagery depends on effective integration of the spectral, structural and semantic features contained in the images. In this paper, we propose a new multifeature model, aiming to construct a support vector machine (SVM) ensemble combining multiple spectral and spatial features at both pixel and object levels. The features employed in this study include a gray-level co-occurrence matrix, differential morphological profiles, and an urban complexity index. Subsequently, three algorithms are proposed to integrate the multifeature SVMs: certainty voting, probabilistic fusion, and an object-based semantic approach, respectively. The proposed algorithms are compared with other multifeature SVM methods including the vector stacking, feature selection, and composite kernels. Experiments are conducted on the hyperspectral digital imagery collection experiment DC Mall data set and two WorldView-2 data sets. It is found that the multifeature model with semantic-based postprocessing provides more accurate classification results (an accuracy improvement of 1-4% for the three experimental data sets) compared to the voting and probabilistic models.	algorithm;co-occurrence matrix;complexity index;document-term matrix;experiment;feature selection;focus stacking;image resolution;object-based language;pixel;support vector machine	Xin Huang;Liangpei Zhang	2013	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2012.2202912	computer vision;image resolution;machine learning;pattern recognition;remote sensing	Vision	30.988532149517116	-44.425110032164106	92694
bafed008ce998e664154567cf8965946f4dacef6	recognition human by gait using pca, dtw		With the increasing demand of visual surveillance systems, human recognition at a distance has gained extensive research interest. Gait is a potential behavioral feature to identify humans based on their motion. This paper describes a new scheme for extracting and selecting features from the gait of a human for recognition. The scheme combines two methods: principal component analysis (PCA) and dynamic time warping (DTW). PCA is applied to remove correlation between the features and also to reduce its dimensionality of the data features. These extracted feature vectors are used to recognizing the individuals. Dynamic time warping is used to recognize the individual. Firstly, the binary silhouette of a walking person is detected from each frame of the monocular image sequences. Then we divide the two dimensional silhouette of the walker into 7 parts in order to calculate the angle between front and back thigh. Finally, similarity measurement based on the gait cycles is calculed to recognize the different subjects. The result of experiments conducted on CASIA-A gait database show that the proposed gait recognition approach can obtain encouraging accurate recognition rate.	amiga walker;dynamic time warping;experiment;feature vector;gait analysis;principal component analysis	Soumia Benbakreti;Mohammed Benyettou	2011			silhouette;dynamic time warping;gait;curse of dimensionality;principal component analysis;pattern recognition;feature vector;artificial intelligence;computer science	Vision	36.7442965738562	-50.26293044456486	92881
2b507f659b341ed0f23106446de8e4322f4a3f7e	deep identity-aware transfer of facial attributes		This paper presents a Deep convolutional network model for Identity-Aware Transfer (DIAT) of facial attributes. Given the source input image and the reference attribute, DIAT aims to generate a facial image (i.e., target image) that not only owns the reference attribute but also keep the same or similar identity to the input image. We develop a two-stage scheme to transfer the input image to each reference attribute label. A feed-forward transform network is first trained by combining perceptual identity-aware loss and GAN-based attribute loss, and a face enhancement network is then introduced to improve the visual quality. We further define perceptual identity loss on the convolutional feature maps of the attribute discriminator, resulting in a DIAT-A model. Our DIAT and DIAT-A models can provide a unified solution for several representative facial attribute transfer tasks such as expression transfer, accessory removal, age progression, and gender transfer. The experimental results validate their effectiveness. Even for some identity-related attribute (e.g., gender), our DIAT-A can obtain visually impressive results by changing the attribute while retaining most identity features of the source image.	attribute grammar;color gradient;discriminator;experiment;map;network model	Mu Li;Wangmeng Zuo;David Zhang	2016	CoRR		computer vision;computer science;machine learning;pattern recognition;data mining	Vision	25.331086428438226	-50.30553284542413	92882
69abd57a49c6b430a83d9a1e09dce5a347c9c63e	face recognition from multiple images per subject	set classification;set distance metric learning;face recognition	For face recognition, we show that knowing that each subject corresponds to multiple face images can improve classification performance. For domains such as video surveillance, it is easy to deduce which group of images belong to the same subject; in domains such as family album identification, we lose group membership information but there is still a group of images for each subject. We define these two types of problems as multiple faces per subject. In this paper, we propose a Bipart framework to take advantage of this group information in the testing set as well as in the training set. From these two sources of information, two models are learned independently and combined to form a unified discriminative distance space. Furthermore, this framework is generalized to allow both subspace learning and distance metric learning methods to take advantage of this group information. Bipart is evaluated on the multiple faces per subject problem using several benchmark datasets, including video and static image data, subjects of various ages, various lighting conditions, and many facial expressions. Comparisons against state-of-the-art distance and subspace learning methods demonstrate much better performance when utilizing group information with the Bipart framework.	benchmark (computing);closed-circuit television;facial recognition system;protein family;test set	Yang Mu;Henry Z. Lo;Wei Ding;Dacheng Tao	2014		10.1145/2647868.2655054	facial recognition system;computer vision;computer science;machine learning;pattern recognition	Vision	27.052404259374722	-46.044797565914244	92897
5d85db96f7af20e6c009bc11cbe4619304fd6229	robust fall detection using human shape and multi-class support vector machine	geriatrics;histograms;intelligent video surveillance;video surveillance;image motion analysis;image segmentation;support vector machines;magnetic heads;elderly;video surveillance feature extraction geriatrics health hazards image classification image motion analysis image segmentation object detection pose estimation shape recognition support vector machines;behavior detection;image classification;shape recognition;robustness humans shape support vector machines senior citizens hazards intelligent systems machine intelligence video surveillance histograms;multi class support vector machine;human shape variation;feature vector;segmented silhouette;shape;movement pattern;range of motion;multiclass support vector machine;health hazard;feature extraction;human body;multi class support vector machine human fall detection posture recognition human shape variation;projection histogram;posture recognition;physical psychological consequence;robust fall detection;humans;independent living;support vector machine;intelligent video surveillance system;discrete fourier transforms;health hazards;head pose;object detection;motion classification;temporal change;motion classification robust fall detection multiclass support vector machine physical psychological consequence elderly health hazard intelligent video surveillance system human fall detection human shape variation projection histogram segmented silhouette head pose behavior detection feature extraction;human fall detection;pose estimation	Falls and resulting physical-psychological consequences in the elderly are a major health hazard and a serious obstacle for independent living. So development of intelligent video surveillance systems is so important due to providing safe and secure environments. To this end, this paper proposes a novel approach for human fall detection based on human shape variation. Combination of best-fit approximated ellipse around the human body, projection histograms of the segmented silhouette and temporal changes of head pose, would provide a useful cue for detection different behaviors. Extracted feature vectors are finally fed to a multi-class support vector machine for precise classification of motions and determination of a fall event. Unlike existent fall detection systems that only deal with limited movement patterns, we considered wide range of motions consisting of normal daily life activities, abnormal behaviors and also unusual events. Reliable recognition rate of experimental results underlines satisfactory performance of our system.	activity recognition;approximation algorithm;closed-circuit television;curve fitting;experiment;feature vector;hidden surface determination;kinesiology;mathematical optimization;oddworld: abe's oddysee;open agent architecture;optimization problem;projection-slice theorem;support vector machine	Homa Foroughi;Alireza Rezvanian;Amirhossien Paziraee	2008	2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing	10.1109/ICVGIP.2008.49	computer vision;engineering;machine learning;pattern recognition	Vision	38.9240135979177	-49.534896852018136	92900
98720e3b5b36160a73c09086afc88405f7ed517f	jointly learning structured analysis discriminative dictionary and analysis multiclass classifier		"""In this paper, we propose an analysis mechanism-based structured analysis discriminative dictionary learning analysis discriminative dictionary learning, framework. The ADDL seamlessly integrates ADDL, analysis representation, and analysis classifier training into a unified model. The applied analysis mechanism can make sure that the learned dictionaries, representations, and linear classifiers over different classes are independent and discriminating as much as possible. The dictionary is obtained by minimizing a reconstruction error and an analytical incoherence promoting term that encourages the subdictionaries associated with different classes to be independent. To obtain the representation coefficients, ADDL imposes a sparse <inline-formula> <tex-math notation=""""LaTeX"""">$l_{2,1}$ </tex-math></inline-formula>-norm constraint on the coding coefficients instead of using <inline-formula> <tex-math notation=""""LaTeX"""">$l_{0}$ </tex-math></inline-formula> or <inline-formula> <tex-math notation=""""LaTeX"""">$l_{1}$ </tex-math></inline-formula> norm, since the <inline-formula> <tex-math notation=""""LaTeX"""">$l_{0}$ </tex-math></inline-formula>- or <inline-formula> <tex-math notation=""""LaTeX"""">$l_{1}$ </tex-math></inline-formula>-norm constraint applied in most existing DL criteria makes the training phase time consuming. The code-extraction projection that bridges data with the sparse codes by extracting special features from the given samples is calculated via minimizing a sparse code approximation term. Then we compute a linear classifier based on the approximated sparse codes by an analysis mechanism to simultaneously consider the classification and representation powers. Thus, the classification approach of our model is very efficient, because it can avoid the extra time-consuming sparse reconstruction process with trained dictionary for each new test data as most existing DL algorithms. Simulations on real image databases demonstrate that our ADDL model can obtain superior performance over other state of the arts."""	add3 wt allele;approximation algorithm;class;code;coefficient;computer simulation;dictionary [publication type];linear classifier;machine learning;multiclass classification;naive bayes classifier;neural coding;power (psychology);promotion (action);published database;reconstruction filter;sparse matrix;structured analysis;test data;unified model	Zhaoxing Zhang;Weiming Jiang;Jie Qin;Li Zhang;Fanzhang Li;Min Zhang;Shuicheng Yan	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2017.2740224	coding (social sciences);machine learning;artificial intelligence;discriminative model;pattern recognition;computer science;structured analysis;real image;encoding (memory);k-svd;classifier (linguistics);linear classifier	ML	24.64187090314847	-45.08996782115574	92917
0025bf3dcd7d321713e8f7cf6ac34b2652494d30	exploiting transfer learning for personalized view invariant gesture recognition	video streaming gesture recognition human computer interaction mobile robots;transfer learning gesture recognition low cost cameras view invariance;support vector machines gesture recognition computer vision accuracy robustness cameras feature extraction;self similarity matrix descriptors transfer learning personalized view invariant gesture recognition robust gesture recognition human computer interaction portable devices video streams mobile device mobile robot user personalization gesture interaction systems domain adaptation framework feature space augmentation	A robust gesture recognition system is an essential component in many human-computer interaction applications. In particular, the widespread adoption of portable devices and the diffusion of autonomous systems with limited power and load capacity has increased the need of developing efficient recognition algorithms which operates on video streams recorded from low cost devices and which can cope with the challenging issue of point of view changes. A further challenge arises as different users tend to perform the same gesture with different styles and speeds. Thus a classifier trained with gestures data of certain set of users may work poorly when data from other users are being processed. However, as often a mobile device or a robot are intended to be used by a single or by a small group of people, it would be desirable to have a gesture recognition system designed specifically for these users. In this paper we introduce a novel approach to face the problems of view-invariance and user personalization in the context of gesture interaction systems. More specifically, we propose a domain adaptation framework based on a feature space augmentation approach operating on robust view-invariant Self Similarity Matrix descriptors. To prove the effectiveness of our method a dataset corresponding to 17 users performing 10 different gestures under 3 point of views is collected and an extensive experimental evaluation is performed.	algorithm;autonomous system (internet);domain adaptation;feature vector;gesture recognition;human–computer interaction;mobile device;personal digital assistant;personalization;point of view (computer hardware company);self-similarity;statistical classification;streaming media	Gabriele Costante;Valerio Galieni;Yuqing Chen;Mario Luca Fravolini;Elisa Ricci;Paolo Valigi	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6853797	computer vision;computer science;gesture recognition;multimedia;sketch recognition;interaction technique	Robotics	36.72402178674719	-46.263485001108364	93112
043c71a25dedacbbf95b02d99716a89dc11fef7a	end-to-end weakly-supervised semantic alignment		We tackle the task of semantic alignment where the goal is to compute dense semantic correspondence aligning two images depicting objects of the same category. This is a challenging task due to large intra-class variation, changes in viewpoint and background clutter. We present the following three principal contributions. First, we develop a convolutional neural network architecture for semantic alignment that is trainable in an end-to-end manner from weak image-level supervision in the form of matching image pairs. The outcome is that parameters are learnt from rich appearance variation present in different but semantically related images without the need for tedious manual annotation of correspondences at training time. Second, the main component of this architecture is a differentiable soft inlier scoring module, inspired by the RANSAC inlier scoring procedure, that computes the quality of the alignment based on only geometrically consistent correspondences thereby reducing the effect of background clutter. Third, we demonstrate that the proposed approach achieves state-of-the-art performance on multiple standard benchmarks for semantic alignment.		Ignacio Rocco;Relja Arandjelovic;Josef Sivic	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00723	differentiable function;task analysis;pattern recognition;artificial intelligence;ransac;machine learning;convolutional neural network;semantics;architecture;computer science;feature extraction;annotation	Vision	30.073968999331417	-49.79843335102199	93137
388b345e4d9e306e7072d0d2604fcc385d32acf4	dynamic feature learning for partial face recognition		Partial face recognition (PFR) in unconstrained environment is a very important task, especially in video surveillance, mobile devices, etc. However, a few studies have tackled how to recognize an arbitrary patch of a face image. This study combines Fully Convolutional Network (FCN) with Sparse Representation Classification (SRC) to propose a novel partial face recognition approach, called Dynamic Feature Matching (DFM), to address partial face images regardless of size. Based on DFM, we propose a sliding loss to optimize FCN by reducing the intra-variation between a face patch and face images of a subject, which further improves the performance of DFM. The proposed DFM is evaluated on several partial face databases, including LFW, YTF and CASIA-NIR-Distance databases. Experimental results demonstrate the effectiveness and advantages of DFM in comparison with state-of-the-art PFR methods.		Lingxiao He;Haiqing Li;Qi Zhang;Zhenan Sun	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00737	microsoft windows;computer vision;artificial intelligence;feature extraction;facial recognition system;sparse approximation;mobile device;design for manufacturability;convolution;pattern recognition;computer science;feature learning	Vision	29.933984635378714	-51.39211105110581	93170
8582fd09981cb5fe9729b2014112674c2e991c1f	on the performance of convnet features for place recognition	semantics;feature extraction robustness visualization real time systems semantics computer vision;convnet feature semantic place categorization optimization technique semantic search space partitioning locality sensitive hashing condition invariance viewpoint invariance slam visual navigation robotic field convolutional network computer vision place recognition;computer vision;visualization;feature extraction;slam robots neural nets object recognition optimisation robot vision search problems;robustness;real time systems	After the incredible success of deep learning in the computer vision domain, there has been much interest in applying Convolutional Network (ConvNet) features in robotic fields such as visual navigation and SLAM. Unfortunately, there are fundamental differences and challenges involved. Computer vision datasets are very different in character to robotic camera data, real-time performance is essential, and performance priorities can be different. This paper comprehensively evaluates and compares the utility of three state-of-the-art ConvNets on the problems of particular relevance to navigation for robots; viewpoint-invariance and condition-invariance, and for the first time enables real-time place recognition performance using ConvNets with large maps by integrating a variety of existing (locality-sensitive hashing) and novel (semantic search space partitioning) optimization techniques. We present extensive experiments on four real world datasets cultivated to evaluate each of the specific challenges in place recognition. The results demonstrate that speed-ups of two orders of magnitude can be achieved with minimal accuracy degradation, enabling real-time performance. We confirm that networks trained for semantic place categorization also perform better at (specific) place recognition when faced with severe appearance changes and provide a reference for which networks and layers are optimal for different aspects of the place recognition problem.	categorization;computer vision;convolutional neural network;deep learning;elegant degradation;experiment;high- and low-level;locality of reference;locality-sensitive hashing;machine vision;map;mathematical optimization;pan–tilt–zoom camera;real-time clock;real-time locating system;relevance;robot;semantic search;simultaneous localization and mapping;space partitioning	Niko Sünderhauf;Sareh Shirazi;Feras Dayoub;Ben Upcroft;Michael Milford	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353986	computer vision;visualization;feature extraction;computer science;artificial intelligence;machine learning;semantics;robustness	Robotics	30.65017904842062	-50.48010823730627	93336
57480437ddc88d106e869dbcb942abc3d72a52b5	hierarchical convolutional features for end-to-end representation-based visual tracking		Recently, deep learning is widely developed in computer vision applications. In this paper, a novel simple tracker with deep learning is proposed to complete the tracking task. A simple fully convolutional Siamese network is applied to capture the similarity between different frames. Nevertheless, the detailed information from lower layers, which is also important for locating the target object, is not considered into the tracking task. In this paper, the detailed information from two lower layers is considered into the response map to improve the performance and not to increase much time spent. This leads more significant improvement for feature representation and localization of the target object. The experimental results demonstrate that the proposed algorithm is efficient and robust compared with the baseline and the state-of-the-art trackers.	algorithm;baseline (configuration management);computer vision;deep learning;end-to-end encryption;end-to-end principle;experiment;internationalization and localization;video tracking	Suguo Zhu;Zhenying Fang;Fei Gao	2018	Machine Vision and Applications	10.1007/s00138-018-0947-6	computer science;computer vision;deep learning;end-to-end principle;bittorrent tracker;eye tracking;artificial intelligence	Vision	30.385554157036662	-50.90264764237725	93347
2111d546ac1cbf170302e44a17c88d26b1c55999	adadnns: adaptive ensemble of deep neural networks for scene text recognition		Recognizing text in the wild is a really challenging task because of complex backgrounds, various illuminations and diverse distortions, even with deep neural networks (convolutional neural networks and recurrent neural networks). In the end-to-end training procedure for scene text recognition, the outputs of deep neural networks at different iterations are always demonstrated with diversity and complementarity for the target object (text). Here, a simple but effective deep learning method, an adaptive ensemble of deep neural networks (AdaDNNs), is proposed to simply select and adaptively combine classifier components at different iterations from the whole learning system. Furthermore, the ensemble is formulated as a Bayesian framework for classifier weighting and combination. A variety of experiments on several typical acknowledged benchmarks, i.e., ICDAR Robust Reading Competition (Challenge 1, 2 and 4) datasets, verify the surprised improvement from the baseline DNNs, and the effectiveness of AdaDNNs compared with the recent state-of-	adaptive grammar;artificial neural network;baseline (configuration management);benchmark (computing);complementarity theory;convolutional neural network;deep learning;distortion;end-to-end principle;experiment;grand challenges;international conference on document analysis and recognition;iteration;neural network software;object detection;optical character recognition;outline of object recognition;recurrent neural network;snapshot isolation;speech recognition	Chun Yang;Xu-Cheng Yin;Zejun Li;Jianwei Wu;Chunchao Guo;Hongfa Wang;Lei Xiao	2017	CoRR		computer science;machine learning;convolutional neural network;pattern recognition;artificial neural network;deep learning;recurrent neural network;artificial intelligence;weighting	AI	28.63273067501506	-50.56876849086467	93549
a9fc8efd1aa3d58f89c0f53f0cb112725b5bda10	three-dimensional attention-based deep ranking model for video highlight detection		The video highlight detection task is to localize key elements (moments of user's major or special interest) in a video. Most of existing highlight detection approaches extract features from the video segment as a whole without considering the difference of local features both temporally and spatially. Due to the complexity of video content, this kind of mixed features will impact the final highlight prediction. In temporal extent, not all frames are worth watching because some of them only contain the background of the environment without human or other moving objects. In spatial extent, it is similar that not all regions in each frame are highlights especially when there are lots of clutters in the background. To solve the above problem, we propose a novel three-dimensional (3-D) (spatial+temporal) attention model that can automatically localize the key elements in a video without any extra supervised annotations. Specifically, the proposed attention model produces attention weights of local regions along both the spatial and temporal dimensions of the video segment. The regions of key elements in the video will be strengthened with large weights. Thus, the more effective feature of the video segment is obtained to predict the highlight score. The proposed 3-D attention scheme can be easily integrated into a conventional end-to-end deep ranking model that aims to learn a deep neural network to compute the highlight score of each video segment. Extensive experimental results on the YouTube and SumMe datasets demonstrate that the proposed approach achieves significant improvement over state-of-the-art methods. With the proposed 3-D attention model, video highlights can be accurately retrieved in spatial and temporal dimensions without human supervision in several domains, such as gymnastics, parkour, skating, skiing, surfing, and dog activities, on the public datasets.	artificial neural network;deep learning;digital video;end-to-end principle;temporal logic	Yifan Jiao;Zhetao Li;Shucheng Huang;Xiaoshan Yang;Bin Liu;Tianzhu Zhang	2018	IEEE Transactions on Multimedia	10.1109/TMM.2018.2815998	task analysis;pattern recognition;computer science;special interest group;artificial intelligence;feature extraction;support vector machine;artificial neural network;solid modeling;ranking	Vision	31.67242215692487	-51.09890945715523	93717
709d7f6b86c01fe90f10ae9216a91f95b1dcd2fb	mixture statistic metric learning for robust human action and expression recognition		Background objects and textures in real-world video sequences often pose great challenges for human action and facial expression recognition. This paper proposes a mixture statistic metric learning for recognizing human actions and facial expressions in realistic “in the wild” scenarios. In the proposed method, multiple statistics, including temporal means and covariance matrices, as well as parameters of spatial Gaussian mixture distributions, are explicitly mapped to or generated on symmetric positive definite Riemannian manifolds. An implicit mixture of Mahalanobis metrics is learned from the Riemannian manifolds. The learned metrics place similar pairs in local neighborhoods and dissimilar pairs in relatively orthogonal regions on a regularized manifold. The proposed metric learning method also explores the prior distributions within the multiple statistics in the video sequences. The proposed method is tested on five action video data sets and three facial expression data sets and is compared with various state-of-the-art methods. Recognition accuracy and computational efficiency are evaluated in terms of average recognition rates and computational times in seconds, respectively. Competitive performances achieved on both action and facial expression recognition tasks demonstrate the effectiveness of the proposed method.	computation;performance;uml state machine	Shuanglu Dai;Hong Man	2018	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2017.2772026	mahalanobis distance;manifold;kernel (linear algebra);pattern recognition;statistic;facial recognition system;computer science;covariance;facial expression;machine learning;data set;artificial intelligence	Vision	35.1249062444787	-48.044766847011	93883
128d99fbe91b0ee593d70f3e78fe582aaa9f8ded	residual encoder decoder network and adaptive prior for face parsing		Face parsing assigns every pixel in a facial image with a semantic label, which could be applied in various applications including face expression recognition, facial beautification, affective computing and animation. While lots of progress have been made in this field, current state-of-the-art methods still fail to extract real effective feature and restore accurate score map, especially for those facial parts which have large variations of deformation and fairly similar appearance, e.g. mouth, eyes and thin eyebrows. In this paper, we propose a novel pixel-wise face parsing method called Residual Encoder Decoder Network (RED-Net), which combines a feature-rich encoder-decoder framework with adaptive prior mechanism. Our encoder-decoder framework extracts feature with ResNet and decodes the feature by elaborately fusing the residual architectures into deconvolution. This framework learns more effective feature comparing to that learnt by decoding with interpolation or classic deconvolution operations. To overcome the appearance ambiguity between facial parts, an adaptive prior mechanism is proposed in term of the decoder prediction confidence, allowing refining the final result. The experimental results on two public databases demonstrate that our method outperforms the state-of-thearts significantly, achieving improvements of F-measure from 0.854 to 0.905 on the Helen dataset, and pixel accuracy from 95.12% to 97.59% on the LFW dataset. In particular, convincing qualitative examples show that our method parses eye, eyebrow and lip regions more accurately.	affective computing;database;deconvolution;encoder;interpolation;mac os x 10.3 panther;parsing;pixel;software feature	Tianchu Guo;Youngsung Kim;Hui Zhang;Deheng Qian;ByungIn Yoo;Jingtao Xu;Dongqing Zou;Jae-Joon Han;Changkyu Choi	2018			residual;encoder;machine learning;artificial intelligence;parsing;computer science	Vision	26.61725753538392	-50.89080486617858	93886
0b56e42d4450fc01ed354ecad5912885a50fea70	detecting closely spaced and occluded pedestrians using specialized deep models for counting		Pedestrian detection is an important task in surveillance applications and becomes particularly challenging when pedestrians are close together or occluding one another. This paper presents a novel approach to detect pedestrians in such challenging scenarios. A deep convolutional neural network trained for counting is specialized to count one pedestrian. The feature extractor learned thereby is exploited to detect one pedestrian at a time iteratively. For the base counting model and the specialization, extensive annotation efforts are not required since only a single number at the image level is used. Use of our method on pedestrian datasets with occlusion showed an improvement in the average miss rate values as compared to other methods for handling occlusion.	artificial neural network;convolutional neural network;iteration;iterative method;minimum bounding box;partial template specialization;pedestrian detection;pixel;randomness extractor;sensor	Sanjukta Ghosh;Peter Amon;Andreas Hutter;André Kaup	2017	2017 IEEE Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2017.8305064	convolutional neural network;artificial intelligence;computer vision;pedestrian;extractor;computer science;pedestrian detection	Vision	29.796470082535528	-51.41319376755384	93956
2b55dcb7383348411e519c21e7ae639561984548	projective robust nonnegative factorization	robust;face recognition;nonnegative matrix factorization;graph regularization	Nonnegative matrix factorization (NMF) has been successfully used in many fields as a low-dimensional representation method. Projective nonnegative matrix factorization (PNMF) is a variant of NMF that was proposed to learn a subspace for feature extraction. However, both original NMF and PNMF are sensitive to noise and are unsuitable for feature extraction if data is grossly corrupted. In order to improve the robustness of NMF, a framework named Projective Robust Nonnegative Factorization (PRNF) is proposed in this paper for robust image feature extraction and classification. Since learned projections can weaken noise disturbances, PRNF is more suitable for classification and feature extraction. In order to preserve the geometrical structure of original data, PRNF introduces a graph regularization term which encodes geometrical structure. In the PRNF framework, three algorithms are proposed that add a sparsity constraint on the noise matrix based on L 1 / 2 norm, L 1 norm, and L 2 , 1 norm, respectively. Robustness and classification performance of the three proposed algorithms are verified with experiments on four face image databases and results are compared with state-of-the-art robust NMF-based algorithms. Experimental results demonstrate the robustness and effectiveness of the algorithms for image classification and feature extraction. © 2016 Elsevier Inc. All rights reserved.	algorithm;computer vision;database;experiment;feature (computer vision);feature extraction;non-negative matrix factorization;sparse matrix;taxicab geometry	Yuwu Lu;Yong Xu;Jane You;Xuelong Li;Chun Yuan	2016	Inf. Sci.	10.1016/j.ins.2016.05.001	facial recognition system;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;non-negative matrix factorization	AI	25.34633603725623	-42.28318980632564	93975
1ce7b042e67d909ccf6088c30bbf095dfe53597d	face recognition using sparse representations and manifold learning	manifold learning;computer vision;face recognition;sparse representation;dimensional reduction;sparse representations	Manifold learning is a novel approach in non-linear dimensionality reduction that has shown great potential in numerous applications and has gained ground compared to linear techniques. In addition, sparse representations have been recently applied on computer vision problems with success, demonstrating promising results with respect to robustness in challenging scenarios. A key concept shared by both approaches is the notion of sparsity. In this paper we investigate how the framework of sparse representations can be applied in various stages of manifold learning. We explore the use of sparse representations in two major components of manifold learning: construction of the weight matrix and classification of test data. In addition, we investigate the benefits that are offered by introducing a weighting scheme on the sparse representations framework via the weighted LASSO algorithm. The underlying manifold learning approach is based on the recently proposed spectral regression framework that offers significant benefits compared to previously proposed manifold learning techniques. We present experimental results on these techniques in three challenging face recognition datasets.	comparison and contrast of classification schemes in linguistics and metadata;computation;computational resource;computer vision;facial recognition system;k-nearest neighbors algorithm;lasso;mathematical optimization;nonlinear dimensionality reduction;nonlinear system;sparse approximation;sparse matrix;test data	Grigorios Tsagkatakis;Andreas E. Savakis	2010		10.1007/978-3-642-17289-2_49	facial recognition system;computer vision;k-svd;computer science;machine learning;pattern recognition;sparse approximation;mathematics;nonlinear dimensionality reduction;manifold alignment	Vision	25.21160853430905	-42.498512136610195	94000
79b21ee9b80d88edc606ab47838a2dfadb34b4c3	language for learning complex human-object interactions	probability;probabilistic model language complex human object interaction learning hierarchical hidden markov model hhmm complex activities task learning action primitives grammar complex human behaviour;probability grammars hidden markov models human robot interaction learning artificial intelligence natural language processing;robotics;human robot interaction;hidden markov models robots joints probabilistic logic accuracy abstracts data models;grammars;hidden markov models;robotteknik och automation;learning artificial intelligence;natural language processing;conference proceeding	In this paper we use a Hierarchical Hidden Markov Model (HHMM) to represent and learn complex activities/task performed by humans/robots in everyday life. Action primitives are used as a grammar to represent complex human behaviour and learn the interactions and behaviour of human/robots with different objects. The main contribution is the use of a probabilistic model capable of representing behaviours at multiple levels of abstraction to support the proposed hypothesis. The hierarchical nature of the model allows decomposition of the complex task into simple action primitives. The framework is evaluated with data collected for tasks of everyday importance performed by a human user.	action potential;hierarchical hidden markov model;interaction;markov chain;principle of abstraction;robot;statistical model	Mitesh Patel;Carl Henrik Ek;Nikolaos Kyriazis;Antonis A. Argyros;Jaime Valls Miró;Danica Kragic	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6631291	natural language processing;computer science;artificial intelligence;machine learning;probability;robotics	Robotics	36.904669494265775	-41.511650963804115	94003
40072de305bdb6429484e398cc7a4597240f762f	group level activity recognition in crowded environments across multiple cameras	computer vision community;agglomerative clustering;video surveillance;tracking system;suspicious behavior detection;social interaction;cameras target tracking clustering algorithms feature extraction surveillance real time systems;highly realistic aggressive behaviors;surveillance;group level activity recognition;real time;social sciences;crowded environments;suspicious behavior prediction;highly realistic aggressive behaviors group level activity recognition crowded environments multiple cameras school environments public parks social interactions suspicious behavior detection suspicious behavior prediction prison yard gangs mature multicamera multitarget person tracking system agglomerative clustering computer vision community decisive clustering social network analysis community grouping analysis;prison yard gangs;social network analysis community;computer vision;decisive clustering;grouping analysis;aggressive behavior;feature extraction;public parks;clustering algorithms;social network analysis;social interactions;school environments;target tracking;video surveillance computer vision public administration social sciences;multiple cameras;cameras;real time systems;mature multicamera multitarget person tracking system;activity recognition;public administration	Environments such as schools, public parks and prisonsand others that contain a large number of people are typi-cally characterized by frequent and complex social interac-tions. In order to identify activities and behaviors in suchenvironments, it is necessary to understand the interactionsthat take place at a group level. To this end, this paper ad-dresses the problem of detecting and predicting suspiciousand in particular aggressive behaviors between groups ofindividuals such as gangs in prison yards. The work buildson a mature multi-camera multi-target person tracking sys-tem that operates in real-time and has the ability to han-dle crowded conditions. We consider two approaches forgrouping individuals: (i) agglomerative clustering favoredby the computer vision community, as well as (ii) decisiveclustering based on the concept of modularity, which is fa-vored by the social network analysis community. We showthe utility of such grouping analysis towards the detectionof group activities of interest. The presented algorithm isintegrated with a system operating in real-time to success-fully detect highly realistic aggressive behaviors enacted bycorrectional officers in a simulated prison environment. Wepresent results from these enactments that demonstrate theefficacy of our approach.	activity recognition;algorithm;cluster analysis;computer vision;han unification;interaction;nikon cx format;onset (audio);real-time clock;sensor;social network analysis;tracking system	Ming-Ching Chang;Nils Krahnstoever;Ser-Nam Lim;Ting Yu	2010	2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2010.65	computer vision;social network analysis;simulation;tracking system;feature extraction;computer science;machine learning;hierarchical clustering;cluster analysis;computer security;activity recognition	Vision	38.45994523234661	-46.12563471365825	94022
603ffcfad879aaf559cac118894cd38666158f2f	learning from scratch a confidence measure		Stereo vision is a popular technique to infer depth from two or more images. In this field, confidence measures, typically obtained from the analysis of the cost volume, aim at detecting uncertain disparity assignments. As recently proved, multiple confidence measures combined with hand-crafted features extracted from the cost volume can be used also for other purposes and in particular to improve the overall disparity accuracy leveraging on machine learning techniques. In this paper, starting from the observation that recurrent local patterns occurring in the disparity maps can tell a correct assignment from a wrong one, we follow a completely different methodology to infer a novel confidence measure from scratch. Specifically, leveraging on Convolutional Neural Networks, we pose the confidence formulation as a regression problem by analyzing the disparity map provided by a stereo vision system. Once trained on a subset of the KITTI 2012 dataset with the disparity maps provided by the simple block-matching algorithm, our confidence measure outperforms state-of-the-art with two datasets (KITTI 2015 and Middlebury 2014) as well as with two stereo algorithms. The experimental evaluation reported clearly highlights that our approach is capable to better generalize its behavior in different circumstances with respect to state-of-the-art. Finally, not being based on cost volume analysis, our proposal is also potentially suited for out-of-the-box depth generation devices which usually do not expose the cues required by top-performing approaches.	binocular disparity;block-matching algorithm;convolutional neural network;cross-validation (statistics);graphics processing unit;machine learning;map;neural networks;out of the box (feature);sensor;stereopsis	Matteo Poggi;Stefano Mattoccia	2016			computer science;computer vision;deep learning;scratch;stereopsis;artificial intelligence	Vision	28.058817357298974	-49.72201310403147	94187
f6b7897ce16a65260fc1eea5c20343fdb965444d	application of ilp to cardiac arrhythmia characterization for chronicle recognition	inductive logic;learning algorithm;surveillance;electrocardiografo;hombre;algorithme apprentissage;logical programming;logique inductive;electrocardiographe;vigilancia;monitoring;programmation logique;arritmia fisiologia;robustesse;human;pattern recognition;robustness;arythmie physiologie;monitorage;reconnaissance forme;reconocimiento patron;monitoreo;arrhythmia physiology;algoritmo aprendizaje;programacion logica;cardiac arrhythmia;homme;robustez	We propose to use ILP techniques to learn sets of temporally constrained events called chronicles that a monitoring tool will use to detect pathological situations. ICL, a system providing a declarative bias language, was used for the experiments on learning cardiac arrhythmias. We show how to obtain properties, such as compactness, robustness or readability, by varying the learning bias.	algorithm;antivirus software;declarative programming;experiment;hemodynamics;high- and low-level;icl;inductive bias;online and offline;signal processing;temporal logic	Rene Quiniou;Marie-Odile Cordier;Guy Carrault;Feng Wang	2001		10.1007/3-540-44797-0_18	computer science;artificial intelligence;programming language;algorithm;robustness	ML	35.50352137738999	-40.8338759887385	94258
bf26718af3bd0d7d77bce4f5855e8a3e82b6d49c	real-time face recognition based on sparse illumination learning		Real-time face recognition is one of the most challenging problems in face recognition. We propose a novel algorithm to address this problem based on a sparse representation based classification (SRC) framework. First, we remove the background through the background subtraction algorithm, extract the foreground, and then, extract the face, thereby reducing the workload of the latter algorithm to improve the operation speed of the algorithm. We have adopted a sparse illumination learning and transfer (SILT) with robustness. The illumination in SILT is learned by fitting illumination examples of auxiliary face images from one or more additional subjects with a sparsely-used illumination dictionary. By enforcing a sparse representation of the query image in the illumination dictionary, the SILT can effectively recover and transfer the illumination and pose information from the alignment stage to the recognition stage. The new algorithms significantly outperform the state of the art in the single-sample regime with less restriction. In particular, the single-sample face alignment accuracy is comparable to that of the well-known Deformable SRC algorithm using multiple gallery images per class. Furthermore, the face recognition accuracy exceeds those of the SRC and Extended SRC algorithms using hand labeled alignment initialization.	algorithm;background subtraction;dictionary;facial recognition system;illumination (image);pattern recognition;real-time transcription;sample rate conversion;sparse approximation;sparse matrix;whole earth 'lectronic link	WuYang Ding;Bo Liu;Yanshan Xiao	2017	22017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)	10.1109/CSE-EUC.2017.56	robustness (computer science);workload;initialization;facial recognition system;computer science;artificial intelligence;sparse approximation;computer vision;background subtraction	Vision	30.682115065611914	-48.079973807112125	94293
0251e4bbe9899a5966c4ffeda44cfe357faa042d	multi-label classification based on low rank representation for image annotation	multi label classiﬁcation;low rank representation;image annotation;remote sensing images;graph construction;semantic graph	Annotating remote sensing images is a challenging task for its labor demanding annotation process and requirement of expert knowledge, especially when images can be annotated with multiple semantic concepts (or labels). To automatically annotate these multi-label images, we introduce an approach called Multi-Label Classification based on Low Rank Representation (MLC-LRR). MLC-LRR firstly utilizes low rank representation in the feature space of images to compute the low rank constrained coefficient matrix, then it adapts the coefficient matrix to define a feature-based graph and to capture the global relationships between images. Next, it utilizes low rank representation in the label space of labeled images to construct a semantic graph. Finally, these two graphs are exploited to train a graph-based multi-label classifier. To validate the performance of MLC-LRR against other related graph-based multi-label methods in annotating images, we conduct experiments on a public available multi-label remote sensing images (Land Cover). We perform additional experiments on five real-world multi-label image datasets to further investigate the performance of MLC-LRR. Empirical study demonstrates that MLC-LRR achieves better performance on annotating images than these comparing methods across various evaluation criteria; it also can effectively exploit global structure and label correlations of multi-label images.	algorithm;automatic image annotation;coefficient;experiment;feature vector;multi-label classification;multi-level cell;synergy	Qiaoyu Tan;Yezi Liu;Xia Chen;Guoxian Yu	2017	Remote Sensing	10.3390/rs9020109	computer vision;machine learning;pattern recognition;mathematics;automatic image annotation	AI	26.83071717128563	-45.929308139178616	94378
fe1f382184505cdd9c8857119b95974b8396c3d9	joint learning dictionary and discriminative features for high dimensional data	manifolds;cost function;visualization;covariance matrices;dictionaries;encoding;sparse matrices	Recently, sparse representation (SR) over a redundant dictionary has become a popular way of representing the data. It has been verified as an efficient and useful tool to promote the discrimination between signals. This work develops a joint learning approach to find the low dimensional discriminative features for high dimensional data. To avoid the high computational cost of direct sparse coding on large scale input data, we first learn SR in an orthogonal projected space over a task-driven sparsifying dictionary. We then exploit the discriminative projection on SR. The whole learning process is treated as an optimization problem of trace quotient maximization, which involves an orthogonal projection on original data space, a dictionary and a discriminative projection on sparse codes. The related cost function is well defined on a product manifold of the Stiefel manifold, the Oblique manifold and the Grassmann manifold. Finally, we employ a stochastic gradient descent algorithm on the smooth product manifold to maximize the cost function. Our numerical experiments on visual recognition demonstrate the effectiveness of the proposed algorithm, in comparison with the state of the arts.	algorithm;algorithmic efficiency;cluster analysis;code;computation;computer vision;dataspaces;dictionary;difference quotient;dimensionality reduction;entropy maximization;experiment;linear discriminant analysis;loss function;manifold regularization;mathematical optimization;neural coding;numerical analysis;oblique projection;optimization problem;sparse approximation;sparse matrix;stochastic gradient descent	Xian Wei;Yuanxiang Li;Hao Shen;Martin Kleinsteuber;Yi Lu Murphey	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899661	speech recognition;visualization;sparse matrix;manifold;k-svd;computer science;machine learning;pattern recognition;mathematics;encoding;manifold alignment	Vision	26.364169177945147	-41.26850034633731	94557
5619ee36cb33fce70709800586ad97b1cdf1e2f2	who are they looking at? automatic eye gaze following for classroom observation video analysis		We develop an end-to-end neural network-based computer vision system to automatically identify where each person within a 2-D image of a school classroom is looking (“gaze following”), as well as who she/he is looking at. Automatic gaze following could help facilitate data-mining of large datasets of classroom observation videos that are collected routinely in schools around the world in order to understand social interactions between teachers and students. Our network is based on the architecture by [27] but is extended to predict whether each person is looking at a target inside or outside the image; and to predict not only where, but who the person is looking at. Moreover, since our focus is on classroom observation videos, we collected a dataset from scratch of publicly available classroom sessions from 70 YouTube videos and collected labels from 408 labelers who annotated a total of 17, 758 gazes in 2, 263 unique image frames. Results of our experiments indicate that the proposed neural network can estimate the gaze target – either the spatial location or the face of a person – with substantially higher accuracy compared to several baselines.		Arkar Min Aung;Anand Ramakrishnan;Jacob Whitehill	2018			artificial intelligence;machine learning;computer science;human–computer interaction;eye tracking;eye movement	Vision	32.479864455894734	-50.29785105409304	94642
0ffd230f2f1ad98a10fbc8a5cc39099922e833b6	bidirectional visible neighborhood preserving embedding	neighborhood preserving embedding;manifold learning;dimensionality reduction;bidirectional visible neighborhood preserving embedding;k nearest neighbor;dimensional reduction	In this paper, we propose a series of dimensionality reduction algorithms according to a novel neighborhood graph construction method. This paper begins with the presentation of a new manifold learning algorithm called bidirectional visible neighborhood preserving embedding (BVNPE). Similar with existing manifold techniques, BVNPE first links every data point with its k nearest neighbors (NNs). Then, we construct a reliable neighborhood graph by checking two criteria: bidirectional linkage and visible neighborhood preserving. Third, we assign the weights to each edge in this reliable graph based on the pairwise distance between data points. Finally, we compute the low-dimensional embedding, trying to preserve the manifold structure of input dataset by mapping nearby points on the manifold to nearby points in low-dimensional space. Moreover, this paper also proposes a linear BVNPE called BVNPE/L for straightforward embedding of new data, and a multilinear BVNPE called BVNPE/M, which represents the tensor structure of image and video data better. Experiments on various datasets validate the effectiveness of proposed algorithms.	data point;k-nearest neighbors algorithm;linkage (software);nonlinear dimensionality reduction	Yang Liu;Yan Liu;Keith C. C. Chan	2009		10.1145/1734605.1734642	combinatorics;topology;machine learning;mathematics;semidefinite embedding	Vision	26.649999563808905	-40.881093390024404	94763
29a1acf032fb28574115852dc8a05e37a34a5a7e	3d-assisted feature synthesis for novel views of an object	object augmented multiview representation 3d assisted feature synthesis image patches image retrieval instance retrieval image recognition;image retrieval image recognition image representation;three dimensional displays shape solid modeling correlation computer vision visualization estimation	Comparing two images from different views has been a long-standing challenging problem in computer vision, as visual features are not stable under large view point changes. In this paper, given a single input image of an object, we synthesize its features for other views, leveraging an existing modestly-sized 3D model collection of related but not identical objects. To accomplish this, we study the relationship of image patches between different views of the same object, seeking what we call surrogate patches -- patches in one view whose feature content predicts well the features of a patch in another view. Based upon these surrogate relationships, we can create feature sets for all views of the latent object on a per patch basis, providing us an augmented multi-view representation of the object. We provide theoretical and empirical analysis of the feature synthesis process, and evaluate the augmented features in fine-grained image retrieval/recognition and instance retrieval tasks. Experimental results show that our synthesized features do enable view-independent comparison between images and perform significantly better than other traditional approaches in this respect.	3d modeling;computer vision;ibm notes;image retrieval;internet information services;visual computing	Hao Su;Fan Wang;Eric Yi;Leonidas J. Guibas	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.307	computer vision;feature detection;visual word;computer science;machine learning;pattern recognition;automatic image annotation;feature	Vision	32.69604469762152	-51.92670033122593	94771
95f8cfd71e7c1aa5ae720e1bd68be2fd7ef1047e	isolated sign language recognition with grassmann covariance matrices	sign language;grassmann manifold;hearing loss;covariance matrix	In this article, to utilize long-term dynamics over an isolated sign sequence, we propose a covariance matrix--based representation to naturally fuse information from multimodal sources. To tackle the drawback induced by the commonly used Riemannian metric, the proximity of covariance matrices is measured on the Grassmann manifold. However, the inherent Grassmann metric cannot be directly applied to the covariance matrix. We solve this problem by evaluating and selecting the most significant singular vectors of covariance matrices of sign sequences. The resulting compact representation is called the Grassmann covariance matrix. Finally, the Grassmann metric is used to be a kernel for the support vector machine, which enables learning of the signs in a discriminative manner. To validate the proposed method, we collect three challenging sign language datasets, on which comprehensive evaluations show that the proposed method outperforms the state-of-the-art methods both in accuracy and computational cost.	algorithm;algorithmic efficiency;coat of arms;computation;digital single-lens reflex camera;google cloud messaging;hidden markov model;language identification;multimodal interaction;real-time operating system;real-time transcription;significant figures;singular value decomposition;statistical manifold;support vector machine	Hanjie Wang;Xiujuan Chai;Xiaopeng Hong;Guoying Zhao;Xilin Chen	2016	TACCESS	10.1145/2897735	matérn covariance function;estimation of covariance matrices;covariance matrix;sign language;grassmannian;linguistics;rational quadratic covariance function;covariance function	AI	35.79823075248791	-51.61731572394149	95417
8c2a3f1601e1c52b33d73f7691ce0a513207dbc6	hand gesture recognition for a virtual mouse application using geometric feature of finger's trajectories	dynamic hand gesture;gesture classification;recognition;hand tracking;principle component analysis;conference proceeding	We aim to enable a computer to comprehend and perform the mouse functions by analyzing a video with hand motions. For this purpose, dynamic gestures are captured by a web cam and are recognized as pre-defined gestures which are used to suggest mouse functions. The proposed algorithm initially detects the hand. Then, it tracks fingertips' trajectories within a frame sequence. Finally, hand gestures are recognized through computing a set of proposed geometric features of fingers' trajectories and comparing with our collected gestures dataset. In this paper, four types of descriptors are defined for a dynamic gesture. Each descriptor includes different number of features, which compose a feature vector with 135 dimensions. Different classification algorithms (e.g. KNN, LDA, Naïve Bayes and SVM) are applied to compare the detection results. The minimal misclassification error rate (MCR) reaches about 4% (i.e. Correct Recognition rate of 96%). Furthermore, we applied Principle Component Analysis (PCA) to reduce the number of features. With 30 dimensional features (principle components), LDA classifier can achieve about 0.09% misclassification error rate.	feature vector;gesture recognition;k-nearest neighbors algorithm;linear discriminant analysis;memory card reader;mouse keys;naive bayes classifier;principal component analysis;webcam	Behnam Maleki;Hossein Ebrahimnezhad;Min Xu;Xiangjian He	2015		10.1145/2808492.2808566	computer vision;speech recognition;computer science;gesture recognition;communication	Vision	36.57786522677836	-48.66149754099654	95496
09f58353e48780c707cf24a0074e4d353da18934	unconstrained face recognition: establishing baseline human performance via crowdsourcing	visual databases face recognition image matching video signal processing;india unconstrained face recognition crowdsourcing still images videos unconstrained imaging environments matching algorithms amazon mechanical turk youtube face database united states;face videos databases face recognition accuracy lighting protocols	Research focus in face recognition has shifted towards recognition of faces “in the wild” for both still images and videos which are captured in unconstrained imaging environments and without user cooperation. Due to confounding factors of pose, illumination, and expression, as well as occlusion and low resolution, current face recognition systems deployed in forensic and security applications operate in a semi-automatic manner; an operator typically reviews the top results from the face recognition system to manually determine the final match. For this reason, it is important to analyze the accuracies achieved by both the matching algorithms (machines) and humans on unconstrained face recognition tasks. In this paper, we report human accuracy on unconstrained faces in still images and videos via crowd-sourcing on Amazon Mechanical Turk. In particular, we report the first human performance on the YouTube Faces database and show that humans are superior to machines, especially when videos contain contextual cues in addition to the face image. We investigate the accuracy of humans from two different countries (United States and India) and find that humans from the United States are more accurate, possibly due to their familiarity with the faces of the public figures in the YouTube Faces database. A fusion of recognitions made by humans and a commercial-off-the-shelf face matcher improves performance over humans alone.	algorithm;amazon mechanical turk;baseline (configuration management);crowdsourcing;facial recognition system;human reliability;humans;image resolution;semiconductor industry;the turk	Lacey Best-Rowden;Shiwani Bisht;Joshua C. Klontz;Anil K. Jain	2014	IEEE International Joint Conference on Biometrics	10.1109/BTAS.2014.6996296	computer vision;face detection;speech recognition;object-class detection;computer science;three-dimensional face recognition;face recognition grand challenge;multimedia	Vision	33.871974809894965	-51.98687449073202	95624
e22022de2db3432b3d77a49180b58d29058750d2	3d gloh features for human action recognition	histograms;image motion analysis;computer vision;visualization;three dimensional displays;feature extraction;videos;qa76 computer software	Human action recognition from videos has wide applicability and receives significant interests. In this work, to better identify spatio-temporal characteristics, we propose a novel 3D extension of Gradient Location and Orientation Histograms, which provides discriminative local features representing not only the gradient orientation, but also their relative locations. We further propose a human action recognition system based on the Bag of Visual Words model, by combining the new 3D GLOH local features with Histograms of Oriented Optical Flow (HOOF) global features. Along with the idea from our recent work to extract features only in salient regions, our overall system outperforms existing feature descriptors for human action recognition for challenging real-world video datasets.	bag-of-words model in computer vision;feature vector;gloh;gradient;optical flow;video content analysis	Ashwan Abdulmunem;Yu-Kun Lai;Xianfang Sun	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899734	computer vision;visualization;feature extraction;gloh;computer science;machine learning;pattern recognition;histogram	Vision	35.73971478653148	-50.924026564001124	95742
1c94562a7448dc8b0391a948719df4d79a40512f	fast concurrent object localization and recognition	article	Object localization and recognition are important problems in computer vision. However, in many applications, exhaustive search over all object models and image locations is computationally prohibitive. While several methods have been proposed to make either recognition or localization more efficient, few have dealt with both tasks simultaneously. This paper proposes an efficient method for concurrent object localization and recognition based on a data-dependent multi-class branch-and-bound formalism. Existing bag-of-features recognition techniques which can be expressed as weighted combinations of feature counts can be readily adapted to our method. We present experimental results that demonstrate the merit of our algorithm in terms of recognition accuracy, localization accuracy, and speed, compared to baseline approaches including exhaustive search, implicit-shape model (ISM), and efficient sub-window search (ESS). Moreover, we develop two extensions to consider non-rectangular bounding regions-composite boxes and polygons-and demonstrate their ability to achieve higher recognition scores compared to traditional rectangular bounding boxes.	algorithm;baseline (configuration management);branch and bound;brute-force search;computer vision;data dependency;ibm notes;implicit shape model;internationalization and localization;pattern recognition;semantics (computer science);window function	Tom Yeh;John J. Lee;Trevor Darrell	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206805	computer vision;speech recognition;pattern recognition	Vision	31.32224496494941	-48.626187005808426	95862
4db62d2f236c708916ae0d078bc1713aaca4cc46	contextual constraints for person retrieval in camera networks	measurement;person retrieval;metric learning;object tracking cameras image retrieval learning artificial intelligence;training data;vectors;caviar dataset contextual constraints person retrieval task camera networks positive constraints negative constraints person tracking general metric learning euclidean distance initial query track query set energy minimization problem track scores track constraints;image color analysis;feature extraction;object tracking;optimization;camera networks;cameras measurement feature extraction optimization training data vectors image color analysis;learning artificial intelligence;camera networks person retrieval metric learning;cameras;image retrieval	We use contextual constraints for person retrieval in camera networks. We start by formulating a set of general positive and negative constraints on the identities of person tracks in camera networks, such as a person cannot appear twice in the same frame. We then show how these constraints can be used to improve person retrieval. First, we use the constraints to obtain training data in an unsupervised way to learn a general metric that is better suited to discriminate between different people than the Euclidean distance. Second, starting from an initial query track, we enhance the query-set using the constraints to obtain additional positive and negative samples for the query. Third, we formulate the person retrieval task as an energy minimization problem, integrate track scores and constraints in a common framework and jointly optimize the retrieval over all interconnected tracks. We evaluate our approach on the CAVIAR dataset and achieve 22% relative performance improvement in terms of mean average precision over standard retrieval where each track is treated independently.	baseline (configuration management);discriminative model;energy minimization;euclidean distance;experiment;information retrieval;unsupervised learning	Martin Bäuml;Makarand Tapaswi;Arne Schumann;Rainer Stiefelhagen	2012	2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance	10.1109/AVSS.2012.28	computer vision;training set;feature extraction;image retrieval;computer science;machine learning;video tracking;pattern recognition;measurement	Vision	32.18561512711287	-48.16883823133774	95939
572529f1350df7172ce2e96cd3b9f6461c12559b	data domain description using support vectors.	support vector	This paper introduces a new method for data domain description , inspired by the Support Vector Machine by V.Vapnik, called the Support Vector Domain Description SVDD. This method computes a sphere shaped decision boundary with minimal volume around a set of objects. This data description can be used for novelty or outlier detection. It contains support vectors describing the sphere boundary and it has the possibility of obtaining higher order boundary descriptions without much extra computational cost. By using the diierent k ernels this SVDD can obtain more exible and more accurate data descriptions. The error of the rst kind, the fraction of the training objects which will be rejected, can be estimated immediately from the description.	algorithmic efficiency;anomaly detection;data domain;decision boundary;support vector machine	David M. J. Tax;Robert P. W. Duin	1999			support vector machine;computer science;machine learning	AI	25.01081786731445	-39.53510650969301	96100
b5937ccefb0f9a9a188de2040d1006bb9109f5b6	a new cascade model for the hierarchical joint classification of multitemporal and multiresolution remote sensing data	satellite image time series hierarchical multiresolution markov random fields multitemporal classification;time series image classification remote sensing;hierarchical multiresolution markov random fields;simulated annealing;satellite image time series;time series analysis;remote sensing;spatial resolution time series analysis remote sensing simulated annealing signal resolution data models;signal resolution;hierarchical joint classification multiresolution optical data multitemporal optical data high resolution image analysis finer resolution level wavelet transform coregistered image time series hierarchical graph based model classifier multiresolution remote sensing imagery multidate remote sensing imagery multiresolution remote sensing data multitemporal remote sensing data;data models;spatial resolution;multitemporal classification	In this paper, we propose a novel method for the joint classification of both multidate and multiresolution remote sensing imagery, which represents an important and relatively unexplored classification problem. The proposed classifier is based on an explicit hierarchical graph-based model that is sufficiently flexible to address a coregistered time series of images collected at different spatial resolutions. Within this framework, a novel element of the proposed approach is the use of multiple quadtrees in cascade, each associated with the images available at each observation date in the considered time series. For each date, the input images are inserted in a hierarchical structure on the basis of their resolutions, whereas missing levels are filled in with wavelet transforms of the images embedded in finer-resolution levels. This approach is aimed at both exploiting multiscale information, which is known to play a crucial role in high-resolution image analysis, and supporting input images acquired at different resolutions in the input time series. The experimental results are shown for multitemporal and multiresolution optical data.	embedded system;image analysis;image resolution;quadtree;time series;wavelet transform	Ihsen Hedhli;Josiane Zerubia;Sebastiano B. Serpico	2016	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2016.2580321	data modeling;computer vision;image resolution;simulated annealing;time series;pattern recognition;statistics;remote sensing	Visualization	31.193248342663733	-44.6586294435231	96205
19c4634ccedc7b68ddc5d788da5b6fee086a674e	quaternionic signal processing techniques for automatic evaluation of dance performances from mocap data	emc2 dataset quaternionic vector signal processing techniques dance mocap data automatic dance performance evaluation 4d human motion capture data kinect based human skeleton tracking global temporal synchronization local temporal synchronization spatial alignment dance motion signals spatiotemporal human motion data quaternionic representation quaternionic cross correlations dynamic time warping techniques quaternionic correlation based measures quaternion based scores particle swarm optimization huawei dataset 3dlife dataset;time warp simulation correlation methods data handling humanities particle swarm optimisation signal representation synchronisation;vector signal processing dance analysis motion capture data quaternions skeleton tracking;quaternions joints tracking vectors synchronization motion segmentation	In this paper, the problem of automatic dance performance evaluation from human Motion Capture (MoCap) data is addressed. A novel framework is presented, using data captured by Kinect-based human skeleton tracking, where the evaluation of user's performance is achieved against a gold-standard performance of a teacher. The framework addresses several technical challenges, including global and local temporal synchronization, spatial alignment and comparison of two “dance motion signals.” Towards the solution of these technical challenges, a set of appropriate quaternionic vector-signal processing methodologies is proposed, where the 4D (spatiotemporal) human motion data are represented as sequences of pure quaternions. Such a quaternionic representation offers several advantages, including the facts that joint angles and rotations are inherently encoded in the phase of quaternions and the three coordinates variables ( X,Y,Z) are treated jointly, with their intra-correlations being taken into account. Based on the theory of quaternions, a number of advantageous algorithms are formulated. Initially, global temporal synchronization of dance MoCap data is achieved by the use of quaternionic cross-correlations, which are invariant to rigid spatial transformations between the users. Secondly, a quaternions-based algorithm is proposed for the fast spatial alignment of dance MoCap data. Thirdly, the MoCap data can be temporally synchronized in a local fashion, using Dynamic Time Warping techniques adapted to the specific problem. Finally, a set of quaternionic correlation-based measures (scores) are proposed for evaluating and ranking the performance of a dancer. These quaternions-based scores are invariant to rigid transformations, as proved and demonstrated. A total score metric, through a weighted combination of three different metrics is proposed, where the weights are optimized using Particle Swarm Optimization (PSO). The presented experimental results using the Huawei/3DLife/EMC2 dataset are promising and verify the effectiveness of the proposed methods.	algorithm;cross-correlation;dynamic time warping;human–computer interaction;kinect;kinesiology;motion capture;particle swarm optimization;performance evaluation;signal processing	Dimitrios S. Alexiadis;Petros Daras	2014	IEEE Transactions on Multimedia	10.1109/TMM.2014.2317311	computer vision;simulation;mathematics	Vision	36.61043574744637	-49.421936276869864	96262
4daadc3d746be581ce38c27c9326830ce47ffd78	deep convolutional neural networks for action recognition using depth map sequences		Recently, deep learning approach has achieved promising results in various fields of computer vision. In this paper, a new framework called Hierarchical Depth Motion Maps (HDMM) + 3 Channel Deep Convolutional Neural Networks (3ConvNets) is proposed for human action recognition using depth map sequences. Firstly, we rotate the original depth d ata in 3D pointclouds to mimic the rotation of cameras, so that our algorithms can handle view variant cases. Secondly, in orde r to effectively extract the body shape and motion information,we generate weighted depth motion maps (DMM) at several tempor al scales, referred to as Hierarchical Depth Motion Maps (HDMM). Then, three channels of ConvNets are trained on the HDMMs from three projected orthogonal planes separately. The proposed algorithms are evaluated on MSRAction3D, MSRAction3DExt, UTKinect-Action and MSRDailyActivity3D datasets respectively. We also combine the last three datasets into a larger one (cal led Combined Dataset) and test the proposed method on it. The results show that our approach can achieve state-of-the-ar t esults on the individual datasets and without dramatical performance degradation on the Combined Dataset. I. I NTRODUCTION Human action recognition has been an active research topic in computer vision due to its wide range of applications, suc h as smart surveillance and human-computer interactions. In the past decades, research on action recognition mainly focuse d on recognising actions from conventional RGB videos. In the previous video-based motion action recognition, most researchers aimed to design hand-crafted features and achieved significant progress. However, in the evaluation c onducted by Wang et al. [1], one interesting finding is that ther e is no universally best hand-engineered feature for all data sets. Recently, the release of the Microsoft Kinect brings up new opportunities in this field. The Kinect device can provide bo th depth maps and RGB images in real-time at low cost. Depth maps have several advantages compared to traditional color images. For example, depth maps reflect pure geometry and shape cues, which can often be more discriminative than colo r and texture in many problems including object segmentation and detection. Moreover, depth maps are insensitive to chan ges in lighting conditions. Based on depth data, many works [2], [3], [4], [5] have been reported with respect to specific feat ure descriptors to take advantage of the properties of depth map s. However, all of them are based on hand-crafted features, whi ch are shallow high-dimensional descriptions of local or glob al spatio-temporal information and their performance varies from dataset to dataset. Deep Convolutional Neural Networks (ConvNets) have been demonstrated as an effective class of models for understanding image content, offering state-of-the-art res ults on image recognition, segmentation, detection and retrieval [6], [7], [8], [9]. With the success of ImageNet classification with ConvNets [10], many works take advantage of trained ImageNet models and achieve very promising performance on several tasks, from attributes classification [11] to ima ge representations [12] to semantic segmentation [13]. The ke y enabling factors behind these successes are techniques for scaling up the networks to millions of parameters and massiv e labelled datasets that can support the learning process. In this work, we propose to apply ConvNets to depth map sequences for action recognition. An architecture of Hierarchical De pth Motion Maps (HDMM) + 3 Channel Convolutional Neural Network (3ConvNets) is proposed. HDMM is a technique that can transform the problem of action recognition to image classification and artificially enlarge the training dat a. Specifically, to make our algorithms more robust to viewpoin t variations, we directly process the 3D pointclouds and rota te the depth data into different views. To make full use of the additional body shape and motion information from depth sequences, each rotated depth frame is first projected onto t hree orthogonal Cartesian planes, and then for each projection v iew, the absolute differences (motion energy) between consecut iv and sub-sampled frames are accumulated through an entire depth video sequence. To weight the importances of differen t motion energy, a weighted factor is used to make the motion energy more important for the recent poses than the past ones . Three HDMMs are constructed after above steps and three ConvNets are trained on the HDMMs. The final classification scores are combined by late fusion of the three ConvNets. We evaluate our method on the MSRAction3D, MSRAction3DExt, UTKinect-Action and MSRDailyActivity3D datasets individually and achieve results which are better than or comparable to the state-of-the-art. To further verify th e robustness of our method, we combine the last three datasets into a single one and test the proposed method on it. The results show that that our approach could achieve consisten t performance without much degradation in performance on the combined dataset. The main contributions of this paper can be summarized as follows. First of all, we propose a new architecture, name ly, HDMM + 3ConvNets for depth-based action recognition, which achieves state-of-the-art results on four datasets. Secondly, our method can handle view variant cases for action recognition to some extent due to the simply and directly pro cessing of 3D pointclouds. Lastly, a large dataset is genera ted by combining the existing ones to evaluate the stability of t he proposed method, because the combined dataset contains lar ge variances of within actions, background, viewpoint and num ber of samples of each action across the three datasets. The remainder of this paper is organized as follows. Section 2 reviews the related work on deep learning on 2D video action recognition and action recognition using depth sequences. Section 3 describes the proposed architecture. In Section 4, va rious experimental results and analysis are presented. Conclusi on and future work are made in Section 5. II. RELATED WORK With the recent resurgence of neural networks invoked by Hinton and others [14], deep neural architectures have been used as an effective solution for extracting high level feat ures from data. There are a number of attempts to apply deep architectures for 2D video recognition. In [15], spatio-te mporal features are leaned unsupervised by a Convolutional Restri cted Boltzmann Machine (CRBM) and then plugged into a ConvNet for action recognition. In [16], 3D convolutional network i s used to automatically learn spatio-temporal features dire ctly from raw data. Recently, several ConvNet architectures for action recognition in [17] is compared based on Sport-1M dataset, comprising 1.1 M YouTube videos of sports activiti es. They find that for a network, operating on individual video frames, performs similarly to the networks whose input is the stack of frames, which indicates that the learned spatio temporal features do not capture the motion effectively. In [18], spatial and temporal streams, are proposed for action recognition. Two ConvNets are trained on the two streams and combined by late fusion. The spatial stream is comprised of individual frames while the temporal stream is stacked by optical flow. However, the best results of all above deep learning methods can only match the state-of-the-art resul ts achieved by hand-crafted features. For depth-based action recognition, many works have been reported in the past few years. Li et al. [2] sample points from silhouette of a depth image to obtain a bag of 3D points which are clustered to enable recognition. Yang et al. [19] s tack differences between projected depth maps as DMM and then use HOG to extract the features on the DMM. This method transforms the problem of action recognition from 3D space t o 2D space. In [4], HON4D is proposed, in which surface normal is extended to 4D space and quantized by regular polychorons . Following this method, Yang and Tian [5] cluster hypersurfa ce normals and form the polynormal which can be used to jointly capture the local motion and geometry information. Super Normal Vector (SNV) is generated by aggregating the lowlevel polynormals. However, all of these methods are based on carefully hand designed features, which are restricted t o specific datasets and applications. Our work is inspired by [19] and [18], where we transform the problem of 3D action recognition to 2D image classification in order to take advantage of trained ImageNet models [10]. III. HDMM + 3C ONVNETS A depth map can be used to capture the 3D structure and shape information. By projecting the difference betwee n depth maps (DMM) onto three orthogonal Cartesian planes can further characterize the motion information of an actio n [19]. To make our method more robust to viewpoint variances, we directly process the 3D pointclouds and rotate the depth data into different views. In order to explore speed invaria nce and weight the importance of motion energy in time axis, subsampled and weighted HDMM is generated from the rotated projected maps. Three deep ConvNets are trained on three projected planes of HDMM. Late fusion is performed by combining the softmax class posteriors from the three nets. The overall framework is illustrated in Figure 1. Our algorithm s can be divided into three modules: Rotation in 3D Pointclouds, Hierarchical DMM and Networks Training & Class Score Fusion. A. Rotation One of the challenges for action recognition is the view invariance. To handle this problem, we rotate the depth data in 3D pointclouds, imitating the rotation of cameras around th e subject as illustrated in Figure 2 (b), where the rotation is in the world coordinate system (Figure 2 (a)). f Z Image center (Cx,Cy) o X Po Pt Pd	activiti;algorithm;apache axis;artificial neural network;boltzmann machine;call of duty: black ops;cartesian closed category;cartography;colocation centre;computer vision;convolutional neural network;cylinder-head-sector;deep learning;depth map;digital molecular matter (dmm);elegant degradation;emoticon;genera;glob (programming);high-level programming language;human–computer interaction;image scaling;imagenet;independence day: resurgence;kinect;normal (geometry);optical flow;pattern recognition;quantization (signal processing);real-time clock;softmax function;yang	Pichao Wang;Wanqing Li;Zhimin Gao;Jing Zhang;Chang Tang;Philip Ogunbona	2015	CoRR		computer vision;computer science;machine learning;pattern recognition;data mining	Vision	29.526081243318135	-51.92382471385853	96337
d24839f4151804d86541593382ba3fd078a104dd	fast and robust object detection using asymmetric totally corrective boosting	boosting training algorithm design and analysis object detection optimization detectors feature extraction;detectors;optimisation;totally corrective boosting;adaptive boosting;training;totally corrective boosting adaboost asymmetric learning column generation object detection;image classification;totally correctives;object recognition adaboost;journal article;optimization problems;optimization asymmetric totally corrective boosting algorithm object detection viola jones detection framework objective asymmetric loss optimization adaboost optimization problem lagrange duals stagewise optimization column generation strong classifier weak classifiers face detection pedestrian detection;optimization problem;boosting;optimisation image classification learning artificial intelligence object detection;feature extraction;pedestrian detection;adaboost;keywords asymmetric learning;algorithms;optimization;detection performance;stagewise optimizations;learning artificial intelligence;algorithm design and analysis;column generation;training algorithm;object detection;robust object detection;asymmetric learning	Boosting-based object detection has received significant attention recently. In this paper, we propose totally corrective asymmetric boosting algorithms for real-time object detection. Our algorithms differ from Viola and Jones' detection framework in two ways. Firstly, our boosting algorithms explicitly optimize asymmetric loss of objectives, while AdaBoost used by Viola and Jones optimizes a symmetric loss. Secondly, by carefully deriving the Lagrange duals of the optimization problems, we design more efficient boosting in that the coefficients of the selected weak classifiers are updated in a totally corrective fashion, in contrast to the stagewise optimization commonly used by most boosting algorithms. Column generation is employed to solve the proposed optimization problems. Unlike conventional boosting, the proposed boosting algorithms are able to de-select those irrelevant weak classifiers in the ensemble while training a classification cascade. This results in improved detection performance as well as fewer weak classifiers in the learned strong classifier. Compared with AsymBoost of Viola and Jones, our proposed asymmetric boosting is nonheuristic and the training procedure is much simpler. Experiments on face and pedestrian detection demonstrate that our methods have superior detection performance than some of the state-of-the-art object detectors.	adaboost;algorithm;anatomic node;boosting (machine learning);cascade device component;coefficient;column generation;converge;detectors;experiment;face detection;global optimization;iteration;jones calculus;loss function;mathematical optimization;neural coding;object detection;pkm2 wt allele;pedestrian detection;real-time clock;relevance;shox gene;sparse matrix;taxicab geometry;time complexity;trusted computing base;viola–jones object detection framework;weak ai;weight;exponential	Peng Wang;Chunhua Shen;Nick Barnes;Hong Zheng	2012	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2011.2178324	optimization problem;brownboost;boosting methods for object categorization;computer science;artificial intelligence;machine learning;pattern recognition;gradient boosting;boosting	Vision	24.941089801987058	-44.39999755177237	96429
8468eaac179443bf91e80dc9eddc56d478e75bab	practical non-parametric density estimation on a transformation group for vision	object recognition;distortion modeling nonparametric density estimation machine vision object appearance variability shape transformation texture transformation probability density linear shape deformation euclidean space kernel density estimate variable kernel estimator configuration space geometry transformation group invariant estimator nonsingular matrix positive determinant image transformation modeling digit recognition euclidean estimator linear invariance;probability;probability density;transformation group;computer vision image texture probability object recognition matrix algebra;matrix algebra;shape deformation;shape kernel deformable models costs independent component analysis machine vision computer science information geometry image recognition probability;image texture;computer vision;configuration space;machine vision;kernel density estimate;euclidean space;non parametric density estimation;kernel estimate	It is now common practice in machine vision to define the variability in an object’s appearance in a factored manner, as a combination of shape and texture transformations. In this context, we present a simpleandpractical method for estimating non-parametric probability densities over a grou p of linear shape deformations. Samples drawn from such a distribution do not lie in a Euclidean space, and standard kernel density estimates may perform poorly. While variable kernel estimators may mitigate this problem to some extent, the geometry of the underlying configuration space ultimately demands a kernel which accommodates its group structure. In this perspective, we propose a suitable invariant estimator on the linear group of non-singular matrices with positive determinant. We illustrate this approach by modeling image transformations in digit recognition problems, and present results showing the superiority of our estimator to comparable Euclidean estimators in this domain.	machine vision;spatial variability;variable kernel density estimation	Erik G. Miller;Christophe Chefd'Hotel	2003		10.1109/CVPR.2003.1211460	image texture;configuration space;kernel density estimation;computer vision;probability density function;discrete mathematics;machine vision;computer science;euclidean space;cognitive neuroscience of visual object recognition;machine learning;probability;mathematics;geometry;variable kernel density estimation;statistics	Vision	27.96385481467121	-44.289878986356825	96502
9f7984f8f70fc4610794737ea286c09219fcfbe8	multi-task and multi-kernel gaussian process dynamical systems	unsupervised learning;variational bayes;data completion;gaussian processes;0801 artificial intelligence and image processing;journal article;matrix decomposition;0899 other information and computing sciences;human motion;multi task learning;factor models;0906 electrical and electronic engineering;gaussian process latent variable models;artificial intelligence image processing	In this work, we propose a novel method for rectifying damaged motion sequences in an unsupervised manner. In order to achieve maximal accuracy, the proposed model takes advantage of three key properties of the data: their sequential nature, the redundancy that manifests itself among repetitions of the same task, and the potential of knowledge transfer across di erent tasks. In order to do so, we formulate a factor model consisting of Gaussian Process Dynamical Systems (GPDS), where each factor corresponds to a single basic pattern in time and is able to represent their sequential nature. Factors collectively form a dictionary of fundamental trajectories shared among all sequences, thus able to capture recurrent patterns within the same or across di erent tasks. We employ variational inference to learn directly from incomplete sequences and perform maximum a-posteriori (MAP) estimates of the missing values. We have evaluated our model with a number of motion datasets, including robotic and human motion capture data. We have compared our approach to wellestablished methods in the literature in terms of their reconstruction error and our results indicate signi cant accuracy improvement across di erent datasets and missing data ratios. Concluding, we investigate the performance bene ts of ∗Corresponding author. Email addresses: d.korkinof10@alumni.impetial.ac.uk (Dimitrios Korkinof), y.demiris@impetial.ac.uk (Yiannis Demiris) Preprint submitted to Pattern Recognition December 17, 2016 the multi-task learning scenario and how this improvement relates to the extent of component sharing that takes place.	calculus of variations;computer multitasking;dictionary;dynamical system;email;gaussian process;kernel (operating system);kinesiology;maximal set;missing data;motion capture;multi-task learning;pattern recognition;rectifier;robot;unsupervised learning	Dimitrios Korkinof;Yiannis Demiris	2017	Pattern Recognition	10.1016/j.patcog.2016.12.014	unsupervised learning;multi-task learning;computer vision;computer science;artificial intelligence;machine learning;pattern recognition;data mining;gaussian process;mathematics;factor analysis;matrix decomposition;algorithm;statistics	ML	33.268059649267435	-39.22713169161259	97248
eee24e29b1b73bbd1e75ad3cebe28c360f4aab84	evaluating the feasibility of deep learning for action recognition in small datasets		Action recognition is the computer vision task of identifying what action is happening in a given sequence of frames. Traditional approaches rely on handcrafted features and domain-specific image processing algorithms, often resulting in limited accuracy. The substantial advances in deep learning and the availability of larger datasets have allowed techniques that yield much better performance without domain-specific knowledge to recognize actions being performed based on the raw information from video sequences, a strategy called representation learning. However, deep neural networks usually require very large labeled datasets for training and properly generalizing, and due to their high capacity, they often overfit small data, hence providing suboptimal results. This work aims to check the real feasibility of employing deep learning in the context of smallsized action recognition datasets. Our goal is to verify whether deep learning approaches can provide improved performance in cases in which labeled data is not abundant. In order to do so, we perform a thorough empirical analysis in which we investigate distinct network architectures with hyperparameter optimization, as well as different data pre-processing techniques and fusion methods.	algorithm;best practice;computer vision;data pre-processing;deep learning;domain-specific language;feature engineering;feature learning;image processing;machine learning;mathematical optimization;matrix regularization;neural networks;overfitting;pattern recognition;preprocessor	Juarez Monteiro;Roger Granada;João Paulo Aires;Rodrigo C. Barros	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489297	artificial intelligence;overfitting;task analysis;hyperparameter optimization;artificial neural network;machine learning;pattern recognition;feature extraction;deep learning;computer science;small data;feature learning	ML	27.21591469032315	-50.154396779910975	97260
83a556d5fc4131398c5095ceb16fecd28a5063c1	real-time eye tracking techni que for multiview 3d systems	gaze tracking three dimensional displays feature extraction face face detection real time systems;image matching face recognition gaze tracking haar transforms;gaze tracking;three dimensional displays;feature extraction;face;biological proportion multiview 3d systems real time multiuser eye tracking haar feature face detection face classification matching template eye positions;face detection;real time systems	This paper presents the real-time multi-user eye tracking technique for multiview 3D systems. The proposed technique used the Haar feature-based face detection and classification. Then, it calculated the best matching template, and extracts eye positions based on biological proportion. Simulation results showed the proposed method enhanced the average F1 score up to 0.312, compared with conventional methods.	eye tracking;f1 score;face detection;haar wavelet;multi-user;real-time locating system;real-time transcription;simulation;statistical classification	Suk-Ju Kang;Yong Woo Jeong;Jae-Jung Yun;Sungwoo Bae	2016	2016 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2016.7430728	face;computer vision;face detection;speech recognition;feature extraction;computer science;pattern recognition	Robotics	38.24436419788691	-50.12079334125777	97316
3b8a7701906e59c424dec8342cd49a29f04c6dd1	learning object representations using a priori constraints within orassyll	object representation;neurologie;contrainte a priori;object recognition;correspondance;correspondence;fisiologia;learning;systeme apprentissage;autonomous system;neurology;coaccion;contrainte;orassyll;reconnaissance objet;biology;biologia;feature space;a priori contraint;developmental psychology;sistema autonomo;statistical evaluation;aprendizaje;physiologie;learning systems;apprentissage;physiology;a priori knowledge;constraint;modelo n dimensiones;multidimensional model;feature extraction;image sequence;psychologie developpementale;learning object;systeme autonome;neurologia;aparato visual;appareil visuel;evaluation;evaluacion;object recognition with autonomous learned and sparse symbolic representations based on local line detectors;visual system;autonomous learning;development psychology;modele n dimensions;biologie	In this article, a biologically plausible and efficient object recognition system (called ORASSYLL) is introduced, based on a set of a priori constraints motivated by findings of developmental psychology and neuro-physiology. These constraints are concerned with the organization of the input in local and corresponding entities, the interpretation of the input by its transformation in a highly structured feature space, and the evaluation of features extracted from an image sequence by statistical evaluation criteria. In the context of the bias-variance dilemma, the functional role of a priori knowledge within ORASSYLL is discussed. In contrast to systems in which object representations are defined manually, the introduced constraints allow an autonomous learning from complex scenes.	area striata structure;autonomous robot;bias–variance tradeoff;entity;extraction;feature vector;microsoft outlook for mac;outline of object recognition;psychology, developmental;sample variance;physiological aspects	Norbert Krüger	2001	Neural Computation	10.1162/089976601300014583	computer vision;neurology;neuroscience;a priori and a posteriori;feature vector;visual system;feature extraction;computer science;autonomous system;artificial intelligence;evaluation;cognitive neuroscience of visual object recognition;machine learning;constraint;communication	ML	35.522752281327286	-40.8847533191293	97516
4cc328c4f6e243e0eca6b54744f833833e19c143	introduction to the issue on robust subspace learning and tracking: theory, algorithms, and applications		The papers in this special section focus on robust subspace learning and tracking. Subspace learning theory for dimensionality reduction was initiated with the Principal Component Analysis (PCA) formulation proposed by Pearson in 1901. PCA was first widely used for data analysis in the field of psychometrics and chemometrics but today it is often the first step in more various types of exploratory data analysis, predictive modeling, classification and clustering problems. It finds modern applications in signal processing, biomedical imaging, computer vision, process fault detection, recommendation system design and many more domains. Since one century, numerous other subspace learning models, either reconstructive and discriminative, were developed over time in literature to address dimensionality reduction while keeping the relevant information in a different manner from PCA. However, PCA can also be viewed as a soft clustering method that seeks to find clusters in different subspaces within a dataset, and numerous clustering methods are based on dimensionality reduction. These methods are called subspace clustering methods that are extension of traditional PCA based clustering, and divide data points belonging to the union of subspaces (UoS) into the respective subspaces. In several modern applications, the main limitation of the subspace learning and clustering models are their sensitivity to outliers. Thus, further developments concern robust subspace learning which refers to the problem of subspace learning in the presence of outliers. In fact, even the classical subspace learning problem with speed or memory constraints is not a solved problem.		Thierry Bouwmans;Namrata Vaswani;P. Rodriguez;R. Vidal;Z. Lin	2018	J. Sel. Topics Signal Processing	10.1109/JSTSP.2018.2879245	artificial intelligence;discriminative model;linear subspace;computer vision;exploratory data analysis;principal component analysis;dimensionality reduction;computer science;fuzzy clustering;cluster analysis;subspace topology;pattern recognition	ML	25.939590134394937	-40.97620389138137	97523
865113a6d44623aeda656d97f0d24f6cad3a186e	mining hard negative samples for sar-optical image matching using generative adversarial networks		In this paper, we propose a generative framework to produce similar yet novel samples for a specified image. We then propose the use of these images as hard-negatives samples, within the framework of hard-negative mining, in order to improve the performance of classification networks in applications which suffer from sparse labelled training data. Our approach makes use of a variational autoencoder (VAE) which is trained in an adversarial manner in order to learn a latent distribution of the training data, as well as to be able to generate realistic, high quality image patches. We evaluate our proposed generative approach to hard-negative mining on a synthetic aperture radar (SAR) and optical image matching task. Using an existing SAR-optical matching network as the basis for our investigation, we compare the performance of the matching network trained using our approach to the baseline method, as well as to two other hard-negative mining methods. Our proposed generative architecture is able to generate realistic, very high resolution (VHR) SAR image patches which are almost indistinguishable from real imagery. Furthermore, using the patches as hard-negative samples, we are able to improve the overall accuracy, and significantly decrease the false positive rate of the SAR-optical matching task—thus validating our generative hard-negative mining approaches’ applicability to improve training in data sparse applications.	aperture (software);autoencoder;baseline (configuration management);calculus of variations;conceptualization (information science);data curation;digital curation;display resolution;film-type patterned retarder;generative adversarial networks;image registration;image resolution;impedance matching;sparse matrix;synthetic intelligence;variational principle	Lloyd Haydn Hughes;Michael Schmitt;Xiao xiang Zhu	2018	Remote Sensing	10.3390/rs10101552	adversarial system;generative grammar;geology;computer vision;artificial intelligence	ML	27.091154138002977	-49.45642271502625	97557
eeada9ba06307eec266fcf9fa34a1a6b8c25b349	video saliency detection based on random walk with restart	graph theory;random walk with restart;image motion analysis;video signal processing feature extraction graph theory image motion analysis markov processes object detection statistical distributions;video signal processing;saliency detection;random walk with restart markov chain rwr simulation fast motion node slow motion node subsequent frame stationary distribution saliency level extraction geometrical distance incident nodes edge weight image block fully connected graph image plane eye movement model graph based video saliency detection algorithm;statistical distributions;random walk with restart saliency detection video saliency markov chain;feature extraction;video saliency;markov processes;object detection;markov chain	A graph-based video saliency detection algorithm is proposed in this work. We model eye movements on an image plane as random walks on a graph. To detect the saliency of the first frame in a video sequence, we construct a fully connected graph, in which each node represents an image block. We assign an edge weight to be proportional to the dissimilarity between the incident nodes and inversely proportional to their geometrical distance. We extract the saliency level of each node from the stationary distribution of the random walker on the graph. Next, to detect the saliency of each subsequent frame, we add the criterion that an edge, connecting a slow motion node to a fast motion node, should have a large weight. We then compute the stationary distribution of the random walk with restart (RWR) simulation, in which the saliency of the previous frame is used as the restarting distribution. Experimental results show that the proposed algorithm provides more reliable and accurate saliency detection performance than conventional algorithms.	amiga walker;connectivity (graph theory);image plane;random walker algorithm;running with rifles;simulation;stationary process	Jun-Seong Kim;Hansang Kim;Jae-Young Sim;Chang-Su Kim;Sang Uk Lee	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738508	probability distribution;computer vision;markov chain;feature extraction;graph theory;kadir–brady saliency detector;machine learning;pattern recognition;mathematics;markov process;statistics	Vision	38.5711303262978	-47.69197485608792	97618
b5a3fd4cb228532b524197f20200bc5bd595491f	real-time temporal action localization in untrimmed videos by sub-action discovery.		This paper presents a computationally efficient approach for temporal action detection in untrimmed videos that outperforms state-of-the-art methods by a large margin. We exploit the temporal structure of actions by modeling an action as a sequence of sub-actions. A novel and fully automatic sub-action discovery algorithm is proposed, where the number of sub-actions for each action as well as their types are automatically determined from the training videos. We find that the discovered sub-actions are semantically meaningful. To localize an action, an objective function combining appearance, duration and temporal structure of sub-actions is optimized as a shortest path problem in a network flow formulation. A significant benefit of the proposed approach is that it enables real-time action localization (40 fps) in untrimmed videos. We demonstrate state-of-the-art results on THUMOS’14 and MEXaction2 datasets.	algorithm;algorithmic efficiency;deep learning;flow network;high- and low-level;loss function;optimization problem;real-time clock;real-time computing;real-time locating system;real-time transcription;shortest path problem	Rui Hou;Rahul Sukthankar;Mubarak Shah	2017			computer vision;computer science;artificial intelligence	Vision	32.99158905790631	-48.76745617149998	97640
b11ad1ecf005327d6894633fa48161c1b92fecf1	geographic information use in weakly-supervised deep learning for landmark recognition		The successful deep convolutional neural networks for visual object recognition typically rely on a massive number of training images that are well annotated by class labels or object bounding boxes with great human efforts. Here we explore the use of the geographic metadata, which are automatically retrieved from sensors such as GPS and compass, in weakly-supervised learning techniques for landmark recognition. The visibility of a landmark in a frame can be calculated based on the camera's field-of-view and the landmark's geometric information such as location and height. Subsequently, a training dataset is generated as the union of the frames with presence of at least one target landmark. To reduce the impact of the intrinsic noise in the geo-metadata, we present a frame selection method that removes the mistakenly labeled frames with a two-step approach consisting of (1) Gaussian Mixture Model clustering based on camera location followed by (2) outlier removal based on visual consistency. We compare the classification results obtained from the ground truth labels and the noisy labels derived from the raw geo-metadata. Experiments show that training based on the raw geo-metadata achieves a Mean Average Precision (MAP) of 0.797. Moreover, by applying our proposed representative frame selection method, the MAP can be further improved by 6.4%, which indicates the promising use of the geo-metadata in weakly-supervised learning techniques.	artificial neural network;cluster analysis;convolutional neural network;deep learning;global positioning system;ground truth;information retrieval;mixture model;outline of object recognition;sensor;supervised learning	Yifang Yin;Zhenguang Liu;Roger Zimmermann	2017	2017 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2017.8019376	convolutional neural network;artificial intelligence;pattern recognition;computer science;computer vision;mixture model;deep learning;ground truth;noise measurement;cluster analysis;outlier;geospatial metadata	Vision	29.742522454006313	-50.06403736875297	98092
e71b886e6aee9c81027e158d53c6641dae024ed3	boosting chain learning for object detection	optimisation;cascade coupling;face recognition object detection learning artificial intelligence pattern classification optimisation;bootstrap training boosting chain learning object detection pattern classification linear optimization redundancy cascade coupling face detection problem;boosting chain learning;bootstrap training;learning systems;face recognition;redundancy;pattern classification;face detection problem;error rate;linear optimization;learning artificial intelligence;face detection;boosting object detection detectors face detection support vector machines computational efficiency iterative algorithms support vector machine classification asia redundancy;object detection;optimization methods	A general classification framework, called boosting chain, is proposed for learning boosting cascade. In this framework, a “chain” structure is introduced to integrate historical knowledge into successive boosting learning. Moreover, a linear optimization scheme is proposed to address the problems of redundancy in boosting learning and threshold adjusting in cascade coupling. By this means, the resulting classifier consists of fewer weak classifiers yet achieves lower error rates than boosting cascade in both training and test. Experimental comparisons of boosting chain and boosting cascade are provided through a face detection problem. The promising results clearly demonstrate the effectiveness made by boosting chain.	algorithm;computer vision;face detection;linear programming;mathematical optimization;object detection	Rong Xiao;Long Zhu;HongJiang Zhang	2003		10.1109/ICCV.2003.1238417	facial recognition system;brownboost;computer vision;face detection;speech recognition;boosting methods for object categorization;word error rate;computer science;linear programming;machine learning;pattern recognition;redundancy;lpboost;gradient boosting;boosting	ML	26.144829229706605	-45.25448745429527	98112
dc8c8c1d198d34881a4f7d93312518e1740f022a	schatten-p norm based linear regression discriminant analysis for face recognition		Locality-regularized linear regression classification (LLRC) shows good performance on face recognition. However, it sorely performs on the original space, which results in degraded classification efficiency. To solve this problem, we propose a dimensionality reduction algorithm named schatten-p norm based linear regression discriminant analysis (SPLRDA) for image feature extraction. First, it defines intra-class and inter-class scatters based on schatten-p norm, which improves the capability to deal with illumination changes. Then the objective function which incorporates discriminant analysis is derived from the minimization of intra-class compactness and the maximization of inter-class separability. Experiments carried on some typical databases validate the effectiveness and robustness of our method.	facial recognition system;linear discriminant analysis	Lijiang Chen;Wentao Dou;Xia Mao	2018		10.1007/978-981-13-1702-6_5	norm (mathematics);dimensionality reduction;robustness (computer science);feature extraction;linear regression;compact space;linear discriminant analysis;pattern recognition;artificial intelligence;maximization;mathematics	Vision	25.1885268474942	-41.60314160181421	98259
021e008282714eaefc0796303f521c9e4f199d7e	ncc-net: normalized cross correlation based deep matcher with robustness to illumination variations		The task of matching image patches is a fundamental problem in computer vision. When sufficiently textured patches are normalized up to similarity transformation, a simple Normalized Cross Correlation (NCC) of corresponding patches will give a high value. In practice, using it on patches per se may not perform well due to the noisy variations of pixel intensities. A more prudent approach will be to apply it to the abstract features extracted by a deep convolutional network. We study the applicability of an NCC based convolutional network for the task of Patch Matching. Further, there may be cases where the network may fail due to insufficient textures. In those cases, a simple pixel difference based method will be beneficial. To this end, we propose to improve the two basic architectures, Siamese networks and Central-Surround stream networks, using robust matching layers for learning the similarities of patches, assisted by a simple cross-entropy loss function. We empirically verify the performance of the proposed models on the challenging UBC Patches dataset and show that they are close to the state-of-the-art. Further, we evaluate their resilience to large illumination changes in two experimental scenarios: 1) by manually varying the patches of UBC Patches by an affine model 2) by using the publicly available Webcam dataset. We demonstrate that our models are indeed very resilient to illumination variations; they reduce the false positive rate to nearly 10%, and improve over the popular methods by nearly 5%. Further, we demonstrate the generalisability of the proposed NCC based matching layer by applying it to Face Recognition and show that it improves the performances of well known networks on a real-world, surveillance dataset.	computer vision;cross entropy;cross-correlation;data descriptor;experiment;facial recognition system;high- and low-level;illumination (image);jsp model 2 architecture;loss function;neural correlates of consciousness;patch (computing);performance;pixel;texture mapping;webcam	Arulkumar Subramaniam;Prashanth Balasubramanian;Anurag Mittal	2018	2018 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2018.00215	robustness (computer science);computer vision;false positive rate;cross-correlation;normalization (statistics);feature extraction;pixel;artificial intelligence;computer science;pattern recognition;facial recognition system;affine transformation	Vision	30.877217892892627	-51.39419333731866	98329
c1edc6dc088b8860bfb440822c8e2fc769e9970e	distribution distance minimization for unsupervised user identity linkage		Nowadays, it is common for one natural person to join multiple social networks to enjoy different services. Linking identical users across different social networks, also known as the User Identity Linkage (UIL), is an important problem of great research challenges and practical value. Most existing UIL models are supervised or semi-supervised and a considerable number of manually matched user identity pairs are required, which is costly in terms of labor and time. In addition, existing methods generally rely heavily on some discriminative common user attributes, and thus are hard to be generalized. Motivated by the isomorphism across social networks, in this paper we consider all the users in a social network as a whole and perform UIL from the user space distribution level. The insight is that we convert the unsupervised UIL problem to the learning of a projection function to minimize the distance between the distributions of user identities in two social networks. We propose to use the earth mover's distance (EMD) as the measure of distribution closeness, and propose two models UUIL$_gan $ and UUIL$_omt $ to efficiently learn the distribution projection function. Empirically, we evaluate the proposed models over multiple social network datasets, and the results demonstrate that our proposal significantly outperforms state-of-the-art methods.	centrality;linkage (software);semi-supervised learning;semiconductor industry;social network;supervised learning;transformation matrix;unsupervised learning;user space	Chaozhuo Li;Senzhang Wang;Philip S. Yu;Lei Zheng;Xiaoming Zhang;Zhoujun Li;Yanbo Liang	2018		10.1145/3269206.3271675	earth mover's distance;data mining;discriminative model;minification;user space;theoretical computer science;isomorphism;projection (set theory);computer science;social network;closeness	AI	25.591253258495218	-43.99168727945543	98333
04d4849939f7466ff7e03d3183c2f43bfe01443d	hyperspectral image classification via kernel extreme learning machine using local receptive fields	kernel;convolution;training;feature extraction;neurons;hyperspectral imaging	This paper proposes a classification approach for hyperspectral image (HSI) using the local receptive fields based kernel extreme learning machine. Extreme learning machine (ELM) has drawn increasing attention in the pattern recognition filed due to its simpleness, speediness and good generalization ability. A kernel method is often used to promote ELM's performance, which is known as kernel ELM. The local receptive field concept originates from research in neuroscience. Considering the local correlations of spectral features, it is promising to improve the performance of HSI classification by combining local receptive fields with kernel ELM. Experimental results on the Pavia University dataset confirm the effectiveness of the proposed HSI classification method.	computer vision;elm;horizontal situation indicator;kernel (operating system);kernel method;pattern recognition	Qi Lv;Xin Niu;Yong Dou;Yueqing Wang;Jiaqing Xu;Jie Zhou	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532358	computer vision;kernel;radial basis function kernel;feature extraction;computer science;hyperspectral imaging;machine learning;pattern recognition;mathematics;convolution;tree kernel;polynomial kernel	Robotics	29.771409890879493	-44.897566086356	98523
71b4f2997cc8d735a0f664fddc31d0df96ab9cb0	does human action recognition benefit from pose estimation?	psi_visics	Early works on human action recognition focused on tracking and classifying articulated body motions. Such methods required accurate localisation of body parts, which is a difficult task, particularly under realistic imaging conditions. As such, recent trends have shifted towards the use of more abstract, low-level appearance features such as spatio-temporal interest points. Motivated by the recent progress in pose estimation, we feel that pose-based action recognition systems warrant a second look. In this paper, we address the question of whether pose estimation is useful for action recognition or if it is better to train a classifier only on low-level appearance features drawn from video data. We compare pose-based, appearance-based and combined pose and appearance features for action recognition in a home-monitoring scenario. Our experiments show that posebased features outperform low-level appearance features, even when heavily corrupted by noise, suggesting that pose estimation is beneficial for the action recognition task.	3d pose estimation;dolev–yao model;experiment;high- and low-level;high-level programming language;preprocessor;statistical classification;switzerland;utility functions on indivisible goods;yao graph	Angela Yao;Juergen Gall;Gabriele Fanelli;Luc Van Gool	2011		10.5244/C.25.67	computer vision;simulation;3d pose estimation;computer science;articulated body pose estimation	Vision	33.307640425518684	-50.66847236485214	98543
1564b2bea0a22a014b33dbb97c8067ffcb15c10e	optimal mean two-dimensional principal component analysis with f-norm minimization		Two-dimensional principal component analysis (2DPCA) employs the squared F-norm as distance metric for feature extraction and is widely used in the field of pattern analysis and recognition, especially face image analysis. But it is sensitive to the presence of outliers due to the fact that squared F-norm remarkably enlarges the role of outliers in the criterion function. To handle this problem, we propose a robust formulation for 2DPCA, namely optimal mean 2DPCA with F-norm minimization (OMF-2DPCA). In OMF-2DPCA, distance in spatial dimensions (attribute dimensions) is measured in F-norm, while the summation over different data points uses 1-norm. Moreover, we center the data using the optimized mean rather than the fixed mean. This helps further improve robustness of our method. To solve OMF-2DPCA, we propose a fast iterative algorithm, which has a closed-form solution in each iteration. Experimental results on face image databases illustrate its effectiveness and advantages.	principal component analysis	Qianqian Wang;Quanxue Gao;Xinbo Gao;Feiping Nie	2017	Pattern Recognition	10.1016/j.patcog.2017.03.026	mathematical optimization;machine learning;pattern recognition;mathematics;statistics	Vision	25.733019189656016	-40.85216713132713	98586
c4233f7865f1fe62a1afca2108721bc53d1cb305	a stochastic minimum spanning forest approach for spectral-spatial classification of hyperspectral images	forestry;image segmentation;image resolution;support vector machines;minimum spanning forest hyperspectral image classification multiple classifiers stochastic markers;stochastic markers;hyperspectral imaging support vector machines accuracy image segmentation vegetation conferences;minimum spanning forest;image classification;hyper spectral;classification;vegetation;learning artificial intelligence forestry image classification image resolution;pixelwise classification stochastic minimum spanning forest approach spectral spatial classification hyperspectral images supervised hyperspectral data classification msf maximum vote decision rule aviris image vegetation area;multiple classifiers;accuracy;hyperspectral data;support vector machine;learning artificial intelligence;hyperspectral imaging;hyperspectral image;decision rule;conferences	A new method for supervised hyperspectral data classification is proposed. In particular, the notion of Stochastic Minimum Spanning Forests (MSFs) is introduced. For a given hyper-spectral image, a pixelwise classification is first performed. From this classification map, M marker maps are generated by randomly selecting pixels and labeling them as markers for the construction of MSFs. The next step consists in building an MSF from each of the M marker maps. Finally, all the M realizations are aggregated with a maximum vote decision rule, resulting in a final classification map. The experimental results presented on an AVIRIS image of the vegetation area show that the proposed approach yields accurate classification maps, and thus is attractive for hyperspectral data analysis.	file spanning;map;microsoft solutions framework;minimum spanning tree;pixel;randomness	Kévin Bernard;Yuliya Tarabalka;Jesús Angulo;Jocelyn Chanussot;Jon Atli Benediktsson	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6115664	support vector machine;computer vision;computer science;machine learning;pattern recognition	Vision	31.421553700151485	-44.309885130631706	98721
aa16ac2131af4ec673deb0ccb53360c7edad087e	design of adaptive biometric gait recognition algorithm with free walking directions	free walking directions;viewing angle;gait signature extraction;gait sequence;interval type 2 fuzzy set;spatial domain energy deviation image;gait period estimation;intelligent gait recognition system;interval type 2 fuzzy k nearest neighbour classifier;motion style;clustering technique;adaptive biometric gait recognition algorithm	Gait is one of well-identified biometrics that has been broadly applied for human identification at a distance based on their motion style. However, the current gait recognition might have difficulties due to changing the viewing angles anduncertainty associated with gait signature extraction. This study deals with the design of an intelligent gait recognition system that tackles the problems mentioned above. This system is based on spatial-domain energy deviation image as a gait signature by adopting clustering technique to estimate the gait period in the gait sequence with arbitrary walking directions. To further improve the performance of the proposed system, interval type-2 fuzzy K-nearest neighbor classifier is used to diminish the effect of uncertainty formed by variations in gait signature extraction. Interval type-2 fuzzy set is involved in extending themembership values of each gait signature by using several initial K in order to handle and manage uncertainty that exists in choosing the initial value K. The proposed method realises the reduction in the dimensions of the gait feature and over-fitting. The comprehensive analyses reveal that the proposed algorithm can significantly enhance the multiple view gait recognition performance when being matched to the similar methods in the literature.	algorithm;biometrics;gait analysis	Saad M. Darwish	2017	IET Biometrics	10.1049/iet-bmt.2015.0082	computer vision;effect of gait parameters on energetic cost;pattern recognition	Vision	35.871995568899194	-51.57140829618072	98893
e1097d081236759d7727b33bc6a131e742b755a6	probabilistic nearest neighbor search for robust classification of face image sets	databases;statistical distributions face recognition image classification image enhancement probability search problems;manifolds;nickel;public databases probabilistic nearest neighbor search robust face image set classification video based face recognition face detection questionable tracking pronn search method image set enhancement statistical distribution point probability honda ucsd databases youtube celebrities multiple biometric grand challenge mbgc;会议论文;youtube;robustness;face;face manifolds nickel probabilistic logic databases youtube robustness;probabilistic logic	Classification with image sets is recently a compelling technique for video-based face recognition. Previous methods in this line mostly assume each image set is pure, i.e., containing well-aligned face images of the same subject, which however is hardly satisfied in real-world applications due to incorrect face detection, questionable tracking, or multiple faces in a single image. This paper proposes a Probabilistic Nearest Neighbor (ProNN) search method to enhance the robustness of NN search against impure image sets by leveraging the statistical distribution of the involved image sets. Specifically, we represent image sets by affine hull, a well-recognized set model, to account for the unseen appearances in each image set. We further exploit a constraint that these unseen appearances statistically follow some pre-specified distribution (Gaussian in this work). Finally, in search of a pair of nearest neighbor points (one per hull), at the same time their distance being minimized, the probability of each point belonging to the same class as that of its corresponding hull is maximized. The proposed ProNN method is evaluated on three widely-studied public databases, Honda/UCSD, YouTube Celebrities and Multiple Biometric Grand Challenge (MBGC), under two kinds of experimental settings where image sets are contaminated either with false positive faces or images of other subjects. Extensive experiments demonstrate the superiority of the proposed approach over state-of-the-art methods.	algorithm;autostereogram;biometrics;conjugate gradient method;database;disk image;experiment;face detection;facial recognition system;kernel density estimation;loss function;multiple biometric grand challenge;nearest neighbor search;nearest-neighbor interpolation;optimization problem;simulation;stochastic gradient descent	Wen Wang;Ruiping Wang;Shiguang Shan;Xilin Chen	2015	2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)	10.1109/FG.2015.7163138	machine learning;pattern recognition;data mining;mathematics	Vision	28.914317465909278	-47.05323039279846	99074
4da4e58072c15904d4ce31076061ebd3ab1cdcd5	learning deep facial expression features from image and optical flow sequences using 3d cnn		Facial expression is highly correlated with the facial motion. According to whether the temporal information of facial motion is used or not, the facial expression features can be classified as static and dynamic features. The former, which mainly includes the geometric features and appearance features, can be extracted by convolution or other learning filters; the latter, which are aimed to model the dynamic properties of facial motion, can be calculated through optical flow or other methods, respectively. When 3D convolutional neural networks (CNNs) are introduced, the extraction of two different types of features mentioned above becomes easy. In this paper, one 3D CNN architecture is presented to learn the static and dynamic features from facial image sequences and extract high-level dynamic features from optical flow sequences. Two types of dense optical flow, which contain the tracking information of facial muscle movement, are calculated according to different image pair construction methods. One is the common optical flow, and the other is an enhanced optical flow which is called accumulative optical flow. Four components of each type of optical flow are used in experiments. Three databases, two acted databases and one nearly realistic database, are selected to conduct the experiments. The experiments on the two acted databases achieve state-of-the-art accuracy, and indicate that the vertical component of optical flow has an advantage over other components in recognizing facial expression. The experimental results on the three selected databases show that more discriminative features can be learned from image sequences than from optical flow or accumulative optical flow sequences, and the accumulative optical flow contains more motion information than optical flow if the frame distance of the image pairs used to calculate them is not too large.	artificial neural network;convolution;convolutional neural network;database;experiment;high- and low-level;optical flow	Jianfeng Zhao;Xia Mao;Jian Zhang	2018	The Visual Computer	10.1007/s00371-018-1477-y	computer vision;discriminative model;convolutional neural network;artificial intelligence;computer science;convolution;facial muscles;facial expression;optical flow	Vision	35.79421900808245	-50.289023386979224	99096
ad563f3e81991a9f607d3480dd2986cd09a4f54c	fast detection of retail fraud using polar touch buttons	event recognition;image motion analysis;retail fraud detection;video signal processing;real time;data mining;video signal processing data mining fraud image motion analysis security of data;video analytics;algorithm design and analysis marketing and sales belts merchandise viterbi algorithm event detection detectors energy capture humans surveillance;fraud;video analytics retail fraud detection;data mining techniques retail fraud detection system polar touch button video analytic algorithm point of sale motion energy representation polar motion map event recognition techniques;point of sale;security of data;fraud detection	Video analytics have recently emerged as a promising technique of retail fraud detection for loss prevention. Efficient video analytic algorithms are highly desired for a practical fraud detection system. In this paper, we present a real-time algorithm for recognizing a cashier's actions at the Point of Sale (POS), which can be further used to analyze cashier behaviors for identifying fraudulent incidents. The algorithm uses a set of simple but effective features derived from a global representation of motion energy called Polar Motion Map (PMM). These features capture the motion patterns exhibited in a cashier's actions as a focused beam of motion energy, characterizing the actions as the extension and retraction movement of the cashier's arm with respect to a prespecified region. Our algorithm demonstrates comparable accuracy against one of the state-of-the-art event recognition techniques [1] while running significantly faster.	algorithm;point of sale;real-time clock;video content analysis	Quanfu Fan;Akira Yanagawa;Russell Bobbitt;Yun Zhai;Rick Kjeldsen;Sharath Pankanti;Arun Hampapur	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202732	computer vision;computer science;internet privacy;point of sale;computer security	Robotics	39.18361047972363	-45.13349617920384	99427
1876a6ea8d01ae3cb35c3b91ad474ec555d15171	trajectory predictor by using recurrent neural networks in visual tracking		Motion models have been proved to be a crucial part in the visual tracking process. In recent trackers, particle filter and sliding windows-based motion models have been widely used. Treating motion models as a sequence prediction problem, we can estimate the motion of objects using their trajectories. Moreover, it is possible to transfer the learned knowledge from annotated trajectories to new objects. Inspired by recent advance in deep learning for visual feature extraction and sequence prediction, we propose a trajectory predictor to learn prior knowledge from annotated trajectories and transfer it to predict the motion of target objects. In this predictor, convolutional neural networks extract the visual features of target objects. Long short-term memory model leverages the annotated trajectory priors as well as sequential visual information, which includes the tracked features and center locations of the target object, to predict the motion. Furthermore, to extend this method to videos in which it is difficult to obtain annotated trajectories, a dynamic weighted motion model that combines the proposed trajectory predictor with a random sampler is proposed. To evaluate the transfer performance of the proposed trajectory predictor, we annotated a real-world vehicle dataset. Experiment results on both this real-world vehicle dataset and an online tracker benchmark dataset indicate that the proposed method outperforms several state-of-the-art trackers.	area striata structure;artificial neural network;benchmark (computing);branch predictor;convolutional neural network;deep learning;disease response domain;dreamwidth;feature extraction;graphics processing unit;kerrison predictor;long short-term memory;microsoft windows;neural network simulation;neural networks;particle filter;performance evaluation;physical object;real-time clock;recurrent neural network;sampling (signal processing);silo (dataset);video tracking;recurrent childhood visual pathway glioma;videocassette	Lituan Wang;Lei Zhang;Zhang Yi	2017	IEEE Transactions on Cybernetics	10.1109/TCYB.2017.2705345	machine learning;convolutional neural network;artificial intelligence;prior probability;feature extraction;eye tracking;deep learning;trajectory;recurrent neural network;particle filter;computer science	Vision	27.523291815356274	-51.41798730136259	99440
986f8ff5835f1a93e21006569ce82e21a699ee5b	semi-latent dirichlet allocation: a hierarchical model for human action recognition	latent dirichlet allocation;computer vision;action recognition;bag of words;hierarchical model	We propose a new method for human action recognition from video sequences using latent topic models. Video sequences are represented by a novel “bag-of-words” representation, where each frame corresponds to a “word”. The major difference between our model and previous latent topic models for recognition problems in computer vision is that, our model is trained in a “semi-supervised” way. Our model has several advantages over other similar models. First of all, the training is much easier due to the decoupling of the model parameters. Secondly, it naturally solves the problem of how to choose the appropriate number of latent topics. Thirdly, it achieves much better performance by utilizing the information provided by the class labels in the training set. We present action classification and irregularity detection results, and show improvement over previous methods.	algorithm;bag-of-words model;computer vision;coupling (computer programming);experiment;hierarchical database model;latent dirichlet allocation;preprocessor;semiconductor industry;statistical classification;statistical model;test set	Yang Wang;Payam Sabzmeydani;Greg Mori	2007		10.1007/978-3-540-75703-0_17	latent dirichlet allocation;speech recognition;computer science;machine learning;pattern recognition	Vision	32.69066411296928	-47.854612511798585	99486
9323656e93f137b798c7ac443aeaeac045a7fae7	predicting pedestrian counts in crowded scenes with rich and high-dimensional features	prediction method;statistical landscape features slfs;video surveillance;high dimensionality;image processing;ensemble learning;gaussian processes;intelligent transport system;dimension reduction;edge detection;visual texture recognition;statistical model;journal;surveillance video intelligent transportation systems statistical learning algorithms statistical modeling regression problem pedestrian count prediction high dimensional feature space surveillance image;accuracy;prediction methods;statistical learning;image edge detection;feature extraction;statistical landscape features slfs ensemble learning gaussian processes kernel dimension reduction kdr pedestrian counting;kernel dimension reduction kdr;visual features;statistical inference;image edge detection feature extraction statistical learning prediction methods accuracy;traffic engineering computing;video surveillance learning artificial intelligence regression analysis traffic engineering computing;regression analysis;gaussian process;pedestrian counting;learning artificial intelligence;estimates;pedestrian counts	Estimating the number of pedestrians in surveillance images and videos has important applications in intelligent transportation systems. This problem is particularly challenging when the scenes are densely crowded, in which the techniques of tracking a single pedestrian has limited effectiveness. Alternative approaches employ statistical learning algorithms to infer pedestrian counts directly from visual features computed on images or scenes. In this paper, we describe a system for predicting pedestrian counts that significantly extends the utility of those ideas. Our approach incorporates a richer set of features for statistical modeling. While these features give rise to regression problems in a high-dimensional space, we leverage learning techniques to reduce dimensionality while still attaining high accuracy for predicting the number of pedestrians. Empirical results have validated our strategy. Specifically, our system outperforms state-of-the-art methods on standard benchmark tasks by a large margin.	algorithm;benchmark (computing);machine learning;statistical model	Junping Zhang;Ben Tan;Fei Sha;Li He	2011	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2011.2132759	computer vision;image processing;computer science;machine learning;pattern recognition;gaussian process;mathematics;ensemble learning;statistics	Vision	33.588221753717065	-52.05667987691444	99494
d337eca1bfa874fe20e8e803154945c67890ff37	learning convolutional filters for interest point detection	image sampling;stereo image processing convolution distance measurement feature extraction filtering theory image sampling learning artificial intelligence random processes search problems;convolution;distance measurement;learning convolutional filters ground truth data learning system search space search bias random sampling feature detector parameterization stereo visual odometry in situ learning approach feature detectors interest point detection;feature extraction;stereo image processing;random processes;search problems;learning artificial intelligence;filtering theory;detectors feature extraction visualization cameras discrete cosine transforms three dimensional displays optimization	We present a method for learning efficient feature detectors based on in-situ evaluation as an alternative to hand-engineered feature detection methods. We demonstrate our in-situ learning approach by developing a feature detector optimized for stereo visual odometry. Our feature detector parameterization is that of a convolutional filter. We show that feature detectors competitive with the best hand-designed alternatives can be learned by random sampling in the space of convolutional filters and we provide a way to bias the search toward regions of the search space that produce effective results. Further, we describe our approach for obtaining the ground-truth data needed by our learning system in real, everyday environments.	feature detection (computer vision);feature detection (web development);interest point detection;sampling (signal processing);sensor;visual odometry	Andrew Richardson;Edwin Olson	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6630639	stochastic process;feature learning;computer vision;feature detection;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;convolution;k-nearest neighbors algorithm;feature;dimensionality reduction	Robotics	36.551786922371285	-44.19029527845994	99524
8d1124f2525761bf46433214f52840114e931722	unbalanced learning in content-based image classification and retrieval	classifier overtraining unbalanced learning content based image classification digital cameras personal computer internet content based image retrieval problem relevance feedback techniques nearest neighbor paradigm k nn paradigm;small sample size;personal computer;nearest neighbor paradigm;training;semantics;digital camera;image classification;content based image retrieval problem;feature space;content based image classification;noise measurement;digital cameras;artificial neural networks;internet;image classification unbalanced learning small sample size artificial pattern injection image retrieval;relevance feedback content based retrieval image classification learning artificial intelligence;image color analysis;nearest neighbor;semantics image retrieval artificial neural networks training image color analysis noise noise measurement;classifier overtraining;unbalanced learning;relevance feedback techniques;digital image;learning artificial intelligence;content based image retrieval;relevance feedback;content based retrieval;noise;k nn paradigm;generalization capability;image retrieval;artificial pattern injection	Nowadays very large archives of digital images can be easily produced thanks to the availability of digital cameras as standalone devices, or embedded into a number of portable devices. Each personal computer is typically a repository for thousands of images, while the Internet can be seen as a very large repository. One of the most severe problems in the classification and retrieval of images from very large repositories is the very limited number of elements belonging to each semantic class compared to the number of images in the repository. As a consequence, an even smaller fraction of images per semantic class can be used as training set in a classification problem, or as a query in a content-based image retrieval problem. In this paper we propose a technique aimed at artificially increasing the number of examples in the training set in order to improve the learning capabilities, reducing the unbalance between the semantic class of interest, and all other images. The proposed approach is tailored to classification and relevance feedback techniques based on the Nearest-Neighbor paradigm. A number of new points in the feature space are created based on the available training patterns, so that they better represent the distribution of the semantic class of interest. These new points are created according to the k-NN paradigm, and take into account both relevant and non-relevant images with respect to the semantic class of interest. The proposed approach allows increasing the generalization capability of NN techniques, and mitigates the risk of classifier over-training on few patterns. Reported experiments show the effectiveness of the proposed technique in Content-Based Image Retrieval tasks, where the Nearest-Neighbor approach is used to exploit user's relevance feedback. The improvement in precision and recall gained in one feature space allows also to outperform the improvement in performances attained by combining different feature spaces.	archive;computer vision;content-based image retrieval;digital camera;digital image;embedded system;experiment;feature vector;internet;k-nearest neighbors algorithm;mobile device;performance;personal computer;precision and recall;programming paradigm;relevance feedback;test set	Luca Piras;Giorgio Giacinto	2010	2010 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2010.5583045	computer vision;contextual image classification;the internet;feature vector;image retrieval;computer science;noise measurement;noise;machine learning;pattern recognition;semantics;k-nearest neighbors algorithm;digital image	Vision	26.810708679359426	-48.03277881861204	99536
0c59071ddd33849bd431165bc2d21bbe165a81e0	person recognition in personal photo collections	head face recognition training context robustness;image recognition computer vision;pipa personal photo collections everyday photos machine vision convnet based person recognition system body cues training data open source open data social media photos	Recognising persons in everyday photos presents major challenges (occluded faces, different clothing, locations, etc.) for machine vision. We propose a convnet based person recognition system on which we provide an in-depth analysis of informativeness of different body cues, impact of training data, and the common failure modes of the system. In addition, we discuss the limitations of existing benchmarks and propose more challenging ones. Our method is simple and is built on open source and open data, yet it improves the state of the art results on a large dataset of social media photos (PIPA).	benchmark (computing);convolutional neural network;database;feature learning;machine vision;open-source software;social media	Seong Joon Oh;Rodrigo Benenson;Mario Fritz;Bernt Schiele	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.440	computer vision;computer science;multimedia	Vision	32.78877043716294	-51.3480190255285	99576
09628e9116e7890bc65ebeabaaa5f607c9847bae	semantically consistent regularization for zero-shot recognition		The role of semantics in zero-shot learning is considered. The effectiveness of previous approaches is analyzed according to the form of supervision provided. While some learn semantics independently, others only supervise the semantic subspace explained by training classes. Thus, the former is able to constrain the whole space but lacks the ability to model semantic correlations. The latter addresses this issue but leaves part of the semantic space unsupervised. This complementarity is exploited in a new convolutional neural network (CNN) framework, which proposes the use of semantics as constraints for recognition. Although a CNN trained for classification has no transfer ability, this can be encouraged by learning an hidden semantic layer together with a semantic code for classification. Two forms of semantic constraints are then introduced. The first is a loss-based regularizer that introduces a generalization constraint on each semantic predictor. The second is a codeword regularizer that favors semantic-to-class mappings consistent with prior semantic knowledge while allowing these to be learned from data. Significant improvements over the state-of-the-art are achieved on several datasets.	artificial neural network;code word;complementarity theory;convolutional neural network;kerrison predictor	Pedro Morgado;Nuno Vasconcelos	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.220	code word;artificial intelligence;convolutional neural network;semantic computing;semantic similarity;machine learning;computer vision;natural language processing;pattern recognition;semantic memory;computer science;semantic compression;subspace topology;semantics	Vision	24.94251045089624	-46.86455859191829	100031
f3757ab90dd5a5f267c1e58ee23d2387f910e526	representation and recognition of agent interactions using marking analysis in generalized stochastic petri nets	logical relation;global scene view;generalized stochastic petri nets;video event representation;agent interaction;time dependent;video surveillance;surveillance;surveillance system;marking analysis;behavior modeling;bayesian methods;layout;time dependent activities;multiple objectives;hidden markov models;stochastic processes;surveillance system marking analysis generalized stochastic petri nets video event representation multiagent interactions time dependent activities global scene view;spatiotemporal phenomena;video surveillance petri nets stochastic processes;multiagent interactions;stochastic processes petri nets layout hidden markov models delay surveillance spatiotemporal phenomena bayesian methods fires information analysis;petri nets;scene understanding;petri net;fires;information analysis;generalized stochastic petri net	This paper presents a novel approach for video event representation and recognition of multi agent interactions. The proposed approach integrates behavior modeling techniques based on Generalized Stochastic Petri Nets (GSPN) and introduces Petri net marking analysis for better scene understanding. The GSPN model provides remarkable flexibility in representation of time dependent activities which usually coexist with logical, spatial and temporal relations in real life scenes. The nature of Petri net concept allows efficient modeling of the complex sequential and simultaneous activities but disregards the global scope of a given model. The proposed marking analysis creates a new model extension based on the global scene view and uses historical and training information for current and future state interpretations. The GSPN approach is evaluated using the developed surveillance system which can recognize events from videos and give a textual expression for the detected behavior. The experimental results illustrate the ability of the system to create complex spatio-temporal and logical relations and to recognize the interactions of multiple objects in various video scenes using GSPN and marking analysis capabilities.	behavior model;coexist (image);interaction;item unique identification;logical relations;petri net;real life;scope (computer science)	Artyom Borzin;Ehud Rivlin;Michael Rudzsky	2007	2007 International Workshop on Content-Based Multimedia Indexing	10.1109/CBMI.2007.385389	stochastic process;simulation;computer science;artificial intelligence;machine learning;petri net;hidden markov model;statistics	Vision	38.47732521105893	-47.55293991316378	100108
c89a230ac0faee883a42b88c3349dec59630e39a	investigation in spatial-temporal domain for face spoof detection		This paper focuses on face spoofing detection using video. The purpose is to find out the best scheme for this task in the end-to-end learning manner. We investigate 4 different types of structure to fully exploit the raw data in its spatial-temporal domain, which are the pure CNN, CNN with 3D convolution, CNN+LSTM and CNN+Conv-LSTM. Moreover, another stream built on optical flow is also used, and with a proper fusion method, it can improve the accuracy. In experiments, we compare schemes on the raw data in single stream and fusion methods with optical flow in two streams. The performance are not only given within each dataset, but also measured across different datsets, which is crucial to avoid the overfitting.	collision detection;convolution;end-to-end principle;experiment;long short-term memory;optical flow;overfitting	Zhonglin Sun;Li Sun;Qingli Li	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461942	task analysis;overfitting;raw data;streams;spoofing attack;artificial intelligence;pattern recognition;exploit;convolution;computer science;optical flow	Vision	28.038509011798173	-51.68620028633696	100206
5d1608e03ab9c529d0b05631f9d2a3afcbf1c3e3	sparsity and robustness in face recognition	face recognition;error correction;signal representation;pattern recognition;sparse representation	Background. This note concerns the use of techniques for sparse signal representation and sparse error correction for automatic face recognition. Much of the recent interest in these techniques comes from the paper [WYG09], which showed how, under certain technical conditions, one could cast the face recognition problem as one of seeking a sparse representation of a given input face image in terms of a “dictionary” of training images and images of individual pixels. To be more precise, the method of [WYG09] assumes access to a sufficient number of well-aligned training images of each of the k subjects. These images are stacked as the columns of matrices A1, . . . ,Ak. Given a new test image y, also well aligned, but possibly subject to illumination variation or occlusion, the method of [WYG09] seeks to represent y as a sparse linear combination of the database as whole. Writing A = [A1 | · · · | Ak], this approach solves	column (database);dictionary;error detection and correction;facial recognition system;illumination (image);pixel;sparse approximation;sparse matrix;standard test image	John Wright;Arvind Ganesh;Allen Y. Yang;Zihan Zhou;Yi Ma	2011	CoRR		facial recognition system;computer vision;error detection and correction;computer science;machine learning;pattern recognition;sparse approximation;three-dimensional face recognition	Vision	29.3294905731617	-46.883633876753734	100216
2a0e2e787d16f32a14c6f5a6b0b9bf116c3103f1	deep learning on lie groups for skeleton-based action recognition		In recent years, skeleton-based action recognition has become a popular 3D classification problem. State-of-the-art methods typically first represent each motion sequence as a high-dimensional trajectory on a Lie group with an additional dynamic time warping, and then shallowly learn favorable Lie group features. In this paper we incorporate the Lie group structure into a deep network architecture to learn more appropriate Lie group features for 3D action recognition. Within the network structure, we design rotation mapping layers to transform the input Lie group features into desirable ones, which are aligned better in the temporal domain. To reduce the high feature dimensionality, the architecture is equipped with rotation pooling layers for the elements on the Lie group. Furthermore, we propose a logarithm mapping layer to map the resulting manifold data into a tangent space that facilitates the application of regular output layers for the final classification. Evaluations of the proposed network for standard 3D human action recognition datasets clearly demonstrate its superiority over existing shallow Lie group feature learning methods as well as most conventional deep learning methods.	deep learning;dynamic time warping;end-to-end principle;feature learning;feature model;isometric projection;network architecture;rectifier (neural networks)	Zhiwu Huang;Chengde Wan;Thomas Probst;Luc Van Gool	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.137	computer vision;artificial intelligence;machine learning;curse of dimensionality;artificial neural network;computer science;pattern recognition;feature extraction;deep learning;dynamic time warping;lie group;feature learning;contextual image classification	Vision	25.29223579093646	-50.23738414586847	100238
f2d52a5996263ff80a011a511d878e082c05348e	a coarse-to-fine semi-supervised change detection for multispectral images		Change detection is an important technique providing insights to urban planning, resources monitoring, and environmental studies. For multispectral images, most semi-supervised change detection methods focus on improving the contribution of training samples hard to be classified to the trained classifier. However, hard training samples will weaken the discrimination of the training model for multispectral change detection. Besides, these methods only use the spectral information, while the limited spectral information cannot represent objects very well. In this paper, a method named as coarse-to-fine semi-supervised change detection is proposed to solve the aforementioned problems. First, a novel multiscale feature is exploited by concatenating the spectral vector of the pixel to be detected and its adjacent pixels by different scales. Second, the enhanced metric learning is proposed to acquire more discriminant metric by strengthening the contribution of training samples easy to be classified and weakening the contribution of training samples hard to be classified to the trained model. Finally, a coarse-to-fine strategy is adopted to detect testing samples from the viewpoint of distance metric and label information of neighborhood in spatial space. The coarse detection result obtained from the enhanced metric learning is used to guide the final detection. The effectiveness of our proposed method is verified on two real-life operating scenarios, Taizhou and Kunshan data sets. Extensive experimental results demonstrate that our proposed algorithm has better performance than those of other state-of-the-art algorithms.	algorithm;concatenation;discriminant;multispectral image;pixel;real life;semi-supervised learning;semiconductor industry	Wuxia Zhang;Xiaoqiang Lu;Xuelong Li	2018	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2018.2802785	metric (mathematics);computer vision;artificial intelligence;pixel;multispectral image;feature extraction;mathematics;change detection;environmental studies;hyperspectral imaging;data set	Vision	31.450298459388318	-45.79801156095652	100253
296fde2acd81a1b2dadbd89b20436911ed3d6ead	zoom-net: mining deep feature interactions for visual relationship recognition		Recognizing visual relationships (langle )subject-predicate-object(rangle ) among any pair of localized objects is pivotal for image understanding. Previous studies have shown remarkable progress in exploiting linguistic priors or external textual information to improve the performance. In this work, we investigate an orthogonal perspective based on feature interactions. We show that by encouraging deep message propagation and interactions between local object features and global predicate features, one can achieve compelling performance in recognizing complex relationships without using any linguistic priors. To this end, we present two new pooling cells to encourage feature interactions: (i) Contrastive ROI Pooling Cell, which has a unique deROI pooling that inversely pools local object features to the corresponding area of global predicate features. (ii) Pyramid ROI Pooling Cell, which broadcasts global predicate features to reinforce local object features. The two cells constitute a Spatiality-Context-Appearance Module (SCA-M), which can be further stacked consecutively to form our final Zoom-Net. We further shed light on how one could resolve ambiguous and noisy object and predicate annotations by Intra-Hierarchical trees (IH-tree). Extensive experiments conducted on Visual Genome dataset demonstrate the effectiveness of our feature-oriented approach compared to state-of-the-art methods (Acc@1 (11.42%) from (8.16%)) that depend on explicit modeling of linguistic interactions. We further show that SCA-M can be incorporated seamlessly into existing approaches to improve the performance by a large margin.	computer vision;experiment;explicit modeling;feature learning;global serializability;image retrieval;interaction;lvm;region of interest;software propagation;virtual retinal display	Guojun Yin;Lu Sheng;Bin Liu;Nenghai Yu;Xiaogang Wang;Jing Shao;Chen Change Loy	2018		10.1007/978-3-030-01219-9_20	pattern recognition;machine learning;computer science;artificial intelligence;predicate (grammar);prior probability;pooling;zoom	Vision	28.850206513880263	-52.01036440654232	100370
4053ef7d9ec1fd21d54b17609e6e1afea6bb4940	representing feature quantization approach using spatial-temporal relation for action recognition	spatio temporal feature;feature quantization;histogram;action recognition	In this paper we propose an efficient & intuitive algorithm for the design of feature vector quantization using space-time interest point in video surveillance. The performance of activity recognition is generally depend upon the quantity of significant features but with proper feature quantization one can delivered the same performance with less number of features. The basic characteristics of algorithm are discussed and demonstrated by experiment. It is scalable in nature and work efficiently under varying conditions. In an experiment section, we show that our novel feature quantization approach takes less number of features in compared to standard quantization, while delivering the same performance.		Sarvesh Vishwakarma;Anupam Agrawal	2012		10.1007/978-3-642-27387-2_13	computer vision;feature;machine learning;pattern recognition;mathematics;linde–buzo–gray algorithm;quantization;feature;vector quantization	Vision	36.324504275104545	-51.78257339044238	100636
0f7df0a9d96141a9ad4d34ae08163f09437bdd4c	visual image reconstruction from fmri activation using multi-scale support vector machine decoders	fmri;image reconstrucion;multi-scale;svm	The correspondence between the detailed contents of a personu0027s men- tal state and human neuroimaging has yet to be fully explored. Previous re- search reconstructed contrast-defined images using combination of multi-scale local image decoders, where contrast for local image bases was predicted from fMRI activity by sparse logistic regression (SLR). The present study extends this research to probe into accurate and effective reconstruction of images from fMRI. First, support vector machine (SVM) was employed to model the rela- tionship between contrast of local image and fMRI; second, additional 3-pixel image bases were considered. Reconstruction results demonstrated that the time consumption in modeling the local image decoder was reduced to 1% by SVM compared to SLR. Our method also improved the spatial correlation between the stimulus and reconstructed image. This finding indicated that our method could read out what a subject was viewing and reconstruct simple images from brain activity at a high speed.	iterative reconstruction;support vector machine	Yu Zhan;Jiacai Zhang;Sutao Song;Li Yao	2013		10.1007/978-3-642-39342-6_54	computer vision;computer science;machine learning;pattern recognition	ML	31.933117921753738	-49.37948607815168	100734
135fe2a0a0e6b726e5d81299edad4b3ce39d6614	multichannel-kernel canonical correlation analysis for cross-view person reidentification	late fusion;person re identification;kcca;person reidentification	In this article, we introduce a method to overcome one of the main challenges of person reidentification in multicamera networks, namely cross-view appearance changes. The proposed solution addresses the extreme variability of person appearance in different camera views by exploiting multiple feature representations. For each feature, kernel canonical correlation analysis with different kernels is employed to learn several projection spaces in which the appearance correlation between samples of the same person observed from different cameras is maximized. An iterative logistic regression is finally used to select and weight the contributions of each projection and perform the matching between the two views. Experimental evaluation shows that the proposed solution obtains comparable performance on the VIPeR and PRID 450s datasets and improves on the PRID and CUHK01 datasets with respect to the state of the art.	computational complexity theory;iteration;iterative method;kernel (operating system);logistic regression;multimedia pc;spatial variability	Giuseppe Lisanti;Svebor Karaman;Iacopo Masi	2017	TOMCCAP	10.1145/3038916	computer vision;machine learning;pattern recognition	Vision	32.981638058341794	-48.582610886588434	100789
f2dd9899f57e0da087752423d8ce002ad9740a3b	use of roadway scene semantic information and geometry-preserving landmark pairs to improve visual place recognition in changing environments	image matching;semantics;navigation;visualization;feature extraction;robots;robustness	Visual place recognition (VPR) in changing environments is an urgent challenge for long-term autonomous navigation. One recent ConvNet landmark-based approach exploits region landmarks coupled with ConvNet features to match images, and the approach has shown promising results under significant environmental and viewpoint changes. In this paper, we propose a robust ConvNet landmark-based system for VPR in changing outdoor roadway environments by extension of this approach from the following two aspects. First, our method utilizes more discriminative landmarks obtained by a novel refinement method called SemLandmarks, which leverages roadway scene semantic information to screen landmarks directly detected by an existing object proposal method. Second, our method improves the accuracy of image matching by introducing consistent spatial constraints based on the use of geometry-preserving landmark pairs. Experimental results demonstrate that our method significantly improves the state of the art in VPR in terms of recognition accuracy on three challenging benchmark data sets with various environmental and viewpoint changes.	autonomous robot;benchmark (computing);convolutional neural network;discriminative model;image registration;refinement (computing)	Yi Hou;Hong Zhang;Shilin Zhou;Huanxin Zou	2017	IEEE Access	10.1109/ACCESS.2017.2698524	robot;computer vision;navigation;simulation;visualization;feature extraction;computer science;semantics;robustness	Vision	32.54987919209481	-50.87328733115294	100801
0f9e3fc917797ea0d43f0be34fc4690bcb3cd356	fully convolutional siamese fusion networks for object tracking		In this paper, we propose fully convolutional siamese fusion networks for object tracking. We adopt the fusion strategy of convolutional layers for object tracking to achieve good feature representation based on convolutional neural networks. Specifically, we fuse three convolutional layers of VGGNet based on normalized cross correlation (NCC). First, we use three convolutional layers of VGGNet as the basis for fusion, and reduce the size of the convolutional layers based on a convolution kernel. Then, we resize the convolutional layers to be the same size as the deconvolutional layers for layer fusion. Next, we fuse the three layers based on NCC between the target and search region, and produce the response map. Finally, we get the tracking result from the response map by the maximum response. Various experiments on large-scale data sets verify that the proposed method is robust to occlusion, deformation, motion blur, and background clutter as well as outperforms state-of-the-art trackers in terms of distance precision and overlap success.		Miaobin Cen;Zhendong Zhang	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451102	cross-correlation;computer vision;motion blur;convolutional neural network;kernel (image processing);video tracking;artificial intelligence;fusion;clutter;pattern recognition;computer science	Vision	31.394616772030616	-51.19527552283589	100919
71ce856c4a83d71409390f657499a3085c139e41	improving human action recognition using fusion of depth camera and inertial sensors	biomedical monitoring;wearable inertial sensor;depth motion map dmm;training;fusion of depth camera and inertial sensor;berkeley multimodal human action database human action recognition depth camera fusion inertial sensor fusion modality sensors fusion approach action feature extraction depth images accelerometer signals depth motion maps statistical signal attributes feature level fusion decision level fusion collaborative representation classifier dempster shafer theory classification outcomes;feature extraction;human action recognition;wearable inertial sensor depth motion map dmm fusion of depth camera and inertial sensor human action recognition;cameras feature extraction accelerometers sensor fusion training biomedical monitoring;uncertainty handling feature extraction image classification image fusion image representation inference mechanisms object recognition;sensor fusion;accelerometers;cameras	This paper presents a fusion approach for improving human action recognition based on two differing modality sensors consisting of a depth camera and an inertial body sensor. Computationally efficient action features are extracted from depth images provided by the depth camera and from accelerometer signals provided by the inertial body sensor. These features consist of depth motion maps and statistical signal attributes. For action recognition, both feature-level fusion and decision-level fusion are examined by using a collaborative representation classifier. In the feature-level fusion, features generated from the two differing modality sensors are merged before classification, while in the decision-level fusion, the Dempster-Shafer theory is used to combine the classification outcomes from two classifiers, each corresponding to one sensor. The introduced fusion framework is evaluated using the Berkeley multimodal human action database. The results indicate that because of the complementary aspect of the data from these sensors, the introduced fusion approaches lead to 2% to 23% recognition rate improvements depending on the action over the situations when each sensor is used individually.	map;modality (human–computer interaction);multimodal interaction;sensor	Chen Chen;Roozbeh Jafari;Nasser Kehtarnavaz	2015	IEEE Transactions on Human-Machine Systems	10.1109/THMS.2014.2362520	computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;sensor fusion;accelerometer	Vision	35.81766446206216	-48.87484790401452	101059
887199931819276606f5bf39dc288bc640d017a8	a novel borderline preserving embedding manifold learning algorithm	borderline preserving embedding bpe;borderline preserving embedding manifold learning algorithm face recognition classification accuracy representation local reconstruction subspace learning techniques bpe dimensionality reduction technique;manifold learning;face recognition manifolds face eigenvalues and eigenfunctions training laplace equations principal component analysis;face recognition;dimensionality reduction;borderline preserving embedding bpe dimensionality reduction face recognition manifold learning;learning artificial intelligence data reduction image recognition	The notorious curse of dimensionality is a well-known phenomenon in pattern recognition. A lot of algorithms have been proposed to find a compact representation of data as well as to facilitate the recognition task. In order to solve the problem of dimension disaster, a novel dimensionality reduction technique called borderline preserving embedding (BPE) is proposed in this paper. Unlike the traditional dimensional reduction algorithms such as principal component analysis (PCA) and linear discriminant analysis (LDA) which project data in a global sense, BPE seeks for a local structure in the manifold. From this perspective, it is similar to other subspace learning techniques. However, BPE has the advantage of preserving the borderline in local reconstruction. Theoretical analysis and experimental study show that the improved manifold learning algorithm can provide better representation in low dimensional space and achieves higher classification accuracy in face recognition in comparison with traditional dimensionality reduction algorithms.	algorithm;curse of dimensionality;experiment;facial recognition system;linear discriminant analysis;mathematical optimization;nonlinear dimensionality reduction;norton power eraser;optimization problem;pattern recognition;principal component analysis;statistical classification;test data	Ruqing Chen	2013	2013 Ninth International Conference on Natural Computation (ICNC)	10.1109/ICNC.2013.6818099	computer vision;machine learning;pattern recognition;mathematics;dimensionality reduction	Vision	25.58168853906057	-42.052466141552046	101097
982b7f3bf02233d83eafae75b27d0e9aeb7f35cb	deep event learning boost-up approach: delta		Nowadays, the video surveillance systems may be omnipresent, but essential for supervision everywhere, e.g., ATM, airport, railway station and other crowded situations. In the multi-view video systems, various cameras are producing a huge amount of video content around the clock which makes it difficult for fast browsing, retrieval, and analysis. Accessing and managing such huge data in real time becomes a real challenging task because of inter-view dependencies, illumination changes and the bearing of many inactive frames. The work highlights an accurate and efficient technique to detect and summarize the event in multi-view surveillance videos using boosting, a machine learning algorithm, as a solution to the above issues. Interview dependencies across multiple views of the video are captured via weak learning classifiers in boosting algorithm. The light changes and still frames are tackled with moving an object in the frame by Deep learning framework. It helps to reach the correct decision for the active frame and inactive frame, without any prior information about the number of issues in a video. Target, as well as subjective ratings, clearly indicate the potency of our proposed DELTA model, where it successfully reduces the video data, while keeping the important information as events.	atm turbo;adaboost;algorithm;boosting (machine learning);closed-circuit television;deep learning;digital video;interdependence;key frame;machine learning;real-time computing;test set	Krishan Kumar;Deepti D. Shrimankar	2018	Multimedia Tools and Applications	10.1007/s11042-018-5882-z	adaboost;boosting (machine learning);delta;computer vision;deep learning;key frame;computer science;artificial intelligence;delta model;machine learning	Vision	31.925565299230783	-50.02148616960348	101105
44affd82cdda52b389bd78e33f5507b34d829e44	locality constrained low-rank representation for hyperspectral image classification	hyperspectral imaging training testing support vector machines sparse matrices;pattern recognition computer vision geophysical techniques hyperspectral imaging;support vector machines;training;aviris hyperspectral image low rank representation hyperspectral image classification computer vision pattern recognition subspace segmentation spectral similarity locality constrained lrr;testing;local constraint hyperspectral image classification low rank representation lrr;hyperspectral imaging;sparse matrices	This paper addresses the problem of hyperspectral image classification with the low-rank representation (LRR) which has been widely applied in computer vision and pattern recognition. As is known, it has been proved to be effective in subspace segmentation under the assumption that all the subspaces are mutually independent. Nevertheless, in practical applications, this assumption could hardly be guaranteed. In this paper, to sidestep this limitation, we simultaneously exploit the spectral similarity and spatial information of pixels to design a local constraint as the regularizer of LRR, which is referred to as the locality constrained LRR (LCLRR). The experimental results on the AVIRIS hyperspectral image confirm the effectiveness of our proposed method.	computer vision;locality of reference;pattern recognition;pixel	Lei Pan;Hengchao Li;Xiang-dong Chen	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7729122	support vector machine;computer vision;sparse matrix;computer science;hyperspectral imaging;machine learning;pattern recognition;mathematics;software testing;remote sensing	Vision	29.288378743025962	-43.478792769386004	101120
232d68a0b3a2479a77ae1804a0a6c59ca5635ac6	sparse representation for recognizing object-to-object actions under occlusions	action analysis;hamming distance classification;sparse coding;occlusions	In this paper, we describe the formatting guidelines for ACM SIG Proceedings. This paper proposes a novel event classification scheme to analyze various interaction actions between persons using sparse representation. The occlusion problem and the high complexity to model complicated interactions are two major challenges in person-to-person action analysis. To address the occlusion problem, the proposed scheme represents an action sample in an over-complete dictionary whose base elements are the training samples themselves. This representation is naturally sparse and makes errors (caused by different environmental changes like lighting or occlusions) sparsely appear in the training library. Because of the sparsity, it is robust to occlusions and lighting changes. In addition, a novel Hamming distance classification (HDC) scheme is proposed to classify action events to detailed types. Because the nature of Hamming code is highly tolerant to noise, the HDC scheme is also robust to occlusions. The high complexity of complicated action modeling can be tackled by adding more examples to the over-complete dictionary. Thus, even though the interaction relations are complicated, the proposed method still works successfully to recognize them and can be easily extended to analyze action events among multiple persons. More importantly, the HDC scheme is very efficient and suitable for real-time applications because no optimization process is involved to calculate the reconstruction error.	cellular automaton;dictionary;graphics device interface;hamming code;hamming distance;interaction;mathematical optimization;real-time clock;sparse approximation;sparse matrix	Jun-Wei Hsieh;Kai-Ting Chuang;Yilin Yan;Li-Chih Chen	2013		10.1145/2499788.2499843	computer vision;computer science;machine learning;data mining	Vision	33.5534839272946	-46.75276651676613	101182
3e667c54e848233db092b794f2cfbf47ea63b771	combined convolutional and recurrent neural networks for hierarchical classification of images		Deep learning models based on CNNs are predominantly used in image classification tasks. Such approaches, assuming independence of object categories, normally use a CNN as a feature learner and apply a flat classifier on top of it. Object classes in many settings have hierarchical relations, and classifiers exploiting these relations should perform better. We propose hierarchical classification models combining a CNN to extract hierarchical representations of images, and an RNN or sequence-to-sequence model to capture a hierarchical tree of classes. In addition, we apply residual learning to the RNN part in oder to facilitate training our compound model and improve generalization of the model. Experimental results on a real world proprietary dataset of images show that our hierarchical networks perform better than state-of-the-art CNNs.	artificial neural network;computer vision;deep learning;feature learning;feature vector;random neural network;recurrent neural network;tree network	Jaehoon Koo;Diego Klabjan;Jean Utke	2018	CoRR		machine learning;residual;deep learning;mathematics;recurrent neural network;contextual image classification;artificial intelligence	ML	25.05316043551446	-48.776169637947135	101203
5d01b4bac3b5052dd1b23877b56ec47fe5797023	streaming video segmentation via short-term hierarchical segmentation and frame-by-frame markov random field optimization	agglomerative clustering;video segmentation;graph matching;online segmentation;streaming segmentation	[1] Khoreva, A., Galasso, F., Hein, M., Schiele, B.: Classifier based graph construction for video segmentation. In: CVPR. (2015) [2] Yi, S., Pavlovic, V.: Multi-cue structure preserving MRF for unconstrained video segmentation. In: ICCV. (2015) [3] Xu, C., Xiong, C., Corso, J.J.: Streaming hierarchical video segmentation. In: ECCV. (2012) [4] Galasso, F., Keuper, M., Brox, T., Schiele, B.: Spectral graph reduction for efficient image and streaming video segmentation. In: CVPR. (2014) • Feature extraction for each frame • Estimate both forward and backward optical flows • Over-segment each frame into superpixels (Mean-shift) • Color feature • LAB histogram (20 bins for each channel) • LAB and RGB BoW histogram (300 and 300 words) • Motion feature • Forward motion BoW histogram (100 words) • Backward motion BoW histogram (100 words) • Boundary feature • Image segmentation using various parameters	cvpr;european conference on computer vision;feature extraction;graph reduction;iccv;image segmentation;markov chain;markov random field;streaming media	Won-Dong Jang;Chang-Su Kim	2016		10.1007/978-3-319-46466-4_36	computer vision;computer science;theoretical computer science;segmentation-based object categorization;mathematics;hierarchical clustering;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation;matching	Vision	38.21367576795656	-52.009569029678175	101305
3883857fe4e66f7e3c88d5e91fe5450b7985b3d3	a self-organizing neural scheme for road detection in varied environments	kohonen self organizing map;unsupervised learning;self organizing maps;neural networks;autonomous vehicle;road detection;autonomous vehicle control;online learning architecture selforganizing neural scheme road detection drivable space detection autonomous vehicle control adaptive vision diverse outdoor conditions feature based classification kohonen self organizing map intelligent vehicle k nearest neighbor algorithm;automated highways;mobile robots;online learning;computer vision;self organising feature maps automated highways computer vision control engineering computing feature extraction learning artificial intelligence mobile robots road vehicles;roads training neurons classification algorithms accuracy feature extraction image edge detection;self organising feature maps;feature based classification autonomous vehicle control road detection neural networks unsupervised learning self organizing maps;feature extraction;intelligent vehicles;self organization;k nearest neighbor;self organized map;control engineering computing;learning artificial intelligence;feature based classification;autonomous control;neural network;road vehicles	Detection of a drivable space is a key step in the autonomous control of a vehicle. In this paper we propose an adaptive vision based algorithm for road detection in diverse outdoor conditions. Our novel approach employs feature based classification and uses the Kohonen Self-Organizing Map (SOM) for the purpose of road detection. The robustness of the algorithm lies in the unique ability of SOM to organize information while learning diverse inputs. Features used for the training and testing of SOM are identified. The proposed method is capable of working with structured as well as unstructured roads and noisy environments that may be encountered by an intelligent vehicle. The proposed technique is extensively compared with the k-Nearest Neighbor (KNN) algorithm. Results show that SOM outperforms KNN in classification consistency and is independent to the lighting conditions while taking comparable classification time which shows that the network can also be used as an online learning architecture.	autonomous robot;k-nearest neighbors algorithm;nearest-neighbor interpolation;organizing (structure);self-organization;self-organizing map	Usman Ali Malik;Syed Usman Ahmed;Faraz Kunwar	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033623	mobile robot;computer vision;self-organization;self-organizing map;feature extraction;computer science;artificial intelligence;machine learning;k-nearest neighbors algorithm;artificial neural network	Robotics	35.424406969059085	-39.15727543418158	101311
9c3b281bf268c96eba3052ab8c58a6aa212de9f6	joint regularized nearest points for image set based face recognition		Face recognition based on image set has attracted much attention due to its promising performance to overcome various variations. Recently, classifiers of regularized nearest points, including sparse approximated nearest points (SANP), regularized nearest points (RNP) and collaborative regularized nearest points (CRNP), have achieved state-of-the-art performance for image set based face recognition. From a query set and a single-class gallery set, SANP and RNP both generate a pair of nearest points, between which the distance is regarded as the between-set distance. However, the computing of nearest points for each single-class gallery set in SANP and RNP ignores collaboration and competition with other classes, which may cause a wrong-class gallery set to have a small between-set distance. CRNP used collaborative representation to overcome this shortcoming but it doesn't explicitly minimize the between-set distance. In order to solve these issues and fully exploit the advantages of nearest points based approaches, in this paper a novel joint regularized nearest points (JRNP) is proposed for face recognition based on image sets. In JRNP, the nearest point in the query set is generated by considering the entire gallery set of all classes; at the same time, JRNP explicitly minimizes the between-set distance of the query set and a single-class gallery set. Furthermore, we proposed algorithms of greedy JRNP and adaptive JRNP to solve the presented model, and the classification is then based on the joint distance between the regularized nearest points in image sets. Extensive experiments were conducted on benchmark databases (e.g., Honda/UCSD, CMU Mobo, You Tube Celebrities databases, and the large-scale You Tube Face datasets). The experimental results clearly show that our JRNP leads the performance in face recognition based on image sets.	facial recognition system	Meng Meng Yang;Xing Wang;Weiyang Liu;Linlin Shen	2017	Image Vision Comput.	10.1016/j.imavis.2016.07.008	machine learning;pattern recognition;data mining;mathematics	Vision	26.292244674575226	-43.40066920192998	101375
6111832ed676ad0789d030577c87d4a539242bd3	cu-net: coupled u-nets		We design a new connectivity pattern for the U-Net shape architecture. Given several U-Nets, we couple each U-Net pair through the connections of their semantic blocks, resulting in the coupled U-Nets (CU-Net). The coupling connections could make the information flow more efficiently across U-Nets. The feature reuse across U-Nets makes each U-Net very parameter efficient. We evaluate the coupled U-Nets on two benchmark datasets of human pose estimation. Both the accuracy and model parameter number are compared. The CU-Net obtains comparable accuracy as state-of-the-art methods. However, it only has at least 60% fewer parameters than other approaches.	benchmark (computing);bottom-up proteomics;tip (unix utility);top-down and bottom-up design	Zhiqiang Tang;Xi Peng;Shijie Geng;Yizhe Zhu;Dimitris N. Metaxas	2018			artificial intelligence;architecture;computer science;machine learning;reuse;information flow (information theory);pose	Vision	25.631229461427488	-52.03468362394153	101562
2d8b907f2ca8c47c7c436d9de86cd2e05b2e8d98	learning multi-modal dictionaries: application to audiovisual data	tratamiento datos;analisis contenido;base dato multidimensional;learning algorithm;audiovisual;multimedia;analisis estadistico;fonction generatrice;securite informatique;data processing;traitement donnee;statistical method;intelligence artificielle;algorithme apprentissage;probabilistic approach;multidimensional database;classification;audiovisual equipment;recurrence;computer security;content analysis;dictionnaire;statistical analysis;audiovisuel;enfoque probabilista;approche probabiliste;recurrencia;seguridad informatica;estructura datos;high dimensional data;dictionaries;analyse statistique;funcion generatriz;equipement audiovisuel;invariante;artificial intelligence;generating function;structure donnee;base donnee multidimensionnelle;inteligencia artificial;analyse contenu;source separation;algoritmo aprendizaje;equipo audiovisual;data structure;diccionario;clasificacion;shift invariant;invariant;lts2	This paper presents a methodology for extracting meaningful synchronous structures from multi-modal signals. Simultaneous processing of multi-modal data can reveal information that is unavailable when handling the sources separately. However, in natural high-dimensional data, the statistical dependencies between modalities are, most of the time, not obvious. Learning fundamental multi-modal patterns is an alternative to classical statistical methods. Typically, recurrent patterns are shift invariant, thus the learning should try to find the best matching filters. We present a new algorithm for iteratively learning multimodal generating functions that can be shifted at all positions in the signal. The proposed algorithm is applied to audiovisual sequences and it demonstrates to be able to discover underlying structures in the data.	algorithm;covox speech thing;dictionary;loss function;modal logic;multimodal interaction;optimization problem;programming paradigm	Gianluca Monaci;Philippe Jost;Pierre Vandergheynst;Boris Mailhé;Sylvain Lesage;Rémi Gribonval	2006		10.1007/11848035_71	generating function;speech recognition;data structure;data processing;content analysis;biological classification;computer science;artificial intelligence;invariant;database;programming language;computer security;algorithm;shift-invariant system;clustering high-dimensional data	ML	30.016276184457585	-39.13563589370249	101892
c268c0d62eac349468f786ac50342213ef7865e0	visual motif discovery via first-person vision		Visual motifs are images of visual experiences that are significant and shared across many people, such as an image of an informative sign viewed by many people and that of a familiar social situation such as when interacting with a clerk at a store. The goal of this study is to discover visual motifs from a collection of first-person videos recorded by a wearable camera. To achieve this goal, we develop a commonality clustering method that leverages three important aspects: inter-video similarity, intra-video sparseness, and people’s visual attention. The problem is posed as normalized spectral clustering, and is solved e ciently using a weighted covariance matrix. Experimental results suggest the e↵ectiveness of our method over several state-of-the-art methods in terms of both accuracy and e ciency of visual motif discovery.	cluster analysis;global positioning system;information sign;interaction;neural coding;sequence motif;simultaneous localization and mapping;spectral clustering;visual basic[.net];wearable computer	Ryo Yonetani;Kris Makoto Kitani;Yoichi Sato	2016		10.1007/978-3-319-46475-6_12	motif (music);machine learning;spectral clustering;artificial intelligence;computer science;wearable computer;cluster analysis;covariance matrix	Vision	36.26855603606429	-52.018989045292855	102009
e8b906123b2e21af0c903fd95f3b3aa440909247	adversarial zero-shot learning with semantic augmentation		In situations in which labels are expensive or difficult to obtain, deep neural networks for object recognition often suffer to achieve fair performance. Zero-shot learning is dedicated to this problem. It aims to recognize objects of unseen classes by transferring knowledge from seen classes via a shared intermediate representation. Using the manifold structure of seen training samples is widely regarded as important to learn a robust mapping between samples and the intermediate representation, which is crucial for transferring the knowledge. However, their irregular structures, such as the lack in variation of samples for certain classes and highly overlapping clusters of different classes, may result in an inappropriate mapping. Additionally, in a high dimensional mapping space, the hubness problem may arise, in which one of the unseen classes has a high possibility to be assigned to samples of different classes. To mitigate such problems, we use a generative adversarial network to synthesize samples with specified semantics to cover a higher diversity of given classes and interpolated semantics of pairs of classes. We propose a simple yet effective method for applying the augmented semantics to the hinge loss functions to learn a robust mapping. The proposed method was extensively evaluated on smalland largescale datasets, showing a significant improvement over stateof-the-art methods.	artificial neural network;deep learning;effective method;experiment;hinge loss;image retrieval;intermediate representation;interpolation;loss function;outline of object recognition;semantic mapper;synthetic intelligence	Bin Tong;Martin Klinkigt;Junwen Chen;Xiankun Cui;Quan Kong;Tomokazu Murakami;Yoshiyuki Kobayashi	2018			machine learning;adversarial system;artificial intelligence;computer science	AI	25.260123832901787	-49.596092796346056	102011
e565e0e4f906e348d654ca11f71484220d43812c	non-local auto-encoder with collaborative stabilization for image restoration	neurophysiology brain image denoising image resolution image restoration medical image processing neural nets;image restoration neurons biological neural networks visualization collaboration training image reconstruction;non local auto encoder network stabilization image restoration;brain image denoising image resolution image restoration medical image processing neural nets neurophysiology;nonlocal auto encoder image super resolution image denoising forward propagation nonlocal similar image block hidden representation network propagation natural images neurological signal neuron human brain pivotal property visual cues knowledge layerwise abstraction deep neural network image restoration collaborative stabilization;image restoration non local auto encoder network stabilization;image super resolution image denoising forward propagation nonlocal similar image block hidden representation network propagation natural images neurological signal neuron human brain pivotal property visual cues knowledge layerwise abstraction deep neural network image restoration collaborative stabilization nonlocal auto encoder	Deep neural networks have been applied to image restoration to achieve the top-level performance. From a neuroscience perspective, the layerwise abstraction of knowledge in a deep neural network can, to some extent, reveal the mechanisms of how visual cues are processed in human brain. A pivotal property of human brain is that similar visual cues can stimulate the same neuron to induce similar neurological signals. However, conventional neural networks do not consider this property, and the resulting models are, as a result, unstable regarding their internal propagation. In this paper, we develop the (stacked) non-local auto-encoder, which exploits self-similar information in natural images for stability. We propose that similar inputs should induce similar network propagation. This is achieved by constraining the difference between the hidden representations of non-local similar image blocks during training. By applying the proposed model to image restoration, we then develop a collaborative stabilization step to further rectify forward propagation. To obtain a reliable deep model, we employ several strategies to simplify training and improve testing. Extensive image restoration experiments, including image denoising and super-resolution, demonstrate the effectiveness of the proposed method.	algorithm;artificial neural network;autoencoder;biological neural networks;circuit restoration;control theory;deep learning;encoder device component;experiment;focus stacking;image restoration;neural network simulation;neuron;neuroscience discipline;noise reduction;self-similarity;software propagation;super-resolution imaging;turbulence;unstable medical device problem	Ruxin Wang;Dacheng Tao	2016	IEEE Transactions on Image Processing	10.1109/TIP.2016.2541318	image restoration;computer vision;feature detection;image processing;computer science;artificial intelligence;machine learning	Vision	24.981309077564813	-51.36962513914163	102258
188345f441f7e36fdcd8616723abfb7e6135bcf1	eigengait: motion-based recognition of people using image self-similarity	moving image;selfsimilarity;image recognition;reconocimiento imagen;analisis componente principal;walking;estimation mouvement;caminata;image processing;legged locomotion;biometrie;unbiased estimator;estimacion movimiento;locomotion avec jambes;gait;biometrics;biometria;procesamiento imagen;motion estimation;marcha;gait recognition;imagen movil;traitement image;image mobile;face recognition;marche a pied;principal component analysis;autosimilitud;reconnaissance image;analyse composante principale;autosimilitude;k nearest neighbor;allure;leave one out cross validation	We present a novel technique for motion-based recognition of individual gaits in monocular sequences. Recent work has suggested that the image self-similarity plot of a moving person/object is a projection of its planar dynamics. Hence we expect that these plots encode much information about gait motion patterns, and that they can serve as good discriminants between different types of gait. We propose a method for gait recognition that uses similarity plots the same way that face images are used in eigenface-based face recognition techniques. Specifically, we first apply Principal Component Analysis (PCA) to a set of training similarity plots. This maps them to a lower dimensional space with less unwanted variation. A supervised clustering technique is then used to group these training samples into classes (clusters) in this space. Gait recognition is subsequently done by assigning the similarity plot of an input gait to the closest cluster. We test our method on a data set of 42 sequences at 30 fps each, depicting six different walking subjects. We use k-means clustering to build the classifier and the leave-one-out cross-validation technique to obtain an unbiased estimate of the recognition rate of about 93% (39	cluster analysis;cross-validation (statistics);encode;eigenface;facial recognition system;gait analysis;k-means clustering;map;population;principal component analysis;self-similarity;semantic similarity	Chiraz BenAbdelkader;Ross Cutler;Harsh Nanda;Larry S. Davis	2001		10.1007/3-540-45344-X_42	computer vision;image processing;computer science;artificial intelligence;motion estimation;gait;bias of an estimator;k-nearest neighbors algorithm;cross-validation;biometrics;principal component analysis	Vision	36.235760128701905	-52.04037430701498	102426
c62b1afad3397002e985bc7dfb7a4b8b3eb0139f	properties of the singular value decomposition for efficient data clustering	video signal processing;singular value decomposition;data clustering;singular value decomposition matrix decomposition signal processing algorithms clustering algorithms euclidean distance signal processing image processing;k means algorithm;singular value decomposition video signal processing;video abstraction singular value decomposition svd k means algorithm data clustering	We introduce some interesting properties of the singular value decomposition (SVD), and illustrate how they may be used in conjunction with the k-means algorithm for efficiently clustering a set of vectors. Specifically, we use the SVD to preprocess and sort the data vectors, and then use the k-means algorithm on the modified vectors. To illustrate the effectiveness of this approach, we compare it to the k-means algorithm without preprocessing and show that significant gains in clustering speed may be realized.	algorithm;cluster analysis;k-means clustering;key frame;preprocessor;signal processing;singular value decomposition	SangKeun Lee;Monson H. Hayes	2004	IEEE Signal Processing Letters	10.1109/LSP.2004.833513	correlation clustering;mathematical optimization;data stream clustering;k-svd;computer science;theoretical computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;mathematics;singular spectrum analysis;cluster analysis;singular value decomposition;k-means clustering	ML	26.760408989173047	-40.27498549758244	102545
a230bc54cac8e7aa20256df6027d403d5d32fe46	recognition of social dancing from auditory and visual information	image recognition;music information processing social dancing recognition auditory information visual information real image sequence acoustic signal processing music human posture image matching articulated human body model image sequence frame video annotation motion description computer vision;coordinated movement;human movement recognition;human body model;motion understanding;beat tracking;social dancing;image sequences humans image recognition multiple signal classification music biological system modeling signal processing feature extraction data mining legged locomotion;image sequence;information processing	We discuss the recognition of social dancing from a real image sequence and an acoustic signal of music. Since social dancing involves somewhat complicated movements by multiple humans, it is difficult to recover a detailed description of each dancer's posture by the conventional approach based on matching of an articulated human body model to each frame of an image sequence. Using the results of the recognition process for annotating a video of social dancing, we focus on the bare minimum of motion description. We show that the motion description can be acquired from a real image sequence and an acoustic signal by simple familiar techniques of computer vision and music information processing.		Koh Kakusho;Noboru Babaguchi;Tadahiro Kitahashi	1996		10.1109/AFGR.1996.557279	computer vision;speech recognition;image processing;computer science;communication	HCI	38.994838489201875	-49.127643567284004	102646
5f8a921a303806722a623a56905339a642053102	learning spatially localized, parts-based representation	principal component analysis feature extraction pattern analysis pixel decorrelation independent component analysis face recognition pattern recognition humans image analysis;localization constraint;spatially localized parts based subspace representation learning;visual patterns;independent component analysis;objective function;local nonnegative matrix factorization;face recognition local nonnegative matrix factorization spatially localized parts based subspace representation learning visual patterns localization constraint localized features face representation;localized features;face recognition;face representation;local features;image representation;feature extraction;principal component analysis;pixel;pattern recognition;decorrelation;image analysis;pattern analysis;humans;spatial locality;feature extraction face recognition image representation;local non negative matrix factorization	In this paper, we propose a novel method, called local nonnegative matrix factorization (LNMF), for learning spatially localized, parts-based subspace representation of visual patterns. An objective function is defined to impose localization constraint, in addition to the non-negativity constraint in the standard NMF [1]. This gives a set of bases which not only allows a non-subtractive (part-based) representation of images but also manifests localized features. An algorithm is presented for the learning of such basis components. Experimental results are presented to compare LNMF with the NMF and PCA methods for face representation and recognition, which demonstrates advantages of LNMF.	algorithm;blitzkrieg;constraint (mathematics);facial recognition system;independent computing architecture;initial condition;international standard book number;internationalization and localization;loss function;machine learning;negativity (quantum mechanics);non-negative matrix factorization;optimization problem;principal component analysis;test set	Stan Z. Li;XinWen Hou;HongJiang Zhang;QianSheng Cheng	2001		10.1109/CVPR.2001.990477	facial recognition system;independent component analysis;computer vision;image analysis;decorrelation;feature extraction;computer science;machine learning;pattern recognition;mathematics;pixel;principal component analysis	Vision	27.336562826835618	-43.34327636139281	102719
a603edb840607bb2b1563e51cf6fc1ea99f10929	subspace clustering guided convex nonnegative matrix factorization		Abstract As one of the most important information of the data, the geometry structure information is usually modeled by a similarity graph to enforce the effectiveness of nonnegative matrix factorization (NMF). However, pairwise distance based graph is sensitive to noise and can not capture the subspace structure of the data. Reconstruction coefficients based graph can capture the subspace structure of the data, but the procedure of building the representation based graph is usually independent to the framework of NMF. To address this issue, a novel subspace clustering guided convex nonnegative matrix factorization (SC-CNMF) is proposed. In this NMF framework, the nonnegative subspace clustering is incorporated to learning the representation based graph, and meanwhile, a convex nonnegative matrix factorization is also updated simultaneously. To tackle the noise influence of the dataset, only k largest entries of each representation are kept in the subspace clustering. To capture the complicated geometry structure of the data, multiple centroids are also introduced to describe each cluster. Additionally, a row constraint is used to remove the relevance among the rows of the encoding matrix, which can help to improve the clustering performance of the proposed model. For the proposed NMF framework, two different objective functions with different optimizing schemes are designed. Image clustering experiments are conducted to demonstrate the effectiveness of the proposed methods on several datasets and compared with some related works based on NMF together with k -means clustering method and PCA as baseline.	cluster analysis;clustering high-dimensional data;non-negative matrix factorization	Guosheng Cui;Xuelong Li;Yongsheng Dong	2018	Neurocomputing	10.1016/j.neucom.2018.02.067	row;cluster analysis;artificial intelligence;regular polygon;pairwise comparison;pattern recognition;mathematics;subspace topology;centroid;matrix (mathematics);non-negative matrix factorization	AI	26.07890607256063	-40.469790250091265	102917
fdeb574c0847c41dcee7de59e917da9c0dd3f2c1	document image binarization using lstm: a sequence learning approach	optical character recognition;document image binarization;long short term memory;neural network	We propose to address the problem of Document Image Binarization (DIB) using Long Short-Term Memory (LSTM) which is specialized in processing very long sequences. Thus, the image is considered as a 2D sequence of pixels and in accordance to this a 2D LSTM is employed for the classification of each pixel as text or background. The proposed approach processes the information using local context and then propagates the information globally in order to achieve better visual coherence. The method is robust against most of the document artifacts. We show that with a very simple network without any feature extraction and with limited amount of data the proposed approach works reasonably well for the DIBCO 2013 dataset. Furthermore a synthetic dataset is considered to measure the performance of the proposed approach with both binarization and OCR groundtruth. The proposed approach significantly outperforms standard binarization approaches both for F-Measure and OCR accuracy with the availability of enough training samples.	binary image;dhrystone;f1 score;feature extraction;long short-term memory;optical character recognition;pixel	Muhammad Zeshan Afzal;Joan Pastor-Pellicer;Faisal Shafait;Thomas M. Breuel;Andreas Dengel;Marcus Liwicki	2015		10.1145/2809544.2809561	computer vision;computer science;pattern recognition;data mining	Vision	27.996693554432646	-52.03930358275855	103225
7e0018ac4bfac52d9d192972c7155c240b899c51	image classification with kernelized spatial-context	image features;spatial context;kernel;image categorization;spatial contextual model;distance measure;particle measurements;hidden markov model;prototypes;local patches;image classification;image classification hidden markov models context modeling kernel prototypes particle measurements size measurement image retrieval internet asia;size measurement;spatial structure;2 d hidden markov model;spatial context 2 d hidden markov model image classification kernel method;hidden markov models;internet;intraclass variances;recursive formulation;image representation;feature extraction;image representation feature extraction hidden markov models image classification;kernel method;classification accuracy;context modeling;spatial image features;asia;image categorization image classification local patches spatial contextual model hidden markov model intraclass variances kernel method spatial image features recursive formulation;image retrieval	The goal of image classification is to classify a collection of unlabeled images into a set of semantic classes. Many methods have been proposed to approach this goal by leveraging visual appearances of local patches in images. However, the spatial context between these local patches also provides significant information to improve the classification accuracy. Traditional spatial contextual models, such as two-dimensional hidden Markov model, attempt to construct one common model for each image category to depict the spatial structures of the images in this class. However due to large intra-class variances in an image category, one single model has difficulties in representing various spatial contexts in different images. In contrast, we propose to construct a prototype set of spatial contextual models by leveraging the kernel methods rather than only one model. Such an algorithm combines the advantages of rich representation ability of spatial contextual models as well as the powerful classification ability of kernel method. In particular, we propose a new distance measure between different spatial contextual models by integrating joint appearance-spatial image features. Such a distance measure can be efficiently computed in a recursive formulation that scales well to image size. Extensive experiments demonstrate that the proposed approach significantly outperforms the state-of-the-art approaches.	algorithm;computer vision;experiment;hidden markov model;image resolution;kernel method;kernelization;markov chain;modal logic;patch (computing);prototype;recursion;reference model;similarity measure	Guo-Jun Qi;Xian-Sheng Hua;Yong Rui;Jinhui Tang;HongJiang Zhang	2010	IEEE Transactions on Multimedia	10.1109/TMM.2010.2046270	computer vision;kernel method;contextual image classification;kernel;the internet;feature extraction;computer science;spatial contextual awareness;machine learning;pattern recognition;mathematics;prototype;context model;feature;hidden markov model	Vision	30.882087727003295	-46.95422601086299	103327
67dd2d536023814f4b29023436a0f34b6ce1850d	saccade gaze prediction using a recurrent neural network		We present a model that generates close-to-human gaze sequences for a given image in the free viewing task. The proposed approach leverages recent advances in image recognition using convolutional neural networks and sequence modeling with recurrent neural networks. Feature maps from convolutional neural networks are used as inputs to a recurrent neural network. The recurrent neural network acts like a visual working memory that integrates the scene information and outputs a sequence of saccades. The model is trained end-to-end with real-world human eye-tracking data using back propagation and adaptive stochastic gradient descent. Overall, the proposed model is simple compared to the state-of-the-art methods while offering favorable performance on a standard eye-tracking data set.		Thuyen Ngo;B. S. Manjunath	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296920	convolutional neural network;hidden markov model;logic gate;artificial intelligence;feature extraction;working memory;stochastic gradient descent;recurrent neural network;computer science;pattern recognition;backpropagation	Vision	27.31584018714778	-51.48850732098052	103422
64c05490e40de948867275fe68eef4db96f5beeb	structural sparse coding seeds-active appearance model for object tracking		In this paper, we propose a tracking algorithm that can robustly handle appearance variations in tracking process. Our method is based on seeds–active appearance model, which is composed by structural sparse coding. In order to compensate for illumination changes, heavy occlusion and appearance self-updating problem, we proposed a mixture online learning scheme formodeling the target object appearance model. The proposed object tracking scheme involves three stages: training, detection and tracking. In the training stage, an incremental SVM model that directly measures the candidates samples and target difference. The proposed mixture generate–discriminative method can well separate two highly correlated positive candidates images. In the detection stage, the trained weighted vector is used to separate the target object in positive candidates images with respect to the seeds images. In the tracking stage, we employ the particle filter to track the object through an appearance adaptive updating algorithm with seeds–active constrained sparse representation.Basedona set of comprehensive experiments, our algorithm has demonstrated better performance than alternatives reported in the current literature.	active appearance model;algorithm;code;experiment;generative model;mixture model;neural coding;online algorithm;particle filter;sparse approximation;sparse matrix	Yi Ouyang	2017	Signal, Image and Video Processing	10.1007/s11760-017-1063-1	computer vision;support vector machine;artificial intelligence;video tracking;pattern recognition;sparse approximation;neural coding;particle filter;active appearance model;computer science	Vision	33.729526149442314	-47.68069381736452	103424
4c24176d8f3a25eb96aa874f663680970602b77b	discriminative subvolume search for efficient action detection	feature extraction;naive bayes;clothing;search algorithm;artificial neural networks;mutual information;upper bound;training data;voting;gesture recognition;kernel;pattern search;background subtraction;pattern matching	Actions are spatio-temporal patterns which can be characterized by collections of spatio-temporal invariant features. Detection of actions is to find the re-occurrences (e.g. through pattern matching) of such spatio-temporal patterns. This paper addresses two critical issues in pattern matching-based action detection: (1) efficiency of pattern search in 3D videos and (2) tolerance of intra-pattern variations of actions. Our contributions are two-fold. First, we propose a discriminative pattern matching called naive-Bayes based mutual information maximization (NBMIM) for multi-class action categorization. It improves the state-of-the-art results on standard KTH dataset. Second, a novel search algorithm is proposed to locate the optimal subvolume in the 3D video space for efficient action detection. Our method is purely data-driven and does not rely on object detection, tracking or background subtraction. It can well handle the intra-pattern variations of actions such as scale and speed variations, and is insensitive to dynamic and clutter backgrounds and even partial occlusions. The experiments on versatile datasets including KTH and CMU action datasets demonstrate the effectiveness and efficiency of our method.	background subtraction;categorization;clutter;discriminative model;expectation–maximization algorithm;experiment;mutual information;naive bayes classifier;object detection;pattern matching;pattern search (optimization);search algorithm	Junsong Yuan;Zicheng Liu;Ying Wu	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206671	pattern search;computer vision;kernel;naive bayes classifier;background subtraction;feature extraction;computer science;machine learning;pattern matching;pattern recognition;gesture recognition;mutual information;upper and lower bounds;search algorithm	Vision	35.94811622320713	-50.415425141664	103438
7a1fe5b76af6fbf77954c97877e32d22668a9514	structure from motion using dense cnn features with keypoint relocalization		Structure from motion (SfM) using imagery that involves extreme appearance changes is yet a challenging task due to a loss of feature repeatability. Using feature correspondences obtained by matching densely extracted convolutional neural network (CNN) features significantly improves the SfM reconstruction capability. However, the reconstruction accuracy is limited by the spatial resolution of the extracted CNN features which is not even pixel-level accuracy in the existing approach. Providing dense feature matches with precise keypoint positions is not trivial because of memory limitation and computational burden of dense features. To achieve accurate SfM reconstruction with highly repeatable dense features, we propose an SfM pipeline that uses dense CNN features with relocalization of keypoint position that can efficiently and accurately provide pixel-level feature correspondences. Then, we demonstrate on the Aachen Day-Night dataset that the proposed SfM using dense CNN features with the keypoint relocalization outperforms a state-of-the-art SfM (COLMAP using RootSIFT) by a large margin.	artificial neural network;convolutional neural network;pixel;repeatability;structure from motion	Aji Resindra Widya;Akihiko Torii;Masatoshi Okutomi	2018	IPSJ Transactions on Computer Vision and Applications	10.1186/s41074-018-0042-y	computer vision;3d reconstruction;convolutional neural network;artificial intelligence;structure from motion;mathematics;pattern recognition;image resolution	Vision	28.95408508228275	-49.62616350816838	103492
3c8e5f23d4388cc11d059cca038fe618f52c5f4d	a framework for a video analysis tool for suspicious event detection	video analysts;event understanding;video analysis;event detection;surveillance video;unusual event;event classification;machine learning;semantic gap	This paper proposes a framework to aid video analysts in detecting suspicious activity within the tremendous amounts of video data that exists in today's world of omnipresent surveillance video. Ideas and techniques for closing the semantic gap between low-level machine readable features of video data and high-level events seen by a human observer are discussed. An evaluation of the event classification and diction technique is presented and future an experiment to refine this technique is proposed. These experiments are used as a lead to a discussion on the most optimal machine learning algorithm to learn the event representation scheme proposed in this paper.	access control;algorithm;closed-circuit television;closing (morphology);continuation;digital video;experiment;high- and low-level;human-readable medium;machine learning;sensor;sorting;video content analysis	Gal Lavee;Latifur Khan;Bhavani M. Thuraisingham	2005	Multimedia Tools and Applications	10.1007/s11042-007-0117-8	computer vision;simulation;computer science;machine learning;video tracking;data mining;algorithm;semantic gap	AI	38.82628973575122	-45.6185985477136	103493
982fed5c11e76dfef766ad9ff081bfa25e62415a	undersampled face recognition via robust auxiliary dictionary learning	dictionaries face recognition robustness face training data training image reconstruction;face recognition dictionary learning sparse representation;learning artificial intelligence face recognition image representation;sparse representation based methods undersampled face recognition robust auxiliary dictionary learning robust face recognition undersampled training data unseen occlusions	In this paper, we address the problem of robust face recognition with undersampled training data. Given only one or few training images available per subject, we present a novel recognition approach, which not only handles test images with large intraclass variations such as illumination and expression. The proposed method is also to handle the corrupted ones due to occlusion or disguise, which is not present during training. This is achieved by the learning of a robust auxiliary dictionary from the subjects not of interest. Together with the undersampled training data, both intra and interclass variations can thus be successfully handled, while the unseen occlusions can be automatically disregarded for improved recognition. Our experiments on four face image datasets confirm the effectiveness and robustness of our approach, which is shown to outperform state-of-the-art sparse representation-based methods.	algorithm;dictionary [publication type];experiment;facial recognition system;hidden surface determination;illumination (image);machine learning;mathematical optimization;obstruction;sparse approximation;sparse matrix;undersampling	Chia-Po Wei;Yu-Chiang Frank Wang	2015	IEEE Transactions on Image Processing	10.1109/TIP.2015.2409738	speech recognition;computer science;machine learning;pattern recognition;three-dimensional face recognition	Vision	29.047342630751707	-46.92494561995747	103503
8fd534a81f710e13b4753ab9f16279c5f67179fb	support vector machine for data on manifolds: an application to image analysis	kernel;medical images;manifolds;support vector machines;shape analysis;image classification;biomedical imaging;indexing terms;data analysis;shape;medical image;image shape analysis image classification;image representation;medical image processing;displays;medial representations;support vector machines biomedical mri image classification image representation medical image processing;euclidean space;support vector machine classification;image analysis;support vector machines manifolds image analysis support vector machine classification biomedical imaging shape kernel diffusion tensor imaging displays data analysis;support vector machine;diffusion tensor imaging;diffusion tensor mri;image shape analysis;diffusion tensor mri support vector machine image classification image shape analysis medial representations medical images;biomedical mri	The Support Vector Machine (SVM) is a powerful tool for classification. We generalize SVM to work with data objects that are naturally understood to be lying on curved manifolds, and not in the usual d-dimensional Euclidean space. Such data arise from medial representations (m-reps) in medical images, Diffusion Tensor-MRI (DT-MRI), diffeomorphisms, etc. Considering such data objects to be embedded in higher dimensional Euclidean space results in invalid projections (on the separating direction) while Kernel Embedding does not provide a natural separating direction. We use geodesic distances, defined on the manifold to formulate our methodology. This approach addresses the important issue of analyzing the change that accompanies the difference between groups by implicitly defining the notions of separating surface and separating direction on the manifold. The methods are applied in shape analysis with target data being m-reps of 3 dimensional medical images.	embedded system;geodesic convexity;image analysis;medial graph;shape analysis (digital geometry);support vector machine	Suman K. Sen;Mark Foskey;J. S. Marron;Martin Styner	2008	2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2008.4541216	support vector machine;computer vision;image analysis;computer science;machine learning;pattern recognition;mathematics	Vision	27.95893863870333	-40.777151638653315	103621
e8d71e2fbde15671c2f572960c10ff193fa19549	evaluation of traffic sign recognition methods trained on synthetically generated data	traffic sign recognition;nearest neighbor search;synthetic data	Most of today's machine learning techniques requires large manually labeled data. This problem can be solved by using synthetic images. Our main contribution is to evaluate methods of traffic sign recognition trained on synthetically generated data and show that results are comparable with results of classifiers trained on real dataset. To get a representative synthetic dataset we model different sign image variations such as intra-class variability, imprecise localization, blur, lighting, and viewpoint changes. We also present a new method for traffic sign segmentation, based on a nearest neighbor search in the large set of synthetically generated samples, which improves current traffic sign recognition algorithms.	traffic sign recognition	Boris Moiseev;Artem Konev;Alexander Chigorin;Anton Konushin	2013		10.1007/978-3-319-02895-8_52	computer science;machine learning;pattern recognition;data mining;traffic sign recognition;nearest neighbor search;synthetic data	AI	31.58550815580263	-52.06305154540181	103659
851b3937663909249389d6a327215d56b1768ae3	a-optimal non-negative projection for image representation	databases;nmf;matrix decomposition optimization vectors encoding principal component analysis covariance matrix databases;parts based representation;encodings;image coding;computer science and information systems;image data representation;computer vision;vectors;a optimal nonnegative projection;matrix decomposition;data structures;nonnegative matrix factorization;image representation;principal component analysis;pattern recognition;matrix decomposition data structures encoding image coding image representation;stable linear model a optimal nonnegative projection image representation computer vision pattern recognition nonnegative matrix factorization nmf parts based representation data matrix encodings nonnegative constraints image data representation;optimization;nonnegative constraints;stable linear model;encoding;data matrix;covariance matrix	As a central problem in computer vision and pattern recognition, data representation has attracted great attention in the past years. Non-negative matrix factorization (NMF) which is a useful data representation method makes great contribution on finding the latent structure of the data and leads to a parts-based representation by decomposing the data matrix into a few bases and encodings with nonnegative constraints. However, non-negative constraint is insufficient for getting more robust data representation. In this paper, we propose a novel method, called A-Optimal Non-negative Projection (ANP) for image data representation and further analysis. ANP imposes a constraint on the encoding factor as a regularizer during matrix factorization. In this way, the learned data representation leads to a stable linear model no matter what kind of data label is selected for further processing. Thus, it can preserve more intrinsic characteristics of the data regardless of any specific labels. We demonstrate the effectiveness of this novel algorithm through a set of evaluations on real world applications.	algorithm;computer vision;data (computing);linear model;non-negative matrix factorization;pattern recognition	Haifeng Liu;Zheng Yang;Zhaohui Wu;Xuelong Li	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247851	discrete mathematics;data structure;computer science;machine learning;pattern recognition;mathematics;non-negative matrix factorization	Vision	27.122333351484514	-41.092434736834655	103817
4630341279a01d25f06a614ca8f3eb7f92903b47	recursive autoencoders-based unsupervised feature learning for hyperspectral image classification		For hyperspectral image (HSI) classification, it is very important to learn effective features for the discrimination purpose. Meanwhile, the ability to combine spectral and spatial information together in a deep level is also important for feature learning. In this letter, we propose an unsupervised feature learning method for HSI classification, which is based on recursive autoencoders (RAE) network. RAE utilizes the spatial and spectral information and produces high-level features from the original data. It learns features from the neighborhood of the investigated pixel to represent the whole local homogeneous area of the image. In addition, to obtain more accurate representation of the investigated pixel, a weighting scheme is adopted based on the neighboring pixels, where the weights are determined by the spectral similarity between the neighboring pixels and the investigated pixel. The effectiveness of our method is evaluated by the experiments on two hyperspectral data sets, and the results show that our proposed method has a better performance.		Xiangrong Zhang;Yanjie Liang;Chen Li;Ning Huyan;Licheng Jiao;Huiyu Zhou	2017	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2737823	support vector machine;pixel;computer vision;feature (computer vision);machine learning;feature extraction;deep learning;hyperspectral imaging;contextual image classification;computer science;pattern recognition;artificial intelligence;feature learning	Vision	30.59097375882019	-44.701149570806145	103957
e3fd38d00d7e3cae101991c46fa2b3d42134126c	online multi-person tracking based on metric learning		The correct associations of detections and tracklets are the key to online multi-person tracking. Good appearance models can guide data association and play an important role in the association. In this paper, we construct a discriminative appearance model by using metric learning which can obtain accurate appearance affinities with human appearance variations. The novel appearance model can significantly guide data association. Furthermore, the model is learned incrementally according to the association results and its parameters are automatically updated to be suitable for the next online tracking. Based on an online tracking-by-detection framework, our method achieves reliable tracking of multiple persons even in complex scenes. Our experimental evaluation on publicly available data sets shows that the proposed online multi-person tracking method works well.		Changyong Yu;Min Yang;Yanmei Dong;Mingtao Pei;Yunde Jia	2016		10.1007/978-3-319-48890-5_13	computer vision;machine learning	Vision	33.82426036044559	-48.30003189934739	103971
f526756cac3f72f8abcfa06c0de89f58d2683c1f	an appearance-based approach to assistive identity inference using lbp and colour histograms	colour histogram;object recognition;ensemble learning;local binary patterns	Robust identity inference is one of the biggest challenges in current visual surveillance systems. Although, face is an important biometric for generic identity inference, it is not always accessible in video-based surveillance systems due to the poor quality of the video or ineffective viewpoints where the captured face is not clearly visible. Hence, taking advantage of additional features to increase the accuracy and reliability of these systems is an increasing need. Appearance and clothing are potentially suitable for visual identification and tracking suspects. In this research we present a novel approach for recognition of upper body clothing, using local binary patterns (LBP) and colour information, as an assistive tool for identity inference.	belief propagation	Sareh Abolahrari Shirazi;Farhad Dadgostar;Brian C. Lovell	2010		10.1007/978-3-642-22822-3_24	computer vision;local binary patterns;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;ensemble learning	Vision	32.90284303381413	-51.635819215762595	104065
6355f7fd956466e8e9f09b297e6cdd155d66740e	egoreid: cross-view self-identification and human re-identification in egocentric and surveillance videos		Human identification remains to be one of the challenging tasks in computer vision community due to drastic changes in visual features across different viewpoints, lighting conditions, occlusion, etc. Most of the literature has been focused on exploring human re-identification across viewpoints that are not too drastically different in nature. Cameras usually capture oblique or side views of humans, leaving room for a lot of geometric and visual reasoning. Given the recent popularity of egocentric and top-view vision, re-identification across these two drastically different views can now be explored. Having an egocentric and a top view video, our goal is to identify the cameraman in the content of the top-view video, and also re-identify the people visible in the egocentric video, by matching them to the identities present in the top-view video. We propose a CRF-based method to address the two problems. Our experimental results demonstrates the efficiency of the proposed approach over a variety of video recorded from two views.	computer vision;conditional random field;oblique projection;regular expression	Shervin Ardeshir;Sandesh Sharma;Ali Broji	2016	CoRR		computer vision;simulation;multimedia	Vision	33.56644274623023	-50.54050347119348	104262
99e2ea69f8e4bb59948851ca92942f1c15dab1b6	action recognition using rate-invariant analysis of skeletal shape trajectories	statistical analysis computational geometry gesture recognition image sensors;skeletal data action recognition riemannian geometry manifold trajectories depth sensors;measurement;skeleton;hidden markov models;trajectory;shape;three dimensional displays;action recognition;skeletal data;shape trajectory skeleton three dimensional displays measurement hidden markov models space vehicles;depth sensors;riemannian geometry;3d action pairs datasets action recognition rate invariant analysis skeletal shape trajectories kinect sensors depth sensors dynamical skeletons kendall s shape manifold statistical analysis parameterization invariant metric transported square root vector fields standard euclidean norm msr action 3d datasets msr daily activity datasets;manifold trajectories;space vehicles	We study the problem of classifying actions of human subjects using depth movies generated by Kinect or other depth sensors. Representing human body as dynamical skeletons, we study the evolution of their (skeletons') shapes as trajectories on Kendall's shape manifold. The action data is typically corrupted by large variability in execution rates within and across subjects and, thus, causing major problems in statistical analyses. To address that issue, we adopt a recently-developed framework of Su et al. [1], [2] to this problem domain. Here, the variable execution rates correspond to re-parameterizations of trajectories, and one uses a parameterization-invariant metric for aligning, comparing, averaging, and modeling trajectories. This is based on a combination of transported square-root vector fields (TSRVFs) of trajectories and the standard Euclidean norm, that allows computational efficiency. We develop a comprehensive suite of computational tools for this application domain: smoothing and denoising skeleton trajectories using median filtering, up- and down-sampling actions in time domain, simultaneous temporal-registration of multiple actions, and extracting invertible Euclidean representations of actions. Due to invertibility these Euclidean representations allow both discriminative and generative models for statistical analysis. For instance, they can be used in a SVM-based classification of original actions, as demonstrated here using MSR Action-3D, MSR Daily Activity and 3D Action Pairs datasets. Using only the skeletal information, we achieve state-of-the-art classification results on these datasets.	application domain;classification;generative model;kinect;median filter;movies;noise reduction;plant roots;problem domain;sampling (signal processing);skeleton;smoothing (statistical technique);spatial variability;xangio1 protein, xenopus;manifold;sensor (device)	Boulbaba Ben Amor;Jingyong Su;Anuj Srivastava	2016	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2015.2439257	riemannian geometry;computer vision;shape;computer science;trajectory;machine learning;mathematics;geometry;skeleton;hidden markov model;measurement	Vision	36.44513248819487	-43.176643974428664	104404
8212f6af711170e1517736c3b40a5d3bdf2b2f23	computer vision		“Intelligent” computers require knowledge of their environment, and the most effective means of acquiring such knowledge is by seeing. Vision opens a new realm of computer applications. Weather analysis from cloud maps and medical diagnosis from x-rays and blood cell counts are two of many high volume data processing tasks that require pictorial input. Man-machine communication will be facilitated if a person can directly show the computer a real object, rather than describe it symbolically. Designers, for example, could then easily obtain a computerized structural analysis of scale models.	computer vision;image;map;structural analysis	Thomas O. Binford;Jay M. Tenenbaum	1973	Computer	10.1109/MC.1973.6536714		HCI	37.27942262811305	-38.54714538349494	104464
9794064fe64c0b7b849f10e20ddf80b7311bd29c	generative model with coordinate metric learning for object recognition based on 3d models		One of the bottlenecks in acquiring a perfect database for deep learning is the tedious process of collecting and labeling data. In this paper, we propose a generative model trained with synthetic images rendered from 3D models which can reduce the burden on collecting real training data and make the background conditions more realistic. Our architecture is composed of two sub-networks: a semantic foreground object reconstruction network based on Bayesian inference and a classification network based on multi-triplet cost training for avoiding overfitting on the monotone synthetic object surface and utilizing accurate information of synthetic images like object poses and lighting conditions which are helpful for recognizing regular photos. First, our generative model with metric learning utilizes additional foreground object channels generated from semantic foreground object reconstruction sub-network for recognizing the original input images. Multi-triplet cost function based on poses is used for metric learning which makes it possible to train an effective categorical classifier purely based on synthetic data. Second, we design a coordinate training strategy with the help of adaptive noise applied on the inputs of both of the concatenated sub-networks to make them benefit from each other and avoid inharmonious parameter tuning due to different convergence speeds of two sub-networks. Our architecture achieves the state-of-the-art accuracy of 50.5% on the ShapeNet database with data migration obstacle from synthetic images to real images. This pipeline makes it applicable to do recognition on real images only based on 3D models. Our codes are available at https://github.com/wangyida/gm-cml.	3d modeling;code;coder device component;concatenation;converge;deep learning;generative model;imagenet;immunostimulating conjugate (antigen);inference;linear classifier;loss function;numerous;outline of object recognition;overfitting;parametric model;population parameter;sample variance;subnetwork;synthetic data;triplet state;variational principle;emotional dependency;monotone	Yida Wang;Weihong Deng	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2858553	computer vision;machine learning;pattern recognition;generative design	Vision	25.20295859902329	-49.32811952931232	104522
7e5d66723839698190e0668aec18cbf9433f8378	visual localization in highly crowded urban environments	mobile robots;visualization robots feature extraction reliability training vehicle dynamics urban areas;robot vision cameras feature extraction image representation image retrieval mobile robots;robot vision;image representation;feature extraction;cameras;dynamic pixels visual localization crowded dynamic environments static objects dynamic objects imaging conditions feature extraction bag of words model image retrieval image localization query image feature augmentation feature elimination representation size reduction highly crowded indian urban outdoor environments feature detection camera localization accuracy occluded pixels;image retrieval	Visual localization in crowded dynamic environments requires information about static and dynamic objects. This paper presents a robust method that learns the useful features from multiple runs in highly crowded urban environments. Useful features are identified as distinctive ones that are also reliable to extract in diverse imaging conditions. Relative importance of features is used to derive the weight for each feature. The popular Bag-of-words model is used for image retrieval and localization, where query image is the current view of the environment and database contains the visual experience from previous runs. Based on the reliability, features are augmented and eliminated over runs. This reduces the size of representation, and makes it more reliable in crowded scenes. We tested the proposed method on data sets collected from highly crowded Indian urban outdoor settings. Experiments have shown that with the help of a small subset (10%) of the detected features, we can reliably localize the camera. We achieve superior results in terms of localization accuracy even when more than 90% of the pixels are occluded or dynamic.	bag-of-words model;computation;experiment;image retrieval;internationalization and localization;pixel;requirement	A. H. Abdul Hafez;Manpreet Singh;K. Madhava Krishna;C. V. Jawahar	2013	2013 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2013.6696749	mobile robot;computer vision;feature detection;visual word;feature extraction;image retrieval;computer science;multimedia;feature;computer graphics (images)	Robotics	33.061667539647964	-51.12589069002605	104570
ab45a3fd43f9d3874a1ea2c5bc8d42e2b3bf71ac	scene text detection via deep semantic feature fusion and attention-based refinement		Despite tremendous progress in scene text detection in the past few years, efficient text detection in the wild remains challenging, particularly for the texts have large rotations, and the complicated background areas that are easily confused with text. In this paper, we propose an effective approach for scene text detection, which consists of initial text detection using the proposed deep semantic feature fusion of a fully convolutional network (FCN), and text detection refinement by our attention based text vs. non-text classifier learned in a fine-to-coarse fashion. The proposed approach outperforms the state-of-the-art scene text detection algorithms on the public-domain ICDAR2015 dataset, achieving an accuracy of 0.83 in terms of F-measure.		Yu Song;Yuanshun Cui;Hu Han;Shiguang Shan;Xilin Chen	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8546050	semantic feature;task analysis;computer vision;feature extraction;artificial intelligence;object detection;semantics;fusion;classifier (linguistics);pattern recognition;computer science	Vision	29.423644020696333	-51.91605726663929	104876
818255157eb69360ea17651f5e1c392b064f4ccf	fully convolutional network with polarimetric manifold for sar imagery classification		Image classification performance depends on the understanding of image features and classifier selection. Owing to the special imaging mechanism, achieving precise classification for remote sensing imagery is still quite challenging. In this paper, a fully convolutional network with polarimetric manifold, is proposed for Synthetic Aperture Radar (SAR) image classification. First, the polarimetric features are extracted to describe the target information; then the feature points in high-dimension are mapped to low-dimension through the manifold structure. In this way, the effect of single manifold is equal to that of multi -layer convolution. The experimental results on SAR image data indicate that the presented manifold network can effectively separate the polarimetric features and improve the classification accuracy.	computer vision;convolution;manifold regularization;polarimetry	Mingxia Tu;Gong Han;Xinlong Liu;Chu He	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8517608	computer vision;manifold;feature (computer vision);feature extraction;polarimetry;synthetic aperture radar;artificial intelligence;convolution;contextual image classification;computer science	Vision	30.499631727571607	-44.58127789014597	104881
391ad25a3c5c7c504d46bbd21cb13464dfe052e6	deep multi-view correspondence for identity-aware multi-target tracking		A multi-view multi-target correspondence framework employing deep learning on overlapping cameras for identity-aware tracking in the presence of occlusion is proposed. Our complete pipeline of detection, multi-view correspondence, fusion and tracking, inspired by AI greatly improves person correspondence across multiple wide-angled views over traditionally used features set and handcrafted descriptors. We transfer the learning of a deep convolutional neural net (CNN) trained to jointly learn pedestrian features and similarity measures, to establish identity correspondence of non-occluding targets across multiple overlapping cameras with varying illumination and human pose. Subsequently, the identity-aware foreground principal axes of visible targets in each view are fused onto top view without requirement of camera calibration and precise principal axes length information. The problem of ground point localisation of targets on top view is then solved via linear programming for optimal projected axes intersection points to targets assignment using identity information from individual views. Finally, our proposed scheme is evaluated under tracking performance measures of MOTA and MOTP on benchmark video sequences which demonstrate high accuracy results when compared to other well-known approaches.	algorithm;artificial neural network;benchmark (computing);camera resectioning;convolutional neural network;deep learning;graphics processing unit;linear programming;loss function;network switch	Adnan Hanif;Atif Bin Mansoor;Ali Shariq Imran	2017	2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2017.8227423	camera resectioning;point localisation;computer vision;calibration;deep learning;principal axis theorem;artificial neural network;computer science;pattern recognition;linear programming;artificial intelligence	Vision	30.563569571485953	-50.95082255768752	105080
b9fdd627a9c0d48acbea6880433149ed111672a0	2d clustering based discriminant analysis for 3d head model classification	2d subspace analysis;2d fisher discriminant analysis;discriminant analysis;fisher discriminant analysis;extended gaussian image;subspace method;3d head model classification;2d clustering based discriminant analysis	This paper introduces a novel framework for 3D head model recognition based on the recently proposed 2D subspace analysis method. Two main contributions have been made. First, a 2D version of clustering-based discriminant analysis (CDA) is proposed, which combines the capability to model the multiple cluster structure embedded within a single class with the computational advantage that is characteristic of 2D subspace analysis methods. Second, we extend the applications of 2D subspace methods to the field of 3D head model classification by characterizing these models with 2D feature sets. 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	.cda file;cluster analysis;computation;egi;embedded system;linear discriminant analysis;pattern recognition	Bo Ma;Hau-San Wong	2006	Pattern Recognition	10.1016/j.patcog.2005.10.017	kernel fisher discriminant analysis;computer science;machine learning;pattern recognition;optimal discriminant analysis;linear discriminant analysis;multiple discriminant analysis;statistics	Vision	27.69606740185279	-41.68495381547288	105189
760975f82e75f2e713a380b8c0282a771fbab0d4	infrared and visible image registration using transformer adversarial network		In this paper we address the task of infrared and visible image registration in complex scenes. Due to the difference of infrared and visible images, it is neither easy to reliably find features nor suitable for directly training in deep learning architecture. Thus, we propose a two-stage adversarial network, which first conducts a multi-spectral image transfer to obtain a mapped image. And then the proposed network incorporate a transformer module into the conditional adversarial network architecture to get the refined warped image. Our method can back propagate the multi-spectral registration loss and achieve end-to-end training. Experiments on our multi -spectral dataset demonstrate that this approach is effective and robust, which outperforms other state-of-the-art methods.		Lan Wang;Chenqiang Gao;Yue Zhao;Tiecheng Song;Qi Feng	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451370	adversarial system;task analysis;computer vision;feature extraction;deep learning;artificial intelligence;network architecture;architecture;infrared;image registration;computer science	Vision	26.351200111489135	-51.25639778850933	105240
0fd8d0d01794279d0c8b96791a665157ed3ae0fe	efficient large-scale action recognition in videos using extreme learning machines	multimedia mining;extreme learning machine;action recognition;fisher vector	We describe a novel approach for large-scale action recognition from videos in a realistic setting.We represent each video by Fisher vector encoding computed on improved trajectory features.We use extreme learning machines for fast and accurate classification.We report comparative results where we show that the proposed approach outperforms the baseline approaches, and reaches accuracy close to state of the art without using deep neural networks. In this paper, we propose a novel and efficient system for large-scale action recognition from realistic video clips. Our approach combines several recent advances in this area. We use improved dense trajectory features in combination with Fisher vector encoding, and perform learning and classification with extreme learning machine classifiers. The resulting system is a fast and accurate alternative to more traditional action classification approaches like bag of words and support vector machines. Additionally, we use mid-level features that encode information about presence of humans in the videos, as well as color distributions. We extensively evaluate each step of our pipeline in a comparative manner, and report results on the recently published THUMOS 2014 benchmark, which was introduced as a challenge dataset with temporally untrimmed videos and 101 action classes. We achieve 63.37% mean average precision using the challenge protocol (i.e. sequestered test labels and limited system submissions), and got the third rank among eleven participants. The results show that it is possible to obtain a high accuracy with extreme learning machines in an efficient way, without using the extensively trained and computationally heavy deep neural networks that the top performing systems of the challenge incorporated.		Gül Varol;Albert Ali Salah	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.06.013	computer science;artificial intelligence;machine learning;pattern recognition;data mining	Vision	31.83327683150182	-50.44195642590768	105616
703dc33736939f88625227e38367cfb2a65319fe	trespassing the boundaries: labeling temporal bounds for object interactions in egocentric video		Manual annotations of temporal bounds for object interactions (i.e. start and end times) are typical training input to recognition, localization and detection algorithms. For three publicly available egocentric datasets, we uncover inconsistencies in ground truth temporal bounds within and across annotators and datasets. We systematically assess the robustness of state-of-the-art approaches to changes in labeled temporal bounds, for object interaction recognition. As boundaries are trespassed, a drop of up to 10% is observed for both Improved Dense Trajectories and Two- Stream Convolutional Neural Network.,, We demonstrate that such disagreement stems from a limited understanding of the distinct phases of an action, and propose annotating based on the Rubicon Boundaries, inspired by a similarly named cognitive model, for consistent temporal bounds of object interactions. Evaluated on a public dataset, we report a 4% increase in overall accuracy, and an increase in accuracy for 55% of classes when Rubicon Boundaries are used for temporal annotations.	algorithm;cognitive model;color gradient;convolutional neural network;distributed transaction;end-to-end encryption;expectation propagation;farmville;ground truth;interaction;internationalization and localization;interrupt descriptor table;long short-term memory;randomness;supervised learning;web page	Davide Moltisanti;Michael Wray;Walterio W. Mayol-Cuevas;Dima Damen	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.314	computer vision;cognitive model;robustness (computer science);machine learning;artificial intelligence;pattern recognition;ground truth;trajectory;computer science	Vision	32.55406829120189	-49.60961931627391	105705
1ca48999f07b3205a550b1c4590c731674ad2a7c	deep neural network for foreground object segmentation: an unsupervised approach		Saliency plays a key role in various computer vision tasks. Extracting salient regions from images and videos have been a well established problem of computer vision. While segmenting salient objects from images depend only on static information, temporal information in a video can make non salient objects be salient due to movement. Besides the temporal information, there are other challenges involved with video segmentation, such as 3D parallax, camera shake, motion blur, etc. In this work, we propose a novel unsupervised end to end trainable, fully convolutional deep neural network for object segmentation. Our model is robust and scalable across scenes, as it is tested unsupervisedly and can easily infer which objects constitute the foreground of the image. We run various tests on two well established benchmarks of video object segmentation, DAVIS and FBMS-59 datasets. We report our results and compare them against the state of the art methods.	deep learning;unsupervised learning	Avishek Majumder;R. Venkatesh Babu	2017		10.1007/978-981-13-0020-2_32	motion blur;computer vision;market segmentation;artificial neural network;salient;pattern recognition;parallax;salience (neuroscience);artificial intelligence;computer science;segmentation	Vision	27.764116674805393	-51.947998752391044	105809
bc93a5d09aad16356808843338bdd34df6b5b01b	deep-see face: a mobile face recognition system dedicated to visually impaired people		In this paper, we introduce the DEEP-SEE FACE framework, an assistive device designed to improve cognition, interaction, and communication of visually impaired (VI) people in social encounters. The proposed approach jointly exploits computer vision algorithms (region proposal networks, ATLAS tracking and global, and low-level image descriptors) and deep convolutional neural networks in order to detect, track, and recognize, in real-time, various persons existent in the video streams. The major contribution of the paper concerns a global, fixed-size face representation that takes into the account of various video frames while remaining independent of the length of the image sequence. To this purpose, we introduce an effective weight adaptation scheme that is able to determine the relevance assigned to each face instance, depending on the frame degree of motion/camera blur, scale variation, and compression artifacts. Another relevant contribution involves a hard negative mining stage that helps us differentiating between known and unknown face identities. The experimental results, carried out on a large-scale data set, validate the proposed methodology with an average accuracy and recognition rates superior to 92%. When tested in real life, indoor/outdoor scenarios, the DEEP-SEE FACE prototype proves to be effective and easy to use, allowing the VI people to access visual information during social events.	atlas;algorithm;artificial neural network;assistive technology;cognition;compression artifact;computer vision;convolutional neural network;gaussian blur;high- and low-level;prototype;real life;real-time clock;relevance;streaming media;visual descriptor;weight function	Bogdan Cosmin Mocanu;Ruxandra Tapu;Titus Zaharia	2018	IEEE Access	10.1109/ACCESS.2018.2870334	convolutional neural network;face detection;streams;feature extraction;computer vision;exploit;facial recognition system;distributed computing;computer science;cognition;compression artifact;artificial intelligence	Vision	34.08855969457075	-50.35828656230647	105834
72d99ac5c0c2276e19036420e408916783621172	multiple layer model for object detection and sketch representation	support vector machines;convolution;training;feature extraction;europe;signal processing algorithms;object detection	In this paper we propose a multiple layer model for object detection and sketch representation. Unlike most traditional detection models focusing on the object localization, we investigate both the object detection and sketch representation within an unified framework. Based on the multiple layer architecture, our model can provide the sketch information of the detected object. Meanwhile, we generalize it from single scale structure to multiple scales, which efficiently saves time consumed in the image pyramids construction. To efficiently train the classifier at the top layer, we employ the stochastic gradient descent algorithm to minimize the training error and back propagate it to the bottom layer. The experimental results demonstrate that our model outperforms the conventional active basis model.	algorithm;backpropagation;object detection;pyramid (geometry);software propagation;stochastic gradient descent;unified framework	Wencheng Li;Xin Wu;Ling Cai;Fuqiao Hu;Yuming Zhao	2016	2016 24th European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2016.7760591	computer vision;method;computer science;viola–jones object detection framework;theoretical computer science;machine learning	Vision	28.512250523190865	-49.098264997281746	105874
150690ff7900ace7eb58139de30766623f587921	image classification by multimodal subspace learning	image classification;semi supervised learning;journal article;drntu engineering computer science and engineering computing methodologies image processing and computer vision;subspace;multimodality;article	In recent years we witnessed a surge of interest in subspace learning for image classification. However, the previous methods lack of high accuracy since they do not consider multiple features of the images. For instance, we can represent a color image by finding a set of visual features to represent the information of its color, texture and shape. According to the ‘‘Patch Alignment’’ Framework, we developed a new subspace learning method, termed Semi-Supervised Multimodal Subspace Learning (SS-MMSL), in which we can encode different features from different modalities to build a meaningful subspace. In particular, the new method adopts the discriminative information from the labeled data to construct local patches and aligns these patches to get the optimal low dimensional subspace for each modality. For local patch construction, the data distribution revealed by unlabeled data is utilized to enhance the subspace learning. In order to find a low dimensional subspace wherein the distribution of each modality is sufficiently smooth, SS-MMSL adopts an alternating and iterative optimization algorithm to explore the complementary characteristics of different modalities. The iterative procedure reaches the global minimum of the criterion due to the strong convexity of the criterion. Our experiments of image classification and cartoon retrieval demonstrate the validity of the proposed method. 2012 Elsevier B.V. All rights reserved.	algorithm;color image;computer vision;convex function;digital media;encode;experiment;iterative method;loss function;mathematical optimization;maxima and minima;modal logic;modality (human–computer interaction);multimodal interaction;optimization problem;romp;semi-supervised learning;semiconductor industry;supervised learning	Jun Yu;Feng Lin;Seah Hock Soon;Cuihua Li;Ziyu Lin	2012	Pattern Recognition Letters	10.1016/j.patrec.2012.02.002	semi-supervised learning;random subspace method;computer vision;contextual image classification;computer science;machine learning;pattern recognition	Vision	25.02065650222063	-43.18715924180644	106131
2416e06ec0b4840d638745004d59d3c88259c292	rgb-d object recognition: features, algorithms, and a large scale benchmark		Over the last decade, the availability of public image repositories and recognition benchmarks has enabled rapid progress in visual object category and instance detection. Today we are witnessing the birth of a new generation of sensing technologies capable of providing high quality synchronized videos of both color and depth, the RGB-D (Kinect-style) camera. With its advanced sensing capabilities and the potential for mass adoption, this technology represents an opportunity to dramatically increase robotic object recognition, manipulation, navigation, and interaction capabilities. We introduce a large-scale, hierarchical multi-view object dataset collected using an RGB-D camera. The dataset consists of two parts: The RGB-D Object Dataset containing views of 300 objects organized into 51 categories, and the RGB-D Scenes Dataset containing 8 video sequences of office and kitchen environments. The dataset has been made publicly available to the research community so as to enable rapid progress based on this promising technology. We describe the dataset collection procedure and present techniques for RGB-D object recognition and detection of objects in scenes recorded using RGB-D videos, demonstrating that combining color and depth information substantially improves quality of results.	algorithm;benchmark (computing);outline of object recognition	Kevin Lai;Liefeng Bo;Xiaofeng Ren;Dieter Fox	2013		10.1007/978-1-4471-4640-7_9	haar-like features;pattern recognition;3d single-object recognition	Vision	32.65341797522339	-50.94720811572939	106174
9ba1bebb7a547831f0db6ad52bd9135f3a0be846	visual recognition of multi-agent action using binary temporal relations	belief networks;object recognition;plan recognition;multi agent action recognition;uncertainty;surveillance;temporal structure descriptions;plan recognition multi agent action binary temporal relations probabilistic framework model based object recognition temporal structure descriptions belief networks multi agent action recognition motion understanding;motion estimation;contracts;binary temporal relations;automatic generation;computer vision;motion understanding;multi agent systems;research and development;uncertainty object recognition surveillance laboratories marine vehicles trajectory computer vision large scale systems research and development contracts;trajectory;marine vehicles;probabilistic framework;football;action recognition;motion estimation belief networks object recognition multi agent systems;multi agent action;model based object recognition;belief network;large scale systems	A probabilistic framework for representing and visually recognizing complex multi-agent action is presented. Motivated by work in model-based object recognition and designed for the recognition of action from visual evidence, the representation has three components: (1) temporal structure descriptions representing the temporal relationships between agent goals, (2) belief networks for probabilistically representing and recognizing individual agent goals from visual evidence, and (3) belief networks automatically generated from the temporal structure descriptions that support the recognition of the complex action. We describe our current work on recognizing American football plays from noisy trajectory data.		Stephen S. Intille;Aaron F. Bobick	1999		10.1109/CVPR.1999.786917	computer vision;uncertainty;computer science;trajectory;cognitive neuroscience of visual object recognition;machine learning;multi-agent system;motion estimation;bayesian network	Vision	38.31434156966177	-48.15602748471809	106175
1e1cf726763e888b2a5e0c53ed828da6ba0deb01	learning 3d action models from a few 2d videos for view invariant action recognition	2d annotations;cluttered grocery store environments;video signal processing;videos inference algorithms sampling methods tracking feature extraction training data human computer interaction shape appropriate technology data mining;multiview action models;curve fitting problem;joints;video signal processing curve fitting gesture recognition;hidden markov models;mocap data;three dimensional displays;feature extraction;action recognition;solid modeling;feature weighting;3d action models;inference algorithms;curve fitting;standard weizmann dataset;visual gesture recognition;cluttered grocery store environments 3d action models 2d videos action recognition multiview action models mocap data curve fitting problem 2d annotations latent state perceptron algorithm standard weizmann dataset visual gesture recognition test ratio;test ratio;gesture recognition;2d videos;videos;latent state perceptron algorithm	Most existing approaches for learning action models work by extracting suitable low-level features and then training appropriate classifiers. Such approaches require large amounts of training data and do not generalize well to variations in viewpoint, scale and across datasets. Some work has been done recently to learn multi-view action models from Mocap data, but obtaining such data is time consuming and requires costly infrastructure. We present a method that addresses both these issues by learning action models from just a few video training samples. We model each action as a sequence of primitive actions, represented as functions which transform the actor's state. We formulate model learning as a curve-fitting problem, and present a novel algorithm for learning human actions by lifting 2D annotations of a few keyposes to 3D and interpolating between them. Actions are inferred by sampling the models and accumulating the feature weights learned discriminatively using a latent state Perceptron algorithm. We show results comparable to state-of-art on the standard Weizmann dataset, with a much smaller train:test ratio, and also in datasets for visual gesture recognition and cluttered grocery store environments.	algorithm;curve fitting;discriminative model;feature extraction;gesture recognition;graphical model;high- and low-level;interpolation;lifting scheme;motion capture;perceptron;prith banerjee;requirement;sampling (signal processing)	Pradeep Natarajan;Vivek Kumar Singh;Ramakant Nevatia	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5539876	computer vision;feature extraction;computer science;machine learning;pattern recognition;gesture recognition;solid modeling;statistics;curve fitting	Vision	35.09174761595492	-47.78489862111156	106255
e03e86ac61cfac9148b371d75ce81a55e8b332ca	unsupervised learning using sequential verification for action recognition		In this paper, we consider the problem of learning a visual representation from the raw spatiotemporal signals in videos for use in action recognition. Our representation is learned without supervision from semantic labels. We formulate it as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful unsupervised representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pretraining for action recognition, our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. Our method can also be combined with supervised representations to provide an additional boost in accuracy for action recognition. Finally, to quantify its sensitivity to human pose, we show results for human pose estimation on the FLIC dataset that are competitive with approaches using significantly more supervised training data.	benchmark (computing);convolutional neural network;flic (file format);human-based computation;imagenet;supervised learning;temporal logic;unsupervised learning	Ishan Misra;C. Lawrence Zitnick;Martial Hebert	2016	CoRR		unsupervised learning;computer vision;computer science;machine learning;pattern recognition	Vision	27.627991860789002	-51.00364706141961	106421
84c33edc17d39ed3bdfc5d3d815c12f948c6c0a5	per-sample kernel adaptation for visual recognition and grouping	object recognition feature selection gesture recognition image classification image representation learning artificial intelligence;visual representation per sample kernel adaptation visual grouping object representation action representation scene representation partial occlusion clutter feature selection kernel method suboptimal classification feature dimension per sample selection informative dimension kernel computation kernel classifier joint objective function learning stage kernel based visual recognition problem computational performance retrieval phase action recognition indoor scene classification;kernel noise measurement visualization training support vector machines reliability image recognition	Object, action, or scene representations that are corrupted by noise significantly impair the performance of visual recognition. Typically, partial occlusion, clutter, or excessive articulation affects only a subset of all feature dimensions and, most importantly, different dimensions are corrupted in different samples. Nevertheless, the common approach to this problem in feature selection and kernel methods is to down-weight or eliminate entire training samples or the same dimensions of all samples. Thus, valuable signal is lost, resulting in suboptimal classification. Our goal is, therefore, to adjust the contribution of individual feature dimensions when comparing any two samples and computing their similarity. Consequently, per-sample selection of informative dimensions is directly integrated into kernel computation. The interrelated problems of learning the parameters of a kernel classifier and determining the informative components of each sample are then addressed in a joint objective function. The approach can be integrated into the learning stage of any kernel-based visual recognition problem and it does not affect the computational performance in the retrieval phase. Experiments on diverse challenges of action recognition in videos and indoor scene classification show the general applicability of the approach and its ability to improve learning of visual representations.	academy;biconnected component;clutter;computation;feature selection;information;kernel (operating system);kernel method;loss function;optimization problem;partial index	Borislav Antic;Björn Ommer	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.148	computer vision;kernel method;kernel embedding of distributions;radial basis function kernel;feature;machine learning;pattern recognition;graph kernel;mathematics;tree kernel;polynomial kernel	Vision	26.772560756812396	-43.524834363169624	106460
9159a02533ae8db3124051749a30c6c2c0bbedcb	a generic framework for behavior recognition of complex activities in robotics	graph theory;service robots gesture recognition graph theory mobile robots probability self organising feature maps;self organizing maps;probability;probabilistic graphical models;service robots;behavior recognition;mobile robots;robots training context hidden markov models vectors probability probabilistic logic;behavior recognition self organizing maps probabilistic graphical models;self organising feature maps;real world environments behavior recognition complex activity service robotics textual programming error prone programming complex behaviors atomic behaviors spatial temporal analysis arbitrary atomic behavior self organizing maps som probabilistic graphical models pgm data driven training spatial temporal model reasoning process generalization;gesture recognition	Considering the current state in service-robotics, an expert is still necessary to add new tasks and execution behaviors by textual and error-prone programming. Under the consideration that humans typically execute same activities almost identical (or at least similar) and further combine simple behaviors to more complex activities, we follow the constitutive assumption that all complex behaviors are composed of a limited set of atomic behaviors. This work introduces a generic framework for spatial-temporal analysis and classification of arbitrary atomic behaviors. Therefore, we propose the combination of Self-Organizing Maps (SOM) and Probabilistic Graphical Models (PGM) in order to exploit the advantages of both concepts. In this work, we describe the essential methods of the framework briefly, whereas the data-driven training of the spatial-temporal model and the reasoning process are described in detail. In order to demonstrate the potential and to emphasize the high level of generalization and flexibility in real-world environments, the framework is evaluated in an exemplary scenario.	cluster analysis;cognitive dimensions of notations;experiment;graphical model;high-level programming language;interpolation;programming paradigm;robotics;self-organization;unsupervised learning	Kai Häussermann;Oliver Zweigle;Paul Levi	2013	2013 IEEE 25th International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2013.49	mobile robot;computer vision;self-organizing map;computer science;artificial intelligence;graph theory;machine learning;probability;gesture recognition;graphical model	Robotics	36.9447802075508	-41.54374970278124	106490
59be0bc6397f99386dd6a87b5966735e88948b54	generative versus discriminative methods for object recognition	image features;object recognition;probability;object recognition character generation training data predictive models object detection context modeling animals large scale systems computer vision machine learning;image classification;large scale;weakly labelled training data discriminative method object recognition probability theory image feature generative method object classification;feature extraction;probability theory;learning artificial intelligence object recognition probability feature extraction image classification;learning artificial intelligence;discriminative model;object detection	Many approaches to object recognition are founded on probability theory, and can be broadly characterized as either generative or discriminative according to whether or not the distribution of the image features is modelled. Generative and discriminative methods have very different characteristics, as well as complementary strengths and weaknesses. In this paper we introduce new generative and discriminative models for object detection and classification based on weakly labelled training data. We use these models to illustrate the relative merits of the two approaches in the context of a data set of widely varying images of non-rigid objects (animals). Our results support the assertion that neither approach alone will be sufficient for large scale object recognition, and we discuss techniques for combining them.	discriminative model;object detection;outline of object recognition;statistical classification	Ilkay Ulusoy;Christopher M. Bishop	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.167	probability theory;computer vision;contextual image classification;feature extraction;computer science;viola–jones object detection framework;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;probability;3d single-object recognition;feature;discriminative model;statistics	Vision	34.70476950552363	-45.47298982815421	106601
a314d998296f94476d364e11f98204d40624b2ff	enhanced deep image super-resolution		Recent advances in deep learning have facilitated new modalities for transforming the lower resolution image to higher resolution. The generated high resolution image must reconstruct the high frequency details of the image to generate a plausible result. To facilitate feature reuse for the task of super-resolution, we propose residual learning based convolutional neural network architecture. A pixel shuffle operation is performed in the upsampling procedure to mitigate the commonly encountered problem of artifacts in the predicted high resolution image. Our model makes use of a joint loss function consisting of pixel-wise loss and feature loss to learn the mapping from low resolution to its high resolution version. Additionally, our model has the ability to progressively increment to perform multiscale super-resolution. An extensive experiment is performed to validate our model on the diverse ImageNet dataset. We show the effectiveness of our model through visual comparative assessment as well as quantitative comparative analysis with the state-of-the-art.		Shrey Singh;Nishat Afreen;Sanjay Kumar	2018	2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2018.8554363	control engineering;convolutional neural network;residual;iterative reconstruction;computer science;pixel;architecture;deep learning;computer vision;upsampling;artificial intelligence;image resolution	Vision	25.212525921710288	-51.68352422227709	106648
4171b6b34bbc24088451486cf1ae7ef65c5ba677	spacio-temporal situation assessment for mobile robots	automated situation awareness;streaming data;robot sensing systems;cluster algorithm;pattern clustering;state matching spatiotemporal situation assessment mobile robot data pattern spacial sequence temporal sequence dynamic markov chain stream clustering;intelligent robots;mobile robot;hidden markov model;hidden markov models markov processes data models clustering algorithms robot sensing systems mobile robots;spatiotemporal phenomena intelligent robots markov processes mobile robots pattern clustering;mobile robots;data model;markov model;hidden markov models;streaming data automated situation awareness markov models clustering;clustering;markov process;spatiotemporal phenomena;situation awareness;markov models;clustering algorithms;markov processes;situation assessment;data models	In this paper, we present a framework for situation modeling and assessment for mobile robot applications. We consider situations as data patterns that characterize unique circumstances for the robot, and represented not only by the data but also its temporal and spacial sequence. Dynamic Markov chains are used to model the situation states and sequence, where stream clustering is used for state matching and dealing with noise. In experiments using simulated and real data, we show that we are able to learn a situation sequence for a mobile robot passing through a narrow passage. After learning the situation models we are able to robustly recognize and predict the situation.	algorithm;autonomous system (internet);birch;cluster analysis;control system;experiment;markov chain;mobile robot;online and offline;robot control;streaming media	Anders B. Beck;Claus Risager;Nils A. Andersen;Ole Ravn	2011	14th International Conference on Information Fusion		computer vision;simulation;computer science;machine learning	Robotics	38.47603528558543	-41.735497444991715	106696
59031a35b0727925f8c47c3b2194224323489d68	sparse variation dictionary learning for face recognition with a single training sample per person	lfw databases sparse variation dictionary learning face recognition single training sample per person stspp query sample representation generic training set svdl method large scale cmu multipie frgc;psi_visics;dictionaries training face lighting databases bismuth encoding;learning artificial intelligence face recognition image representation;face recognition;image representation;learning artificial intelligence	Face recognition (FR) with a single training sample per person (STSPP) is a very challenging problem due to the lack of information to predict the variations in the query sample. Sparse representation based classification has shown interesting results in robust FR, however, its performance will deteriorate much for FR with STSPP. To address this issue, in this paper we learn a sparse variation dictionary from a generic training set to improve the query sample representation by STSPP. Instead of learning from the generic training set independently w.r.t. the gallery set, the proposed sparse variation dictionary learning (SVDL) method is adaptive to the gallery set by jointly learning a projection to connect the generic training set with the gallery set. The learnt sparse variation dictionary can be easily integrated into the framework of sparse representation based classification so that various variations in face images, including illumination, expression, occlusion, pose, etc., can be better handled. Experiments on the large-scale CMU Multi-PIE, FRGC and LFW databases demonstrate the promising performance of SVDL on FR with STSPP.	database;dictionary;experiment;facial recognition system;illumination (image);machine learning;pattern recognition;sparse approximation;sparse matrix;test set	Meng Meng Yang;Luc Van Gool;Lei Zhang	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.91	facial recognition system;computer vision;speech recognition;k-svd;computer science;machine learning;pattern recognition	Vision	27.43889798106828	-46.851403105246156	106766
c04b4e1a85e8c414ca64ae7cb5cd7077f575c77f	weakly supervised object localization via maximal entropy random walk	support vector machines image classification probability;support vector machines visualization entropy training trajectory shape histograms;object localization;probabilistic object localization maximal entropy random walk linear svm classifier bag of words feature representation;weakly supervised learning;maximal entropy random walk object localization weakly supervised learning;maximal entropy random walk	In this paper, we investigate the problem of weakly supervised object localization in images. For such a problem, the goal is to predict the locations of objects in test images while the labels of the training images are given at image-level. That means a label only indicates whether an image contains objects or not, but does not provide the exact locations of the objects. We propose to address this problem using Maximal Entropy Random Walk (MERW). Specifically, we first train a linear SVM classifier with the weakly labeled data. Based on bag-of-words feature representation, the response of a region to the linear SVM classifier can be formulated as the sum of the feature-weights within the region. For a test image, by properly constructing a graph on the feature-points, the stationary distribution of a MERW can indicate the region with the densest positive feature-weights, and thus provides a probabilistic object localization. Experiments compared with state-of-the-art methods on two datasets validate the performance of our method.	bag-of-words model in computer vision;experiment;maximal set;standard test image;stationary process;supervised learning	Liantao Wang;Ji Zhao;Xuelei Hu;Jianfeng Lu	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025323	combinatorics;machine learning;pattern recognition;mathematics	Vision	37.40905941290229	-47.350507419337696	106852
394ea2610db6bcaeb85c81c3d438a9813cae3fc6	facial expression recognition by multi-scale cnn with regularized center loss		Facial Expression Recognition (FER) has attracted considerable attention due to its potential applications in computer vision. Recently, convolutional neural network (CNN) has shown excellent performance on FER. However, most established deeper, wider and more complex network structures trained by small facial expression training dataset have a risk of overfitting. Moreover, most existing CNN models utilize the softmax loss as a supervision signal to penalize the deviation of classification, which enhances inter-class separation, yet intra-class compactness is not taken into consideration. In this paper, we propose a novel multi-scale CNN integrated with an attention-based learning layer (AMSCNN) for robust facial expression recognition. The attention-based learning layer is designed to automatically learn the importance of different receptive fields in the face during training. Moreover, the multi-scale CNN is optimized by the proposed Regularized Center Loss (RCL). Regularized center loss learns a center for deep features of each class and penalizes the distance between deep features and corresponding center, aiming to strengthen the discriminability of different facial expression. Extensive experiments conducted on two popular human FER benchmarks (CK+ and Oulu-CASIA dataset) demonstrated the effective of our proposed AMSCNN, and it obtained competitive results compared to the state-of-the-art.		Zhenghao Li;Song Wu;Guoqiang Xiao	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8545489	convolutional neural network;overfitting;complex network;feature extraction;facial recognition system;pattern recognition;artificial intelligence;receptive field;facial expression;softmax function;computer science	Vision	25.13903142439599	-50.59168864720667	106922
578ffc8760d154c5c6045239e860abdc58c96c28	wavelet kernel local fisher discriminant analysis with particle swarm optimization algorithm for bearing defect classification	wavelet analysis;kernel;kernel feature extraction wavelet analysis pattern recognition particle swarm optimization mechanical bearings fault diagnosis;feature extraction;particle swarm optimization;vibration bearing data wavelet kernel local fisher discriminant analysis particle swarm optimization algorithm bearing defect classification feature extraction dimensionality reduction linear local fisher discriminant analysis dr algorithm wavelet kernel lfda wavelet kernel function synthetic data;pattern recognition;mechanical bearings;wavelet transforms feature extraction machine bearings mechanical engineering computing particle swarm optimisation;fault diagnosis;wavelet kernel bearing defect classification dimensional reduction feature extraction local fisher discriminant analysis lfda pattern recognition	Feature extraction and dimensionality reduction (DR) are necessary and helpful preprocessing steps for bearing defect classification. Linear local Fisher discriminant analysis (LFDA) has recently been developed as a popular method for feature extraction and DR. However, the linear method tends to give undesired results if the samples between classes are nonlinearly separated in the input space. To enhance the performance of LFDA in bearing defect classification, a new feature extraction and DR algorithm based on wavelet kernel LFDA (WKLFDA) is presented in this paper. Herein, a new wavelet kernel function is proposed to construct the kernel function of LFDA. To seek the optimal parameters for WKLFDA, particle swarm optimization (PSO) is used; as a result, a new PSO-WKLFDA algorithm is proposed. The experimental results for the synthetic data and measured vibration bearing data show that the proposed WKLFDA and PSO-WKLFDA outperform other state-of-the-art algorithms.	algorithm;dimensionality reduction;feature extraction;kernel (operating system);linear discriminant analysis;mathematical optimization;multimodal interaction;nonlinear system;particle swarm optimization;pattern recognition;preprocessor;software bug;synthetic data;wavelet	Mien Van;Hee-Jun Kang	2015	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2015.2450352	wavelet;mathematical optimization;kernel fisher discriminant analysis;kernel;feature extraction;computer science;machine learning;pattern recognition;mathematics;particle swarm optimization;variable kernel density estimation;statistics;dimensionality reduction	ML	28.540869245896236	-42.31930468927742	106996
0af2f02b599e3dd272e6714e0696a8f6bdd73bf6	sign spotting using hierarchical sequential patterns with temporal intervals	sign recognition;video signal processing image classification image sequences sign language recognition trees mathematics;conference proceedings;machine learning sign recognition sequential pattern learning temporal intervals;machine learning;itemsets videos assistive technology silicon indexes hidden markov models gesture recognition;temporal intervals;sign recogniser sign spotting hierarchical sequential patterns temporal intervals videos spatiotemporal sign signatures sequential interval patterns multiclass classifier hierarchical sip tree hsp tree hsp forest concatenated isolated sign sequences continuous sign sequences;sequential pattern learning	This paper tackles the problem of spotting a set of signs occuring in videos with sequences of signs. To achieve this, we propose to model the spatio-temporal signatures of a sign using an extension of sequential patterns that contain temporal intervals called Sequential Interval Patterns (SIP). We then propose a novel multi-class classifier that organises different sequential interval patterns in a hierarchical tree structure called a Hierarchical SIP Tree (HSP-Tree). This allows one to exploit any subsequence sharing that exists between different SIPs of different classes. Multiple trees are then combined together into a forest of HSP-Trees resulting in a strong classifier that can be used to spot signs. We then show how the HSP-Forest can be used to spot sequences of signs that occur in an input video. We have evaluated the method on both concatenated sequences of isolated signs and continuous sign sequences. We also show that the proposed method is superior in robustness and accuracy to a state of the art sign recogniser when applied to spotting a sequence of signs.	algorithm;antivirus software;biconnected component;concatenation;electronic signature;experiment;hidden markov model;language model;lexicon;random forest;tree structure	Eng-Jon Ong;Nicolas Pugeault;Richard Bowden	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.248	speech recognition;computer science;machine learning;pattern recognition	Vision	37.15363421564928	-51.630148208979065	107117
0400414466c63d93b24fc2b366243bf9ea9844ec	a unified fuzzy framework for human-hand motion recognition	time clustering tc fuzzy active curve axis gaussian mixture model facagmm fuzzy empirical copula fec human hand motion recognition;pattern clustering;image motion analysis;convergence;takagi sugeno;gaussian processes;hidden markov model;real time;training;fuzzy active curve axis gaussian mixture model facagmm;indexing terms;fuzzy set theory;fuzzy clustering;gaussian mixture model;electronic and computer engineering;hidden markov models;data dependence;pattern clustering fuzzy set theory gaussian processes gesture recognition hidden markov models image motion analysis;robots;mathematical model humans hidden markov models equations robots training convergence;fuzzy empirical copula fec;mathematical model;dependence structure;humans;convergence time;human hand motion recognition;hidden markov model unified fuzzy framework human hand motion recognition grasp motions in hand manipulation recognition time clustering fuzzy active axis gaussian mixture model fuzzy empirical copula numerical clustering data dependence structure fuzzy time modeling approach takagi sugeno modeling fuzzy clustering hand gestures;gesture recognition;time clustering tc	Unconstrained human-hand motions that consist grasp motions and in-hand manipulations lead to a fundamental challenge that many algorithms have to face in both theoretical and practical development, mainly due to the complexity and dexterity of the human hand. There is no effective solution reported to recognize in-hand manipulations, although recognition algorithms have been proposed to recognize grasp motions in constrained scenarios. This paper proposes a novel unified fuzzy framework of a set of recognition algorithms: time clustering, fuzzy active axis Gaussian mixture mode, and fuzzy empirical copula, from numerical clustering to data dependence structure in the context of optimally real-time human-hand motion recognition. Time clustering is a fuzzy time-modeling approach that is based on fuzzy clustering and Takagi--Sugeno modeling with a numerical value as output. The fuzzy active axis Gaussian mixture model effectively extract abstract Gaussian pattern to represent components of hand gestures with a fast convergence. A fuzzy empirical copula utilizes the dependence structure among the finger joint angles to recognize the motion type. The proposed algorithms have been evaluated on a wide range of scenarios of human-hand recognition: 1) datasets that include 13 grasps and ten in-hand manipulations; 2) single subject and multiple subjects; and 3) varying training samples. The experimental results have demonstrated that the proposed framework outperforms the hidden Markov model (HMM) and Gaussian mixture model in terms of both effectiveness and efficiency criteria.	algorithm;apache axis;cluster analysis;complexity;data dependency;fuzzy clustering;hand geometry;hidden markov model;markov chain;mixture model;numerical analysis;real-time locating system	Zhaojie Ju;Honghai Liu	2011	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2011.2150756	robot;index term;convergence;fuzzy clustering;computer science;artificial intelligence;fuzzy number;machine learning;pattern recognition;mixture model;mathematical model;gaussian process;mathematics;fuzzy set;hidden markov model	Robotics	35.66103603519088	-42.715631520726234	107370
ab98de928eda575dc9a66d2c4df129e019b55157	autonomous terrain classification with co- and self-training approach	self training autonomous terrain classification planetary rovers onboard imagery vibration data self supervised learning mechanical vibrations co training vision based classifier four wheeled test rover mars analogous terrain support vector machines wavelet based features color features;wavelet transforms aerospace computing image classification image colour analysis learning artificial intelligence planetary rovers support vector machines terrain mapping vibrations;training training data vibrations mars wheels soil image color analysis;space robotics visual learning semantic scene understanding	Identifying terrain type is crucial to safely operating planetary exploration rovers. Vision-based terrain classifiers, which are typically trained by thousands of labeled images using machine learning methods, have proven to be particularly successful. However, since planetary rovers are to boldly go where no one has gone before, training data are usually not available a priori; instead, rovers have to quickly learn from their own experiences in an early phase of surface operation. This research addresses the challenge by combining two key ideas. The first idea is to use both onboard imagery and vibration data, and let rovers learn from physical experiences through self-supervised learning. The underlying fact is that visually similar terrain may be disambiguated by mechanical vibrations. The second idea is to employ the co- and self-training approaches. The idea of co-training is to train two classifiers separately for vision and vibration data, and re-train them iteratively on each other's output. Meanwhile, the self-training approach, applied only to the vision-based classifier, re-trains the classifier on its own output. Both approaches essentially increase the amount of labels, hence enable the terrain classifiers to operate from a sparse training dataset. The proposed approach was validated with a four-wheeled test rover in Mars-analogous terrain, including bedrock, soil, and sand. The co-training setup based on support vector machines with color and wavelet-based features successfully estimated terrain types with 82% accuracy with only three labeled images.	bedrock (framework);co-training;experience;machine learning;planetary scanner;rover (the prisoner);sparse matrix;statistical classification;supervised learning;support vector machine;terrain rendering;wavelet	Kyohei Otsu;Masahiro Ono;Thomas J. Fuchs;Ian Baldwin;Takashi Kubota	2016	IEEE Robotics and Automation Letters	10.1109/LRA.2016.2525040	computer vision;simulation;remote sensing	Robotics	33.130644483286794	-42.80528340754871	107474
919cf6064f6a9cd02f377752ce259cdf46562ac0	regional maximum activations of convolutions with attention for cross-domain beauty and personal care product retrieval		Cross-domain beauty and personal care product image retrieval is a challenging problem due to data variations (e.g., brightness, viewpoint, and scale), and the rich types of items. In this paper, we present a regional maximum activations of convolutions with attention (RA-MAC) descriptor to extract image features for retrieval. RA-MAC improves the regional maximum activations of convolutions (R-MAC) descriptor considering the influence of background in cross-domain images (i.e., shopper domain and seller domain). More specifically, RA-MAC utilizes the characteristics of the convolutional layer to find the attention of an image, and reduces the influence of the unimportant regions in an unsupervised manner. Furthermore, a few strategies have been exploited to improve the performance, such as multiple features fusion, query expansion, and database augmentation. Extensive experiments conducted on a dataset consisting of half a million images of beauty care products (Perfect-500K) manifest the effectiveness of RA-MAC. Our approach achieves the 2nd place in the leader board of the Grand Challenge of AI Meets Beauty in ACM Multimedia 2018. Our code is available at: https://github.com/RetrainIt/Perfect-Half-Million-Beauty-Product-Image-Recognition-Challenge.	convolution;darpa grand challenge;experiment;image retrieval;query expansion;traffic collision avoidance system	Zehang Lin;Zhenguo Yang;Feitao Huang;Junhong Chen	2018		10.1145/3240508.3266436	feature (computer vision);beauty;computer vision;query expansion;artificial intelligence;image retrieval;convolution;personal care product;computer science	Vision	31.770613886065586	-51.62396074939433	107666
19eaad7f48669d9e316b781183c5c07f0d867b02	graph transformation for keypoint trajectory coding	image coding;transform coding;trajectory;discrete cosine transforms;image analysis;encoding	In contrast to still image analysis, motion information offers a powerful means to analyze video. In particular, motion trajectories determined from keypoints have become very popular in recent years for a variety of video analysis tasks, including search, retrieval and classification. Additionally, cloud-based analysis of media content has been gaining momentum, so efficient communication of salient video information to perform the necessary analysis of video at the cloud server is needed. In this paper, we propose a novel graph transformation to efficiently represent the keypoint trajectories, motivated by the fact that keypoints are distributed irregularly across the images. Compared to conventional DCT-like transformation, it is easier for graph transform to compact the energy and make the coding efficiently. Experimental results on several popular datasets including Stanford MAR, Hopkin155, KITTI, etc. demonstrate a significant rate saving between 26% and 42% with our proposed trajectory coding approaches relative to a DCT based transformation approach, provided that the coding errors are between 2 pixels to 4 pixels.	cloud computing;discrete cosine transform;graph rewriting;image analysis;pixel;server (computing);video content analysis;virtual private server	Dong Tian;Huifang Sun;Anthony Vetro	2016	2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2016.7905881	computer vision;theoretical computer science;mathematics;computer graphics (images)	Vision	37.45295308588209	-51.57362749888614	107818
37ed3f01f9c4dc96fee6e5e7676ee6e1344ca01a	vlad is not necessary for cnn		Global convolutional neural networks (CNNs) activations lack geometric invariance, and in order to address this problem, Gong et al proposed multi-scale orderless pooling(MOP-CNN), which extracts CNN activations for local patches at multiple scale levels, and performs orderless VLAD pooling to extract features. However, we find that this method can improve the performance mainly because it extracts global and local representation simultaneously, and VLAD pooling is not necessary as the representations extracted by CNN is good enough for classification. In this paper, we propose a new method to extract multi-scale features of CNNs, leading to a new structure of deep learning. The method extracts CNN representations for local patches at multiple scale levels, then concatenates all the representations at each level separately, finally, concatenates the results of all levels. The CNN is trained on the ImageNet dataset to extract features and it is then transferred to other datasets. The experimental results obtained on the databases MITIndoor and Caltech-101 show that the performance of our proposed method is superior to the MOP-CNN.	artificial neural network;caltech 101;convolutional neural network;database;deep learning;imagenet;principle of good enough	Dan Yu;Xiaojun Wu	2016		10.1007/978-3-319-49409-8_41	machine learning;artificial intelligence;convolutional neural network;computer science;pooling;deep learning	Vision	26.003716739317102	-52.02129792328214	107856
856c09ab10efbc8c61a84a951746654d947370f3	human action recognition by learning bases of action attributes and parts	detectors;image recognition;object recognition;image classification;vectors;feature extraction;image reconstruction;action recognition;humans;learning artificial intelligence;object recognition image classification image reconstruction learning artificial intelligence;stanford 40 actions dataset human action recognition action attributes action parts still images sparse bases learning sparse coefficients action image reconstruction feature reconstruction approach pascal action dataset;humans detectors image reconstruction vectors feature extraction noise image recognition;noise	In this work, we propose to use attributes and parts for recognizing human actions in still images. We define action attributes as the verbs that describe the properties of human actions, while the parts of actions are objects and poselets that are closely related to the actions. We jointly model the attributes and parts by learning a set of sparse bases that are shown to carry much semantic meaning. Then, the attributes and parts of an action image can be reconstructed from sparse coefficients with respect to the learned bases. This dual sparsity provides theoretical guarantee of our bases learning and feature reconstruction approach. On the PASCAL action dataset and a new “Stanford 40 Actions” dataset, we show that our method extracts meaningful high-order interactions between attributes and parts in human actions while achieving state-of-the-art classification performance.	coefficient;interaction;sparse matrix	Bangpeng Yao;Xiaoye Jiang;Aditya Khosla;Andy Lai Lin;Leonidas J. Guibas;Li Fei-Fei	2011	2011 International Conference on Computer Vision	10.1109/ICCV.2011.6126386	iterative reconstruction;computer vision;detector;contextual image classification;feature extraction;computer science;noise;cognitive neuroscience of visual object recognition;machine learning;pattern recognition	Vision	25.841658638497407	-46.349686999018125	107948
08b676de6a31f1d442741ab841838560d367da87	scalable 3d tracking of multiple interacting objects	3d;computational time scalable 3d tracking multiple interacting object tracking rgbd input hypothesize and test approach camera naive approach set of independent trackers sit disjoint consideration occlusions computational complexity middle ground ensemble of collaborative trackers ect multiobject 3d tracking problem;efficient 3d tracking interaction occlusions joint optimization;interaction;efficient;three dimensional displays optimization linear programming joints collaboration accuracy tracking;object tracking cameras computational complexity;optimization;joint;occlusions;tracking	We consider the problem of tracking multiple interacting objects in 3D, using RGBD input and by considering a hypothesize-and-test approach. Due to their interaction, objects to be tracked are expected to occlude each other in the field of view of the camera observing them. A naive approach would be to employ a Set of Independent Trackers (SIT) and to assign one tracker to each object. This approach scales well with the number of objects but fails as occlusions become stronger due to their disjoint consideration. The solution representing the current state of the art employs a single Joint Tracker (JT) that accounts for all objects simultaneously. This directly resolves ambiguities due to occlusions but has a computational complexity that grows geometrically with the number of tracked objects. We propose a middle ground, namely an Ensemble of Collaborative Trackers (ECT), that combines best traits from both worlds to deliver a practical and accurate solution to the multi-object 3D tracking problem. We present quantitative and qualitative experiments with several synthetic and real world sequences of diverse complexity. Experiments demonstrate that ECT manages to track far more complex scenes than JT at a computational time that is only slightly larger than that of SIT.	baseline (configuration management);computation;computational complexity theory;electroconvulsive therapy;experiment;forth;interaction;jt (visualization format);synthetic intelligence;time complexity	Nikolaos Kyriazis;Antonis A. Argyros	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.438	joint;computer vision;interaction;simulation;computer science;tracking;3d computer graphics	Vision	34.11441644052556	-46.27977123132147	107959
a9cecfbc47a39fa0158a5f6fd883e0e5ac2aa134	optimal subspace analysis for face recognition	face recognition;linear discriminant analysis;small sample size problem;subspace analysis	Fisher Linear Discriminant Analysis (LDA) has been successfully used as a data discriminantion technique for face recognition. This paper has developed a novel subspace approach in determining the optimal projection. This algorithm effectively solves the small sample size problem and eliminates the possibility of losing discriminative information. Through the theoretical derivation, we compared our method with the typical PCA-based LDA methods, and also showed the relationship between our new method and perturbation-based method. The feasibility of the new algorithm has been demonstrated by comprehensive evaluation and comparison experiments with existing LDAbased methods.	algorithm;ar (unix);benchmark (computing);computation;database;experiment;feret (facial recognition technology);facial recognition system;kernel (linear algebra);linear discriminant analysis;optimal projection equations;pattern recognition;return loss	Haitao Zhao;Pong C. Yuen;Jingyu Yang	2005	IJPRAI	10.1142/S0218001405004071	computer vision;computer science;machine learning;pattern recognition;mathematics;linear discriminant analysis;statistics	AI	24.7776222558119	-40.560494053938356	108046
6d517a4896d87a11aad13af9e5651029add41714	multiscale integration approach for land cover classification based on minimal entropy of posterior probability	image segmentation;uncertainty;earth;training;indexes;pixel based entropy land cover mapping multiscale object based;remote sensing;entropy;image segmentation entropy uncertainty earth training remote sensing indexes	Object-based land cover mapping has drawn increasing attention for its ability to overcome the salt-and-pepper problem associated with pixel-based methods by considering spatial information from neighboring regions. However, the performance of object-based classification is strongly affected by over- or undersegmented objects. The optimal scale is difficult to determine; moreover, it usually varies along with the application purpose or classification targets. Most previous efforts on scale determination based only on image information are not flexible in adapting to different classification systems; consequently, their use is not advisable. In this paper, to better consider classification targets, the information from training samples for classification is also used for determining the optimal scale based on the concept of minimal entropy of posterior probability (MEPP). The proposed MEPP method consists mainly of two stages: 1) training samples from the original pixel level are applied to classify segmented images and obtain posterior probability maps on multiple scales; and 2) the optimal object scale is determined according to the MEPP that corresponds to the minimum classification uncertainty. Experiments on high-spatial-resolution images and Landsat images confirm the superiority of the proposed MEPP method in land cover classification.	map;object-based language;pixel;statistical classification	Dedi Yang;Xuehong Chen;Jin Chen;Xin Cao	2017	IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing	10.1109/JSTARS.2016.2615073	database index;computer vision;entropy;uncertainty;computer science;machine learning;pattern recognition;earth;image segmentation;physics;statistics;remote sensing	Vision	30.895860991392382	-44.033199035002646	108353
caab74e7ad5605207041a010847c5cbce27a7842	geometric spherical networks for visual data processing	haptic device geometric spherical networks visual data processing geometric neural network sensors robot mechanisms spherical radial basis function network spherical general regression network conformal geometric algebra framework activation functions medical robotics;geometry;regression analysis biosensors control engineering computing geometry medical image processing medical robotics radial basis function networks;medical robotics;radial basis function networks;medical image processing;vectors blades haptic interfaces image reconstruction biomedical imaging neurons;regression analysis;control engineering computing;biosensors	The main goal of this work is to develop a geometric neural network which can be used as an interface between sensors and robot mechanisms. For this goal we have developed two new geometric network called Spherical Radial Basis Function Network and Spherical General Regression Network using the conformal geometric algebra framework. The motivation to use circles or spheres as activation functions is due to the fact that the sphere is the computational unity of the conformal geometric algebra, as a result these Networks can be advantageously used as interface between the sensor domain and the robotic mechanism so that all the computing can be done in the same mathematical framework. In fact, there will be no need to abandon the system for the interpolation or reconstruction using this network. This article presents the design principles and a comparison with a standard Radial Basis Function Network and a standard General regression Neural Network. In the area of medical robotics the use of haptics is quite common. This is an interesting domain to apply our network for capturing data with a haptic device and using spheres reconstruct automatically the shape of a human organ. As we shown in other works, the differential robot kinematics can be formulated using lines, planes and spheres using geometric algebra, having the organ tissue modelled also with spheres with the networks, this helps greatly to related the perceptual and the mechanical devices and ensure their control. We show reconstruction results of an organ using both Networks.	activation function;artificial neural network;computer vision;conformal geometric algebra;geometric networks;haptic technology;interpolation;radial (radio);radial basis function network;robot;robot end effector;robotics;sensor	Efrain Castillo-Muñiz;Eduardo Bayro-Corrochano	2012	The 2012 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2012.6252770	computer vision;computer science;artificial intelligence;machine learning;mathematics;geometric networks;regression analysis;biosensor	Robotics	37.10565723048278	-38.98830858391392	108464
73939315e47df4b9da1eeaaa6204cc08b3a5e030	techniques based on support vector machines for cloud detection on quickbird satellite imagery	geophysical image processing;quickbird satellite imagery cloud detection techniques cloud cover optical images landsat 7 fusion technique singular value decomposition feature extraction thematic classification tree kernel function svm lighttk similarity level tree structures cloud classification problem cloud cover maps classification accuracy support vector machines;thematic classification;neighborhood model;settore ing inf 02 campi elettromagnetici;kernel;support vector machines;svm cloud detection neighborhood model svd;training;singular value decomposition;cloud classification problem;kernel function;cloud detection techniques;tree structures;cloud detection;accuracy;quickbird satellite imagery;fusion technique;optical imaging;cloud cover maps;svd;similarity level;feature extraction;clouds;remote sensing;satellites;tree structure;svm lighttk;optical images;kernel accuracy feature extraction training support vector machines remote sensing satellites;svm;satellite imagery;atmospheric techniques;support vector machine;point of view;classification accuracy;cloud cover;support vector machines atmospheric techniques clouds geophysical image processing singular value decomposition;landsat 7;optimal algorithm;tree kernel function	Purpose of this work is the study of cloud detection techniques. This work identifies the cloud cover of optical images acquired by the QuickBird satellite, comparing these with others of the same area, acquired by Landsat 7 in which there are no clouds. The images are combined using an early fusion technique [1]. The tool exploits the neighborhood model [2] for increasing the amount of information for the training set and the Singular Value Decomposition for carrying out the feature extraction [3]. In order to introduce these structures into thematic classification tasks by SVMs it was necessary develop a tree kernel function based on tree kernel function defined in SVM-LightTK. The aim of the tree kernel function is evaluate the similarity level between a generic couples of tree structures. In this paper we report the results obtained comparing the performance of different approaches in cloud classification problem. The final purpose is the production of cloud cover maps. Throughout such different experimental setups we measured the capabilities of each algorithm under different points of view. First of all, we considered the classification accuracy by computing traditional parameter such as overall accuracy. A second analysis regarded the efforts that are required in the design of optimal algorithms. Indeed, these techniques are characterized by different parameters that have to be appropriately tuned in order to obtain the best performance. Finally the robustness of the techniques has been also considered. In particular the classification accuracy has been evaluated also for images not considered in the training phase.	algorithm;feature extraction;map;norm (social);singular value decomposition;support vector machine;test set	Riccardo Rossi;Roberto Basili;Fabio Del Frate;Matteo Luciani;Francesco Mesiano	2011	2011 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2011.6049178	support vector machine;computer vision;computer science;machine learning;data mining;remote sensing	Vision	31.86351291579495	-44.077758048291564	108569
dc73b2762ed952976bd10b8fb0b16e31dbaf5da7	two-dimensional discriminant locality preserving projection based on ℓ1-norm maximization		In this paper, a new linear dimensionality reduction method named Two-Dimensional Discriminant Locality Preserving Projection Based on l1-norm Maximization (2DDLPP-L1) is proposed for preprocessing of image data. 2DDLPP-L1 makes full use of the robustness of l1-norm to noises and outliers. Furthermore, 2DDLPP-L1 is a 2D-based method which extracts image features directly from image matrices, avoiding instability and high complexity of matrix computation. Two graphs, separation graph and cohesiveness graph, are constructed with feature vectors as vertices to represent the inter-class separation and intra-class cohesiveness. An iterative algorithm with proof of convergence is proposed to solve the optimal projection matrix. Experiments on several face image databases demonstrate that the performance and robustness of 2DDLPP-L1 are better than its related methods.	discriminant;expectation–maximization algorithm;locality of reference;taxicab geometry	Sibao Chen;Jing Wang;Cai-Yin Liu;Bin Luo	2017	Pattern Recognition Letters	10.1016/j.patrec.2016.04.007	mathematical optimization;machine learning;pattern recognition;mathematics	Vision	25.96756455755951	-40.859855241547976	108652
46ecd7bbbb682abe153e469a5c8d4b0ef14b97d3	self-regularized fixed-rank representation for subspace segmentation		Consider a set of data points generated from various linear subspaces. Subspace segmentation tasks, which are important in fields such as computer vision and image processing, aim to partition the set of data points so as to recover these subspaces. The subspace segmentation method, fixed rank representation (FRR), was introduced to remedy the problem of insufficient sampling in classical low rank representation (LRR). In many subspace segmentation applications, FRR has achieved much better results than those of LRR. In this paper, a new FRR-related algorithm, called self-regularized fixed rank representation (SRFRR), is proposed. In SRFRR, a Laplacian regularizer is constructed using the coefficient matrix obtained by SRFRR itself. Further, by proving that the Laplacian regularizer can be transformed into a structure constraint on the coefficient matrix, we show that another existing method, sparse FRR (SFRR), is a special case of SRFRR. To implement the SRFRR method, we present two optimization algorithms. Experiments on both synthetic and real databases show that SRFRR outperforms some existing FRR and LRR related algorithms. ∗Corresponding author. Tel. 86-21-38282826, fax. 86-21-38282800 Email addresses: weilai@shmtu.edu.cn (Lai Wei), xfwang@shmtu.edu.cn (Xiaofeng Wang), junyin@shmtu.edu.cn (Jun Yin), ahwu@shmtu.edu.cn (Aihua Wu) Preprint submitted to Information Sciences May 8, 2017	algorithm;benchmark (computing);coefficient;computer vision;cylinder-head-sector;data point;database;digraphs and trigraphs;email;emoticon;experiment;fax;image processing;iteration;laplacian matrix;loss function;low-rank approximation;mathematical optimization;maxima and minima;optimization problem;sql server compact;sampling (signal processing);sparse matrix;synthetic data;synthetic intelligence	Lai Wei;Xiaofeng Wang;Jun Yin;Aihua Wu	2017	Inf. Sci.	10.1016/j.ins.2017.05.007	mathematical optimization;combinatorics;discrete mathematics;machine learning;mathematics	Vision	26.117868178648312	-40.6597665235289	108669
9b73daf547ecf9c612d8efaf6022b0a6637f33ca	robust gradient learning with applications	regression analysis concave programming covariance analysis gradient methods hilbert spaces learning artificial intelligence;robustness gradient learning gl instance based kernelized dictionary nonlinear variable selection regularization;robustness input variables noise measurement computational modeling dictionaries numerical models;coordinate covariance estimation robust gradient learning problem rgl problem gradient vector supervised learning problems variable selection algorithm coordinate covariance estimation supervised dimension reduction robust regression loss function robust classification loss instance based kernelized dictionary kernel hilbert space nonconvex model computational algorithm gradient descent method rgl model;sista	This paper addresses the robust gradient learning (RGL) problem. Gradient learning models aim at learning the gradient vector of some target functions in supervised learning problems, which can be further used to applications, such as variable selection, coordinate covariance estimation, and supervised dimension reduction. However, existing GL models are not robust to outliers or heavy-tailed noise. This paper provides an RGL framework to address this problem in both regression and classification. This is achieved by introducing a robust regression loss function and proposing a robust classification loss. Moreover, our RGL algorithm works in an instance-based kernelized dictionary instead of some fixed reproducing kernel Hilbert space, which may provide more flexibility. To solve the proposed nonconvex model, a simple computational algorithm based on gradient descent is provided and the convergence of the proposed method is also analyzed. We then apply the proposed RGL model to applications, such as nonlinear variable selection and coordinate covariance estimation. The efficiency of our proposed model is verified on both synthetic and real data sets.	addresses (publication format);computation;convergence (action);dictionary [publication type];dimensionality reduction;feature selection;gradient descent;hearing loss, high-frequency;hilbert space;kernel method;learning disorders;loss function;maxima and minima;nonlinear system;r language;selection algorithm;supervised learning;synthetic intelligence;tail	Yunlong Feng;Yuning Yang;Johan A. K. Suykens	2016	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2015.2425215	mathematical optimization;machine learning;pattern recognition;mathematics	ML	25.48980569040429	-38.60285774490193	108708
2e4bb88ac792ddcd26609654ed79de8956661725	linear discriminant analysis for large-scale data: application on text and image data	eigenvalues and eigenfunctions;approximation algorithms;training;matrix decomposition;feature extraction;linear discriminant analysis;algorithm design and analysis	Linear Discriminant Analysis (LDA) is a technique which is frequently used to extract discriminative features that preserve the class separability. LDA involves matrices eigen decomposition which can be computationally expensive in both time and memory, in particular when the number of samples and the number of features are large. This is the case for text and image data sets where the dimension can reach in order of hundreds of thousands or more. In this paper, we propose an efficient algorithm Fast-LDA to handle large scale data for discriminant analysis. The proposed approach uses a feature extraction method based on random projection to reduce the dimensionality and then perform LDA in the reduced space. By reducing data dimension, we reduce the complexity of data analysis. The accuracy and the computational time of the proposed approach are provided for a wide variety of real image and text data sets. The results show the relevance of the proposed method compared to other methods.	analysis of algorithms;approximation algorithm;computation;eigen (c++ library);experiment;fast fourier transform;feature extraction;linear discriminant analysis;linear separability;random projection;relevance;singular value decomposition;text corpus;time complexity	Nassara Elhadji Ille Gado;Edith Grall-Maës;Malika Kharouf	2016	2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2016.0173	algorithm design;kernel fisher discriminant analysis;feature extraction;computer science;machine learning;pattern recognition;data mining;optimal discriminant analysis;mathematics;linear discriminant analysis;matrix decomposition;multiple discriminant analysis;statistics	Robotics	26.23271778730928	-40.340053873821475	108752
a231fdbd9065ec7ca76484816bf64a6748426b42	low-light pedestrian detection from rgb images using multi-modal knowledge distillation		While deep learning based pedestrian detection systems have continued to scale new heights in recent times, the performance of such algorithms tends to degrade under challenging illumination conditions. This causes a bottleneck in ready portability of such systems to Advanced Driver Assistance Systems (ADAS), where consistent performance across varying environmental lighting is desired. Inspired by the concept of dark knowledge, this paper proposes a novel guided deep network that distills knowledge from a multi-modal pedestrian detector. The proposed network learns to extract both RGB and thermal-like features from RGB images alone, thus compensating for the requirement of significantly costly automotive-grade thermal cameras. Compelling detection performance in severe lighting conditions is demonstrated on a publicly available night-time pedestrian dataset — KAIST. We achieve an effective miss-rate of 12% lower than the recent state-of-the-art methods.	algorithm;architecture design and assessment system;deep learning;modal logic;pedestrian detection	Srinivas S. S. Kruthiventi;Pratyush Sahay;Rajesh Biswal	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8297075	advanced driver assistance systems;computer vision;knowledge engineering;rgb color model;feature extraction;deep learning;software portability;computer science;pedestrian detection;bottleneck;artificial intelligence	Robotics	28.61931811728921	-50.60429388521357	108780
32e4610bc6aec577f8c660a8c82ac90b23488fbb	active deformable part models inference		This paper presents an active approach for part-based object detection, which optimizes the order of part filter evaluations and the time at which to stop and make a prediction. Statistics, describing the part responses, are learned from training data and are used to formalize the part scheduling problem as an offline optimization. Dynamic programming is applied to obtain a policy, which balances the number of part evaluations with the classification accuracy. During inference, the policy is used as a look-up table to choose the part order and the stopping time based on the observed filter responses. The method is faster than cascade detection with deformable part models (which does not optimize the part order) with negligible loss in accuracy when evaluated on the PASCAL VOC 2007 and 2010 datasets.	dynamic programming;kalman filter;lookup table;mathematical optimization;object detection;online and offline;scheduling (computing)	Menglong Zhu;Nikolay Atanasov;George J. Pappas;Kostas Daniilidis	2014		10.1007/978-3-319-10584-0_19	computer vision;simulation;computer science;artificial intelligence;machine learning;data mining;statistics	Vision	36.703732224184904	-44.33105611314772	108812
0b6f776c45b6f81cd9f9ae2bb8aca0bc2f541528	crowd counting using accumulated hog	measurement;training;data mining;positron emission tomography;feature extraction;robustness;benchmark testing	People count is an important indicator in video surveillance. Due to the overlapping objects and cluttered background, counting people accurately in actual crowded scene remains a non-trivial problem. Existing regression-based methods either learn a single model mapping the global feature to people count, or estimate localized count by training a large number of regressors. In this paper, we present an intermediate approach using the accumulated HOG feature. Our approach is able to capture the spatial difference of crowd structure and does not need to train a large number of regressors. Contrast to the low-level features existing regression-based methods generally use, the accumulated HOG feature is more robust. Extensive evaluations have been done on five benchmark datasets in the field of crowd counting, which demonstrate the robustness and effectiveness of our approach. In particular, the processing speed is fast enough to be applied to practical applications.	benchmark (computing);closed-circuit television;experiment;high- and low-level;real-time computing;real-time transcription	Tianchun Xu;Xiaohui Chen;Guo Wei;Weidong Wang	2016	2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2016.7603465	benchmark;computer vision;simulation;feature extraction;computer science;machine learning;data mining;measurement;statistics;robustness	Vision	32.48382619069067	-51.50525953054308	108915
03e72b4f932998bc363df1d49650bfa33dcc2872	fisher discrimination dictionary pair learning for image classification		Abstract Dictionary learning has played an important role in the success of sparse representation. Although several dictionary learning approaches have been developed for image classification, discriminative dictionary pair learning, i.e., jointly learning a synthesis dictionary and an analysis dictionary, is still in its infant stage. In this paper, we proposed a novel model of Fisher discrimination dictionary pair learning (FDDPL), in which Fisher discrimination information is embedded into analysis representation, analysis dictionary, and synthesis dictionary representation. With the proposed Fisher-like discrimination term, discrimination of both synthesis dictionary representation and analysis dictionary representation is introduced into the dictionary pair learning model. An iterative algorithm to efficiently solve the proposed FDDPL and a FDDPL based classifier are also presented in this paper. The experiments on face recognition, scene categorization, gender classification, and action recognition clearly show the advantage of the proposed FDDPL.	algorithm;categorization;computer vision;dictionary;embedded system;experiment;facial recognition system;iterative method;kullback–leibler divergence;machine learning;sparse approximation;sparse matrix	Meng Meng Yang;Heyou Chang;Weixin Luo;Jian Yang	2017	Neurocomputing	10.1016/j.neucom.2016.08.146	discriminative model;machine learning;artificial intelligence;facial recognition system;iterative method;categorization;k-svd;pattern recognition;sparse approximation;classifier (linguistics);speech recognition;contextual image classification;computer science	Vision	25.689408617929672	-45.79261908541382	108936
dc6de5102e70b4a814cab1c9e85326cbec39c80e	discriminative weighted band selection via one-class svm for hyperspectral imagery	one class svm framework discriminative weighted band selection hyperspectral image classification informative bands computation cost storage cost supervised band selection method one class classifier band weight vector;support vector machines;support vector machines hyperspectral imaging training testing feature extraction optimization;training;support vector machines geophysical image processing hyperspectral imaging image classification;supervised learning band selection hyperspectral imagery image classification one class svm;testing;feature extraction;optimization;hyperspectral imaging	In the task of hyperspectral image classification, band selection is often adopted to select a subset of informative bands to reduce the computation and storage cost. We propose a supervised band selection method which allows calculation of a discriminative weight for each band. Specifically, we consider discriminative bands as those that contribute more positive scores to a one-class classifier than those for other classes during the training stage. Based on this observation, we learn discriminative a band weight vector for each class, then bands with larger discriminative weights can be selected. Our method can be efficiently solved in one-class SVM framework. Experimental results demonstrate the effectiveness of our method.	computation;computer vision;discriminative model;information;support vector machine	Yu Tang;Enlong Fan;Cheng Yan;Xiao Bai;Jun Zhou	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7729714	support vector machine;computer vision;feature extraction;computer science;hyperspectral imaging;machine learning;pattern recognition;mathematics;software testing;remote sensing	Robotics	29.976293916042696	-43.505024143580364	109126
f87ae55502267f82e031a8101b0efa626f3e6c7a	pedestrian detection based on hierarchical co-occurrence model for occlusion handling	pedestrian detection;partial occlusions;co occurrence relations;visibility status	In pedestrian detection, occlusions are typically treated as an unstructured source of noise and explicit models have lagged behind those for object appearance, which will result in degradation of detection performance. In this paper, a hierarchical co-occurrence model is proposed to enhance the semantic representation of a pedestrian. In our proposed hierarchical model, a latent SVM structure is employed to model the spatial co-occurrence relations among the parent–child pairs of nodes as hidden variables for handling the partial occlusions. Moreover, the visibility statuses of the pedestrian can be generated by learning co-occurrence relations from the positive training data with large numbers of synthetically occluded instances. Finally, based on the proposed hierarchical co-occurrence model, a pedestrian detection algorithm is implemented to incorporate visibility statuses by means of a Random Forest ensemble. The experimental results on three public datasets demonstrate the log-average miss rate of the proposed algorithm has 5% improvement for pedestrians with partial occlusions compared with the state-of-the-arts. & 2015 Elsevier B.V. All rights reserved.	algorithm;average-case complexity;elegant degradation;emoticon;hidden variable theory;hierarchical database model;latent dirichlet allocation;latent variable;neurocomputing;part-based models;pedestrian detection;random forest;sensor	Xiaowei Zhang;Hai-Miao Hu;Fan Jiang;Bo Li	2015	Neurocomputing	10.1016/j.neucom.2015.05.038	computer vision;simulation;machine learning	AI	33.26876800566841	-47.49806412898913	109422
58d2a5c4cfa5f84a3f3d3b25cac060c3ddc8f9e4	learning and inference in parametric switching linear dynamic systems	parametric hmm;human gesture;superluminescent diodes hidden markov models humans computer vision target tracking parameter estimation educational institutions food technology trajectory computer errors;image motion analysis;learning;switching linear dynamic systems;hidden markov model;global parameter;systematic temporal variation;honey bee;honeybee;proceedings;spatial variation;generalized expectation maximization;systematic spatial variation;systematic spatial variations;inference mechanisms;computer vision;learning systems;inference phase;hidden markov models;parametrized motion;expectation maximization;honeybee dance;expectation maximization method;machine vision;linear dynamical system;parametric switching linear dynamic system;learning artificial intelligence computer vision expectation maximisation algorithm hidden markov models image motion analysis inference mechanisms;learning parametric switching linear dynamic system parametrized motion systematic temporal variation systematic spatial variation honeybee dance complex motion parametric hmm human gesture global parameter expectation maximization method inference phase hidden markov model;post print;learning artificial intelligence;systematic temporal variations;inference;complex motion;expectation maximisation algorithm	We introduce parametric switching linear dynamic systems (P-SLDS) for learning and interpretation of parametrized motion, i.e., motion that exhibits systematic temporal and spatial variations. Our motivating example is the honeybee dance: bees communicate the orientation and distance to food sources through the dance angles and waggle lengths of their stylized dances. Switching linear dynamic systems (SLDS) are a compelling way to model such complex motions. However, SLDS does not provide a means to quantify systematic variations in the motion. Previously, Wilson & Bobick (1999) presented parametric HMMs, an extension to HMMs with which they successfully interpreted human gestures. Inspired by their work, we similarly extend the standard SLDS model to obtain parametric SLDS. We introduce additional global parameters that represent systematic variations in the motion, and present general expectation-maximization (EM) methods for learning and inference. In the learning phase, P-SLDS learns canonical SLDS model from data. In the inference phase, P-SLDS simultaneously quantifies the global parameters and labels the data. We apply these methods to the automatic interpretation of honey-bee dances, and present both qualitative and quantitative experimental results on actual bee-tracks collected from noisy video data	dynamical system;encode;expectation–maximization algorithm;ibm notes;parametric polymorphism;time series;wilson–cowan model	Sang Min Oh;James M. Rehg;Tucker R. Balch;Frank Dellaert	2005	Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1	10.1109/ICCV.2005.135	linear dynamical system;spatial variability;computer vision;expectation–maximization algorithm;computer science;artificial intelligence;machine learning;hidden markov model;statistics	Vision	36.843810134122045	-41.605155594028545	109434
6dd6edf59da0065603cf94533786528f24f1728e	terrain classification with polarimetric sar based on deep sparse filtering network	filtering;terrain mapping geophysical image processing image classification radar polarimetry remote sensing by radar synthetic aperture radar;training;machine learning;filtering feature extraction machine learning classification algorithms synthetic aperture radar training remote sensing;feature extraction;remote sensing;classification algorithms;feature learning polarimetric synthetic aperture radar polsar deep sparse filtering network spatial information;classification accuracy terrain classification polarimetric sar deep sparse filtering network polarimetric synthetic aperture radar deep learning network;synthetic aperture radar	A new method for Polarimetric Synthetic Aperture Radar (PolSAR) terrain classification based on Deep Sparse Filtering Network (DSFN) is proposed in this paper. It uses a novel deep learning network to learn features from the input raw data automatically. And the spatial information between pixels on PolSAR image is combined into the input data. Moreover, unlike the conventional deep networks, the DSFN only needs to tune very few parameters during pre-training and fine-tuning. A real PolSAR data is used to verify the proposed method. Experimental results show that the proposed DSFN is efficient with less parameters and effectively improves the classification accuracy compared with conventional deep networks.	deep belief network;deep learning;pixel;polarimetry;sparse matrix	Hongying Liu;Qiang Min;Chen Sun;Jin Zhao;Shuyuan Yang;Biao Hou;Jie Feng;Licheng Jiao	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7729007	filter;statistical classification;computer vision;synthetic aperture radar;feature extraction;computer science;machine learning;pattern recognition;radar imaging;inverse synthetic aperture radar;remote sensing	Mobile	30.743814785156	-44.654278726108146	109551
9e7259d6fd1e05d3fe4ca6729acde52c9e4edbc9	accelerating near-duplicate video matching by combining visual similarity and alignment distortion	quadratic program;duplicate detection;cost function;interest points;video retrieval;keyframe alignment;paired comparison;near duplicate web video retrieval	In this paper, we investigate a novel approach to accelerate the matching of two video clips by exploiting the temporal coherence property inherent in the keyframe sequence of a video. Motivated by the fact that keyframe correspondences between near-duplicate videos typically follow certain spatial arrangements, such property could be employed to guide the alignment of two keyframe sequences. We set the alignment problem as an integer quadratic programming problem, where the cost function takes into account both the visual similarity of the corresponding keyframes as well as the alignment distortion among the set of correspondences. The set of keyframe-pairs found by our algorithm provides our proposal on the list of candidate keyframe-pairs for near-duplicate detection using local interest points. This eliminates the need for exhaustive keyframe-pair comparisons, which significantly accelerates the matching speed. Experiments on a dataset of 12,790 web videos demonstrate that the proposed method maintains a similar near-duplicate video retrieval performance as the hierarchical method proposed in [12] but with a significantly reduced number of keyframe-pair comparisons.	algorithm;coherence (physics);collision detection;distortion;key frame;loss function;quadratic programming;video clip	Hung-Khoon Tan;Xiao Wu;Chong-Wah Ngo;Wanlei Zhao	2008		10.1145/1459359.1459506	pairwise comparison;computer vision;computer science;theoretical computer science;multimedia;quadratic programming;statistics	Vision	38.755761313262894	-51.682861016101434	109570
00ade59a80a186000eb043656a96a4633fb38423	compressed domain real-time action recognition	video databases;video sequence;image recognition;image motion analysis;data compression;motion correlation;real time;low complexity;correlation methods;video database compressed domain scheme action recognition video query video sequence feature extraction motion correlation;video databases correlation methods data compression feature extraction image motion analysis image recognition image sequences video coding;video coding;feature extraction;action recognition;video compression testing video sequences motion measurement fluid flow measurement performance evaluation cameras image motion analysis support vector machines support vector machine classification;video database;compressed domain scheme;video query;image sequences	We present a compressed domain scheme that is able to recognize and localize actions in real-time. The recognition problem is posed as performing a video query on a test video sequence. Our method is based on computing motion similarity using compressed domain features which can be extracted with low complexity. We introduce a novel motion correlation measure that takes into account differences in motion magnitudes. Our method is appearance invariant, requires no prior segmentation, alignment or stabilization, and is able to localize actions in both space and time. We evaluated our method on a large action video database consisting of 6 actions performed by 25 people under 3 different scenarios. Our classification results compare favorably with existing methods at only a fraction of their computational cost	algorithmic efficiency;coefficient;computation;database;discrete cosine transform;real-time clock;real-time locating system;similarity measure	Chuohao Yeo;Parvez Ahammad;Kannan Ramchandran;S. Shankar Sastry	2006	2006 IEEE Workshop on Multimedia Signal Processing	10.1109/MMSP.2006.285263	data compression;computer vision;feature extraction;computer science;pattern recognition;multimedia	Vision	38.70066739244418	-50.99403968320777	109813
1ab3e3a0048f660021fca0868e025972db271483	learning saliency features for face detection and recognition using multi-task network	multi-task network;deep learning;face detection;face recognition	In this work, we have proposed a method to learn a type of saliency features, which merely makes response in face regions. Based on the saliency features, a joint pipeline is designed to detect and recognize faces as a part of human–robot interaction (HRI) system of SRU robot. The characteristics of the architecture can be described as follows: (i) In the network, detectors can only be activated by face regions. By convoluting the input image, the detectors can produce a group of saliency feature maps, which indicate the location of faces. (ii) The face representations are achieved by pooling on these high response regions. They enjoy discriminative ability to face identification. Hence, classification and detection can be blended using a single network. (iii) To enhance the saliency of features, false responses are suppressed by introducing a saliency term in loss function, which forces the feature detector to ignore non-face inputs. It also can be seen as a branch of multi-task network to learn background. By restricting false responses, the performance of face verification can be improved, especially when the training and testing are implemented on different dataset. In experiments, the effects of saliency term on face verification and benchmark discriminative ability of saliency features on LFW are analyzed. And the effectiveness of this method in face detection is verified by the experimental results on FDDB.	face detection	Qian Zhao;Shuzhi Sam Ge;Mao Ye;Sibang Liu;Wei He	2016	I. J. Social Robotics	10.1007/s12369-016-0347-x	computer vision;computer science;machine learning;pattern recognition	Robotics	29.550287815026905	-51.325426301021324	109878
53e0c4fc3c44ef25986cca960100f78a9dcac2d7	people monitoring using face recognition with observation constraints	maximum likelihood estimation;domain knowledge;synthetic data;probability;posterior probability;face recognition;feature extraction	We propose a people monitoring system to recognize people by probabilistic inference methods exploiting low-level facial feature and high-level domain knowledge. In particular, the faces of people in the view of a monitoring camera are first detected and modeled. Optimal recognition of people leaving and entering a closed-room is accomplished by exploiting temporal correlation and constraints among the observed face sequence. The optimality is achieved in the sense of maximizing a joint posterior probability of multiple observations. Experimental results of real and synthetic data suggest the efficacy of the proposed system.	database;facial recognition system;high- and low-level;synthetic data	Ji Tao;Yap-Peng Tan	2004	2004 International Conference on Image Processing, 2004. ICIP '04.		facial recognition system;computer vision;feature extraction;computer science;machine learning;pattern recognition;probability;mathematics;maximum likelihood;posterior probability;domain knowledge;statistics;synthetic data	Vision	36.39084148827909	-45.67878848648936	110071
b7d339eb5605061c577d55514110270ae3524934	unusual scene detection using distributed behaviour model and sparse representation	video surveillance;image coding;probability;latent dirichlet allocation unusual scene detection distributed behaviour model sparse representation surveillance footage surveillance system dbm people clustering computer graphics video event detection sparse coding social force model;computer graphics;force;acceleration;computational modeling;optical imaging;force computational modeling encoding cameras acceleration optical imaging vectors;unusual scene detection;vectors;image representation;sparse coding distributed behaviour model unusual scene detection;video surveillance computer graphics image coding image representation object detection probability;distributed behaviour model;encoding;sparse coding;cameras;object detection	The ability to detect unusual events in surviellance footage as they happen is a highly desireable feature for a surveillance system. However, this problem remains challenging in crowded scenes due to occlusions and the clustering of people. In this paper, we propose using the Distributed Behavior Model (DBM), which has been widely used in computer graphics, for video event detection. Our approach does not rely on object tracking, and is robust to camera movements. We use sparse coding for classification, and test our approach on various datasets. Our proposed approach outperforms a state-of-the-art work which uses the social force model and Latent Dirichlet Allocation.	algorithm;behavior model;benchmark (computing);closed-circuit television;cluster analysis;computer graphics;dbm;latent dirichlet allocation;neural coding;scene graph;social force model;sparse approximation;sparse matrix;ucsd pascal/p-system	Jingxin Xu;Simon Denman;Clinton Fookes;Sridha Sridharan	2012	2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance	10.1109/AVSS.2012.80	acceleration;computer vision;computer science;machine learning;probability;optical imaging;neural coding;computer graphics;computational model;force;encoding;statistics;computer graphics (images)	Vision	38.84002729620183	-47.12178770996484	110085
d1561bfa6a4caa1c9e0a85005510792761d02458	recognizing point clouds using conditional random fields	robot vision interactive devices learning artificial intelligence mobile robots object detection object recognition optimisation probability;object recognition;three dimensional displays graphical models vectors training laser radar object recognition mathematical model;comunicacion de congreso;constrained optimization point cloud recognition conditional random fields object detection robotic tasks 3d sensing devices kinect supervised learning object recognition probabilistic graphical model;computer vision;conference report;proceedings22nd international conferenceon pattern recognitionicpr 2014 24 28 august 2014stockholm sweden;object detection	Detecting objects in cluttered scenes is a necessary step for many robotic tasks and facilitates the interaction of the robot with its environment. Because of the availability of efficient 3D sensing devices as the Kinect, methods for the recognition of objects in 3D point clouds have gained importance during the last years. In this paper, we propose a new supervised learning approach for the recognition of objects from 3D point clouds using Conditional Random Fields, a type of discriminative, undirected probabilistic graphical model. The various features and contextual relations of the objects are described by the potential functions in the graph. Our method allows for learning and inference from unorganized point clouds of arbitrary sizes and shows significant benefit in terms of computational speed during prediction when compared to a state-of-the-art approach based on constrained optimization.	algorithm;belief propagation;computation;conditional random field;constrained optimization;fletcher's checksum;graph (discrete mathematics);graphical model;kinect;markov chain;markov random field;mathematical optimization;point cloud;robot;software propagation;stochastic gradient descent;supervised learning	Farzad Husain;Babette Dellen;Carme Torras	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.730	computer vision;computer science;artificial intelligence;viola–jones object detection framework;cognitive neuroscience of visual object recognition;machine learning;3d single-object recognition	Vision	36.883528562913	-44.70641509766892	110109
6bf764a8ba86be5e5a11807b6aaad14c88230bd4	graph-regularized local coordinate concept factorization for image representation	nmf;concept factorization;graph regularized;local coordinate coding;image clustering	Existing matrix factorization based techniques, such as nonnegative matrix factorization and concept factorization, have been widely applied for data representation. In order to make the obtained concepts to be as close to the original data points as possible, one state-of-the-art method called locality constraint concept factorization is put forward, which represent the data by a linear combination of only a few nearby basis concepts. But its locality constraint does not well reveal the intrinsic data structure since it only requires the concept to be as close to the original data points as possible. To address these problems, by considering the manifold geometrical structure in local concept factorization via graph-based learning, we propose a novel algorithm, called graph-regularized local coordinate concept factorization (GRLCF). By constructing a parameter-free graph using constrained Laplacian rank (CLR) algorithm, we also present an extension of GRLCF algorithm as $$\hbox {GRLCF}_{\mathrm{CLR}}$$ GRLCF CLR . Moreover, we develop the iterative updating optimization schemes, and provide the convergence proof of our optimization scheme. Since GRLCF simultaneously considers the geometric structures of the data manifold and the locality conditions as additional constraints, it can obtain more compact and better structured data representation. Experimental results on ORL, Yale and Mnist image datasets demonstrate the effectiveness of our proposed algorithm.	algorithm;data (computing);data point;data structure;iterative method;laplacian matrix;locality of reference;mathematical optimization;non-negative matrix factorization;return loss	Jun Ye;Zhong Jin	2017	Neural Processing Letters	10.1007/s11063-017-9598-2	dixon's factorization method;mathematical optimization;combinatorics;discrete mathematics;quadratic sieve;machine learning;mathematics	AI	26.6556459534545	-40.64327900751402	110111
e566b620483e328c600fcb43a302d888fc44c743	route separation strategies for human movement datasets	cybernetics;human movement;pattern;pattern algorithm gps trajectory;optimal method;algorithm;mobile environment;gps;trajectory;shape;global positioning system;handheld device;trajectory shape global positioning system cybernetics videos character recognition;character recognition;videos	Learning patterns of human movement is a complex and hard task, including several computationally expensive algorithms. This issue has even higher emphasis in mobile environment, since handheld devices contain significantly less memory and computing power than a usual PC does. In this paper we are going to compare novel, mobile-optimized methods for separating trajectories in a human routine recognition framework.	algorithm;analysis of algorithms;mobile device	Marcell Fehér;Krisztian Fekete;Kristóf Csorba;Bertalan Forstner	2012	2012 IEEE 19th International Conference and Workshops on Engineering of Computer-Based Systems	10.1109/ECBS.2012.35	computer vision;simulation;global positioning system;cybernetics;computer science	Robotics	37.363881023936614	-43.882023631679836	110332
8aa9606b99699d161e4e0d55c6a6eba36ee0e6be	a novel system for content based retrieval of remote sensing images based on bag of spectral values		This paper presents a novel system for the content based retrieval in remote sensing images based on the Bag of Spectral Values. In the proposed method, the spatial and spectral information contents of RS images are treated separately by parallel pipelines. Defining a novel spectral descriptor, the Bag of Spectral Values, this approach allows for a computationally efficient extraction of features from multiband RS images. The conjunction of the features extracted from the two pipelines are then passed onto the retrieval system that computes the likelihood of any image to contain a given label based on sparse reconstruction performances of sample images. This label based approach allows the retrieval method to be suitable for single label archives as well as multi-labeled ones. Retrieved images are chosen such that their label sets are most likely to be a close match to that of the query image. Experimental results demonstrate the effectiveness of the proposed spectral description method as well the success of the retrieval system in the case of a small number training samples.	algorithmic efficiency;archive;performance;pipeline (computing);sparse matrix;stellar classification	Osman Emre Dai;Begüm Demir;Bülent Sankur;Lorenzo Bruzzone	2017	2017 25th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2017.7960703	iterative reconstruction;artificial intelligence;computer vision;computer science;pattern recognition;visual word;feature extraction;remote sensing;small number;histogram	Vision	31.50074115724617	-45.574264056133806	110361
bb827b7e998f3f7ee944424ad783523025b41abd	support vector motion clustering	crowd analysis unsupervised motion clustering quasiconformal kernel transformation on line clustering performance evaluation;kernel static var compensators shape support vector machines clustering methods performance evaluation indexes;journal article	We present a closed-loop unsupervised clustering method for motion vectors extracted from highly dynamic video scenes. Motion vectors are assigned to nonconvex homogeneous clusters characterizing direction, size and shape of regions with multiple independent activities. The proposed method is based on support vector clustering. Cluster labels are propagated over time via incremental learning. The proposed method uses a kernel function that maps the input motion vectors into a high-dimensional space to produce nonconvex clusters. We improve the mapping effectiveness by quantifying feature similarities via a blend of position and orientation affinities. We use the Quasiconformal Kernel Transformation to boost the discrimination of outliers. The temporal propagation of the clusters’ identities is achieved via incremental learning based on the concept of feature obsolescence to deal with appearing and disappearing features. Moreover, we design an online clustering performance prediction algorithm used as a feedback that refines the cluster model at each frame in an unsupervised manner. We evaluate the proposed method on synthetic data sets and real-world crowded videos and show that our solution outperforms state-of-the-art approaches.	algorithm;cluster analysis;map;performance prediction;software propagation;synthetic data	Isah Abdullahi Lawal;Fabio Poiesi;Davide Anguita;Andrea Cavallaro	2017	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2016.2580401	correlation clustering;simulation;telecommunications;electrical engineering;artificial intelligence;machine learning;data mining;cluster analysis;algorithm	Vision	28.155138454786613	-45.776211277473635	110402
30e2e85b7cf2e85f8965ec74b03f569caa2b5cdd	invariant scattering convolution networks	convolution networks;support vector machines;gaussian processes;scattering convolution fourier transforms wavelet coefficients computer architecture;convolution;image classification;generative pca classifier invariant scattering convolution networks wavelet scattering network translation invariant image representation deformations high frequency information wavelet transform convolutions nonlinear modulus averaging operators network layer sift type descriptors complementary invariant information mathematical analysis deep convolution networks scattering representation stationary process fourier power spectrum state of the art classification handwritten digits texture discrimination gaussian kernel svm;classification;deformations;wavelet transforms convolution gaussian processes handwritten character recognition image classification image representation image texture principal component analysis support vector machines;image texture;invariants;wavelet transforms;image representation;principal component analysis;wavelets classification convolution networks deformations invariants;wavelets;handwritten character recognition	A wavelet scattering network computes a translation invariant image representation which is stable to deformations and preserves high-frequency information for classification. It cascades wavelet transform convolutions with nonlinear modulus and averaging operators. The first network layer outputs SIFT-type descriptors, whereas the next layers provide complementary invariant information that improves classification. The mathematical analysis of wavelet scattering networks explains important properties of deep convolution networks for classification. A scattering representation of stationary processes incorporates higher order moments and can thus discriminate textures having the same Fourier power spectrum. State-of-the-art classification results are obtained for handwritten digits and texture discrimination, with a Gaussian kernel SVM and a generative PCA classifier.	apricot kernel oil;convolution;digit structure;mathematics;modulus of continuity;musculoskeletal diseases;nonlinear system;normal statistical distribution;principal component analysis;scale-invariant feature transform;spectral density;stationary process;statistical classification;wavelet transform;anatomical layer	Joan Bruna;Stéphane Mallat	2013	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2012.230	image texture;wavelet;support vector machine;computer vision;contextual image classification;biological classification;machine learning;invariant;pattern recognition;gaussian process;mathematics;convolution;statistics;wavelet transform;principal component analysis	Vision	31.12472958315736	-39.09966869393805	110679
c99d04d38b50b71d08e2286524a589641bff777b	co-segmentation of 3d shapes via multi-view spectral clustering	co segmentation;multi view clustering;low rankness;sparsity	Co-segmentation of 3D shapes in the same category is an intensive topic in computer graphics. In this paper, we present an unsupervised method to segment a set of meshes into corresponding parts in a consistent manner. Given the over-segmented patches as input, the co-segmentation result is generated by grouping them. In contrast to the previous method, we formulate the problem as a multi-view spectral clustering task by co-training a set of affinity matrices derived from different shape descriptors. For each shape descriptor, the affinity matrix is constructed via combining low-rankness and sparse representation. The integration of multiple features makes our method tolerate the large geometry and topology variations among the 3D meshes in a set. Moreover, the low-rank and sparse representation can capture not only the global structure but also the local relationship, which demonstrate robust to outliers. The experimental results show that our approach successfully segments each category in the benchmark dataset into corresponding parts and generates more reliable results compared with the state-of-the-art.	3d computer graphics;affinity analysis;algorithm;algorithmic efficiency;benchmark (computing);cluster analysis;clustering high-dimensional data;co-training;experiment;free viewpoint television;mathematical optimization;patch (computing);processor affinity;sparse approximation;sparse matrix;spectral clustering;unsupervised learning	Pei Luo;Zhuangzhi Wu;Chunhe Xia;Lu Feng;Teng Ma	2013	The Visual Computer	10.1007/s00371-013-0824-2	mathematical optimization;machine learning;pattern recognition;mathematics;sparsity-of-effects principle;statistics	Vision	27.878863751623314	-46.01950298966114	110817
a633bcb2baf0c9cb0800db1dba028fd553385395	mobile banknote recognition: topological models in scene understanding	mobile device;real time;video processing;assistive device;visual features;visual impairment;real time video processing;scene understanding;banknote recognition	Portable assistive devices are getting more and more widespread. Since most cell phones are nowadays equipped with cameras they can provide an easy way for blind and visually impaired people to obtain visual information about their environment. In this paper we present a banknote recognition system for our previously proposed mobile device, called the bionic eyeglass. We employ a recognition scheme based on the structural topology of banknotes that enables that the visual features detected are not joined in an ensemble classifier independently, but they are coupled via an excitation-inhibition model. We have shown that the technique outperforms classical ensemble classifiers.	algorithm;assistive technology;computational complexity theory;ensemble kalman filter;ensemble learning;mobile device;mobile phone;reference implementation	Mihály Radványi;Zóra Solymár;Attila Stubendek;Kristóf Karacs	2011		10.1145/2093698.2093883	computer vision;simulation;engineering;communication	HCI	32.94292449235865	-51.73125292394671	110845
9ffd5de6f4ab1076885d112810e2da65e081ac7b	context-aware activity recognition and anomaly detection in video	video signal processing convex programming image motion analysis;image motion analysis;video signal processing;convex programming;structural model context aware activity recognition context aware anomaly detection;wide area scene context aware activity recognition anomaly detection motion information context information spatial distribution temporal distribution database training data motion pattern context pattern learning process learned model unconstrained convex optimization problem generated labels virat ground dataset;context vehicles context modeling joints feature extraction vectors testing	In this paper, we propose a mathematical framework to jointly model related activities with both motion and context information for activity recognition and anomaly detection. This is motivated from observations that activities related in space and time rarely occur independently and can serve as context for each other. The spatial and temporal distribution of different activities provides useful cues for the understanding of these activities. We denote the activities occurring with high frequencies in the database as normal activities. Given training data which contains labeled normal activities, our model aims to automatically capture frequent motion and context patterns for each activity class, as well as each pair of classes, from sets of predefined patterns during the learning process. Then, the learned model is used to generate globally optimum labels for activities in the testing videos. We show how to learn the model parameters via an unconstrained convex optimization problem and how to predict the correct labels for a testing instance consisting of multiple activities. The learned model and generated labels are used to detect anomalies whose motion and context patterns deviate from the learned patterns. We show promising results on the VIRAT Ground Dataset that demonstrates the benefit of joint modeling and recognition of activities in a wide-area scene and the effectiveness of the proposed method in anomaly detection.	activity recognition;anomaly detection;context switch;convex optimization;experiment;greedy algorithm;interaction;mathematical optimization;optimization problem;temporal logic;virat	Yingying Zhu;Nandita M. Nayak;Amit K. Roy-Chowdhury	2013	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2012.2234722	computer vision;convex optimization;computer science;machine learning;pattern recognition;mathematics	Vision	36.883338094032	-45.71486386434315	110890
69e4bcb9d7103165176112c675120f88b2ee7a13	learning to recognise unseen classes by a few similes		Existing image classification systems often suffer from re-training models for novel unseen classes. Zero-shot learning (ZSL) aims to recognise these unseen classes directly using trained models with a further inference procedure. However, existing approaches highly rely on human-defined class-attribute associations to achieve the inference, which significantly increases the annotation cost. This paper aims to address ZSL on non-attribute tasks, i.e. only training images with labels are used as most of the supervised settings. Our main contributions are: 1) to circumvent expensive attributes, we propose to use semantic similes that directly indicate the unseen-to-seen associations; 2) a novel similarity-based representation is proposed to represent both visual images and semantic similes in a unified embedding space; 3) in order to reduce the annotation cost, we use only a few similes to infer a class-level prototype for each unseen class. On two popular benchmarks, AwA and aPY, extensive experiments manifest that our method can significantly improve the state-of-the-art results using only two similes for each unseen class. Furthermore, we revisit the Caltech 101 dataset without attributes. Our ZSL results can exceed that of previous supervised methods.	benchmark (computing);caltech 101;computer vision;experiment;html attribute;kriging;prototype;supervised learning	Yang Long;Ling Shao	2017		10.1145/3123266.3123323	caltech 101;computer vision;artificial intelligence;inference;computer science;machine learning;contextual image classification;annotation	AI	25.334491025700228	-47.62647518431256	110950
31ace8c9d0e4550a233b904a0e2aabefcc90b0e3	learning deep face representation		Face representation is a crucial step of face recognition systems. An optimal face representation should be discriminative, robust, compact, and very easyto-implement. While numerous hand-crafted and learning-based representations have been proposed, considerable room for improvement is still present. In this paper, we present a very easy-to-implement deep learning framework for face representation. Our method bases on a new structure of deep network (called Pyramid CNN). The proposed Pyramid CNN adopts a greedy-filter-and-down-sample operation, which enables the training procedure to be very fast and computationefficient. In addition, the structure of Pyramid CNN can naturally incorporate feature sharing across multi-scale face representations, increasing the discriminative ability of resulting representation. Our basic network is capable of achieving high recognition accuracy (85.8% on LFW benchmark) with only 8 dimension representation. When extended to feature-sharing Pyramid CNN, our system achieves the state-of-the-art performance (97.3%) on LFW benchmark. We also introduce a new benchmark of realistic face images on social network and validate our proposed representation has a good ability of generalization.	benchmark (computing);deep learning;discriminative model;facial recognition system;greedy algorithm;robustness (computer science);social network	Haoqiang Fan;Zhimin Cao;Yuning Jiang;Qi Yin;Chinchilla Doudou	2014	CoRR		computer vision;computer science;machine learning;pattern recognition	Vision	25.69704839636956	-50.420994687172694	110973
685fcf13c5e261bf4851ddd1273e048869124ac2	joint label-interaction learning for human action recognition		Human interactions and their action categories preserve strong correlations, and the identification of the interaction configuration is of significant importance to improve the action recognition result. However, interactions are typically estimated using heuristics or treated as latent variables. The former usually produces incorrect interaction configuration while the latter introduces challenging training problem. Hence we propose a framework to jointly learn interactions and actions by designing a potential function using both features learned via deep neural networks and human interaction context. We propose an iterative approach to solve the associated inference problem efficiently and approximately. Experimental results on real datasets demonstrate that the proposed approach outperforms baselines by a large margin, and is competitive compared with the state-of-the-arts.	artificial neural network;baseline (configuration management);deep learning;heuristic (computer science);interaction;iterative method;latent variable	Jiali Jin;Zhenhua Wang;Sheng Liu;Jianhua Zhang;Shengyong Chen;Qiu Guan	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8297135	support vector machine;baseline (configuration management);artificial neural network;artificial intelligence;feature extraction;pattern recognition;inference;computer science;heuristics;latent variable	Vision	27.9362187359085	-48.09019200302184	111364
a70b25da6934526a549cbc3acb05db844fe61278	event detection on roads using perceptual video summarization		Roads are the vital mode of transportation for people and goods around the globe and its use has grown dramatically over the years. There is one death every four minutes due to road accidents in the developing nations. This is of deep concern to the entire humanity. Road accident detection and vehicle behavior analysis is of great interest to the research community in intelligent transportation systems. It is very difficult from the state of the art techniques to provide the abstract form of salient parts of accidents from road surveillance videos. To resolve these issues, we present perceptual video summarization techniques to enrich the speed of visualizing the accident content from a stack of videos. The problem of vehicle analysis is formulated as an optimization problem. To the best of our knowledge, this is the first time we solve an accident detection as an optimization problem and filter the frames to be selected, through a single formulation. With the camera in a surrounding infrastructure and capturing a video, we exploited the properties of sub modularity to provide a relevant and condensed key frame summary. We have studied it for various real world traffic surveillance videos comprising of vehicular accidents and thus making it a promising approach.	automatic summarization;key frame;mathematical optimization;optimization problem	Sinnu Susan Thomas;Sumana Gupta;Venkatesh Subramanian	2018	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2017.2769719	simulation;automatic summarization;salient;perception;key frame;engineering;intelligent transportation system;optimization problem	Vision	38.63770736580942	-45.756506072562644	111413
333f083671da1010cbb080c6ce607ed87c19d73d	multiple instance learning convolutional neural networks for object recognition	object recognition;neural networks;training;prediction algorithms;machine learning;optimization;benchmark testing	Convolutional Neural Networks (CNN) have demonstrated its successful applications in computer vision, speech recognition, and natural language processing. For object recognition, CNNs might be limited by its strict label requirement and an implicit assumption that images are supposed to be target-object-dominated for optimal solutions. However, the labeling procedure, necessitating laying out the locations of target objects, is very tedious, making high-quality large-scale dataset prohibitively expensive. Data augmentation schemes are widely used when deep networks suffer the insufficient training data problem. All the images produced through data augmentation share the same label, which may be problematic since not all data augmentation methods are label-preserving. In this paper, we propose a weakly supervised CNN framework named Multiple Instance Learning Convolutional Neural Networks (MILCNN) to solve this problem. We apply MILCNN framework to object recognition and report state-of-the-art performance on three benchmark datasets: CIFAR10, CIFAR100 and ILSVRC2015 classification dataset.	benchmark (computing);computer vision;convolutional neural network;deep learning;image resolution;multiple instance learning;natural language processing;neural networks;neural network software;outline of object recognition;speech recognition	Miao Sun;Tony X. Han;Ming-Chang Liu;Ahmad Khodayari-Rostamabad	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900139	benchmark;computer vision;prediction;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;deep learning;artificial neural network	Vision	27.517542324454865	-49.830086105184975	111557
ec9b5d19aa441742b7a4177fe08f0acfbe1e5597	image registration based on patch matching using a novel convolutional descriptor		In this paper we introduce a novel feature descriptor based on deep learning that trains a model to match the patches of images on scenes captured under different viewpoints and lighting conditions. The patch matching of images capturing the same scene in varied circumstances and diverse manners is challenging. Our approach is influenced by recent success of CNNs in classification tasks. We develop a model which maps the raw image patch to a low dimensional feature vector. As our experiments show, the proposed approach is much better than state-of-the-art descriptors and can be considered as a direct replacement of SURF. The results confirm that these techniques further improve the performance of the proposed descriptor. Then we propose an improved Random Sample Consensus algorithm for removing false matching points. Finally, we show that our neural network based image descriptor for image patch matching outperforms state-of-the-art methods on a number of benchmark datasets and can be used for image registration with high quality.	image registration	Wang Xie;Hongxia Gao;Zhanhong Chen	2018		10.1007/978-3-030-03335-4_25	feature vector;deep learning;artificial neural network;image registration;computer science;artificial intelligence;pattern recognition	Vision	30.896503519038912	-51.54354983433468	111564
d50ec545a8fbb96089c919d862f19d57e28984c3	condition directed multi-domain adversarial learning for loop closure detection		Loop closure detection (LCD) is the key module in appearance based simultaneously localization and mapping (SLAM). However, in the real life, the appearance of visual inputs are usually affected by the illumination changes and texture changes under different weather conditions. Traditional methods in LCD usually rely on handcraft features, however, such methods are unable to capture the common descriptions under different weather conditions, such as rainy, foggy and sunny. Furthermore, traditional handcraft features could not capture the highly level understanding for the local scenes. In this paper, we proposed a novel condition directed multi-domain adversarial learning method, where we use the weather condition as the direction for feature inference. Based on the generative adversarial networks (GANs) and a classification networks, the proposed method could extract the high-level weather-invariant features directly from the raw data. The only labels required here are the weather condition of each visual input. Experiments are conducted in the GTAV game simulator, which could generated lifelike outdoor scenes under different weather conditions. The performance of LCD results shows that our method outperforms the state-of-arts significantly. Keywords—Loop Closure Detection; Generative Adversarial Networks; Multi-domain adversarial learning	experiment;for loop;generative adversarial networks;high- and low-level;real life;simulation	Peng Yin;Yuqing He;Na Liu;Jianda Han	2017	CoRR		raw data;simulation;engineering;for loop;adversarial system;inference	AI	27.02987232669072	-50.511666054303355	111713
fe6e95523f8be042a2d311cb848a7ba9d9414d10	visual saliency based active learning for prostate mri segmentation		We propose an active learning (AL) approach for prostate segmentation from magnetic resonance (MR) images. Our label query strategy is inspired from the principles of visual saliency that has similar considerations for choosing the most salient region. These similarities are encoded in a graph using classification maps and low level features. Random walks identify the most informative node which is equivalent to the label query sample in AL. Experimental results on the MICCAI 2012 Prostate segmentation challenge show the superior performance of our approach to conventional methods using fully supervised learning.	active learning (machine learning);graph (discrete mathematics);information;map;resonance;supervised learning	Dwarikanath Mahapatra;Joachim M. Buhmann	2015		10.1007/978-3-319-24888-2_2	computer vision;computer science;machine learning;pattern recognition	Vision	27.13101555560723	-46.417289672811336	111879
5ed4677092adfce61857d282d72df3cdfd41880a	a brief summary of dictionary learning based approach for classification (revised)		This note presents some representative methods which are based on dictionary learning (DL) for classification. We do not review the sophisticated methods or frameworks that involve DL for classification, such as online DL and spatial pyramid matching (SPM), but rather, we concentrate on the direct DL-based classification methods. Here, the “so-called direct DL-based method” is the approach directly deals with DL framework by adding some meaningful penalty terms. By listing some representative methods, we can roughly divide them into two categories, i.e. (1) directly making the dictionary discriminative and (2) forcing the sparse coefficients discriminative to push the discrimination power of the dictionary. From this taxonomy, we can expect some extensions of them as future researches.	coefficient;data dictionary;expect;linear discriminant analysis;machine learning;sparse matrix;super paper mario	Shu Kong;Donghui Wang	2012	CoRR			ML	26.246143591224524	-44.56955523254103	112149
07217f987f5db4802e8d4b18fe5f60374092c56b	a machine learning system for human-in-the-loop video surveillance	video surveillance;video retrieval machine learning system human in the loop video surveillance system human operator eye gaze position tracking automatic interesting object detection synthetic video;video retrieval;video surveillance learning artificial intelligence object detection object tracking video retrieval;object tracking;learning artificial intelligence;surveillance humans feature extraction streaming media training cameras;object detection	We propose a novel human-in-the-loop surveillance system that continuously learns the properties of objects that are interesting for a human operator. The interesting objects are automatically learned by tracking the eye gaze positions of the operator while he or she monitors the surveillance video. The system automatically detects interesting objects in the surveillance video and forms a new synthetic video that contains interesting objects at earlier positions in the time dimension. The operator always views this synthetically formed video which makes manual video retrieval tasks more convenient. Sensitivity to operator interests and interest changes are other major advantages. We tested our system both on synthetic and real videos, which are provided as supplementary materials [1]. The results show the effectiveness of the proposed system.	binary classification;closed-circuit television;computer vision;digital video;experiment;human–computer interaction;machine learning;online and offline;robustness (computer science);streaming media;synthetic intelligence;the times	Ulas Vural;Yusuf Sinan Akgül	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		computer vision;simulation;computer science;video tracking;multimedia	Vision	38.551185183909304	-44.93010561485168	112187
4f8acabfa681255822520a1272ba6cf553281753	global versus local methods in nonlinear dimensionality reduction	conformal map;nonlinear dimensionality reduction;local linear embedding	Recently proposed algorithms for nonlinear dimensionality reduction fall broadly into two categories which have different advantages and disadvantages: global (Isomap [1]), and local (Locally Linear Embedding [2], Laplacian Eigenmaps [3]). We present two variants of Isomap which combine the advantages of the global approach with what have previously been exclusive advantages of local methods: computational sparsity and the ability to invert conformal maps.	algorithm;computation;isomap;nonlinear dimensionality reduction;nonlinear system;provable security;sparse matrix	Vin de Silva;Joshua B. Tenenbaum	2002			conformal map;mathematical optimization;topology;computer science;machine learning;mathematics;nonlinear dimensionality reduction;dimensionality reduction	ML	25.409702461497133	-41.48753190663683	112431
4cf28fc05744d9cfd3206b6b6e8a81c823e740f6	spectral–spatial feature extraction for hyperspectral image classification: a dimension reduction and deep learning approach	training;image classification feature extraction hyperspectral imaging;fusion feature spectral spatial feature extraction hyperspectral image classification deep learning approach dimension reduction approach spectral spatial feature based classification framework ssfc framework deep learning techniques balanced local discriminant embedding algorithm high dimensional hyperspectral data sets convolutional neural network;machine learning;feature extraction;feature extraction balanced local discriminant embedding blde convolutional neural network cnn deep learning dl dimension reduction dr;hyperspectral imaging;algorithm design and analysis;feature extraction training hyperspectral imaging machine learning algorithm design and analysis	In this paper, we propose a spectral-spatial feature based classification (SSFC) framework that jointly uses dimension reduction and deep learning techniques for spectral and spatial feature extraction, respectively. In this framework, a balanced local discriminant embedding algorithm is proposed for spectral feature extraction from high-dimensional hyperspectral data sets. In the meantime, convolutional neural network is utilized to automatically find spatial-related features at high levels. Then, the fusion feature is extracted by stacking spectral and spatial features together. Finally, the multiple-feature-based classifier is trained for image classification. Experimental results on well-known hyperspectral data sets show that the proposed SSFC method outperforms other commonly used methods for hyperspectral image classification.	algorithm;artificial neural network;computer vision;convolutional neural network;deep learning;dimensionality reduction;discriminant;feature extraction;focus stacking;high- and low-level;horizontal situation indicator;lr parser;technological singularity	Wenzhi Zhao;Shihong Du	2016	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2016.2543748	algorithm design;feature learning;computer vision;feature extraction;computer science;hyperspectral imaging;machine learning;pattern recognition;k-nearest neighbors algorithm;feature;dimensionality reduction	ML	29.940942517610566	-44.13233869897784	112597
c900f90f7883ca0e2af31197f0368fe324d2a644	visual tracking via adaptive multi-task feature learning with calibration and identification	object tracking;feature selection;sparse representation;multi task feature learning	Recently multi-task feature learning has become a widely applied approach for visual tracking, since it is benefited from the shared features across tasks. However, selecting features appropriately from multiple tasks is still a challenging problem due to the complex variation of the appearance of moving objects, which influences not only the features of single task but also the relationships between the features of multiple tasks. To address this problem, this paper presents a novel sparse learning model for selecting multi-task features adaptively. Compared to the existing multi-task models, the proposed model is capable of both calibrating the loss function according to the noise level of a task to keep its specific features, and identifying the relevant and irrelevant (outlier) tasks simultaneously by decomposing the regularized matrix into two specified structures. The proposed model allows to preserve specific features of individual tasks via calibration and to exploit sparse pattern over the relevant task via identification. Empirical evaluations demonstrate that the proposed method has better performance than a number of the state-of-the-art trackers on available public image sequences.	computer multitasking;feature learning	Pengguang Chen;Xingming Zhang;Aihua Mao;Jianbin Xiong	2016	Sig. Proc.: Image Comm.	10.1016/j.image.2016.09.009	multi-task learning;computer vision;computer science;machine learning;video tracking;pattern recognition;sparse approximation;feature selection;feature	Vision	32.67938872867305	-47.35030372169084	112609
163b13cd61d94b19902d1dede69dbb711c025a3c	supervised feature extraction of hyperspectral images using partitioned maximum margin criterion	iron;data mining;image color analysis;feature extraction;correlation;hyperspectral imaging	Dimensionality reduction is an important task where the aim is to reduce the number of features and make the system less time consuming for classification. Here, the drawbacks of Fisher's linear-discriminant-analysis-based feature extraction (FE) methods are addressed and a proposal is made to overcome it as well as to reduce the Hughes phenomenon and computational complexity of the system. The proposed FE technique initially partitions the complete set of features into several highly correlated subgroups. Then a linear transformation is performed using a maximal margin criterion over each subgroup. The proposed method is supervised in nature, because prior information about the class label of data is required to calculate the maximum margin criterion based on interclass and intraclass scatter matrices. Experiments are conducted with the PaviaU and Indian pine data sets, and the results are compared with five state-of-the-art techniques, both qualitatively and quantitatively, to demonstrate the effectiveness of the proposed method.	computational complexity theory;dimensionality reduction;eisenstein's criterion;feature extraction;linear discriminant analysis;maximal set;memory management controller;real-time clock;supervised learning	Aloke Datta;Susmita Ghosh;Ashish Ghosh	2017	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2016.2628078	feature extraction;computer science;hyperspectral imaging;machine learning;pattern recognition;data mining;mathematics;iron;correlation;statistics;remote sensing	AI	29.85050776029821	-43.719232258598495	112701
22fb1ccc75e4429061ec46e90a4ee7b7490f0a03	hidden markov models as a process monitor in robotic assembly	event recognition;knowledge based system;hidden markov model;discrete event dynamic system;estimation algorithm;baum welch;process monitoring;discrete event	A process monitor for robotic assembly based on hidden Markov models (HMMs) is presented. The HMM process monitor is based on the dynamic force/torque signals arising from interaction between the workpiece and the environment. The HMMs represent a stochastic, knowledge-based system in which the models are trained off-line with the Baum-Welch reestimation algorithm. The assembly task is modeled as a discrete event dynamic system in which a discrete event is defined as a change in contact state between the workpiece and the environment. Our method (1) allows for dynamic motions of the workpiece, (2) accounts for sensor noise and friction, and (3) exploits the fact that the amount of force information is large when there is a sudden change of discrete state in robotic assembly. After the HMMs have been trained, the authors use them on-line in a 2D experimental setup to recognize discrete events as they occur. Successful event recognition with an accuracy as high as 97% was achieved in 0.5-0.6 s with a training set size of only 20 examples for each discrete event.	baum–welch algorithm;dynamical system;hidden markov model;image noise;knowledge-based systems;line level;markov chain;online and offline;robot;stochastic process;test set;welch's method	Geir Hovland;Brenan J. McCarragher	1998	I. J. Robotics Res.	10.1177/027836499801700204	forward algorithm;discrete event dynamic system;computer science;baum–welch algorithm;artificial intelligence;machine learning;pattern recognition;markov process;markov model;hidden markov model;statistics	Robotics	38.31145378069577	-41.58918494632974	112754
82f352ebf904dc1670699390c2693bf7bd993361	global feature learning with human body region guided for person re-identification		Person reidentification (re-id) is a very challenging task in video surveillance due to background clutters, variations in occlusion, and the human body misalignment in the detected images. To tackle these problems, we utilize a multi-channel convolutional neural network (CNN) with a novel embedding training strategy. First, some parts of the body were detected with existing methods of human pose estimation and then different parts were feed into different network branches to learn local and global representations. But for the global network branch, we proposed a embedding strategy for training, which uses local features to guide learning more robust global features. The promising experimental results on the large-scale Market-1501 and CUHK03 datasets demonstrate the effectiveness of our proposed embedding training strategy for features.	feature learning	Zhiqiang Li;Nong Sang;Kezhou Chen;Chuchu Han;Changxin Gao	2018		10.1007/978-3-030-03398-9_2	convolutional neural network;global network;pose;feature learning;embedding;pattern recognition;computer science;artificial intelligence	Vision	30.324151676626556	-51.005958515218374	112900
b31bc46ad690294450464e1d8b2041fa9dd823a9	srn: side-output residual network for object symmetry detection in the wild		In this paper, we establish a baseline for object symmetry detection in complex backgrounds by presenting a new benchmark and an end-to-end deep learning approach, opening up a promising direction for symmetry detection in the wild. The new benchmark, named Sym-PASCAL, spans challenges including object diversity, multi-objects, part-invisibility, and various complex backgrounds that are far beyond those in existing datasets. The proposed symmetry detection approach, named Side-output Residual Network (SRN), leverages output Residual Units (RUs) to fit the errors between the object symmetry ground-truth and the outputs of RUs. By stacking RUs in a deep-to-shallow manner, SRN exploits the flow of errors among multiple scales to ease the problems of fitting complex outputs with limited layers, suppressing the complex backgrounds, and effectively matching object symmetry of different scales. Experimental results validate both the benchmark and its challenging aspects related to real-world images, and the state-of-the-art performance of our symmetry detection approach. The benchmark and the code for SRN are publicly available at https://github.com/KevinKecc/SRN.	academy;baseline (configuration management);benchmark (computing);computer vision;deep learning;end-to-end encryption;end-to-end principle;flow network;ground truth;recurrent neural network;stacking;touchstone file	Wei Ke;Jie Chen;Jianbin Jiao;Guoying Zhao;Qixiang Ye	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.40	pattern recognition;skeleton (computer programming);residual;machine learning;artificial intelligence;computer vision;deep learning;computer science;benchmark (computing);image segmentation;exploit	Vision	26.23604144609971	-51.49131341938289	113298
4173728d315e04455668d2694a99caf645cd3443	scene detection using visual and audio attention	audio attention;video analysis;basic step;scene segmentation;automatic scene detection;audio feature;scene boundary;audio stream;visual pattern;audio analysis;new method;tv;information retrieval	Shot and scene segmentation are basic steps for a variety of applications in video analysis and processing. In this paper, we propose a new method for automatic scene detection which takes visual patterns of movies and audio features into account. In particular, we also show that the use of audio analysis, to detect transitions in an audio stream, is suitable in order to capture the scene boundaries as well.	scene graph;streaming media;video content analysis	Angelo Chianese;Vincenzo Moscato;Antonio Penta;Antonio Picariello	2008			computer vision;audio mining;computer science;multimedia;information retrieval	Vision	39.07853244568941	-51.87316432372112	113694
fd5af54bcce66f0982a118748e423f13dbe2cb63	large scale indefinite kernel fisher discriminant		Indefinite similarity measures can be frequently found in bio-informatics by means of alignment scores. Lacking an underlying vector space, the data are given as pairwise similarities only. Indefinite Kernel Fisher Discriminant (iKFD) is a very effective classifier for this type of data but has cubic complexity and does not scale to larger problems. Here we propose an extension of iKFD such that linear runtime and memory complexity is achieved for low rank indefinite kernels. Evaluation at several larger similarity data from various domains shows that the proposed method provides similar generalization capabilities while being substantially faster for large scale data.	discriminant;fisher information	Frank-Michael Schleif;Andrej Gisbrecht;Peter Tiño	2015		10.1007/978-3-319-24261-3_13	kernel fisher discriminant analysis;linear discriminant analysis;fisher kernel	Vision	24.78657109594956	-41.23704038180212	113809
98256bbb39b0cea91a29aa405a51553ab111563b	semisupervised hyperspectral image classification based on affinity scoring	training hyperspectral imaging accuracy image segmentation yttrium indexes;image segmentation;training;accuracy;indexes;yttrium;hsi semisupervised hyperspectral image classification map internal class variability affinity scoring fuzzy state spectral features spatial features local class consistency spectral variability spatial smoothness;iterative methods fuzzy set theory geophysical image processing hyperspectral imaging image classification image segmentation;hyperspectral imaging;affinity scoring semisupervised classification segmentation	There are two great challenges for classification of hyperspectral images (HSIs): lack in prior knowledge and serious internal-class variability. To address the issues, we propose a novel semisupervised method based on affinity scoring (AS). It can harness the fuzzy state of the contributions of spectral and spatial features to classification. The method consists of three major steps: over-segmentation, semisupervised classification and modification. First, superpixels are generated to maintain local class consistency, which can balance spectral variability. Then unlabeled samples are classified by AS in an iterative manner, whereas precious labeled samples are made most use of. Finally, AS is adopted again to refine the classification map, which further exploits spatial smoothness in HSIs. Experiments show that the proposed method can largely outperform several state-of-the-art classifiers.	affinity analysis;computer vision;experiment;heart rate variability;inner class;iterative method;scoring functions for docking;semi-supervised learning;spatial variability	Zhao Chen;Bin Wang;Yubin Niu;Wei Xia;Jian Qiu Zhang;Bo Hu	2015	2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2015.7326947	database index;computer vision;computer science;hyperspectral imaging;yttrium;machine learning;pattern recognition;mathematics;accuracy and precision;image segmentation;remote sensing	Vision	30.570425555542794	-43.77372855350731	113891
49ca83ef6277581ac2b8bf7c121cf8d1f222992e	generalized multiview analysis: a discriminative latent space	eigenvalues and eigenfunctions;quadratic programming;bismuth;nickel;image classification;correlation methods;multipie face dataset generalized multiview analysis discriminative latent space general multiview feature extraction approach gma cross view classification cross view retrieval eigenvalue based solution unsupervised feature extraction techniques quadratic constrained quadratic program qcqp generalized eigenvalue problem feature spaces canonical correlational analysis cca text image retrieval pascal text image data wiki text image data pose invariant face recognition lighting invariant face recognition;face recognition;feature extraction;lighting feature extraction nickel eigenvalues and eigenfunctions face recognition bismuth face;face;lighting;quadratic programming correlation methods eigenvalues and eigenfunctions face recognition feature extraction image classification image retrieval;image retrieval	This paper presents a general multi-view feature extraction approach that we call Generalized Multiview Analysis or GMA. GMA has all the desirable properties required for cross-view classification and retrieval: it is supervised, it allows generalization to unseen classes, it is multi-view and kernelizable, it affords an efficient eigenvalue based solution and is applicable to any domain. GMA exploits the fact that most popular supervised and unsupervised feature extraction techniques are the solution of a special form of a quadratic constrained quadratic program (QCQP), which can be solved efficiently as a generalized eigenvalue problem. GMA solves a joint, relaxed QCQP over different feature spaces to obtain a single (non)linear subspace. Intuitively, GMA is a supervised extension of Canonical Correlational Analysis (CCA), which is useful for cross-view classification and retrieval. The proposed approach is general and has the potential to replace CCA whenever classification or retrieval is the purpose and label information is available. We outperform previous approaches for textimage retrieval on Pascal and Wiki text-image data. We report state-of-the-art results for pose and lighting invariant face recognition on the MultiPIE face dataset, significantly outperforming other approaches.	content-based image retrieval;facial recognition system;feature extraction;intel gma;krylov subspace;local-density approximation;norton power eraser;partial least squares regression;principal component analysis;quadratic programming;quadratically constrained quadratic program;supervised learning;wiki	Abhishek Sharma;Abhishek Kumar;Hal Daumé;David W. Jacobs	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247923	facial recognition system;face;nickel;computer vision;contextual image classification;feature extraction;image retrieval;computer science;machine learning;bismuth;pattern recognition;lighting;mathematics;quadratic programming	Vision	25.90763413434889	-42.84493455501386	113916
8739ea954b095e5c5009e9371c945bec47bb50dc	structure preserving transfer learning for unsupervised hyperspectral image classification		Recent advances on remote sensing techniques allow easier access to imaging spectrometer data. Manually labeling and processing of such collected hyperspectral images (HSIs) with a vast quantities of samples and a large number of bands is labor and time consuming. To relieve these manual processes, machine learning based HSI processing methods have attracted increasing research attention. A major assumption in many machine learning problems is that the training and testing data are in the same feature space and follow the same distribution. However, this assumption doesn’t always hold true in many real world problems, especially in certain HSI processing problems with extremely insufficient or even without training samples. In this letter, we present a transfer learning framework to address this unsupervised challenge (i.e., without training samples in the target domain), by making the following three main contributions: 1) to the best of our knowledge, this is the first time for transfer learning framework to be used for the classification of totally unknown target HSI data with no training samples; 2) the characteristics of HSI are learned on dual spaces to exploit its structure knowledge to better label HSI samples; and 3) two specific new scenarios suitable for transfer learning are investigated. Experimental results on several real world HSIs support the superiority of the proposed work.	central processing unit;computational complexity theory;data structure;dataspaces;feature vector;horizontal situation indicator;machine learning;markov random field;semantic role labeling;unsupervised learning	Jianzhe Lin;Chen He;Z. Jane Wang;Shuiying Li	2017	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2723763	transfer of learning;remote sensing;computer vision;artificial intelligence;feature vector;data modeling;test data;hyperspectral imaging;exploit;machine learning;markov process;contextual image classification;computer science	AI	29.682264774757375	-44.123202378742754	113920
022bae998e2935c67a1df666f5fd5c577190f80d	learning with structured sparsity	compressed sensing;statistical learning;greedy algorithm	This paper investigates a new learning formulation called structured sparsity, which is a natural extension of the standard sparsity concept in statistical learning and compressive sensing. By allowing arbitrary structures on the feature set, this concept generalizes the group sparsity idea. A general theory is developed for learning with structured sparsity, based on the notion of coding complexity associated with the structure. Moreover, a structured greedy algorithm is proposed to efficiently solve the structured sparsity problem. Experiments demonstrate the advantage of structured sparsity over standard sparsity.	backward chaining;compressed sensing;greedy algorithm;machine learning;sparse matrix	Junzhou Huang;Tong Zhang;Dimitris N. Metaxas	2009	Journal of Machine Learning Research	10.1145/1553374.1553429	mathematical optimization;greedy algorithm;computer science;machine learning;pattern recognition;mathematics;compressed sensing	ML	26.1836969612963	-39.344252197698076	113925
3e30ec6ad7b756180d0e80e4da2c14d21db622b6	spectral and spatial classification of hyperspectral data using svms and morphological profiles	personal communication networks;support vector machines;image classification;urban structure;morphological profiles;testing;data fusion;dual problem;decision boundary feature extraction;pixel wise classification;hyperspectral imaging urban areas spatial resolution pixel concatenated codes feature extraction support vector machines support vector machine classification testing personal communication networks;support vector machine classifier;geomorphology;urban areas;terrain mapping feature extraction geomorphology image classification sensor fusion support vector machines;feature extraction;pixel;urban structures;hyperspectral data;rosis data;urban area;support vector machine classification;svm;principal components;terrain mapping;concatenated codes;support vector machine;spatial classification;sensor fusion;hyperspectral imaging;spectral classification;rosis data spectral classification spatial classification hyperspectral data svm morphological profiles urban areas principal components urban structures pixel wise classification data fusion decision boundary feature extraction support vector machine classifier;high spatial resolution;principal component;spatial resolution	A method is proposed for the classification of urban hyperspectral data with high spatial resolution. The approach is an extension of previous approaches and uses both the spatial and spectral information for classification. One previous approach is based on using several principal components (PCs) from the hyperspectral data and building several morphological profiles (MPs). These profiles can be used all together in one extended MP. A shortcoming of that approach is that it was primarily designed for classification of urban structures and it does not fully utilize the spectral information in the data. Similarly, the commonly used pixelwise classification of hyperspectral data is solely based on the spectral content and lacks information on the structure of the features in the image. The proposed method overcomes these problems and is based on the fusion of the morphological information and the original hyperspectral data, i.e., the two vectors of attributes are concatenated into one feature vector. After a reduction of the dimensionality, the final classification is achieved by using a support vector machine classifier. The proposed approach is tested in experiments on ROSIS data from urban areas. Significant improvements are achieved in terms of accuracies when compared to results obtained for approaches based on the use of MPs based on PCs only and conventional spectral classification. For instance, with one data set, the overall accuracy is increased from 79% to 83% without any feature reduction and to 87% with feature reduction. The proposed approach also shows excellent results with a limited training set.	concatenation;experiment;feature vector;stellar classification;support vector machine;test set	Mathieu Fauvel;Jocelyn Chanussot;Jon Atli Benediktsson;Johannes R. Sveinsson	2007	IEEE Transactions on Geoscience and Remote Sensing	10.1109/IGARSS.2007.4423943	support vector machine;computer vision;computer science;machine learning;pattern recognition;data mining	Vision	31.447521850394075	-43.782243556573945	113963
0d6d767782e64f36f9fc3717e471a170e7e4d776	building virtual anatomic models using java3d	virtual reality;java3d;visualization;visual development;anatomic model;medicine;surgery simulation;medical education	Virtual reality is extensively used in medicine, from medical education and training to surgery simulation and planning and many other areas. Virtual anatomic models are critical components in the medicine virtual reality applications. Java3D has provided a multi-platform scene graph based visualization development package that suitable for building virtual reality applications. This paper discussed the characteristics of virtual anatomic models for medicine virtual reality applications and the features of Java3D, and meanwhile presented our efforts of building a framework based on Java3D that is suitable for producing virtual anatomic models for medicine virtual reality applications.	java 3d;scene graph;simulation;virtual reality	Su Huang;Rafail Baimouratov;Wieslaw Lucjan Nowinski	2004		10.1145/1044588.1044676	simulation;visualization;computer science;virtual reality;multimedia	Visualization	39.04686706475131	-38.737882654139305	114019
9093a9c9275dee14c2d28c9ae2dd67f8cd9a4863	dyadic interaction detection from pose and flow	interaction;computer vision;action recognition;dyadic	We propose a method for detecting dyadic interactions: finegrained, coordinated interactions between two people. Our model is capable of recognizing interactions such as a hand shake or a high five, and locating them in time and space. At the core of our method is a pictorial structures model that additionally takes into account the finegrained movements around the joints of interest during the interaction. Compared to a bag-of-words approach, our method not only allows us to detect the specific type of actions more accurately, but it also provides the specific location of the interaction. The model is trained with both video data and body joint estimates obtained from Kinect. During testing, only video data is required. To demonstrate the efficacy of our approach, we introduce the ShakeFive dataset that consists of videos and Kinect data of hand shake and high five interactions. On this dataset, we obtain a mean average precision of 49.56%, outperforming a bag-ofwords approach by 23.32%. We further demonstrate that the model can be learned from just a few interactions.		Coert Van Gemeren;Robby T. Tan;Ronald Poppe;Remco C. Veltkamp	2014		10.1007/978-3-319-11839-0_9	computer vision;simulation;computer science;communication	Vision	34.2662790838282	-49.3986539016278	114058
7557e81c1189f0ef9643519e0664d60baed51721	robust and efficient graph correspondence transfer for person re-identification		Spatial misalignment caused by variations in poses and viewpoints is one of the most critical issues that hinders the performance improvement in existing person re-identification (Re-ID) algorithms. To address this problem, in this paper, we present a robust and efficient graph correspondence transfer (REGCT) approach for explicit spatial alignment in ReID. Specifically, we propose to establish the patch-wise correspondences of positive training pairs via graph matching. By exploiting both spatial and visual contexts of human appearance in graph matching, meaningful semantic correspondences can be obtained. To circumvent the cumbersome on-line graph matching in testing phase, we propose to transfer the off-line learned patchwise correspondences from the positive training pairs to test pairs. In detail, for each test pair, the training pairs with similar pose-pair configurations are selected as references. The matching patterns (i.e., the correspondences) of the selected references are then utilized to calculate the patch-wise feature distances of this test pair. To enhance the robustness of correspondence transfer, we design a novel pose context descriptor to accurately model human body configurations, and present an approach to measure the similarity between a pair of pose context descriptors. Meanwhile, to improve testing efficiency, we propose a correspondence template ensemble method using the voting mechanism, which significantly reduces the amount of patch-wise matchings involved in distance calculation. With aforementioned strategies, the REGCT model can effectively and efficiently handle the spatial misalignment problem in Re-ID. Extensive experiments on five challenging benchmarks, including VIPeR, Road, PRID450S, 3DPES and CUHK01, evidence the superior performance of REGCT over other state-of-the-art approaches.	algorithm;baseline (configuration management);caller id;experiment;geometric complexity theory;line graph;matching (graph theory);online and offline	Qin Zhou;Heng Fan;Hua Yang;Hang Su;Shibao Zheng;Shuang Wu;Haibin Ling	2018	CoRR		robustness (computer science);matching (graph theory);performance improvement;viewpoints;artificial intelligence;voting;pattern recognition;mathematics;graph	AI	33.61907977055043	-48.64332142313258	114299
49e6636cadff564e47f6fee1063648459d92d847	latent structured models for human pose estimation	structural model;kernel;prediction theory approximation theory image reconstruction image segmentation inference mechanisms learning artificial intelligence pose estimation;image segmentation;hollywood movie latent structured model human pose estimation automatic 3d human pose reconstruction monocular images discriminative formulation latent segmentation inputs figure ground segment hypothesis prediction problem learning inference 3d human articular configuration tractable formulation combined segment selection augmented kernel complex dependency encoding primal linear reformulation fourier kernel approximation nonlinear latent structured prediction methodology humaneva benchmark;kernel image segmentation three dimensional displays humans estimation training joints;training;inference mechanisms;joints;three dimensional;scaling up;approximation theory;prediction theory;estimation;three dimensional displays;image reconstruction;structure prediction;humans;learning artificial intelligence;pose estimation	We present an approach for automatic 3D human pose reconstruction from monocular images, based on a discriminative formulation with latent segmentation inputs. We advanced the field of structured prediction and human pose reconstruction on several fronts. First, by working with a pool of figure-ground segment hypotheses, the prediction problem is formulated in terms of combined learning and inference over segment hypotheses and 3D human articular configurations. Beside constructing tractable formulations for the combined segment selection and pose estimation problem, we propose new augmented kernels that can better encode complex dependencies between output variables. Furthermore, we provide primal linear re-formulations based on Fourier kernel approximations, in order to scale-up the non-linear latent structured prediction methodology. The proposed models are shown to be competitive in the HumanEva benchmark and are also illustrated in a clip collected from a Hollywood movie, where the model can infer human poses from monocular images captured in complex environments.	3d pose estimation;approximation;augmented lagrangian method;benchmark (computing);cobham's thesis;cynbe ru taren;encode;experiment;hollywood;human-based computation;kernel (operating system);latent variable;mathematical model;mathematical optimization;motion capture;nonlinear system;scalability;structured prediction	Catalin Ionescu;Fuxin Li;Cristian Sminchisescu	2011	2011 International Conference on Computer Vision	10.1109/ICCV.2011.6126500	iterative reconstruction;three-dimensional space;computer vision;estimation;kernel;pose;computer science;machine learning;pattern recognition;mathematics;image segmentation;approximation theory	Vision	27.99593370922925	-48.0748743743037	114507
00d9d88bb1bdca35663946a76d807fff3dc1c15f	subjects and their objects: localizing interactees for a person-centric view of importance	human-object interaction;importance;objectness	Understanding images with people often entails understanding their interactions with other objects or people. As such, given a novel image, a vision system ought to infer which other objects/people play an important role in a given person’s activity. However, existing methods are limited to learning action-specific interactions (e.g., how the pose of a tennis player relates to the position of his racquet when serving the ball) for improved recognition, making them unequipped to reason about novel interactions with actions or objects unobserved in the training data. We propose to predict the “interactee” in novel images—that is, to localize the object of a person’s action. Given an arbitrary image with a detected person, the goal is to produce a saliency map indicating the most likely positions and scales where that person’s interactee would be found. To that end, we explore ways to learn the generic, action-independent connections between (a) representations of a person’s pose, gaze, and scene cues and (b) the interactee object’s position and scale. We provide results on a newly collected UT Interactee dataset spanning more than 10,000 images from SUN, PASCAL, and COCO. We show that the proposed interaction-informed saliency metric has practical utility for four tasks: contextual object detection, image retargeting, predicting object importance, and data-driven natural language scene description. All four scenarios reveal the value in linking the subject to its object in order to understand the story of an image.	emoticon;file spanning;interaction;natural language;object detection;retargeting;seam carving;ut-vpn	Chao-Yeh Chen;Kristen Grauman	2016	International Journal of Computer Vision	10.1007/s11263-016-0958-6	computer vision	Vision	33.4966654995271	-49.97797989303679	114729
785a7e01267683deb54a94f6a8b4019806e3f085	video anomaly detection based on local statistical aggregates	video surveillance;probability;video surveillance object detection probability statistical analysis;training;video segments video anomaly detection local statistical aggregates video surveillance applications local spatio temporal signatures time window spatio temporal anomalous region probabilistic framework optimal decision rules characterization nominal behavior global spatial statistical dependency global temporal statistical dependency data driven local empirical rules scores functions local nearest neighbor distances spatio temporal locations spatio temporal scales;training feature extraction hidden markov models vectors markov processes streaming media training data;training data;hidden markov models;statistical analysis;vectors;streaming media;feature extraction;markov processes;object detection	Anomalies in many video surveillance applications have local spatio-temporal signatures, namely, they occur over a small time window or a small spatial region. The distinguishing feature of these scenarios is that outside this spatio-temporal anomalous region, activities appear normal. We develop a probabilistic framework to account for such local spatio-temporal anomalies. We show that our framework admits elegant characterization of optimal decision rules. A key insight of the paper is that if anomalies are local optimal decision rules are local even when the nominal behavior exhibits global spatial and temporal statistical dependencies. This insight helps collapse the large ambient data dimension for detecting local anomalies. Consequently, consistent data-driven local empirical rules with provable performance can be derived with limited training data. Our empirical rules are based on scores functions derived from local nearest neighbor distances. These rules aggregate statistics across spatio-temporal locations & scales, and produce a single composite score for video segments. We demonstrate the efficacy of our scheme on several video surveillance datasets and compare with existing work.	aggregate data;anomaly detection;antivirus software;closed-circuit television;provable security;sensor	Venkatesh Saligrama;Zhu Chen	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247917	training set;feature extraction;computer science;machine learning;pattern recognition;probability;data mining;mathematics;markov process;hidden markov model;statistics	Vision	35.697851848785454	-44.2254778589539	115022
f0dcfebaab5801319d7c615c88d6b30896a9bcdc	appearance changes detection during tracking	semantics;feature extraction;correlation;target tracking;discrete fourier transforms;benchmark testing;image sequences	Correlation tracker has made a huge success in visual object tracking. However, it is mainly because that the tracker cannot catch the occurrence of appearance changes, tracking based on correlation filters often drifts due to the unexpected appearance changes caused by occlusion, deformation and background clutter. In this paper, we propose a new method to detect the case when the tracker encountered the unexpected appearance changes. This method uses the following points: 1) Filter response curve would decreases dramatically when target suffers heavy appearance changes. 2) Features extracted from deeper layers of convolutional neural networks (CNNs) have more semantics information and features extracted from shadower layers have more spatial information. Extensive experimental results on several public benchmark datasets show that the proposed method can deal with the appearance changes effectively.	artificial neural network;benchmark (computing);clutter;convolutional neural network;experiment;maplestory	Wei Chen;Xifeng Guo;Xinwang Liu;En Zhu;Jianping Yin	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899901	benchmark;computer vision;simulation;feature extraction;computer science;machine learning;mathematics;semantics;correlation	Vision	35.36916425014031	-49.76019928362593	115027
2e6776cd582c015b46faf616f29c98ce9cff51a2	facial expression recognition using kernel canonical correlation analysis (kcca)	reconnaissance visage;transformation ondelette;graph theory;mimica;kernel canonical correlation analysis;facial expression recognition;teoria grafo;wavelet transforms face recognition emotion recognition correlation methods graph theory;methode noyau;mimique;face recognition kernel vectors matrix decomposition image converters facial features testing performance evaluation image databases algorithm design and analysis;gabor transformation;emotion recognition;generalized discriminant analysis facial expression recognition kernel canonical correlation analysis labeled graph vector gabor wavelet transformation method semantic expression vector gram matrix;correlation methods;filtro gabor;theorie graphe;generalized discriminant analysis gda;kernel method facial expression recognition fer generalized discriminant analysis gda kernel canonical correlation analysis kcca;discriminant analysis;analyse discriminante;wavelet transforms;analisis discriminante;gabor filter;algorithms databases factual facial expression image processing computer assisted pattern recognition automated;face recognition;facial expression recognition fer;metodo nucleo;analyse correlation;filtre gabor;transformation gabor;kernel canonical correlation analysis kcca;facial features;kernel method;generalized discriminant analysis;transformacion ondita;facial expression;reseau neuronal;correlation canonique;gabor wavelets;correlacion canonica;red neuronal;transformacion gabor;wavelet transformation;analisis correlacion;neural network;canonical correlation;correlation analysis	"""In this correspondence, we address the facial expression recognition problem using kernel canonical correlation analysis (KCCA). Following the method proposed by Lyons et al. and Zhang et al. , we manually locate 34 landmark points from each facial image and then convert these geometric points into a labeled graph (LG) vector using the Gabor wavelet transformation method to represent the facial features. On the other hand, for each training facial image, the semantic ratings describing the basic expressions are combined into a six-dimensional semantic expression vector. Learning the correlation between the LG vector and the semantic expression vector is performed by KCCA. According to this correlation, we estimate the associated semantic expression vector of a given test image and then perform the expression classification according to this estimated semantic expression vector. Moreover, we also propose an improved KCCA algorithm to tackle the singularity problem of the Gram matrix. The experimental results on the Japanese female facial expression database and the Ekman's """"Pictures of Facial Affect"""" database illustrate the effectiveness of the proposed method."""	algorithm;face;gabor wavelet;gramian matrix;graph - visual representation;graph labeling;kernel (operating system);mupirocin, 14c-labeled;standard test image;technological singularity;tracer;wavelet transform;gram	Wenming Zheng;Xiaoyan Zhou;Cairong Zou;Lu Zhao	2006	IEEE Transactions on Neural Networks	10.1109/TNN.2005.860849	kernel method;canonical correlation;speech recognition;computer science;graph theory;machine learning;pattern recognition;mathematics;facial expression;artificial neural network;wavelet transform	Vision	26.86017095614444	-42.30680824144504	115038
3fbb1498257e4cac39a03a4b98b745238bcf2f9e	a temporally piece-wise fisher vector approach for depression analysis	databases;histograms;face active appearance model training histograms databases feature extraction encoding;depression analysis;vectors health care statistical analysis;training;active appearance model;temporally piecewise fisher vector approach avec 2014 german speaking depression database audio video emotion challenge facial dynamics representation spontaneous clinical data statistical aggregation techniques block wise local binary pattern three orthogonal plane descriptors unimodal visual cues affective computing community automatic depression analysis mood disorders;conference paper;feature extraction;fisher vector;face;encoding;fisher vector depression analysis	Depression and other mood disorders are common, disabling disorders with a profound impact on individuals and families. Inspite of its high prevalence, it is easily missed during the early stages. Automatic depression analysis has become a very active field of research in the affective computing community in the past few years. This paper presents a framework for depression analysis based on unimodal visual cues. Temporally piece-wise Fisher Vectors (FV) are computed on temporal segments. As a low-level feature, block-wise Local Binary Pattern-Three Orthogonal Planes descriptors are computed. Statistical aggregation techniques are analysed and compared for creating a discriminative representative for a video sample. The paper explores the strength of FV in representing temporal segments in a spontaneous clinical data. This creates a meaningful representation of the facial dynamics in a temporal segment. The experiments are conducted on the Audio Video Emotion Challenge (AVEC) 2014 German speaking depression database. The superior results of the proposed framework show the effectiveness of the technique as compared to the current state-of-art.	affective computing;experiment;farmville;fisher–yates shuffle;high- and low-level;spontaneous order	Abhinav Dhall;Roland Goecke	2015	2015 International Conference on Affective Computing and Intelligent Interaction (ACII)	10.1109/ACII.2015.7344580	psychology;face;computer vision;active appearance model;speech recognition;feature extraction;computer science;artificial intelligence;machine learning;data mining;histogram;communication;encoding;statistics	Vision	35.31409643365996	-48.864095136826045	115059
9a5f3bde520fa1be393867ef82dc0b49ac053d13	sample diversity, representation effectiveness and robust dictionary learning for face recognition	dictionary learning;face recognition;sparse coding	Conventional dictionary learning algorithms suffer from the following problems when applied to face recognition. First, since in most face recognition applications there are only a limited number of original training samples, it is difficult to obtain a reliable dictionary with a large number of atoms from these samples. Second, because the face images of the same person vary with facial poses and expressions as well as illumination conditions, it is difficult to obtain a robust dictionary for face recognition. Thus, obtaining a robust and reliable dictionary is a crucial key to improve the performance of dictionary learning algorithms for face recognition. In this paper, we propose a novel dictionary learning framework to achieve this. The proposed algorithm framework takes training sample diversities of the same face image into account and tries to obtain more effective representations of face images and a more robust dictionary. It first produces virtual face images and then designs an elaborate objective function. Based on this objective function, we obtain a mathematically tractable and computationally efficient algorithm to generate a robust dictionary. Experimental results demonstrate that the proposed algorithm framework outperforms some previous state-of-the-art dictionary learning and sparse coding algorithms in face recognition. Moreover, the proposed algorithm framework can also be applied to other pattern classification tasks. © 2016 Elsevier Inc. All rights reserved.	algorithm;algorithmic efficiency;cobham's thesis;dictionary;experiment;facial recognition system;loss function;machine learning;nos;neural coding;optimization problem;sparse matrix;statistical classification	Yong Xu;Zhengming Li;Bob Zhang;Jian Yang;Jane You	2017	Inf. Sci.	10.1016/j.ins.2016.09.059	speech recognition;k-svd;computer science;machine learning;pattern recognition	Vision	25.88438701167873	-43.19312336454235	115116
fd0700bb460643d658f9659cf0860dc8ab8be08d	adversarial scene editing: automatic object removal from weak supervision		While great progress has been made recently in automatic image manipulation, it has been limited to object centric images like faces or structured scene datasets. In this work, we take a step towards general scene-level image editing by developing an automatic interaction-free object removal model. Our model learns to find and remove objects from general scene images using image-level labels and unpaired data in a generative adversarial network (GAN) framework. We achieve this with two key contributions: a two-stage editor architecture consisting of a mask generator and image in-painter that co-operate to remove objects, and a novel GAN based prior for the mask generator that allows us to flexibly incorporate knowledge about object shapes. We experimentally show on two datasets that our method effectively removes a wide variety of objects using weak supervision only.	clone tool;experiment;image editing;level editor	Rakshith Shetty;Mario Fritz;Bernt Schiele	2018			generative grammar;computer science;adversarial system;artificial intelligence;pattern recognition;architecture;unpaired data;image editing	ML	25.99250752791284	-50.326022938370585	115744
b5fbfde5d5204c4bd804df8ff2d0bcfc8b3baed0	dlwv2: a deep learning-based wearable vision-system with vibrotactile-feedback for visually impaired people to reach objects		We develop a Deep Learning-based Wearable Vision-system with Vibrotactile-feedback (DLWV2)to guide Blind and Visually Impaired (BVI)people to reach objects. The system achieves high accuracy in object detection and tracking in 3-D using an extended deep learning-based 2.5-D detector and a 3-D object tracker with the ability to track 3-D object locations even outside the camera field-of-view. We train our detector with a large number of images with 2.5-D object ground-truth (i.e., 2-D object bounding boxes and distance from the camera to objects). A novel combination of HTC Vive Tracker with our system enables us to automatically obtain the ground-truth labels for training while requiring very little human effort to set up the system. Moreover, our system processes frames in real-time through a client-server computing platform such that BVI people can receive realtime vibrotactile guidance. We conduct a thorough user study on 12 BVI people in new environments with object instances which are unseen during training. Our system outperforms the non-assistive guiding strategy with statistic significance in both time and the number of contacting irrelevant objects. Finally, the interview with BVI users confirms that our system with distance-based vibrotactile feedback is mostly preferred, especially for objects requiring gentle manipulation such as a bottle with water inside.		Meng-Li Shih;Yi-Chun Chen;Chia-Yu Tung;Cheng Sun;Ching-Ju Cheng;Liwei Chan;Srenivas Varadarajan;Min Sun	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8593711	computer vision;computer science;bounding overwatch;statistic;object detection;wearable computer;deep learning;machine vision;detector;artificial intelligence	Robotics	32.49151389488607	-50.2887615485871	115777
197cb23fe869a6a4bcd309377e95b5cfb2786978	visualizing horn evolution by morphing high-resolution x-ray ct images	scientific application;x ray computed tomography;computer vision;human factors;x ray ct	Reconstructing the evolutionary history of diverse species is a basic goal of systematic biology and essential to comparative biology. Phylogenies representing this history are constructed from analyses of molecular or morphological data and used as tools for understanding the evolution of complex traits. Objectives of this research are to create visualization tools that dynamically reconstruct horn morphology in three-dimensions (3D) for horned lizards and show how changes occur along a phylogeny via metamorphosis (morphing). These objectives will be met by incorporating results from phylogenetic analyses with ancestral trait reconstructions applied to 3-D images generated from a highresolution x-ray computed tomography (CT) scanner. Ancestral reconstruction algorithms are incorporated with nonlinear morphing to accommodate different rates of change. Application of 3-D morphing to systematics and ancestral reconstruction is a novel approach in visualization methods and opens a new realm of collaborative learning and discovery.	algorithm;ancestral reconstruction;ct scan;galaxy morphological classification;image resolution;morphing;nonlinear system;phylogenetic tree;phylogenetics;tomography	Wendy L. Hodges;Theodore Garland;Reuben Reyes;Timothy Rowe	2003		10.1145/965400.965504	computer vision;computer science;human factors and ergonomics	Visualization	38.19189245313303	-39.153716427269764	115868
b35e6150be354b682fab191f28c9bc9ab6201c74	semi supervised semantic segmentation using generative adversarial network		Semantic segmentation has been a long standing challenging task in computer vision. It aims at assigning a label to each image pixel and needs a significant number of pixel-level annotated data, which is often unavailable. To address this lack of annotations, in this paper, we leverage, on one hand, a massive amount of available unlabeled or weakly labeled data, and on the other hand, non-real images created through Generative Adversarial Networks. In particular, we propose a semi-supervised framework – based on Generative Adversarial Networks (GANs) – which consists of a generator network to provide extra training examples to a multi-class classifier, acting as discriminator in the GAN framework, that assigns sample a label y from the K possible classes or marks it as a fake sample (extra class). The underlying idea is that adding large fake visual data forces real samples to be close in the feature space, which, in turn, improves multiclass pixel classification. To ensure a higher quality of generated images by GANs with consequently improved pixel classification, we extend the above framework by adding weakly annotated data, i.e., we provide class level information to the generator. We test our approaches on several challenging benchmarking visual datasets, i.e. PASCAL, SiftFLow, Stanford and CamVid, achieving competitive performance compared to state-of-the-art semantic segmentation methods.	benchmark (computing);computer vision;condition number;dhrystone;discriminator;feature vector;generative adversarial networks;pixel;semi-supervised learning;semiconductor industry;supervised learning	Nasim Souly;Concetto Spampinato;Mubarak Shah	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.606	artificial intelligence;generative grammar;pixel;visualization;benchmarking;computer science;feature vector;semantics;pattern recognition;discriminator;image segmentation	Vision	27.042549600666995	-49.90970329616177	115974
ccd902f8a13a5700502098a40786b3215fca9e75	evaluation of synthetic data for deep learning stereo depth algorithms on embedded platforms		Stereo vision is a very active field in the realm of computer vision and in recent years Convolutional Neural Networks (CNNs) have proven to be very competitive against the state-of-the-art. However the performance of these networks are limited by the quality of the data that is used when training the CNNs. Data acquisition of high quality labelled images is a time-consuming and expensive process. By exploiting the power of modern-day powerful GPUs, we present a synthetic dataset with fully rectified stereo image pairs and accompanying accurate ground truth information that can be used for training and testing stereo algorithms. We provide validation of the quality of our dataset by performing quantitative experiments that suggest pre-training deep learning algorithms on synthetic data can perform competitively against networks trained on real life data. Testing on the KITTI data-set[1], we found the accuracy performance difference between the real and synthetically trained networks was within a margin of 1.8%. We also illustrate the functionality synthetic data can provide, by conducting a key performance index on a selection of conventional and deep learning stereo algorithms available on embedded platforms and compared them under common metrics. We also focused on power consumption and performance and we were able to achieve a compute the matching cost from a CNN performing inference on an embedded device at 11.9 FPS at 1.2 Watts.	algorithm;artificial neural network;computer vision;convolutional neural network;data acquisition;deep learning;depth perception;display resolution;embedded system;experiment;floating point systems;graphics processing unit;ground truth;impedance matching;intel compute stick;machine learning;performance evaluation;real life;stereopsis;synthetic data;synthetic intelligence;watts humphrey	Kevin Lee;David Moloney	2017	2017 4th International Conference on Systems and Informatics (ICSAI)	10.1109/ICSAI.2017.8248284	convolutional neural network;computer science;data modeling;deep learning;data acquisition;inference;ground truth;synthetic data;algorithm;artificial intelligence	Vision	27.11525255110187	-49.83200948802557	116164
96d8412c14d8756d0b5682ec39fad5063fee2d9d	exploiting spatial structure for localizing manipulated image regions		The advent of high-tech journaling tools facilitates an image to be manipulated in a way that can easily evade state-of-the-art image tampering detection approaches. The recent success of the deep learning approaches in different recognition tasks inspires us to develop a high confidence detection framework which can localize manipulated regions in an image. Unlike semantic object segmentation where all meaningful regions (objects) are segmented, the localization of image manipulation focuses only the possible tampered region which makes the problem even more challenging. In order to formulate the framework, we employ a hybrid CNN-LSTM model to capture discriminative features between manipulated and non-manipulated regions. One of the key properties of manipulated regions is that they exhibit discriminative features in boundaries shared with neighboring non-manipulated pixels. Our motivation is to learn the boundary discrepancy, i.e., the spatial structure, between manipulated and non-manipulated regions with the combination of LSTM and convolution layers. We perform end-to-end training of the network to learn the parameters through back-propagation given ground-truth mask information. The overall framework is capable of detecting different types of image manipulations, including copy-move, removal and splicing. Our model shows promising results in localizing manipulated regions, which is demonstrated through rigorous experimentation on three diverse datasets.	backpropagation;clone tool;convolution;deep learning;discrepancy function;end-to-end principle;experiment;ground truth;internationalization and localization;long short-term memory;pixel;sensor;software propagation;unified framework	Jawadul H. Bappy;Amit K. Roy-Chowdhury;Jason Bunk;Lakshmanan Nataraj;B. S. Manjunath	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.532	journaling file system;computer vision;visualization;pixel;discriminative model;pattern recognition;computer science;deep learning;semantics;image segmentation;convolution;artificial intelligence	Vision	26.05315941609418	-51.61687822969591	116483
b5f2846a506fc417e7da43f6a7679146d99c5e96	ucf101: a dataset of 101 human actions classes from videos in the wild		We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user-uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 43.9%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips.	action potential;bag-of-words model;baseline (configuration management)	Khurram Soomro;Amir Roshan Zamir;Mubarak Shah	2012	CoRR		computer vision;simulation;computer science;multimedia	Vision	32.514537829350985	-50.364046216435035	116622
7ae479941ce8bc52c8115aa6e87c5eecb7527aba	key frame extraction of skeleton joint based on kinect sensor		In order to reduce the computational complexity and improve the recognition accuracy for human activities recognition base on Kinect sensor, this paper presents a key frame extraction of 3D skeleton joint. this paper takes the key frame body structural similarity from the self-adapted search algorithm, and determinates the threshold of search algorithm. The results show that the extraction of features is discriminative.	computational complexity theory;key frame;kinect;search algorithm;structural similarity	Cheche Xie;Sheng Bi;Min Dong;Al Montasser Hussien	2018	2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00075	skeleton (computer programming);discriminative model;information processing;computational complexity theory;structural similarity;big data;key frame;search algorithm;computer vision;artificial intelligence;computer science	AI	36.36504897053925	-50.761262711121034	116648
bf95072ec8f769eb36cc268c446d3d5c1a545eb9	road detection from high-resolution satellite images using artificial neural networks	network design;high resolution;neural networks;harbors;road detection;isla;detection;classification;satelite;puerto;islands;windows;haute resolution;propagacion;route;roads;satellite;fenetre;feature extraction;satellites;pixel;alta resolucion;carretera;ventana;network structure;reseau neuronal;ile;port;high resolution satellite images;back propagation;clasificacion;red neuronal;propagation;artificial neural network	In this article, the possibility of using artificial neural networks for road detection from high resolution satellite images is tested on a part of RGB Ikonos and Quick-Bird images from Kish Island and Bushehr Harbour respectively. Then, the effects of different input parameters on network’s ability are verified to find out optimum input vector for this problem. A variety of network structures with different iteration times are used to determine the best network structure and termination condition in training stage. It was discovered when the input parameters are made up of spectral information and distances of pixels to road mean vector in a 3*3 window, network's ability in both road and background detection can be improved in comparison with simple networks that just use spectral information of a single pixel in their input vector.	artificial neural network;image resolution;iteration;pixel	Mehdi Mokhtarzade;Mohammad Javad Valadan Zoej	2007	Int. J. Applied Earth Observation and Geoinformation	10.1016/j.jag.2006.05.001	geography;telecommunications;computer science;machine learning;artificial neural network;satellite;cartography;remote sensing	ML	33.72330805846115	-43.047856932459055	116813
03959f002d7ea34b948d9f62527490a1fe8240a3	lda-based non-negative matrix factorization for supervised face recognition	principal component analysis;nonnegative matrix factorization;eigenface	In PCA based face recognition, the basis images may contain negative pixels and thus do not facilitate physical interpretation. Recently, the technique of nonnegative matrix Factorization (NMF) has been applied to face recognition: the non-negativity constraint of NMF yields a localized parts-based representation which achieves a recognition rate that is on par with the eigenface approach. In this paper, we propose a new variation of the NMF algorithm that incorporates training information in a supervised learning setting. We integrate an additional term based on Fisher’s Linear Discriminant Analysis into the NMF algorithm and prove that our new update rule can maintain the non-negativity constraint under a mild condition and hence preserve the intuitive meaning for the base vectors and weight vectors while facilitating the supervised learning of within-class and between-class information. We tested our new algorithm on the well-known ORL database, CMU PIE database and FERET database, and the results from experiments are very encouraging compared with traditional techniques including the original NMF, the Eigenface method, the sequential NMF+LDA method and the Fisherface method.	algorithm;blitzkrieg;computational complexity theory;eigenface;experiment;feret database;facial recognition system;feature selection;feature vector;image analysis;linear discriminant analysis;local-density approximation;loss function;negativity (quantum mechanics);non-negative matrix factorization;pixel;principal component analysis;return loss;selection algorithm;supervised learning;wavelet transform;whole earth 'lectronic link	Yun Xue;Chong Sze Tong;Jing Yun Yuan	2014	JSW		speech recognition;computer science;machine learning;pattern recognition;eigenface;non-negative matrix factorization;principal component analysis	Vision	25.436783513806663	-40.95490330893488	116954
0a9345ea6e488fb936e26a9ba70b0640d3730ba7	deep bi-directional cross-triplet embedding for cross-domain clothing retrieval	cross domain;triplet embedding;clothing retrieval	In this paper, we address two practical problems when shopping online: 1) What will I look like when wearing this clothing on the street? 2) How to find the exact same or similar clothing that other people are wearing on the street or in a movie? In this paper, we jointly solve these two problems with one bi-directional shop-to-street street-to-shop clothing retrieval framework. There are three main challenges of cross-domain clothing retrieval task. First is to learn the discrepancy (e.g., background, pose, illumination) between street domain and shop domain clothing. Second, both intra-domain and cross-domain similarity need to be considered during feature embedding. Third, there is large bias between the number of matched and non-matched street and shop pairs. To solve these challenges, in this paper, we propose a deep bi-directional cross-triplet embedding algorithm by extending the start-of-the-art triplet embedding into cross-domain retrieval scenario. Extensive experiments demonstrate the effectiveness of the proposed algorithm.	algorithm;discrepancy function;experiment;online shopping;regular expression;triplet state	Shuhui Jiang;Yue Wu;Yun Fu	2016		10.1145/2964284.2967182	simulation	AI	27.304974376188337	-47.86052864106069	117076
56b9c6efe0322f0087d2f82b52129cc6b41ab356	acquire, augment, segment & enjoy: weakly supervised instance segmentation of supermarket products.		Grocery stores have thousands of products that are usually identified using barcodes with a human in the loop. For automated checkout systems, it is necessary to count and classify the groceries efficiently and robustly. One possibility is to use a deep learning algorithm for instanceaware semantic segmentation. Such methods achieve high accuracies but require a large amount of annotated training data. We propose a system to generate the training annotations in a weakly supervised manner, drastically reducing the labeling effort. We assume that for each training image, only the object class is known. The system automatically segments the corresponding object from the background. The obtained training data is augmented to simulate variations similar to those seen in real-world setups. Our experiments show that with appropriate data augmentation, our approach obtains competitive results compared to a fully-supervised baseline, while drastically reducing the amount of manual labeling.	algorithm;barcode;baseline (configuration management);convolutional neural network;deep learning;experiment;lifting scheme;point of sale;simulation;supervised learning;test set	Patrick Follmann;Bertram Drost;Tobias Böttger	2018	CoRR		artificial intelligence;pattern recognition;augment;deep learning;machine learning;training set;segmentation;computer science;human-in-the-loop	NLP	28.00341689540054	-50.27950997785149	117400
be209ac9ae19d06fa54111c58fb84d5364bf6b39	gait analysis for human identification through manifold learning and hmm	hidden markov model;gait analysis;dimension reduction;gaussian process;latent variable model;manifold learning	With the increasing demands of visual surveillance systems, human identification at a distance has gained more interest. Gait is often used as an unobtrusive biometric offering the possibility to identify individuals at a distance without any interaction or co-operation with the subject. This paper presents a novel effectively method for automatic viewpoint and person identification by using only the sequence of gait silhouette. The gait silhouettes are nonlinearly transformed into low dimensional embedding and the dynamics in time-series images are modeled by HMM in the corresponding embedding space. The experimental results demonstrate that the proposed algorithm is an encouraging progress for automatic human identification.	algorithm;biometrics;gait analysis;hidden markov model;nonlinear dimensionality reduction;nonlinear system;time series	Ming-Hsu Cheng;Meng-Fen Ho;Chung-Lin Huang	2007	2007 IEEE Workshop on Motion and Video Computing (WMVC'07)	10.1109/ISCAS.2007.378088	computer vision;speech recognition;gait analysis;system identification;computer science;machine learning;pattern recognition;gaussian process;nonlinear dimensionality reduction;latent variable model;hidden markov model;biometrics;dimensionality reduction	Vision	35.84788043220863	-51.797911938143	117537
32fca37dcd496dc4306fb19be52f0988a742e4d9	clustering-based discriminative locality alignment for face gender recognition	pattern clustering;face face recognition geometry manifolds support vector machines principal component analysis robots;representative method clustering based discriminative locality alignment face gender recognition human robot interaction human gender information manifold learning visual recognition cdla algorithm low dimensional intrinsic submanifold discovery high dimensional ambient space embedding global geometry k means clustering discriminative information extraction margin maximization local geometry intracluster sample concentration feret data set;manifolds;support vector machines;geometry;human robot interaction;face recognition;robot vision;image representation;feature extraction;principal component analysis;robots;face;learning artificial intelligence;conference proceeding;robot vision face recognition feature extraction geometry human robot interaction image representation learning artificial intelligence pattern clustering	To facilitate human-robot interactions, human gender information is very important. Motivated by the success of manifold learning for visual recognition, we present a novel clustering-based discriminative locality alignment (CDLA) algorithm to discover the low-dimensional intrinsic submanifold from the embedding high-dimensional ambient space for improving the face gender recognition performance. In particular, CDLA exploits the global geometry through k-means clustering, extracts the discriminative information through margin maximization and explores the local geometry through intra cluster sample concentration. These three properties uniquely characterize CDLA for face gender recognition. The experimental results obtained from the FERET data sets suggest the superiority of the proposed method in terms of recognition speed and accuracy by comparing with several representative methods.	cluster analysis;computation;expectation–maximization algorithm;feret (facial recognition technology);interaction;k-means clustering;locality of reference;nonlinear dimensionality reduction	Duo Chen;Jun Cheng;Dacheng Tao	2012	2012 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2012.6385793	facial recognition system;human–robot interaction;robot;face;support vector machine;computer vision;manifold;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;principal component analysis	Robotics	27.324215354033598	-43.59008490921817	117681
44b480e9c2ce3d3093d704f99463a35a2f8524f1	unsupervised adaptation for acceleration-based activity recognition: robustness to sensor displacement and rotation	sensor displacement;unsupervised adaptation;opportunity;expectation maximisation;linear discriminant analysis;activity recognition	A common assumption in activity recognition is that the system remains unchanged between its design and its posterior operation. However, many factors affect the data distribution between two different experimental sessions. One of these factors is the potential change in the sensor location (e.g. due to replacement or slippage) affecting the classification performance. Assuming that changes in the sensor placement mainly result in shifts in the feature distributions, we propose an unsupervised adaptive classifier that calibrates itself using an online version of expectation–maximisation. Tests using three activity recognition scenarios show that the proposed adaptive algorithm is robust against shift in the feature space due to sensor displacement and rotation. Moreover, since the method estimates the change in the feature distribution, it can also be used to roughly evaluate the reliability of the system during online operation.	activity recognition;adaptive algorithm;displacement mapping;expectation–maximization algorithm;feature vector;sensor	Ricardo Chavarriaga;Hamidreza Bayati;José del R. Millán	2011	Personal and Ubiquitous Computing	10.1007/s00779-011-0493-y	computer vision;computer science;machine learning;pattern recognition;linear discriminant analysis;activity recognition	HCI	38.563550012815476	-42.804397360174654	117740
1505c5166cf765a56a0484447dea7ce329f4b864	non-homogeneous hidden markov chain models for wavelet-based hyperspectral image processing	multiscale fashion nonhomogeneous hidden markov chain models wavelet based hyperspectral image processing nhmc models wavelet transformations hyperspectral signatures signal processing purpose hidden markov trees natural images absorption bands structural features mineral spectra spectral signature dataset standard classification algorithms spectral family;image classification;trees mathematics;wavelet transforms;hidden markov models;hyperspectral imaging;wavelet transforms hidden markov models hyperspectral imaging image classification natural scenes trees mathematics;minerals hyperspectral imaging hidden markov models wavelet transforms absorption measurement;natural scenes	We consider the use of non-homogeneous Markov chain (NHMC) models for wavelet transformations of hyperspectral signatures to generate features for signal processing purposes. Inspired by the use of hidden Markov trees for natural images, the NHMC model enables the characterization of absorption bands and other structural features of mineral spectra that are used by experts in tasks like classification and unmixing, primarily in an ad-hoc fashion. We show that NHMC models can successfully identify and capture the information in a spectral signature dataset that can be exploited by standard classification algorithms to identify and differentiate spectral families. We also identify several metrics that can help determine whether each spectral band is informative to classification in a multiscale fashion.	algorithm;expect;feature selection;heart rate variability;hidden markov model;hoc (programming language);image processing;image segmentation;information;markov chain;rca spectra 70;signal processing;software metric;triune continuum paradigm;type signature;wavelet	Marco F. Duarte;Mario Parente	2013	2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/Allerton.2013.6736518	computer vision;machine learning;pattern recognition;mathematics	Vision	31.344016884852063	-39.07465362479225	117813
f0e17f27f029db4ad650ff278fe3c10ecb6cb0c4	the eurocity persons dataset: a novel benchmark for object detection		Big data has had a great share in the success of deep learning in computer vision. Recent works suggest that there is significant further potential to increase object detection performance by utilizing even bigger datasets. In this paper, we introduce the EuroCity Persons dataset, which provides a large number of highly diverse, accurate and detailed annotations of pedestrians, cyclists and other riders in urban traffic scenes. The images for this dataset were collected on-board a moving vehicle in 31 cities of 12 European countries. With over 238200 person instances manually labeled in over 47300 images, EuroCity Persons is nearly one order of magnitude larger than person datasets used previously for benchmarking. The dataset furthermore contains a large number of person orientation annotations (over 211200). We optimize four state-of-the-art deep learning approaches (Faster R-CNN, R-FCN, SSD and YOLOv3) to serve as baselines for the new object detection benchmark. In experiments with previous datasets we analyze the generalization capabilities of these detectors when trained with the new dataset. We furthermore study the effect of the training set size, the dataset diversity (dayvs. night-time, geographical region), the dataset detail (i.e. availability of object orientation information) and the annotation quality on the detector performance. Finally, we analyze error sources and discuss the road ahead.	baseline (configuration management);benchmark (computing);big data;closing (morphology);computer multitasking;computer vision;deep learning;experiment;geolocation;holism;human reliability;object detection;on-board data handling;scenario testing;sensor;solid-state drive;test set	Markus Braun;Sebastian Krebs;Fabian Flohr;Dariu Gavrila	2018	CoRR		computer vision;computer science;big data;artificial intelligence;deep learning;object detection;training set;annotation;object-orientation	Vision	30.41992568345082	-52.01840737713191	117832
728d44bc3fec1cdd38ae691e93b49043a7b1acb4	progressive hard-mining network for monocular depth estimation		Depth estimation from the monocular RGB image is a challenging task for computer vision due to no reliable cues as the prior knowledge. Most existing monocular depth estimation works including various geometric or network learning methods lack of an effective mechanism to preserve the cross-border details of depth maps, which yet is very important for the performance promotion. In this paper, we propose a novel end-to-end progressive hard-mining network (PHN) framework to address this problem. Specifically, we construct the hard-mining objective function, the intra-scale and inter-scale refinement subnetworks to accurately localize and refine those hard-mining regions. The intra-scale refining block recursively recovers details of depth maps from different semantic features in the same receptive field while the inter-scale block favors a complementary interaction among multi-scale depth cues of different receptive fields. For further reducing the uncertainty of the network, we design a difficulty-ware refinement loss function to guide the depth learning process, which can adaptively focus on mining these hard-regions where accumulated errors easily occur. All three modules collaborate together to progressively reduce the error propagation in the depth learning process, and then, boost the performance of monocular depth estimation to some extent. We conduct comprehensive evaluations on several public benchmark data sets (including NYU Depth V2, KITTI, and Make3D). The experiment results well demonstrate the superiority of our proposed PHN framework over other state of the arts for monocular depth estimation task.	benchmark (computing);computer vision;depth map;depth perception;end-to-end principle;evaluation;glossary of computer graphics;hearing loss, high-frequency;loss function;matching;numerous;optical flow;postherpetic neuralgia;propagation of uncertainty;recursion;refinement (computing);simultaneous localization and mapping;software propagation;unsupervised learning;warez;negative regulation of endoplasmic reticulum tubular network organization	Zhenyu Zhang;Chunyan Xu;Jian Yang;Junbin Gao;Zhen Cui	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2821979	computer vision;artificial intelligence;rgb color model;recursion;propagation of uncertainty;data set;mathematics;depth perception;monocular;pattern recognition	Vision	26.00135654758123	-51.27788591232502	117852
c9c934b46a301b2f9975d55f2ca6ebf7c14af741	fusion multiscale superpixel features for classification of hyperspectral images		A novel multiscale superpixel-based fusion classification approach is proposed for hyperspectral images in this study. Superpixels are considered as basic processing unit for spectral-spatial based classification. The proposed technique consists of three steps. In the first step, an improved superpixel segmentations are proposed to perform from coarse to fine scales for the original hyperspectral image. In the second step, features of each superpixel are used for classification at each scale. Multiple classifiers are trained for each scale separately. Finally, the multiscale classification is obtained via decision fusion. Experiments are presented for two hyperspectral images and compared with pixelwise and spectral-spatial classification based on segmentation approaches. The results demonstrate that the proposed method works effectively and the accuracy improvement with fusion of multiscale superpixel for hyperspectral classification.	image fusion	Shanshan Li;Bing Zhang;Xiuping Jia;Hua Wu	2016	2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)	10.1109/WHISPERS.2016.8071751	fusion;hyperspectral imaging;computer vision;pattern recognition;artificial intelligence;computer science	Vision	30.77203984983511	-44.40160563622241	117909
467ce1784f3cfec0a696034a5fda748cd1055485	advanced machine learning techniques for computer vision	vision system;image understanding;computer vision;machine learning;target recognition;knowledge acquisition;shape modeling;autonomic computing;parameter optimization;optimization model	Learning is a critical research field for autonomous computer vision systems. It can bring solutions to the knowledge acquisition bottleneck of image understanding systems. Recent developments of machine learning for computer vision are reported in this paper. We describe several different approaches for learning at different levels of the image understanding process, including learning 2-D shape models, learning strategic knowledge for optimizing model matching, learning for adaptative target recognition systems, knowledge acquisition of constraint rules for labelling and automatic parameter optimization for vision systems.	autonomous robot;computer vision;knowledge acquisition;machine learning;mathematical optimization	Stéphane Moscatelli;Yves Kodratoff	1992		10.1007/3-540-55681-8_35	robot learning;computer vision;machine vision;computer science;machine learning;pattern recognition;autonomic computing	AI	35.49555514792711	-40.60243512087958	118018
a0009929b52eae97ec0f043e219abf1ce2fa4604	a classification framework of neural networks fusing spectrum and texture information	classifier fusion;fuzzy artmap;neural networks;spectrum;data mining;fuzzy sets;subspace constraints;fuzzy logic;computational complexity;feature extraction;remote sensing;satellites;nature reserve;classification algorithms;classification accuracy;neural network;neural networks classification algorithms data mining remote sensing fuzzy sets subspace constraints fuzzy logic feature extraction satellites computational complexity	Against the shortages in spectrum classification and texture classification algorithms, this paper advances a spectrum and texture information fusion framework based on improved Fuzzy ARTMAP network. Either spectrum classification algorithm or texture classification algorithm has some weakness separately, How to combine the two different classification algorithms effectively so as to improve the classification accuracy is a valuable research direction. In this paper, an improved Fuzzy ARTMAP is advanced, and the fusion framework of spectrum and texture advanced in this paper is based on the improved Fuzzy ARTMAP. The fusion framework is on classifiers fusion level that integrates results of spectrum classifier and texture classifier. The data source is TM images of Zhalong wetland nature reserve in north China, and both spectral and textural characters are extracted from them. The simulation results indicate that the classifiers fusion framework based on improved Fuzzy ARTMAP can improve the classification accuracy dramatically, both in spectral confusion and texture irregularity changing situations. Keywords-Fuzzy ARTMAP; fusion; Wetland Classification	algorithm;artificial neural network;simulation;statistical classification	Min Han;Xiaoliang Tang	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1525740	fuzzy logic;spectrum;feature extraction;computer science;machine learning;pattern recognition;data mining;nature reserve;fuzzy set;computational complexity theory;artificial neural network;satellite	Robotics	32.10659224540786	-44.27424724244273	118227
8c7f4c11b0c9e8edf62a0f5e6cf0dd9d2da431fa	dataset augmentation for pose and lighting invariant face recognition		The performance of modern face recognition systems is a function of the dataset on which they are trained. Most datasets are largely biased toward “near-frontal” views with benign lighting conditions, negatively effecting recognition performance on images that do not meet these criteria. The proposed approach demonstrates how a baseline training set can be augmented to increase pose and lighting variability using semisynthetic images with simulated pose and lighting conditions. The semi-synthetic images are generated using a fast and robust 3d shape estimation and rendering pipeline which includes the full head and background. Various methods of incorporating the semi-synthetic renderings into the training procedure of a state of the art deep neural network-based recognition system without modifying the structure of the network itself are investigated. Quantitative results are presented on the challenging IJB-A identification dataset using a state of the art recognition pipeline as a baseline.	3d modeling;artificial neural network;baseline (configuration management);deep learning;facial recognition system;graphics pipeline;semiconductor industry;spatial variability;synthetic intelligence;test set	Daniel E. Crispell;Octavian Biris;Nate Crosswhite;Jeffrey Byrne;Joseph L. Mundy	2017	CoRR		computer vision;simulation;machine learning;three-dimensional face recognition	Vision	29.170237954634395	-50.418784265868695	118329
4ed6c7740ba93d75345397ef043f35c0562fb0fd	two faces are better than one: face recognition in group photographs	statistical independence;face recognition cameras decision making;spectrum;camera motion;face recognition;scene geometry group photographs image acquisition consistent lighting camera motion camera geometry face recognition systems joint decision making independent decision making;cameras	Face recognition systems classically recognize people individually. When presented with a group photograph containing multiple people, such systems implicitly assume statistical independence between each detected face. We question this basic assumption and consider instead that there is a dependence between face regions from the same image; after all, the image was acquired with a single camera, under consistent lighting (distribution, direction, spectrum), camera motion, and scene/camera geometry. Such naturally occurring commonalities between face images can be exploited when recognition decisions are made jointly across the faces, rather than independently. Furthermore, when recognizing people in isolation, some features such as color are usually uninformative in unconstrained settings. But by considering pairs of people, the relative color difference provides valuable information. This paper reconsiders the independence assumption, introduces new features and methods for recognizing pairs of individuals in group photographs, and demonstrates a marked improvement when these features are used in joint decision making vs. independent decision making. While these features alone are only moderately discriminative, we combine these new features with state-of-art attribute features and demonstrate effective recognition performance. Initial experiments on two datasets show promising improvements in accuracy.	autostereogram;baseline (configuration management);color;discriminative model;experiment;facial recognition system;human height;image processing;vertex-transitive graph;virtual camera system	Ohil K. Manyam;Neeraj Kumar;Peter N. Belhumeur;David J. Kriegman	2011	2011 International Joint Conference on Biometrics (IJCB)	10.1109/IJCB.2011.6117516	facial recognition system;independence;spectrum;computer vision;pattern recognition;three-dimensional face recognition;3d single-object recognition;statistics	Vision	34.022114003465994	-51.57017456199394	118422
578e755e669caee147964f9412c23943cd0f0789	l2, 1 regularized correntropy for robust feature selection	minimisation;robust feature selection;concave programming;quadratic programming;minimization;nonconvex correntropy objective;convergence;algorithmic development;1 regularized correntropy;informative features;hq optimization;training concave programming convergence face recognition feature extraction minimisation quadratic programming;training;open face recognition datasets;appearance based model;robustness minimization face vectors face recognition optimization feature extraction;training data;half quadratic optimization;offace recognition;face recognition;vectors;feature extraction;1 norm minimization;l 2;robustness;face;optimization;large scale face recognition datasets l 2 1 regularized correntropy robust feature selection l 2 1 norm minimization half quadratic optimization hq optimization algorithmic development informative features training data nonconvex correntropy objective offace recognition appearance based model sparse fisherfaces state of the art subspace methods open face recognition datasets;state of the art subspace methods;large scale face recognition datasets;sparse fisherfaces	In this paper, we study the problem of robust feature extraction based on l2,1 regularized correntropy in both theoretical and algorithmic manner. In theoretical part, we point out that an l2,1-norm minimization can be justified from the viewpoint of half-quadratic (HQ) optimization, which facilitates convergence study and algorithmic development. In particular, a general formulation is accordingly proposed to unify l1-norm and l2,1-norm minimization within a common framework. In algorithmic part, we propose an l2,1 regularized correntropy algorithm to extract informative features meanwhile to remove outliers from training data. A new alternate minimization algorithm is also developed to optimize the non-convex correntropy objective. In terms of face recognition, we apply the proposed method to obtain an appearance-based model, called Sparse-Fisherfaces. Extensive experiments show that our method can select robust and sparse features, and outperforms several state-of-the-art subspace methods on largescale and open face recognition datasets.	algorithm;experiment;facial recognition system;feature extraction;feature selection;information;mathematical optimization;sparse approximation;sparse matrix;taxicab geometry;test set	Ran He;Tieniu Tan;Liang Wang;Wei-Shi Zheng	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247966	facial recognition system;face;minimisation;training set;mathematical optimization;convergence;feature extraction;computer science;machine learning;pattern recognition;mathematics;quadratic programming;robustness	Vision	25.14173407241143	-42.07109933259076	118475
75638e102c9abf881b320238847afa92dc8308fc	towards a robust face recognition system using compressive sensing	compressed sensing;face recognition	An application of compressive sensing (CS) theory in imagebased robust face recognition is considered. Most contemporary face recognition systems suffer from limited abilities to handle image nuisances such as illumination, facial disguise, and pose misalignment. Motivated by CS, the problem has been recently cast in a sparse representation framework: The sparsest linear combination of a query image is sought using all prior training images as an overcomplete dictionary, and the dominant sparse coefficients reveal the identity of the query image. The ability to perform dense error correction directly in the image space also provides an intriguing solution to compensate pixel corruption and improve the recognition accuracy exceeding most existing solutions. Furthermore, a local iterative process can be applied to solve for an image transformation applied to the face region when the query image is misaligned. Finally, we discuss the state of the art in fast `1-minimization to improve the speed of the robust face recognition system. The paper also provides useful guidelines to practitioners working in similar fields, such as acoustic/speech recognition.	acoustic cryptanalysis;coefficient;compressed sensing;dictionary;error detection and correction;facial recognition system;iterative method;pattern recognition;pixel;sparse approximation;sparse matrix;speech recognition	Allen Y. Yang;Zihan Zhou;Yi Ma;S. Shankar Sastry	2010			error detection and correction;pixel;artificial intelligence;compressed sensing;linear combination;iterative and incremental development;pattern recognition;facial recognition system;sparse approximation;computer science	Vision	29.288440870367296	-46.972115535516494	118684
11511cc23119927f427d6432f25ae2914df13b35	learning with transformation invariant kernels	positive definite;gaussian kernel;representation theorem;length scale;support vector machine	This paper considers kernels invariant to translation, rotation and dilation. We show that no non-trivial positive definite (p.d.) kernels exist which are radial and dilation invariant, only conditionally positive definite (c.p.d.) ones. Accordingly, we discuss the c.p.d. case and provide some novel analysis, including an elementary derivation of a c.p.d. representer theorem. On the practical side, we give a support vector machine (s.v.m.) algorithm for arbitrary c.p.d. kernels. For the thinplate kernel this leads to a classifier with only one parameter (the amount of regularisation), which we demonstrate to be as effective as an s.v.m. with the Gaussian kernel, even though the Gaussian involves a second parameter (the length scale).	algorithm;dilation (morphology);elementary;existential quantification;experiment;kernel (operating system);machine learning;model selection;radial (radio);representer theorem;support vector machine	Christian Walder;Olivier Chapelle	2007			support vector machine;kernel method;mathematical optimization;combinatorics;discrete mathematics;computer science;machine learning;mathematics;positive-definite matrix;length scale;gaussian function;statistics	ML	25.039592763334475	-39.487097436010664	119002
35498b80ee457e409c0962e03a6e170a917c83af	look into person: self-supervised structure-sensitive learning and a new benchmark for human parsing		Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark Look into Person (LIP) that makes a significant advance in terms of scalability, diversity and difficulty, a contribution that we feel is crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analysis of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for specifically labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-Person-Part dataset demonstrate the superiority of our method.	artificial neural network;benchmark (computing);high- and low-level;parsing expression grammar;scalability;supervised learning	Ke Gong;Xiaodan Liang;Xiaohui Shen;Liang Lin	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.715	artificial neural network;computer vision;discriminative model;artificial intelligence;machine learning;pattern recognition;parsing;data mining;scalability;computer science;image segmentation;benchmark (computing);semantics;server	Vision	28.052338228156138	-50.68716963585552	119184
10e7dd3bbbfbc25661213155e0de1a9f043461a2	cross euclidean-to-riemannian metric learning with application to face recognition from video		Riemannian manifolds have been widely employed for video representations in visual classification tasks including video-based face recognition. The success mainly derives from learning a discriminant Riemannian metric which encodes the non-linear geometry of the underlying Riemannian manifolds. In this paper, we propose a novel metric learning framework to learn a distance metric across a Euclidean space and a Riemannian manifold to fuse average appearance and pattern variation of faces within one video. The proposed metric learning framework can handle three typical tasks of video-based face recognition: Video-to-Still, Still-to-Video and Video-to-Video settings. To accomplish this new framework, by exploiting typical Riemannian geometries for kernel embedding, we map the source Euclidean space and Riemannian manifold into a common Euclidean subspace, each through a corresponding high-dimensional Reproducing Kernel Hilbert Space (RKHS). With this mapping, the problem of learning a cross-view metric between the two source heterogeneous spaces can be converted to learning a single-view Euclidean distance metric in the target common Euclidean space. By learning information on heterogeneous data with the shared label, the discriminant metric in the common space improves face recognition from videos. Extensive experiments on four challenging video face databases demonstrate that the proposed framework has a clear advantage over the state-of-the-art methods in the three classical video-based face recognition scenarios.	database;discriminant;embedding;euclidean distance;experiment;face;facial recognition system;fuse device component;gene distance metric;genetic heterogeneity;hilbert space;kernel;nonlinear system;manifold	Zhiwu Huang;Ruiping Wang;Shiguang Shan;Luc Van Gool;Xilin Chen	2018	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2017.2776154	convex metric space;computer vision;topology;metric;machine learning;mathematics;geometry	Vision	26.978161035930253	-44.87720503152102	119200
13128953c52c9e8376870bc7ac8c728e4e1ed429	efficient discriminative learning of class hierarchy for many class prediction	comparable prediction performance;class hierarchical model;class prediction;redundant class;class hierarchy;discriminative class;new efficient discriminative class;associated class;active class selection strategy;selected class;state-of-the-art method	Recently the maximum margin criterion has been employed to learn a discriminative class hierarchical model, which shows promising performance for rapid multi-class prediction. Specifically, at each node of this hierarchy, a separating hyperplane is learned to split its associated classes from all of the corresponding training data, leading to a time-consuming training process in computer vision applications with many classes such as large-scale object recognition and scene classification. To address this issue, in this paper we propose a new efficient discriminative class hierarchy learning approach for many class prediction. We first present a general objective function to unify the two stateof-the-art methods for multi-class tasks. When there are many classes, this objective function reveals that some classes are indeed redundant. Thus, omitting these redundant classes will not degrade the prediction performance of the learned class hierarchical model. Based on this observation, we decompose the original optimization problem into a sequence of much smaller sub-problems by developing an adaptive classifier updating method and an active class selection strategy. Specifically, we iteratively update the separating hyperplane by efficiently using the training samples only from a limited number of selected classes that are well separated by the current separating hyperplane. Comprehensive experiments on three large-scale datasets demonstrate that our approach can significantly accelerate the training process of the two state-of-the-art methods while achieving comparable prediction performance in terms of both classification accuracy and testing speed.	adaptive grammar;class hierarchy;computer vision;digital media;experiment;hierarchical database model;iterative method;markov switching multifractal;mathematical optimization;optimization problem;outline of object recognition;statistical classification	Lin Chen;Lixin Duan;Ivor W. Tsang;Dong Xu	2012		10.1007/978-3-642-37331-2_21	artificial intelligence;machine learning;pattern recognition;mathematics	Vision	25.488362133233952	-45.18409722000562	119278
bad43c1ce070be2812a248feac3dad5a74fbf21b	functional decomposition for bundled simplification of trail sets		Bundling visually aggregates curves to reduce clutter and help finding important patterns in trail-sets or graph drawings. We propose a new approach to bundling based on functional decomposition of the underling dataset. We recover the functional nature of the curves by representing them as linear combinations of piecewise-polynomial basis functions with associated expansion coefficients. Next, we express all curves in a given cluster in terms of a centroid curve and a complementary term, via a set of so-called principal component functions. Based on the above, we propose a two-fold contribution: First, we use cluster centroids to design a new bundling method for 2D and 3D curve-sets. Secondly, we deform the cluster centroids and generate new curves along them, which enables us to modify the underlying data in a statistically-controlled way via its simplified (bundled) view. We demonstrate our method by applications on real-world 2D and 3D datasets for graph bundling, trajectory analysis, and vector field and tensor field visualization.		Christophe Hurter;Stéphane Puechmorel;Florence Nicol;Alexandru Telea	2018	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2017.2744338	theoretical computer science;vector field;tensor field;principal component analysis;centroid;functional decomposition;linear combination;data modeling;basis function;mathematical optimization;mathematics	Visualization	32.29131507263985	-38.57278262894609	119282
c1d11b94c8fecaed79f97966d4bfa8381d32ad16	deep domain adaptation for face recognition using images captured from surveillance cameras		Learning based on convolutional neural networks (CNNs) or deep learning has been a major research area with applications in face recognition (FR). However, performances of algorithms designed for FR are unsatisfactory when surveillance conditions severely degrade the test probes. The work presented in this paper has three contributions. First, it proposes a novel adaptive-CNN architecture of deep learning refurbished for domain adaptation (DA), to overcome the difference in feature distributions between the gallery and probe samples. The proposed architecture consists of three components: feature (FM), adaptive (AM) and classification (CM) modules. Secondly, a novel 2-stage algorithm for Mutually Exclusive Training (2-MET) based on stochastic gradient descent, has been proposed. The final stage of training in 2-MET freezes the layers of the FM and CM, while updating (tuning) only the parameters of the AM using a few probe (as target) samples. This helps the proposed deep-DA CNN to bridge the disparities in the distributions of the gallery and probe samples, resulting in enhanced domain-invariant representation for efficient deep-DA learning and classification. The third contribution comes from rigorous experimentations performed on three benchmark real-world surveillance face datasets with various kinds of degradations. This reveals the superior performance of the proposed adaptive-CNN architecture with 2-MET training, using Rank-1 recognition rates and ROC and CMC metrics, over many recent state-of-the-art techniques of CNN and DA.		Samik Banerjee;Avishek Bhattacharjee;Sukhendu Das	2018	2018 International Conference of the Biometrics Special Interest Group (BIOSIG)	10.23919/BIOSIG.2018.8553278	convolutional neural network;artificial intelligence;task analysis;computer science;architecture;pattern recognition;deep learning;domain adaptation;stochastic gradient descent;facial recognition system;frequency modulation	Vision	25.831498914611895	-50.563038275043695	119442
a7fae7dba0db74cc21a7c7b70157fe601784b681	image denoising via cnns: an adversarial approach		Is it possible to recover an image from its noisy version using convolutional neural networks? This is an interesting problem as convolutional layers are generally used as feature detectors for tasks like classification, segmentation and object detection. We present a new CNN architecture for blind image denoising which synergically combines three architecture components, a multi-scale feature extraction layer which helps in reducing the effect of noise on feature maps, an ℓp regularizer which helps in selecting only the appropriate feature maps for the task of reconstruction, and finally a three step training approach which leverages adversarial training to give the final performance boost to the model. The proposed model shows competitive denoising performance when compared to the state-of-the-art approaches.	artificial neural network;convolutional neural network;feature extraction;iterative reconstruction;map;neural networks;noise reduction;object detection;sensor;statistical classification	Nithish Divakar;R. Venkatesh Babu	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.145	convolutional neural network;pattern recognition;iterative reconstruction;machine learning;feature (computer vision);artificial intelligence;computer vision;computer science;architecture;object detection;feature extraction;feature detection (computer vision);video denoising	Vision	25.236448440621235	-51.41094639307033	119498
bcb7ca44ba03362c79392cc5629b7fc79ed80b46	two-character motion analysis and synthesis	motion analysis;graph theory;labeling scheme;motion capture and reuse;art;image motion analysis;image segmentation;network synthesis;motion modeling;semiautomatic motion labeling scheme;bayes methods;two character motion;bayesian methods;algorithms computer graphics computer simulation humans image enhancement image interpretation computer assisted imaging three dimensional martial arts models biological movement pattern recognition automated whole body imaging;image classification;motion synthesis;layout;motion analysis art animation bayesian methods motion segmentation computer vision network synthesis layout arm leg;statistical animation;computer vision;taekwondo;coupled motion sequence;motion segmentation;dynamic bayesian network;animation;kickboxing;motion transition model;coupled motion sequence two character motion analysis standing up martial arts kickboxing karate taekwondo example based paradigm motion modeling interaction modeling motion synthesis semiautomatic motion labeling scheme force based motion segmentation learning based action classification motion transition graphs dynamic bayesian network motion transition model coupled motion transition graph;character animation;arm;two character motion analysis;motion transition graphs;interaction modeling;interaction model;standing up martial arts;signal synthesis;learning based action classification;statistical;example based paradigm;karate;learning artificial intelligence;martial art;signal synthesis bayes methods graph theory image classification image motion analysis image segmentation image sequences learning artificial intelligence;force based motion segmentation;leg;coupled motion transition graph;image sequences	In this paper, we deal with the problem of synthesizing novel motions of standing-up martial arts such as kickboxing, karate, and taekwondo performed by a pair of humanlike characters while reflecting their interactions. Adopting an example-based paradigm, we address three nontrivial issues embedded in this problem: motion modeling, interaction modeling, and motion synthesis. For the first issue, we present a semiautomatic motion-labeling scheme based on force-based motion segmentation and learning-based action classification. We also construct a pair of motion transition graphs, each of which represents an individual motion stream. For the second issue, we propose a scheme for capturing the interactions between two players. A dynamic Bayesian network is adopted to build a motion transition model on top of the coupled motion transition graph that is constructed from an example motion stream. For the last issue, we provide a scheme for synthesizing a novel sequence of coupled motions, guided by the motion transition model. Although the focus of the present work is on martial arts, we believe that the framework of the proposed approach can be conveyed to other two-player motions as well.	ball project;blast e-value;cpu cache;decorrelation;dietary nickel;discontinuous galerkin method;dynamic bayesian network;exptime;embedded system;embedding;entity name part qualifier - adopted;expectation–maximization algorithm;generalization (psychology);graph - visual representation;interaction;iterative method;liter per hour;mbnl1 gene;martial arts;motion;multinomial logistic regression;normal statistical distribution;personality character;ping (networking utility);probability;programming paradigm;sports;test set;thrombocytopenia;tracer;transformation matrix;adenotonsillectomy;biologic segmentation;decigram	Taesoo Kwon;Young-Sang Cho;Sang Il Park;Sung Yong Shin	2008	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2008.22	network synthesis filters;layout;anime;character animation;computer vision;contextual image classification;simulation;bayesian probability;computer science;graph theory;motion estimation;image segmentation;arm architecture;dynamic bayesian network;computer graphics (images)	Visualization	35.855320447414016	-45.09208052500184	119522
732b249fe884be14160e63a1e2eb82d3a1c74c89	classifying salsa dance steps from skeletal poses	histograms;data classes skeletal poses classifying salsa dance steps choreographies e learning;pose estimation computer aided instruction image motion analysis;trajectory data models games histograms visualization hidden markov models three dimensional displays;visualization;hidden markov models;trajectory;three dimensional displays;games;data models	In this paper, we explore building classifiers to detect Salsa dance step primitives in choreographies available in the Huawei 3DLife data set. These can collectively be an important component of dance tuition systems that support e-learning. A dance step is reasoned as the shortest possible extract of bodily motion that can uniquely identify a particularly repeatable movement through time. The representation of dance steps adopted is a concatenation of vectorized matrices involving the 3D coordinates of tracked body joints. Under this modeling context, a Salsa dance performance is seen as an ordered sequence of Salsa dance steps, requiring a multiple of the variables allocated in the representation of a single step. Following a previous work by Masurelle & Essid that discusses the classification of six Salsa dance steps from 3DLife, we show that it is possible to obtain better classifiers under a similar experimental protocol in terms of both test accuracy and F-measure. By carefully re-annotating the data in 3DLife, we refocus on the six-step classification problem and then extend the protocol to the case of 20 dance steps. In comparison to common classifiers of the trade operating on full-dimensions, we show that it is possible to produce more accurate models by computing a subspace of the data. At the same time it is possible to reduce problematic bias in resulting models due to the uneven distribution of samples across step data classes. We provide and discuss experimental findings to support both hypotheses for the two experimental settings.	concatenation;experiment;f1 score;occam's razor;preprocessor;real-time transcription;salsa;v-optimal histograms	Sotiris Karavarsamis;Dimitrios Ververidis;Giannis K. Chantas;Spiros Nikolopoulos;Yiannis Kompatsiaris	2016	2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI)	10.1109/CBMI.2016.7500244	games;data modeling;computer vision;simulation;visualization;computer science;trajectory;machine learning;histogram;multimedia;world wide web;hidden markov model;statistics;computer graphics (images)	ML	36.29117617809872	-47.06815509601894	119899
4cde85d5eb60f99adee97befcceb5f2eba6c9a59	robust facial expression recognition algorithm based on local metric learning	databases;matrices;algorithms;facial recognition systems	In facial expression recognition tasks, different facial expressions are often confused with each other. Motivated by the fact that a learned metric can significantly improve the accuracy of classification, a facial expression recognition algorithm based on local metric learning is proposed. First, k -nearest neighbors of the given testing sample are determined from the total training data. Second, chunklets are selected from the k -nearest neighbors. Finally, the optimal transformation matrix is computed by maximizing the total variance between different chunklets and minimizing the total variance of instances in the same chunklet. The proposed algorithm can find the suitable distance metric for every testing sample and improve the performance on facial expression recognition. Furthermore, the proposed algorithm can be used for vector-based and matrix-based facial expression recognition. Experimental results demonstrate that the proposed algorithm could achieve higher recognition rates and be more robust than baseline algorithms on the JAFFE, CK, and RaFD databases.	algorithm	Bin Jiang;Kebin Jia	2016	J. Electronic Imaging	10.1117/1.JEI.25.1.013022	computer vision;computer science;machine learning;pattern recognition;three-dimensional face recognition;algorithm;matrix	Vision	26.409182730017054	-43.56972171501426	119904
8a7987ce058b368771a8c4e8abbd5fbf9fa04d31	spectrum sensing based on blindly learned signal feature	learning algorithm;signal sampling;prior knowledge;cognitive radio;local features;spectrum sensing;signal to noise ratio;template matching;information theory;eigenvectors	Spectrum sensing is the major challenge in the cognitive radio (CR). We propose to learn local feature and use it as the prior knowledge to improve the detection performance. We define the local feature as the leading eigenvector derived from the received signal samples. A feature learning algorithm (FLA) is proposed to learn the feature blindly. Then, with local feature as the prior knowledge, we propose the feature template matching algorithm (FTM) for spectrum sensing. We use the discrete Karhunen–Loève transform (DKLT) to show that such a feature is robust against noise and has maximum effective signalto-noise ratio (SNR). Captured real-world data shows that the learned feature is very stable over time. It is almost unchanged in 25 seconds. Then, we test the detection performance of the FTM in very low SNR. Simulation results show that the FTM is about 2 dB better than the blind algorithms, and the FTM does not have the noise uncertainty problem.	algorithm;approximation;cognitive radio;decibel;feature learning;kernel principal component analysis;nonlinear dimensionality reduction;nonlinear system;signal-to-noise ratio;simulation;template matching	Peng Zhang;Robert Caiming Qiu	2011	CoRR		cognitive radio;speech recognition;template matching;information theory;eigenvalues and eigenvectors;computer science;machine learning;pattern recognition;mathematics;signal-to-noise ratio;feature;statistics	ML	27.801592458385123	-42.077206888772785	120488
52d9850392907347be8d61b42a43f79377fc5c80	submodular object recognition	g400 computer science;object recognition;image segmentation object recognition entropy linear programming training benchmark testing layout;regression analysis image segmentation object recognition;regression;submodularity;cpmc;regression object recognition submodularity cpmc;ethz shape submodular object recognition multiple figure ground hypothesis large object spatial support unsupervised manner discriminating segments overlapping observations figure ground segment hypothesis ground truth image target category submodular objective function facility locations group elements maximum regression values category specific regressors impressive recognition benchmark datasets pascal voc 2007 caltech 101	We present a novel object recognition framework based on multiple figure-ground hypotheses with a large object spatial support, generated by bottom-up processes and mid-level cues in an unsupervised manner. We exploit the benefit of regression for discriminating segments' categories and qualities, where a regressor is trained to each category using the overlapping observations between each figure-ground segment hypothesis and the ground-truth of the target category in an image. Object recognition is achieved by maximizing a submodular objective function, which maximizes the similarities between the selected segments (i.e., facility locations) and their group elements (i.e., clients), penalizes the number of selected segments, and more importantly, encourages the consistency of object categories corresponding to maximum regression values from different category-specific regressors for the selected segments. The proposed framework achieves impressive recognition results on three benchmark datasets, including PASCAL VOC 2007, Caltech-101 and ETHZ-shape.	benchmark (computing);bottom-up parsing;caltech 101;discriminative model;facility location problem;greedy algorithm;loss function;object detection;optimization problem;outline of object recognition;pure function;submodular set function;unsupervised learning	Fan Zhu;Zhuolin Jiang;Ling Shao	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.315	computer vision;regression;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics;3d single-object recognition	Vision	27.81583095398164	-47.12747679222016	120650
a943fc93201dcb37940fa0428d98a5c55fb4dd35	unsupervised facial geometry learning for sketch to photo synthesis		Face sketch-photo synthesis is a critical application in law enforcement and digital entertainment industry where the goal is to learn the mapping between a face sketch image and its corresponding photo-realistic image. However, the limited number of paired sketch-photo training data usually prevents the current frameworks to learn a robust mapping between the geometry of sketches and their matching photo-realistic images. Consequently, in this work, we present an approach for learning to synthesize a photo-realistic image from a face sketch in an unsupervised fashion. In contrast to current unsupervised image-to-image translation techniques, our framework leverages a novel perceptual discriminator to learn the geometry of human face. Learning racial prior information empowers the network to remove the geometrical artifacts in the face sketch. We demonstrate that a simultaneous optimization of the face photo generator network, employing the proposed perceptual discriminator in combination with a texture-wise discriminator, results in a significant improvement in quality and recognition rate of the synthesized photos. We evaluate the proposed network by conducting extensive experiments on multiple baseline sketch-photo datasets.		Hadi Kazemi;Fariborz Taherkhani;Nasser M. Nasrabadi	2018	2018 International Conference of the Biometrics Special Interest Group (BIOSIG)	10.23919/BIOSIG.2018.8552937	machine learning;computer vision;computer science;multiple baseline design;discriminator;training set;sketch;perception;artificial intelligence;geometry	Vision	25.758089030265804	-49.69791217876351	120799
274959f26d04848f71a355c09500fd7ebc271d69	two-stream flow-guided convolutional attention networks for action recognition		This paper proposes a two-stream flow-guided convolutional attention networks for action recognition in videos. The central idea is that optical flows, when properly compensated for the camera motion, can be used to guide attention to the human foreground. We thus develop crosslink layers from the temporal network (trained on flows) to the spatial network (trained on RGB frames). These crosslink layers guide the spatial-stream to pay more attention to the human foreground areas and be less affected by background clutter. We obtain promising performances with our approach on the UCF101, HMDB51 and Hollywood2 datasets.	clutter;flash crowds alleviation network;performance;recurrent neural network;spatial network	An Tran;Loong Fah Cheong	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.368	computer vision;optical imaging;artificial intelligence;clutter;feature extraction;pattern recognition;computer science;rgb color model;spatial network;convolution	Vision	28.326092945650196	-51.745702588302834	120807
ae936628e78db4edb8e66853f59433b8cc83594f	prism: person reidentification via structured matching	measurement;training;testing;co occurrence of visual patterns person re identification structured learning weighted bipartite matching;visualization;image color analysis;lighting;cameras;cameras visualization training testing measurement image color analysis lighting	Person reidentification (re-id), an emerging problem in visual surveillance, deals with maintaining the identities of individuals while they traverse various locations surveilled by a camera network. Motivated by real-world scenarios, we propose a method that seeks to simultaneously identify who among a group of individuals viewed in one view are present/absent in the other. From a visual perspective, re-id is challenging due to significant changes in visual appearance of individuals in cameras with different pose, illumination, and calibration. Globally, the challenge arises from the need to maintain structurally consistent matches among all the individual entities across different camera views. We propose person re-id via structured matching (PRISM), an SM method to jointly account for these challenges. We view the global problem as a weighted graph matching problem and estimate edge weights by learning to predict them based on the co-occurrences of visual patterns in the training examples. These co-occurrence-based scores in turn account for appearance changes by inferring likely and unlikely visual co-occurrences appearing in training instances. We implement PRISM on single-shot and multishot scenarios. PRISM uniformly outperforms state of the art in terms of matching rate while being robust and computationally efficient.	algorithmic efficiency;entity;matching (graph theory);prism (surveillance program);proxy re-encryption;traverse	Ziming Zhang;Venkatesh Saligrama	2017	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2016.2596159	computer vision;simulation;visualization;computer science;machine learning;lighting;software testing;measurement	Vision	33.777672543169956	-48.666282140463004	120832
8a2d242d4c72438498ff54740144675065a2ecad	multi-view distributed coding and selection of local binary features	object recognition data visualisation feature selection image coding image representation;image coding;uncompressed descriptor rate local binary feature selection multiview distributed coding feature representation feature learning visual analysis object recognition accuracy distributed visual analysis camera sensor quantized representation visual feature descriptors decoder joint statistics multiview feature coding multiview feature selection bitrate reductions;distributed visual analysis feature selection feature coding distributed coding;decoding;feature coding;distributed coding;visualization;distributed visual analysis;feature extraction;feature selection;feature extraction cameras decoding image coding visualization encoding correlation;correlation;encoding;cameras	Recently, the latest advances in compact feature representation and feature learning have provided an efficient framework for several visual analysis tasks, such as object recognition. However, when multiple cameras with overlapping fields-of-view are employed, other visual analysis tasks such as depth estimation can be supported and object recognition accuracy can be improved. In this paper the problem of distributed visual analysis from multiple views of a scene is addressed, considering that computational power and bandwidth, at each camera sensor, are rather limited. More specifically, an efficient coding technique for local binary features is proposed which exploits the correlation at the decoder side between each descriptor and its quantized representation. Moreover, considering that descriptors representing the same visual feature across different views are well correlated, a technique to avoid the transmission of redundant descriptors from multiple views is proposed. At the decoder, the joint statistics of all descriptors from all views is used to drive the selection of the best descriptors to be transmitted by each sensing node. The proposed multi-view feature coding and selection techniques allow obtaining bitrate reductions up to 80%, with respect to the uncompressed descriptor rate, for a certain task accuracy.	feature learning;feature model;image sensor;outline of object recognition	Nuno Monteiro;Catarina Brites;Fernando da Cruz Pereira;João Ascenso	2016	2016 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2016.7552995	feature learning;computer vision;visualization;feature extraction;computer science;machine learning;pattern recognition;feature selection;correlation;feature;encoding;dimensionality reduction	Vision	33.011079004826506	-49.48068373622453	120896
43f69e3a7a8bbd12ea9b53ac998879580b95dc54	a visual domain adaptation method based on enhanced subspace distribution matching		One of the challenges in computer vision is how to learn an accurate classifier for a new domain by using labeled images from an old domain under the condition that there is no available labeled images in the new domain. Domain adaptation is an outstanding solution that tackles this challenge by employing available source-labeled datasets, even with significant difference in distribution and properties. However, most prior methods only reduce the difference in subspace marginal or conditional distributions across domains while completely ignoring the source data label dependence information in a subspace. In this paper, we put forward a novel domain adaptation approach, referred to as Enhanced Subspace Distribution Matching. Specifically, it aims to jointly match the marginal and conditional distributions in a kernel principal dimensionality reduction procedure while maximizing the source label dependence in a subspace, thus raising the subspace distribution matching degree. Extensive experiments verify that it can significantly outperform several state-of-the-art methods for cross-domain image classification problems.	computer vision;dimensionality reduction;domain adaptation;experiment;generative adversarial networks;kernel (operating system);marginal model;source data	Kai Zhang;Qi Kang;Xuesong Wang;Mengchu Zhou;Sisi Li	2018	2018 IEEE 15th International Conference on Networking, Sensing and Control (ICNSC)	10.1109/ICNSC.2018.8361269	kernel (linear algebra);source data;dimensionality reduction;domain adaptation;control theory;subspace topology;contextual image classification;conditional probability distribution;computer science;pattern recognition;artificial intelligence	Vision	25.449402477751278	-44.921664626793	121054
5f9253997ce7ad510989e58c5c2b98e5af017fb0	add-on strategies for fine-grained pedestrian classification	training;layout;graphical models;feature extraction;robustness;spatial resolution	In this paper, we present four add-on strategies for the fine-grained pedestrian classification task. These strategies are: (1) super-resolution based image preprocessing, which helps to recover the image details; (2) patch dividing based deep feature extraction, which extracts features in a way that preserves the spatial layout of input images; (3) pose- wise classifier sharing, which learns robust classifiers and makes robust predictions using pose information; and (4) graphical model based inference, which utilizes the interdependence between different subcategories to update raw estimations. The proposed strategies are independent and flexible, which make it easy to implement them in practice. We evaluated these strategies on the CRP dataset and confirmed that all of them lead to improvements over the baseline. We also confirmed an improvement over the state-of-the-art when all strategies are combined together.	add-ons for firefox;baseline (configuration management);feature extraction;graphical model;interdependence;pipeline (computing);preprocessor;scalability;super-resolution imaging	Yoshihito Kokubo;Yu Wang;Jien Kato;Guanwen Zhang;Kenji Mase	2016	2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2016.7797001	layout;computer vision;image resolution;feature extraction;computer science;machine learning;pattern recognition;data mining;graphical model;statistics;robustness	Robotics	29.140778403264907	-50.0173781752056	121118
0cf6a3720946c02518b557953b0eba92e16f8191	tensr: multi-dimensional tensor sparse representation	computer vision tensr multidimensional tensor sparse representation data representation conventional sparse model sparse linear combination multidimensional signals md signals computational resources md sparse coding md dictionary learning algorithms adaptive separable structure dictionaries competitive performance memory costs computational costs;tensors computer vision data structures image representation learning artificial intelligence;dictionaries tensile stress encoding two dimensional displays computational modeling signal processing algorithms sparse matrices	The conventional sparse model relies on data representation in the form of vectors. It represents the vector-valued or vectorized one dimensional (1D) version of an signal as a highly sparse linear combination of basis atoms from a large dictionary. The 1D modeling, though simple, ignores the inherent structure and breaks the local correlation inside multidimensional (MD) signals. It also dramatically increases the demand of memory as well as computational resources especially when dealing with high dimensional signals. In this paper, we propose a new sparse model TenSR based on tensor for MD data representation along with the corresponding MD sparse coding and MD dictionary learning algorithms. The proposed TenSR model is able to well approximate the structure in each mode inherent in MD signals with a series of adaptive separable structure dictionaries via dictionary learning. The proposed MD sparse coding algorithm by proximal method further reduces the computational cost significantly. Experimental results with real world MD signals, i.e. 3D Multi-spectral images, show the proposed TenSR greatly reduces both the computational and memory costs with competitive performance in comparison with the state-of-the-art sparse representation methods. We believe our proposed TenSR model is a promising way to empower the sparse representation especially for large scale high order signals.	algorithmic efficiency;approximation algorithm;coefficient;computation;computational resource;data (computing);dictionary;machine learning;molecular dynamics;multispectral image;neural coding;noise reduction;quantum nonlocality;sparse approximation;sparse matrix;time complexity	Na Qi;Yunhui Shi;Xiaoyan Sun;Baocai Yin	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.637	speech recognition;k-svd;computer science;theoretical computer science;machine learning;sparse approximation	Vision	28.642887101429793	-40.414612556862245	121331
978716708762dab46e91059e170d43551be74732	a pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking		Person re-identification is a challenging retrieval task that requires matching a person's acquired image across non-overlapping camera views. In this paper we propose an effective approach that incorporates both the fine and coarse pose information of the person to learn a discriminative embedding. In contrast to the recent direction of explicitly modeling body parts or correcting for misalignment based on these, we show that a rather straightforward inclusion of acquired camera view and/or the detected joint locations into a convolutional neural network helps to learn a very effective representation. To increase retrieval performance, re-ranking techniques based on computed distances have recently gained much attention. We propose a new unsupervised and automatic re-ranking framework that achieves state-of-the-art re-ranking performance. We show that in contrast to the current state-of-the-art re-ranking methods our approach does not require to compute new rank lists for each image pair (e.g., based on reciprocal neighbors) and performs well by using simple direct rank list based comparison or even by just using the already computed euclidean distances between the images. We show that both our learned representation and our re-ranking method achieve state-of-the-art performance on a number of challenging surveillance image and video datasets. Code is available at https://github.com/pse-ecn.		M. Saquib Sarfraz;Arne Schumann;Andreas Eberle;Rainer Stiefelhagen	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00051	convolutional neural network;discriminative model;computer vision;visualization;machine learning;pattern recognition;artificial intelligence;computer science;reciprocal;ranking;embedding	Vision	31.911964320787703	-51.49050882060683	121356
f17e93c1a87a18c9709bf44127950d3e1390dc9e	multiview learning via deep discriminative canonical correlation analysis	deep cca;cca;discriminative;transforms correlation methods emotion recognition learning artificial intelligence speech recognition;cca ddcca deep cca discriminative cca;speech based emotion recognition multiview learning deep discriminative canonical correlation analysis ddcca nonlinear transformation within class correlation maximization interclass correlation minimization handwritten digit recognition;decision support systems computer vision indexes handheld computers pattern recognition multimedia communication ieee multimedia;ddcca	In this paper, we propose Deep Discriminative Canonical Correlation Analysis (DDCCA), a method to learn the nonlinear transformation of two data sets such that the within-class correlation is maximized and the inter-class correlation is minimized. Parameters of the two deep transformations are jointly learned. Unlike CCA and Discriminative CCA, the proposed DDCCA does not need inner product. The proposed DDCCA was evaluated in two applications, handwritten digit recognition and speech-based emotion recognition. The experimental results demonstrated that the proposed DDCCA can get a higher recognition accuracy compared to the existing Deep CCA method.	emotion recognition;nonlinear system	Nour El-Din El-Madany;Yifeng He;Ling Guan	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472109	speech recognition;computer science;machine learning;pattern recognition;discriminative model	Vision	25.791925165631216	-42.6654310218068	121376
7179ba274b27a0969092e9a0f2131167856f913f	discriminant manifold learning via sparse coding for image analysis		Traditional subspace learning methods directly calculate the statistical properties of the original input images, while ignoring different contributions of different image components. In fact, the noise (e.g., illumination, shadow) in the image often has a negative influence on learning the desired subspace and should have little contribution to image recognition. To tackle this problem, we propose a novel subspace learning method named Discriminant Manifold Learning via Sparse Coding (DML_SC). In our method, we first decompose the input image into several components via dictionary learning, and then regroup the components into a More Important Part (MIP) and a Less Important Part (LIP). The MIP can be regarded as the clean part of the original image residing on a nonlinear submanifold, while LIP as noise in the image. Finally, the MIP and LIP are incorporated into manifold learning to learn a desired discriminative subspace. The proposed method is able to deal with data with and without labels, yielding supervised and unsupervised DML SCs. Experimental results show that DML_SC achieves best performance on image recognition and clustering tasks compared with well-known subspace learning and sparse representation methods.	discriminant;image analysis;neural coding;nonlinear dimensionality reduction;sparse	Meng Pang;Binghui Wang;Xin Fan;Chuang Lin	2016		10.1007/978-3-319-27674-8_22	machine learning;pattern recognition;sparse approximation;multiple discriminant analysis;manifold alignment	AI	26.54871188218018	-43.408374366862375	121405
de7b815a83de8496a3bd384b3caa54352a85cbaa	similarity learning for object recognition based on derived kernel	object recognition;derived kernel;template selection;journal;hierarchical learning;neural response;image similarity	Recently, derived kernel method which is a hierarchical learning method and leads to an effective similarity measure has been proposed by Smale. It can be used in a variety of application domains such as object recognition, text categorization and classification of genomic data. The templates involved in the construction of the derived kernel play an important role. To learn more effective similarity reduced and the label information of the training images is used. In this way, the proposed method can obtain compact template sets with better discrimination ability. Experiments on four standard databases show that the derived kernel based on the proposed method achieves high accuracy with low computational complexity. & 2012 Elsevier B.V. All rights reserved.	cbcl (mit);categorization;computation;computational complexity theory;database;document classification;experiment;kernel (operating system);kernel method;linear discriminant analysis;machine learning;non-negative matrix factorization;nonlinear dimensionality reduction;outline of object recognition;similarity learning;similarity measure	Hong Li;Yantao Wei;Luoqing Li;Yuan Yuan	2012	Neurocomputing	10.1016/j.neucom.2011.12.005	kernel method;string kernel;kernel embedding of distributions;radial basis function kernel;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;data mining;tree kernel;polynomial kernel	AI	24.979691556457066	-41.60320283707543	121432
3ff8b59021fc4652ae59af4db805c27a1ee40ee2	learning to reconstruct 3d structure from object motion		In this paper, we propose a new approach for reconstructing 3D structure from motion parallax. Instead of obtaining 3D structure from multi-view geometry or factorization, a Deep Neural Network (DNN) based method is proposed without assuming the camera model explicitly. In the proposed method, the targets are first split into connected 3D corners, and then the DNN regressor is trained to estimate the relative 3D structure of each corner from the target rotation. Finally, a temporal integration is performed to further improve the reconstruction accuracy. The effectiveness of the method is proved by a typical experiment of the Kinetic Depth Effect (KDE) in human visual system, in which the DNN regressor reconstructs the structure of a rotating 3D bent wire. The proposed method is also applied to reconstruct another two real targets. Experimental results on both synthetic and real images show that the proposed method is accurate and effective.		Wentao Liu;Haobin Dou;Xihong Wu	2015		10.1007/978-3-319-26532-2_15	computer vision;structure from motion;machine learning;pattern recognition	Robotics	28.70107997938428	-45.677771600707395	121519
e357d28c92dc436ce60201adfae2523a3132a84e	uncorrelated feature encoding for faster image style transfer		Recent fast style transfer methods use a pre-trained convolutional neural network as a feature encoder and a perceptual loss network. Although the pre-trained network is used to generate responses of receptive fields effective for representing style and content of image, it is not optimized for image style transfer but rather for image classification. Furthermore, it also requires a time-consuming and correlation-considering feature alignment process for image style transfer because of its interchannel correlation. In this paper, we propose an end-to-end learning method which optimizes an encoder/decoder network for the purpose of style transfer as well as relieves the feature alignment complexity from considering inter-channel correlation. We used uncorrelation loss, i.e., the total correlation coefficient between the responses of different encoder channels, with style and content losses for training style transfer network. This makes the encoder network to be trained to generate interchannel uncorrelated features and to be optimized for the task of image style transfer which maintained the quality of image style only with a light-weighted and correlation-unaware feature alignment process. Moreover, our method drastically reduced redundant channels of the encoded feature and this resulted in the efficient size of structure of network and faster forward processing speed. Our method can also be applied to cascade network scheme for multiple scaled style transferring and allows user-control of style strength by using a content-style trade-off parameter.	algorithmic efficiency;artificial neural network;coefficient;computation;computer vision;convolutional neural network;elegant degradation;encoder;end-to-end principle;experiment;image quality;methods of computing square roots;real-time transcription;total correlation;transformer	Minseong Kim;Jongju Shin;Myung-Cheol Roh;Hyun-Chul Choi	2018	CoRR		artificial intelligence;convolutional neural network;encoder;encoding (memory);pattern recognition;machine learning;receptive field;loss network;total correlation;computer science;communication channel;contextual image classification	Vision	25.21203852440787	-50.59268695179588	121520
75b1790ffcf51489fcfbf14b11f1b90a076345cc	a coarse-fine network for keypoint localization		We propose a coarse-fine network (CFN) that exploits multi-level supervisions for keypoint localization. Recently, convolutional neural networks (CNNs)-based methods have achieved great success due to the powerful hierarchical features in CNNs. These methods typically use confidence maps generated from ground-truth keypoint locations as supervisory signals. However, while some keypoints can be easily located with high accuracy, many of them are hard to localize due to appearance ambiguity. Thus, using strict supervision often fails to detect keypoints that are difficult to locate accurately To target this problem, we develop a keypoint localization network composed of several coarse detector branches, each of which is built on top of a feature layer in a CNN, and a fine detector branch built on top of multiple feature layers. We supervise each branch by a specified label map to explicate a certain supervision strictness level. All the branches are unified principally to produce the final accurate keypoint locations. We demonstrate the efficacy, efficiency, and generality of our method on several benchmarks for multiple tasks including bird part localization and human body pose estimation. Especially, our method achieves 72.2% AP on the 2016 COCO Keypoints Challenge dataset, which is an 18% improvement over the winning entry.	3d pose estimation;artificial neural network;benchmark (computing);canadian football network;convolutional neural network;feature learning;ground truth;human-based computation;internationalization and localization;map;sensor	Shaoli Huang;Mingming Gong;Dacheng Tao	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.329	artificial intelligence;generality;convolutional neural network;computer vision;pattern recognition;computer science;pose;feature extraction;exploit;coco;ambiguity	Vision	29.356072141755877	-50.51962690354345	121597
84e48da837978a0660184a0df7647e4b22b078e7	low rank subspace clustering (lrsc)	000 computer science knowledge systems;motion segmentation;510 mathematics;principal component analysis;subspace clustering;face clustering;low rank and sparse methods	We consider the problem of fitting a union of subspaces to a collection of data points drawn from one or more subspaces and corrupted by noise and/or gross errors. We pose this problem as a non-convex optimization problem, where the goal is to decompose the corrupted data matrix as the sum of a clean and self-expressive dictionary plus a matrix of noise and/or gross errors. By self-expressive we mean a dictionary whose atoms can be expressed as linear combinations of themselves with low-rank coefficients. In the case of noisy data, our key contribution is to show that this non-convex matrix decomposition problem can be solved in closed form from the SVD of the noisy data matrix. The solution involves a novel polynomial thresholding operator on the singular values of the data matrix, which requires minimal shrinkage. For one subspace, a particular case of our framework leads to classical PCA, which requires no shrinkage. For multiple subspaces, the low-rank coefficients obtained by our framework can be used to construct a data affinity matrix from which the clustering of the data according to the subspaces can be obtained by spectral clustering. In the case of data corrupted by gross errors, we solve the problem using an alternating minimization approach, which combines our polynomial thresholding operator with the more traditional shrinkage-thresholding operator. Experiments on motion segmentation and face clustering show that our framework performs on par with state-of-the-art techniques at a reduced computational cost. ! 2013 Elsevier B.V. All rights reserved.	algorithmic efficiency;cluster analysis;clustering high-dimensional data;coefficient;convex optimization;data point;dictionary;mathematical optimization;optimization problem;polynomial;principal component analysis;processor affinity;signal-to-noise ratio;singular value decomposition;spectral clustering;thresholding (image processing)	René Vidal;Paolo Favaro	2014	Pattern Recognition Letters	10.1016/j.patrec.2013.08.006	mathematical optimization;computer science;machine learning;pattern recognition;mathematics;cluster analysis;statistics;principal component analysis;clustering high-dimensional data	ML	26.60840226259156	-38.58625463304737	121766
f11f460d0297fb1de91e5a63dfbaa721af5b6a70	improving 2d face recognition via discriminative face depth estimation		As face recognition progresses from constrained scenarios to unconstrained scenarios, new challenges such as large pose, bad illumination, and partial occlusion, are encountered. While 3D or multi-modality RGB-D sensors are helpful for face recognition systems to achieve robustness against these challenges, the requirement of new sensors limits their application scenarios. In our paper, we propose a discriminative face depth estimation approach to improve 2D face recognition accuracies under unconstrained scenarios. Our discriminative depth estimation method uses a cascaded FCN and CNN architecture, in which FCN aims at recovering the depth from an RGB image, and CNN retains the separability of individual subjects. The estimated depth information is then used as a complementary modality to RGB for face recognition tasks. Experiments on two public datasets and a dataset we collect show that the proposed face recognition method using RGB and estimated depth information can achieve better accuracy than using RGB modality alone.	end-to-end principle;facial recognition system;information theory;linear separability;liveness;modality (human–computer interaction);sensor	Jiyun Cui;Hao Zhang;Hu Han;Shiguang Shan;Xilin Chen	2018	2018 International Conference on Biometrics (ICB)	10.1109/ICB2018.2018.00031	computer science;pattern recognition;robustness (computer science);facial recognition system;computer vision;architecture;artificial intelligence;rgb color model;discriminative model	Vision	29.227104228084606	-50.52706778384075	122321
0e0390caa0c138363d50c1657916e21015348030	recognition from hand cameras		We propose HandCam (Fig. 1), a novel wearable camera capturing activities of hands, for recognizing human behaviors. HandCam has two main advantages over egocentric systems [10, 5, 23]: (1) it avoids the need to detect hands and manipulation regions; (2) it observes the activities of hands almost at all time. These nice properties enable HandCam to recognize hand states (free v.s. active hands, hand gestures, object categories), and discover object categories through interaction with hands. We propose a new method to accomplish these tasks by fusing per-frame state prediction with state changes estimation from a pair of frames. We have collected one of the first HandCam dataset with 20 videos captured in three scenes. Experiments show that HandCam system significantly outperforms a state-ofthe-art HeadCam system (frame-based re-implementation similar to [17]) in all tasks by at most 10.8% improvement in accuracy. We also show that HandCam videos captured by different users can be easily aligned to improve free v.s. active recognition accuracy (3.3% improvement) in crossscenes use case. Moreover, we observe that finetuning Convolutional Neural Network [14] consistently improves accuracy (at most 4.9% better). Finally, a method combining HandCam and HeadCam features achieves the best performance. With more data, we believe a joint HandCam and HeadCam system can robustly log hand states in daily life.	algorithmic efficiency;baseline (configuration management);convolution;convolutional neural network;dvd region code;ground truth;mean shift;minimum bounding box;performance;softmax function;unary operation;wearable computer	Cheng-Sheng Chan;Shou-Zhong Chen;Pei-Xuan Xie;Chiung-Chih Chang;Min Sun	2015	CoRR		computer vision;simulation;computer science	Vision	34.34215900531486	-49.2970393717004	122698
cb378b0d7fde83059dc25e2a3e54942529bb7dd1	video summarization based on extracted key position of spotted objects	surveillance;data mining;acceleration;indexes;video surveillance image fusion;feature extraction;acceleration indexes feature extraction conferences data mining algorithm design and analysis surveillance;algorithm design and analysis;conferences;motion pattern video summarization novel fusion method video surveillance key position extraction	This paper proposes a novel fusion method for summarization of surveillance videos based on extracted key positions of spotted objects in observed area. Accumulated energy of object is calculated by analyzing its motion pattern for key position extraction. The method allows to summarize long videos into a single index frame. Experimental results demonstrate its effectiveness.	single-index model	Zaur Fataliyev;David K. Han;Yadigar Imamverdiyev;Hanseok Ko	2015	2015 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2015.7066433	acceleration;database index;algorithm design;computer vision;feature extraction;computer science;automatic summarization;data mining;information retrieval	Robotics	39.160365857456746	-51.11843757540572	122711
0886b3146ed32118a2c3899cfe39d4a68c5555c5	classifier grids for robust adaptive object detection	data mining;computer vision;learning artificial intelligence;image classification;layout;robustness;sliding window;computer graphics;feature extraction;false alarm rate;detectors;boosting;discriminative model;mesh generation	In this paper we present an adaptive but robust object detector for static cameras by introducing classifier grids. Instead of using a sliding window for object detection we propose to train a separate classifier for each image location, obtaining a very specific object detector with a low false alarm rate. For each classifier corresponding to a grid element we estimate two generative representations in parallel, one describing the object's class and one describing the background. These are combined in order to obtain a discriminative model. To enable to adapt to changing environments these classifiers are learned on-line (i.e., boosting). Continuously learning (24 hours a day, 7 days a week) requires a stable system. In our method this is ensured by a fixed object representation while updating only the representation of the background. We demonstrate the stability in a long-term experiment by running the system for a whole week, which shows a stable performance over time. In addition, we compare the proposed approach to state-of-the-art methods in the field of person and car detection. In both cases we obtain competitive results.	boosting (machine learning);discriminative model;feature selection;object detection;online and offline;statistical classification	Peter M. Roth;Sabine Sternig;Helmut Grabner;Horst Bischof	2009	2009 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPRW.2009.5206616	sliding window protocol;computer vision;detector;contextual image classification;feature extraction;boosting methods for object categorization;computer science;viola–jones object detection framework;machine learning;pattern recognition;constant false alarm rate;boosting;discriminative model;robustness	Vision	37.510728449386036	-47.10547010358004	122794
a380817fb24ff67ae988d4cb2326573f6b7ccb02	linear dimensionality reduction using relevance weighted lda	mahalanobis distance;chernoff criterion;linear discriminate analysis;weighted lda;feature extraction;evolution strategies;scattering matrix;evolution strategy;linear discriminant analysis;dimensional reduction;approximate pairwise accuracy criterion	The linear discriminant analysis (LDA) is one of the most traditional linear dimensionality reduction methods. This paper incorporates the inter-class relationships as relevance weights into the estimation of the overall within-class scatter matrix in order to improve the performance of the basic LDA method and some of its improved variants. We demonstrate that in some specific situations the standard multi-class LDA almost totally fails to find a discriminative subspace if the proposed relevance weights are not incorporated. In order to estimate the relevance weights of individual within-class scatter matrices, we propose several methods of which one employs the evolution strategies. 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	algorithm;dimensionality reduction;evolution strategy;linear discriminant analysis;mathematical optimization;pattern recognition;relevance	E. Ke Tang;Ponnuthurai Nagaratnam Suganthan;Xin Yao;A. Kai Qin	2005	Pattern Recognition	10.1016/j.patcog.2004.09.005	s-matrix;feature extraction;computer science;mahalanobis distance;machine learning;pattern recognition;mathematics;evolution strategy;linear discriminant analysis;statistics	Vision	24.783518657545997	-40.7074564842169	122899
9bd8b7645549a62e4914db43c9cec59fcd7418f3	a shape-based approach for salient object detection using deep learning	pharmacy;levitra;mastercard;canadian;cheapest;cialis;tadalafil;online	Salient object detection is a key step in many image analysis tasks as it not only identifies relevant parts of a visual scene but may also reduce computational complexity by filtering out irrelevant segments of the scene. In this paper, we propose a novel salient object detection method that combines a shape prediction driven by a convolutional neural network with the mid and low-region preserving image information. Our model learns a shape of a salient object using a CNN model for a target region and estimates the full but coarse saliency map of the target image. The map is then refined using image specific low-to-mid level information. Experimental results show that the proposed method outperforms previous state-of-the-arts methods in salient object detection.	deep learning;object detection	Jongpil Kim;Vladimir Pavlovic	2016		10.1007/978-3-319-46493-0_28	simulation;multimedia;pharmacy	Vision	31.97793500029574	-51.23088445753378	122906
e13360cda1ebd6fa5c3f3386c0862f292e4dbee4	range loss for deep face recognition with long-tailed training data		Deep convolutional neural networks have achieved significant improvements on face recognition task due to their ability to learn highly discriminative features from tremendous amounts of face images. Many large scale face datasets exhibit long-tail distribution where a small number of entities (persons) have large number of face images while a large number of persons only have very few face samples (long tail). Most of the existing works alleviate this problem by simply cutting the tailed data and only keep identities with enough number of examples. Unlike these work, this paper investigated how long-tailed data impact the training of face CNNs and develop a novel loss function, called range loss, to effectively utilize the tailed data in training process. More specifically, range loss is designed to reduce overall intrapersonal variations while enlarge interpersonal differences simultaneously. Extensive experiments on two face recognition benchmarks, Labeled Faces in the Wild (LFW) [11] and YouTube Faces (YTF) [33], demonstrate the effectiveness of the proposed range loss in overcoming the long tail effect, and show the good generalization ability of the proposed methods.	academy;artificial neural network;baseline (configuration management);bayesian information criterion;convolutional neural network;entity;experiment;facial recognition system;javaserver faces;long tail;loss function	Xiao Zhang;Zhiyuan Fang;Yandong Wen;Zhifeng Li;Yu Qiao	2016	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.578	convolutional neural network;artificial intelligence;discriminative model;data modeling;facial recognition system;pattern recognition;computer science;intrapersonal communication;training set;small number	Vision	27.1114774121092	-47.948131733633645	123002
e61a3c757a35a7d684a239d6ac71c5eaa9de4e78	spectral magnitude and spectral derivative feature fusion for improved classification of hyperspectral images	svm classification;support vector machines feature extraction geophysical techniques geophysics computing image classification image fusion principal component analysis remote sensing;support vector machine hyperspectral images feature fusion spectral derivative features principle component analysis;support vector machines;hyperspectral images;image fusion;image classification;hyperspectral sensors;data mining;feature extraction method;accuracy;geophysics computing;feature extraction;signal processing;principal component analysis;remote sensing;hyperspectral data;dimensionality reduction algorithm spectral derivative feature hyperspectral image classification image fusion fusion spectral magnitude feature principle component analysis feature extraction method support vector machine svm classification;support vector machine classification;feature fusion;principle component analysis;support vector machine;hyperspectral imaging;classification accuracy;spectral derivative features;spectral derivative feature;hyperspectral image classification;hyperspectral image;hyperspectral imaging hyperspectral sensors feature extraction principal component analysis support vector machines support vector machine classification data mining laboratories signal processing telecommunications;geophysical techniques;telecommunications;dimensionality reduction algorithm;fusion spectral magnitude feature	This paper proposes to increase the classification accuracy of hyperspectral images by fusing spectral magnitude features and spectral derivative features. Principle component analysis (PCA) is used as feature extraction method to reduce the final number of features of the hyperspectral data before feature fusion. PCA is applied separately to magnitude and derivative features to determine significant components of each. Different fusion approaches of the significant components of magnitude features and the significant components of the first as well as second spectral derivatives are evaluated to construct the desired number of final features. Support vector machine (SVM) classification is used for classification of hyperspectral images after feature fusion and it is shown that the proposed approach improves classification accuracy.	feature extraction;principal component analysis;stellar classification;support vector machine	Begüm Demir;Sarp Ertürk	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4779526	full spectral imaging;support vector machine;computer vision;computer science;hyperspectral imaging;machine learning;signal processing;pattern recognition;remote sensing;principal component analysis	Robotics	30.240110113532285	-42.8486616523156	123035
d185f8f854b31da361929fa3712eee39bcd35540	learning from web data: the benefit of unsupervised object localization		"""Annotating a large number of training images is very time-consuming. In this background, this paper focuses on learning from easy-to-acquire web data and utilizes the learned model for fine-grained image classification in labeled datasets. Currently, the performance gain from training with web data is incremental, like a common saying""""better than nothing, but not by much"""". Conventionally, the community looks to correcting the noisy web labels to select informative samples. In this work, we first systematically study the built-in gap between the web and standard datasets, i.e. different data distributions between the two kinds of data. Then, in addition to using web labels, we present an unsupervised object localization method, which provides critical insights into the object density and scale in web images. Specifically, we design two constraints on web data to substantially reduce the difference of data distributions for the web and standard datasets. First, we present a method to control the scale, localization and number of objects in the detected region. Second, we propose to select the regions containing objects that are consistent with the web tag. Based on the two constraints, we are able to process web images to reduce the gap, and the processed web data is used to better assist the standard dataset to train CNNs. Experiments on several fine-grained image classification datasets confirm that our method performs favorably against the state-of-the-art methods."""		Xiaoxiao Sun;Liang Zheng;Yu-Kun Lai;Jufeng Yang	2018	CoRR			Web+IR	29.452260983818537	-50.48886574146639	123041
4df8f0dc47cf9aa72165a743552fb9863d25c43f	video spatio-temporal filtering based on cameras and target objects trajectories -- videosurveillance forensic framework	trajectory cameras mobile communication roads metadata;metadata;videosurveillance systems;video surveillance image filtering image forensics query processing spatiotemporal phenomena video cameras;theorie de l information;spatio temporal queries;forensic tool metadata spatio temporal queries videosurveillance systems;transit network video spatio temporal filtering cameras target objects trajectories videosurveillance forensic framework video surveillance agent query processing geometrical segments symbolic segments;forensic tool;recherche d information	This paper presents our work about assisting video-surveillance agents in the search for particular video scenes of interest in transit network. This work has been developed based on requirements defined within different projects with the French National Police in a forensic goal. The video-surveillance agent inputs a query in the form of a hybrid trajectory (date, time, locations expressed with regards to different reference systems) and potentially some visual descriptions of the scene. The query processing starts with the interpretation of the hybrid trajectory and continues with a selection of a set of cameras likely to have filmed the spatial trajectory. The main contributions of this paper are: (1) a definition of the hybrid trajectory query concept, trajectory that is constituted of geometrical and symbolic segments represented with regards to different reference systems (e.g., Geodesic system, road network), (2) a spatio-temporal filtering framework based on a spatio-temporal modeling of the transit network and associated cameras.	closed-circuit television;database;requirement	Dana Codreanu;André Péninou;Florence Sèdes	2015	2015 10th International Conference on Availability, Reliability and Security	10.1109/ARES.2015.102	computer vision;simulation;geography;multimedia	Robotics	39.06215313589669	-47.67969317711129	123114
b13f1ac6c7a13d323c0f06f2decfea984ce8144b	iterative reconstrained low-rank representation via weighted nonconvex regularizer		Benefiting from the joint consideration of geometric structures and low-rank constraint, graph low-rank representation (GLRR) method has led to the state-of-the-art results in many applications. However, it faces the limitations that the structure of errors should be known a prior, the isolated construction of graph Laplacian matrix, and the over shrinkage of the leading rank components. To improve GLRR in these regards, this paper proposes a new LRR model, namely iterative reconstrained LRR via weighted nonconvex regularization, using three distinguished properties on the concerned representation matrix. The first characterizes various distributions of the errors into an adaptively learned weight factor for more flexibility of noise suppression. The second generates an accurate graph matrix from weighted observations for less afflicted by noisy features. The third employs a parameterized rational function to reveal the importance of different rank components for better approximation to the intrinsic subspace structure. Following a deep exploration of automatic thresholding, parallel update, and partial SVD operation, we derive a computationally efficient low-rank representation algorithm using an iterative reconstrained framework and accelerated proximal gradient method. Comprehensive experiments are conducted on synthetic data, image clustering, and background subtraction to achieve several quantitative benchmarks as clustering accuracy, normalized mutual information, and execution time. Results demonstrate the robustness and efficiency of IRWNR compared with other state-of-the-art models.	algorithm;algorithmic efficiency;background subtraction;cluster analysis;constraint graph;experiment;iterative method;laplacian matrix;low-rank approximation;mutual information;proximal gradient method;proximal gradient methods for learning;run time (program lifecycle phase);singular value decomposition;synthetic data;thresholding (image processing);zero suppression	Jianwei Zheng;Cheng Lu;Hongchuan Yu;Wanliang Wang;Shengyong Chen	2018	IEEE Access	10.1109/ACCESS.2018.2870371	proximal gradient methods;parameterized complexity;cluster analysis;mathematical optimization;laplacian matrix;approximation algorithm;singular value decomposition;computer science;distributed computing;matrix (mathematics);background subtraction	AI	26.98192823084494	-39.08057388397105	123254
665265289471d08a4b472329eb42965b51ac485a	fairness gan		In this paper, we introduce the Fairness GAN, an approach for generating a dataset that is plausibly similar to a given multimedia dataset, but is more fair with respect to protected attributes in allocative decision making. We propose a novel auxiliary classifier GAN that strives for demographic parity or equality of opportunity and show empirical results on several datasets, including the CelebFaces Attributes (CelebA) dataset, the Quick, Draw! dataset, and a dataset of soccer player images and the offenses they were called for. The proposed formulation is well-suited to absorbing unlabeled data; we leverage this to augment the soccer dataset with the much larger CelebA dataset. The methodology tends to improve demographic parity and equality of opportunity while generating plausible images.	allocative efficiency;binary classification;binocular disparity;fairness measure;feature vector;odds algorithm;parity bit;preprocessor;softmax function	Prasanna Sattigeri;Samuel C. Hoffman;Vijil Chenthamarakshan;Kush R. Varshney	2018	CoRR		machine learning;artificial intelligence;mathematics;leverage (finance);parity (mathematics);allocative efficiency	NLP	25.160129035592067	-48.58122134540677	123318
992ac558520095596ded1f448ab201b44eb3546c	sparse coding with invariance constraints	vision ordenador;image processing;champ visuel;procesamiento imagen;invarianza;sparse set;campo visual;intelligence artificielle;traitement image;corteza visual;computer vision;invariance;feature vector;artificial intelligence;vision ordinateur;ensemble epars;receptive field;inteligencia artificial;visual field;sparse coding;cortex visuel;visual cortex	We suggest a new approach to optimize the learning of sparse f eatures under the constraints of explicit transformation symmetri es imposed on the set of feature vectors. Given a set of basis feature vectors and inv ariance transformations, from each basis feature a family of transformed featu r s is generated. We then optimize the basis features for optimal sparse reconst ruction of the input pattern ensemble using the whole transformed feature family. I f the predefined transformation invariance coincides with an invariance in the in put data, we obtain a less redundant basis feature set, compared to sparse coding approaches without invariances. We demonstrate the application to a test scena rio of overlapping bars and the learning of receptive fields in hierarchical visual c ortex models.	compactrio;feature vector;neural coding;sparse matrix	Heiko Wersing;Julian Eggert;Edgar Körner	2003		10.1007/3-540-44989-2_46	computer vision;feature vector;image processing;computer science;artificial intelligence;invariant;machine learning;pattern recognition;mathematics;neural coding;receptive field;feature	AI	29.51192751289991	-41.44854462968225	123338
9f96b12427e65eb00ca6cacd9839f21892ee55d9	feature learning for scene flow estimation from lidar		To perform tasks in dynamic environments, many mobile robots must estimate the motion in the surrounding world. Recently, techniques have been developed to estimate scene flow directly from LIDAR scans, relying on handdesigned features. In this work, we build an encoding network to learn features from an occupancy grid. The network is trained so that these features are discriminative in finding matching or non-matching locations between successive timesteps. This learned feature space is then leveraged to estimate scene flow. We evaluate our method on the KITTI dataset and demonstrate performance that improves upon the accuracy of the current state-of-the-art. We provide an implementation of our method at https://github.com/aushani/flsf.	energy minimization;feature learning;feature vector;mobile robot;national lidar dataset	Arash K. Ushani;Ryan M. Eustice	2018			computer vision;lidar;feature learning;artificial intelligence;computer science	Robotics	30.17030456834259	-50.03155423089039	123385
3c535434cc23cc8708abff671a3c8dcd3c623c77	a model for classifying multisource remote sensing images by kohonen neural networks	remote sensing image;maximum likelihood;remote sensing neural networks statistical analysis remote monitoring digital elevation models data mining feature extraction image classification earth image recognition;image classification;statistical method;backpropagation;maximum likelihood estimation;back propagation neural network;digital terrain model;kohonen neural network;self organising feature maps;geophysical signal processing;remote sensing;self organization;self organized map;data acquisition terrain mapping geophysical signal processing maximum likelihood estimation self organising feature maps backpropagation image classification;terrain mapping;data acquisition;neural network;data acquisition multisource remote sensing image classification kohonen neural networks self organized neural network self organizing map digital terrain model geographical locations iran maximum likelihood statistical method backpropagation neural network supervised trained system	In this work a self-organized neural network for classifying of multisource remote sensing images is proposed. The method is a modified self-organizing map based on the Kohonen model that uses training patterns which belong to the known classes in the training phase of the network. In this way we will have a supervised trained system, which results in more accurate and rapid information. Acquiring this accuracy is more guaranteed by using multisource in remote sensing such as digital terrain model data, which is highly effective for extracting the features of the collected data. The model used for the classification of multisource remote sensing images was collected from two geographical locations in Iran and then its performance in classification was compared with two other methods: maximum likelihood (MLH) statistical method, and back-propagation neural network. The applied model proved to be the best in accuracy and speed.	artificial neural network;backpropagation;convolutional neural network;digital elevation model;organizing (structure);self-organization;self-organizing map;software propagation;supervised learning	Jafar Razmara	2004	IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2004.1369963	computer vision;computer science;machine learning;pattern recognition;maximum likelihood;artificial neural network;remote sensing	Robotics	32.65711189411435	-43.95245330778674	123643
86c213bed6ea628a684424ce941680796c0dc5ae	task recognition and style analysis in dance sequences	image recognition;image motion analysis;task model;tensors image motion analysis image recognition image sequences music;motion segmentation;music analysis;motion sequence task recognition style analysis dance sequences multi factor tensor persons identification musical information analysis method;humans motion analysis tensile stress information analysis rhythm legged locomotion intelligent systems performance evaluation face recognition probes;information analysis;music;image sequences;tensors	In this paper we present a novel approach to recognizing motion styles and identifying persons using the multi factor tensor (MFT) model. We apply a musical information analysis method in segmenting the motion sequence relevant to the key poses and the musical rhythm. We define a task model considering the repetitive motion segment, in which motion is decomposed into task and style. Given the motion data set, we formulate the MFT model and factorize it efficiently in recognizing the tasks, the styles, and the identities of persons. In our experiments, traditional dance by several people is chosen as the motion sequence. We capture the motion data for a few cycles, segment it using the musical analysis approach, normalize the segments, and recognize a task model from them. Various experiments to evaluate the potential of the recognition ability of our proposed approach are performed, and the results demonstrate the high accuracy of our model	experiment;media foundation;rsa problem	Manoj Perera;Takaaki Shiratori;Shunsuke Kudoh;Atsushi Nakazawa;Katsushi Ikeuchi	2006	2006 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems	10.1109/MFI.2006.265645	psychology;computer vision;speech recognition;communication	Robotics	38.130471534729395	-48.927886409675175	123694
3d727f95c6771cba6608eb7d3900b25e745659cd	continuous human action recognition using depth-mhi-hog and a spotter model	models theoretical;hidden markov model;action modeling;human activities;video recording;continuous human action recognition;artificial intelligence;pattern recognition automated;humans;depth mhi hog dmh;spotter model;action spotting;markov chains	In this paper, we propose a new method for spotting and recognizing continuous human actions using a vision sensor. The method is comprised of depth-MHI-HOG (DMH), action modeling, action spotting, and recognition. First, to effectively separate the foreground from background, we propose a method called DMH. It includes a standard structure for segmenting images and extracting features by using depth information, MHI, and HOG. Second, action modeling is performed to model various actions using extracted features. The modeling of actions is performed by creating sequences of actions through k-means clustering; these sequences constitute HMM input. Third, a method of action spotting is proposed to filter meaningless actions from continuous actions and to identify precise start and end points of actions. By employing the spotter model, the proposed method improves action recognition performance. Finally, the proposed method recognizes actions based on start and end points. We evaluate recognition performance by employing the proposed method to obtain and compare probabilities by applying input sequences in action models and the spotter model. Through various experiments, we demonstrate that the proposed method is efficient for recognizing continuous human actions in real environments.	altretamine;clotting time:time:pt:bld:qn:lee white;cluster analysis;communication endpoint;conflict (psychology);depth of invasion by tumor:len:pt:skin melanoma:qn;experiment;feature extraction;harel-yoon syndrome;hidden markov model;image segmentation;interaction;k-means clustering;metrorrhagia;part dosing unit;probability;revision procedure;simulation;sam so eum;statistical cluster	Hyukmin Eum;Changyong Yoon;Heejin Lee;Mignon Park	2015		10.3390/s150305197	computer vision;markov chain;computer science;engineering;artificial intelligence;hidden markov model	Vision	39.10086622179501	-48.62282522930847	123723
d072a116b8870b3955c25148c7e37d099905fcef	previewer for multi-scale object detector		Most multi-scale detectors face a challenge of small-size false positives due to the inadequacy of low-level features, which have small receptive field sizes and weak semantic capabilities. This paper demonstrates independent predictions from different feature layers on the same region is beneficial for reducing false positives. We propose a novel light-weight previewer block, which previews the objectness probability for the potential regression region of each prior box, using the stronger features with larger receptive fields and more contextual information for better predictions. This previewer block is generic and can be easily implemented in multi-scale detectors, such as SSD, RFBNet and MS-CNN. Extensive experiments are conducted on PASCAL VOC and KITTI pedestrian benchmark to show the superiority of the proposed method.	benchmark (computing);experiment;high- and low-level;sensor;solid-state drive	Zhihang Fu;Zhongming Jin;Guo-Jun Qi;Chen Shen;Rongxin Jiang;Yaowu Chen;Xian-Sheng Hua	2018		10.1145/3240508.3240544	computer vision;convolutional neural network;object detection;artificial intelligence;false positive paradox;receptive field;detector;computer science	Vision	29.83440962575555	-51.37528887335829	123731
21765df4c0224afcc25eb780bef654cbe6f0bc3a	multi-channel correlation filters	frequency domain analysis;correlation detectors frequency domain analysis training equations vectors linear systems;computer vision;object detection computer vision filtering theory frequency domain analysis learning artificial intelligence;pattern recognition multi channel features correlation filter learning;correlation filter learning;pattern recognition;computer vision multichannel correlation filters hog descriptor sift descriptor pattern detection video signal processing perspective detection process multichannel image multichannel detector filter learning single channel response map frequency domain training time memory footprint visual detection localization tasks computational efficiency memory efficiency;multi channel features;learning artificial intelligence;filtering theory;object detection	Modern descriptors like HOG and SIFT are now commonly used in vision for pattern detection within image and video. From a signal processing perspective, this detection process can be efficiently posed as a correlation/ convolution between a multi-channel image and a multi-channel detector/filter which results in a single channel response map indicating where the pattern (e.g. object) has occurred. In this paper, we propose a novel framework for learning a multi-channel detector/filter efficiently in the frequency domain, both in terms of training time and memory footprint, which we refer to as a multichannel correlation filter. To demonstrate the effectiveness of our strategy, we evaluate it across a number of visual detection/ localization tasks where we: (i) exhibit superior performance to current state of the art correlation filters, and (ii) superior computational and memory efficiencies compared to state of the art spatial detectors.	computation;convolution;filter design;internationalization and localization;memory footprint;pattern recognition;scale-invariant feature transform;sensor;signal processing	Hamed Kiani Galoogahi;Terence Sim;Simon Lucey	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.381	computer vision;speech recognition;computer science;machine learning;frequency domain	Vision	34.48058660945557	-49.598842420481006	123824
f75e7f20dc04dc3c8a9002b8895aa638eac75411	supervised regularization locality-preserving projection method for face recognition	supervised learning;small sample size problem;regularization;locality preserving projections;face recognition	Locality-preserving projection (LPP) is a promising manifold-based dimensionality reduction and linear feature extraction method for face recognition. However, there exist two main issues in traditional LPP algorithm. LPP does not utilize the class label information at the training stage and its performance will be affected for classification tasks. In addition, LPP often suffers from small sample size (3S) problem, which occurs when the dimension of input pattern space is greater than the number of training samples. Under this situation, LPP fails to work. To overcome these two limitations, this paper presents a novel supervised regularization LPP (SRLPP) approach based on a supervised graph and a new regularization strategy. It theoretically proves that regularization matrix approaches to the original one as the regularized parameter tends to zero. The proposed SRLPP method is subsequently applied to face recognition. The experiments are conducted on two publicly available face databases, namely ORL database and FERET database. Compared with some existing LDA-based and LPP-based linear feature extraction approaches, experimental results show that our SRLPP approach gives superior performance.	facial recognition system;locality of reference;matrix regularization;projection method (fluid dynamics)	Wensheng Chen;Wei Wang;Jianwei Yang;Yuan Yan Tang	2012	IJWMIP	10.1142/S0219691312500531	facial recognition system;regularization;computer vision;computer science;machine learning;pattern recognition;mathematics;supervised learning	Vision	24.731400697140323	-41.54907069595171	123975
025f4b7b72498df85fc257e26c6d8a2e460cdfbf	depth video-based human activity recognition system using translation and scaling invariant features for life logging at smart home	image recognition;radon transforms;hidden markov models depth silhouette images r transformation principle component analysis;video signal processing;r transformation;hidden markov models;shape;vectors;feature extraction;principal component analysis;transforms humans feature extraction vectors hidden markov models principal component analysis shape;transforms;video signal processing hidden markov models image recognition radon transforms;principle component analysis;humans;depth silhouette images;radon transform depth video based human activity recognition system smart home healthcare services video based translation scaling invariant human activity recognition har system human body shape information 2d feature map 1d feature profile hidden markov model hmm mean recognition rate human activities pca lda r transforming depth silhouettes	Video-based human activity recognition systems have potential contributions to various applications such as smart homes and healthcare services. In this work, we present a novel depth video-based translation and scaling invariant human activity recognition (HAR) system utilizing R transformation of depth silhouettes. To perform HAR in indoor settings, an invariant HAR method is critical to freely perform activities anywhere in a camera view without translation and scaling problems of human body silhouettes. We obtain such invariant features via R transformation on depth silhouettes. Furthermore, in R transforming depth silhouettes, shape information of human body reflected in depth values is encoded into the features. In R transformation, 2D feature maps are computed first through Radon transform of each depth silhouette followed by computing 1D feature profile through R transform to get the translation and scaling invariant features. Then, we apply Principle Component Analysis (PCA) for dimension reduction and Linear Discriminant Analysis (LDA) to make the features more prominent, compact and robust. Finally, Hidden Markov Models (HMMs) are used to train and recognize different human activities. Our proposed system shows superior recognition rate over the conventional approaches, reaching up to the mean recognition rate of 93.16% for six typical human activities whereas the conventional PC and IC-based depth silhouettes achieved only 74.83% and 86.33% ,while binary silhouettes-based R transformation approach achieved 67.08% respectively. Our experimental results show that the proposed method is robust, reliable, and efficient in recognizing the daily human activities.	activity recognition;algorithm;dimensionality reduction;glossary of computer graphics;hidden markov model;home automation;image scaling;linear discriminant analysis;map;markov chain;principal component analysis;robustness (computer science)	Ahmad Jalal;Md. Zia Uddin;Tae-Seong Kim	2012	IEEE Transactions on Consumer Electronics	10.1109/TCE.2012.6311329	computer vision;speech recognition;computer science;pattern recognition;mathematics;hidden markov model;principal component analysis	Vision	36.208548850399104	-51.655183686867815	124010
020dacf104023118161a9a38ec7a548290a9c00e	mitral valve models reconstructor: a python based gui software in a hpc environment for patient-specific fem structural analysis	surgical planning;structural model;mitral valve;image processing;learning curve;graphical interface;hypotheses testing;finite element model;high performance computer;parallel computer;graphic user interface;structure analysis;numerical simulation	A new approach in the biomechanical analysis of the mitral valve (MV) focusing on patient-specific modelling has recently been pursued. The aim is to provide a useful tool to be used in clinic for hypotheses testing in pre-operative surgical planning and post-operative follow-up prediction. In particular, the integration of finite element models (FEMs) with 4D echocardiographic advanced images processing seems to be the key turn in patient-specific modelling. The development of this approach is quite slow and hard, due to three main limitations: i) the time needed for FEM preparation; ii) the high computational costs of FEM calculation; iii) the long learning curve needed to complete the analysis without a unified integrated tool which is not currently available. In this context, the purpose of this work is to present a novel Python-based graphic user interface (GUI) software working in a high performance computing (HPC) environment, implemented to overcome the above mentioned limitations. The Mitral Valve Models Reconstructor (MVMR) integrates all the steps needed to simulate the dynamic closure of a MV through a structural model based on human in vivo experimental data. MVMR enables the FEM reconstruction of the MV by means of efficient scientific routines, which ensure a very small time consuming and make the model easily maintainable. Results on a single case study reveal that both FEM building and structural computation are notably reduced with this new approach. The time needed for the FEM implementation is reduced by 1900% with respect to the previous manual procedure, while the time originally needed for the numerical simulation on a single CPU is decreased by 980% through parallel computing using 32 CPUs. Moreover the user-friendly graphic interface provides a great usability also for non-technical personnel like clinicians and bio-researchers, thus removing the need for a long learning curve.	british informatics olympiad;central processing unit;computation;computer simulation;finite element method;graphical user interface;light field;parallel computing;python;reconstructor;structural analysis;supercomputer;system integration;usability;video-in video-out;widget toolkit	A. Arnoldi;Alessandro Invernizzi;Raffaele Ponzini;Emiliano Votta;Enrico G. Caiani;Alberto Redaelli	2008		10.1007/978-90-481-3658-2_37	simulation;image processing;computer science;artificial intelligence;operating system;graphical user interface;graphical user interface testing;statistics;computer graphics (images)	HPC	37.99909563033273	-38.5984609953362	124113
62857147a6809063671614d62f43605130757b1c	unsupervised discriminant projection analysis for feature extraction	locality preserving projection;unsupervised discriminant projection analysis;biometrics access control;image database;image classification;image classification biometrics access control feature extraction;conference paper;rayleigh scattering principal component analysis linear discriminant analysis biometrics feature extraction image databases face recognition computer science helium laplace equations;feature extraction;principal component analysis;database systems;linear programming;face biometrics unsupervised discriminant projection analysis feature extraction locality preserving projection image classification;face biometrics;problem solving	This paper develops an unsupervised discriminant projection (UDP) technique for feature extraction. UDP takes the local and non-local information into account, seeking to find a projection that maximizes the non-local scatter and minimizes the local scatter simultaneously. This characteristic makes UDP more intuitive and more powerful than the up-to-date method ocality preserving projection (LPP, which considers the local information only) for classification tasks. The proposed method is applied to face biometrics and examined using the ORL and FERET face image databases. Our experimental results show that UDP consistently outperforms LPP, PCA, and LDA.	biometrics;database;discriminant;feret (facial recognition technology);feature extraction;local-density approximation;principal component analysis;quantum nonlocality;return loss	Jian Yang;David Zhang;Zhong Jin;Jingyu Yang	2006		10.1109/ICPR.2006.1143	contextual image classification;feature extraction;computer science;linear programming;machine learning;pattern recognition;data mining;principal component analysis	AI	25.14505640241786	-41.837436035611844	124428
02a5b7a41ffa8518eb3b7cae9914a2bd2bbc886b	fast online object tracking and segmentation: a unifying approach		In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 35 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semisupervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is http://www. robots.ox.ac.uk/ ̃qwang/SiamMask.		Qiang Wang;Li Zhang;Luca Bertinetto;Weiming Hu;Philip H. S. Torr	2018	CoRR			Vision	30.41870436184427	-48.71209956135474	124531
347df3541a7d2a232a77d603824eaf8cd1f6563c	mr-ntd: manifold regularization nonnegative tucker decomposition for tensor data dimension reduction and representation	nonnegative tensors dimension reduction manifold learning;tensile stress manifolds algorithm design and analysis matrix decomposition videos correlation convergence	With the advancement of data acquisition techniques, tensor (multidimensional data) objects are increasingly accumulated and generated, for example, multichannel electroencephalographies, multiview images, and videos. In these applications, the tensor objects are usually nonnegative, since the physical signals are recorded. As the dimensionality of tensor objects is often very high, a dimension reduction technique becomes an important research topic of tensor data. From the perspective of geometry, high-dimensional objects often reside in a low-dimensional submanifold of the ambient space. In this paper, we propose a new approach to perform the dimension reduction for nonnegative tensor objects. Our idea is to use nonnegative Tucker decomposition (NTD) to obtain a set of core tensors of smaller sizes by finding a common set of projection matrices for tensor objects. To preserve geometric information in tensor data, we employ a manifold regularization term for the core tensors constructed in the Tucker decomposition. An algorithm called manifold regularization NTD (MR-NTD) is developed to solve the common projection matrices and core tensors in an alternating least squares manner. The convergence of the proposed algorithm is shown, and the computational complexity of the proposed method scales linearly with respect to the number of tensor objects and the size of the tensor objects, respectively. These theoretical results show that the proposed algorithm can be efficient. Extensive experimental results have been provided to further demonstrate the effectiveness and efficiency of the proposed MR-NTD algorithm.	computational complexity theory;convergence (action);data acquisition;dimensionality reduction;least squares;manifold regularization;mitral valve insufficiency;neural tube defects;notice and take down;physical object;reside;small;tucker decomposition;algorithm;videocassette	Xutao Li;Michael K. Ng;Gao Cong;Yunming Ye;Qingyao Wu	2017	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2016.2545400	mathematical optimization;mathematical analysis;topology;computer science;mathematics;complex dimension;manifold alignment	ML	28.34025562613814	-40.64074464168736	124557
d833c48334e906537f21757b6f9fa44da66f6c76	memc-net: motion estimation and motion compensation driven neural network for video interpolation and enhancement		Motion estimation (ME) and motion compensation (MC) have been widely used for classical video frame interpolation systems over the past decades. Recently, a number of data-driven frame interpolation methods based on convolutional neural networks have been proposed. However, existing learning based methods typically estimate either flow or compensation kernels, thereby limiting performance on both computational efficiency and interpolation accuracy. In this work, we propose a motion estimation and compensation driven neural network for video frame interpolation. A novel adaptive warping layer is developed to integrate both optical flow and interpolation kernels to synthesize target frame pixels. This layer is fully differentiable such that both the flow and kernel estimation networks can be optimized jointly. The proposed model benefits from the advantages of motion estimation and compensation methods without using hand-crafted features. Compared to existing methods, our approach is computationally efficient and able to generate more visually appealing results. Furthermore, the proposed MEMC-Net can be seamlessly adapted to several video enhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive quantitative and qualitative evaluations demonstrate that the proposed method performs favorably against the state-of-the-art video frame interpolation and enhancement algorithms on a wide range of datasets.	adaptive filter;algorithm;algorithmic efficiency;artificial neural network;benchmark (computing);convolutional neural network;deblocking filter;end-to-end principle;motion compensation;motion estimation;motion interpolation;noise reduction;optical flow;pixel;super-resolution imaging	Wenbo Bao;Wei-Sheng Lai;Xiaoyun Zhang;Zhiyong Gao;Ming-Hsuan Yang	2018	CoRR		convolutional neural network;deblocking filter;interpolation;motion interpolation;pattern recognition;motion estimation;artificial neural network;artificial intelligence;computer science;motion compensation;computer vision;optical flow	Vision	24.890203036770867	-51.63258105825416	124974
201f46baa46257fcb0b2e78b88f45ddc2f8c31e8	segmented mixture-of-gaussian classification for hyperspectral image analysis	geophysical image processing;graph theory;image segmentation;gaussian processes;image classification;image sensors;statistical analysis;segmented mixture of gaussian classification reduced dimensional subspace multimodal statistical structure graph theoretic locality preserving discriminant analysis block diagonal correlation structure divide and conquer algorithm hyperspectral classification task feature selection projection selection training sample statistical pattern classification procedure chemical composition hyperspectral image analysis;statistical analysis gaussian processes geophysical image processing graph theory hyperspectral imaging image classification image segmentation image sensors learning artificial intelligence;information fusion hyperspectral data;learning artificial intelligence;hyperspectral imaging	The same high dimensionality of hyperspectral imagery that facilitates detection of subtle differences in spectral response due to differing chemical composition also hinders the deployment of traditional statistical pattern-classification procedures, particularly when relatively few training samples are available. Traditional approaches to addressing this issue, which typically employ dimensionality reduction based on either projection or feature selection, are at best suboptimal for hyperspectral classification tasks. A divide-and-conquer algorithm is proposed to exploit the high correlation between successive spectral bands and the resulting block-diagonal correlation structure to partition the hyperspectral space into approximately independent subspaces. Subsequently, dimensionality reduction based on a graph-theoretic locality-preserving discriminant analysis is combined with classification driven by Gaussian mixture models independently in each subspace. The locality-preserving discriminant analysis preserves the potentially multimodal statistical structure of the data, which the Gaussian mixture model classifier learns in the reduced-dimensional subspace. Experimental results demonstrate that the proposed system significantly outperforms traditional classification approaches, even when few training samples are employed.	algorithm;dimensionality reduction;feature extraction;feature selection;google map maker;graph theory;image analysis;linear discriminant analysis;locality of reference;mixture model;multimodal interaction;programming paradigm;software deployment;statistical classification	Saurabh Prasad;Minshan Cui;Wei Li;James E. Fowler	2014	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2013.2250902	computer vision;contextual image classification;computer science;graph theory;hyperspectral imaging;machine learning;pattern recognition;image sensor;gaussian process;mathematics;image segmentation;physics;statistics;remote sensing	Vision	29.846779744094917	-43.238455404685745	125013
f932c930bf9f5fb2208fee1b676cb3761ecfa4a2	head pan angle estimation by a nonlinear regression on selected features	sequential feature selection;neural nets;skin;regression analysis face recognition feature extraction fuzzy set theory neural nets pose estimation;gaze tracking;fuzzy set theory;head pose estimation;boosting;face recognition;estimation;general regression neural network;image color analysis;feature extraction;head filters boosting neural networks face recognition computer networks image databases spatial databases pixel region 2;head pose head pan angle estimation nonlinear feature regression gaze tracking face recognition filter based feature selection generalized regression neural network fuzzy functional criterion pointing 04 database;feature selection;regression analysis;face;regression problem;regression problem head pose estimation sequential feature selection;nonlinear regression;neural network;pose estimation	Head pose is a crucial step for numerous face applications such as gaze tracking and face recognition. In this paper, we introduce a new method to learn the mapping between a set of features and the corresponding head pose. It combines a filter based feature selection and a Generalized Regression Neural Network where inputs are sequentially selected through a boosting process. We propose the Fuzzy Functional Criterion, a new filter used to select relevant features. At each step, features are evaluated using weights on examples computed using the error produced by the neural network at the previous step. This boosting strategy helps to focus on hard examples and selects a set of complementary features. Results are compared with two state-of-the-art methods on the Pointing 04 database.	artificial neural network;eye tracking;facial recognition system;feature selection;nonlinear system	Kevin Bailly;Maurice Milgram	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414310	face;computer vision;estimation;pose;feature extraction;computer science;machine learning;pattern recognition;skin;fuzzy set;boosting;artificial neural network;regression analysis;nonlinear regression	Robotics	35.15944568791618	-43.557062102863185	125052
b399eaa174a3a4ca0d05be5dfbb30cb5dda09855	st-hmp: unsupervised spatio-temporal feature learning for tactile data	icub hands unsupervised spatio temporal feature learning tactile sensing robot grasping spatio temporal hierarchical matching pursuit st hmp spatio temporal structures raw tactile data grasp stability assessment object instance recognition schunk dexterous schunk parallel;databases matching pursuit algorithms tactile sensors hidden markov models synchronous digital hierarchy;tactile sensors learning artificial intelligence manipulators object recognition robot vision stability	Tactile sensing plays an important role in robot grasping and object recognition. In this work, we propose a new descriptor named Spatio-Temporal Hierarchical Matching Pursuit (ST-HMP) that captures properties of a time series of tactile sensor measurements. It is based on the concept of unsupervised hierarchical feature learning realized using sparse coding. The ST-HMP extracts rich spatio-temporal structures from raw tactile data without the need to predefine discriminative data characteristics. We apply it to two different applications: (1) grasp stability assessment and (2) object instance recognition, presenting its universal properties. An extensive evaluation on several synthetic and real datasets collected using the Schunk Dexterous, Schunk Parallel and iCub hands shows that our approach outperforms previously published results by a large margin.	cog (project);cylinder-head-sector;database;dictionary;dynamic programming;emoticon;existential quantification;experiment;feature learning;gaussian process;hidden markov model;host media processing;icub;markov chain;matching pursuit;modal logic;neural coding;outline of object recognition;robot;sparse matrix;synthetic intelligence;tactile sensor;time series;ubiquitous computing;unified framework	Marianna Madry;Liefeng Bo;Danica Kragic;Dieter Fox	2014	2014 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2014.6907172	computer vision;computer science;machine learning;pattern recognition	Robotics	35.36599145773632	-48.23413279853858	125263
621e96fcd354e3ce4771f8927a5af9e8954cce93	eigendecomposition-free training of deep networks with zero eigenvalue-based losses		Many classical Computer Vision problems, such as essential matrix computation and pose estimation from 3D to 2D correspondences, can be solved by finding the eigenvector corresponding to the smallest, or zero, eigenvalue of a matrix representing a linear system. Incorporating this in deep learning frameworks would allow us to explicitly encode known notions of geometry, instead of having the network implicitly learn them from data. However, performing eigendecomposition within a network requires the ability to differentiate this operation. While theoretically doable, this introduces numerical instability in the optimization process in practice. In this paper, we introduce an eigendecomposition-free approach to training a deep network whose loss depends on the eigenvector corresponding to a zero eigenvalue of a matrix predicted by the network. We demonstrate on several tasks, including keypoint matching and 3D pose estimation, that our approach is much more robust than explicit differentiation of the eigendecomposition. It has better convergence properties and yields state-of-the-art results on both tasks.	3d pose estimation;computation;computer vision;deep learning;encode;essential matrix;instability;linear system;mathematical optimization;numerical analysis;numerical linear algebra;numerical stability;rejection sampling;switzerland	Zheng Dang;Kwang Moo Yi;Yinlin Hu;Fei Wang;Pascal Fua;Mathieu Salzmann	2018		10.1007/978-3-030-01228-1_47	machine learning;matrix (mathematics);eigenvalues and eigenvectors;mathematical optimization;computer science;eigendecomposition of a matrix;deep learning;pose;artificial intelligence;singular value decomposition;essential matrix;linear system	Vision	27.332956006837982	-45.22046796103838	125720
c7c9e8e2d2ab55c2766439dbe9dd9547537e1415	automatization of the mexican fruit fly activity recognition process using 3d and gray-level features	three dimensional;feature extraction;stereo vision;feature fusion;stereo vision and feature fusion;animal model;fruit fly;activity recognition	In this project, we propose a strategy to recognize behaviors of a fly from a sequence of features extracted from 3D scenes. Three dimensional (3D) level features and gray-level images of the object are extracted, creating vectors with the features along a sequence of fly images, representing particular activities (walking, feeding, drinking, flying, ovipositing, courting and resting). A stereo vision system was used to extract the sequence of 3D features and gray-level values obtained from the sequence of visible images. Three methods were proposed to use both kinds of information for recognition process. In the first method it is used 3D features, the second method is based on data level fusion and the third on a decision level fusion. This work will contribute to the study of lifespan behavior in insects and has the potential to contribute to our knowledge and understanding of agespecific dependant behaviors, general principles on disablement, and in general, the aging process of the insect. The Mexican fruit fly, a pest of economic importance, represents a good animal model for this purpose.	activity recognition;activity tracker;concatenation;grayscale;sensor;stereopsis	Selene Hernández-Rodríguez;Leopoldo Altamirano Robles;James R. Carey;Pablo Liedo	2006			computer vision;simulation;engineering;communication	Vision	37.11824536882777	-49.86674787705735	125815
61d0b41973ada023099439e2c68cacd3d968e3db	single image super-resolution via cascaded multi-scale cross network		The deep convolutional neural networks have achieved significant improvements in accuracy and speed for single image super-resolution. However, as the depth of network grows, the information flow is weakened and the training becomes harder and harder. On the other hand, most of the models adopt a single-stream structure with which integrating complementary contextual information under different receptive fields is difficult. To improve information flow and to capture sufficient knowledge for reconstructing the high-frequency details, we propose a cascaded multi-scale cross network (CMSC) in which a sequence of subnetworks is cascaded to infer high resolution features in a coarse-to-fine manner. In each cascaded subnetwork, we stack multiple multi-scale cross (MSC) modules to fuse complementary multi-scale information in an efficient way as well as to improve information flow across the layers. Meanwhile, by introducing residual-features learning in each stage, the relative information between high-resolution and low-resolution features is fully utilized to further boost reconstruction performance. We train the proposed network with cascaded-supervision and then assemble the intermediate predictions of the cascade to achieve high quality image reconstruction. Extensive quantitative and qualitative evaluations on benchmark datasets illustrate the superiority of our proposed method over state-of-the-art superresolution methods.		Yanting Hu;Xinbo Gao;Jie Li;Yuanfei Huang;Hanzi Wang	2018	CoRR		convolutional neural network;iterative reconstruction;pattern recognition;fuse (electrical);cascade;information flow (information theory);artificial intelligence;subnetwork;computer science;superresolution;qualitative evaluations	Vision	25.459846192073872	-51.86995865269926	125867
aeedc6b7f2ceaaf9d9cd8e327ca979128c1947e9	locality-sensitive dictionary learning for sparse representation based classification	data locality;dictionary learning;sparse representation	Motivated by image reconstruction, sparse representation based classification (SRC) has been shown to be an effective method for applications like face recognition. In this paper, we propose a localitysensitive dictionary learning algorithm for SRC, in which the designed dictionary is able to preserve local data structure, resulting in improved image classification. During the dictionary update and sparse coding stages in the proposed algorithm, we provide closed-form solutions and enforce the data locality constraint throughout the learning process. In contrast to previous dictionary learning approaches utilizing sparse representation techniques, which did not (or only partially) take data locality into consideration, our algorithm is able to produce a more representative dictionary and thus achieves better performance. We conduct experiments on databases designed for face and handwritten digit recognition. For such reconstruction-based classification problems, we will confirm that our proposed method results in better or comparable performance as state-of-the-art SRC methods do, while less training time for dictionary learning can be achieved. & 2012 Elsevier Ltd. All rights reserved.	approximation algorithm;computer vision;data (computing);data structure;database;dictionary;effective method;emoticon;experiment;facial recognition system;iterative reconstruction;kernel (linear algebra);linear algebra;linux/rk;locality of reference;machine learning;matrix regularization;neural coding;sample rate conversion;semiconductor industry;sparse approximation;sparse matrix	Chia-Po Wei;Yu-Wei Chao;Yi-Ren Yeh;Yu-Chiang Frank Wang	2013	Pattern Recognition	10.1016/j.patcog.2012.11.014	speech recognition;k-svd;computer science;machine learning;pattern recognition;sparse approximation	AI	25.479178682057782	-41.44656394229868	125902
107ad2163a8e5543b443021626a6ff2aaf79818c	a zgpca algorithm for subspace estimation	zgpca algorithm;estimation theory;pattern clustering;sports video clustering;distance measure;principal component analysis estimation theory fir filters pattern clustering;generalized principal component analysis;sports video;sports video clustering zgpca algorithm subspace estimation generalized principal component analysis fir filter filter coefficient k means clustering face clustering;principal component analysis;fir filter;subspace estimation;face clustering;fir filters;clustering algorithms iterative algorithms finite impulse response filter polynomials principal component analysis vectors computer science face recognition computational complexity convergence;synthetic data;filter coefficient;k means clustering	We propose a new algorithm called the ZGPCA algorithm for subspace estimation based on the GPCA (generalized principal component analysis) algorithm. It is formulated within an FIR filter framework so that the norm vectors of the sub-spaces correspond to filter coefficients. It is shown that such an approach leads to a more accurate and computationally efficient method compared to the GPCA algorithm. We extend the ZGPCA algorithm to make it recursive so that subspaces with possibly different dimensions can be obtained. We also propose a new distance measure that can be used for k-means clustering of sample points within a subspace. Experimental results on synthetic data and applications on face clustering and sports video clustering show good performance of the proposed algorithm.	algorithm;algorithmic efficiency;cluster analysis;coefficient;finite impulse response;k-means clustering;principal component analysis;recursion;synthetic data	Haoran Yi;Deepu Rajan;Liang-Tien Chia	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284764	mathematical optimization;data stream clustering;subclu;ramer–douglas–peucker algorithm;canopy clustering algorithm;machine learning;finite impulse response;pattern recognition;cure data clustering algorithm;mathematics;fsa-red algorithm;k-medoids;statistics;population-based incremental learning	Vision	26.718994346135673	-40.1144640232433	125925
51c202abf7665cbd461edd4c59758603c5081a01	towards the computational perception of action	computer vision image sequences;image sequence computational perception of action interacting objects qualitative scene dynamics;computer vision;dynamic information;image sequence;electrical capacitance tomography layout image sequences hidden markov models national electric code computer science grasping read only memory video sequences image recognition;image sequences	Understanding observations of interacting objects requires one to reason about qualitative scene dynamics. For example, on observing a hand lifting a can, we may infer that an ‘active’ hand is applying an upwards force (by grasping) to lift a ‘passive’ can. In previous work [6] we presented a system that infers qualitative scene dynamics from the instantaneous motion of objects. However, since that analysis only considered single frames in isolation, there were often multiple interpretations for each frame. In this work we show how the dynamic information inferred at each frame can be integrated over time to reduce ambiguity. Our approach to integrating information is to extend our representation to describe objects by a set of properties or capabilities that are assumed to persist over time. Given this extended representation we find interpretations that require the smallest set(s) of properties over the whole image sequence.	computation;interaction;lambda lifting	Richard Mann;Allan D. Jepson	1998		10.1109/CVPR.1998.698694	computer vision;simulation;computer science;machine learning	Vision	38.6292915556622	-47.92489432668805	125951
c4050da9e739360d98091cecfdd0a813458d3305	co-segmentation of 3d shapes via subspace clustering	i 4 6 computer graphics segmentation	We present a novel algorithm for automatically co-segmenting a set of shapes from a common family into consistent parts. Starting from over-segmentations of shapes, our approach generates the segmentations by grouping the primitive patches of the shapes directly and obtains their correspondences simultaneously. The core of the algorithm is to compute an affinity matrix where each entry encodes the similarity between two patches, which is measured based on the geometric features of patches. Instead of concatenating the different features into one feature descriptor, we formulate co-segmentation into a subspace clustering problem in multiple feature spaces. Specifically, to fuse multiple features, we propose a new formulation of optimization with a consistent penalty, which facilitates both the identification of most similar patches and selection of master features for two similar patches. Therefore the affinity matrices for various features are sparsity-consistent and the similarity between a pair of patches may be determined by part of (instead of all) features. Experimental results have shown how our algorithm jointly extracts consistent parts across the collection in a good manner.	algorithm;blackwell (series);cluster analysis;clustering high-dimensional data;concatenation;eurographics;feature model;feature vector;holographic principle;mathematical optimization;pacific symposium on biocomputing;processor affinity;shape analysis (digital geometry);sparse matrix;unsupervised learning;visual descriptor	Ruizhen Hu;Lubin Fan;Ligang Liu	2012	Comput. Graph. Forum	10.1111/j.1467-8659.2012.03175.x	computer vision;computer science;machine learning;pattern recognition;mathematics	Vision	27.852180733138823	-46.0213551114564	125993
b86d58792a1b195efbfcab54559de0d139a49a6d	evaluation of histogram-based similarity functions for different color spaces	evaluation;similarity functions;tracking;color spaces	In this paper we evaluate similarity functions for histograms such as chi-square and Bhattacharyya distance for different color spaces such as RGB or L*a*b*. Our main contribution is to show the performance of these histogram-based similarity functions combined with several color spaces. The evaluation is done on image sequences of the PETS 2009 dataset, where a sequence of frames is used to compute the histograms of three different persons in the scene. One of the most popular applications where similarity functions can be used is tracking. Data association is done in multiple stages where the first stage is the computation of the similarity of objects between two consecutive frames. Our evaluation concentrates on this first stage, where we use histograms as data type to compare the objects with each other. In this paper we present a comprehensive evaluation on a dataset of segmented persons with all combinations of the used similarity functions and color spaces.	spaces	Andreas Zweng;Thomas Rittler;Martin Kampel	2011		10.1007/978-3-642-23678-5_54	color histogram;computer vision;computer science;evaluation;pattern recognition;data mining;mathematics;tracking;color space	ML	37.777049297747155	-51.40360707342624	126141
34d40fac91d8fbb4d9c1ad605ee73ae2249df229	unsupervised optimization for universal spatial image steganalysis	image steganalysis;ensemble classifiers;image parameters;feature set optimization;decision template;multi layer perceptron	 Blind universal steganalysis has been the choice of Steganalysers owing to it’s capability to detect stego images without any prior information about the embedding method. Universal steganalysis is a two class optimization problem and the detecting efficiency depends on the feature set chosen from the stego and clean images. Though extracting all possible features of an image may lead to more efficiency the classification suffers due to large dimension of feature set. To overcome the problem of dimensionality appropriate feature reduction techniques need to be employed. This paper presents a blind universal image steganalysis technique that extracts the noise models of adjacent pixels of an image. The exact model construction involves the formation of four dimensional co-occurrence matrices of the quantised and truncated noise residues. From the 106 sub models 34,671 features have been extracted and further reduced by a novel unsupervised optimization technique to identify the most appropriate features for classification. The classifiers implemented include Support Vector Machines (SVM), Multi Layer Perceptron (MLP) and three fusion classifiers based on Bayes, Decision Template and Dempster Schafer fusion schemes. It has been identified that MLP performs better than SVM but is not superior to fusion classifiers. Comparing all the classifiers, Decision Template based fusion method gives the best classification accuracy (99.25%). Thus the proposed unsupervised optimization method combined with Decision Template fusion classification scheme provides the best classification of stego and clear images as compared to the existing research work.	steganalysis	J. Anita Christaline;Ramesh Rangaswamy;Gomathy Chidambaram;Vaishali Durgamahanthi	2018	Wireless Personal Communications	10.1007/s11277-018-5790-6	support vector machine;computer science;real-time computing;steganography;pixel;multilayer perceptron;bayes' theorem;curse of dimensionality;steganalysis;optimization problem;pattern recognition;artificial intelligence	Vision	29.389507803424852	-44.755833811628335	126146
7ed4d134e1910ded71697aa7420f2fb720596d4b	pcca: a new approach for distance learning from sparse pairwise constraints	person reidentification distance learning sparse pairwise constraint pairwise constrained component analysis learning distance metrics sparse pairwise similarity constraints sparse pairwise dissimilarity constraints generalization property high dimensional data vision task face verification;histograms;kernel;measurement;distance learning;training;training measurement face kernel histograms training data vectors;face verification;sparse pairwise constraint;computer vision;training data;face recognition;vectors;vision task;high dimensional data;face;sparse pairwise similarity constraints;generalisation artificial intelligence;person reidentification;learning artificial intelligence;learning distance metrics;learning artificial intelligence computer vision face recognition generalisation artificial intelligence;sparse pairwise dissimilarity constraints;generalization property;pairwise constrained component analysis	This paper introduces Pairwise Constrained Component Analysis (PCCA), a new algorithm for learning distance metrics from sparse pairwise similarity/dissimilarity constraints in high dimensional input space, problem for which most existing distance metric learning approaches are not adapted. PCCA learns a projection into a low-dimensional space where the distance between pairs of data points respects the desired constraints, exhibiting good generalization properties in presence of high dimensional data. The paper also shows how to efficiently kernelize the approach. PCCA is experimentally validated on two challenging vision tasks, face verification and person re-identification, for which we obtain state-of-the-art results.	algorithm;ambiguous name resolution;cluster analysis;data point;experiment;performance;sparse matrix;statistical model;test set;viewpoint	Alexis Mignon;Frédéric Jurie	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247987	facial recognition system;face;distance education;computer vision;training set;kernel;computer science;theoretical computer science;machine learning;pattern recognition;histogram;mathematics;geometry;measurement;clustering high-dimensional data	Vision	26.903448609590672	-44.350334213898265	126242
9f2328931be1cf81a9ea14b35f3373b6122f0714	motion-based object segmentation based on dense rgb-d scene flow		Given two consecutive RGB-D images, we propose a model that estimates a dense three-dimensional (3D) motion field, also known as scene flow. We take advantage of the fact that in robot manipulation scenarios, scenes often consist of a set of rigidly moving objects. Our model jointly estimates the following: First, the segmentation of the scene into an unknown but finite number of objects, second, the motion trajectories of these objects, and finally, the object scene flow. We employ an hourglass, deep neural network architecture. In the encoding stage, the RGB and depth images undergo spatial compression and correlation. In the decoding stage, the model outputs three images containing a per-pixel estimate of the corresponding object center as well as object translation and rotation. This forms the basis for inferring the object segmentation and final object scene flow. To evaluate our model, we generated a new and challenging, large scale, synthetic dataset that is specifically targeted at robotic manipulation: It contains a large number of scenes with a very diverse set of simultaneously moving 3D objects and is recorded with a simulated, static RGB-D camera. In quantitative experiments, we show that we outperform state-of-the-art scene flow and motion-segmentation methods on this data set. In qualitative experiments, we show how our learned model transfers to challenging real-world scenes, visually generating better results than existing methods.	artificial neural network;data compression;deep learning;experiment;motion field;network architecture;pixel;robot;synthetic intelligence	Lin Shao;Parth B. Shah;Vikranth Reddy Dwaracherla;Jeannette Bohg	2018	IEEE Robotics and Automation Letters	10.1109/LRA.2018.2856525	simulation;computer vision;motion field;decoding methods;image segmentation;artificial neural network;architecture;rgb color model;engineering;correlation;trajectory;artificial intelligence	Vision	27.539660254185364	-51.470263052468155	126440
4da2c15f320ee2f719785c44ca2427fef04a0948	deep dictionary learning for fine-grained image classification		Fine-grained image classification is quite challenging due to high inter-class similarity and large intra-class variations. Another issue is the small amount of training images with a large number of classes to be identified. To address the challenges, we propose a model for fine-grained image classification with its application to bird species recognition. Based on the features extracted by bilinear convolutional neural network (BCNN), we propose an on-line dictionary learning algorithm where the principle of sparsity is integrated into classification. The features extracted by BCNN encode pairwise neuron interaction in a translation-invariant manner. This property is valuable to fine-grained classification. The proposed algorithm for dictionary learning further carries out sparsity based classification, where training data can be represented with a less number of dictionary atoms. It alleviates the problems caused by insufficient training data, and makes classification much more efficient. Our approach is evaluated and compared with the state-of-the-art approaches on the CUB-200-2011 dataset. The promising experimental results demonstrate its efficacy and superiority.	algorithm;artificial neural network;bilinear filtering;computer vision;convolutional neural network;dictionary;encode;machine learning;neuron;online and offline;sparse matrix	M. Srinivas;Yen-Yu Lin;Hong-Yuan Mark Liao	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296398	task analysis;convolutional neural network;feature extraction;artificial intelligence;bilinear interpolation;pattern recognition;contextual image classification;pairwise comparison;computer science;training set	Vision	25.99422205447625	-47.83898807409703	126613
b2294a3b75e48fc2e9fd68249890a1e6b15077eb	action key frames extraction using l1-norm and accumulative optical flow for compact video shot summarisation		Key frame extraction is an important algorithm for video summarisation, video retrieval, and generating video fingerprint. The extracted key frames should represent a video sequence in a compact way and brief the main actions to achieve meaningful key frames. Therefore, we present a key frames extraction algorithm based on the L1-norm by accumulating action frames via optical flow method. We then evaluate our proposed algorithm using the action accuracy rate and action error rate of the extracted action frames in comparison to user extraction. The video shot summarisation evaluation shows that our proposed algorithm outperforms the-state-of-the-art algorithms in terms of compression ratio. Our proposed algorithm also achieves approximately 100% and 0.91% for best and worst case in terms of action appearance accuracy in human action dataset KTH in the extracted key frames.	optical flow;taxicab geometry	Manar Abduljabbar Ahmad Mizher;Mei Choo Ang;Siti Norul Huda Sheikh Abdullah;Kok Weng Ng	2017		10.1007/978-3-319-70010-6_34	word error rate;key frame;computer vision;artificial intelligence;optical flow;computer science	Vision	37.68748036184926	-51.13430160588022	126666
e8a80f05c25997a64ea418bc91a5c17900fd2853	classification of natural scene multi spectral images using a new enhanced crf	probability;image segmentation;image classification;spectral analysis image classification image segmentation matrix algebra probability;matrix algebra;conference paper;confusion matrix multi spectral classification fuzzy svm crf pairwise potential;support vector machines feature extraction materials smoothing methods image segmentation context roads;class probabilities natural scene multispectral image classification crf terrestrial multispectral imaging unary classifier pairwise potential confusion matrix vertical configurations segmentation granularity;spectral analysis	In this paper, a new enhanced CRF for discriminating between different materials in natural scenes using terrestrial multi spectral imaging is established. Most of the existing formulations of the CRF often suffer from over smoothing and loss of small detail, thereby deteriorating the information from the underlying unary classifier in areas with a high spatial frequency. This work specifically addresses this issue by incorporating a new pairwise potential that is better at taking local context into account. Certain materials are very unlikely to appear next to each other in the scene and such configurations are penalised by employing the confusion matrix of the unary classifier. Similarly, horizontal as well as vertical configurations, which may be more or less likely for certain combinations of materials, are regarded in this formulation. Furthermore, the proposed pairwise potential also considers the length of boundaries between regions to account for the segmentation granularity issues and also uses class probabilities of the neighbouring regions to make up for the uncertainty of the unary classifier results. Seven band terrestrial multi spectral imaging were used due to its potential in distinguishing between different materials and objects. The proposed approach was evaluated using cross-validation, resulting in an average accuracy of 88.9% which is about 17% more than the accuracy of a standard CRF, which demonstrates the superiority of our approach in preserving local details.	conditional random field;confusion matrix;cross-validation (statistics);linear classifier;naive bayes classifier;smoothing;terrestrial television;unary operation	Mohammad Najafi;Sarah Taghavi Namin;Lars Petersson	2013	2013 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2013.6696885	computer vision;contextual image classification;computer science;machine learning;pattern recognition;probability;mathematics;image segmentation;statistics	Robotics	31.41265900029201	-44.37226712703175	126790
08fbe3187f31b828a38811cc8dc7ca17933b91e9	statistical computations on grassmann and stiefel manifolds for image and video-based recognition	activity recognition statistical computations stiefel manifolds grassmann manifolds video based recognition image recognition linear subspace structure linear dynamic models finite dimensional linear subspaces riemannian geometry geometric properties riemannian metrics maximum likelihood classification unsupervised clustering object recognition activity based video clustering video based face recognition;parametric model;image recognition;object recognition;unsupervised clustering;stiefel;manifolds;video signal processing;manifolds geometry data models face recognition shape computational modeling humans;computer model;stiefel manifold;image and video models;dynamic model;geometry;computational geometry;grassmann;maximum likelihood estimation;statistical models;statistical model;data model;maximum likelihood classification;computational modeling;face recognition;riemannian metric;shape;grassmann manifold;video signal processing computational geometry image recognition maximum likelihood estimation;statistical computing;grassmann image and video models feature representation statistical models manifolds stiefel;humans;feature representation;riemannian geometry;data models;activity recognition	In this paper, we examine image and video-based recognition applications where the underlying models have a special structure-the linear subspace structure. We discuss how commonly used parametric models for videos and image sets can be described using the unified framework of Grassmann and Stiefel manifolds. We first show that the parameters of linear dynamic models are finite-dimensional linear subspaces of appropriate dimensions. Unordered image sets as samples from a finite-dimensional linear subspace naturally fall under this framework. We show that an inference over subspaces can be naturally cast as an inference problem on the Grassmann manifold. To perform recognition using subspace-based models, we need tools from the Riemannian geometry of the Grassmann manifold. This involves a study of the geometric properties of the space, appropriate definitions of Riemannian metrics, and definition of geodesics. Further, we derive statistical modeling of inter and intraclass variations that respect the geometry of the space. We apply techniques such as intrinsic and extrinsic statistics to enable maximum-likelihood classification. We also provide algorithms for unsupervised clustering derived from the geometry of the manifold. Finally, we demonstrate the improved performance of these methods in a wide variety of vision applications such as activity recognition, video-based face recognition, object recognition from image sets, and activity-based video clustering.	activity recognition;algorithm;cluster analysis;computation;dimensions;facial recognition system;inference;manifold regularization;outline of object recognition;statistical model;unified framework;statistical cluster	Pavan K. Turaga;Ashok Veeraraghavan;Anuj Srivastava;Rama Chellappa	2011	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2011.52	statistical model;topology;computational geometry;machine learning;pattern recognition;mathematics;geometry;statistics;activity recognition	Vision	36.211741956626305	-43.23354835904959	126830
8c802081102065ea76d13e6500ec1ddf17961217	real-time gesture recognition from depth data through key poses learning and decision forests	support vector machines feature extraction gesture recognition image classification image representation image sensors image sequences learning artificial intelligence pose estimation;joints training real time systems gesture recognition support vector machines robustness;support vector machines;training;pose identification;image classification;joints;image sensors;gesture time warping real time gesture recognition depth data key poses learning decision forests human gesture recognition real time depth sensors end user natural user interface nui noisy skeleton stream kinect depth sensors tailored angular representation skeleton joints multiclass classifier support vector learning machines key pose sequence;image representation;feature extraction;natural user interface gesture recognition pose identification depth sensors 3d motion;robustness;natural user interface;depth sensors;learning artificial intelligence;gesture recognition;3d motion;real time systems;image sequences;pose estimation	Human gesture recognition is a challenging task with many applications. The popularization of real time depth sensors even diversifies potential applications to end-user natural user interface (NUI). The quality of such NUI highly depends on the robustness and execution speed of the gesture recognition. This work introduces a method for real-time gesture recognition from a noisy skeleton stream, such as the ones extracted from Kinect depth sensors. Each pose is described using a tailored angular representation of the skeleton joints. Those descriptors serve to identify key poses through a multi-class classifier derived from Support Vector learning machines. The gesture is labeled on-the-fly from the key pose sequence through a decision forest, that naturally performs the gesture time warping and avoids the requirement for an initial or neutral pose. The proposed method runs in real time and shows robustness in several experiments.	angularjs;experiment;gesture recognition;kinect;natural user interface;real-time transcription;robustness (computer science);sensor	Leandro Miranda;Thales Vieira;Dimas Martínez Morera;Thomas Lewiner;Antônio Wilson Vieira;Mario Fernando Montenegro Campos	2012	2012 25th SIBGRAPI Conference on Graphics, Patterns and Images	10.1109/SIBGRAPI.2012.44	computer vision;speech recognition;computer science;pattern recognition;gesture recognition	Vision	38.563263861137614	-49.152944220101254	126834
c7a64822ef004a6e2cbd7e45dee1f0690f96d668	contextual action recognition in multi-sensor nighttime video sequences	action dataset contextual action recognition multisensor nighttime video sequences contextual information human actions interpretation interactive relationship contextual clues unfavorable conditions extreme low light nighttime scenarios multisensor imagery context enhancement contextual knowledge multisensor nighttime videos information fusion encapsulating visual information space time action information 3d fourier transform fused action silhouette volume sift context images principal component analysis feature fusion contextual dissimilarity context sift flow energy;fourier transform;sensors;video signal processing;image fusion;contextual information;video sequences;spectrum;infrared spectra;visible spectra data encapsulation feature extraction fourier transforms gesture recognition image enhancement image fusion image sequences infrared spectra principal component analysis video signal processing;space time;infra red;data encapsulation;context visualization streaming media video sequences humans image color analysis sensors;visualization;image enhancement;streaming media;image color analysis;feature extraction;principal component analysis;action recognition;fourier transforms;feature fusion;humans;information fusion;low light;gesture recognition;context;image sequences;visible spectra	Contextual information is important for interpreting human actions especially when actions exhibit interactive relationship with their context. Contextual clues become even more crucial when videos are captured in unfavorable conditions like extreme low light nighttime scenarios. These conditions encourage the use of multi-senor imagery and context enhancement. In this paper, we explore the importance of contextual knowledge for recognizing human actions in multi-sensor nighttime videos. Information fusion is utilized for encapsulating visual information about actions and their context. Space-time action information is contained using 3D fourier transform of fused action silhouette volume. In parallel, SIFT context images are extracted and fused using principal component analysis based feature fusion for each action class. Contextual dissimilarity is penalized by minimizing context SIFT flow energy. The action dataset comprises multi-sensor night vision video data from infra-red and visible spectrum. Experimental results show that fused contextual action information boost action recognition performance as compared to the baseline action recognition approach.	baseline (configuration management);principal component analysis;scale-invariant feature transform;sensor	Anwaar Ul Haq;Iqbal Gondal;M. Manzur Murshed	2011	2011 International Conference on Digital Image Computing: Techniques and Applications	10.1109/DICTA.2011.49	fourier transform;computer vision;computer science;machine learning;pattern recognition;gesture recognition	Vision	37.057701114738414	-49.26689704578411	126957
a4ad110c0bc382d9655578f167f715a0843aa987	data fusion of different spatial resolution remote sensing images applied to forest-type mapping	remote sensing image;geophysical image processing;vegetation mapping;robustness data fusion spatial resolution remote sensing images forest type mapping land cover classification europe;land cover lc classification data fusion forest types;image fusion;forest types;land cover classification;image classification;data fusion;indexing terms;spatial resolution remote sensing data fusion satellites vegetation;vegetation;land cover lc classification;remote sensing data;forest type mapping;remote sensing;satellites;robustness;terrain mapping;remote sensing images;europe;vegetation mapping geophysical image processing image classification image fusion terrain mapping;land cover;spatial resolution	A data fusion method for land cover (LC) classification is proposed that combines remote sensing data at a fine and a coarse spatial resolution. It is a two-step approach, based on the assumption that some of the LC classes can be merged into a more generalized LC class. Step one creates a generalized LC map, using only the information available at the fine spatial resolution. In the second step, a new classifier refines the generalized LC classes to create distinct subclasses of its parent class, using the generalized LC map as a mask. This classifier uses all image information (bands) available at both fine and coarse spatial resolutions. We followed a simple data fusion technique by stacking the individual image bands into a multidimensional vector. The advantage of the proposed approach is that the spatial detail of the generalized LC classes is retained in the final LC map. The method has been designed for operational LC mapping over large areas. Within this paper, it is shown that the proposed data fusion approach increased the robustness of forest-type mapping within Europe. Robustness is particularly important when creating continental LC maps at fine spatial resolution. These maps become more popular now that remote sensing data at fine resolution are easier to access.	focus stacking;information source;köppen climate classification;map;multispectral image	Pieter Kempeneers;Fernando Sedano;Lucia Maria Seebach;Peter Strobl;Jesus San-Miguel-Ayanz	2011	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2011.2158548	computer vision;contextual image classification;index term;image resolution;sensor fusion;image fusion;satellite;vegetation;robustness;remote sensing	Robotics	31.45106068685955	-44.33214780812075	126998
e015d8716a11e1fc355956a46ec9748a674b8d6f	kernel class-wise locality preserving projection	locality preserving projection;manifold learning;feature space;local structure;feature extraction;pattern recognition;kernel method;article;class wise locality preserving projection	In the recent years, the pattern recognition community paid more attention to a new kind of feature extraction method, the manifold learning methods, which attempt to project the original data into a lower dimensional feature space by preserving the local neighborhood structure. Among them, locality preserving projection (LPP) is one of the most promising feature extraction techniques. However, when LPP is applied to the classification tasks, it shows some limitations, such as the ignorance of the label information. In this paper, we propose a novel local structure based feature extraction method, called class-wise locality preserving projection (CLPP). CLPP utilizes class information to guide the procedure of feature extraction. In CLPP, the local structure of the original data is constructed according to a certain kind of similarity between data points, which takes special consideration of both the local information and the class information. The kernelized (nonlinear) counterpart of this linear feature extractor is also established in the paper. Moreover, a kernel version of CLPP namely Kernel CLPP (KCLPP) is developed through applying the kernel trick to CLPP to increase its performance on nonlinear feature extraction. Experiments on ORL face database and YALE face database are performed to test and evaluate the proposed algorithm.	format-preserving encryption;kernel (operating system);locality of reference	Junbao Li;Jeng-Shyang Pan;Shu-Chuan Chu	2008	Inf. Sci.	10.1016/j.ins.2007.12.001	kernel method;feature vector;feature extraction;computer science;machine learning;pattern recognition;data mining;mathematics;nonlinear dimensionality reduction;dimensionality reduction	AI	25.04416177100773	-41.933835851700955	127067
8d8f159a1835c9875481339cc34b849d2a325806	detecting motion patterns via direction maps with application to surveillance	motion analysis;dominant direction;surveillance;real time;event detection;space time;pattern detection;region of interest;spatiotemporal analysis;direction maps;false positive	Detection of motion patterns in video data can be significantly simplified by abstracting away from pixel intensity values towards representations that explicitly and compactly capture movement across space and time. A novel representation that captures the spatiotemporal distributions of motion across regions of interest, called the ''Direction Map,'' abstracts video data by assigning a two-dimensional vector, representative of local direction of motion, to quantized regions in space-time. Methods are presented for recovering direction maps from video, constructing direction map templates (defining target motion patterns of interest) and comparing templates to newly acquired video (for pattern detection and localization). These methods have been successfully implemented and tested (with real-time considerations) on over 6300 frames across seven surveillance/traffic videos, detecting potential targets of interest as they traverse the scene in specific ways. Results show an overall recognition rate of approximately 91% hits vs 8% false positives.	map;sensor	Jacob M. Gryn;Richard P. Wildes;John K. Tsotsos	2009	Computer Vision and Image Understanding	10.1016/j.cviu.2008.10.006	computer vision;simulation;type i and type ii errors;computer science;space time;motion estimation;computer graphics (images);region of interest	Vision	38.12322681111965	-48.6259705935473	127359
8f78075488b92da8080554eee1184385fe56d923	application of the particle filter for simple gesture recognition	particle filter;bayesian estimator;conditional probability;gesture recognition	This paper describes a gesture recognition algorithm based on the particle filters, namely CONDENSATION. The particle filter is more efficient than any other tracking algorithm because the tracking mechanism follows Bayesian estimation rule of conditional probability propagation.		Yang Weon Lee	2008		10.1007/978-3-540-85984-0_64	monte carlo localization;speech recognition;conditional probability;particle filter;computer science;machine learning;pattern recognition;gesture recognition;statistics	Vision	38.21663257028469	-41.53974237316528	127419
ebc2a3e8a510c625353637e8e8f07bd34410228f	dual sparse constrained cascade regression for robust face alignment	biological patents;shape face adaptation models robustness training context feature extraction;biomedical journals;text mining;europe pubmed central;training;citation search;citation networks;cascade regression robust face alignment sparse shape constraint;sparse shape constraint robust face alignment facial landmarks localization facial image analysis dual sparse constrained cascade regression model least squares method training process explicit shape constraints deep convolutional neuron network pose variation fiducial landmarks implicit context information sparse feature selection;regression analysis face recognition feature selection least squares approximations neural nets;shape;research articles;abstracts;feature extraction;open access;life sciences;clinical guidelines;robustness;face;full text;adaptation models;context;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Localizing facial landmarks is a fundamental step in facial image analysis. However, the problem continues to be challenging due to the large variability in expression, illumination, pose, and the existence of occlusions in real-world face images. In this paper, we present a dual sparse constrained cascade regression model for robust face alignment. Instead of using the least-squares method during the training process of regressors, sparse constraint is introduced to select robust features and compress the size of the model. Moreover, sparse shape constraint is incorporated between each cascade regression, and the explicit shape constraints are able to suppress the ambiguity in local features. To improve the model's adaptation to large pose variation, face pose is estimated by five fiducial landmarks located by deep convolutional neuron network, which is used to adaptively design the cascade regression model. To the best of our best knowledge, this is the first attempt to fuse explicit shape constraint (sparse shape constraint) and implicit context information (sparse feature selection) for robust face alignment in the framework of cascade regression. Extensive experiments on nine challenging wild data sets demonstrate the advantages of the proposed method over the state-of-the-art methods.	acclimatization;algorithm;alignment;cascade device component;complement system proteins;compresses (device);computation;dictionary [publication type];dual;embedded system;embedding;experiment;face;feature selection;fiducial marker;fuse device component;image analysis;k-svd;lasso;least squares;mathematical optimization;neuron;pose (computer vision);singular value decomposition;sparse matrix;spatial variability	Qingshan Liu;Jiankang Deng;Dacheng Tao	2016	IEEE Transactions on Image Processing	10.1109/TIP.2015.2502485	face;computer vision;text mining;feature extraction;shape;computer science;machine learning;pattern recognition;sparse approximation;mathematics;robustness	Vision	29.370622364423976	-46.91294745554053	127634
3e40d200c47aeaac4b1b3af7a48362e3386fbc71	incremental on-line learning of human motion using gaussian adaptive resonance hidden markov model	graph theory;learning artificial intelligence art neural nets gait analysis gaussian processes graph theory hidden markov models;t technology;gaussian processes;hidden markov models adaptation models neurons subspace constraints covariance matrices joints robots;hidden markov models;gait analysis;art neural nets;learning artificial intelligence;incremental online learning motion capturing graph node gart gaussian adaptive resonance theory topology learning mechanism human motion pattern encoding tgart hmm topological gaussian adaptive resonance hidden markov model continuous motion observation	In this paper we present an approach for on-line and incremental learning of human motion patterns through continuous observation of motion using novel Topological Gaussian Adaptive Resonance Hidden Markov Model (TGART-HMM). The observed human motion patterns are encoded in a novel modified version of Hidden Markov Model (HMM) called TGART-HMM. The on-line learning process consists of updating the structure of Hidden Markov Model using a topology-learning mechanism based on Gaussian Adaptive Resonance Theory (GART). The model size is adaptable based on the observed motion patterns. The resulting HMM structure is a graph where each node represents an encoded motion pattern. The parameters of TGART-HMM are updated incrementally to incorporate incessant motion patterns. The algorithm is tested on motion captured data to test the efficacy of the system.	adaptive resonance theory;algorithm;encode;experiment;graphics address remapping table;hidden markov model;humanoid robot;human–robot interaction;kinesiology;markov chain;motion capture;online and offline;online machine learning;organizing (structure);self-organization;statistical model	Farhan Dawood;Chu Kiong Loo;Wei Hong Chin	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6706826	markov chain;maximum-entropy markov model;gait analysis;computer science;artificial intelligence;graph theory;machine learning;hidden semi-markov model;pattern recognition;gaussian process;mathematics;markov model;hidden markov model;variable-order markov model	Robotics	36.821655583423805	-41.20812680250759	127635
2bd78639eede66dab3529263a999abfc807b632f	structured learning for multiple object tracking	seqam;rutgers	Adaptive tracking-by-detection methods use previous tracking results to generate a new training set for object appearance, and update the current model to predict the object location in subsequent frames. Such approaches are typically bootstarpped by manual or semi-automatic initialization in the first several frames. However, most adaptive tracking-bydetection methods focus on tracking of a single object or multiple unrelated objects. Although one can trivially engage several single object trackers to track multiple objects, such solution is frequently suboptimal because it does not utilize the inter-object constraints or the obejct layout information [2]. We propose in this paper an adaptive tracking-by-detection method for multiple objects, inspired by recent work in [1] and [2]. The constraints for structured Support Vector Machine (SVM) in [1] are modified to localize multiple objects simultaneously with both appearance and layout information. Moreover, additional binary constraints are introduced to detect the existences of respective objects and to prevent possible model drift. Thus the method can handle frequent occlusion in multiple object tracking, as well as objects entering or leaving the scene. Those binary constraints make the optimization problem significantly different from the original Structured SVM [3]. The inter-object constraints, embedded in a linear programming technique similar to [2] for optimal position assignment, are applied to diminish false detections. In single object tracking case, given a set of frames {x1,x2, . . . ,xn} indexed by time, and the corresponding set of labeling, i.e. bounding box, {y1,y2, . . . ,yn}, structured SVM tries to find a model f (x,y), such that the task of predicting object location in a testing frame x could be conquered by maximizing: f (x,y) = 〈w,Ψ(x|y)〉, (1)	embedded system;hidden surface determination;linear programming;mathematical optimization;minimum bounding box;model f keyboard;optimization problem;semiconductor industry;sensor;structured support vector machine;test set	Wang Yan;Xiaoye Han;Vladimir Pavlovic	2012		10.5244/C.26.48	simulation;computer science;artificial intelligence;computer graphics (images)	Vision	36.98304433565266	-45.12255302042213	127958
1dd2a388838c429fad24c779fc48fc5a1d1cc90a	image characterization and classification by physical complexity	image classification;bennett s logical depth;information content;kolmogorov complexity;visual representation;computational complexity;algorithmic complexity;algorithmic randomness;information theory	We present a method for estimating the complexity of an image based on Bennett’s concept of logical depth. Bennett identified logical depth as the appropriate measure of organized complexity, and hence as being better suited to the evaluation of the complexity of objects in the physical world. Its use results in a different, and in some sense a finer characterization than is obtained through the application of the concept of Kolmogorov complexity alone. We use this measure to classify images by their information content. The method provides a means for classifying and evaluating the complexity of objects by way of their visual representations. To the authors’ knowledge, the method and application inspired by the concept of logical depth presented herein are being proposed and implemented for the first time.	algorithm;analysis of algorithms;blum axioms;carrier-to-noise ratio;computational complexity theory;data compression;experiment;kolmogorov complexity;logical depth;mike lesser;self-information;unsupervised learning	Cédric Gaucherel	2012	Complexity	10.1002/cplx.20388	parameterized complexity;contextual image classification;combinatorics;complexity;average-case complexity;self-information;information theory;quantum complexity theory;computer science;artificial intelligence;theoretical computer science;structural complexity theory;machine learning;worst-case complexity;complexity index;mathematics;computational complexity theory;game complexity;algorithm;descriptive complexity theory;statistics	Vision	32.94844278562575	-38.16071668694145	128252
a213094b94110b9fa5ae6f81382a24b36dd979bc	a wisard-based multi-term memory framework for online tracking of objects		In this paper it is proposed a generic object tracker with realtime performance. The proposed tracker is inspired on the hierarchical short-term and medium-term memories for which patterns are stored as discriminators of a WiSARD weightless neural network. This approach is evaluated through benchmark video sequences published by Babenko et al. Experiments show that the WiSARD-based approach outperforms most of the previous results in the literature, with respect to the same dataset.		Daniel Nascimento;Rafael Lima de Carvalho;Félix Mora-Camino;Priscila Machado Vieira Lima;Felipe Maia Galvão França	2015			computer vision;simulation;computer science;artificial intelligence	AI	31.066582353902962	-50.44194758394193	128258
01add5c7872d7cc5aae36a1bb31f98a3eb84bb23	soccer players identification based on visual local features	sports video analysis;similarity metric;person recognition;video streaming;interest points;sports video;sport video analysis;face recognition;local features;automatic annotation	Semantic detection and recognition of objects and events contained in a video stream has to be performed in order to provide content-based annotation and retrieval of videos. This annotation is done as a means to be able to reuse the video material at a later stage, e.g. to produce new TV programmes. A typical example is that of sports videos, where videos are annotated in order to reuse the video clips that show key highlights and players to produce short summaries for news and sports programmes. In order to select the most interesting actions among all the possibly detected highlights further analysis is required; i.e. the shots that contain a key action are typically followed by close-ups of the players that take part in the action. Therefore the automatic identification of these players would add considerable value both to the annotation and retrieval of the key highlights and key players of a sport event. The problem of detecting and recognizing faces in broadcast videos is a widely studied topic. However, in the case of soccer videos, and sports videos in general, the current techniques are not suitable for the task of face recognition, due to the high variations in pose, illumination, scale and occlusion that may happen in an uncontrolled environment. In this paper a method that copes with these problems, exploiting local features to describe a face, without requiring a precise localization of the distinguishing parts of a face, and the set of poses to describe a person and perform a more robust recognition, is presented. A similarity metric based on the number of matched interest points, able to cope with different face sizes, is also presented and experimentally validated.	automatic identification and data capture;experiment;facial recognition system;sensor;streaming media;television;uncontrolled format string;video clip	Lamberto Ballan;Marco Bertini;Alberto Del Bimbo;Walter Nunziati	2007		10.1145/1282280.1282321	facial recognition system;computer vision;simulation;computer science;multimedia	Vision	33.511169834025075	-50.73059884759123	128345
6879881d45a98c0aa8660f593528f97ba919ec38	learning inter-related visual dictionary for object recognition	optimisation correlation methods dictionaries image classification learning artificial intelligence object recognition;object recognition;optimisation;visual benchmarks learning interrelated visual dictionary object recognition joint dictionary learning algorithm jdl algorithm visual correlation commonly shared dictionary multiple category specific dictionaries joint optimization fisher discrimination criterion jdl model;jdl model;joint optimization;visual correlation;training;visual benchmarks;image classification;dictionaries visualization optimization training learning systems sparse matrices joints;joints;correlation methods;learning systems;visualization;learning interrelated visual dictionary;jdl algorithm;fisher discrimination criterion;joint dictionary learning algorithm;dictionaries;commonly shared dictionary;optimization;learning artificial intelligence;multiple category specific dictionaries;sparse matrices	Object recognition is challenging especially when the objects from different categories are visually similar to each other. In this paper, we present a novel joint dictionary learning (JDL) algorithm to exploit the visual correlation within a group of visually similar object categories for dictionary learning where a commonly shared dictionary and multiple category-specific dictionaries are accordingly modeled. To enhance the discrimination of the dictionaries, the dictionary learning problem is formulated as a joint optimization by adding a discriminative term on the principle of the Fisher discrimination criterion. As well as presenting the JDL model, a classification scheme is developed to better take advantage of the multiple dictionaries that have been trained. The effectiveness of the proposed algorithm has been evaluated on popular visual benchmarks.	algorithm;benchmark (computing);cellular automaton;dictionary;machine learning;mathematical optimization;ontology components;outline of object recognition;unsupervised learning;visual objects	Ning Zhou;Yi Shen;Jinye Peng;Jianping Fan	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6248091	natural language processing;contextual image classification;visualization;sparse matrix;k-svd;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition	Vision	25.785334006617926	-45.641177131309725	128348
942cf897d641e5bcaeef100954aec240262880e1	image inpainting based on sparse representation with dictionary pre-clustering		This paper proposed a new image inpainting algorithm based on sparse representation. In traditional exemplar-based methods, the image patch is inpainted by the best matched patch from the source region. This greedy search will introduce unwanted objects and has huge time consuming. The proposed algorithm directly employs all the known image patches to form an over-complete dictionary. And then, the over-complete dictionary is clustered into several sub-dictionaries. Finally, the unrepaired image patches are repaired over their corresponding closest sub-dictionaries through non-negative orthogonal matching pursuit algorithm. Experimental results show that the proposed method achieves superior performance than state-of-the-art methods. In addition, the time complexity is greatly reduced in comparison with the traditional exemplar-based inpainting algorithm.	cluster analysis;dictionary;inpainting;sparse approximation	Kai Xu;Nannan Wang;Xinbo Gao	2016		10.1007/978-981-10-3005-5_21	k-svd;pattern recognition	Vision	28.43360565058693	-46.28630630364002	128397
e46f60c9efb1ab749be85a70fd298845d1555ae3	a fast learning algorithm for robotic emotion recognition	emotion recognition support vector machines support vector machine classification face recognition robotics and automation human robot interaction intelligent robots control engineering training data computational intelligence;robotic emotion recognition;learning algorithm;support vector machines;gaussian processes;intelligent robots;computational intelligence;svm hyperplane;image classification;emotion recognition;control engineering;human robot interaction;feature space;support vector;gaussian kernel space;iterative methods;training data;face recognition;robots;visual database;proceedings paper;gaussian kernel;visual database incremental learning algorithm robotic emotion recognition human robot interaction facial data svm hyperplane gaussian kernel space support vector pursuit learning image classification;support vector pursuit learning;support vector machine classification;facial data;incremental learning algorithm;learning artificial intelligence;man machine systems;robotics and automation;visual databases emotion recognition face recognition gaussian processes image classification iterative methods learning artificial intelligence man machine systems robots support vector machines;visual databases;emotional expression	The capability of robotic emotion recognition is an important factor for human-robot interaction. In order to facilitate a robot to function in daily live environments, a emotion recognition system needs to accommodate itself to various persons. In this paper, an emotion recognition system that can adapt to new facial data is proposed. The main idea of the proposed learning algorithm is to adjust parameters of SVM hyperplane for learning emotional expressions of a new face. After mapping the input space to Gaussian-kernel space, support vector pursuit learning (SVPL) is applied to retrain the hyperplane in the new feature space. To expedite the retraining procedure, only samples classified incorrectly in previous iteration are combined with critical historical sets to restrain a new SVM classifier. After adjusting hyperplane parameters, the new classifier will recognize previous erroneous facial data. Experimental results show that the proposed system recognize new facial data with high correction rates after fast retraining the hyperplane. Moreover, the proposed method also keeps satisfactory recognition rate of old facial samples.	algorithm;emotion recognition;feature vector;human–robot interaction;iteration;robot	Jung-Wei Hong;Meng-Ju Han;Kai-Tai Song;Fuh-Yu Chang	2007	2007 International Symposium on Computational Intelligence in Robotics and Automation	10.1109/CIRA.2007.382865	support vector machine;computer vision;computer science;artificial intelligence;machine learning;computational intelligence;pattern recognition	Robotics	35.31810298037356	-43.406598145788614	128501
443acd268126c777bc7194e185bec0984c3d1ae7	retrieving relative soft biometrics for semantic identification	neural networks;biometrics access control;surveillance;training;semantics;machine learning;cameras	Automatically describing pedestrians in surveillance footage is crucial to facilitate human accessible solutions for suspect identification. We aim to identify pedestrians based solely on human description, by automatically retrieving semantic attributes from surveillance images, alleviating exhaustive label annotation. This work unites a deep learning solution with relative soft biometric labels, to accurately retrieve more discriminative image attributes. We propose a Semantic Retrieval Convolutional Neural Network to investigate automatic retrieval of three soft biometric modalities, across a number of ‘closed-world’ and ‘open-world’ re-identification scenarios. Findings suggest that relative-continuous labels are more accurately predicted than absolute-binary and relative-binary labels, improving semantic identification in every scenario. Furthermore, we demonstrate a top rank-1 improvement of 23.2% and 26.3% over a traditional, baseline retrieval approach, in one-shot and multi-shot re-identification scenarios respectively.	baseline (configuration management);bitwise operation;closed-circuit television;convolutional neural network;deep learning;open world;soft biometrics	Daniel Martinho-Corbishley;Mark S. Nixon;John N. Carter	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900105	computer vision;computer science;machine learning;pattern recognition;data mining;semantics;artificial neural network	Vision	32.13688939635063	-51.0911636499587	128548
07c935411c597b16e3c1c46796ecd1cd4f072e8f	manifold learning by graduated optimization	unsupervised learning;optimisation learning artificial intelligence;optimisation;empirical analysis;prior knowledge;manifold learning;convex functions;nonlinear dimensionality reduction;unsupervised learning manifold learning nonlinear dimensionality reduction;convex function;optimization algorithm design and analysis convex functions unsupervised learning;maximum variance unfolding;optimization;learning artificial intelligence;landmark maximum variance unfolding algorithm manifold learning graduated optimization manifold sculpting algorithm manifold embedding isomap algorithm locally linear embedding algorithm hessian lle algorithm;optimal algorithm;algorithm design and analysis;local linear embedding	We present an algorithm for manifold learning called manifold sculpting , which utilizes graduated optimization to seek an accurate manifold embedding. An empirical analysis across a wide range of manifold problems indicates that manifold sculpting yields more accurate results than a number of existing algorithms, including Isomap, locally linear embedding (LLE), Hessian LLE (HLLE), and landmark maximum variance unfolding (L-MVU), and is significantly more efficient than HLLE and L-MVU. Manifold sculpting also has the ability to benefit from prior knowledge about expected results.	algorithm;arc diagram;graduated optimization;hessian;isomap;linear iga bullous dermatosis;mds-updrs - rest tremor amplitude left lower extremity;manifold regularization;mathematical optimization;nonlinear dimensionality reduction;sample variance;semidefinite embedding;unfolding (dsp implementation)	Michael S. Gashler;Dan Ventura;Tony R. Martinez	2011	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2011.2151187	convex function;unsupervised learning;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;nonlinear dimensionality reduction;manifold alignment	ML	25.1702932228846	-38.49583923994022	128550
ed530a38be935185264775579865dd968c496089	spectral clustering using multilinear svd: analysis, approximations and applications	clustering;hypergraph partitioning;multilinear svd	Spectral clustering, a graph partitioning technique, has gained immense popularity in machine learning in the context of unsupervised learning. This is due to convincing empirical studies, elegant approaches involved and the theoretical guarantees provided in the literature. To tackle some challenging problems that arose in computer vision etc., recently, a need to develop spectral methods that incorporate multi-way similarity measures surfaced. This, in turn, leads to a hypergraph partitioning problem. In this paper, we formulate a criterion for partitioning uniform hypergraphs, and show that a relaxation of this problem is related to the multilinear singular value decomposition (SVD) of symmetric tensors. Using this, we provide a spectral technique for clustering based on higher order affinities, and derive a theoretical bound on the error incurred by this method. We also study the complexity of the algorithm and use Nyström’s method and column sampling techniques to develop approximate methods with significantly reduced complexity. Experiments on geometric grouping and motion segmentation demonstrate the practical significance of the proposed methods.	affinity analysis;approximation algorithm;cluster analysis;computer vision;experiment;graph partition;k-svd;linear programming relaxation;machine learning;partition problem;sampling (signal processing);singular value decomposition;spectral clustering;spectral method;time complexity;unsupervised learning	Debarghya Ghoshdastidar;Ambedkar Dukkipati	2015			mathematical optimization;computer science;machine learning;cluster analysis;statistics	AI	26.877160246338452	-38.29007469994993	128586
d356893e5eb4ba3960ec60c88228e8e977025ba8	rgb-d salient object detection based on discriminative cross-modal transfer learning		"""In this work, we propose to utilize Convolutional Neural Networks (CNNs) to boost the performance of depth-induced salient object detection by capturing the high-level representative features for depth modality. We formulate the depth-induced saliency detection as a CNN-based cross-modal transfer problem to bridge the gap between the """" data-hungry """" nature of CNNs and the unavailability of sufficient labeled training data in depth modality. In the proposed approach, we leverage the auxiliary data from the source modality (i.e., RGB) effectively by training the RGB saliency detection network to obtain the task-specific pre-understanding layers for the target modality (i.e., depth). Meanwhile, we exploit the depth-specific information by pre-training a modality classification network that encourages modal-specific representations during the optimizing course. Thus, it could make the feature representations of the RGB and depth modalities as discriminative as possible. These two modules are pre-trained independently and then stitched to initialize and optimize the eventual depth-induced saliency detection model. Experiments demonstrate the effectiveness of the proposed novel pre-training strategy as well as the significant and consistent improvements of the proposed approach over other state-of-the-art methods."""	aerial photography;convolutional neural network;experiment;high- and low-level;image stitching;modal logic;modality (human–computer interaction);object detection;point cloud;sensor;unavailability	Hao Chen;Yumin Li;Dan Su	2017	CoRR		computer vision;computer science;machine learning;pattern recognition	AI	27.619043162323212	-50.47656630751042	128619
3e32b4775f3aeff0e1eaf37cc46771376a0c8d08	a novel transductive svm for semisupervised classification of remote-sensing images	ill posed problems;remote sensing image;teledetection;model selection;time dependent;difference operator;support vector machines;classification supervisee;unlabeled patterns;image classification;estrategia;inference mechanisms;classification;iterative algorithm;deteccion a distancia;algorithme;strategy;iterative methods;labeled and unlabeled patterns;transductive inference;machine vecteur support;modelo;apprentissage;geophysics computing;transductive inference ill posed problems labeled and unlabeled patterns machine learning remote sensing semisupervised classification support vector machines svms;machine learning;statistical learning theory;remote sensing;theory;ill posed problem;teoria;support vector machines geophysics computing image classification inference mechanisms iterative methods learning artificial intelligence remote sensing;algorithms;ill posed remote sensing problems;modele;transductive svm;semisupervised classification;labeled patterns;support vector machine;remote sensing images;support vector machines support vector machine classification remote sensing classification algorithms communications technology training data hyperspectral sensors statistical learning iterative algorithms kernel;learning artificial intelligence;machine learning support vector machines transductive svm semisupervised classification remote sensing images ill posed remote sensing problems statistical learning theory transductive inference iterative algorithm labeled patterns unlabeled patterns;strategie;models;clasificacion;support vector machines svms;theorie;algoritmo	This paper introduces a semisupervised classification method that exploits both labeled and unlabeled samples for addressing ill-posed problems with support vector machines (SVMs). The method is based on recent developments in statistical learning theory concerning transductive inference and in particular transductive SVMs (TSVMs). TSVMs exploit specific iterative algorithms which gradually search a reliable separating hyperplane (in the kernel space) with a transductive process that incorporates both labeled and unlabeled samples in the training phase. Based on an analysis of the properties of the TSVMs presented in the literature, a novel modified TSVM classifier designed for addressing ill-posed remote-sensing problems is proposed. In particular, the proposed technique: 1) is based on a novel transductive procedure that exploits a weighting strategy for unlabeled patterns, based on a time-dependent criterion; 2) is able to mitigate the effects of suboptimal model selection (which is unavoidable in the presence of small-size training sets); and 3) can address multiclass cases. Experimental results confirm the effectiveness of the proposed method on a set of ill-posed remote-sensing classification problems representing different operative conditions	algorithm;iterative method;machine learning;model selection;semi-supervised learning;statistical learning theory;support vector machine;transduction (machine learning);user space;well-posed problem	Lorenzo Bruzzone;Mingmin Chi;Mattia Marconcini	2006	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2006.877950	support vector machine;transduction;computer science;machine learning;pattern recognition;data mining;iterative method	ML	30.730951253100624	-42.31124289468767	128761
5754fba6500c553ddeb15caaf941f7903ef59b8e	application of radial basis function network and locality preserving projections for face recognition	transfer functions face recognition gradient methods image classification radial basis function networks;locality preserving projection;face manifold;gradient descent algorithm radial basis function network locality preserving projections face recognition linear method local structure preservation transformation difference face manifold feature classification multiquadrics function activation function;transfer functions;activation function;multiquadrics function;image classification;radial basis function networks;local structure preservation;locality preserving projections;local structure;face recognition;radial basis function network;gradient descent;gradient methods;linear method;transformation difference;radial basis function networks face recognition principal component analysis linear discriminant analysis face detection automation laplace equations multi layer neural network mean square error methods application software;feature classification;gradient descent algorithm	Locality preserving projections (LPP) is a linear method that optimally preserves the local structure of the data set. However, because of the continuing existence of transformation difference, LPP subspace is failed to detect the important nonlinear variations of the face manifold. In order to improve the recognition performance, we propose to use radial basis function network (RBFN) to classify the features in the LPP subspace. The multi-quadrics function is taken as the activation function of the RBFN and the hidden layer of RBFN is trained via the gradient descent algorithm in our approach. The experimental results show that the LPP+RBFN method has achieved a better rate than Laplacianfaces, Fisherfaces and Eigenfaces.	activation function;algorithm;eigenface;facial recognition system;format-preserving encryption;gradient descent;locality of reference;nonlinear system;radial (radio);radial basis function network	Jian-Qiang Mei;Zhengguang Liu;Ming Ming	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.262	mathematical optimization;machine learning;pattern recognition;mathematics	Vision	27.37019555921207	-41.29271650940581	128784
760dfe23e37baf3f224adfff5e2c130e03d9eb89	head pose estimation based on manifold embedding and distance metric learning	mahalanobis distance;empirical study;distance metric learning;head pose estimation;k nearest neighbor;pose estimation	In this paper, we propose an embedding method to seek an optimal low-dimensional manifold describing the intrinsical pose variations and to provide an identity-independent head pose estimator. In order to handle the appearance variations caused by identity, we use a learned Mahalanobis distance to seek optimal subjects with similar manifold to construct the embedding. Then, we propose a new smooth and discriminative embedding method supervised by both pose and identity information. To estimate pose of a head new image, we first find its knearest neighbors of different subjects, and then embed it into the manifold of the subjects to estimate the pose angle. The empirical study on the standard databases demonstrates that the proposed method achieves high pose estimation accuracy.	3d pose estimation;database;linear separability	Xiangyang Liu;Hongtao Lu;Daqiang Zhang	2009		10.1007/978-3-642-12307-8_6	computer vision;pose;computer science;mahalanobis distance;machine learning;pattern recognition;empirical research;k-nearest neighbors algorithm	Vision	26.57913110444049	-43.39776064922776	128871
2dd6c988b279d89ab5fb5155baba65ce4ce53c1e	learning deformable shape manifolds	biological patents;biomedical journals;text mining;europe pubmed central;citation search;manifold learning;citation networks;face recognition;research articles;detailed face shape detection;abstracts;open access;life sciences;clinical guidelines;shape modeling;full text;face detection;rest apis;orcids;europe pmc;biomedical research;nonlinear regression;bioinformatics;literature search	We propose an approach to shape detection of highly deformable shapes in images via manifold learning with regression. Our method does not require shape key points be defined at high contrast image regions, nor do we need an initial estimate of the shape. We only require sufficient representative training data and a rough initial estimate of the object position and scale. We demonstrate the method for face shape learning, and provide a comparison to nonlinear Active Appearance Model. Our method is extremely accurate, to nearly pixel precision and is capable of accurately detecting the shape of faces undergoing extreme expression changes. The technique is robust to occlusions such as glasses and gives reasonable results for extremely degraded image resolutions.	active appearance model;algorithm;energy minimization;experiment;face;feature (computer vision);feature vector;generative model;iterative method;nonlinear dimensionality reduction;nonlinear system;obstruction;pixel;sensor;statistical classification;benefit;manifold	Samuel Rivera;Aleix M. Martínez	2012	Pattern recognition	10.1016/j.patcog.2011.09.023	active shape model;computer vision;face detection;text mining;computer science;data science;machine learning;pattern recognition;data mining;shape analysis;nonlinear dimensionality reduction;nonlinear regression;statistics	Vision	29.945530595336038	-47.90482230832194	129200
86274e426bfe962d5cb994d5d9c6829f64410c32	face recognition in different subspaces: a comparative study	projection method;computer and information science;comparative study;face recognition;image analysis	Face recognition is one of the most successful applications of image analysis and understanding and has gained much attention in recent years. Among many approaches to the problem of face recognition, appearance-based subspace analysis still gives the most promising results. In this paper we study the three most popular appearance-based face recognition projection methods (PCA, LDA and ICA). All methods are tested in equal working conditions regarding preprocessing and algorithm implementation on the FERET data set with its standard tests. We also compare the ICA method with its whitening preprocess and find out that there is no significant difference between them. When we compare different projection with different metrics we found out that the LDA+COS combination is the most promising for all tasks. The L1 metric gives the best results in combination with PCA and ICA1, and COS is superior to any other metric when used with LDA and ICA2. Our results are compared to other studies and some discrepancies are pointed out.	algorithm;cos;feret (facial recognition technology);facial recognition system;image analysis;independent computing architecture;linear discriminant analysis;local-density approximation;preprocessor;principal component analysis;through-hole technology;whitening transformation	Borut Batagelj;Franc Solina	2006				Vision	25.03030137848063	-40.5564614494381	129287
1e9461b2e48e11638b85c2f2dc7bca043f9d60a8	gait representation using flow fields	flow field;optical flow	Gait is characterised by the relative motions between different body parts during walking. However, most recently proposed gait representation approaches such as Gait Energy Image (GEI) and Motion Silhouettes Image (MSI) capture only the motion intensity information whilst ignoring the equally important but less reliable information about the direction of relative motion. They thus essentially sacrifice discriminative power in exchange for robustness. In this paper, we propose a novel gait representation based on optical flow fields computed from normalized and centred person images over a complete gait cycle. In our representation, both the motion intensity and the motion direction information is captured in a set of motion descriptors. To achieve robustness against noise, instead of relying on the exact value of the flow vectors, the flow direction is discretised and a histogram based direction representation is formulated. Compared to the existing model-free gait representations, our representation is not only more discriminative, but also less sensitive to changes in various covariate conditions including clothing, carrying, shoe, and speed. Extensive experiments on both indoor and outdoor public datasets have been carried out to demonstrate that our representation outperforms the state-of-the-art.	discretization;experiment;optical flow;while	Khalid Bashir;Tao Xiang;Shaogang Gong	2009		10.5244/C.23.113	computer vision;simulation;computer science;optical flow	Vision	36.6802466078846	-49.84676057530373	129295
bfeed5fb15576c6b17db95fff7618866e288e6f3	decision rule steered discriminant analysis: a paradigm of unifying dimension reduction and classification into a framework	decision rule steered discriminant analysis;eigenvalues and eigenfunctions;local mean discriminant analysis;pattern classification data reduction feature extraction;nearest neighbor searches;fisher linear discriminant analysis;dimension reduction;cenparmi handwritten numeral database;nearest mean classifier;eth80 object category database decision rule steered discriminant analysis dimension reduction algorithm pattern recognition nearest mean classifier global mean disciminant analysis gmda classical fisher linear discriminant analysis flda optimal feature extractor nearest local mean classifier local mean discriminant analysis lmda algorithm cenparmi handwritten numeral database;training;training feature extraction classification algorithms nearest neighbor searches eigenvalues and eigenfunctions transforms algorithm design and analysis;lmda algorithm;nearest local mean classifier;discriminant analysis;optimal feature extractor;feature extraction;classical fisher linear discriminant analysis;classification algorithms;transforms;pattern classification;eth80 object category database;pattern recognition;gmda;data reduction;flda;global mean disciminant analysis;algorithm design and analysis;decision rule;dimension reduction algorithm	Dimension reduction (feature extraction) and classification are two elementary tasks in pattern recognition. This paper presents a paradigm of unifying dimension reduction and classification tasks into one framework. We start with a simplest classifier, the nearest (global) mean classifier, and use its decision rule to steer the design of the global mean disciminant analysis (GMDA). GMDA is proven equivalent to the classical Fisher linear discriminant analysis (FLDA). FLDA is thus an optimal feature extractor for the nearest (global) mean classifier. We then consider the nearest local mean classifier and use its decision rule to steer the design of the local mean discriminant analysis (LMDA). LMDA matches the nearest local mean classifier optimally in theory. The proposed LMDA algorithm has two advantages over the current dimension reduction algorithms. First, it has a natural connection to classification. Second, it examines the separability of samples in the transformed space where classifiers works thereby it can achieve more desirable performance. Experiments are done on the CENPARMI handwritten numeral database and the ETH80 object category database and results confirm our idea and the effectiveness of the proposed algorithm.	algorithm;dimensionality reduction;feature extraction;linear discriminant analysis;linear separability;mean squared error;pattern recognition;programming paradigm;randomness extractor;statistical classification	Jian Yang	2010	9th IEEE International Conference on Cognitive Informatics (ICCI'10)	10.1109/COGINF.2010.5599830	quadratic classifier;machine learning;pattern recognition;data mining;mathematics	ML	28.384188747097447	-42.08060862243026	129297
3fa28a144722deacd7472e8507545429bdd25aaf	spectral–spatial classification for hyperspectral data using rotation forests with local feature extraction and markov random fields	hyperspectral spectral spatial classification hyperspectral data markov random fields hyperspectral images rotation forest integration class probability spectral information subset features classifiers ensemble classification and regression tree base classifier cart linear feature extraction method principal component analysis neighborhood preserving embedding linear local tangent space alignment linearity preserving projection mrf maximum a posteriori problem α expansion graph cuts optimization method rotation forest ensemble supervised classification method local feature extraction method npe lltsa lpp pca mrf spatial contextual information;feature extraction hyperspectral imaging training accuracy training data;training;rotation forests feature extraction hyperspectral image classification markov random fields mrfs;training data;accuracy;feature extraction;trees mathematics feature extraction geophysical image processing graph theory hyperspectral imaging image classification markov processes maximum likelihood estimation optimisation principal component analysis regression analysis;hyperspectral imaging	In this paper, we propose a new spectral-spatial classification strategy to enhance the classification performances obtained on hyperspectral images by integrating rotation forests and Markov random fields (MRFs). First, rotation forests are performed to obtain the class probabilities based on spectral information. Rotation forests create diverse base learners using feature extraction and subset features. The feature set is randomly divided into several disjoint subsets; then, feature extraction is performed separately on each subset, and a new set of linear extracted features is obtained. The base learner is trained with this set. An ensemble of classifiers is constructed by repeating these steps several times. The weak classifier of hyperspectral data, classification and regression tree (CART), is selected as the base classifier because it is unstable, fast, and sensitive to rotations of the axes. In this case, small changes in the training data of CART lead to a large change in the results, generating high diversity within the ensemble. Four feature extraction methods, including principal component analysis (PCA), neighborhood preserving embedding (NPE), linear local tangent space alignment (LLTSA), and linearity preserving projection (LPP), are used in rotation forests. Second, spatial contextual information, which is modeled by MRF prior, is used to refine the classification results obtained from the rotation forests by solving a maximum a posteriori problem using the α-expansion graph cuts optimization method. Experimental results, conducted on three hyperspectral data with different resolutions and different contexts, reveal that rotation forest ensembles are competitive with other strong supervised classification methods, such as support vector machines. Rotation forests with local feature extraction methods, including NPE, LLTSA, and LPP, can lead to higher classification accuracies than that achieved by PCA. With the help of MRF, the proposed algorithms can improve the classification accuracies significantly, confirming the importance of spatial contextual information in hyperspectral spectral-spatial classification.	algorithm;control theory;cut (graph theory);decision tree learning;feature extraction;local tangent space alignment;machine learning;markov chain;markov random field;mathematical optimization;norton power eraser;performance;principal component analysis;randomness;supervised learning;support vector machine	Junshi Xia;Jocelyn Chanussot;Peijun Du;Xiyan He	2015	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2014.2361618	computer vision;training set;feature extraction;hyperspectral imaging;machine learning;pattern recognition;mathematics;accuracy and precision;remote sensing	Vision	28.927888960946532	-43.835049420893704	129313
8241008f9d3d5e866f648eb454db2054202121ef	heterogeneous multi-task learning for human pose estimation with deep convolutional neural network	deep learning;human pose estimation	We propose a heterogeneous multi-task learning framework for human pose estimation from monocular images using a deep convolutional neural network. In particular, we simultaneously learn a human pose regressor and sliding-window body-part and joint-point detectors in a deep network architecture. We show that including the detection tasks helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several datasets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts.	3d pose estimation;artificial neural network;computer multitasking;converge;convolutional neural network;multi-task learning;network architecture;sensor	Sijin Li;Zhi-Qiang Liu;Antoni B. Chan	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops	10.1007/s11263-014-0767-8	computer vision;computer science;machine learning;pattern recognition;deep belief network	Vision	25.90699348039144	-50.94673269039043	129326
eb65bea4586db7aff66895d13c1a6b94c5364497	integration of uncertainty in the analysis of dyadic human activities	video signal processing computer vision gesture recognition image classification;uncertainty;uncertainty feature extraction encoding visualization vocabulary pipelines activity recognition;video classification dyadic human activities video data computer vision action analysis problem activity recognition methods time slices;activity prediction;time slice activity prediction uncertainty categorical likelihoods;time slice;categorical likelihoods	Action analysis from video data has been attract-ing more and more attention in computer vision over the pastdecade. The main focus has been classifying videos into oneof k action classes from fully observed videos. However, allof the time, intelligent systems are enforced to make decisionsunder uncertainty and based on incomplete information. Thisneed motivates us to introduce the problem of analyzing theuncertainty associated with dyadic human activities and moveto a new level of generality in the action analysis problem. Analyzing the uncertainty, here, refers to categorizing thelikelihood of activities in trimmed video clips called time-slices which are extracted from the full video. To this intent, we exploit the state-of-the-art methods to extract interestpoints in time-slices and represent them. We also present anaccumulative uncertainty to depict the uncertainty associatedwith partially observed videos for the task of early activityrecognition. The experiments demonstrate the effectiveness ofour framework in analyzing dyadic activities under uncertaintyand in evaluating the performance of early activity recognition methods.	activity diagram;activity recognition;algorithm;categorization;computer vision;dyadic transformation;experiment;farmville;preemption (computing);robert;scale-invariant feature transform;video clip	Maryam Ziaeefard;Robert Bergevin	2016	2016 13th Conference on Computer and Robot Vision (CRV)	10.1109/CRV.2016.35	computer vision;speech recognition;uncertainty;computer science;artificial intelligence;machine learning;preemption;statistics	Vision	33.517273423997075	-49.235362632531185	129413
53a2ae4eaa3467cbb6c71805e2f33a76a8854550	sampling hidden objects using nearest-neighbor oracles	sample size;hidden web;size estimation;voronoi cell;sampling;nearest neighbors;nearest neighbor	Given an unknown set of objects embedded in the Euclidean plane and a nearest-neighbor oracle, how to estimate the set size and other properties of the objects? In this paper we address this problem. We propose an efficient method that uses the Voronoi partitioning of the space by the objects and a nearest-neighbor oracle. Our method can be used in the hidden web/databases context where the goal is to estimate the number of certain objects of interest. Here, we assume that each object has a geographic location and the nearest-neighbor oracle can be realized by applications such as maps, local, or store-locator APIs. We illustrate the performance of our method on several real-world datasets.	binary space partitioning;dark web;database;deep web;embedded system;geographic coordinate system;nearest neighbor search;online locator service	Nilesh N. Dalvi;Ravi Kumar;Ashwin Machanavajjhala;Vibhor Rastogi	2011		10.1145/2020408.2020606	sample size determination;sampling;computer science;machine learning;pattern recognition;data mining;mathematics;k-nearest neighbors algorithm;deep web;statistics	ML	31.999279279303945	-39.96437723147765	129634
391585c531f9a3c14dfdd580192252a878a777a0	image retrieval using nonlinear manifold embedding	image database;user feedback;linear discriminate analysis;semi supervised learning;dimensionality reduction;principal component analysis;semantic gap;visual features;kernel method;content based image retrieval;dimensional reduction;relevance feedback;image retrieval	The huge number of images on the Web gives rise to the content-based image retrieval (CBIR) as the text-based search techniques cannot cater to the needs of precisely retrieving Web images. However, CBIR comes with a fundamental flaw: the semantic gap between high-level semantic concepts and lowlevel visual features. Consequently, relevance feedback is introduced into CBIR to learn the subjective overwhelmed by the large number of dimensionalities of the visual feature space. To address this issue, a novel semi-supervised learning method for dimensionality reduction, namely kernel maximum margin projection (KMMP) is proposed in this paper based on our previous work of maximum margin projection (MMP). Unlike traditional dimensionality reduction algorithms such as principal component analysis (PCA) and linear discriminant analysis (LDA), which only see the global Euclidean structure, KMMP is designed for discovering the local manifold structure. After projecting the images into a lower dimensional subspace, KMMP significantly improves the performance of image retrieval. The experimental results on Corel image database demonstrate the effectiveness of our proposed nonlinear algorithm. & 2009 Elsevier B.V. All rights reserved.	algorithm;content-based image retrieval;euler;feature vector;flaw hypothesis methodology;high- and low-level;hilbert space;homology (biology);linear discriminant analysis;manifold regularization;map;mathematics-mechanization platform;nonlinear dimensionality reduction;nonlinear system;principal component analysis;relevance feedback;semi-supervised learning;semiconductor industry;supervised learning;text-based (computing);world wide web	Can Wang;Jun Zhao;Xiaofei He;Chun Chen;Jiajun Bu	2009	Neurocomputing	10.1016/j.neucom.2009.04.011	computer vision;kernel method;visual word;image retrieval;computer science;machine learning;pattern recognition;mathematics;semantic gap;dimensionality reduction;principal component analysis	AI	24.623779610443663	-42.00874750873928	129659
fd6c853ac1ccb83aa493a5e2495c34dd66bdbdc2	combining deep convolutional neural network and svm to sar image target recognition		To address the challenging problem on target recognition from synthetic aperture radar (SAR) images, a novel method is proposed by combining Deep Convolutional Neural Network (DCNN) and Support Vector Machine (SVM). First, an improved DCNN is employed to learn the features of SAR images. Then, a SVM is utilized to map the leant features into the output labels. To enhance the feature extraction capability of DCNN, a class of separation information is also added to the cross-entropy cost function as a regularization term. As a result, this explicitly facilitates the intra-class compactness and separability in the process of feature learning. Numerical experiments are performed on the Moving and Stationary Target Acquisition and Recognition (MSTAR) database. The results demonstrate that the proposed method can achieve an average accuracy of 99.15% on ten types of targets.		Lada Vukušić;Teng Huang;Jun Wang;Jin Ping Sun;Erfu Yang;Amir Hussain	2017	2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)	10.1109/iThings-GreenCom-CPSCom-SmartData.2017.165	support vector machine;convolutional neural network;feature extraction;artificial intelligence;synthetic aperture radar;computer science;pattern recognition	Vision	29.55674692608444	-44.88453018734106	129689
8ca486941720d25e7b63b440282da711c41d728b	mplp++: fast, parallel dual block-coordinate ascent for dense graphical models		Dense, discrete Graphical Models with pairwise potentials are a powerful class of models which are employed in state-of-the-art computer vision and bio-imaging applications. This work introduces a new MAP-solver, based on the popular Dual Block-Coordinate Ascent principle. Surprisingly, by making a small change to the low-performing solver, the Max Product Linear Programming (MPLP) algorithm [1], we derive the new solver MPLP++ that significantly outperforms all existing solvers by a large margin, including the state-of-the-art solver Tree-Reweighted Sequential (TRW-S) message-passing algorithm [2]. Additionally, our solver is highly parallel, in contrast to TRW-S, which gives a further boost in performance with the proposed GPU and multi-thread CPU implementations. We verify the superiority of our algorithm on dense problems from publicly available benchmarks, as well, as a new benchmark for 6D Object Pose estimation. We also provide an ablation study with respect to graph density.	algorithm;benchmark (computing);british informatics olympiad;central processing unit;computer graphics;computer vision;graphical model;graphics processing unit;linear programming;message passing;solver;times ascent	Siddharth Tourani;Alexander Shekhovtsov;Carsten Rother;Bogdan Savchynskyy	2018		10.1007/978-3-030-01225-0_16	implementation;machine learning;artificial intelligence;theoretical computer science;computer science;dense graph;pose;linear programming;solver;graphical model;pairwise comparison	Vision	34.05534365403117	-45.930861599090164	129830
aa469a00ecc50ff7c4310cd2f0128a554109a4b7	human motion segmentation via robust kernel sparse subspace clustering		Studies on human motion have attracted a lot of attentions. Human motion capture data, which much more precisely records human motion than videos do, has been widely used in many areas. Motion segmentation is an indispensable step for many related applications, but current segmentation methods for motion capture data do not effectively model some important characteristics of motion capture data, such as Riemannian manifold structure and containing non-Gaussian noise. In this paper, we convert the segmentation of motion capture data into a temporal subspace clustering problem. Under the framework of sparse subspace clustering, we propose to use the geodesic exponential kernel to model the Riemannian manifold structure, use correntropy to measure the reconstruction error, use the triangle constraint to guarantee temporal continuity in each cluster and use multi-view reconstruction to extract the relations between different joints. Therefore, exploiting some special characteristics of motion capture data, we propose a new segmentation method, which is robust to non-Gaussian noise, since correntropy is a localized similarity measure. We also develop an efficient optimization algorithm based on block coordinate descent method to solve the proposed model. Our optimization algorithm has a linear complexity while sparse subspace clustering is originally a quadratic problem. Extensive experiment results both on simulated noisy data set and real noisy data set demonstrate the advantage of the proposed method.	algorithm;articular system;attention;cluster analysis;clustering high-dimensional data;coordinate descent;dictionary [publication type];experiment;frame (physical object);kernel method;kinesiology;linear programming;mathematical optimization;motion capture;normal statistical distribution;numerous;quadratic equation;scott continuity;signal-to-noise ratio;similarity measure;sparse approximation;sparse matrix;biologic segmentation;exponential;manifold;statistical cluster;videocassette	Guiyu Xia;Huaijiang Sun;Lei Feng;Guoqing Zhang;Yazhou Liu	2018	IEEE Transactions on Image Processing	10.1109/TIP.2017.2738562	robustness (computer science);geodesic;computer vision;kernel (linear algebra);artificial intelligence;coordinate descent;similarity measure;mathematics;pattern recognition;cluster analysis;motion estimation;subspace topology	Vision	29.00316416135454	-40.647166238278515	129967
a6871fae631cf8abe55e99b826a892a07d6e182b	graph-based video sequence matching using dominant colour graph profile (dcgp)		This paper presents a fast and effective technique for videos’ visual similarity detection and measurement using compact fixed-length signatures. The proposed technique (dominant colour graph profile DCGP) extracts and encodes the spatio-temporal information of a given video shot into a graph-based structure (tree) that fully captures this vital information. The graph structured properties are utilized to construct a fixed-length video signature of 112 decimal values per video shot. The encoded spatio-temporal information is extracted following channelling each video frame into a block-based structure, where the positions of respective blocks are tracked across video frames and encoded into multiple DCGP trees. The proposed technique provides a high matching speed (u003e2000 fps) and robust retrieval performance. The experiments on various standard and challenging datasets shows the framework’s robust performance, in terms of both, retrieval and computational performances.		Saddam Bekhet;Amr Ahmed	2018	Signal, Image and Video Processing	10.1007/s11760-017-1157-9	computer vision;artificial intelligence;decimal;pattern recognition;mathematics;graph	Vision	37.91326961681503	-51.96741144522985	130095
07e83a255c9879154f4c2778f0593f806c147464	action reconginiton using human pose	poses;pattern clustering;probability;clocks;smoothing methods;hcrf;abstracts clocks;abstracts;feature extraction;kalman smooth;smoothing methods feature extraction image denoising image sequences pattern clustering pose estimation probability;image denoising;limb masks;recognition accuracy improvement human action recognition method human pose based features conditional probability relationship models hidden conditional random field hcrf limb mask extraction image feature clustering human region background interference reduction double counting problem pose estimation pose sequence kalman smoothing noise removal feature sequence extraction;image sequences;hcrf limb masks kalman smooth poses;pose estimation	In this paper, we present a novel method for recognizing human actions in videos. The method applies the human pose based features to describe actions and models the conditional probability relationship between feature sequences and actions using hidden conditional random field (HCRF). Given a video, limb masks are extracted by clustering image features in human region. Limb masks are helpful to reduce the interference from background and partially address the “double-counting” problem during the pose estimation. Then, the extracted pose sequence is smoothed using the kalman smooth to remove the noise and make the pose sequence consistent. Multiple kinds of feature sequences based on poses are extracted to describe the information of actions from different views. We train one HCRF for each feature sequence and combine the confidence from different HCRFs to improve the recognition accuracy. Experiments on the benchmark dataset show different features have their own advantages in action recognition and combine them can reach a good result.	benchmark (computing);cluster analysis;conditional random field;counting problem (complexity);experiment;interference (communication);smoothing	Cong Chen;Hua-Qing Min;Rong-Hua Luo	2012	2012 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2012.6359525	computer vision;pose;3d pose estimation;feature extraction;computer science;machine learning;pattern recognition;probability;statistics	Vision	38.970443987795775	-48.7785930732833	130397
0c777519733413b16ceefbd7498fda4d6c1fb520	end-to-end depth from motion with stabilized monocular videos		Abstract. We propose a depth map inference system from monocular videos based on a novel dataset for navigation that mimics aerial footage from gimbal stabilized monocular camera in rigid scenes. Unlike most navigation datasets, the lack of rotation implies an easier structure from motion problem which can be leveraged for different kinds of tasks such as depth inference and obstacle avoidance. We also propose an architecture for end-to-end depth inference with a fully convolutional network. Results show that although tied to camera inner parameters, the problem is locally solvable and leads to good quality depth prediction.	aerial photography;decision problem;depth map;end-to-end principle;inference engine;obstacle avoidance;structure from motion	Clement Pinard;Laure Chevalley;Antoine Manzanera;David Filliat	2017	CoRR	10.5194/isprs-annals-IV-2-W3-67-2017	gimbal;architecture;artificial intelligence;computer science;computer vision;structure from motion;deep learning;monocular;inference;obstacle avoidance;depth map	Vision	28.192101304448567	-49.34992800537345	130430
283181a2173b485726664edc6fe73f0465387629	random temporal skipping for multirate video analysis		Current state-of-the-art approaches to video understanding adopt temporal jittering to simulate analyzing the video at varying frame rates. However, this does not work well for multirate videos, in which actions or subactions occur at different speeds. The frame sampling rate should vary in accordance with the different motion speeds. In this work, we propose a simple yet effective strategy, termed random temporal skipping, to address this situation. This strategy effectively handles multirate videos by randomizing the sampling rate during training. It is an exhaustive approach, which can potentially cover all motion speed variations. Furthermore, due to the large temporal skipping, our network can see video clips that originally cover over 100 frames. Such a time range is enough to analyze most actions/events. We also introduce an occlusion-aware optical flow learning method that generates improved motion maps for human action recognition. Our framework is end-to-end trainable, runs in real-time, and achieves state-of-the-art performance on six widely adopted video benchmarks.	artificial neural network;end-to-end principle;feature learning;frame language;graphics processing unit;map;optical flow;real-time clock;real-time computing;recurrent neural network;sampling (signal processing);simulation;titan;video clip	Yi Zhu;Shawn D. Newsam	2018	CoRR			Vision	31.502070277963153	-50.31888412282506	130712
a2953e81e422fb608fb5cb78d34e50729d032722	extracting sparse error of robust pca for face recognition in the presence of varying illumination and occlusion	illumination;occlusion;face recognition;robust principal component analysis;sparse error	In this paper, we consider the problem of recognizing human faces from frontal views with varying illumination, as well as occlusion and disguise. Motivated by the latest research on the recovery of low-rank matrix using robust principal component analysis (RPCA), we present a novel approach of robust face recognition by exploiting the sparse error component obtained by RPCA. Compared with low-rank component, it is revealed that the associated sparse error component exhibits more discriminating information which is of benefit to face identification. We define two descriptors (i.e., sparsity and smoothness) to represent characteristic of the sparse error component, and give two recognition protocols (i.e., the weighted based method and the ratio based method) to classify face images. The efficacy of the proposed approach is verified on publicly available databases (i.e., Extended Yale B and AR) with promising results. Meanwhile, the proposed algorithm manifests robustness since it does not assume any explicit prior knowledge about the illumination conditions, as well as the nature of corrupted and occluded regions. Furthermore, the proposed method is not limited to face recognition, also can be extended to other image-based object recognition. HighlightsThis paper is motivated by robust principal component analysis (RPCA).We exploit the sparse error component to perform face recognition.We define two descriptors (i.e., sparsity and smoothness) to represent the sparse error image.We present the weighted based method and ratio based method to classify face images.Our method shows good performance on public face databases with illumination and occlusion.	facial recognition system;principal component analysis;sparse matrix	Xiao Luan;Bin Fang;Linghui Liu;Weibin Yang;Jiye Qian	2014	Pattern Recognition	10.1016/j.patcog.2013.06.031	facial recognition system;computer vision;computer science;machine learning;pattern recognition;sparse approximation;lighting	Vision	32.87504118185864	-46.729141442209524	131324
77f7d9ca39235ae2d9378347da3a8b68d25fb242	spectral-spatial classification of hyperspectral images via multiscale superpixels based sparse representation	matching pursuit algorithms;classification algorithms hyperspectral imaging training matching pursuit algorithms dictionaries;training;mssr algorithm spectral spatial hyperspectral image classification multiscale superpixel sparse representation superpixel segmentation joint sparse representation classification hsi classification sparse representation algorithm;dictionaries;classification algorithms;hyperspectral imaging;joint sparse representation hyperspectral image classification superpixel multiscale;image segmentation geophysical image processing geophysical techniques hyperspectral imaging image classification image representation	Recently, the superpixel segmentation is introduced into the hyperspectral image (HSI) classification to exploit the spatial information. However, the size of superpixel is hard to determine since small superpixels lack enough spatial information and large superpixels usually result in error segmentation. Therefore, a multiscale superpixels based sparse representation (MSSR) algorithm is proposed to utilize the spatial-spectral information of multiscale superpixels for the HSI classification. Specifically, multiscale superpixels of a HSI are generated firstly. Then, the joint sparse representation classification (JSRC) is used to obtain the class labels of superpixels of different scales. Finally, the majority voting is applied on the labels of different scales to create the final class label for each pixel. Experimental results show that the proposed MSSR algorithm outperforms several well-known classification algorithms.	algorithm;final (java);horizontal situation indicator;multiscale modeling;pixel;sparse approximation;sparse matrix	Shuzhen Zhang;Shutao Li	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7729625	statistical classification;computer vision;computer science;hyperspectral imaging;machine learning;pattern recognition;mathematics;remote sensing	Vision	30.592149411507805	-44.18758726853174	131482
eb8c4c57419a82b3509a81562371d4c6fcf4ad88	is there a preferred classifier for operational thematic mapping?	geophysical image processing;support vector machines geophysical image processing image classification image segmentation maximum likelihood estimation;image segmentation;support vector machines;image classification;maximum likelihood estimation;journal article;thematic mapping classification maximum likelihood classifier mlc neural network support vector machine svm;spectral domain segmentation operational thematic mapping inherent geometric characteristics classification methodology optimal performance maximum likelihood approach support vector machine flexibility	The importance of properly exploiting a classifier's inherent geometric characteristics when developing a classification methodology is emphasized as a prerequisite to achieving near optimal performance when carrying out thematic mapping. When used properly, it is argued that the long-standing maximum likelihood approach and the more recent support vector machine can perform comparably. Both contain the flexibility to segment the spectral domain in such a manner as to match inherent class separations in the data, as do most reasonable classifiers. The choice of which classifier to use in practice is determined largely by preference and related considerations, such as ease of training, multiclass capabilities, and classification cost.	support vector machine	John A. Richards;Nick G. Kingsbury	2014	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2013.2264831	margin classifier;support vector machine;contextual image classification;quadratic classifier;computer science;machine learning;linear classifier;pattern recognition;data mining;maximum likelihood;image segmentation;structured support vector machine	Visualization	31.185482328230723	-43.016578574837254	131665
5fb3c0cbaf3068b9da1014768509ffa18d017c8f	improving constrained bundle adjustment through semantic scene labeling		There is no doubt that SLAM and deep learning methods can benefit from each other. Most recent approaches to coupling those two subjects, however, either use SLAM to improve the learning process, or tend to ignore the geometric solutions that are currently used by SLAM systems. In this work, we focus on improving city-scale SLAM through the use of deep learning. More precisely, we propose to use CNN-based scene labeling to geometrically constrain bundle adjustment. Our experiments indicate a considerable increase in robustness and precision.	bundle adjustment	Achkan Salehi;Vincent Gay-Bellile;Steve Bourgeois;Frédéric Chausse	2016		10.1007/978-3-319-49409-8_13	computer vision;robustness (computer science);artificial intelligence;machine learning;deep learning;computer science;semantics;bundle adjustment	Vision	28.477564614070477	-50.0340529197932	131736
efdd1b8ae007031cb657b96ca006d4f969b77630	a bayesian approach to face hallucination using dlpp and krr	locality preserving projection;map;kernel;face hallucination dlpp krr map;high resolution;kernel ridge regression bayesian approach face hallucination dlpp krr face recognition low resolution faces surveillance systems learning based two step approach direct locality preserving projections maximum a posterior estimation;image resolution;manifolds;bayesian approach;direct locality preserving projections;surveillance system;bayes methods;face image resolution kernel training laplace equations manifolds estimation;maximum a posterior estimation;training;low resolution;manifold learning;maximum likelihood estimation;low resolution faces;laplace equations;maximum a posterior;face recognition;estimation;regression analysis bayes methods face recognition image resolution maximum likelihood estimation;map estimation;super resolution;regression analysis;face;kernel ridge regression;face hallucination;krr;high resolution imager;dlpp;learning based two step approach;surveillance systems;high frequency	Low resolution faces are the main barrier to efficient face recognition and identification in several problems primarily surveillance systems. To mitigate this problem we proposes a novel learning based two-step approach by the use of Direct Locality Preserving Projections (DLPP), Maximum a posterior estimation (MAP) and Kernel Ridge Regression (KRR) for super-resolution of face images or in other words Face Hallucination. First using DLPP for manifold learning and MAP estimation, a smooth Global high resolution image is obtained. In second step to introduce high frequency components KRR is used to model the Residue high resolution image, which is then added to Global image to get final high quality detail featured Hallucinated face image. As shown in experimental results the proposed system is robust and efficient in synthesizing low resolution faces similar to the original high resolution faces.	display resolution;face hallucination;facial recognition system;image resolution;locality of reference;nonlinear dimensionality reduction;super-resolution imaging	Muhammad Tanveer;Naveed Iqbal	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.528	facial recognition system;computer vision;image resolution;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	29.734084782240785	-46.41802613068201	131766
c3da60330497cb948ce1741f1624347ee7acfceb	estimation de pose humaine et reconnaissance d'action par un système multi-robots. (human pose estimation and action recognition by multi-robot systems)		Estimating human pose and recognizing human activities are important steps in many applications, such as human computer interfaces (HCI), health care, smart conferencing, robotics, security surveillance etc. Despite the ongoing effort in the domain, these tasks remained unsolved in unconstrained and non cooperative environments in particular. Pose estimation and activity recognition face many challenges under these conditions such as occlusion or self occlusion, variations in clothing, background clutter, deformable nature of human body and diversity of human behaviors during activities. Using depth imagery has been a popular solution to address appearance and background related challenges, but it has restricted application area due to its hardware limitations and fails to handle remaining problems. Specifically, we considered action recognition scenarios where the position of the recording device is not fixed, and consequently require a method which is not affected by the viewpoint. As a second problem, we tackled the human pose estimation task in particular settings where multiple visual sensors are available and allowed to collaborate. In this thesis, we addressed these two related problems separately. In the first part, we focused on indoor action recognition from videos and we consider complex activities. To this end, we explored several methodologies and eventually introduced a 3D spatio-temporal representation for a video sequence that is viewpoint independent. More specifically, we captured the movement of the person over time using depth sensor and we encoded it in 3D to represent the performed action with a single structure. A 3D feature descriptor was employed afterwards to build a codebook and classify the actions with the bag-of-words approach. As for the second part, we concentrated on articulated pose estimation, which is often an intermediate step for activity recognition. Our motivation was to incorporate information from multiple sources and views and fuse them early in the pipeline to overcome the problem of self-occlusion, and eventually obtain robust estimations. To achieve this, we proposed a multi-view flexible mixture of parts model inspired by the classical pictorial structures methodology. In addition to the single-view appearance of the human body and its kinematic priors, we demonstrated that geometrical constraints and appearanceconsistency parameters are effective for boosting the coherence between the viewpoints in a multi-view setting. Both methods that we proposed was evaluated on public benchmarks and showed that the use of view-independent representations and integrating information from multiple viewpoints improves the performance of action recognition and pose estimation tasks, respectively.		Emre Dogan	2017				Vision	33.74456290992697	-49.98843562899701	131828
935f94ee0e327035f02c349b2fdaa4d8730c47e8	hybrid support vector machines for robust object tracking	one class classification;regression;object tracking;svm;binary classification	Tracking-by-detection techniques always formulate tracking as a binary classification problem. However, in this formulation, there exists a potential issue that the boundary of the positive targets and the negative background samples is fuzzy, which may be an important factor causing drift. To address this problem, we propose a novel hybrid formulation for tracking based on binary classification, regression and one-class classification, which comprehensively represents the appearance from different perspectives. In particular, the proposed regression model is a novel formulation for tracking and plays an important role in solving the fuzzy boundary problem. Moreover, we present a new tracking approach with different support vector machines (SVMs) and a novel distribution-based collaboration strategy as a specific implementation. Experimental results demonstrate that our method is robust and can achieve the state-of-the-art performance. HighlightsA novel hybrid formulation for object tracking is proposed.A novel regression model is presented to exploit more compact spatial constraint.Three different SVMs are used to build a specific tracker under above formulation.A distribution-based strategy is introduced to combine the hybrid SVMs.Experimental results demonstrate the state-of-the-art tracking performance.	support vector machine	Shunli Zhang;Yao Sui;Xin Yu;Sicong Zhao;Xiang Lin	2015	Pattern Recognition	10.1016/j.patcog.2015.02.008	binary classification;support vector machine;computer vision;regression;computer science;machine learning;video tracking;pattern recognition;one-class classification	Vision	32.77806185054644	-48.65712901589248	132086
18216d4f5d835f8c2018b87dd7f7977e913f1e22	probabilistic dance performance alignment by fusion of multimodal features	probability;probability feature extraction hidden markov models humanities image sequences motion estimation;probabilistic framework hidden markov model hmm motion features depth maps audio features audio recordings multimodal feature fusion dance movements multimodal alignment;dance gestures multimodal alignment hidden markov model;motion estimation;hidden markov models;humanities;feature extraction;hidden markov models feature extraction probabilistic logic data models mathematical model three dimensional displays skeleton;image sequences	This paper presents a probabilistic framework for the multimodal alignment of dance movements. The approach is based on a Hidden Markov Model (HMM) and considers different feature functions, each corresponding to a particular modality, namely motion features, extracted from depth maps, and audio features, extracted from audio recordings of dancers' steps. We show that this approach allows performing accurate dancer alignment, while constituting a general framework for various multimodal alignment tasks.	hidden markov model;map;markov chain;modality (human–computer interaction);multimodal interaction	Angélique Dremeau;Slim Essid	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638337	computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;motion estimation;probability;hidden markov model;statistics	Vision	38.04622506704027	-49.457384182546576	132362
8b1c3e377c7eac42cecfd026c506df6a75b086ea	on image similarity, sparse representation and kolmogorov complexity		This paper presents a very general method for the computation of a suitable measure of similarity between two images. The proposed measure relies on the parsimony of some suitable representation of one image using the information of the other. Two quite different theories which capitalize on the parsimony of representation are - Sparse representation and Kolmogorov complexity. The first is used to efficiently model the structures of an object while the latter measures the length of the shortest program that can effectively reconstruct it. This work understands and exploits the link between Sparse representation and Kolmogorov complexity. We show that for an image having a sparse representation w.r.t. some bases, the sparsity of its representation can be used as a practical analog to its Kolmogorov complexity; and the sparsity can be used to develop a meaningful measure of (dis)similarity. The generality of the proposed distance measure is demonstrated through a variety of tasks such as perceptual similarity measurement, clustering, and texture classification. The usefulness of the measure is indicated by the accurate experimental results achieved on a diverse collection of datasets. Index Terms—Image similarity, Compression, Kolmogorov complexity, Overcomplete dictionary, Sparsity, F	kolmogorov complexity;sparse approximation	Tanaya Guha;Rabab Kreidieh Ward	2012	CoRR		kolmogorov structure function;combinatorics;discrete mathematics;machine learning;pattern recognition;mathematics;statistics	NLP	32.638959807397036	-38.199572883628335	132490
12e46e8bebeb36875d19bc6d61cde3531bb39ca5	the relationships among various nonnegative matrix factorization methods for clustering	simultaneous clustering;pattern clustering;matrix factorization;normalization nonnegative matrix factorization methods clustering interpretation;clustering algorithms matrix decomposition algorithm design and analysis data mining principal component analysis computer science clustering methods pattern recognition text mining dna;convergence rate;pattern clustering matrix decomposition;matrix decomposition;clustering;factorization method;nonnegative matrix factorization;normalization;interpretation;relation;empirical evaluation;normalization matrix factorization relation simultaneous clustering;nonnegative matrix factorization methods	The nonnegative matrix factorization (NMF) has been shown recently to be useful for clustering and various extensions and variations of NMF have been proposed recently. Despite significant research progress in this area, few attempts have been made to establish the connections between various factorization methods while highlighting their differences. In this paper we aim to provide a comprehensive study on matrix factorization for clustering. In particular, we present an overview and summary on various matrix factorization algorithms and theoretically analyze the relationships among them. Experiments are also conducted to empirically evaluate and compare various factorization methods. In addition, our study also answers several previously unaddressed yet important questions for matrix factorizations including the interpretation and normalization of cluster posterior and the benefits and evaluation of simultaneous clustering. We expect our study would provide good insights on matrix factorization research for clustering.	algorithm;cluster analysis;experiment;non-negative matrix factorization;programming paradigm;unsupervised learning	Tao Li;Chris H. Q. Ding	2006	Sixth International Conference on Data Mining (ICDM'06)	10.1109/ICDM.2006.160	combinatorics;computer science;machine learning;pattern recognition;mathematics;matrix decomposition;non-negative matrix factorization	ML	27.045759118720802	-38.28942262741094	132564
983c16a89da5d9f6c09d3cffbbc632822cb8ae16	kinship verification based on status-aware projection learning		Kinship verification for parent-child is considered to be an asymmetric metric process, in which parents and children are associated with different status where the parents are priorly known to be significantly older than the children. To address the asymmetric metric learning, a status-aware projection learning (SaPL) method is proposed for facial image-based kinship verification, especially for the parent-child kinship. SaPL learns two status-specific projections to capture the significant appearance commonality between parents and children, respectively. Each status-specific projection consists of two components: a common component shared by the two status projections and a status-specific component. SaPL generally outperforms the one Mahalanobis distance metric. Extensive experimental results and comparisons with state-of-the-art approaches demonstrate the effectiveness of the proposed SaPL for kinship verification.		Haijun Liu;Jian Cheng;Feng Wang	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296446	mahalanobis distance;artificial intelligence;pattern recognition;computer science;linear programming;kinship	Robotics	27.837024876972635	-47.98769619815662	132635
7227c1d8f53279041822266c7a6d0ac128e96d05	multi-view people tracking via hierarchical trajectory composition		This paper presents a hierarchical composition approach for multi-view object tracking. The key idea is to adaptively exploit multiple cues in both 2D and 3D, e.g., ground occupancy consistency, appearance similarity, motion coherence etc., which are mutually complementary while tracking the humans of interests over time. While feature online selection has been extensively studied in the past literature, it remains unclear how to effectively schedule these cues for the tracking purpose especially when encountering various challenges, e.g. occlusions, conjunctions, and appearance variations. To do so, we propose a hierarchical composition model and re-formulate multi-view multi-object tracking as a problem of compositional structure optimization. We setup a set of composition criteria, each of which corresponds to one particular cue. The hierarchical composition process is pursued by exploiting different criteria, which impose constraints between a graph node and its offsprings in the hierarchy. We learn the composition criteria using MLE on annotated data and efficiently construct the hierarchical graph by an iterative greedy pursuit algorithm. In the experiments, we demonstrate superior performance of our approach on three public datasets, one of which is newly created by us to test various challenges in multi-view multi-object tracking.	cache coherence;experiment;free viewpoint television;graph (discrete mathematics);greedy algorithm;iterative method;mathematical optimization;scheduling (computing)	Yuanlu Xu;Xiaobai Liu;Yang Liu;Song-Chun Zhu	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.461	computer vision;simulation;machine learning	Vision	33.72336919876251	-47.97406080421527	132727
7b8c4b4114a10943e53fdfc74ba076b3bda8d535	joint deep learning and gaussian representation for person re-identification		Person re-identification (re-id) has attracted extensive attention worldwide in the computer vision. Unlike other images retrieval tasks, the color and texture information of pedestrian clothing is an important clue of person re-id. However, these special features of re-id are not focused in the process of extracting features by deep learning. We find that some traditional methods of extracting features are better than deep learning in color and texture. Therefore, in this paper, we propose a model of jointing Deep learning and Gaussian representation (JDAG), which is the fusion of CNN network and Gaussian descriptor to enrich the person re-id features. We also adjust the parameters of our model to achieve an optimal performance and obtain more discriminative features. The experimental results generated on two representative and public datasets (a large-scale dataset Market1501 and a small dataset VIPeR) are evaluated using three different distance metric learning strategies. The comparisons show that our method is superior than other existing methods.	deep learning	Nan Song;Xianglei Zhu;Yahong Han	2017		10.1007/978-981-10-8530-7_1	discriminative model;metric (mathematics);deep learning;gaussian;viper;computer science;pattern recognition;artificial intelligence	AI	30.48771922573307	-51.64581620216082	132859
3772d163ec60f0bce93b241d63a70e87632384c9	globally consistent multi-label assignment on the ray space of 4d light fields	image segmentation;computational geometry;image segmentation labeling training optimization geometry cameras vectors;light field analysis;multi label problems;data structures;feature extraction;optimal data structure globally consistent multilabel assignment ray space 4d light fields multilabel segmentation feature extraction geometry information;inproceedings;computational geometry data structures feature extraction image segmentation;variational methods light field analysis multi label problems;image segmentation computational geometry data structures feature extraction;variational methods	We present the first variational framework for multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, features need to be extracted from the 2D projection of a three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.	algorithm;autostereogram;benchmark (computing);calculus of variations;data structure;euclidean distance;ground truth;light field;mathematical optimization;multi-label classification;pixel;statistical classification;variational principle	Sven Wanner;Christoph N. Straehle;Bastian Goldlücke	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.135	computer vision;feature extraction;computational geometry;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	29.4523356531963	-47.81274703951023	132877
9d18904e73c71f6b54dc13aa622ca0f7844d9503	penalized collaborative representation based classification for face recognition	classification;penalized collaborative representation;face recognition;sparse representation	The collaborative representation classification (CRC) exhibits superiority in both accuracy and computational efficiency. However, when representing the test sample by a linear combination of the training samples, the CRC does not account for the following: the probability of the test sample being from the same class as the training sample far from it is small. In this paper, we propose the algorithm, Penalized Collaborative Representation (PCR), which first uses the original collaborative representation to compute the distance between each training and test sample, and then treats these distances as penalized coefficients to design the penalized collaborative representation. The experimental results on multiple face databases show that our classifier, designed according PCR, has a very satisfactory classification performance.	algorithm;algorithmic efficiency;coefficient;cyclic redundancy check;database;elastic net regularization;facial recognition system;sample rate conversion	Wei Huang;Xiaohui Wang;Zhong Jin;Jianzhong Li	2015	Applied Intelligence	10.1007/s10489-015-0672-z	facial recognition system;biological classification;computer science;machine learning;pattern recognition;sparse approximation	AI	25.91432318366001	-42.70682890604381	133019
2a0fa09b22a870e513edafa75d7b19a40ebc72af	towards unified depth and semantic prediction from a single image	image decomposition depth prediction semantic prediction depth estimation semantic segmentation image understanding convolutional neural network cnn training pixel wise depth values semantic labels hierarchical conditional random field hcrf;semantics image segmentation joints training estimation image edge detection layout;neural nets computer vision image segmentation learning artificial intelligence	Depth estimation and semantic segmentation are two fundamental problems in image understanding. While the two tasks are strongly correlated and mutually beneficial, they are usually solved separately or sequentially. Motivated by the complementary properties of the two tasks, we propose a unified framework for joint depth and semantic prediction. Given an image, we first use a trained Convolutional Neural Network (CNN) to jointly predict a global layout composed of pixel-wise depth values and semantic labels. By allowing for interactions between the depth and semantic information, the joint network provides more accurate depth prediction than a state-of-the-art CNN trained solely for depth prediction [6]. To further obtain fine-level details, the image is decomposed into local segments for region-level depth and semantic prediction under the guidance of global layout. Utilizing the pixel-wise global prediction and region-wise local prediction, we formulate the inference problem in a two-layer Hierarchical Conditional Random Field (HCRF) to produce the final depth and semantic map. As demonstrated in the experiments, our approach effectively leverages the advantages of both tasks and provides the state-of-the-art results.	algorithm;autostereogram;computer vision;conditional random field;convolutional neural network;experiment;glossary of computer graphics;ibm notes;interaction;national lidar dataset;pixel;unified framework	Peng Wang;Xiaohui Shen;Zhe L. Lin;Scott Cohen;Brian L. Price;Alan L. Yuille	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298897	computer vision;computer science;machine learning;pattern recognition	Vision	26.44802605045241	-51.49060745109347	133148
0b066fc187aa5627e4e8a3e63a1ab35a3bbade80	content-based video retrieval based on similarity of camera motion	content-based video retrieval;camera motion	This paper proposes a method for content-based sports video retrieval using camera work information. Since particular camera work for a typical scene exists in sports videos, camera work transition becomes an effective cue for retrieving a sports scene based on its content. The proposed method extracts a series of camera parameters from both a user-specified scene of a retrieval key and a video stream of a retrieval target, and detects scenes having content similar to that of the key from the target by applying continuous DP matching. The method was evaluated using a video stream of a baseball game. Recall-Precision curves make its effectiveness clear.		Hiroyoshi Endoh;Ryoji Kataoka	2000			computer vision;camera auto-calibration;geography;video capture;multimedia;computer graphics (images)	Vision	39.057734368174145	-51.20069157691807	133209
c6382de52636705be5898017f2f8ed7c70d7ae96	unconstrained face detection: state of the art baseline and challenges	detectors;pose estimation unconstrained face imagery government rights open source software licensing iapra janus benchmark a ijb a unconstrained face detection dataset manually localized faces video frames image frames recognition accuracy saturation seminal unconstrained face recognition datasets image filtering commodity face pose partial occlusion illumination condition unconstrained face recognition dataset;government;public domain software face recognition image filtering pose estimation;accuracy;face recognition;detectors face face detection accuracy face recognition government benchmark testing;face;face detection;benchmark testing	A large scale study of the accuracy and efficiency of face detection algorithms on unconstrained face imagery is presented. Nine different face detection algorithms are studied, which are acquired through either government rights, open source, or commercial licensing. The primary data set utilized for analysis is the IAPRA Janus Benchmark A (IJB-A), a recently released unconstrained face detection and recognition dataset which, at the time of this study, contained 67,183 manually localized faces in 5,712 images and 20,408 video frames. The goal of the study is to determine the state of the art in face detection with respect to unconstrained imagery which is motivated by the saturation of recognition accuracies on seminal unconstrained face recognition datasets which are filtered to only contain faces detectable by a commodity face detection algorithm. The most notable finding from this study is that top performing detectors still fail to detect the vast majority of faces with extreme pose, partial occlusion, and/or poor illumination. In total, over 20% of faces fail to be detected by all nine detectors studied. The speed of the detectors was generally correlated with accuracy: faster detectors were less accurate than their slower counterparts. Finally, key considerations and guidance is provided for performing face detection evaluations. All software using these methods to conduct the evaluations and plot the accuracies are made available in the open source.	algorithm;authorization;baseline (configuration management);benchmark (computing);face detection;facial recognition system;failure;janus;machine learning;open-source software;partial index;pose (computer vision);sensor	Jordan Cheney;Benjamin Eliot Klein;Anil K. Jain;Brendan Klare	2015	2015 International Conference on Biometrics (ICB)	10.1109/ICB.2015.7139089	facial recognition system;face;computer vision;face detection;speech recognition;object-class detection;computer science;pattern recognition;three-dimensional face recognition;government	Vision	33.936336629420026	-52.04368940195752	133259
2b64a72d53f13417c6352d3e89fd27df91b5d697	learning human interaction by interactive phrases	ut-interaction dataset;interactive phrase;novel hierarchical model;human knowledge;latent svm framework;descriptive model;binary semantic motion relationship;bit-interaction dataset;human interaction recognition;human interaction	In this paper, we present a novel approach for human interaction recognition from videos. We introduce high-level descriptions called interactive phrases to express binary semantic motion relationships between interacting people. Interactive phrases naturally exploit human knowledge to describe interactions and allow us to construct a more descriptive model for recognizing human interactions. We propose a novel hierarchical model to encode interactive phrases based on the latent SVM framework where interactive phrases are treated as latent variables. The interdependencies between interactive phrases are explicitly captured in the model to deal with motion ambiguity and partial occlusion in interactions. We evaluate our method on a newly collected BIT-Interaction dataset and UT-Interaction dataset. Promising results demonstrate the effectiveness of the proposed method.	encode;experiment;hierarchical database model;high- and low-level;interaction;interactivity;interdependence;latent variable	Yu Kong;Yunde Jia;Yun Fu	2012		10.1007/978-3-642-33718-5_22	natural language processing;computer vision;computer science;machine learning;data mining	Vision	34.71145819788383	-47.762221781087106	133433
05531fda7a82b18c42c7ae9d053349fdb8281b33	image annotation by multiple-instance learning with discriminative feature mapping and selection	image processing;image annotation low level features negative concept correlations positive concept correlations mil method supervised learning methods single instance learning problem region level visual information feature selection discriminative feature mapping multiple instance learning;correlation methods;image annotation;multiple instance learning mil feature selection image annotation;期刊论文;learning artificial intelligence correlation methods image processing;multiple instance learning mil;feature selection;learning artificial intelligence	Multiple-instance learning (MIL) has been widely investigated in image annotation for its capability of exploring region-level visual information of images. Recent studies show that, by performing feature mapping, MIL can be cast to a single-instance learning problem and, thus, can be solved by traditional supervised learning methods. However, the approaches for feature mapping usually overlook the discriminative ability and the noises of the generated features. In this paper, we propose an MIL method with discriminative feature mapping and feature selection, aiming at solving this problem. Our method is able to explore both the positive and negative concept correlations. It can also select the effective features from a large and diverse set of low-level features for each concept under MIL settings. Experimental results and comparison with other methods demonstrate the effectiveness of our approach.	automatic image annotation;bag device component;dictionary [publication type];extraction;feature selection;genetic selection;high- and low-level;indexes;multiple-instance learning;prototype;single-instance storage;social media;standard test image;supervised learning	Richang Hong;Meng Wang;Yue Gao;Dacheng Tao;Xuelong Li;Xindong Wu	2014	IEEE Transactions on Cybernetics	10.1109/TCYB.2013.2265601	semi-supervised learning;computer vision;instance-based learning;image processing;computer science;artificial intelligence;machine learning;pattern recognition;feature selection;feature	AI	25.651045355946344	-45.690637012284405	133467
a14160f5e1d6f200242f92697b70895b0e9a184e	mctd: motion-coordinate-time descriptor for 3d skeleton-based action recognition		During the past few years, 3D-skeleton based action recognition has received increasing research attentions. Numerous approaches have been proposed. Most existing approaches extract the pose features in each frame along the video sequence for recognizing different actions. However, the motion information between adjacent poses is missing. In this paper, we propose a new descriptor by employing motion for action recognition. In our approach, the Lie algebra is employed to extract the motion between neighboring poses. The spatial coordinate and the timestamp information are also used to describe the space-temporal distribution of motion in the video sequence. For classification, we modify the SVM kernel to measure the distance between different action instances. Our experiments on three common datasets show that the proposed descriptor outperforms the state-of-the-art approaches.		Qi Liang;Feng Wang	2017		10.1007/978-3-319-77380-3_55	computer vision;computer science;timestamp;artificial intelligence;support vector machine;kernel (linear algebra);lie algebra;coordinate time;skeleton (computer programming);pattern recognition	Vision	36.00572225769197	-50.2724699547939	133564
080d3bccb37eb85334c17b2f3871131ca6862ccd	kernel feature spaces and nonlinear blind souce separation	feature space	In kernel based learning the data is mapped to a kernel feature space of a dimension that corresponds to the number of training data points. In practice, however, the data forms a smaller submanifold in feature space, a fact that has been used e.g. by reduced set techniques for SVMs. We propose a new mathematical construction that permits to adapt to the intrinsic dimension and to find an orthonormal basis of this submanifold. In doing so, computations get much simpler and more important our theoretical framework allows to derive elegant kernelized blind source separation (BSS) algorithms for arbitrary invertible nonlinear mixings. Experiments demonstrate the good performance and high computational efficiency of our kTDSEP algorithm for the problem of nonlinear BSS.	algorithm;blind signal separation;computation;data point;feature vector;intrinsic dimension;kernel (operating system);kernel method;nonlinear system;source separation	Stefan Harmeling;Andreas Ziehe;Motoaki Kawanabe;Klaus-Robert Müller	2001			mathematical optimization;feature vector;radial basis function kernel;computer science;machine learning;pattern recognition;mathematics;statistics	ML	24.974371965042938	-39.760223884866036	133589
adf9998214598469f7a097bc50de4c23784f2a5a	attribute augmentation with sparse coding	object recognition image coding image recognition image reconstruction learning artificial intelligence;learned dictionaries attribute augmentation sparse coding based approach object recognition facial expression recognition binary descriptions zero shot learning image description image recognition reconstruction error concatenated semantic augmented attributes;dictionaries semantics encoding equations face recognition optimization object recognition	This work proposes a novel sparse coding based approach for augmenting attributes in both object recognition and facial expression recognition applications. Attributes are a set of manually specified binary descriptions of visual objects. Though playing an important role in different applications like zero-shot learning, image description and recognition, the manually specified attributes suffer from the incomplete capturing of the original image data. In this work, we propose to augment the original manually specified semantic attributes with the augmented attributes which are also sparse, based on the minimization of the reconstruction error between the original image and the concatenated semantic and augmented attributes. We propose to iteratively learn the dictionaries as well as recover the augmented attributes in the optimization. For our applications of object recognition and facial expression recognition, the augmented attributes combined with the predicted semantic attributes can improve the overall recognition rate. Also, our learned dictionaries show certain meanings captured by the attributes.	concatenation;dictionary;experiment;machine learning;mathematical optimization;neural coding;outline of object recognition;sparse matrix;visual objects	Xiaoyang Wang;Qiang Ji	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.745	computer vision;computer science;machine learning;pattern recognition;3d single-object recognition	Vision	25.702917097167155	-46.770582210612794	133671
fec927f18dbc5d17095fb7e3f862433ec9eed99f	robust manifold matrix factorization for joint clustering and feature extraction		Low-rank matrix approximation has been widely used for data subspace clustering and feature representation in many computer vision and pattern recognition applications. However, in order to enhance the discriminability, most of the matrix approximation based feature extraction algorithms usually generate the cluster labels by certain clustering algorithm (e.g., the kmeans) and then perform the matrix approximation guided by such label information. In addition, the noises and outliers in the dataset with large reconstruction errors will easily dominate the objective function by the conventional 2norm based squared residue minimization. In this paper, we propose a novel clustering and feature extraction algorithm based on an unified low-rank matrix factorization framework, which suggests that the observed data matrix can be approximated by the production of projection matrix and low dimensional representation, among which the low-dimensional representation can be approximated by the cluster indicator and latent feature matrix simultaneously. Furthermore, we have proposed using the 2,1-norm and integrating the manifold regularization to further promote the proposed model. A novel Augmented Lagrangian Method (ALM) based procedure is designed to effectively and efficiently seek the optimal solution of the problem. The experimental results in both clustering and feature extraction perspectives demonstrate the superior performance of the proposed method. Introduction Low-rank matrix factorization, as a promising technique to find two or more lower dimensional matrices whose product provides a good approximation to the original one and by which to capture the underlying low-dimensional structures of data, plays an important role in many computer vision and pattern recognition applications, e.g., dimension reduction, clustering and classification (Zhang and Zhao 2013; la Torre 2012; Zhang et al. 2015a). As one of the standard approaches for low-rank matrix approximation with a given data matrix and a preindicated rank r of the approximation, the well-known principal component analy∗Corresponding author. Copyright c © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. sis (PCA) is nothing but a truncated singular value decomposition (TSVD) applied on re-centered data (Zhang and Zhao 2013). The low-rank representation (LRR) model (Liu, Lin, and Yu 2010; Liu et al. 2013), which shares the same assumption that the observed data matrix should be approximately of low-rank and seeks the lowest-rank representation of all data jointly, has attracted great deal of attention in recent years and in particular employed for data clustering and segmentation (Liu, Lin, and Yu 2010; Yin et al. 2015). As another famous matrix factorization technique, the nonnegative matrix factorization (NMF) (Lee and Seung 2001) aims to find two nonnegative matrices whose product provides a good approximation to the original one, has also received considerable attention due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts based in the human brain (Cai et al. 2011). In the literature, the matrix factorization methods can be directly considered as the feature extraction algorithms by letting the factor matrices as the projection matrix and lowdimensional representation, respectively (Guan et al. 2011; 2012; Zhang et al. 2015b). Additional regularizers are accordingly suggested to match the certain data structure or priori knowledge, e.g., the manifold regularization (Zhang and Zhao 2013; Cai et al. 2011), the loss of a classifier (Gupta and Xiao 2011), the model constraint (Chen et al. 2015), and sparsity (Zheng et al. 2012). As an alternate point of view, it have been demonstrated that the NMF is equivalent to the kmeans clustering by interpreting the factor matrices as the cluster indicator and latent feature matrix, respectively (Ding et al. 2005; Ding, Li, and Jordan 2010). Pioneered by this idea, an orthogonal nonnegative matrix tri-factorization algorithm is developed for clustering, which addresses the orthogonality constraint and leads to rigorous clustering interpretation (Ding et al. 2006), and it has been further generalized to a high-order co-clustering framework for simultaneous clustering of multi-type relational data with a fast version to deal with large scale data (Wang et al. 2011). Moreover, an embedded unsupervised feature selection algorithm is proposed by using a novel constraint on the latent feature matrix (Wang, Tang, and Liu 2015). Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)	approximation algorithm;artificial intelligence;augmented lagrangian method;biclustering;cluster analysis;clustering high-dimensional data;computer vision;data structure;dimensionality reduction;embedded system;entity–relationship model;feature extraction;feature selection;granular computing;k-means clustering;linear algebra;low-rank approximation;manifold regularization;matrix regularization;non-negative matrix factorization;optimization problem;pattern recognition;principal component analysis;selection algorithm;singular value decomposition;sparse matrix;the matrix;triangular function;whole earth 'lectronic link;word lists by frequency	Lefei Zhang;Qian Zhang;Bo Du;Dacheng Tao;Jane You	2017			machine learning;flame clustering;mathematical optimization;artificial intelligence;feature extraction;correlation clustering;computer science;k-means clustering;cluster analysis;essential matrix;matrix decomposition;pattern recognition;non-negative matrix factorization	AI	24.615999635904423	-42.532715060375374	133731
34af045cb53970c694beacd7ef256f919dd16050	evaluating multi-task learning for multi-view head-pose classification in interactive environments	face training cameras support vector machines accuracy skin;support vector machines cameras human computer interaction image classification image resolution interactive systems learning artificial intelligence pose estimation social sciences computing;vital cues multitask learning evaluation multiview head pose classification interactive environments social attention behavior round table meetings cocktail parties head orientation scale based facial appearance variations large field of view cameras visual analysis smart room applications svm based mtl framework kl hog feature combination facial descriptors	Social attention behavior offers vital cues towards inferring one's personality traits from interactive settings such as round-table meetings and cocktail parties. Head orientation is typically employed as a proxy for determining the social attention direction when faces are captured at low-resolution. Recently, multi-task learning has been proposed to robustly compute head pose under perspective and scale-based facial appearance variations when multiple, distant and large field-of-view cameras are employed for visual analysis in smart-room applications. In this paper, we evaluate the effectiveness of an SVM-based MTL (SVM+MTL) framework with various facial descriptors (KL, HOG, LBP, etc.). The KL+HOG feature combination is found to produce the best classification performance, with SVM+MTL outperforming classical SVM irrespective of the feature used.	computer multitasking;electromagnetically induced transparency;homology-derived secondary structure of proteins;local binary patterns;multi-task learning;proxy server;tik	Yuqing Chen;Subramanian Ramanathan;Elisa Ricci;Oswald Lanz;Nicu Sebe	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.717	computer vision;computer science;machine learning;pattern recognition	Vision	34.489492230023245	-49.55402148007446	133804
ce5dd0afc946bb15a3494f5f43721947220217da	interpretable clustering ensembles using binary matrix factorization		The combination of multiple clustering solutions used to obtain accurate and novel output has attracted attention in data clustering research. Despite the success of clustering ensembles, there are still several fundamental limiting issues including the lack of a unified formalized problem formulation and an intuitive interpretation of the resulting solution. We formulate the clustering ensemble problem as a binary matrix factorization imposing assumptions of a binary structure on the resulting matrices. In such a framework, every data object is assigned to its representative ensemble centroid allowing for interpretation and validation of the consensus clustering results. We demonstrate that the formulated problem can be efficiently solved by means of iterative rank-one binary matrix approximation and apply the Proximus algorithm proposing an effective initialization scheme. The evaluation of the proposed clustering ensemble method demonstrates its efficacy on synthetic and real problems.	algorithm;approximation;cluster analysis;consensus clustering;ensemble kalman filter;ensemble forecasting;iterative method;singular value decomposition;synthetic intelligence	Sergey Sukhanov;Christian Debes;Abdelhak M. Zoubir	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462023	mathematical optimization;logical matrix;approximation algorithm;initialization;consensus clustering;matrix decomposition;matrix (mathematics);artificial intelligence;factorization;cluster analysis;pattern recognition;computer science	Vision	26.44218466906634	-38.13419725735888	133815
65086cbda9714c538417f7b25f9cf661e6d72833	tracking using motion patterns for very crowded scenes	very crowded scenes;motion pattern;tracking	This paper proposes Motion Structure Tracker (MST) to solve the problem of tracking in very crowded structured scenes. It combines visual tracking, motion pattern learning and multi-target tracking. Tracking in crowded scenes is very challenging due to hundreds of similar objects, cluttered background, small object size, and occlusions. However, structured crowded scenes exhibit clear motion pattern(s), which provides rich prior information. In MST, tracking and detection are performed jointly, and motion pattern information is integrated in both steps to enforce scene structure constraint. MST is initially used to track a single target, and further extended to solve a simplified version of the multi-target tracking problem. Experiments are performed on real-world challenging sequences, and MST gives promising results. Our method significantly outperforms several state-of-the-art methods both in terms of track ratio and accuracy.	experiment;video tracking	Xuemei Zhao;Dian Gong;Gérard G. Medioni	2012		10.1007/978-3-642-33709-3_23	computer vision;simulation;tracking	Vision	33.67744551012328	-48.13191359637117	134007
7a81967598c2c0b3b3771c1af943efb1defd4482	do we need more training data?	part models;mixture models;object detection	Datasets for training object recognition systems are steadily increasing in size. This paper investigates the question of whether existing detectors will continue to improve as data grows, or saturate in performance due to limited model complexity and the Bayes risk associated with the feature spaces in which they operate. We focus on the popular paradigm of discriminatively trained templates defined on oriented gradient features. We investigate the performance of mixtures of templates as the number of mixture components and the amount of training data grows. Surprisingly, even with proper treatment of regularization and “outliers”, the performance of classic mixture models appears to saturate quickly ( $${\sim }10$$ ∼ 10 templates and $${\sim }100$$ ∼ 100 positive training examples per template). This is not a limitation of the feature space as compositional mixtures that share template parameters via parts and that can synthesize new templates not encountered during training yield significantly better performance. Based on our analysis, we conjecture that the greatest gains in detection performance will continue to derive from improved representations and learning algorithms that can make efficient use of large datasets.	algorithm;big data;computer performance;cross-validation (statistics);discriminant;discriminative model;emoticon;experiment;feature vector;gradient;machine learning;mathematical optimization;matrix regularization;mixture model;object detection;outline of object recognition;programming paradigm;sensor;signal-to-noise ratio;test set	Xiangxin Zhu;Carl Vondrick;Charless C. Fowlkes;Deva Ramanan	2015	International Journal of Computer Vision	10.1007/s11263-015-0812-2	computer science;artificial intelligence;machine learning;mixture model;data mining;mathematics;statistics	Vision	27.591876525889095	-44.80213725732251	134320
59bb264eecf4e3e20783a6452d858ccf5c72e917	kmod - a tw o-parameter svm kernel for pattern recognition	pattern classification handwritten character recognition medical image processing learning automata;handwritten digit recognition support vector machine pattern recognition svm kernel pattern classification breast cancer kernel with moderate decreasing;learning automata;medical image processing;pattern classification;support vector machines kernel pattern recognition entropy image databases h infinity control spectral analysis frequency domain analysis breast cancer;pattern recognition;support vector machine;breast cancer;handwritten character recognition	It has been shown that Support Vector Machine theory optimizes a smoothness functional hypothesis through kernel application. We present KMOD, a two-parameter SVM kernel with distinctive properties of good discrimination between patterns while preserving the data neighborhood information. In classification problems, the experiments we carried out on the Breast Cancer benchmark produced better performance than RBF kernel and some state of the art classifiers. As well, it also generated favorable results when subjected to a 10-class problem of recognizing handwritten digits in the NIST database.	benchmark (computing);centrality;experiment;feature extraction;generalization error;kernel (operating system);mathematical optimization;neural coding;pattern recognition;radial basis function kernel;support vector machine;while	Nedjem-Eddine Ayat;Mohamed Cheriet;Ching Y. Suen	2002		10.1109/ICPR.2002.1047860	support vector machine;least squares support vector machine;kernel method;speech recognition;radial basis function kernel;feature;computer science;breast cancer;machine learning;pattern recognition;tree kernel;polynomial kernel	ML	28.166589370689444	-42.09346811009863	134354
31bf136696c972cb3c928b74ef089ae2f3219a9c	facial expression recognition for hci applications	facial expression recognition	Facial expression plays an important role in cognition of human emotions (Fasel, 2003 & Yeasin, 2006). The recognition of facial expressions in image sequences with significant head movement is a challenging problem. It is required by many applications such as human-computer interaction and computer graphics animation (Cañamero, 2005 & Picard, 2001). To classify expressions in still images many techniques have been proposed such as Neural Nets (Tian, 2001), Gabor wavelets (Bartlett, 2004), and active appearance models (Sung, 2006). Recently, more attention has been given to modeling facial deformation in dynamic scenarios. Still image classifiers use feature vectors related to a single frame to perform classification. Temporal classifiers try to capture the temporal pattern in the sequence of feature vectors related to each frame such as the Hidden Markov Model based methods (Cohen, 2003, Black, 1997 & Rabiner, 1989) and Dynamic Bayesian Networks (Zhang, 2005). The main contributions of the paper are as follows. First, we propose an efficient recognition scheme based on the detection of keyframes in videos where the recognition is performed using a temporal classifier. Second, we use the proposed method for extending the human-machine interaction functionality of a robot whose response is generated according to the user’s recognized facial expression. Our proposed approach has several advantages. First, unlike most expression recognition systems that require a frontal view of the face, our system is viewand texture-independent. Second, its learning phase is simple compared to other techniques (e.g., the Hidden Markov Models and Active Appearance Models), that is, we only need to fit second-order Auto-Regressive models to sequences of facial actions. As a result, even when the imaging conditions change the learned Auto-Regressive models need not to be recomputed. The rest of the paper is organized as follows. Section 2 summarizes our developed appearance-based 3D face tracker that we use to track the 3D head pose as well as the facial actions. Section 3 describes the proposed facial expression recognition based on the detection of keyframes. Section 4 provides some experimental results. Section 5 describes the proposed human-machine interaction application that is based on the developed facial expression recognition scheme.	active appearance model;artificial neural network;bartlett's bisection theorem;cognition;computer graphics;dynamic bayesian network;feature vector;gabor atom;hidden markov model;human–computer interaction;key frame;lawrence rabiner;markov chain;musicbrainz picard;wavelet	Fadi Dornaika;Bogdan Raducanu	2009			psychology;computer vision;speech recognition;communication	Vision	36.960161932389695	-48.47649513026954	134513
30a3768ed749fcac3bee22481dece41fc4276c0a	learning object and state models for ar task guidance	databases;printers;training;maintenance engineering;feature extraction;entropy;augmented reality	We present a method for automatically learning object and state models, which can be used for recognition in an augmented reality task guidance system. We assume that the task involves objects whose appearance is fairly consistent, but the background may vary. The novelty of our approach is that the system can be automatically constructed from examples of experts performing the task. As a result, the system can be easily adapted to new tasks. The approach makes use of the fact that the key features of the object are consistently present in multiple viewing instances; whereas features from the background or irrelevant objects are not consistently present. Using information theory, we automatically identify the features that can best discriminate between object states. In evaluations, our prototype successfully recognized object states in all trials.	augmented reality;guidance system;information theory;prototype;relevance	William A. Hoff;Hao Zhang	2016	2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)	10.1109/ISMAR-Adjunct.2016.0093	maintenance engineering;computer vision;augmented reality;entropy;simulation;feature extraction;computer science;artificial intelligence;machine learning	Robotics	35.376330289584516	-44.6587708002662	134553
235619e33ec4a004a58736dd5cfe4983fcc62563	learning video manifolds for content analysis of crowded scenes	abnormality detection;manifold embedding;visual surveillance;crowd analysis;computer science and informatics	In this paper, we propose a new approach for recognizing group events and abnormality detection in a crowded scene. A manifold learning algorithm with temporal-constraints is proposed to embed a video of a crowded scene in a low-dimensional space. Our low dimensional representation of a video preserves the spatial temporal property of a video as well as the characteristic of the video. Recognizing video events and abnormality detection in a crowded scene is achieved by studying the video trajectory in the manifold space. We evaluate our proposed method on the state-of-the-art public data-sets containing different crowd events. Qualitative and quantitative results show the promising performance of the proposed method.		Myo Thida;How-Lung Eng;Dorothy Ndedi Monekosso;Paolo Remagnino	2012	IPSJ Trans. Computer Vision and Applications	10.2197/ipsjtcva.4.71	computer vision;simulation;computer science;video tracking;multimedia	Vision	34.372039279217184	-50.266616745238906	134627
fd1e66155be432ce58ab569accacf4e73daa4925	a unified continuous optimization framework for center-based clustering methods	cluster algorithm;deterministic annealing;fixed point method;generic algorithm;k means;objective function;continuous optimization;clustering method;k means algorithm;statistical techniques;convex analysis;entropy method;information theoretic;information theory	Center-based partitioning clustering algorithms rely on minimizing an appropriately formulated objective function, and different formulations suggest different possible algorithms. In this paper, we start with the standard nonconvex and nonsmooth formulation of the partitioning clustering problem. We demonstrate that within this elementary formulation, convex analysis tools and optimization theory provide a unifying language and framework to design, analyze and extend hard and soft center-based clustering algorithms, through a generic algorithm which retains the computational simplicity of the popular k-means scheme. We show that several well known and more recent center-based clustering algorithms, which have been derived either heuristically, or/and have emerged from intuitive analogies in physics, statistical techniques and information theoretic perspectives can be recovered as special cases of the proposed analysis and we streamline their relationships.	adobe streamline;algorithm;cluster analysis;continuous optimization;convex analysis;generic programming;heuristic;information theory;k-means clustering;loss function;mathematical optimization;norm (social);optimization problem	Marc Teboulle	2007	Journal of Machine Learning Research		correlation clustering;constrained clustering;mathematical optimization;determining the number of clusters in a data set;combinatorics;data stream clustering;fuzzy clustering;information theory;computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;mathematics;continuous optimization;cluster analysis;statistics;k-means clustering	ML	27.554418570834642	-38.693753685835524	134917
2216340ef40b35d568d340ad163f3a2d6027c733	advances on action recognition in videos using an interest point detector based on multiband spatio-temporal energies	action databases action recognition interest point detector multiband spatio temporal energies visual framework energy detector multiband energy based filterbank video energy tracking 3d gabor filters dominant energy analysis nonlinear energy operators spatio temporal oscillations dynamic visual stream action texture decomposition action motion decomposition multiband filtering energy based saliency measure action videos local spatio temporal interest point extraction;visual databases channel bank filters feature extraction gabor filters image filtering image motion analysis image texture object detection object recognition object tracking video signal processing;energy tracking in videos;multiband gabor filtering;energy tracking in videos human action recognition spatio temporal interest point detectors multiband gabor filtering dominant energy analysis;human action recognition;videos three dimensional displays detectors accuracy databases visualization frequency modulation;dominant energy analysis;spatio temporal interest point detectors	This paper proposes a new visual framework for action recognition in videos, that consists of an energy detector coupled with a carefully designed multiband energy based filterbank. The tracking of video energy is performed using perceptually inspired 3D Gabor filters combined with ideas from Dominant Energy Analysis. Within this framework, we utilize different alternatives such as non-linear energy operators where actions are implicitly considered as manifestations of spatio-temporal oscillations in the dynamic visual stream. Texture and motion decomposition of actions through multiband filtering is the basis of our approach. This new energy-based saliency measure of action videos leads to the extraction of local spatio-temporal interest points that give promising results for the task of action recognition. Such interest points are processed further in order to formulate a robust representation of an action in a video. Theoretical formulation is supported by evaluation in two popular action databases, in which our method seems to outperform the state of the art.	database;filter bank;interest point detection;nonlinear system	Kevis Maninis;Petros Koutras;Petros Maragos	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025298	computer vision;mathematics;multimedia	Vision	36.661518296830856	-50.739547921960465	134950
ed39c2b2706d61170f4a55d55930bddde957cf3b	a lightweight discriminative tracker based on classification and similarity		Convolutional neural network (CNN) based trackers have achieved significant performances in tracking recently. Most existing CNN-based trackers regard tracking as a classification or similarity searching problem. The two methods have their respective superiorities and limitations because of different supervised objectives. In this paper, we propose a multi-task CNN for visual tracking, not only fully leveraging the training data, but also benefiting from a regularization effect that results in more general and discriminative representations that extend to tasks in new domains. Our multi-task CNN approach combines tasks of classification and similarity searching. Specifically, given a pair of examplar and search images, the network predicts the categories of the two images and search for the most similar regions to the examplar image in the search image. And then we use only the similarity module to conduct tracking, which makes our tracker operate at frame-rates beyond real-time. Extensive evaluation on the challenging benchmark sequences demonstrates that the proposed tracker performs favourably against the state-of-the-arts.	artificial neural network;benchmark (computing);computer multitasking;convolutional neural network;performance;real-time clock;real-time transcription;supervised learning;video tracking	Weinong Wang;Fei Wang;Yu Guo	2017	2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2017.8227391	visualization;pattern recognition;convolutional neural network;discriminative model;artificial intelligence;computer science;eye tracking;bittorrent tracker;training set	Vision	30.38804960935756	-50.79896319432122	135081
7a8da3cfe9bc1a3c09a203df1a976b68a7e7298e	a motion-based scene tree for browsing and retrieval of compressed videos	computacion informatica;video retrieval;motion estimation;video similarity;video indexing;camera motion;semantic information;ciencias basicas y experimentales;motion vector;video browsing;grupo a;video database;compressed video;shot boundary detection	This paper describes a fully automatic content-based approach for browsing and retrieval of MPEG-2 compressed video. The first step of the approach is the detection of shot boundaries based on motion vectors available from the compressed video stream. The next step involves the construction of a scene tree from the shots obtained earlier. The scene tree is shown to capture some semantic information as well as to provide a construct for hierarchical browsing of compressed videos. Finally, we build a new model for video similarity based on global as well as local motion associated with each node in the scene tree. To this end, we propose new approaches to camera motion and object motion estimation. The experimental results demonstrate that the integration of the above techniques results in an efficient framework for browsing and searching large video databases.		Haoran Yi;Deepu Rajan;Liang-Tien Chia	2006	Inf. Syst.	10.1016/j.is.2005.12.005	video compression picture types;computer vision;uncompressed video;quarter-pixel motion;computer science;motion interpolation;video tracking;motion estimation;block-matching algorithm;multimedia;motion compensation;multiview video coding;computer graphics (images)	DB	38.8243824188875	-51.855513648810465	135115
421a55d9907589b32040b1d781acb1666ab1ede4	hybrid bayesian eigenobjects: combining linear subspace and deep network methods for 3d robot vision		We introduce Hybrid Bayesian Eigenobjects (HBEOs), a novel representation for 3D objects designed to allow a robot to jointly estimate the pose, class, and full 3D geometry of a novel object observed from a single viewpoint in a single practical framework. By combining both linear subspace methods and deep convolutional prediction, HBEOs efficiently learn nonlinear object representations without directly regressing into high-dimensional space. HBEOs also remove the onerous and generally impractical necessity of input data voxelization prior to inference. We experimentally evaluate the suitability of HBEOs to the challenging task of joint pose, class, and shape inference on novel objects and show that, compared to preceding work, HBEOs offer dramatically improved performance in all three tasks along with several orders of magnitude faster runtime performance.		Benjamin Burchfiel;George Konidaris	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8593795	task analysis;artificial intelligence;robot;computer vision;computer science;linear subspace;nonlinear system;principal component analysis;pattern recognition;inference;bayesian probability;pose	Robotics	29.736684894693894	-48.5379660100329	135117
e556ea0a41e034c5a5829edf327d8911795b1653	hyperspectral image classification using local collaborative representation		In this paper, a new local collaborative representation-based method is proposed for the hyperspectral image classification. First, some significant atoms are selected to represent the neighbors of the pixels based on the collaborative representation algorithm via replacing L1 with L2 to reduce the representation cost. Then, the query pixel is considered as a linear combination of these selected active atoms belong to different classes, and the ultimate classification is carried out based on the contribution of each class to the query pixel and its local neighbors. Experimental results on the real hyperspectral image confirm the effectiveness, accuracy of the method proposed.		Yishu Peng;Yunhui Yan;Wenjie Zhu;Jiuliang Zhao	2014		10.1007/978-3-662-45646-0_22	contextual image classification;pattern recognition	Vision	30.348531846725344	-44.77487016088341	135266
525da67fb524d46f2afa89478cd482a68be8a42b	learning to generate 3d stylized character expressions from humans		We present ExprGen, a system to automatically generate 3D stylized character expressions from humans in a perceptually valid and geometrically consistent manner. Our multi-stage deep learning system utilizes the latent variables of human and character expression recognition convolutional neural networks to control a 3D animated character rig. This end-to-end system takes images of human faces and generates the character rig parameters that best match the human's facial expression. ExprGen generalizes to multiple characters, and allows expression transfer between characters in a semi-supervised manner. Qualitative and quantitative evaluation of our method based on Mechanical Turk tests show the high perceptual accuracy of our expression transfer results.	amazon mechanical turk;artificial neural network;computer animation;convolutional neural network;deep learning;end system;end-to-end principle;humans;interaction;latent variable;null character;semi-supervised learning;semiconductor industry;the turk	Deepali Aneja;Bindita Chaudhuri;Alex Colburn;Gary Faigin;Linda G. Shapiro;Barbara Mones	2018	2018 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2018.00024	stylized fact;convolutional neural network;artificial intelligence;facial recognition system;computer facial animation;deep learning;computer science;pattern recognition;facial expression;expression (mathematics);latent variable	Vision	25.138098283533925	-49.87948810452296	135604
51e4e0353c3758729f62b4acf4f64c98b25720e6	single image super-resolution via 2d sparse representation	image resolution image reconstruction image representation;2d sparse model;dictionaries image reconstruction spatial resolution signal resolution training feature extraction;training;会议论文;feature extraction;image reconstruction;dictionaries;signal resolution;super resolution;dictionary learning;memory usage single image super resolution 2d sparse representation intrinsic 2d structure two dimensional sparse model dictionary learning algorithm;sparse representation;dictionary learning super resolution sparse representation 2d sparse model;spatial resolution	Image super-resolution with sparsity prior provides promising performance. However, traditional sparse-based super resolution methods transform a two dimensional (2D) image into a one dimensional (1D) vector, which ignores the intrinsic 2D structure as well as spatial correlation inherent in images. In this paper, we propose the first image super-resolution method which reconstructs a high resolution image from its low resolution counterpart via a two dimensional sparse model. Correspondingly, we present a new dictionary learning algorithm to fully make use of the corresponding relationship of two pairs of 2D dictionaries of low and high resolution images, respectively. Experimental results demonstrate that our proposed image super-resolution with 2D sparse model outperforms state-of-the-art 1D sparse model based super resolution methods in terms of both reconstruction ability and memory usage.	algorithm;dictionary;image resolution;machine learning;sparse approximation;sparse matrix;super-resolution imaging	Na Qi;Yunhui Shi;Xiaoyan Sun;Wenpeng Ding;Baocai Yin	2015	2015 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2015.7177485	iterative reconstruction;computer vision;image resolution;feature extraction;k-svd;computer science;machine learning;pattern recognition;sparse approximation;sub-pixel resolution;superresolution	Vision	28.713714230073315	-44.52098761379679	135621
f6ae487133d11f44610da1d192cb4d673be614c4	storygan: a sequential conditional gan for story visualization		In this work we propose a new task called Story Visualization. Given a multi-sentence paragraph, the story is visualized by generating a sequence of images, one for each sentence. In contrast to video generation, story visualization focuses less on the continuity in generated images (frames), but more on the global consistency across dynamic scenes and characters – a challenge that has not been addressed by any single-image or video generation methods. Therefore, we propose a new story-to-image-sequence generation model, StoryGAN, based on the sequential conditional GAN framework. Our model is unique in that it consists of a deep Context Encoder that dynamically tracks the story flow, and two discriminators at the story and image levels, respectively, to enhance the image quality and the consistency of the generated sequences. To evaluate the model, we modified existing datasets to create the CLEVR-SV and PororoSV datasets. Empirically, StoryGAN outperformed stateof-the-art models in image quality, contextual consistency metrics, and human evaluation.		Yitong Li;Zhe Gan;Yelong Shen;Jingjing Liu;Yu Cheng;Yuexin Wu;Lawrence Carin;D. Carlson;Jianfeng Gao	2018	CoRR			Vision	26.20148589491434	-50.54259270118087	135677
ec83c63e28ae2a658bc76a6750e078c3a54b9760	deep descriptor transforming for image co-localization		Reusable model design becomes desirable with the rapid expansion of machine learning applications. In this paper, we focus on the reusability of pre-trained deep convolutional models. Specifically, different from treating pre-trained models as feature extractors, we reveal more treasures beneath convolutional layers, i.e., the convolutional activations could act as a detector for the common object in the image colocalization problem. We propose a simple but effective method, named Deep Descriptor Transforming (DDT), for evaluating the correlations of descriptors and then obtaining the category-consistent regions, which can accurately locate the common object in a set of images. Empirical studies validate the effectiveness of the proposed DDT method. On benchmark image co-localization datasets, DDT consistently outperforms existing state-of-the-art methods by a large margin. Moreover, DDT also demonstrates good generalization ability for unseen categories and robustness for dealing with noisy data.	benchmark (computing);computer vision;effective method;feature extraction;machine learning;signal-to-noise ratio;unsupervised learning	Xiu-Shen Wei;Chen-Lin Zhang;Yao Li;Chen-Wei Xie;Jianxin Wu;Chunhua Shen;Zhi-Hua Zhou	2017		10.24963/ijcai.2017/425	noisy data;artificial intelligence;empirical research;pattern recognition;robustness (computer science);effective method;detector;computer science;reusability	AI	24.727128431687884	-49.157880345534664	135730
c3a49a0a992f3ea09a7b783f75bf3d2a3df0c559	intrinsic generalization analysis of low dimensional representations	generalizacion;relation equivalence;linear representations;large dataset;representation image;metodo subespacio;methode sous espace;intrinsic generalisation;histogram;equivalence relation;generalisation;histogramme;spectral histogram;image representation;pattern recognition;subspace method;equivalence classes;reconnaissance forme;reconocimiento patron;histograma;generalization;spectral histogram linear representations	Low dimensional representations of images impose equivalence relations in the image space; the induced equivalence class of an image is named as its intrinsic generalization. The intrinsic generalization of a representation provides a novel way to measure its generalization and leads to more fundamental insights than the commonly used recognition performance, which is heavily influenced by the choice of training and test data. We demonstrate the limitations of linear subspace representations by sampling their intrinsic generalization, and propose a nonlinear representation that overcomes these limitations. The proposed representation projects images nonlinearly into the marginal densities of their filter responses, followed by linear projections of the marginals. We use experiments on large datasets to show that the representations that have better intrinsic generalization also lead to better recognition performance.	experiment;generalization (psychology);generalization error;marginal model;name;nonlinear system;projections and predictions;sampling (signal processing);sampling - surgical action;test data;turing completeness;density	Xiuwen Liu;Anuj Srivastava;DeLiang Wang	2003	Neural networks : the official journal of the International Neural Network Society	10.1016/S0893-6080(03)00089-3	generalization;combinatorics;discrete mathematics;machine learning;mathematics;statistics	ML	30.479606948312504	-38.63060834949434	135939
7f8dd22b84df6fc7b7188199c69257d29adc058f	a foreground inference network for video surveillance using multi-view receptive field		Foreground (FG) pixel labelling plays a vital role in video surveillance. Recent engineering solutions have attempted to exploit the efficacy of deep learning (DL) models initially targeted for image classification to deal with FG pixel labelling. One major drawback of such strategy is the lacking delineation of visual objects when training samples are limited. To grapple with this issue, we introduce a multi-view receptive field fully convolutional neural network (MV-FCN) that harness recent seminal ideas, such as, fully convolutional structure, inception modules, and residual networking. Therefrom, we implement a system in an encoder-decoder fashion that subsumes a core and two complementary feature flow paths. The model exploits inception modules at early and late stages with three different sizes of receptive fields to capture invariance at various scales. The features learned in the encoding phase are fused with appropriate feature maps in the decoding phase through residual connections for achieving enhanced spatial representation. These multi-view receptive fields and residual feature connections are expected to yield highly generalized features for an accurate pixel-wise FG region identification. It is, then, trained with database specific exemplary segmentations to predict desired FG objects. rnThe comparative experimental results on eleven benchmark datasets validate that the proposed model achieves very competitive performance with the prior- and state-of-the-art algorithms. We also report that how well a transfer learning approach can be useful to enhance the performance of our proposed MV-FCN.	bayesian network;closed-circuit television	Akilan Thangarajah	2018	CoRR		pattern recognition;convolutional neural network;residual;machine learning;decoding methods;deep learning;artificial intelligence;computer science;receptive field;contextual image classification;inference;visual objects	Vision	26.187695427238147	-51.912984127663364	136064
2cb7f3f9781e7d90b14919b3d83016930068cc8a	learning adaptive metric for robust visual tracking	motion estimation learning adaptive metric robust visual tracking video based object tracking adaptive metric learning visual object tracking adaptive metric differential tracking;training;motion estimation;target tracking training learning systems visualization transforms;metric learning;learning systems;learning system;visualization;adaptive;discriminative;object tracking;transforms;visual tracking adaptive discriminative metric learning supervised;target tracking;algorithms animals artificial intelligence humans image processing computer assisted pattern recognition automated video recording;visual tracking;supervised;object tracking motion estimation	Matching the visual appearances of the target over consecutive image frames is the most critical issue in video-based object tracking. Choosing an appropriate distance metric for matching determines its accuracy and robustness, and thus significantly influences the tracking performance. Most existing tracking methods employ fixed pre-specified distance metrics. However, this simple treatment is problematic and limited in practice, because a pre-specified metric does not likely to guarantee the closest match to be the true target of interest. This paper presents a new tracking approach that incorporates adaptive metric learning into the framework of visual object tracking. Collecting a set of supervised training samples on-the-fly in the observed video, this new approach automatically learns the optimal distance metric for more accurate matching. The design of the learned metric ensures that the closest match is very likely to be the true target of interest based on the supervised training. Such a learned metric is discriminative and adaptive. This paper substantializes this new approach in a solid case study of adaptive-metric differential tracking, and obtains a closed-form analytical solution to motion estimation and visual tracking. Moreover, this paper extends the basic linear distance metric learning method to a more powerful nonlinear kernel metric learning method. Extensive experiments validate the effectiveness of the proposed approach, and demonstrate the improved performance of the proposed new tracking method.	area striata structure;experiment;feature vector;frame (physical object);gene distance metric;k-nearest neighbors algorithm;kernel (operating system);linear discriminant analysis;matching;motion estimation;nonlinear system;projections and predictions;supervised learning;video tracking	Nan Jiang;Wenyu Liu;Ying Wu	2011	IEEE Transactions on Image Processing	10.1109/TIP.2011.2114895	computer vision;visualization;eye tracking;computer science;adaptive behavior;machine learning;video tracking;pattern recognition;motion estimation;discriminative model	Vision	34.7176160168462	-46.06794727593307	136145
aa4c5db4aead075ae00643e5d146af08e3f4d7d3	facial gender classification using shape-from-shading	shape from shading;gender classification;principal geodesic analysis;satisfiability;statistical model;qa75 electronic computers computer science	The aim in this paper is to show how to use the 2.5D facial surface normals (needle-maps) recovered using shape-from-shading (SFS) to perform gender classification. We use principal geodesic analysis (PGA) to model the distribution of facial surface normals which reside on a Remannian manifold. We incorporate PGA into shape-from-shading, and develop a principal geodesic shape-from-shading (PGSFS) method. This method guarantees that the recovered needle-maps exhibit realistic facial shape by satisfying a statistical model. Moreover, because the recovered facial needle-maps satisfy the data-closeness constraint as a hard constraint, they not only encode facial shape but also implicitly encode image intensity. Experiments explore the gender classification performance using the recovered facial needle-maps on two databases (Notre Dame and FERET), and compare the results with those obtained using intensity images. The results demonstrate the feasibility of gender classification using the recovered facial shape	2.5d;algorithm;centrality;clustered file system;constrained optimization;database;encode;feret (facial recognition technology);linear discriminant analysis;map;normal (geometry);photometric stereo;principal geodesic analysis;shading;statistical classification;statistical model	Jing Wu;William A. P. Smith;Edwin R. Hancock	2010	Image Vision Comput.	10.1016/j.imavis.2009.09.003	statistical model;computer vision;photometric stereo;machine learning;pattern recognition;principal geodesic analysis;mathematics;statistics;satisfiability	Vision	26.977393299799797	-43.28773389156249	136265
189ff58190c6ef1ff49b46f5c68a7b124b9b0059	online multi-target tracking using recurrent neural networks	data association;multi target tracking;recurrent neural networks;long short term memory	We present a novel approach to online multi-target tracking based on recurrent neural networks (RNNs). Tracking multiple objects in real-world scenes involves many challenges, including a) an a-priori unknown and time-varying number of targets, b) a continuous state estimation of all present targets, and c) a discrete combinatorial problem of data association. Most previous methods involve complex models that require tedious tuning of parameters. Here, we propose for the first time, an end-to-end learning approach for online multi-target tracking. Existing deep learning methods are not designed for the above challenges and cannot be trivially applied to the task. Our solution addresses all of the above points in a principled way. Experiments on both synthetic and real data show promising results obtained at ≈300 Hz on a standard CPU, and pave the way towards future research in this direction.	artificial neural network;central processing unit;correspondence problem;deep learning;emoticon;end-to-end principle;experiment;recurrent neural network;synthetic intelligence	Anton Milan;Seyed Hamid Rezatofighi;Anthony R. Dick;Konrad Schindler;Ian D. Reid	2017			computer science;artificial intelligence;recurrent neural network;machine learning;data mining;long short term memory	AI	34.14099975102878	-41.21053887229648	136273
d66092115d05d301dcea970fadeedc140f2ac2d3	toward optimal kernel-based tracking	image sampling;kernel;design and development;surveillance;feature space;design space;photometry;robots;robustness;centralized control;target tracking;sampling methods;target tracking kernel algorithm design and analysis image sampling sampling methods robustness robots surveillance centralized control photometry;algorithm design and analysis	The design and development of methods for tracking targets in visual images has developed rapidly in the past decade. However, in practice the design of tracking algorithms is still largely ad-hoc, based on trial and error. As a result, the performance of such algorithms can vary widely based on the properties of the target of interest and the choice of design. The use of spatial sampling kernels on multiple feature spaces has recently emerged as a promising approach to visual target tracking. In particular, it is possible to show that most popular tracking algorithms can be expressed within this framework. As a result, sampling kernels can be viewed as a flexible design space for tracking algorithms. However, in the current approaches, the kernels are placed in an adhoc fashion at the center of the target with a scale equal to the size of the target. This can lead to sub-optimal tracking results. In this paper, we present results pointing toward the design of optimal and approximately optimal target-specific tracking algorithms. The target tracking problem is formulated in terms of an optimization over a family of kernelbased sampling functions. This optimization is solved to produce an optimal target-specific kernel configuration. Experimental results show greatly improved performance over classical template tracking and naive kernel-based tracking.	algorithm;approximation;computable function;heuristic (computer science);hoare logic;hoc (programming language);kernel (operating system);kerrison predictor;mathematical optimization;optimal design;sampling (signal processing);time series;video tracking	Maneesh Dewan	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.309	robot;algorithm design;sampling;computer vision;mathematical optimization;kernel;simulation;feature vector;photometry;computer science;mathematics;robustness	Vision	32.84269972624444	-40.06908484216452	136692
3becaf50bc06f0779911ab65c9e36bdb18beab93	structured regression gradient boosting		We propose a new way to train a structured output prediction model. More specifically, we train nonlinear data terms in a Gaussian Conditional Random Field (GCRF) by a generalized version of gradient boosting. The approach is evaluated on three challenging regression benchmarks: vessel detection, single image depth estimation and image inpainting. These experiments suggest that the proposed boosting framework matches or exceeds the state-of-the-art.	autostereogram;benchmark (computing);boosting (machine learning);color depth;conditional random field;decision tree;discriminative model;experiment;gradient boosting;inpainting;microsoft outlook for mac;nonlinear system;stochastic gradient descent	Ferran Diego;Fred A. Hamprecht	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.162	mathematical optimization;machine learning;pattern recognition;mathematics;gradient boosting	Vision	25.861468457079717	-48.43911908772233	136827
28ded8808e44e92de2aa7dd6ad9470f18ec05a74	object tracking and multimedia augmented transition network for video indexing and modeling	database indexing;overlapped objects;video databases;segmentation method;image segmentation;multimedia browsing;video signal processing;videoconference;class parameter estimation;information retrieval;spatial relations;multimedia augmented transition network;video segmentation;multimedia systems;input modelling;computer networks;artificial intelligent;video indexing;multimedia database systems object tracking multimedia augmented transition network video indexing multimedia input strings video data modelling video data structuring artificial intelligence input modelling temporal relations spatial relations semantic objects unsupervised video segmentation method spcpe algorithm simultaneous partitioning class parameter estimation segmentation method overlapped objects multimedia browsing backtrack chain update split algorithm split segment identification video frame updating;indexing multimedia databases streaming media multimedia systems video on demand videoconference partitioning algorithms information retrieval computer networks artificial intelligence;multimedia database systems;backtrack chain update split algorithm;natural language understanding;spatial relation;temporal relations;split segment identification;indexing;streaming media;semantic objects;object tracking;video on demand;simultaneous partitioning;multimedia databases;backtracking;video data structuring;video frame updating;spcpe algorithm;artificial intelligence;multimedia input strings;parameter estimation;video data modelling;unsupervised video segmentation method;video signal processing database indexing video databases tracking multimedia databases parameter estimation image segmentation backtracking;spatial information;tracking;partitioning algorithms;multimedia database system	In our previous work, a multimedia augmented transition network (ATN) model together with its multimedia input strings were proposed to model and structure video data. The multimedia ATN model is based on the ATN model that has been used in the artificial intelligence (AI) areas for natural language understanding systems and its inputs are modeled by the multimedia input strings. The temporal and spatial relations of semantic objects are captured by an unsupervised video segmentation method called simultaneous partition and class parameter estimation (SPCPE) algorithm and are modeled by the multimedia input strings. However, the segmentation method used is not able to identify the objects that are overlapped together within video frames. The identification of the overlapped objects is a great challenge. For this purpose, a backtrack-chain-updation split algorithm that identifies the split segment (object) and uses the information in the current frame to update the previous frames in a backtrack-chain manner is developed in this paper. The proposed split algorithm provides more accurate temporal and spatial information of the semantic objects for video indexing.	algorithm;artificial intelligence;augmented transition network;backtrack;estimation theory;natural language understanding	Shu-Ching Chen;Mei-Ling Shyu;Chengcui Zhang;Rangasami L. Kashyap	2000		10.1109/TAI.2000.889878	spatial relation;computer vision;computer science;artificial intelligence;theoretical computer science;video tracking;multimedia	AI	37.15896593839894	-45.67273747232663	136831
71b7fc715e2f1bb24c0030af8d7e7b6e7cd128a6	the do’s and don’ts for cnn-based face verification		While the research community appears to have developed a consensus on the methods of acquiring annotated data, design and training of CNNs, many questions still remain to be answered. In this paper, we explore the following questions that are critical to face recognition research: (i) Can we train on still images and expect the systems to work on videos? (ii) Are deeper datasets better than wider datasets? (iii) Does adding label noise lead to improvement in performance of deep networks? (iv) Is alignment needed for face recognition? We address these questions by training CNNs using CASIA-WebFace, UMD-Faces, and a new video dataset and testing on YouTube-Faces, IJB-A and a disjoint portion of UMDFaces datasets. Our new data set, which will be made publicly available, has 22,075 videos and 3,735,476 human annotated frames extracted from them.		Ankan Bansal;Carlos D. Castillo;Rajeev Ranjan;Rama Chellappa	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.299	machine learning;facial recognition system;artificial intelligence;computer science;disjoint sets	Vision	30.187851854367434	-51.72824518438421	136875
84df6d24a572a241fcf50664a4c7608dce39e631	an intelligent learning approach to l-grammar extraction from image sequences of real plants	image processing;plant branching pattern;l grammar extraction;statistical model;machine learning;image sequence;unfoliaged plants	In this paper, we propose an automatic analyzing and transforming approach to L-system grammar extraction from real plants. Instead of using manually designed rules and cumbersome parameters, our method establishes the relationship between L-system grammars and the iterative trend of botanical entities, which reflect the endogenous factors that caused the plant branching process. To realize this goal, we use a digital camera to take multiple images of unfoliaged (leafless) plants and capture the topological and geometrical data of plant entities using image processing methods. The data then stored into specific data structures. A Hidden Markov based statistical model is then employed to reveal the hidden relations of plant entities which have been classified into categories based on their statistical properties extracted by a classic EM algorithm, the hidden relations have been integrated into the target L-system as grammars. Results show that our method is capable of automatically generating L-grammars for a given unfoliaged plant no matter what branching type it is belongs to.		Hongchun Qu;Qingsheng Zhu;Mingwei Guo;Zhonghua Lu	2009	International Journal on Artificial Intelligence Tools	10.1142/S0218213009000457	statistical model;image processing;computer science;artificial intelligence;machine learning;pattern recognition;data mining;statistics	AI	38.93819592701002	-48.619871158615545	136895
27431de7587dcee8399617cb1afef27b6d4aa1b9	nonlinear manifold clustering by dimensionality	graph theory;cluster algorithm;pattern clustering;feature space;geometrical invariance;clustering algorithms machine learning algorithms pattern recognition partitioning algorithms vectors computer science joining processes data mining machine learning computer vision;data clustering;nonlinear systems;em algorithm nonlinear manifold clustering by dimensionality neighborhood graph geometrical invariance;high dimensional data;pattern clustering expectation maximisation algorithm graph theory nonlinear systems;em algorithm;neighborhood graph;nonlinear manifold clustering by dimensionality;expectation maximisation algorithm	Because of variable dependence, high dimensional data typically have much lower intrinsic dimensionality than the number of its variables. Hence high dimensional data can be expected to lie in (nonlinear) lower dimensional manifold. In this paper, we describe a nonlinear manifold clustering algorithm. By connecting data vectors with their neighbors in feature space, we construct a neighborhood graph from given set data vectors. Furthermore, geometrical invariance, namely dimensionality, are extracted from the neighborhood of vectors, and used to facilitate the clustering procedure. In addition, we discuss a latent model for data cluster descriptions and an EM algorithm to find such descriptions. Preliminary experiments illustrate that this new algorithm can be used to explore the nonlinear structure of data	cluster analysis;dimensionality reduction;expectation–maximization algorithm;experiment;feature vector;nonlinear system	Wenbo Cao;Robert M. Haralick	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.865	correlation clustering;constrained clustering;combinatorics;data stream clustering;feature vector;k-medians clustering;expectation–maximization algorithm;fuzzy clustering;flame clustering;computer science;graph theory;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;mathematics;cluster analysis;dbscan;affinity propagation;clustering high-dimensional data;manifold alignment	ML	27.61166761875973	-39.645426660951905	137038
384888d83e72700b0104dc45cb2cce88478609cd	cofga: classification of fine-grained features in aerial images		Classification between thousands of classes in high-resolution images is one of the heavily studied problems in deep learning over the last decade. However, the challenge of fine-grained multi-class classification of objects in aerial images, especially in low resource cases, is still challenging and an active area of research in the literature. Solving this problem can give rise to various applications in the field of scene understanding and classification and re-identification of specific objects from aerial images. In this paper, we provide a description of our dataset COFGA of multi-class annotated objects in aerial images. We examine the results of existing stateof-the-art models and modified deep neural networks. Finally, we explain in detail the first published competition for solving this task.	aerial photography;algorithm;artificial neural network;deep learning;faceted classification;image resolution;information retrieval;multiclass classification;statistical classification	Eran Dahan;Tzvi Diskin	2018	CoRR		machine learning;deep learning;artificial neural network;artificial intelligence;computer science	ML	28.113029443126678	-50.737436444155975	137109
3ec06ba3680ac75e77f1fd88c0bb0b7518f72a6c	superpixel-based classification of hyperspectral data using sparse representation and conditional random fields	accuracy hyperspectral imaging dictionaries encoding kernel;patch based neighborhood superpixel based classification conditional random fields landcover mapping hyperspectral image data pixel sparse representation training data coarse patch based neighborhood data adapted superpixels hierarchical conditional random field sparse representation output hierarchical structures;land cover geophysical image processing geophysical techniques hyperspectral imaging image classification;random field sparse coding sparse representation superpixel hyperspectral	This paper presents a superpixel-based classifier for landcover mapping of hyperspectral image data. The approach relies on the sparse representation of each pixel by a weighted linear combination of the training data. Spatial information is incorporated by using a coarse patch-based neighborhood around each pixel as well as data-adapted superpixels. The classification is done via a hierarchical conditional random field, which utilizes the sparse-representation output and models spatial and hierarchical structures in the hyperspectral image. The experiments show that the proposed approach results in superior accuracies in comparison to sparse-representation based classifiers that solely use a patch-based neighborhood.	conditional random field;experiment;pixel;sparse approximation;sparse matrix	Ribana Roscher;Björn Waske	2014	2014 IEEE Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2014.6947280	computer vision;machine learning;pattern recognition;mathematics	Vision	30.969075400525767	-44.50847484995303	137193
2f3b291285bd1cbb1a06a645dc357c260452e230	video scene understanding using multi-scale analysis	diffusion maps;image motion analysis;multiscale analysis;computer vision;multiple dominant motion patterns;video scene understanding;roads;bag of words representation;pixel;object tracking;conditional entropy;entropy;vehicles;markov processes;multi scale analysis;scene understanding;multiple dominant motion patterns video scene understanding multiscale analysis object detection object tracking pixelwise optical flow bag of words representation conditional entropy diffusion maps geometric structure diffusion time parameter;layout object detection traffic control computer vision motion detection pattern analysis motion analysis cameras unmanned aerial vehicles roads;geometric structure;object detection entropy image motion analysis image sequences;object detection;pixelwise optical flow;diffusion time parameter;image sequences	We propose a novel method for automatically discovering key motion patterns happening in a scene by observing the scene for an extended period. Our method does not rely on object detection and tracking, and uses low level features, the direction of pixel wise optical flow. We first divide the video into clips and estimate a sequence of flow-fields. Each moving pixel is quantized based on its location and motion direction. This is essentially a bag of words representation of clips. Once a bag of words representation is obtained, we proceed to the screening stage, using a measure called the ‘conditional entropy’. After obtaining useful words we apply Diffusion maps. Diffusion maps framework embeds the manifold points into a lower dimensional space while preserving the intrinsic local geometric structure. Finally, these useful words in lower dimensional space are clustered to discover key motion patterns. Diffusion map embedding involves diffusion time parameter which gives us ability to detect key motion patterns at different scales using multi-scale analysis. In addition, clips which are represented in terms of frequency of motion patterns can also be clustered to determine multiple dominant motion patterns which occur simultaneously, providing us further understanding of the scene. We have tested our approach on two challenging datasets and obtained interesting and promising results.	bag-of-words model;clips;cluster analysis;color;conditional entropy;diffusion map;high- and low-level;object detection;optical flow;pattern recognition;pixel;quantization (signal processing);unsupervised learning	Yang Yang;Jingen Liu;Mubarak Shah	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459376	diffusion map;computer vision;entropy;simulation;computer science;machine learning;video tracking;motion estimation;markov process;conditional entropy;pixel;statistics	Vision	38.54684187757645	-47.57061310853076	137248
3c47022955c3274250630b042b53d3de2df8eeda	discriminant analysis with tensor representation	dater;eigenvalues and eigenfunctions;unsupervised learning;tensor representation;pattern clustering;discriminant tensor criterion;image coding;tensile stress;subspace learning;small sample size;iterative algorithms;eigenvalues and eigenfunctions tensors image coding pattern clustering;k mode cluster based discriminant analysis;curse of dimensionality;scattering;eigenvalue decomposition;higher order;discriminant analysis;supervised dimensionality reduction;principal component analysis;clustering algorithms;feature selection;tensile stress iterative algorithms linear discriminant analysis scattering asia image coding clustering algorithms algorithm design and analysis principal component analysis unsupervised learning;linear discriminant analysis;dimensional reduction;algorithm design and analysis;subspace learning tensor representation dater supervised dimensionality reduction discriminant tensor criterion feature selection k mode cluster based discriminant analysis eigenvalue decomposition;asia;tensors	In this paper, we present a novel approach to solving the supervised dimensionality reduction problem by encoding an image object as a general tensor of 2nd or higher order. First, we propose a discriminant tensor criterion (DTC), whereby multiple interrelated lower-dimensional discriminative subspaces are derived for feature selection. Then, a novel approach called k-mode cluster-based discriminant analysis is presented to iteratively learn these subspaces by unfolding the tensor along different tensor dimensions. We call this algorithm discriminant analysis with tensor representation (DATER), which has the following characteristics: 1) multiple interrelated subspaces can collaborate to discriminate different classes; 2) for classification problems involving higher-order tensors, the DATER algorithm can avoid the curse of dimensionality dilemma and overcome the small sample size problem; and 3) the computational cost in the learning stage is reduced to a large extent owing to the reduced data dimensions in generalized eigenvalue decomposition. We provide extensive experiments by encoding face images as 2nd or 3rd order tensors to demonstrate that the proposed DATER algorithm based on higher order tensors has the potential to outperform the traditional subspace learning algorithms, especially in the small sample size cases.	algorithm;computation;computational complexity theory;curse of dimensionality;dimensionality reduction;discriminative model;experiment;facial recognition system;feature selection;linear discriminant analysis;machine learning;principal component analysis;unfolding (dsp implementation)	Shuicheng Yan;Dong Xu;Qiang Yang;Lei Zhang;Xiaoou Tang;HongJiang Zhang	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.131	algorithm design;mathematical optimization;eigendecomposition of a matrix;higher-order logic;curse of dimensionality;tensor;computer science;machine learning;pattern recognition;mathematics;cluster analysis;stress;linear discriminant analysis;scattering;multilinear subspace learning;principal component analysis	Vision	26.20855240406883	-41.90901976799364	137276
05f1f6807630939dbbd425efcb2fe3736b6fb2ac	a block-based markov random field model estimation for contextual classification using optimum-path forest	landcover classification pattern classification optimum path forest;earth;training;markov random fields;markov processes forestry geophysical image processing image classification learning artificial intelligence;computational modeling;remote sensing;satellites;training satellites remote sensing earth markov random fields computational modeling context modeling;satellite images block based markov random field model estimation contextual image classification learning process locally adaptive optimum path forest classifier mrf parameter;context modeling	Contextual image classification aims at considering the information about nearby samples in the learning process in order to provide more accurate results. In this paper, we propose a locally-adaptive Optimum-Path Forest classifier together with Markov Random Fields (MRF) that surpasses its naïve version, which was recently presented in the literature. The experimental results over four satellite images demonstrated the proposed approach an outperform previous results, as well as it can perform MRF parameter learning much faster than its former version.	computation;computer vision;contextual image classification;markov chain;markov random field;naivety	Daniel Osaku;Alexandre L. M. Levada;João Paulo Papa	2016	2016 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2016.7527410	computer vision;machine learning;pattern recognition;earth;context model;markov model;computational model;satellite	Vision	31.724808669763398	-43.779021625759405	137342
66da47cd029132e6a52286a0c0226c3117ea6fc9	a comparison of texture feature algorithms for urban settlement classification	informal settlement;image classification;urban remote sensing;local binary pattern;texture features;surface morphology;texture features settlement classification;image texture;accuracy;indexes;remote sensing surface morphology discrete wavelet transforms africa remote monitoring feature extraction surface treatment entropy testing classification tree analysis;settlement classification;geophysical signal processing;feature extraction;remote sensing;pixel;classification system;local binary pattern texture features texture feature algorithms urban settlement classification haralick gray level cooccurrence matrix urban remote sensing informal housing low cost housing;classification accuracy;remote sensing feature extraction geophysical signal processing geophysical techniques image classification image texture;buildings;geophysical techniques	Texture features derived using Haralick's Gray-Level Cooccurrence Matrix (GLCM) are by far the most popular in urban remote sensing research - but are they the best features for every application? In order to select the most appropriate texture algorithm for an automated informal settlement classification system, we performed an experiment to compare the performance of the GLCM with that of other texture features. The performance of a texture feature is measured by computing the classification accuracy achieved on a supervised set of images spread over 8 settlement classes, focusing on informal and low-cost housing. The results show that GLCMs perform very well, but that Local Binary Pattern texture features have a small advantage in this classification problem.	algorithm;robert haralick	Leonce P. Abeigne Ella;Frans van den Bergh;Barend J. van Wyk;Michaël A. van Wyk	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4779599	image texture;database index;computer vision;contextual image classification;local binary patterns;feature extraction;computer science;machine learning;pattern recognition;accuracy and precision;pixel;remote sensing	Vision	32.01894806445181	-43.593173904478235	137439
6eefe30286ef36a1953caa368748d2427e495f2b	wake-sleep pca	eigenvalues and eigenfunctions;kernel principal component analysis;rotational ambiguity;neural nets;latent variable;data covariance matrix;coupled helmholtz machine;principal component analysis covariance matrix signal processing algorithms matrix decomposition iterative algorithms kernel machine learning symmetric matrices machine learning algorithms inference algorithms;covariance matrices;wake sleep algorithm;principal component analysis;helmholtz equations;kernel pca;principal eigenvectors;numerical experiment;learning artificial intelligence;principal component analysis covariance matrices eigenvalues and eigenfunctions helmholtz equations learning artificial intelligence neural nets;eigenvectors;covariance matrix;rotational ambiguity coupled helmholtz machine kernel principal component analysis wake sleep algorithm principal eigenvectors data covariance matrix	In this paper we introduce a coupled Helmholtz machine for principal component analysis (PCA), where sub-machines are related through sharing some latent variables and associated weights. We present a wake-sleep algorithm for PCA (referred to as WS-PCA), leading both generative and recognition weights to converge to principal eigenvectors of a data covariance matrix without rotational ambiguity, in contrast to probabilistic PCA and EM-PCA. Then we also present a kernerlized variation, i.e., a wake-sleep algorithm for kernel PCA (WS-KPCA). The coupled Helmholtz machine provides a unified view of principal component analysis, including various existing algorithms as its special cases. The validity of wake-sleep PCA and KPCA algorithms are confirmed by numerical experiments.	converge;experiment;helmholtz machine;kernel principal component analysis;latent variable;numerical analysis;wake-sleep algorithm	Seungjin Choi	2007	2007 International Joint Conference on Neural Networks	10.1109/IJCNN.2007.4371339	sparse pca;kernel principal component analysis;computer science;machine learning;pattern recognition;mathematics;artificial neural network;statistics;principal component analysis	ML	25.61283891909162	-38.50357984647502	137944
6f0f20e3ae83391afc9fff0dff41c91584094ca2	boosting kernel discriminant analysis and its application on tissue classification of gene expression data	kernel method;kernel discriminant analysis;high dimensional data;feature extraction	Kernel discriminant analysis (KDA) is one of the most effective nonlinear techniques for dimensionality reduction and feature extraction. It can be applied to a wide range of applications involving highdimensional data, including images, gene expressions, and text data. This paper develops a new algorithm to further improve the overall performance of KDA by effectively integrating the boosting and KDA techniques. The proposed method, called boosting kernel discriminant analysis (BKDA), possesses several appealing properties. First, like all kernel methods, it handles nonlinearity in a disciplined manner that is also computationally attractive; second, by introducing pairwise class discriminant information into the discriminant criterion and simultaneously employing boosting to robustly adjust the information, it further improves the classification accuracy; third, by calculating the significant discriminant information in the null space of the within-class scatter operator, it also effectively deals with the small sample size problem which is widely encountered in real-world applications for KDA; fourth, by taking advantage of the boosting and KDA techniques, it constitutes a strong ensemblebased KDA framework. Experimental results on gene expression data demonstrate the promising performance of the proposed methodology.	algorithm;dimensionality reduction;feature extraction;kernel (linear algebra);kernel (operating system);kernel method;linear discriminant analysis;nonlinear dimensionality reduction;nonlinear system;text corpus	Guang Dai;Dit-Yan Yeung	2007				AI	24.75118007816444	-40.808422783564076	138110
dfdb4641fd3c5e559e8a850f7059a2ca646a4bc1	mapreduce-based clustering for near-duplicate image identification	near-duplicate identification;image clustering;representative image;mapreduce;large-scale photos	In this paper, an effective algorithm is developed for tackling the problem of near-duplicate image identification from large-scale image sets, where the LLC (locality-constrained linear coding) method is seamlessly integrated with the maxIDF cut model to achieve more discriminative representations of images. By incorporating MapReduce framework for image clustering and pairwise merging, the near duplicates of images can be identified effectively from large-scale image sets. An intuitive strategy is also introduced to guide the process for parameter selection. Our experimental results on large-scale image sets have revealed that our algorithm can achieve significant improvement on both the accuracy rates and the computation efficiency as compared with other baseline methods.	algorithm;apache hadoop;baseline (configuration management);cluster analysis;computation;feature (computer vision);linear code;locality of reference;mapreduce;parallel computing	Wanqing Zhao;Hangzai Luo;Jinye Peng;Jianping Fan	2016	Multimedia Tools and Applications	10.1007/s11042-016-4060-4	machine learning;pattern recognition;data mining	Vision	28.027750901140454	-46.321353117335796	138247
73ab5dc2eccc2cfe2d10de1649a56c05fafb8e9d	visloiter: a system to visualize loiterers discovered from surveillance videos	surveillance video;visualization;loitering discovery	This paper presents a system for visualizing the results of loitering discovery in surveillance videos. Since loitering is a suspicious behaviour that often leads to abnormal situations, such as pickpocketing, its analysis attracts attention from researchers [Bird et al. 2005; Ke et al. 2013; A. et al. 2015]. Most of them mainly focus on how to detect or identify loitering individuals by human tracking techniques. A robust approach in [Nam 2015] is one of the state-of-theart methods for detecting loitering persons in crowded scenes using pedestrian tracking based on spatio-temporal changes. However, such tracking-based methods are quite time-consuming. Therefore, it is hard to apply loitering detection across multiple cameras for a long time, or take into account the visualization of loiterers at a glance. To solve this problem, we propose a system, named VisLoiter (Figure 1), which enables efficient loitering discovery based on face features extracted from longtime videos across multiple cameras, instead of the tracking-based manner. By taking the advantage of efficiency, the VisLoiter realizes the visualization of loiterers at a glance. The visualization consists of three display components for (1) the appearance patterns of loitering individuals, (2) the frequency ranking of faces of loiterers, and (3) the lightweight playback of video clips where the discovered loiterer frequently appeared (see Figure 1 (b) and (c)).	emoticon;human-based computation;nam;sensor;video clip;word lists by frequency	Jianquan Liu;Shoji Nishimura;Takuya Araki	2016		10.1145/2945078.2945125	computer vision;simulation;visualization;computer science;computer graphics (images)	Vision	39.08460333954018	-46.08689680734569	138563
65d5c496c046697a17c8bf06fa51d322fe1dfddc	on learning the statistical representation of a task and generalizing it to various contexts	temporal correlation;humanoid robot;manipulators;gaussian mixture;robot programming gaussian processes humanoid robots manipulators markov processes;constraint optimization;gaussian processes;generic latent space;hidden markov model;biological system modeling;temporal variation;indexing terms;programming by demonstration;humanoid robot simple manipulatory tasks;gaussian mixture model;hidden markov models;humanoid robots;learning by imitation robotics;spatio temporal correlations;mixture of gaussians;humans;markov processes;gaussian mixture regression;robot programming gaussian processes humanoid robots hidden markov models laboratories knowledge engineering humans encoding constraint optimization biological system modeling;encoding;robot programming;robot programming statistical representation humanoid robot simple manipulatory tasks generic latent space mixture of gaussians hidden markov model spatio temporal correlations gaussian mixture regression;statistical representation;knowledge engineering	This paper presents an architecture for solving generically the problem of extracting the constraints of a given task in a programming by demonstration framework and the problem of generalizing the acquired knowledge to various contexts. We validate the architecture in a series of experiments, where a human demonstrator teaches a humanoid robot simple manipulatory tasks. First, the combined joint angles and hand path motions are projected into a generic latent space, composed of a mixture of Gaussians (GMM) spreading across the spatial dimensions of the motion. Second, the temporal variation of the latent representation of the motion is encoded in a hidden Markov model (HMM). This two-step probabilistic encoding provides a measure of the spatio-temporal correlations across the different modalities collected by the robot, which determines a metric of imitation performance. A generalization of the demonstrated trajectories is then performed using Gaussian mixture regression (GMR). Finally, to generalize skills across contexts, we compute formally the trajectory that optimizes the metric, given the new context and the robot's specific body constraints	experiment;hidden markov model;humanoid robot;markov chain;mixture model;programming by demonstration	Sylvain Calinon;Florent Guenter;Aude Billard	2006	Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.	10.1109/ROBOT.2006.1642154	computer vision;simulation;computer science;humanoid robot;artificial intelligence;machine learning;mixture model;hidden markov model	Robotics	36.868223844373254	-41.52954332817573	138565
70dc044e00fc69caeb19b3d8e54616c517cf17e3	retrieving the movement of multiple objects based on spatiotemporal abstraction	spatiotemporal granularity spatiotemporal abstraction content based retrieval video data image feature;image features;video signal processing;image sequences image retrieval feature extraction video signal processing;individual object;multiple objectives;feature extraction;spatiotemporal phenomena information retrieval content based retrieval image retrieval shape data engineering games information analysis floods digital video broadcasting;content based retrieval;image sequences;image retrieval	Content based retrieval of video data based on the movement of objects is one of the method in addition to those based on image features such as color, shape, and/or texture. Most of the studies, which aimed at retrieving the movement of objects, concentrate on evaluating individual movement of objects. However, this strategy is not always effective especially in case where multiple objects are in motion, and the interest of retrieval lies in the movement of multiple objects as a cluster. This paper proposes a framework of modeling abstract movement of multiple objects based on the idea of group. The movement of individual object is extracted first, and the movement is aggregated based on a specified spatiotemporal granularity. A query is evaluated for the aggregated movement of multiple objects instead of individual movement of objects. We applied this framework for analyzing the aggregated flow of soccer games as an example.	computer cluster;spatiotemporal pattern;texture mapping	Atsuo Yoshitaka;Kentaro Ueda	2004	IEEE 6th Workshop on Multimedia Signal Processing, 2004.	10.1109/MMSP.2004.1436590	computer vision;visual word;feature extraction;image retrieval;computer science;multimedia;feature;information retrieval	Vision	39.16154247396006	-50.797813421301555	138656
345e104048b0372a0c44454dff1adc442c1baebd	a fast expected time algorithm for the 2-d point pattern matching problem	point pattern matching expected time algorithm;theoretical analysis;pattern matching;randomized algorithm;pattern recognition;point pattern matching	Point set pattern matching is an integral part of many pattern recognition problems. We study a randomized algorithm for the alignment approach to model-based recognition. Under certain mild assumptions we show that if our scene is a set of n points and our model is a set of m¡n points our algorithm has expected running time O(n(logm)) for 5nding an occurrence of the model in the scene. This is signi5cantly faster than any existing algorithms in the literature. We then describe some experimental results on randomly generated data using a practical version of our algorithm. These results agree well with the theoretical analysis. ? 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	average-case complexity;cluster analysis;data structure;floor and ceiling functions;pattern matching;pattern recognition;procedural generation;randomized algorithm;self-similarity;time complexity	Paul B. van Wamelen;Zhengzhou Li;S. Sitharama Iyengar	2004	Pattern Recognition	10.1016/j.patcog.2003.12.009	computer science;machine learning;pattern matching;pattern recognition;mathematics;randomized algorithm;algorithm	Vision	36.23823485771723	-38.39818565524624	138781
aa8367a4bde888f3fb8d3efe2354352104ad0d8d	dynamic facial expression analysis based on extended spatio-temporal histogram of oriented gradients	emotion recognition;spatio temporal histogram of oriented gradient;spatio temporal pyramid;genetic algorithm;facial expression	Facial expression is crucial for proper analysis of a person's face. It is an indicator of the emotion of a person and thus has attracted the attention of many researchers. In this work, a novel local spatio-temporal descriptor is proposed for motion pattern detection. The proposed feature comprises histogram of 3D gradients and the gradients' variation over time to robustly describe the spatial and temporal information. It also incorporates spatio-temporal pyramid structure to handle different resolution and frame rate. To reduce the dimension of the feature, we applied genetic algorithm for region-based feature selection. We evaluated the performance of our proposed descriptors on facial expression recognition using the Cohn-Kanade CK	gradient;histogram of oriented gradients	Seyedehsamaneh Shojaeilangari;Wei-Yun Yau;Jun Li;Eam Khwang Teoh	2014	IJBM	10.1504/IJBM.2014.059640	computer vision;genetic algorithm;computer science;machine learning;pattern recognition;facial expression	Vision	36.395256095980194	-50.57740106713049	139033
a52cc5b42768cdfb1929e21ab42c8c8d3318121a	transformational sparse coding		A fundamental problem faced by object recognition systems is that objects and their features can appear in different locations, scales and orientations. Current deep learning methods attempt to achieve invariance to local translations via pooling, discarding the locations of features in the process. Other approaches explicitly learn transformed versions of the same feature, leading to representations that quickly explode in size. Instead of discarding the rich and useful information about feature transformations to achieve invariance, we argue that models should learn object features conjointly with their transformations to achieve equivariance. We propose a new model of unsupervised learning based on sparse coding that can learn object features jointly with their affine transformations directly from images. Results based on learning from natural images indicate that our approach matches the reconstruction quality of traditional sparse coding but with significantly fewer degrees of freedom while simultaneously learning transformations from data. These results open the door to scaling up unsupervised learning to allow deep feature+transformation learning in a manner consistent with the ventral+dorsal stream architecture of the primate visual cortex.		Dimitrios C. Gklezakos;Rajesh P. N. Rao	2016	CoRR		neural coding;deep learning;unsupervised learning;architecture;scaling;affine transformation;machine learning;pooling;artificial intelligence;feature learning;pattern recognition;computer science	ML	24.95489082375627	-49.73532395869557	139104
15bdc73a3e51ad0e3b3c28bbbc39b947be61225e	framework for measurement of the intensity of motion activity of video segments	perforation;active measurement;low complexity;compressed domain feature extraction;video segmentation;motion activity;human subjects;video indexing;camera motion;motion vector;feature extraction;ground truth;experimental methodology;mpeg 7	We present a psychophysical and analytical framework for the comparison of the performance of different analytical measures of motion activity in video segments with respect to a subjective ground truth. We first construct a test-set of video segments and conduct a psychophysical experiment to obtain a ground truth for the motion activity. Then we present several low-complexity motion activity descriptors computed from compressed domain block motion vectors. In the first analysis, we quantize the descriptors and show that they perform well against the ground truth. We also show that the MPEG-7 motion activity descriptor is among the best. In the second analysis, we find the pairs of video segments for which the human subjects unanimously rate one as higher activity than the other. Then we examine the specific cases where each descriptor fail to give the correct ordering. We show that the distance from camera, and strong camera motion are main cases where motion vector based descriptors tend to overestimate or underestimate the intensity of motion activity. We finally discuss the experimental methodology and analysis methods we used and possible alternatives. We review the applications of motion activity and how the results presented here relate to those applications.	ground truth;mpeg-7;quantization (signal processing)	Kadir A. Peker;Ajay Divakaran	2004	J. Visual Communication and Image Representation	10.1016/j.jvcir.2004.04.007	computer vision;match moving;simulation;ground truth;feature extraction;quarter-pixel motion;computer science;motion estimation;block-matching algorithm;motion field;motion compensation;computer graphics (images)	Vision	38.74368320236119	-51.43735076550284	139151
b6b10af5eb8410eaebe9a013c0c6ff96ee68bda6	a learning-based frame pooling model for event detection		Detecting complex events in a large video collection crawled from video websites is a challenging task. When applying directly good image-based feature representation, e.g., HOG, SIFT, to videos, we have to face the problem of how to pool multiple frame feature representations into one feature representation. In this paper, we propose a novel learning-based frame pooling method. We formulate the pooling weight learning as an optimization problem and thus our method can automatically learn the best pooling weight configuration for each specific event category. Experimental results conducted on TRECVID MED 2011 reveal that our method outperforms the commonly used average pooling and max pooling strategies on both high-level and low-level 2D image features.	convolutional neural network;debian-med;high- and low-level;mathematical optimization;optimization problem;scale-invariant feature transform	Jiang Liu;Chenqiang Gao;Lan Wang;Deyu Meng	2016	CoRR		computer vision;computer science;machine learning;pattern recognition	Vision	31.80455792624804	-51.238654874103375	139189
82017a64cd63aa7e591ef4e499122d92a89109bc	rule-based approach for enhancing the motion trajectories in human activity recognition	human motion datasets;image motion analysis;visual surviellance system human activity recognition motion trajectory sift algorithm action recognition;rule based system;rule based;motion trajectories;visual surviellance system;sift algorithm;trajectory;human motion datasets motion trajectories human activity recognition rule based system object detection semi normal trajectories;feature extraction;human motion;action recognition;object detection image motion analysis knowledge based systems;trajectory noise videos buildings humans feature extraction tracking;humans;semi normal trajectories;human activity recognition;human activity;knowledge based systems;buildings;motion trajectory;tracking;object detection;noise;videos	In this paper, we propose a rule-based system for semantically understanding and analyzing the motion of the trajectories of the human activity. The proposed system can be used as a preprocessing phase for enhancing the object detection process. Detected trajectories are classified into three categories; normal, semi-normal and abnormal trajectories according to the distances between their adjacent points. Abnormal trajectories are removed from the trajectory space. Semi-normal trajectories are broken into small normal trajectories that are linked later to form a longer normal trajectory. The proposed system does not assume a specific trajectory length and hence is more generic than similar trajectory enhancement approaches. The effectiveness of the proposed approach is demonstrated through several experimental results using known human motion datasets.	activity recognition;kinesiology;object detection;preprocessor;rule-based system;semiconductor industry	Sherien Mohamed Hassan;Ahmed Farouk Al-Sadek;Elsayed E. Hemayed	2010	2010 10th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2010.5687161	rule-based system;computer vision;simulation;feature extraction;computer science;noise;artificial intelligence;trajectory;machine learning;scale-invariant feature transform;tracking	Robotics	38.63371067574132	-48.70600481337973	139192
11aa43176df91cfdac6c389e87f7de821349e3c2	exploring compositional high order pattern potentials for structured output learning	image features;image segmentations;high level structure;restricted boltzmann machines;image segmentation;standards;learning artificial intelligence boltzmann machines feature extraction image segmentation;compositional high order pattern potentials;linear deviation pattern potentials;image dependent high order potentials;convolutional pattern learning;tractable models;joints;materials;rbm;shape;compositional high order pattern potentials image dependent high order potentials convolutional pattern learning multiple patterns image features image dependent mapping internal pattern parameters loss sensitive joint learning procedure rbm restricted boltzmann machines linear deviation pattern potentials chopp pattern like high order potential high level structure tractable models image segmentations structured output learning;loss sensitive joint learning procedure;chopp;feature extraction;multiple patterns;predictive models;inference algorithms;standards joints shape labeling inference algorithms predictive models materials;image dependent mapping;learning artificial intelligence;structured output learning;boltzmann machines;labeling;pattern like high order potential;internal pattern parameters	When modeling structured outputs such as image segmentations, prediction can be improved by accurately modeling structure present in the labels. A key challenge is developing tractable models that are able to capture complex high level structure like shape. In this work, we study the learning of a general class of pattern-like high order potential, which we call Compositional High Order Pattern Potentials (CHOPPs). We show that CHOPPs include the linear deviation pattern potentials of Rother et al. [26] and also Restricted Boltzmann Machines (RBMs), we also establish the near equivalence of these two models. Experimentally, we show that performance is affected significantly by the degree of variability present in the datasets, and we define a quantitative variability measure to aid in studying this. We then improve CHOPPs performance in high variability datasets with two primary contributions: (a) developing a loss-sensitive joint learning procedure, so that internal pattern parameters can be learned in conjunction with other model potentials to minimize expected loss, and (b) learning an image-dependent mapping that encourages or inhibits patterns depending on image features. We also explore varying how multiple patterns are composed, and learning convolutional patterns. Quantitative results on challenging highly variable datasets show that the joint learning and image-dependent high order potentials can improve performance.	cobham's thesis;emoticon;experiment;heart rate variability;high-level programming language;level structure;pattern language;restricted boltzmann machine;structured prediction;turing completeness	Yujia Li;Daniel Tarlow;Richard S. Zemel	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.14	boltzmann machine;computer vision;labeling theory;feature extraction;shape;computer science;machine learning;pattern recognition;mathematics;predictive modelling;image segmentation;feature	Vision	25.12830792541353	-50.62884631844322	139440
29d94f275b1483f575c05b90464994ecfa86e27f	a passive learning sensor architecture for multimodal image labeling: an application for social robots	object recognition;robot sensors;word semantics;deep learning;ambient intelligence sensors;object detection	Object detection and classification have countless applications in human-robot interacting systems. It is a necessary skill for autonomous robots that perform tasks in household scenarios. Despite the great advances in deep learning and computer vision, social robots performing non-trivial tasks usually spend most of their time finding and modeling objects. Working in real scenarios means dealing with constant environment changes and relatively low-quality sensor data due to the distance at which objects are often found. Ambient intelligence systems equipped with different sensors can also benefit from the ability to find objects, enabling them to inform humans about their location. For these applications to succeed, systems need to detect the objects that may potentially contain other objects, working with relatively low-resolution sensor data. A passive learning architecture for sensors has been designed in order to take advantage of multimodal information, obtained using an RGB-D camera and trained semantic language models. The main contribution of the architecture lies in the improvement of the performance of the sensor under conditions of low resolution and high light variations using a combination of image labeling and word semantics. The tests performed on each of the stages of the architecture compare this solution with current research labeling techniques for the application of an autonomous social robot working in an apartment. The results obtained demonstrate that the proposed sensor architecture outperforms state-of-the-art approaches.	algorithm;ambient intelligence;autonomous robot;computer vision;containers;deep learning;experiment;generic drugs;hl7publishingsubsection <query>;high-level programming language;image resolution;language model;multimodal interaction;object detection;physical object;preparation;primary lateral sclerosis, adult, 1;probabilistic latent semantic analysis;robot (device);robotics;social robot;verification of theories;sensor (device)	Marco Antonio Gutierrez;Luis Manso;Harit Pandya;Pedro Núñez Trujillo	2017		10.3390/s17020353	embedded system;computer vision;simulation;robotic sensing;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;deep learning	AI	29.944218365386813	-48.859254564465076	139508
a40f5b4bd4c2f8cc882a767b1ee412c3f081eb2f	instance selection for efficient and reliable camera calibration	calibration cameras training training data robot vision systems kernel mathematical model;parameter variance instance selection camera calibration training data distribution large scale physical experiments nonuniform training data reprojection error;unsupervised learning calibration cameras parameter estimation	The popularity of cameras for perception is enabled in part by powerful intrinsic calibration routines, commonly requiring a user to manually collect images of a known calibration target. The manual nature of this process produces training data that is unevenly spread in the camera relative pose space. If we desire camera parameters that perform well on average over the entire relative pose space, training on such a dataset results in poor performance. To address this, we show that reasoning about the training data distribution to select a more uniformly-spread subset of images produces more accurate and stable calibrations with fewer images. Our approach can be used easily with most camera calibration algorithms. We demonstrate in large-scale physical experiments the effect of non-uniform training data and show that our approach outperforms baselines in reprojection error and parameter variance.	algorithm;baseline (configuration management);camera resectioning;characteristic function (convex analysis);circuit complexity;convolution;emoticon;experiment;reprojection error;virtual reality headset	Humphrey Hu;George Kantor	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487632	computer vision;camera auto-calibration;computer science;machine learning;pattern recognition	Robotics	35.04046220633034	-44.1426853215443	139597
acd4280453b995cb071c33f7c9db5760432f4279	deep transformation learning for face recognition in the unconstrained scene	convolution neural networks;unconstrained face recognition;feature transformation learning;factorized feature transformation;pose loss	Because human pose variations cannot be controlled in unconstrained scene, it is frequently hard to capture frontal face image. This is why either face recognition rate is low, or face image cannot be recognized at all. To tackle the problem, this paper proposes deep transformation learning to extract the pose-robust feature within one model; it includes feature transformation and joint supervision of softmax loss and pose loss. Specifically, the feature transformation is designed to learn the transformation from different poses. The pose loss is designed to simultaneously learn the feature center of different poses and keep intra-pose relationships. The extracted deep features tend to be more pose-robust and discriminative. Experimental results also confirm the performances to be valid on several important face recognition benchmarks, including Labeled Faces in the Wild, IARPA Janus Benchmark A.	benchmark (computing);face detection;facial recognition system;generative adversarial networks;janus;performance;pitch (music);softmax function	Guanhao Chen;Yanqing Shao;Chaowei Tang;Zhuoyi Jin;Jinkun Zhang	2018	Machine Vision and Applications	10.1007/s00138-018-0907-1	artificial intelligence;pattern recognition;discriminative model;computer science;janus;facial recognition system;softmax function	Vision	29.77130164531156	-50.55420791093563	139637
07a29f43713833da42b24e3915b63601c39d7627	action recognition and localization by hierarchical space-time segments	action localization;support vector machines;video signal processing;motion segmentation color tracking vegetation trajectory shape image segmentation;space time representation;image representation;action recognition;hierarchical space time segments representation action recognition action localization hierarchical space time segments two level hierarchy unsupervised method video representation nonstatic relevant space time segments static space time segments temporal relationships hierarchical relationships linear svm;space time representation action recognition action localization;video signal processing gesture recognition image representation support vector machines;gesture recognition	We propose Hierarchical Space-Time Segments as a new representation for action recognition and localization. This representation has a two-level hierarchy. The first level comprises the root space-time segments that may contain a human body. The second level comprises multi-grained space-time segments that contain parts of the root. We present an unsupervised method to generate this representation from video, which extracts both static and non-static relevant space-time segments, and also preserves their hierarchical and temporal relationships. Using simple linear SVM on the resultant bag of hierarchical space-time segments representation, we attain better than, or comparable to, state-of-the-art action recognition performance on two challenging benchmark datasets and at the same time produce good action localization results.	benchmark (computing);experiment;gaussian blur;internationalization and localization;machine learning;resultant;unsupervised learning	Shugao Ma;Jianming Zhang;Nazli Ikizler-Cinbis;Stan Sclaroff	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.341	support vector machine;computer vision;computer science;machine learning;pattern recognition;gesture recognition;mathematics	Vision	36.48226865752383	-50.28446273775875	139817
b0b7f427c1336e35a6b04aac5862ceae7d2a3724	vision-based action recognition of earthmoving equipment using spatio-temporal features and support vector machine classifiers	activity analysis;operational efficiency;computer vision;action recognition;construction productivity;time studies	We present a computer vision based method for equipment action recognition.Our vision-based method is based on a multiple binary SVM classifier and spatio-temporal features.A comprehensive real-world video dataset of excavator and truck actions is presented.We achieve accuracies of 86.33% and 98.33% for excavator and truck action classes.The presented method can be used for construction activity analysis using long sequences of videos. Video recordings of earthmoving construction operations provide understandable data that can be used for benchmarking and analyzing their performance. These recordings further support project managers to take corrective actions on performance deviations and in turn improve operational efficiency. Despite these benefits, manual stopwatch studies of previously recorded videos can be labor-intensive, may suffer from biases of the observers, and are impractical after substantial period of observations. This paper presents a new computer vision based algorithm for recognizing single actions of earthmoving construction equipment. This is particularly a challenging task as equipment can be partially occluded in site video streams and usually come in wide variety of sizes and appearances. The scale and pose of the equipment actions can also significantly vary based on the camera configurations. In the proposed method, a video is initially represented as a collection of spatio-temporal visual features by extracting space-time interest points and describing each feature with a Histogram of Oriented Gradients (HOG). The algorithm automatically learns the distributions of the spatio-temporal features and action categories using a multi-class Support Vector Machine (SVM) classifier. This strategy handles noisy feature points arisen from typical dynamic backgrounds. Given a video sequence captured from a fixed camera, the multi-class SVM classifier recognizes and localizes equipment actions. For the purpose of evaluation, a new video dataset is introduced which contains 859 sequences from excavator and truck actions. This dataset contains large variations of equipment pose and scale, and has varied backgrounds and levels of occlusion. The experimental results with average accuracies of 86.33% and 98.33% show that our supervised method outperforms previous algorithms for excavator and truck action recognition. The results hold the promise for applicability of the proposed method for construction activity analysis.	support vector machine	Mani Golparvar Fard;Arsalan Heydarian;Juan Carlos Niebles	2013	Advanced Engineering Informatics	10.1016/j.aei.2013.09.001	computer vision;simulation;computer science;engineering;artificial intelligence;machine learning;engineering drawing	ML	38.480849506148544	-43.673945103231645	139831
9f96cc4f09e6ba42c97fe5acf3e0827580c8d22e	late fusion via subspace search with consistency preservation		In many real-world applications, data can be represented by multiple ways or multi-view features to describe various characteristics of data. In this sense, the prediction performance can be significantly improved by taking advantages of these features together. Late fusion, which combines the predictions of multiple features, is a commonly used approach to make the final decision for a test instance. However, it is ubiquitous that different features dispute the prediction on the same data with each other, leading to performance degeneration. In this paper, we propose an efficient and effective matrix factorization-based approach to fuse predictions from multiple sources. This approach leverages a hard constraint on the matrix rank to preserve the consistency of predictions by various features, and we thus named it as Hard-rank Constraint Matrix Factorization-based fusion (HCMF). HCMF can avoid the performance degeneration caused by the controversy of multiple features. Extensive experiments demonstrate the efficacy of HCMF for outlier detection and the performance improvement, which outperforms the state-of-the-art late fusion algorithms on many data sets.	abnormal degeneration;algorithm;anomaly detection;application lifecycle management;constrained optimization;convergence (action);experiment;free viewpoint television;fuse device component;hearing loss, high-frequency;name;numerous;the matrix	Xuanyi Dong;Yuqing Chen;Mingkui Tan;Yi Yang;Ivor W. Tsang	2019	IEEE Transactions on Image Processing	10.1109/TIP.2018.2867747	robustness (computer science);anomaly detection;feature extraction;fusion;rank (linear algebra);mathematics;subspace topology;pattern recognition;artificial intelligence;matrix decomposition;data set	ML	27.145523338211756	-41.94293333497226	139873
241df182dc222d67e7a63831ab02edbaf8a1bc48	outdoor view recognition based on landmark grouping and logistic regression	visual saliency;articulo;robot navigation;visual landmarks;article;autonomous robot	Vision-based robot localization outdoors has remained more elusive than its indoors counterpart. Drastic illumination changes and the scarceness of suitable landmarks are the main difficulties. This paper attempts to surmount them by deviating from the main trend of using local features. Instead, a global descriptor called landmark-view is defined, which aggregates the most visually-salient landmarks present in each scene. Thus, landmark co-occurrence and spatial and saliency relationships between them are added to the single landmark characterization, based on saliency and color distribution. A suitable framework to compare landmark-views is developed, and it is shown how this remarkably enhances the recognition performance, compared against single landmark recognition. A view-matching model is constructed using logistic regression. Experimentation using 45 views, acquired outdoors, containing 273 landmarks, yielded good recognition results. The overall percentage of correct view classification obtained was 80.6%, indicating the adequacy of the approach.	landmark point;logistic regression;robotic mapping	Eduardo Todt;Carme Torras	2013	IJPRAI	10.1142/S0218001413550045	computer vision;artificial intelligence	Vision	33.33310254001534	-51.236041472281194	140034
fc98326914556875e70ed3f15449fb8272dc4450	recognizing human activities using appearance metric feature and kinematics feature		The problem of automatically recognizing human activities from videos through the fusion of the two most important cues, appearance metric feature and kinematics feature, is considered. And a system of two-dimensional (2-D) Poisson equations is introduced to extract the more discriminative appearance metric feature. Specifically, the moving human blobs are first detected out from the video by background subtraction technique to form a binary image sequence, from which the appearance feature designated as the motion accumulation image and the kinematics feature termed as centroid instantaneous velocity are extracted. Second, 2-D discrete Poisson equations are employed to reinterpret the motion accumulation image to produce a more differentiated Poisson silhouette image, from which the appearance feature vector is created through the dimension reduction technique called bidirectional 2-D principal component analysis, considering the balance between classification accuracy and time consumption. Finally, a cascaded classifier based on the nearest neighbor classifier and two directed acyclic graph support vector machine classifiers, integrated with the fusion of the appearance feature vector and centroid instantaneous velocity vector, is applied to recognize the human activities. Experimental results on the open databases and a homemade one confirm the recognition performance of the proposed algorithm.		Huimin Qian;Jun Zhou;Xinbiao Lu;Xinye Wu	2017	J. Electronic Imaging	10.1117/1.JEI.26.3.033015	computer vision;pattern recognition;feature	Robotics	37.92972412240648	-48.954352740163586	140072
d97c4da8cbc6b3c5de06eb457acfdf5c7241a522	monocular depth estimation by learning from heterogeneous datasets		Depth estimation provides essential information to perform autonomous driving and driver assistance. In particluar, monocular depth estimation is interesting from a practical point of view, since using a single camera is cheaper than many other options and avoids the need for continuous calibration strategies as required by stereo-vision approaches. State-of-the-art methods for monocular depth estimation are based on Convolutional Neural Networks (CNNs). A promising line of work consists of introducing additional semantic information about the traffic scene when training CNNs for depth estimation. In practice, this means that the depth data used for CNN training is complemented with images having pixelwise semantic labels, which usually are difficult to annotate (e.g. crowded urban images). Moreover, so far it is common practice to assume that the same raw training data is associated with both types of ground truth, i.e., depth and semantic labels. The main contribution of this paper is to show that this hard constraint can be circumvented, i.e., that we can train CNNs for depth estimation by leveraging the depth and semantic information coming from heterogeneous datasets. In order to illustrate the benefits of our approach, we combine KITTI depth and Cityscapes semantic segmentation datasets, outperforming stateof-the-art results on monocular depth estimation.		Akhil Gurram;Onay Urfalioglu;Ibrahim Halfaoui;Fahd Bouzaraa;Antonio M. Lopez	2018	2018 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2018.8500683	convolutional neural network;artificial intelligence;semantics;image segmentation;pattern recognition;computer science;ground truth;monocular;training set	Vision	28.23540545631638	-50.2798914424225	140168
d7cbb8bd8f4675cebf73dacdb55d9b37362203d0	from k-means to higher-way co-clustering: multilinear decomposition with sparse latent factors	unsupervised learning;pattern clustering;multi way analysis;unsupervised clustering;unsupervised clustering co clustering compressed sensing factor analysis k means multi way analysis sparsity tensor decomposition triclustering;compressed sensing;convergence;electronic mail;k means;data mining;co clustering;tensor decomposition;sparsity;factor analysis;enron e mail corpus k means clustering higher way coclustering multilinear decomposition sparse latent factors unsupervised clustering data mining applications data matrix groups columns coclustering data boxes higher way tensors multiple modes constrained multilinear decomposition higher way data three way data matrix coclustering lasso type coordinate updates various line search schemes latent sparsity collateral dividend;unsupervised learning convergence data mining electronic mail pattern clustering sparse matrices tensors;triclustering;sparse matrices;tensors;vectors arrays signal processing algorithms matrix decomposition educational institutions sparse matrices convergence	Co-clustering is a generalization of unsupervised clustering that has recently drawn renewed attention, driven by emerging data mining applications in diverse areas. Whereas clustering groups entire columns of a data matrix, co-clustering groups columns over select rows only, i.e., it simultaneously groups rows and columns. The concept generalizes to data “boxes” and higher-way tensors, for simultaneous grouping along multiple modes. Various co-clustering formulations have been proposed, but no workhorse analogous to K-means has emerged. This paper starts from K-means and shows how co-clustering can be formulated as a constrained multilinear decomposition with sparse latent factors. For three- and higher-way data, uniqueness of the multilinear decomposition implies that, unlike matrix co-clustering, it is possible to unravel a large number of possibly overlapping co-clusters. A basic multi-way co-clustering algorithm is proposed that exploits multilinearity using Lasso-type coordinate updates. Various line search schemes are then introduced to speed up convergence, and suitable modifications are proposed to deal with missing values. The imposition of latent sparsity pays a collateral dividend: it turns out that sequentially extracting one co-cluster at a time is almost optimal, hence the approach scales well for large datasets. The resulting algorithms are benchmarked against the state-of-art in pertinent simulations, and applied to measured data, including the ENRON e-mail corpus.	algorithm;benchmark (computing);biclustering;cluster analysis;column (database);data mining;email;k-means clustering;lasso;latent variable;line search;missing data;qr decomposition;relevance;simulation;singular value decomposition;sparse matrix	Evangelos E. Papalexakis;Nikos D. Sidiropoulos;Rasmus Bro	2013	IEEE Transactions on Signal Processing	10.1109/TSP.2012.2225052	unsupervised learning;multilinear principal component analysis;tensor;convergence;sparse matrix;computer science;machine learning;pattern recognition;data mining;mathematics;factor analysis;compressed sensing;sparsity-of-effects principle;statistics;k-means clustering	ML	26.912312959399596	-38.462586834244895	140211
97026ba93a5d37d6968960df91964b1448d268e7	feature classification for the satellite modulation based on sparse coding algorithm	sparse;satellite;feature;extraction;modulation	In order to improve the characteristics for satellite data expression ability, it is proposed sparse coding with over complete bases, to induce the high dimensional feature vectors form down to up pattern, then to accurately express the original high-dimensional features with very few over complete basis vectors. From top-down semi supervised learning characteristics, to project high dimensional feature to low dimensional space, in order to verify the similarity of coding of the training data, then to express the characteristics of input satellite data. The encoder needs as much as possible reconstruction for the input multi dimensional, and the find the main ingredients for representing the original information about input data. Therefore, the over complete is not only by the coefficient of input data to determine characteristics, but also by the dimensional space. In order to verify the performance of the sparse coding algorithm, using the sparse coding algorithm is to identify the 6 kinds of commonly used digital modulation signal: 4ASK, 4FSK, 4PSK, MSK, 16QAM, π/4QPSK. The performance of correct modulation recognition rate is higher stability. The overall recognition of 6 kinds of modulation rate of SNR is higher, not less than 0dB 99%.	algorithm;basis (linear algebra);coefficient;encoder;modulation;neural coding;semi-supervised learning;semiconductor industry;signal-to-noise ratio;sparse matrix;supervised learning;top-down and bottom-up design	Qingyang Guan;Wenqing Zhu;Xuan Li	2015	2015 10th International Conference on Communications and Networking in China (ChinaCom)	10.1109/CHINACOM.2015.7498046	extraction;feature;telecommunications;computer science;theoretical computer science;machine learning;pattern recognition;satellite;modulation	DB	29.76117607260556	-44.58788645724801	140597
a31b5f41678aea8453e0b189158b1509e4ff9354	learning-based feature extraction for active 3d scan with reducing color crosstalk of multiple pattern projections		3D reconstruction methods based on active stereo technique have been widely used for many practical systems. Many of these systems are configured with a single camera and a single projector. Since such systems can only capture one side of the target object, several attempts have been conducted to enlarge the captured area, especially multi-projector systems attract many researchers. For multi-projector based systems, overlap between multiple pattern projections is a serious problem. Even if different color channels are used for each projector, complete separation is not possible because of color crosstalks. Another open problem is decoding errors of the projected patterns, which causes a failure on extracting positional information of the projected pattern form the captured image. Among several reasons for such errors, color crosstalks are crucial because their features are similar to the main signal and difficult to be decomposed. In this paper, we solve these problems by utilizing machine learning techniques where a convolutional neural network is trained to extract low dimensional pattern features for each projector. In addition, it is trained to suppress the color crosstalks from different projectors. Using this new technique, we succeeded in reconstructing 3D shapes from images where multiple patterns are overlapped.	3d reconstruction;3d scanner;artificial neural network;capture one;channel (digital image);convolutional neural network;crosstalk;feature extraction;machine learning;movie projector;sensor;video projector	Ryusuke Sagawa;Ryo Furukawa;Akiko Matsumoto;Hiroshi Kawasaki	2017	2017 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2017.7989592	convolutional neural network;projector;open problem;3d reconstruction;feature extraction;decoding methods;pattern matching;computer vision;artificial intelligence;channel (digital image);mathematics	Robotics	30.209627708094995	-51.77704604776438	140638
57d87450078e0cf9caac3cde6f69e45b6b6a7760	social behavior analysis of drosophila larvae via motion activity recognition	biology computing;image motion analysis;support vector machines;neural nets;biomechanics;image classification;abstracts indexes;gabor filters;support vector machine drosophila larvae social behavior analysis motion activity recognition automated motion activity pattern identification social activity recognition drosophila brain neuronal structure neuronal structure social behavior correlation larvae videos low cost overhead video motion features gradient weighted optical flow global histogram time frame optical flow estimation horn schunck method spatial gabor filter pair intensity gradient recurrent neural networks velocity activity classification accuracy;feature extraction;gradient methods;zoology biological techniques biology computing biomechanics feature extraction gabor filters gradient methods image classification image motion analysis image sequences neural nets neurophysiology support vector machines;zoology;neurophysiology;biological techniques;optical flow activity recognition hog;image sequences	We aim to analyze the social behavior of Drosophila larvae through automated identification of their motion activity pattern. Social activity recognition is an essential step in identifying the correlation between specific neuronal structures in their brain to the social behavior of the species. Larvae videos are obtained using low-cost overhead video; subsequently, the motion features of the larvae generated by computing a gradient weighted optical flow. These features are used to create a global histogram of gradient-weighted optic flow over a specific time frame. The method for estimating optical flow is based on an implementation by Gautama and Van Hulle that expands on the classical Horn and Schunck method by using spatial Gabor filter pair to find the intensity gradient and recurrent neural networks to find velocity. Activity classification is obtained by using a support vector machine and yields 82% overall accuracy in classification.	activity recognition;artificial neural network;gabor filter;gradient;optical flow;overhead (computing);recurrent neural network;support vector machine;velocity (software development)	Micah Consylman;Suvadip Mukherjee;Dipti Prasad Mukherjee;Barry Condron;Scott T. Acton	2014	2014 Southwest Symposium on Image Analysis and Interpretation	10.1109/SSIAI.2014.6806054	support vector machine;computer vision;contextual image classification;feature extraction;computer science;artificial intelligence;biomechanics;machine learning;neurophysiology	Vision	37.03043569068614	-49.772807337040284	140717
597560e59cc23c49f196124462f38f3475d70ce3	a novel data clustering method based on smooth non-negative matrix factorization		Non-negative matrix factorization (NMF) is a very popular dimensionality reduction method that has been widely used in computer vision and data clustering. However, NMF does not consider the intrinsic geometric information of a data set and also does not produce smooth and stable solutions. To resolve these problems, we propose a Graph regularized Lp Smooth Non-negative Matrix Factorization (GSNMF) method by incorporating graph regularization with Lp smooth constraint. The graph regularization can discover the hidden semantics and simultaneously respect the intrinsic geometric structure information of a data set. The Lp smooth constraint can combine the merits of isotropic (L2-norm) and anisotropic (L1-norm) diffusion smoothing, and produce a smooth and more accurate solution to the optimization problem. Experimental results on some data sets demonstrate that the proposed method outperforms related state-of-the-art NMF methods.		Chengcai Leng;Hai Zhang;Guorong Cai	2018		10.1007/978-3-030-04375-9_35	condensed matter physics;computer science;dimensionality reduction;mathematical optimization;cluster analysis;smoothing;matrix decomposition;regularization (mathematics);optimization problem;non-negative matrix factorization;data set	ML	26.322706734100812	-39.51971658247442	140896
a06c552a76944137be9bc72ea3c4186b3f70abb3	superpixel based spectral classification of hyperspectral images in different color spaces		In this paper, superpixel based spectral classification of hyperspectral images are compared using different color spaces. The effective use of spatial and spectral information by using superpixel-based approaches increases classification performance of hyperspectral data. SLIC superpizel algorithm is applied to color images obtained using different datasets. Support vector machine (SVM) is used as classification method. Classification performance of hyperspectral data is compared in RGB space, LAB space, HSV space, XYZ space and SVM. It is shown that superpixel based spectral classification in LAB space gives better classification accuracy than other color spaces and pixel-based approach.	color space;stellar classification	Yazarlar Gizlenmistir	2018		10.1109/SIU.2018.8404148	computer vision;hyperspectral imaging;computer science;support vector machine;pattern recognition;artificial intelligence;statistical classification;hsl and hsv;rgb color model;color space	Vision	30.53191304091676	-44.276711164175325	140971
3fad55a31b7476ed77589b9bd3fc91abf95a56b1	remote sensing image classification using extreme learning machine-guided collaborative coding	extreme learning machine;collaborative coding;covariance descriptor	Remote sensing image classification is a very challenging problem and covariance descriptor can be introduced in the feature extraction and representation process for remote sensing image. However, due to the reason that covariance descriptor lies in non-Euclidean manifold, conventional extreme learning machine (ELM) cannot effectively deal with this problem. In this paper, we propose an improved ELM framework which incorporates the collaborative coding to tackle the covariance descriptor classification problem. First, a new ELM-guided dictionary learning and coding model is proposed. Then the iterative optimization algorithm is developed to solve the model. By evaluating the proposed approach on the UCMERCED high-resolution aerial image dataset, we show the effectiveness of the proposed strategy.		Chunwei Yang;Huaping Liu;Shicheng Wang;Shouyi Liao	2017	Multidim. Syst. Sign. Process.	10.1007/s11045-016-0403-6	remote sensing;coding (social sciences);extreme learning machine;feature extraction;machine learning;aerial image;covariance;pattern recognition;artificial intelligence;computer science;contextual image classification	ML	25.707475179911075	-43.25549409602341	140980
7b8cfd54bf9392c91af4c4621d793ede638fc81d	cdmma: coupled discriminant multi-manifold analysis for matching low-resolution face images	low resolution;discriminant analysis;face recognition;multi manifold;super resolution	Face images captured by surveillance cameras usually have low-resolution (LR) in addition to uncontrolled poses and illumination conditions, all of which adversely affect the performance of face matching algorithms. In this paper, we develop a novel method to address the problem of matching a LR or poor quality face image to a gallery of highresolution (HR) face images. In recent years, extensive efforts have been made on LR face recognition (FR) research. Previous research has focused on introducing a learning based super-resolution (LBSR) method before matching or transforming LR and HR faces into a unified feature space (UFS) for matching. To identify LR faces, we present a method called coupled discriminant multi-manifold analysis (CDMMA). In CDMMA, we first explore the neighborhood information as well as local geometric structure of the multi-manifold space spanned by the samples. And then, we explicitly learn two mappings to project LR and HR faces to a unified discriminative feature space (UDFS) through a supervised manner, where the discriminative information is maximized for classification. After that, the conventional classification method is applied in the CDMMA for final identification. Experimental results conducted on two standard face recognition databases demonstrate the superiority of the proposed CDMMA. & 2015 Elsevier B.V. All rights reserved.	algorithm;closed-circuit television;database;discriminant;experiment;facial recognition system;feature vector;image resolution;lr parser;signal processing;sketch recognition;super-resolution imaging;uncontrolled format string;universal disk format;universal flash storage	Junjun Jiang;Ruimin Hu;Zhongyuan Wang;Zhihua Cai	2016	Signal Processing	10.1016/j.sigpro.2015.09.026	facial recognition system;computer vision;image resolution;computer science;machine learning;pattern recognition;mathematics;linear discriminant analysis;superresolution	AI	28.281261886371183	-47.15203081785249	141026
1dedd988b5277b322eb008e452f5c729bd0c81da	automatic grasp selection using a camera in a hand prosthesis	object recognition;training;testing;electromyography;prosthetic hand;cameras;real time systems	In this paper, we demonstrate how automatic grasp selection can be achieved by placing a camera in the palm of a prosthetic hand and training a convolutional neural network on images of objects with corresponding grasp labels. Our labeled dataset is built from common graspable objects curated from the ImageNet dataset and from images captured from our own camera that is placed in the hand. We achieve a grasp classification accuracy of 93.2% and show through realtime grasp selection that using a camera to augment current electromyography controlled prosthetic hands may be useful.	artificial neural network;biological neural networks;convolutional neural network;electromyography;imagenet;physical object;prosthesis implantation;silo (dataset);tracer	Joseph DeGol;Aadeel Akhtar;Bhargava Manja;Timothy Bretl	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7590732	computer vision;simulation;computer science;engineering;cognitive neuroscience of visual object recognition;software testing;computer graphics (images)	Robotics	36.47003634986906	-47.117133783565485	141237
55ffbc13fab758b1db9b55d4e9477d92ab9efdf0	set-based feature learning for person re-identification via third-party images	image recognition;support vector machines;support vector machines feature extraction image recognition lighting;feature extraction;feature extraction histograms cameras prototypes lighting support vector machines visualization;lighting;image set discriminative features person re identification third party images illumination variation viewpoint variation pose variation prototype theory cognition field exemplar svm set based feature learning re identification method	Person re-identification from disjoint camera views has been an important and unsolved problem due to large variations in illumination, viewpoint and pose. One way to attack this is by designing a new, more powerful image representation. However, we believe that existing representations are already sufficient. The main difficulty is how to pick the most informative information using these representations. Inspired by the prototype theory from the cognition field and Exemplar-SVM, we propose a novel and simple set-based feature learning re-identification method via third-party images. In our settings, each query/gallery example is an image set of the same individual, not just a single image. Discriminative features of a certain individual image set are explored from the third-party images. Comparisons with state-of-the-art methods on benchmark datasets demonstrate impressive results using simple and common features.	autostereogram;benchmark (computing);cognition;feature learning;information;pose (computer vision);prototype;real life;simple set	Yanna Zhao;Lei Wang;Yuncai Liu	2013	2013 2nd IAPR Asian Conference on Pattern Recognition	10.1109/ACPR.2013.87	computer vision;feature detection;feature vector;feature extraction;computer science;machine learning;pattern recognition;feature	Vision	34.01894244653035	-51.72118815330681	141306
8d5ea0c79eecc9e6c857eac5d494d57960e0f587	watch-list screening using ensembles based on multiple face representations	face principal component analysis iron feature extraction robustness cameras video sequences;ensemble diversity multiple face representations still to video face recognition video surveillance cameras unconstrained surveillance environments semiconstrained surveillance environments performance baseline single high quality reference watchlist screening applications template matchers feature extraction techniques multiple fe techniques nuisance factors relevant feature subsets decision thresholds fusion functions nontarget individuals reference videos universal background model face tracker robust spatiotemporal fr detection threshold chokepoint video dataset patches based techniques;video surveillance face recognition feature extraction image matching image representation	Still-to-video face recognition (FR) is an important function in watch list screening, where faces captured over a network of video surveillance cameras are matched against reference stills of target individuals. Recognizing faces in a watch list is a challenging problem in semi - and unconstrained surveillance environments due to the lack of control over capture and operational conditions, and to the limited number of reference stills. This paper provides a performance baseline and guidelines for ensemble-based systems using a single high-quality reference still per individual, as found in many watch list screening applications. In particular, modular systems are considered, where an ensemble of template matchers based on multiple face representations is assigned to each individual of interest. During enrollment, multiple feature extraction (FE) techniques are applied to patches isolated in the reference still to generate diverse face-part representations that are robust to various nuisance factors (e.g., illumination and pose) encountered in video surveillance. The selection of relevant feature subsets, decision thresholds, and fusion functions of ensembles are achieved using faces of non-target individuals selected from reference videos (forming a universal background model). During operations, a face tracker gradually regroups faces captured from different people appearing in a scene, while each user-specific ensemble generates a decision per face capture. This leads to robust spatio-temporal FR when accumulated ensemble predictions surpass a detection threshold. Simulation results obtained with the Chokepoint video dataset show a significant improvement to accuracy, (1) when performing score-level fusion of matchers, where patches-based and FE techniques generate ensemble diversity, (2) when defining feature subsets and decision thresholds for each individual matcher of an ensemble using non-target videos, and (3) when accumulating positive detections over multiple frames.	baseline (configuration management);bittorrent tracker;closed-circuit television;display resolution;ensemble forecasting;facial recognition system;feature extraction;high- and low-level;nuisance variable;semiconductor industry;sensor;simulation	Saman Bashbaghi;Eric Granger;Robert Sabourin;Guillaume-Alexandre Bilodeau	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.768	computer vision;computer science;machine learning;pattern recognition	Vision	34.862160446323706	-51.730090423581174	141371
2ac833edbf3e623d559c0c5f0186f0ef58e22a0a	a block-based human model for visual surveillance	real time monitoring;domain knowledge;visual surveillance;visual art;human activity;scale invariance	This paper presents BB6-HM, a block-based human model for real-time monitoring of a large number of visual events and states related to human activity analysis, which can be used as components of a library to describe more complex activities in such important areas as surveillance. BB6-HM is inspired by the proportionality rules commonly used in Visual Arts, i.e., for dividing the human silhouette into six rectangles of the same height. The major advantage of this proposal is that analysis of the human can be easily broken down into parts, which allows us to introduce more specific domain knowledge and to reduce the computational load. It embraces both frontal and lateral views, is a fast and scale-invariant method and a large amount of task-focused information can be extracted from it.		Encarnación Folgado;Mariano Rincón;Margarita Bachiller;Enrique J. Carmona	2009		10.1007/978-3-642-02267-8_23	computer vision;simulation;computer science;artificial intelligence;machine learning;scale invariance;human visual system model;domain knowledge	AI	37.40537266974304	-49.83353917999672	141611
aa9f6600d4579e64fb6f256fa373fbd088072979	learning overcomplete sparsifying transforms with block cosparsity	overcomplete representation;transforms image denoising image representation image texture pattern clustering;image denoising sparsifying transform learning overcomplete representation sparse representation clustering;clustering;noise reduction dictionaries analytical models discrete cosine transforms clustering algorithms encoding;image denoising;synthesis dictionary based denoising overcomplete sparsifying transform learning block cosparsity image processing single transform model natural images diverse textures oc tobos closed form updates clustering image representations speedups;sparse representation;sparsifying transform learning	The sparsity of images in a transform domain or dictionary has been widely exploited in image processing. Compared to the synthesis dictionary model, sparse coding in the (single) transform model is cheap. However, natural images typically contain diverse textures that cannot be sparsified well by a single transform. Hence, we propose a union of sparsifying transforms model, which is equivalent to an overcomplete transform model with block cosparsity (OC-TOBOS). Our alternating algorithm for transform learning involves simple closed-form updates. When applied to images, our algorithm learns a collection of well-conditioned transforms, and a good clustering of the patches or textures. Our learnt transforms provide better image representations than learned square transforms. We also show the promising denoising performance and speedups provided by the proposed method compared to synthesis dictionary-based denoising.	algorithm;cluster analysis;condition number;dictionary;image processing;neural coding;noise reduction;sparse matrix	Bihan Wen;Saiprasad Ravishankar;Yoram Bresler	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025161	computer vision;k-svd;computer science;machine learning;pattern recognition;sparse approximation;mathematics;cluster analysis	Vision	26.776914020489386	-40.999582382750944	141761
6c9e1a9c1136a557a2f4a36eba43442c0a731010	complex activity recognition via attribute dynamics	complex activity;attribute;dynamical model;variational inference;fisher score	The problem of modeling the dynamic structure of human activities is considered. Video is mapped to a semantic feature space, which encodes activity attribute probabilities over time. The binary dynamic system (BDS) model is proposed to jointly learn the distribution and dynamics of activities in this space. This is a non-linear dynamic system that combines binary observation variables and a hidden Gauss–Markov state process, extending both binary principal component analysis and the classical linear dynamic systems. A BDS learning algorithm, inspired by the popular dynamic texture, and a dissimilarity measure between BDSs, which generalizes the Binet–Cauchy kernel, are introduced. To enable the recognition of highly non-stationary activities, the BDS is embedded in a bag of words. An algorithm is introduced for learning a BDS codebook, enabling the use of the BDS as a visual word for attribute dynamics (WAD). Short-term video segments are then quantized with a WAD codebook, allowing the representation of video as a bag-of-words for attribute dynamics. Video sequences are finally encoded as vectors of locally aggregated descriptors, which summarize the first moments of video snippets on the BDS manifold. Experiments show that this representation achieves state-of-the-art performance on the tasks of complex activity recognition and event identification.	activity recognition;algorithm;bag-of-words model;codebook;dynamical system;embedded system;feature vector;hidden markov model;manifold regularization;markov chain;nonlinear system;principal component analysis;reinforcement learning;stationary process;visual word	Weixin Li;Nuno Vasconcelos	2016	International Journal of Computer Vision	10.1007/s11263-016-0918-1	computer vision;computer science;machine learning;pattern recognition	Vision	35.45532176219917	-48.030870663517966	141797
03613bbadde699e9e47f0a9cd2da6c8ef8dd44cf	recovering 6d object pose and predicting next-best-view in the crowd		Object detection and 6D pose estimation in the crowd (scenes with multiple object instances, severe foreground occlusions and background distractors), has become an important problem in many rapidly evolving technological areas such as robotics and augmented reality. Single shotbased 6D pose estimators with manually designed features are still unable to tackle the above challenges, motivating the research towards unsupervised feature learning and next-best-view estimation. In this work, we present a complete framework for both single shot-based 6D object pose estimation and next-best-view prediction based on Hough Forests, the state of the art object pose estimator that performs classification and regression jointly. Rather than using manually designed features we a) propose an unsupervised feature learnt from depth-invariant patches using a Sparse Autoencoder and b) offer an extensive evaluation of various state of the art features. Furthermore, taking advantage of the clustering performed in the leaf nodes of Hough Forests, we learn to estimate the reduction of uncertainty in other views, formulating the problem of selecting the next-best-view. To further improve pose estimation, we propose an improved joint registration and hypotheses verification module as a final refinement step to reject false detections. We provide two additional challenging datasets inspired from realistic scenarios to extensively evaluate the state of the art and our framework. One is related to domestic environments and the other depicts a bin-picking scenario mostly found in industrial settings. We show that our framework significantly outperforms state of the art both on public and on our datasets.	3d pose estimation;augmented reality;autoencoder;cluster analysis;convolutional neural network;emoticon;feature learning;hough transform;instance (computer science);object detection;refinement (computing);robotics;sensor;sparse;tree (data structure);unsupervised learning	Andreas Doumanoglou;Rigas Kouskouridas;Sotiris Malassiotis;Tae-Kyun Kim	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.390	computer vision;simulation;3d pose estimation;machine learning;pattern recognition	Vision	30.779355149126854	-49.72544733415263	141861
c2d17655186d58ce56f8952312e7921507a0fb38	appearance based robot activity recognition system	image recognition;legged locomotion;training;feature extraction;webcams;activity recognition	In this work, we present an appearance based human activity recognition system. It uses background modeling to segment the foreground object and extracts useful discriminative features for representing activities performed by humans and robots. Subspace based method like principal component analysis is used to extract low dimensional features from large voluminous activity images. These low dimensional features are then used to classify an activity. An apparatus is designed using a webcam, which watches a robot replicating a human fall under indoor environment. In this apparatus, a robot performs various activities (like walking, bending, moving arms) replicating humans, which also includes a sudden fall. Experimental results on robot performing various activities and standard human activity recognition databases show the efficacy of our proposed method.	activity recognition;coat of arms;database;eigen (c++ library);outliner;principal component analysis;real-time computing;robot;webcam	Bappaditya Mandal	2016	2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)	10.1109/ICARCV.2016.7838755	computer vision;simulation;feature extraction;computer science;engineering;activity recognition	Robotics	36.79431502701405	-48.28208085147579	141927
277cf5d6f27d06e66c83f2eb56369e5e19a6157d	detecting rare actions and events from surveillance big data with bag of dynamic trajectories	video surveillance big data image classification object detection support vector machines video signal processing;classification boundary rare action detection event detection surveillance big data surveillance video bag of dynamic trajectories bodt trajectory descriptor multichannel uneven svm dtw dynamic time warping;会议论文;conferences multimedia communication big data;big data;multimedia communication;conferences	"""Surveillance video is increasingly becoming the """"biggest big data"""". This presents an unprecedented challenge for analyzing and mining the meaningful information (e.g., Rare actions or events) in such a huge amount of videos. Recent studies have shown that feature-trajectories-based methods are effective to encode motion information in video, consequently demonstrating superior performance in action and event detection. However, in existing methods, distance between two trajectories is often measured by linear models, which may be not robust enough when the lengths of trajectories are variable. Moreover, due to the rare distribution of target actions or events, the traditional classifier often tends to identify all samples as negative, consequently producing heavy performance bias. To address both two issues, this paper proposes a trajectory descriptor, BoDT (Bag of Dynamic Trajectories), and a multi-channel uneven SVM. By utilizing the DTW (dynamic time warping) algorithm to measure the similarity between two trajectories, BoDT is robust for variable-length trajectory representation. Meanwhie, as an extension of SVM with uneven margins, the proposed multi-channel uneven SVM can successfully identify rare events by adjusting a margin parameter to make the classification boundary properly moved away from the positive training examples. Extensive experiments on several benchmark datasets including KTH, YouTube, Olympic, MIT, QMUL and TRECVid demonstrate that our approach is feasible and effective."""	algorithm;benchmark (computing);big data;dynamic time warping;encode;experiment;linear model;rare events;sensor	Yaowei Wang;Yonghong Tian;Limin Su;Xiaoyu Fang;Ziwei Xia;Tiejun Huang	2015	2015 IEEE International Conference on Multimedia Big Data	10.1109/BigMM.2015.74	computer vision;simulation;computer science;data mining	Vision	38.052430512501	-46.96233396868888	141995
1ea2a53a6cb9c08312276a2f0646935d5fab5ed3	real-time crowd tracking using parameter optimized mixture of motion models		We present a novel, real-time algorithm to track the trajectory of each pedestrian in moderately dense crowded scenes. Our formulation is based on an adaptive particle-filtering scheme that uses a combination of various multi-agent heterogeneous pedestrian simulation models. We automatically compute the optimal parameters for each of these different models based on prior tracked data and use the best model as motion prior for our particle-filter based tracking algorithm. We also use our “mixture of motion models” for adaptive particle selection and accelerate the performance of the online tracking algorithm. The motion model parameter estimation is formulated as an optimization problem, and we use an approach that solves this combinatorial optimization problem in a model independent manner and hence scalable to any multi-agent pedestrian motion model. We evaluate the performance of our approach on different crowd video datasets and highlight the improvement in accuracy over homogeneous motion models and a baseline mean-shift based tracker. In practice, our formulation can compute trajectories of tens of pedestrians on a multi-core desktop CPU in in real time and offer higher accuracy as compared to prior real time pedestrian tracking algorithms. Aniket Bera and Dinesh Manocha Department of Computer Science The University of North Carolina at Chapel Hill NC 27599, USA E-mail: {ab, dm} @cs.unc.edu David Wolinski and Julien Pettré INRIA-Rennes, Campus de Beaulieu 35042 Rennes Cedex, FRANCE E-mail: {julien.pettre, david.wolinski} @inria.fr	algorithm;baseline (configuration management);central processing unit;chapel;combinatorial optimization;computer science;desktop computer;estimation theory;mathematical optimization;mean shift;multi-agent system;multi-core processor;optimization problem;particle filter;real-time clock;real-time computing;real-time transcription;scalability;simulation	Aniket Bera;David Wolinski;Julien Pettré;Dinesh Manocha	2014	CoRR		computer vision;simulation;computer science;computer graphics (images)	Vision	39.07433693224334	-40.41519175312732	142006
b476684bd07163d1e3183aab3773f7c3bc188884	ik-svd: dictionary learning for spatial big data via incremental atom update	scientific computing big data sparse representation dictionary learning;spatial data structures big data singular value decomposition;dictionaries remote sensing information management data handling data storage systems big data mathematical model learning systems spatial analysis;data storage systems;journal article;big data;redundant information ik svd method dictionary learning process spatial big data incremental atom update data sparse representation orthogonal matching pursuit error matrix contruction svd decomposition process training cycle;remote sensing;information management;dictionaries;scientific computing;mathematical model;dictionary learning;data handling;sparse representation	A large group of dictionary learning algorithms focus on adaptive sparse representation of data. Almost all of them fix the number of atoms in iterations and use unfeasible schemes to update atoms in the dictionary learning process. It's difficult, therefore, for them to train a dictionary from Big Data. A new dictionary learning algorithm is proposed here by extending the classical K-SVD method. In the proposed method, when each new batch of data samples is added to the training process, a number of new atoms are selectively introduced into the dictionary. Furthermore, only a small group of new atoms as subspace controls the current orthogonal matching pursuit, construction of error matrix, and SVD decomposition process in every training cycle. The information, from both old and new samples, is explored in the proposed incremental K-SVD (IK-SVD) algorithm, but only the current atoms are adaptively updated. This makes the dictionary better represent all the samples without the influence of redundant information from old samples.	algorithm;atom;big data;dictionary;iteration;k-svd;machine learning;matching pursuit;singular value decomposition;sparse approximation;sparse matrix	Lizhe Wang;Ke Lu;Peng Liu;Rajiv Ranjan;Lajiao Chen	2014	Computing in Science & Engineering	10.1109/MCSE.2014.52	big data;k-svd;computer science;theoretical computer science;machine learning;group method of data handling;sparse approximation;mathematical model;data mining;information management;matching pursuit	AI	28.643029257224423	-40.11071566921084	142121
1ba9d12f24ac04f0309e8ff9b0162c6e18d97dc3	robust face recognition with deep multi-view representation learning	multi view feature representation;face recognition;deep learning;model ensemble	This paper describes our proposed method targeting at the MSR Image Recognition Challenge MS-Celeb-1M. The challenge is to recognize one million celebrities from their face images captured in the real world. The challenge provides a large scale dataset crawled from the Web, which contains a large number of celebrities with many images for each subject. Given a new testing image, the challenge requires an identify for the image and the corresponding confidence score. To complete the challenge, we propose a two-stage approach consisting of data cleaning and multi-view deep representation learning. The data cleaning can effectively reduce the noise level of training data and thus improves the performance of deep learning based face recognition models. The multi-view representation learning enables the learned face representations to be more specific and discriminative. Thus the difficulties of recognizing faces out of a huge number of subjects are substantially relieved. Our proposed method achieves a coverage of 46.1% at 95% precision on the random set and a coverage of 33.0% at 95% precision on the hard set of this challenge.	computer vision;deep learning;facial recognition system;feature learning;machine learning;noise (electronics);plasma cleaning;world wide web	Jian Zhao;Fang Zhao;Hao Liu;Jing Li;Shengmei Shen;Jiashi Feng;Terence Sim	2016		10.1145/2964284.2984061	computer vision;computer science;machine learning;pattern recognition;three-dimensional face recognition;deep learning	AI	27.403034048021688	-48.38806871605974	142159
eea376e1d44c51d29f58a341e7b81584b407aee2	nam: non-adversarial unsupervised domain mapping		Several methods were recently proposed for the task of translating images between domains without prior knowledge in the form of correspondences. The existing methods apply adversarial learning to ensure that the distribution of the mapped source domain is indistinguishable from the target domain, which suffers from known stability issues. In addition, most methods rely heavily on “cycle” relationships between the domains, which enforce a one-to-one mapping. In this work, we introduce an alternative method: Non-Adversarial Mapping (NAM), which separates the task of target domain generative modeling from the crossdomain mapping task. NAM relies on a pre-trained generative model of the target domain, and aligns each source image with an image synthesized from the target domain, while jointly optimizing the domain mapping function. It has several key advantages: higher quality and resolution image translations, simpler and more stable training and reusable target models. Extensive experiments are presented validating the advantages of our method.		Yedid Hoshen;Lior Wolf	2018		10.1007/978-3-030-01264-9_27	adversarial system;generative grammar;machine learning;artificial intelligence;computer science;generative model	ML	25.032558881870465	-49.067987723472335	142287
3a1b9e6e4142e8da93568a0bdd7eb9241827657d	learning latent spatio-temporal compositional model for human action recognition	structural learning;action recognition;video understanding;and or graph	Action recognition is an important problem in multimedia understanding. This paper addresses this problem by building an expressive compositional action model. We model one action instance in the video with an ensemble of spatio-temporal compositions: a number of discrete temporal anchor frames, each of which is further decomposed to a layout of deformable parts. In this way, our model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent the latent structure of actions \emph{e.g.} triple jumping, swinging and high jumping. The STAOG model comprises four layers: (i) a batch of leaf-nodes in bottom for detecting various action parts within video patches; (ii) the or-nodes over bottom, i.e. switch variables to activate their children leaf-nodes for structural variability; (iii) the and-nodes within an anchor frame for verifying spatial composition; and (iv) the root-node at top for aggregating scores over temporal anchor frames. Moreover, the contextual interactions are defined between leaf-nodes in both spatial and temporal domains. For model training, we develop a novel weakly supervised learning algorithm which iteratively determines the structural configuration (e.g. the production of leaf-nodes associated with the or-nodes) along with the optimization of multi-layer parameters. By fully exploiting spatio-temporal compositions and interactions, our approach handles well large intra-class action variance (\emph{e.g.} different views, individual appearances, spatio-temporal structures). The experimental results on the challenging databases demonstrate superior performance of our approach over other methods.	algorithm;database;interaction;layer (electronics);mathematical optimization;sensor;spatial variability;staog;supervised learning;verification and validation	Xiaodan Liang;Liang Lin;Liangliang Cao	2013		10.1145/2502081.2502089	computer vision;computer science;artificial intelligence;machine learning;pattern recognition	Vision	34.24686478538334	-47.925554383232594	142357
c872d6310f2079db0cee0e69cc96da1470055225	heterogeneous multi-task learning on non-overlapping datasets for facial landmark detection		We propose a heterogeneous multi-task learning framework on non-overlapping datasets, where each sample has only part of the labels and the size of each dataset is different. In particular, we propose two batch sampling strategies for stochastic gradient descent to learn shared CNN representation. First one sets same number of iteration on each dataset while the latter sets same batch size ratio of one task to another. We evaluate the proposed framework by learning the facial expression recognition task and facial landmark detection task. The learned network is memory efficient and able to carry out multiple tasks for one feed forward with the shared CNN. In addition, we show that the learned network achieve more robust facial landmark detection under large variation which appears in the heterogeneous dataset, though the dataset does not include landmark labels. We also investigate the effect of weights on each cost function and batch size ratio of one task to another.	computer multitasking;image analysis;iteration;loss function;multi-task learning;sampling (signal processing);stochastic gradient descent	Takayuki Semitsu;Xiongxin Zhao;Wataru Matsumoto	2016		10.1007/978-3-319-46675-0_68	computer vision;machine learning;pattern recognition	ML	26.848684489907217	-47.316313028220904	142377
3b22bd73a94d177bfbfda93a6b7bea6bade409d3	extracting spatio-temporal local features considering consecutiveness of motions	image features;local features;visual features;optical flow	Recently spatio-temporal local features have been proposed as image features to recognize events or human actions in videos. In this paper, we propose yet another local spatio-temporal feature based on the SURF detector, which is a lightweight local feature. Our method consists of two parts: extracting visual features and extracting motion features. First, we select candidate points based on the SURF detector. Next, we calculate motion features at each point with local temporal units divided in order to consider consecutiveness of motions. Since our proposed feature is intended to be robust to rotation, we rotate optical flow vectors to the main direction of extracted SURF features. In the experiments, we evaluate the proposed spatio-temporal local feature with the common dataset containing six kinds of simple human actions. As the result, the accuracy achieves 86%, which is almost equivalent to stateof-the-art. In addition, we make experiments to classify large amounts of Web video clips downloaded from Youtube.	action potential;closed-circuit television;computer vision;experiment;feature model;optical flow;pattern recognition;speeded up robust features;video clip;yet another	Akitsugu Noguchi;Keiji Yanai	2009		10.1007/978-3-642-12304-7_43	computer vision;computer science;pattern recognition;data mining;optical flow;feature	Vision	36.194944726772796	-50.34918084837761	142560
072319c696de87d99a41df598dbe56538ee995fd	online, self-supervised terrain classification via discriminatively trained submodular markov random fields	image segmentation;terrain mapping gradient methods image classification image segmentation markov processes mobile robots random processes robot vision;real time;subgradient method;averaged subgradient method;robot online self supervised terrain classification markov random fields autonomous terrain classification structured prediction classifier learning image segmentation submodular mrf framework inference task max flow computation averaged subgradient method;markov random fields;online self supervised terrain classification;image classification;mobile robots;inference task;submodular mrf framework;markov random field;robot vision;markov random fields navigation robot sensing systems robot vision systems cameras mobile robots humans stereo vision training data graphical models;structure prediction;classifier learning;random processes;gradient methods;structured prediction;discriminative training;terrain mapping;markov processes;classification accuracy;robot;autonomous terrain classification;max flow computation	"""The authors present a novel approach to the task of autonomous terrain classification based on structured prediction. We consider the problem of learning a classifier that will accurately segment an image into """"obstacle"""" and """"ground"""" patches based on supervised input. Previous approaches to this problem have focused mostly on local appearance; typically, a classifier is trained and evaluated on a pixel-by-pixel basis, making an implicit assumption of independence in local pixel neighborhoods. We relax this assumption by modeling correlations between pixels in the submodular MRF framework. We show how both the learning and inference tasks can be simply and efficiently implemented-exact inference via an efficient max flow computation; and learning, via an averaged-subgradient method. Unlike most comparable MRF-based approaches, our method is suitable for implementation on a robot in real-time. Experimental results are shown that demonstrate a marked increase in classification accuracy over standard methods in addition to real-time performance."""	autonomous robot;computation;discriminative model;markov chain;markov random field;maximum flow problem;pixel;real-time clock;statistical classification;structured prediction;subgradient method;submodular set function	Paul Vernaza;Ben Taskar;Daniel D. Lee	2008	2008 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2008.4543627	robot;mobile robot;stochastic process;computer vision;contextual image classification;computer science;artificial intelligence;subgradient method;machine learning;pattern recognition;structured prediction;image segmentation;markov process	Robotics	36.80907210974506	-44.5378685332569	143060
d121367211fe378b85e691b1c3b0cfa7f12ee0ee	kernel-based improved discriminant analysis and its application to face recognition	kernel discriminant analysis;nonlinear mapping;high dimensionality;small sample size;kernel discriminant analysis kda;feature space;discriminant analysis;face recognition;small sample size sss;feature extraction;classification error;scattering matrix;kernel method	Kernel discriminant analysis (KDA) is a widely used tool in feature extraction community. However, for high-dimensional multi-class tasks such as face recognition, traditional KDA algorithms have the limitation that the Fisher criterion is nonoptimal with respect to classification rate. Moreover, they suffer from the small sample size problem. This paper presents a variant of KDA called kernel-based improved discriminant analysis (KIDA), which can effectively deal with the above two problems. In the proposed framework, origin samples are projected firstly into a feature space by an implicit nonlinear mapping. After reconstructing between-class scatter matrix in the feature space by weighted schemes, the kernel method is used to obtain a modified Fisher criterion directly related to classification error. Finally, simultaneous diagonalization technique is employed to find lower-dimensional nonlinear features with significant discriminant power. Experiments on face recognition task show that the proposed method is superior to the traditional KDA and LDA.	algorithm;experiment;facial recognition system;feature extraction;feature vector;iterative method;kernel (operating system);kernel method;linear discriminant analysis;local-density approximation;mathematical optimization;nonlinear system;randomness extractor;regular expression	Dake Zhou;Zhenmin Tang	2010	Soft Comput.	10.1007/s00500-009-0443-z	facial recognition system;s-matrix;kernel method;kernel fisher discriminant analysis;feature vector;feature extraction;kernel principal component analysis;computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis;k-nearest neighbors algorithm;variable kernel density estimation;multiple discriminant analysis;statistics	AI	24.70942624990915	-40.59135044675186	143637
1ceb314051362873f8b5aabfba4a00185f847702	multidimensional partitioning and bi-partitioning: analysis and application to gene expression data sets	linear algebra;mathematics;data mining dimension reduction;dimension reduction;singular value decomposition;large data sets;gene expression data;data mining;tumour classification;tumour classication;graphlaplacian;variational approach;singular vector;feature selection;microarray;fiedler vector;graph laplacian;high throughput;cell biology;eigenvectors	Eigenvectors and, more generally, singular vectors, have proved to be useful tools for data mining and dimension reduction. Spectral clustering and reordering algorithms have been designed and implemented in many disciplines, and they can be motivated from several different standpoints. Here we give a general, unified, derivation from an applied linear algebra perspective. We use a variational approach that has the benefit of (a) naturally introducing an appropriate scaling, (b) allowing for a solution in any desired dimension, and (c) dealing with both the clustering and bi-clustering issues in the same framework. The motivation and analysis is then backed up with examples involving two large data sets from modern, high-throughput, experimental cell biology. Here, the objects of interest are genes and tissue samples, and the experimental data represents gene activity. We show that looking beyond the dominant, or Fiedler, direction reveals important information.	algorithm;backup;binary space partitioning;cluster analysis;data mining;dimensionality reduction;emoticon;high-throughput computing;image scaling;linear algebra;singular value decomposition;spectral clustering;throughput;variational principle	Gabriela Kalna;J. Keith Vass;Desmond J. Higham	2008	Int. J. Comput. Math.	10.1080/00207160701210158	algebraic connectivity;high-throughput screening;mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;laplacian matrix;eigenvalues and eigenvectors;linear algebra;microarray;mathematics;singular value decomposition;feature selection;statistics;dimensionality reduction;algebra	ML	27.092984005766297	-38.07990831486306	143894
54b107501d55a7ddf67d1074bd7e63c109aab142	support vector machine-based classification of rock texture images aided by efficient feature selection	support vector machines;image classification;feature pattern dimensionality support vector machine based classification rock texture image classification feature selection techniques unsupervised methods data reliability information gain ranking conventional classifiers;image texture;feature extraction reliability rocks image color analysis support vector machines training histograms;geology;support vector machines geology image classification image texture rocks;dk atira pure researchoutput researchoutputtypes contributiontobookanthology conference;rocks	This paper presents a study on rock texture image classification using support vector machines (and also K-nearest neighbours and decision trees) with the aid of feature selection techniques. It offers both unsupervised and supervised methods for feature selection, based on data reliability and information gain ranking respectively. Following this approach, the conventional classifiers which are sensitive to the dimensionality of feature patterns, become effective on classification of images whose pattern representation may otherwise involve a large number of features. The work is successfully applied to complex images. Classifiers built using features selected by either of these methods generally outperform their counterparts that employ the full set of original features which has a dimensionality several folds higher than that of the selected feature subset. This is confirmed by systematic experimental investigations. This study therefore, helps to accomplish challenging image classification tasks effectively and efficiently. In particular, the approach retains the underlying semantics of a selected feature subset. This is very important to ensure that the classification results are understandable by the user.	artificial neural network;computer vision;data redundancy;decision tree;dimensionality reduction;feature selection;feedforward neural network;information gain in decision trees;k-nearest neighbors algorithm;kullback–leibler divergence;multilayer perceptron;replication (computing);supervised learning;support vector machine;unsupervised learning	Changjing Shang;Dave Barnes	2012	The 2012 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2012.6252634	image texture;support vector machine;contextual image classification;feature detection;feature vector;feature extraction;computer science;machine learning;linear classifier;pattern recognition;data mining;feature;dimensionality reduction	ML	30.51368793268903	-43.49937901163499	144095
9abb5697cb9f6804bbf46e7a0eae70b81d32e4af	maximum a posteriori probability estimation for online surveillance video synopsis	video surveillance maximum likelihood estimation video signal processing video streaming;video summarization maximum a posteriori probability estimation problem online surveillance video synopsis human effort reduction long surveillance video browsing video tubes synopsis video generation problem map estimation problem computational complexity reduction online streaming videos synopsis table;streaming media surveillance predictive models real time systems estimation indexes optimization	To reduce human efforts in browsing long surveillance videos, synopsis videos are proposed. Traditional synopsis video generation applying optimization on video tubes is very time consuming and infeasible for real-time online generation. This dilemma significantly reduces the feasibility of synopsis video generation in practical situations. To solve this problem, the synopsis video generation problem is formulated as a maximum a posteriori probability (MAP) estimation problem in this paper, where the positions and appearing frames of video objects are chronologically rearranged in real time without the need to know their complete trajectories. Moreover, a synopsis table is employed with MAP estimation to decide the temporal locations of the incoming foreground objects in the synopsis video without needing an optimization procedure. As a result, the computational complexity of the proposed video synopsis generation method can be significantly reduced. Furthermore, as it does not require prescreening the entire video, this approach can be applied on online streaming videos.	closed-circuit television;computational complexity theory;mathematical optimization;need to know;preprocessor;real-time clock;real-time locating system;self-organizing map;streaming media;video synopsis	Chun-Rong Huang;Pau-Choo Chung;Di-Kai Yang;Hsing-Cheng Chen;Guan-Jie Huang	2014	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2014.2308603	computer vision;computer science;video tracking;pattern recognition;multimedia	Vision	38.096762164156445	-42.44863294698598	144395
ff690e0529bf75df6b05d1f99005d32a21787f8c	a residual encoder-decoder network for semantic segmentation in autonomous driving scenarios		In this paper, we propose an encoder-decoder based deep convolutional network for semantic segmentation in autonomous driving scenarios. The architecture of the proposed model is based on VGG16 [1]. Residual learning is introduced to preserve the context while decreasing the size of feature maps between the stacks of convolutional layers. Also, the resolution is preserved through shortcuts from the encoder stage to the decoder stage. Experiments are conducted on popular benchmark datasets CamVid and CityScapes to demonstrate the efficacy of the proposed model. The experiments are corroborated with comparative analysis with popular encoder-decoder networks such as SegNet and Enet architectures demonstrating that the proposed approach outperforms existing methods despite having fewer trainable parameters.	ablation;architecture as topic;autonomous car;benchmark (computing);biologic preservation;decoder device component;embedded system;embedding;encoder device component;experiment;international standard book number;map;pixel;qualitative comparative analysis;anatomical layer;biologic segmentation	Y. G. Naresh;Suzanne Little;Noel E. O&#x0027;Connor	2018	2018 26th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2018.8553161	residual;image processing;encoder;decoding methods;stack (abstract data type);architecture;image segmentation;machine learning;convolution;artificial intelligence;computer science	Vision	25.31256887645066	-51.99308923804838	144425
2d244d70ed1a2ba03d152189f1f90ff2b4f16a79	an analytical mapping for lle and its application in multi-pose face synthesis		Locally Linear Embedding (LLE) is a nonlinear dimensionali ty reduction method proposed recently. It can reveal the intrinsic manif old of data, which can’t be provided by classical linear dimensionality reduc tion methods. The application of LLE, however, is limited because of its lack o f a mapping between the observation and the low-dimensional output. In his paper, we propose a method to establish an analytical mapping for LLE a nd validate its efficiency with the application in multi-pose face synth esis. Furthermore, through learning the similarity for the same kind of pose cha nge mode of different persons, we generalize our method to small set cas s with methods of statistical learning theory. The experiments of multi-p ose face synthesis on small sets prove that our idea and method are correct.	algorithm;bounce message;embedded system;enea ose;european space information system;experiment;facial recognition system;graph coloring;machine learning;nonlinear dimensionality reduction;nonlinear system;source data;statistical learning theory;unified model	Changshui Zhang;Zhongbao Kou	2003		10.5244/C.17.1	computer science;artificial intelligence;machine learning;pattern recognition	ML	26.130118752867254	-42.16946887730097	144597
63fd7a159e58add133b9c71c4b1b37b899dd646f	exemplar-based human action pose correction	computer vision product;joint based skeleton correction;detection system exemplar based human action pose correction xbox kinect computer vision product gaming industry action recognition human pose estimation depth image pose estimation system occlusion exemplar based method inhomogeneous systematic bias exemplar information human action domain pose tags joint based skeleton correction tag prediction contemporary approach facial landmark correction;facial landmark correction;estimation theory;occlusion;depth image;systematics;inhomogeneous systematic bias;training;exemplar based method;xbox kinect;joints;pose estimation system;skeleton;human action domain;kinect;vegetation;contemporary approach;computer vision;joints estimation training vegetation cameras systematics;face recognition;tag prediction;estimation;pose estimation computer vision estimation theory face recognition gesture recognition;action recognition;exemplar based human action pose correction;random forest;pose tag;gaming industry;exemplar information;pose correction;skeleton kinect pose correction pose tag random forest;gesture recognition;cameras;detection system;human pose estimation;pose estimation;pose tags	The launch of Xbox Kinect has built a very successful computer vision product and made a big impact on the gaming industry. This sheds lights onto a wide variety of potential applications related to action recognition. The accurate estimation of human poses from the depth image is universally a critical step. However, existing pose estimation systems exhibit failures when facing severe occlusion. In this paper, we propose an exemplar-based method to learn to correct the initially estimated poses. We learn an inhomogeneous systematic bias by leveraging the exemplar information within a specific human action domain. Furthermore, as an extension, we learn a conditional model by incorporation of pose tags to further increase the accuracy of pose correction. In the experiments, significant improvements on both joint-based skeleton correction and tag prediction are observed over the contemporary approaches, including what is delivered by the current Kinect system. Our experiments for the facial landmark correction also illustrate that our algorithm can improve the accuracy of other detection/estimation systems.	algorithm;baseline (configuration management);bias–variance tradeoff;computer vision;discriminative model;experiment;ground truth;kinect;mandibular right second molar tooth;natural science disciplines;pokémon red;population parameter;skeleton;tag cloud;vii	Wei Shen;Ke Deng;Xiang Bai;Tommer Leyvand;Baining Guo;Zhuowen Tu	2014	IEEE Transactions on Cybernetics	10.1109/TCYB.2013.2279071	random forest;computer vision;estimation;simulation;pose;3d pose estimation;computer science;gesture recognition;articulated body pose estimation;systematics;estimation theory;skeleton;vegetation;computer graphics (images)	Vision	35.37438613896748	-46.928572475095784	144758
f51b4e576dc13ad119b0a2cd09baf9bac4793d1e	normalized cheeger cut with neighborhood rough approximation		A graph-based collecting study recently attracted significant attention from the scientific research community. Normalized Cheeger cut is a balanced graph partition criterion and a generalized version of normalized graph cut. A stress-free resolution of the normalized Cheeger cut can be obtained by employing the eigenvectors of curve p-Laplacian. However, it is highly sensitive for the original Cheeger cut to collect the interference of noise and disrelated properties. Thus, the performance of the Cheeger cut decreases when high-dimensional data are grouped. To decrease the negative influence of outliers and superfluous properties of collecting, we design an efficient attribute decrease method which is based on neighborhood rough approximation. This design aims to improve the collection of the Cheeger cut. The suggested algorithm introduces information entropy to the neighborhood rough sets in order to measure the importance of attributes. This algorithm reserves the most valuable features and removes the redundant features while retaining the maximum category information of raw data. We then build the p-Laplacian array with the optimized attribute sets and obtain the collecting consequences via the eigen-subspace decomposition of graph p-Laplacian. The cogency of the proposed algorithm is established in various standard data collections. Experimentations demonstrated that our method enjoys sturdy robustness to noise or disrelated feature information in high-dimensional figures.	algorithm;approximation;cut (graph theory);eigen (c++ library);entropy (information theory);graph cuts in computer vision;graph partition;interference (communication);rough set	Lin Li;Jianhua Yue	2018	IEEE Access	10.1109/ACCESS.2018.2823423	raw data;mathematical optimization;entropy (information theory);approximation algorithm;distributed computing;image segmentation;graph partition;cut;linear programming;computer science;rough set	ML	25.866969238563836	-40.161440317859636	144815
18ab703c9959fbea7ad253a4062eb705b245552c	efficient trajectory extraction and parameter learning for data-driven crowd simulation	crowd simulation;multiple people tracking;data driven	We present a trajectory extraction and behavior-learning algorithm for data-driven crowd simulation. Our formulation is based on incrementally learning pedestrian motion models and behaviors from crowd videos. We combine this learned crowd-simulation model with an online tracker based on particle filtering to compute accurate, smooth pedestrian trajectories. We refine this motion model using an optimization technique to estimate the agents’ simulation parameters. We highlight the benefits of our approach for improved data-driven crowd simulation, including crowd replication from videos and merging the behavior of pedestrians from multiple videos. We highlight our algorithm’s performance in various test scenarios containing tens of human-like agents.	algorithm;computation;crowd simulation;crowdsourcing;data parallelism;ibm notes;mathematical optimization;multi-agent system;parallel computing;particle filter;streaming media;video	Aniket Bera;Sunyoung Kim;Dinesh Manocha	2015			computer vision;simulation;computer science;artificial intelligence;crowd simulation;multimedia	Vision	39.03717347608682	-40.459267804363755	144932
2073879f796cb2dbc5a3394eb43efa0aa92fef03	view-invariant human detection from rgb-d data of kinect using continuous hidden markov model		In this paper authors have presented a method to detect human from a Kinect captured Gray-Depth (G-D) using Continuous Hidden Markov models (C-HMMs). In our proposed approach, we initially generate multiple gray scale images from a single gray scale image/ video frame based on their depth connectivity. Thus, we initially segment the G image using depth information and then relevant components were extracted. These components were further filtered out and features were extracted from the candidate components only. Here a robust feature named Local gradients histogram(LGH) is used to detect human from G-D video. We have evaluated our system against the data set published by LIRIS in ICPR 2012 and on our own data set captured in our lab. We have observed that our proposed method can detect human from this data-set with a 94.25% accuracy.	hidden markov model;kinect;markov chain	Sangheeta Roy;Tanushyam Chattopadhyay	2014		10.1007/978-3-319-07230-2_32	speech recognition;machine learning;pattern recognition	Robotics	37.83070145237849	-49.137626549690374	144962
860a86ccdaaa3e77281a038eab766faa69763634	view-invariant representation and learning of human action	image recognition;video signal processing;image matching;humans video sequences image motion analysis computer vision spatiotemporal phenomena legged locomotion hidden markov models computer science data mining aerodynamics;image representation;action recognition;image matching human action learning video sequences visual information extraction video sequence view invariant representation visual information interpretation dynamic instants dynamic intervals spatiotemporal trajectory curvature video segmentation video clips;image matching image representation image sequences image recognition video signal processing;image sequences	Automatically understanding human actions from video sequences is a very challenging problem. This involves the extraction of relevant visual information from a video sequence, representation of that information in a suitable form, and interpretation of visual information for the purpose of recognition and learning. In this paper, we first present a view-invariant representation of action consisting of dynamic instants and intervals, which is computed using spatiotemporal curvature of a trajectory. This representation is then used by our system to learn human actions without any training. The system automatically segments video into individual actions, and computes view invariant representation for each action. The system is able to incrementally learn different actions starting with no model. It is able to discover different instances of the same action performed by different people, and in different viewpoints. In order to validate our approach, we present results on video clips in which roughly 50 actions were performed by five different people in different viewpoints. Our system performed impressively by correctly interpreting most actions.	video clip	Cen Rao;Mubarak Shah	2001		10.1109/EVENT.2001.938867	computer vision;image processing;computer science;machine learning;video tracking;pattern recognition;human visual system model	Vision	38.459605204631856	-48.40193038364409	145142
e255ca660def135eb6ac99127d08df032876d8ab	bidirectional ranking for person re-identification	image recognition;complexity theory;surveillance;person re identification;bidirectional ranking;probes;visualization;image retrieval image recognition;image color analysis;context similarity person reidentification nonoverlapping cameras gallery image querying content similarities context similarities bidirectional ranking lists latent assumption content similarity k nearest neighbors;probes context cameras complexity theory visualization surveillance image color analysis;re ranking person re identification bidirectional ranking content and context similarities;re ranking;context;content and context similarities;cameras;image retrieval	This paper proposes a simple but efficient bidirectional ranking method to improve person re-identification results across non-overlapping cameras. Previous methods treat person reidentification as a special object retrieval problem, and compute the final rank result purely based on a unidirectional matching between the probe and all gallery images. However, the expected person image may be excluded from the probe's ??-nearest neighbor due to appearance changes caused by variations in illuminations, poses, viewpoints and occlusion. To solve the above problem, our method queries every gallery image in a new gallery composed of the original probe image and other gallery images, and revises the initial query result in accordance with both content and context similarities between bidirectional ranking lists. A latent assumption of our method is that images of the same person should not only have similar visual content, known as content similarity, but also possess similar k-nearest neighbors, known as context similarity. Extensive experiments conducted on a series of standard data sets have validated the effectiveness of our proposed method with an average improvement of 5-10% over original baseline methods.	baseline (configuration management);experiment;hidden surface determination;k-nearest neighbors algorithm	Qingming Leng;Ruimin Hu;Chao Liang;Yimin Wang;Jun Chen	2013	2013 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2013.6607577	computer vision;visualization;image retrieval;computer science;pattern recognition;mathematics;information retrieval	Vision	33.93542254747234	-49.10687153636713	145199
73e3acf30ecce3771557aa083d5a83308049582b	two-stream deep encoder-decoder architecture for fully automatic video object segmentation		We propose a two-stream Deep Encoder-Decoder architecture to tackle the task of fully automatic video object segmentation. Both two streams, i.e., ImSeg-Stream (for static image segmentation) and MoSeg-Stream (for optical flow segmentation), hold the totally same Encoder-Decoder architecture. The Encoder part generates a low-resolution mask with accurate locations and smooth boundaries, while the Decoder part refines the details of initial mask and enlarges its resolution via integrating lower-level features progressively. At last two streams learn to integrate for better results. Moreover, to handle the problem of inadequate video object segmentation datasets, we propose a seeking strategy to generate a large-scale handcrafted dataset for training. Experiments on two standard datasets demonstrate that proposed method outperforms most state-of-the-art methods in both segmentation accuracy and run time.	codec;encoder;experiment;image segmentation;online and offline;optical flow;run time (program lifecycle phase)	Jingwei Xu;Li Song;Rong Xie	2017	2017 IEEE Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2017.8305089	computer vision;theoretical computer science;encoder;artificial intelligence;architecture;streams;computer science;image segmentation;segmentation;optical flow	Vision	25.791542355341797	-51.79105250433235	145363
0a24c7814f11de216d64c9d4a6f6708500e5ab45	real-time detection of unusual regions in image streams	model specification;video streaming;real time;pan tilt zoom;unusualness detection;psi_visics;event detection;large scale;incremental learning;traffic monitoring;video;online;active control	Automatic and real-time identification of unusual incidents is important for event detection and alarm systems. In today's camera surveillance solutions video streams are displayed on-screen for human operators, e.g. in large multi-screen control centers. This in turn requires the attention of operators for unusual events and urgent response.  This paper presents a method for the automatic identification of unusual visual content in video streams real-time. In contrast to explicitly modeling specific unusual events, the proposed approach incrementally learns the usual appearances from the visual source and simultaneously identifies potential unusual image regions in the scene. Experiments demonstrate the general applicability on a variety of large-scale datasets including different scenes from public web cams and from traffic monitoring. To further demonstrate the real-time capabilities of the unusual scene detection we actively control a Pan-Tilt-Zoom camera to get close up views of the unusual incidents.	automatic identification and data capture;experiment;pan–tilt–zoom camera;real-time clock;real-time locating system;streaming media;webcam;website monitoring	René Schuster;Roland Mörzinger;Werner Haas;Helmut Grabner;Luc Van Gool	2010		10.1145/1873951.1874208	computer vision;real-time computing;simulation;video;computer science;specification	Vision	38.637884374300825	-44.911371118489924	145494
25c6b443fe4dd843b126f026941a57c95e7cda5b	partial multi-view outlier detection based on collective learning			anomaly detection	Jun Guo;Wenwu Zhu	2018			machine learning;computer science;anomaly detection;artificial intelligence;collaborative learning	AI	34.14982139381928	-42.55061045718062	146174
e9e04b36f88199a492db6ce7ffdb4e76dc7cc966	an enhanced deep convolutional neural network for person re-identification		Traditional hand-crafted feature and Convolutional Neural Network (CNN) feature are two different types of image descriptors in person re-identification task. In this work, we propose a novel network called Enhanced CNN. We take advantage of the complementarity between hand-crafted features and CNN features by making hand-crafted features participate in the construction of our CNN model and the generation of the final features. A feature reconstruction block (FRB) is designed to jointly map hand-crafted features and CNN features to a unitary feature space and fuse them together in a learned way. We adopt identification loss and verification loss simultaneously to supervise the training process of our model, in order to make fully usage of the information in re-id labels. Extensive experiments demonstrate the superiority of our model over a wide range of state-of-the-art methods on four re-id datasets (Market-1501, CUHK03, CUHKOI, VIPeR).	complementarity theory;convolutional neural network;experiment;feature vector;fuse device component;verification of theories;visual descriptor	Tiansheng Guo;Dongfei Wang;Zhuqing Jiang;Aidong Men;Yun Zhou	2018	2018 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2018.8551570		Vision	26.19092218569568	-50.07455751843084	146350
364c79d2d98819b27641c651cf6553142ef747bf	hedging your bets: optimizing accuracy-specificity trade-offs in large scale visual recognition	animals;object recognition;dual accuracy reward trade off search algorithm;darts;training;prediction algorithms;semantics;image classification;accuracy visualization semantics animals materials training prediction algorithms;accuracy specificity trade off optimization;materials;search problems image classification object recognition;darts accuracy specificity trade off optimization large scale visual recognition object category semantic hierarchy classifier information gain maximization dual accuracy reward trade off search algorithm;accuracy;visualization;classifier;large scale visual recognition;semantic hierarchy;information gain maximization;search problems;object category	As visual recognition scales up to ever larger numbers of categories, maintaining high accuracy is increasingly difficult. In this work, we study the problem of optimizing accuracy-specificity trade-offs in large scale recognition, motivated by the observation that object categories form a semantic hierarchy consisting of many levels of abstraction. A classifier can select the appropriate level, trading off specificity for accuracy in case of uncertainty. By optimizing this trade-off, we obtain classifiers that try to be as specific as possible while guaranteeing an arbitrarily high accuracy. We formulate the problem as maximizing information gain while ensuring a fixed, arbitrarily small error rate with a semantic hierarchy. We propose the Dual Accuracy Reward Trade-off Search (DARTS) algorithm and prove that, under practical conditions, it converges to an optimal solution. Experiments demonstrate the effectiveness of our algorithm on datasets ranging from 65 to over 10,000 categories.	experiment;genetic algorithm;information gain in decision trees;kullback–leibler divergence;optimizing compiler;principle of abstraction;sensitivity and specificity	Jia Deng;Jonathan Krause;Alexander C. Berg;Li Fei-Fei	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6248086	computer vision;contextual image classification;visualization;prediction;classifier;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;data mining;mathematics;semantics;accuracy and precision;statistics	Vision	26.758838674504055	-47.33686328222287	146412
554c623c77726ff07efb32ebe218d94877eece20	unsupervised multiclass region cosegmentation via ensemble clustering and energy minimization	unsupervised learning;α expansion unsupervised multiclass region cosegmentation energy minimization object like proposals robust ensemble clustering scheme image groups;pattern clustering;graphcut co segmentation multi class regions;image segmentation;unsupervised learning image segmentation pattern clustering;proposals image segmentation image color analysis shape histograms clustering algorithms minimization	The problem of unsupervised segmentation of multi-class regions can be significantly boosted when they irregularly recur in multiple images. The existing segmentation methods are either weakly supervised, such as tagging images with object classes, or are limited by the assumption that each image contains all the object instances. In this paper, we propose a new method to cosegment multiclass regions from a group of images without the assumption about object configurations. The key idea is to discover the unknown object-like proposals via a robust ensemble clustering scheme. The proposals are then used to derive unary and pairwise energy potentials across all the images, which can be minimized with the α-expansion. Experimental evaluation on a number of image groups demonstrates the good performance of the proposed method on the multiclass region cosegmentation.	cluster analysis;energy minimization;instance (computer science);supervised learning;unary operation;unsupervised learning	Hongliang Li;Fanman Meng;Qingbo Wu;Bing Luo	2014	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2013.2280851	unsupervised learning;computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation	Vision	27.39752079991983	-46.623855049371784	146497
122d2a295024b757684e63dd54635632ce89598c	automatic design of nonlinear filters by nearest neighbor learning	nonlinear filters;image processing;nn searching automatic design nonlinear filters nearest neighbor learning image processing noise elimination digital filter automatic filter construction kd tree k dimensional binary tree texture recognition;nonlinear filter;digital filter;image texture;input output;digital filters;nearest neighbor;kd tree;genetic algorithm;nonlinear filters nearest neighbor searches neural networks acceleration image processing gray scale digital filters computer networks genetics binary trees;search problems;image texture nonlinear filters digital filters search problems;statistical estimation;neural network;binary tree	Nonlinear filters have been used not only in the noise elimination but in a wide variety of image processing applications. Traditionally the design of a digital filter is a manual task and the user accomplishes it based on his previous experiences. Unfortunately often this is not a trivial task. Thus some recent works try to overcome this difficulty constructing the filters automatically by computational learning, neural networks, genetic algorithms and statistical estimation. These works use typical input-output images of the application as the training samples. Many different kinds of filters can be easily constructed using this approach. This paper proposes the use of the nearest neighbor (NN) learning to the automatic filter construction. The kd-tree (k-dimensional binary tree) is used to accelerate the NN searching. A texture recognition application example is depicted.	artificial neural network;binary tree;computer vision;digital filter;estimation theory;experience;genetic algorithm;grayscale;image processing;nonlinear programming;nonlinear system;window function	Hae Yong Kim;Flávio A. M. Cipparrone	1998		10.1109/ICIP.1998.723638	computer vision;digital filter;image processing;computer science;machine learning;pattern recognition	Vision	33.17264494523597	-40.00640565970875	146703
2b9476545a6696159666614b17064f63aa3fd487	algebraic variety models for high-rank matrix completion		We consider a generalization of low-rank matrix completion to the case where the data belongs to an algebraic variety, i.e., each data point is a solution to a system of polynomial equations. In this case the original matrix is possibly high-rank, but it becomes low-rank after mapping each column to a higher dimensional space of monomial features. Many well-studied extensions of linear models, including a ne subspaces and their union, can be described by a variety model. In addition, varieties can be used to model a richer class of nonlinear quadratic and higher degree curves and surfaces. We study the sampling requirements for matrix completion under a variety model with a focus on a union of a ne subspaces. We also propose an e cient matrix completion algorithm that minimizes a convex or non-convex surrogate of the rank of the matrix of monomial features. Our algorithm uses the wellknown “kernel trick” to avoid working directly with the high-dimensional monomial matrix. We show the proposed algorithm is able to recover synthetically generated data up to the predicted sampling complexity bounds. The proposed algorithm also outperforms standard low rank matrix completion and subspace clustering techniques in experiments with real data.	algebraic equation;algorithm;cluster analysis;clustering high-dimensional data;column (database);data model;data point;experiment;feature vector;iterative method;kernel method;least squares;linear algebra;linear model;monomial;ne (complexity);nonlinear system;requirement;sampling (signal processing);schedule (computer science);synthetic data;system of polynomial equations;the matrix	Greg Ongie;Rebecca M Willett;Robert D. Nowak;Laura Balzano	2017			discrete mathematics;mathematical optimization;monomial;mathematics;matrix completion;algebraic variety;matrix (mathematics);low-rank approximation;rank (linear algebra);affine transformation;dimension of an algebraic variety	ML	26.46004104503483	-39.053327957097835	146709
ca6eb707a1a15468234ba9195fc0dfcfa0b26524	spatio-temporal pyramid cuboid matching for action recognition using depth maps	cuboid fusion action recognition pmht stpcm;spatiotemporal phenomena image colour analysis image matching image motion analysis;shape information spatio temporal pyramid cuboid matching action recognition depth maps stpcm depth cameras pyramid motion history templates pmht multiscale 3d motion information;three dimensional displays history shape feature extraction video sequences cameras solid modeling	This paper presents a novel framework, spatio-temporal pyramid cuboid matching (STPCM), which is designed to recognize human actions from sequences captured by depth cameras. A depth sequence is partitioned into sub-volumes and represented using pyramid motion history templates (PMHT), which maintain the multi-scale 3D motion and shape information along the temporal direction. In order to capture the spatial information of PMHT, each projected plane from PMHT is subdivided into pyramid spatio-temporal grids. We then propose a novel cuboid fusion scheme to combine spatial dependent grids from projected planes to construct pyramid cuboids that consider the 3D spatial locations in conjunction with temporal information. In the experiments, we evaluate the proposed framework on three public benchmark datasets. Experimental results demonstrate that the proposed method achieves state-of-the-art performance.	benchmark (computing);cuboid;depth map;experiment	Bin Liang;Lihong Zheng	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351165	computer vision;pattern recognition;computer graphics (images)	Vision	37.652730273405005	-50.58863130292765	146820
6d6e796c5dc641c9bbad90be07c8325d05e37709	a robust regularization path for the doubly regularized support vector machine		The Doubly Regularized SVM (DrSVM) is an extension of SVM using a mixture of L2 and L1 norm penalties. This kind of penalty, sometimes referred as the elastic net, allows to perform variable selection while taking into account correlations between variables. Introduced by Wang [1], an e cient algorithm to compute the whole DrSVM solution path has been proposed. Unfortunately, in some cases, this path is discontinuous, and thus not piecewise linear. To solve this problem, we propose here a new sub gradient formulation of the DrSVM problem. This led us to propose an alternative L1 regularization path algorithm. This reformulation e ciently addresses the aforementioned problem and makes the initialization step more generic. The results show the validity of our sub-gradient formulation and the e ciency compared to the initial formulation.	algorithm;early stopping;elastic net regularization;feature selection;gradient;matrix regularization;piecewise linear continuation;relevance;sparse matrix;support vector machine;t-norm;taxicab geometry	Antoine Lachaud;Stéphane Canu;David Mercier;Frédéric Suard	2014			machine learning;artificial intelligence;support vector machine;feature selection;elastic net regularization;regularization perspectives on support vector machines;piecewise linear function;mathematical optimization;computer science;regularization (mathematics);initialization	ML	24.740168053581165	-39.24499722499466	146907
5f68e2131d9275d56092e9fca05bcfc65abea0d8	cross-modal similarity learning: a low rank bilinear formulation	multimedia retrieval;cross modality;nuclear norm;similarity learning;accelerated proximal gradient	The cross-media retrieval problem has received much attention in recent years due to the rapid increasing of multimedia data on the Internet. A new approach to the problem has been raised which intends to match features of different modalities directly. In this research, there are two critical issues: how to get rid of the heterogeneity between different modalities and how to match the cross-modal features of different dimensions. Recently metric learning methods show a good capability in learning a distance metric to explore the relationship between data points. However, the traditional metric learning algorithms only focus on single-modal features, which suffer difficulties in addressing the cross-modal features of different dimensions. In this paper, we propose a cross-modal similarity learning algorithm for the cross-modal feature matching. The proposed method takes a bilinear formulation, and with the nuclear-norm penalization, it achieves low-rank representation. Accordingly, the accelerated proximal gradient algorithm is successfully imported to find the optimal solution with a fast convergence rate O(1/t2). Experiments on three well known image-text cross-media retrieval databases show that the proposed method achieves the best performance compared to the state-of-the-art algorithms.	algorithm;bilinear filtering;bilinear transform;data point;database;internet;machine learning;modal logic;proximal gradient methods for learning;rate of convergence;similarity learning	Cuicui Kang;Shengcai Liao;Yonghao He;Jian Wang;Wenjia Niu;Shiming Xiang;Chunhong Pan	2015		10.1145/2806416.2806469	computer vision;mathematical optimization;matrix norm;machine learning;pattern recognition;mathematics;algorithm	AI	24.844459895366192	-41.72741663217549	147054
412ac23dd2b180dda430c8186a9bc0c8d835e676	a joint evaluation of different dimensionality reduction techniques, fusion and learning methods for action recognition	different fusion methods;video signal processing;different dimensionality reduction;human action recognition;different machine learning methods	This paper addresses the problem of action recognition with improved dense trajectories (IDT). Recently, IDT achieved a significant performance in action recognition with realistic videos. However, the efficiency of storage and the speed of classification are limited due to the dense samples in feature space. To address this issue, the intuitive way is to reduce the dimension and adopt a fast classification method. Therefore, we explore the influence of dimensionality reduction on the recognition rate. In addition, Extreme Learning Machine (ELM) is adopted to further improve classification efficiency. We present performance on the KTH, UCF11, HMDB51, and UCF101 datasets in all kinds of situations such as the different fusion methods, the different dimensionality reduction, and different learning methods. As a result, it can be observed that ELM with principal components analysis (PCA) improves the performance in terms of mean average precision (mAP) which not only significantly reduces computational cost but improves accuracy. What’s more, the training and testing time decrease 1—2 orders of magnitude without losing accuracy when Fisher vector (FV) adopts reduction techniques before it fed into classifier.	algorithmic efficiency;computation;confusion matrix;dimensionality reduction;elm;farmville;feature vector;heat map;information retrieval;interrupt descriptor table;local-density approximation;principal component analysis	Haiyan Xu;Qian Tian;Zhen Wang;Jianhui Wu	2016	Neurocomputing	10.1016/j.neucom.2016.06.017	computer vision;feature extraction;computer science;machine learning;pattern recognition;dimensionality reduction	AI	35.249622661045514	-50.87560769011168	147158
8052bc5f9beb389b3144d423e7b5d6fcf5d0cc4f	adapting attributes by selecting features similar across domains	animals;support vector machines feature selection prediction theory;support vector machines;training;support vector machine semantic visual properties object recognition content based image search attribute model source domain target domain feature subspace feature selection data distributions auxiliary domain adaptive svm attribute prediction;semantics;adaptation models data models support vector machines semantics visualization animals training;visualization;adaptation models;data models	"""Attributes are semantic visual properties shared by objects. They have been shown to improve object recognition and to enhance content-based image search. While attributes are expected to cover multiple categories, e.g. a dalmatian and a whale can both have """"smooth skin"""", we find that the appearance of a single attribute varies quite a bit across categories. Thus, an attribute model learned on one category may not be usable on another category. We show how to adapt attribute models towards new categories. We ensure that positive transfer can occur between a source domain of categories and a novel target domain, by learning in a feature subspace found by feature selection where the data distributions of the domains are similar. We demonstrate that when data from the novel domain is limited, regularizing attribute models for that novel domain with models trained on an auxiliary domain (via Adaptive SVM) improves the accuracy of attribute prediction."""	consistency model;feature selection;image retrieval;outline of object recognition	Siqi Liu;Adriana Kovashka	2016	2016 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2016.7477731	data modeling;support vector machine;visualization;attribute domain;computer science;machine learning;pattern recognition;data mining;semantics	Vision	24.93443989964463	-46.579375347875555	147429
5ad4b2d6c8253aeb8fc4ad39bcb459a1f2ca10e6	leveraging stereo matching with learning-based confidence measures	modulation vegetation robustness prediction algorithms training data accuracy training;outdoor datasets learning based confidence measures supervised learning based confidence prediction regression forest framework confidence based matching cost modulation scheme stereo matching algorithms semiglobal matching algorithm confidence measure selection cost modulation methods kitti;stereo image processing image matching learning artificial intelligence regression analysis	We propose a new approach to associate supervised learning-based confidence prediction with the stereo matching problem. First of all, we analyze the characteristics of various confidence measures in the regression forest framework to select effective confidence measures using training data. We then train regression forests again to predict the correctness (confidence) of a match by using selected confidence measures. In addition, we present a confidence-based matching cost modulation scheme based on the predicted correctness for improving the robustness and accuracy of various stereo matching algorithms. We apply the proposed scheme to the semi-global matching algorithm to make it robust under unexpected difficulties that can occur in outdoor environments. We verify the proposed confidence measure selection and cost modulation methods through extensive experimentation with various aspects using KITTI and challenging outdoor datasets.	algorithm;computer stereo vision;correctness (computer science);modulation;semi-supervised learning;semiconductor industry;supervised learning	Min-Gyu Park;Kuk-Jin Yoon	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298605	computer science;machine learning;pattern recognition;data mining	Vision	34.332650835688376	-44.546138471344165	147570
148316962e1ebb7086837e25cbee9ecbd71e5940	efficient multi-person pose estimation with provable guarantees		Multi-person pose estimation (MPPE) in natural images is key to the meaningful use of visual data in many fields including movement science, security, and rehabilitation. In this paper we tackle MPPE with a bottom-up approach, starting with candidate detections of body parts from a convolutional neural network (CNN) and grouping them into people. We formulate the grouping of body part detections into people as a minimum-weight set packing (MWSP) problem where the set of potential people is the power set of body part detections. We model the quality of a hypothesis of a person which is a set in the MWSP by an augmented tree-structured Markov random field where variables correspond to body-parts and their state-spaces correspond to the power set of the detections for that part. We describe a novel algorithm that combines efficiency with provable bounds on this MWSP problem. We employ an implicit column generation strategy where the pricing problem is formulated as a dynamic program. To efficiently solve this dynamic program we exploit the problem structure utilizing a nested Bender’s decomposition (NBD) exact inference strategy which we speed up by recycling Bender’s rows between calls to the pricing problem. We test our approach on the MPII-Multiperson dataset, showing that our approach obtains comparable results with the state-of-the-art algorithm for joint node labeling and grouping problems, and that NBD achieves considerable speed-ups relative to a naive dynamic programming approach. Typical algorithms that solve joint node labeling and grouping problems use heuristics and thus can not obtain proofs of optimality. Our approach, in contrast, proves that for over 99 percent of problem instances we find the globally optimal solution and otherwise provide upper/lower bounds.	3d pose estimation;algorithm;artificial neural network;column generation;convolutional neural network;dynamic programming;heuristic (computer science);markov chain;markov random field;maxima and minima;microsoft point-to-point encryption;network block device;provable prime;provable security;sensor;set packing	Shaofei Wang;Konrad Paul Körding;Julian Yarkony	2017	CoRR		set packing;markov random field;machine learning;column generation;convolutional neural network;computer science;pose;artificial intelligence;dynamic programming;heuristics;algorithm;power set	ML	34.65678004704253	-42.12933447874452	147691
8096d92af2b3fdf6c43769d5aca9082a7958a07d	joint object tracking and segmentation with independent convolutional neural networks		Object tracking and segmentation are important research topics in computer vision. They provide the trajectory and boundary of an object based on their appearance and shape features. Most studies on tracking and segmentation focus on encoding methods for the feature of an object. However, the tracking trajectory and segmentation mask are acquired separately, although similar visual information is required for both methods. Therefore, in this paper, we propose a CNN-based joint object tracking and segmentation framework that provides a segmentation mask while improving the performance of object tacker. In our model, the tracking model determines the trajectory of the target object as a bounding box in each frame. Given the bounding box at each frame, the segmentation model predicts a dense mask of the target object in the bounding box. Then, the segmentation mask is used to refine the bounding box for the tracking model. We evaluate the performance of our algorithm on DAVIS benchmark dataset by AUC score and mean IoU. We showed that the performance of original tracker was improved by our proposed framework.	algorithm;area under curve;avian crop;benchmark (computing);chamaecyparis lawsoniana;computer vision;convolutional neural network;depth perception;minimum bounding box;neural networks;object-based language;segmentation action;silo (dataset);biologic segmentation	Hakjin Lee;Gaurav Kattel;Jongwoo Lim	2018		10.1145/3265987.3265992	convolutional neural network;encoding (memory);video tracking;minimum bounding box;computer vision;masking (art);trajectory;computer science;segmentation;artificial intelligence	Vision	31.62194872767944	-51.1884474833159	147737
8b1ceb97f9f8e6efbe8ec3024f56d46fa569ca66	ein lernfähiger fuzzy-verteilungsschätzer zur adaptiven analyse von radardaten	institut fur hochfrequenztechnik	Presented is a new learning fuzzy distribution estimator (LFVS). It estimates similarities between pattern distributions and the distribution of a signal which is to be analysed. Learning capability gives a high flexibility to the system. Its application is not restricted to SAR data analysis. Compared to conventional estimation algorithms there are two significant advantages of the fuzzy approach. One is that the gradual similarity between density distributions can be established and the other is the estimation of reliability without additional computations. The fuzzy distribution estimator is applied to compress SAR raw data and it is a main modul of a new fuzzy classification system. This system allows a user adaptive classification of SAR image data. In addition to the classificaiton itself, the user gets pixel by pixel classification reliability. Pixels belonging to several classes - a typical problem of low resolution radar images - are well detected.		Ursula C. Benz	1999			computer science;artificial intelligence;data mining;statistics	AI	33.30149987048016	-42.53341943010878	147747
86163c4270fa1173640e7b1f526ffdb482f45f17	contextvp: fully context-aware video prediction		Video prediction models based on convolutional networks, recurrent networks, and their combinations often result in blurry predictions. We identify an important contributing factor for imprecise predictions that has not been studied adequately in the literature: blind spots, i.e., lack of access to all relevant past information for accurately predicting the future. To address this issue, we introduce a fully contextaware architecture that captures the entire available past context for each pixel using Parallel Multi-Dimensional LSTM units and aggregates it using blending units. Our model outperforms a strong baseline network of 20 recurrent convolutional layers and yields state-of-the-art performance for next step prediction on three challenging real-world video datasets: Human 3.6M, Caltech Pedestrian, and UCF-101. Moreover, it does so with fewer parameters than several recently proposed models, and does not rely on deep convolutional networks, multi-scale architectures, separation of background and foreground modeling, motion flow learning, or adversarial training. These results highlight that full awareness of past context is of crucial importance for video prediction.	alpha compositing;baseline (configuration management);convolutional neural network;jumbo frame;long short-term memory;pixel;recurrent neural network;window blind	Wonmin Byeon;Qin Wang;Rupesh Kumar Srivastava;Petros Koumoutsakos	2018		10.1007/978-3-030-01270-0_46	computer science;pixel;artificial intelligence;architecture;predictive modelling;machine learning;pedestrian	ML	27.181519121321827	-51.34544210808999	147832
90298f9f80ebe03cb8b158fd724551ad711d4e71	a pursuit of temporal accuracy in general activity detection		Detecting activities in untrimmed videos is an important but challenging task. The performance of existing methods remains unsatisfactory, e.g. they often meet difficulties in locating the beginning and end of a long complex action. In this paper, we propose a generic framework that can accurately detect a wide variety of activities from untrimmed videos. Our first contribution is a novel proposal scheme that can efficiently generate candidates with accurate temporal boundaries. The other contribution is a cascaded classification pipeline that explicitly distinguishes between relevance and completeness of a candidate instance. On two challenging temporal activity detection datasets, THUMOS14 and ActivityNet, the proposed framework significantly outperforms the existing state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling activities with various temporal structures.	programming paradigm;relevance;video	Yuanjun Xiong;Yue Zhao;Limin Wang;Dahua Lin;Xiaoou Tang	2017	CoRR		artificial intelligence;completeness (statistics);machine learning;computer science;data mining;general activity	Vision	31.75789290374991	-50.17277605727367	147909
362f73a74258135d97309d435ff81598653b4bee	fast prism: branch and bound hough transform for object class detection	psi_visics;gaussian mixture model;spatial pyramid histograms;soft matching;discriminative training;hough transform;implicit shape model;branch and bound;sliding window;object detection;object model	This paper addresses the task of efficient object class detection by means of the Hough transform. This approach has been made popular by the Implicit Shape Model (ISM) and has been adopted many times. Although ISM exhibits robust detection performance, its probabilistic formulation is unsatisfactory. The PRincipled Implicit Shape Model (PRISM) overcomes these problems by interpreting Hough voting as a dual implementation of linear sliding-window detection. It thereby gives a sound justification to the voting procedure and imposes minimal constraints. We demonstrate PRISM’s flexibility by two complementary implementations: a generatively trained Gaussian Mixture Model as well as a discriminatively trained histogram approach. Both systems achieve state-of-the-art performance. Detections are found by gradient-based or branch and bound search, respectively. The latter greatly benefits from PRISM’s feature-centric view. It thereby avoids the unfavourable memory trade-off and any on-line pre-processing of the original Efficient Subwindow Search (ESS). Moreover, our approach takes account of the features’ scale value while ESS does not. Finally, we show how to avoid soft-matching and spatial pyramid descriptors during detection without losing their positive effect. This makes algorithms simpler and faster. Both are possible if the object model is properly regularised and we discuss a modification of SVMs which allows for doing so.	algorithm;algorithmic efficiency;approximation algorithm;branch and bound;computation;computational complexity theory;discriminative model;experiment;free viewpoint television;generative model;gradient descent;hough transform;implicit shape model;invariant (computer science);linear search;microsoft windows;mixture model;object detection;online and offline;ordinal data;prism (surveillance program);parametric model;preprocessor;programming paradigm;pyramid (geometry);sfiaplus;scalability;semiconductor industry;semiparametric model;sensor;skolem normal form;smoothing;speedup;switzerland;video post-processing;visual descriptor;word lists by frequency	Alain D. Lehmann;Bastian Leibe;Luc Van Gool	2010	International Journal of Computer Vision	10.1007/s11263-010-0342-x	hough transform;sliding window protocol;computer vision;object model;computer science;machine learning;pattern recognition;mixture model;mathematics;branch and bound	Vision	31.279344538510543	-48.335095613443976	148036
7f671d3c430463c41864b125af6cbd400c45739f	nonparametric weighted feature extraction for classification	teledetection;high dimensionality;modele mathematique;feature extraction covariance matrix scattering pattern recognition availability labeling focusing mathematics computer science education hyperspectral imaging;nonparametric statistics;image classification;modelo matematico;classification;deteccion a distancia;discriminant analysis;analyse discriminante;methode nouvelle;accuracy;analisis discriminante;precision;dimensionality reduction;mathematical models;geophysical signal processing;feature extraction;remote sensing;dimensionality reduction nonparametric feature extraction weighted feature extraction image classification high dimensional pattern recognition multiclass pattern recognition scatter matrices singularity problem parametric discriminant analysis nonnormal datasets decision boundary;new methods;pattern recognition;nonparametric statistics remote sensing geophysical signal processing geophysical techniques image classification feature extraction;reconnaissance forme;extension;classification accuracy;nonparametric feature extraction;metodo nuevo;dimensional reduction;clasificacion;extraction;geophysical techniques	In this paper, a new nonparametric feature extraction method is proposed for high-dimensional multiclass pattern recognition problems. It is based on a nonparametric extension of scatter matrices. There are at least two advantages to using the proposed nonparametric scatter matrices. First, they are generally of full rank. This provides the ability to specify the number of extracted features desired and to reduce the effect of the singularity problem. This is in contrast to parametric discriminant analysis, which usually only can extract L-1 (number of classes minus one) features. In a real situation, this may not be enough. Second, the nonparametric nature of scatter matrices reduces the effects of outliers and works well even for nonnormal datasets. The new method provides greater weight to samples near the expected decision boundary. This tends to provide for increased classification accuracy.	decision boundary;feature extraction;linear discriminant analysis;pattern recognition;technological singularity	Bor-Chen Kuo;David A. Landgrebe	2004	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2004.825578	machine learning;pattern recognition;mathematics;accuracy and precision;linear discriminant analysis;statistics	Vision	30.266432323172545	-42.08967868294144	148058
9f949f6e40e604ef05ed690ad732a2f6625997b1	understanding everyday hands in action from rgb-d images	image segmentation computer vision gesture recognition image colour analysis;3d understanding rgb d image functional manipulation handheld object fine grained grasp classification fine grained taxonomy human object grasp natural interaction computer vision perspective force prediction functional grasp analysis image segmentation object context;taxonomy force three dimensional displays solid modeling robots kinematics cameras	We analyze functional manipulations of handheld objects, formalizing the problem as one of fine-grained grasp classification. To do so, we make use of a recently developed fine-grained taxonomy of human-object grasps. We introduce a large dataset of 12000 RGB-D images covering 71 everyday grasps in natural interactions. Our dataset is different from past work (typically addressed from a robotics perspective) in terms of its scale, diversity, and combination of RGB and depth data. From a computer-vision perspective, our dataset allows for exploration of contact and force prediction (crucial concepts in functional grasp analysis) from perceptual cues. We present extensive experimental results with state-of-the-art baselines, illustrating the role of segmentation, object context, and 3D-understanding in functional grasp analysis. We demonstrate a near 2X improvement over prior work and a naive deep baseline, while pointing out important directions for improvement.	3d modeling;baseline (configuration management);computer vision;handheld game console;ibm notes;information theory;interaction;layout engine;pipeline (computing);robotics;sensor;spatial variability;statistical classification;synthetic intelligence;taxonomy (general);visual computing	Grégory Rogez;James Steven Supancic;Deva Ramanan	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.443	computer vision;simulation;computer science	Vision	35.664733959893866	-46.646562033896835	148375
1ec83b3679a9e8d3ce673cfaea91b02b368c2f5c	cross-domain generative learning for fine-grained sketch-based image retrieval		The key challenge for learning a fine-grained sketch-based image retrieval (FG-SBIR) model is to bridge the domain gap between photo and sketch. Existing models learn a deep joint embedding space with discriminative losses where a photo and a sketch can be compared. In this paper, we propose a novel discriminative-generative hybrid model by introducing a generative task of cross-domain image synthesis. This task enforces the learned embedding space to preserve all the domain invariant information that is useful for cross-domain reconstruction, thus explicitly reducing the domain gap as opposed to existing models. Extensive experiments on the largest FG-SBIR dataset Sketchy [19] show that the proposed model significantly outperforms state-of-the-art discriminative FG-SBIR models.	discriminative model;experiment;image retrieval;job control (unix);network architecture;rendering (computer graphics);sketch	Kaiyue Pang;Yi-Zhe Song;Tony Xiang;Timothy M. Hospedales	2017			artificial intelligence;computer science;computer vision;generative model;sketch;image retrieval	Vision	25.795531350068256	-49.36302015327025	148430
081574c4d5fea47adfa960e22f8031ca9f3bbc75	extended morphological profile-based gabor wavelets for hyperspectral image classification		Hyperspectral image acquired by a hyperspectral sensor contains hundred of narrow contiguous spectral bands, since the spatial distribution of surface materials generally exhibits high regularity and local continuity, spatial texture information should be introduced to improve the classification accuracy of hyperspectral image. The extended morphological profiles (EMP) have been created from the raw hyperspectral image, which has proven to be effective and robust of reflecting the spatial structural features of hyperspectral data. In the meanwhile, because the three-dimensional (3D) Gabor wavelets have been introduced to exploit the joint spectral-spatial features of hyperspectral image. In this paper, for the purpose of combining the advantages of the EMP operator and Gabor wavelet transform together, an extended morphological profile-based Gabor wavelets, which is named as EMP-Gabor, has been proposed for hyperspectral image classification. Definitely, to compute principal components of the hyperspectral image, the most significant principal components are used as base images for an extended morphological profile, and the EMP features can be thus obtained. Secondly, 3D Gabor wavelets with particular orientations are directly convolved with the EMP feature cube. Finally, support vector machine (SVM) classifier is utilized to carry out the classification task. Experimental results on two real hyperspectral data sets have demonstrated the effectiveness of the proposed EMP-Gabor framework for hyperspectral image classification over several state-of-the-art methods.		Sen Jia;Huimin Xie;Xianglong Deng	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8546092	support vector machine;wavelet transform;gabor wavelet;computer vision;spatial distribution;feature extraction;principal component analysis;hyperspectral imaging;artificial intelligence;contextual image classification;pattern recognition;computer science	Vision	30.74949016654818	-44.7609314778965	148465
5debbbcfd11a190053b3924e18d3bcb0ab52de24	spatio-temporal cuboid pyramid for action recognition using depth motion sequences	histograms;support vector machines;geometry;skeleton;motion segmentation;feature extraction;cameras	In this paper, we present an effective method to recognize human actions from sequences of depth maps, which are captured by a consume depth sensor. In our approach, we first project each frame of a depth sequence onto three orthogonal planes and generate the depth motion sequence (DMS) between two consecutive frames from the three projected views. Then we propose a spatio-temporal cuboid pyramid (STCP) to subdivide the DMS volumes into a set of spatial cuboids on scaled temporal levels. And a cuboid fusion scheme is presented to concatenate the histograms of oriented gradients (HOG) features extracted from the spatial cuboid. The proposed approach is evaluated on three public benchmark datasets, i.e., MSRAction3D, MSRGesture3D and MSRActionPairs dataset. The experimental results demonstrate that the proposed method achieves state-of-the-art performance.	benchmark (computing);concatenation;cuboid;depth map;effective method;gradient;range imaging	Xiaopeng Ji;Jun Cheng;Wei Feng	2016	2016 Eighth International Conference on Advanced Computational Intelligence (ICACI)	10.1109/ICACI.2016.7449827	computer vision;geography;geometry;computer graphics (images)	Vision	37.45793647743255	-50.5014782118735	148501
7ebc96b4b7886b263808c2cd62b21158ebf6297c	crowd motion analysis: segmentation, anomaly detection, and behavior classification		The objective of this doctoral study is to develop efficient techniques for flow segmentation, anomaly detection, and behavior classification in crowd scenes. Considering the complexities of occlusion, we focused our study on gathering the motion information at a higher scale, thus not associating it to single objects, but considering the crowd as a single entity. Firstly, we propose methods for flow segmentation based on correlation features, graph cut, Conditional Random Fields (CRF), enthalpy model, and particle mutual influence model. Secondly, methods based on deviant orientation information, Gaussian Mixture Model (GMM), and MLP neural network combined with GoodFeaturesToTrack are proposed to detect two types of anomalies. The first one detects deviant motion of the pedestrians compared to what has been observed beforehand. The second one detects panic situation by adopting the GMM and MLP to learn the behavior of the motion features extracted from a grid of particles and GoodFeaturesToTrack, respectively. Finally, we propose particle-driven and hybrid appraoches to classify the behaviors of crowd in terms of lane, arch/ring, bottleneck, blocking and fountainhead within a region of interest (ROI). For this purpose, the particle-driven approach extracts and fuses spatio-temporal features together. The spatial features represent the density of neighboring particles in the predefined proximity, whereas the temporal features represent the rendering of trajectories traveled by the particles. The hybrid approach exploits a thermal diffusion process combined with an extended variant of the social force model (SFM).	anomaly detection;artificial neural network;blocking (computing);conditional random field;cut (graph theory);google map maker;graph cuts in computer vision;memory-level parallelism;mixture model;quad flat no-leads package;region of interest;social force model	Habib Ullah	2015				ML	37.8396939457476	-48.46304011278462	148613
e376047128ce55155bf26d74e660915e6bc03369	video activity recognition using sequence kernel based support vector machines		This paper addresses issues in performing video activity recognition using support vector machines (SVMs). The videos comprise of sequence of sub-activities where a sub-activity correspond to a segment of video. For building activity recognizer, each segment is encoded into a feature vector. Hence a video is represented as a sequence of feature vectors. In this work, we propose to explore GMM-based encoding scheme ot encode a video segment into bag-of-visual-word vector representation. We also propose to use Fisher score vector as an encoded representation for a video segment. For building SVM-based activity recognizer, it is necessary to use suitable kernel that match sequences of feature vectors. Such kernels are called sequence kernels. In this work, we propose different sequence kernels like modified time flexible kernel, segment level pyramid match kernel, segment level probability sequence kernel and segment level Fisher kernel for matching videos when segments are represented using an encoded feature vector representation. The effectiveness of the proposed sequence kernels in the SVM- based activity recognition are studied using benchmark datasets.		Sony Allappa;Veena Thenkanidiyoor;Dileep Aroor Dinesh	2018		10.1007/978-3-030-05499-1_9	computer science;fisher kernel;encode;artificial intelligence;support vector machine;pattern recognition;kernel (linear algebra);feature vector;encoding (memory);scoring algorithm;activity recognition	Robotics	36.791770716579656	-51.3342521265105	148641
9557edde495aed3d50398ed6fd67ec7ee5d7de45	local quality assessment of point clouds for indoor mobile mapping	degradation;local quality assessment;point clouds;machine learning;indoor mobile mapping	The quality of point clouds obtained by RGB-D camera-based indoor mobile mapping can be limited by local degradation because of complex scenarios such as sensor characteristics, partial occlusions, cluttered backgrounds, and complex illumination conditions. This paper presents a machine learning framework to assess the local quality of indoor mobile mapping point cloud data. In our proposed framework, a point cloud dataset with multiple kinds of quality problems is first created by manual annotation and degradation simulation. Then, feature extraction methods based on 3D patches are treated as operating units to conduct quality assessment in local regions. Also, a feature selection algorithm is deployed to obtain the essential components of feature sets that are used to effectively represent local degradation. Finally, a semi-supervised method is introduced to classify quality types of point clouds. Comparative experiments demonstrate that the proposed framework obtained promising quality assessment results with limited labeled data and a large amount of unlabeled data. & 2016 Elsevier B.V. All rights reserved.	elegant degradation;experiment;feature extraction;feature selection;machine learning;mobile mapping;mobile robot;point cloud;selection algorithm;semi-supervised learning;semiconductor industry;simulation;supervised learning	Fangfang Huang;Chenglu Wen;Huan Luo;Ming Cheng;Cheng Wang;Jonathan Li	2016	Neurocomputing	10.1016/j.neucom.2016.02.033	computer vision;simulation;degradation;computer science;machine learning;point cloud	AI	30.936547660389053	-50.09608706106864	148686
c89112fc5822eb3d77c793814d9c4a3b6ed2d5d0	motion information for video retrieval	dynamic programming;video databases;motion track;cmu;image motion analysis;information retrieval;video analysis;video retrieval;dynamic program;data mining;video retrieval dynamic programming image motion analysis video databases;cmu database;motion tracking;video features;indexation;decision support systems;center of gravity;query by example;dynamic programming motion track video analysis video retrieval n gram matching;video database;n gram matching;cmu database video retrieval video features motion track center of gravity n gram matching query by example dynamic programming	In this paper, we propose the use of motion tracks to generate video features, which are applied to perform video retrieval. We simply use the location of the object's center-of-gravity and its velocity across consecutive video frames to from motion flows, which are then recorded and stored in a video database. In the video retrieval phase, we propose the use of n-gram matching strategy to execute the video retrieval task. The retrieval process can be triggered query-by-example. We identify corresponding query and index contents accurately in order to detect “similar” videos. Experiments are carried out on CMU database. An empirical comparison over the state-of-the-art dynamic programming techniques is encouraging and demonstrates the advantage and feasibility of the n-gram method.	dynamic programming;n-gram;query by example;velocity (software development)	Bashar Tahayna;Mohammed Belkhatir;Saadat M. Alhashmi	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202633	video compression picture types;computer vision;decision support system;computer science;query by example;dynamic programming;video tracking;block-matching algorithm;multimedia;smacker video;center of gravity;motion compensation;information retrieval	Vision	38.768096614054635	-51.388374108069684	148688
7229bf0f7111c6a2856eea30c367ede3b3cc799f	large-scale person re-identification as retrieval		This paper targets to bring together the research efforts on two fields that are growing actively in the past few years: multicamera person Re-Identification (ReID) and large-scale image retrieval. We demonstrate that the essentials of image retrieval and person ReID are the same, i.e., measuring the similarity between images. However, person ReID requires more discriminative and robust features to identify the subtle differences of different persons and overcome the large variance among images of the same person. Specifically, we propose a coarse-to-fine (C2F) framework and a Convolutional Neural Network structure named as Conv-Net to tackle the large-scale person ReID as an image retrieval task. Given a query person image, the C2F firstly employ Conv-Net to extract a compact descriptor and perform the coarse-level search. A robust descriptor conveying more spatial cues is hence extracted to perform the fine-level search. Extensive experimental results show that the proposed method outperforms existing methods on two public datasets. Further, the evaluation on a large-scale Person-520K dataset demonstrates that our work is significantly more efficient than existing works, e.g., only needs 180ms to identify a query person from 520K images.	convolutional neural network;image retrieval	Hantao Yao;Shiliang Zhang;Dongming Zhang;Yongdong Zhang;Jintao Li;Yu Wang;Qi Tian	2017	2017 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2017.8019485	visual word;convolutional neural network;robustness (computer science);discriminative model;artificial intelligence;computer vision;pattern recognition;image retrieval;feature extraction;computer science;machine learning	Vision	32.74563349560411	-51.44178071176089	148791
1efaa128378f988965841eb3f49d1319a102dc36	hierarchical binary cnns for landmark localization with limited resources		Our goal is to design architectures that retain the groundbreaking performance of Convolutional Neural Networks (CNNs) for landmark localization and at the same time are lightweight, compact and suitable for applications with limited computational resources. To this end, we make the following contributions: (a) we are the first to study the effect of neural network binarization on localization tasks, namely human pose estimation and face alignment. We exhaustively evaluate various design choices, identify performance bottlenecks, and more importantly propose multiple orthogonal ways to boost performance. (b) Based on our analysis, we propose a novel hierarchical, parallel and multi-scale residual architecture that yields large performance improvement over the standard bottleneck block while having the same number of parameters, thus bridging the gap between the original network and its binarized counterpart. (c) We perform a large number of ablation studies that shed light on the properties and the performance of the proposed block. (d) We present results for experiments on the most challenging datasets for human pose estimation and face alignment, reporting in many cases state-of-the-art performance. (e) We further provide additional results for the problem of facial part segmentation. Code can be downloaded from https://www.adrianbulat.com/binary-cnn-landmarks.	3d pose estimation;architecture as topic;artificial neural network;biological neural networks;bottleneck (software);bridging (networking);computation;computational resource;convolutional neural network;emoticon;experiment;numerous;biologic segmentation	Adrian Bulat;Georgios Tzimiropoulos	2018	IEEE transactions on pattern analysis and machine intelligence	10.1109/TPAMI.2018.2866051	artificial intelligence;convolutional neural network;landmark;artificial neural network;pattern recognition;computer science;performance improvement;pose;bridging (networking);segmentation;bottleneck	Vision	24.91555857482973	-52.03967018977058	148931
7551dc7fbfd0233db44112928083a17a383ad239	out-of-sample extrapolation of learned manifolds	eigenvalues and eigenfunctions;kernel eigenfunction approximation;processus gauss;fonction propre;finite sample;kernel principal component analysis;analisis componente principal;eigenfunction;kernel;gaussian processes;dimension reduction;analisis forma;out of sample data extrapolation;computational geometry;gaussian basis function;funcion propia;kernel function;extrapolation;manifold learning method;intelligence artificielle;manifold learning;matrix algebra;computer vision;maximum variance unfolding manifold learning out of sample extrapolation;reduction dimension;approximation theory;learning systems;out of sample extrapolation;laplace equations;gaussian basis function out of sample data extrapolation manifold learning method finite sample maximum variance unfolding kernel principal component analysis kernel matrix kernel function kernel eigenfunction approximation;extrapolation kernel learning systems principal component analysis eigenvalues and eigenfunctions computer vision image analysis data acquisition data preprocessing laplace equations;principal component analysis;funcion nucleo;fonction noyau;analyse composante principale;reduccion dimension;algorithms artificial intelligence image enhancement image interpretation computer assisted pattern recognition automated reproducibility of results sensitivity and specificity;maximum variance unfolding;kernel pca;artificial intelligence;image analysis;pattern analysis;inteligencia artificial;gaussian process;extrapolacion;learning artificial intelligence;proceso gauss;data preprocessing;principal component analysis approximation theory computational geometry eigenvalues and eigenfunctions extrapolation gaussian processes learning artificial intelligence matrix algebra;data acquisition;analyse forme;kernel matrix	We investigate the problem of extrapolating the embedding of a manifold learned from finite samples to novel out-of-sample data. We concentrate on the manifold learning method called Maximum Variance Unfolding (MVU) for which the extrapolation problem is still largely unsolved. Taking the perspective of MVU learning being equivalent to Kernel PCA, our problem reduces to extending a kernel matrix generated from an unknown kernel function to novel points. Leveraging on previous developments, we propose a novel solution which involves approximating the kernel eigenfunction using Gaussian basis functions. We also show how the width of the Gaussian can be tuned to achieve extrapolation. Experimental results which demonstrate the effectiveness of the proposed approach are also included.	approximation algorithm;basis function;data dependency;experiment;extrapolation;kernel principal component analysis;nonlinear dimensionality reduction;normal statistical distribution;performance tuning;sample variance;semidefinite embedding;synthetic intelligence;manifold;width	Tat-Jun Chin;David Suter	2008	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2007.70813	mathematical optimization;kernel;image analysis;kernel embedding of distributions;computational geometry;kernel principal component analysis;computer science;machine learning;gaussian process;mathematics;geometry;variable kernel density estimation;statistics	Vision	28.969763273426448	-38.68189088486965	148988
caa26478a03b8333e66f5bcf2a085f6e8221b4bb	clustering-based hyperspectral band selection using information measures	hierarchical clustering;teledetection;pixel image classification task;image processing geophysical signal processing geophysical techniques image classification;image processing;kullback leibler divergence;performance;feature clustering;variance analysis;intercluster variance;image classification;analisis grupo;clustering based hyperspectral band selection;analisis varianza;classification;deteccion a distancia;intracluster variance;accuracy;cluster analysis;precision;dimensionality reduction;hierarchical clustering structure;information theory dimensionality reduction feature clustering feature selection;information measure;geophysical signal processing;clustering;analyse groupe;feature extraction;remote sensing;pixel;teledetection hyperspectrale;pixel image classification task clustering based hyperspectral band selection information measure hyperspectral imaging hierarchical clustering structure intracluster variance intercluster variance mutual information kullback leibler divergence image band;mutual information;feature selection;performances;image band;theorie information;reduction dimensionnalite;hyperspectral imaging;image hyperspectrale;dimensional reduction;hyperspectral image;clasificacion;information theory;geophysical techniques;analyse variance;hyperspectral imaging hyperspectral sensors feature extraction pixel programmable logic arrays mutual information image classification information theory remote sensing	Hyperspectral imaging involves large amounts of information. This paper presents a technique for dimensionality reduction to deal with hyperspectral images. The proposed method is based on a hierarchical clustering structure to group bands to minimize the intracluster variance and maximize the intercluster variance. This aim is pursued using information measures, such as distances based on mutual information or Kullback-Leibler divergence, in order to reduce data redundancy and non useful information among image bands. Experimental results include a comparison among some relevant and recent methods for hyperspectral band selection using no labeled information, showing their performance with regard to pixel image classification tasks. The technique that is presented has a stable behavior for different image data sets and a noticeable accuracy, mainly when selecting small sets of bands.	cluster analysis;computer vision;data redundancy;dimensionality reduction;hierarchical clustering;kullback–leibler divergence;mutual information;pixel	Adolfo Martínez Usó;Filiberto Pla;José Martínez Sotoca;Pedro García-Sevilla	2007	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2007.904951	computer vision;performance;image processing;information theory;machine learning;pattern recognition;mathematics;accuracy and precision;cluster analysis;feature selection;statistics	Robotics	30.855127606559137	-42.23934960563764	149064
1e2e71f8d99542d9cb357c7a5b9b6101eeb3fc65	online view-invariant human action recognition using rgb-d spatio-temporal matrix	view invariant;self similarity;action recognition	We propose a novel approach to recognize action under view changes online with RGBD camera. Perspective effects and camera motions have been considered as difficult problems in recognizing action that when two video sequences record a specific action from various camera views, the resulting appearances of actions would be entirely different. Consequently, if we simply apply feature extraction methods to the raw video, we will end up getting totally different features. Recent studies explored the stability of self-similarities for action sequence over time, an idea that put into practice in view-invariant action recognition. Instead of doing the extraction of spatio-temporal feature for every frame and using these feature vectors directly, our study uses the Euclidean distance between spatio-temporal feature vectors that are represented in a Spatio-Temporal Matrix (STM). To recognize the action, we describe the local tendency of the STM using pyramid-structural bag-of-words (BoW-Pyramid) and train a SVM as our classifier.	action potential;bag-of-words model;euclidean distance;experiment;feature extraction;online and offline;parallel computing;pyramid (geometry);software transactional memory;support vector machine;uncompressed video	Yen-Pin Hsu;Chengyin Liu;Tzu-Yang Chen;Li-Chen Fu	2016	Pattern Recognition	10.1016/j.patcog.2016.05.010	computer vision;self-similarity;artificial intelligence;mathematics	Vision	35.871656524042805	-50.127058825199846	149113
58857830e1e0fdbc7edfb283a1c23c5bcff528d5	deep multiple instance learning for image classification and auto-annotation	machine learning proposals visualization feature extraction noise measurement neural networks supervised learning;supervised deep learning framework deep multiple instance learning mil image classification image annotation;learning artificial intelligence image classification	The recent development in learning deep representations has demonstrated its wide applications in traditional vision tasks like classification and detection. However, there has been little investigation on how we could build up a deep learning framework in a weakly supervised setting. In this paper, we attempt to model deep learning in a weakly supervised learning (multiple instance learning) framework. In our setting, each image follows a dual multi-instance assumption, where its object proposals and possible text annotations can be regarded as two instance sets. We thus design effective systems to exploit the MIL property with deep learning strategies from the two ends; we also try to jointly learn the relationship between object and annotation proposals. We conduct extensive experiments and prove that our weakly supervised deep learning framework not only achieves convincing performance in vision tasks including classification and image annotation, but also extracts reasonable region-keyword pairs with little supervision, on both widely used benchmarks like PASCAL VOC and MIT Indoor Scene 67, and also a dataset for image-and patch-level annotations.	automatic image annotation;benchmark (computing);computer vision;deep learning;experiment;multiple instance learning;statistical classification;supervised learning	Jiajun Wu;Yinan Yu;Chang Huang;Kai Yu	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298968	semi-supervised learning;unsupervised learning;computer vision;instance-based learning;algorithmic learning theory;computer science;online machine learning;machine learning;pattern recognition;data mining;supervised learning;stability;deep belief network;active learning;generalization error	Vision	25.339799653313822	-48.74759440600986	149177
576015cba706425d0bfef6e51452772a213b62ab	fine-to-coarse knowledge transfer for low-res image classification	automobiles;image resolution;training;deep learning fine grained classification low resolution;testing;heating;training data;birds;training image resolution automobiles heating training data birds testing	We address the difficult problem of distinguishing fine-grained object categories in low resolution images. We propose a simple an effective deep learning approach that transfers fine-grained knowledge gained from high resolution training data to the coarse low-resolution test scenario. Such fine-to-coarse knowledge transfer has many real world applications, such as identifying objects in surveillance photos or satellite images where the image resolution at the test time is very low but plenty of high resolution photos of similar objects are available. Our extensive experiments on two standard benchmark datasets containing fine-grained car models and bird species demonstrate that our approach can effectively transfer fine-detail knowledge to coarse-detail imagery.	benchmark (computing);computer vision;deep learning;experiment;image resolution;scenario testing	Xingchao Peng;Judy Hoffman;Stella X. Yu;Kate Saenko	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7533047	computer vision;training set;simulation;image resolution;computer science;machine learning;software testing;computer graphics (images)	Vision	28.048462457222158	-50.68832857667077	149223
7911b9abf98a3d772601a27128e18bb313f294ac	optimal linear representations of images for object recognition	fixed rank subspace image representation imaging analysis image classification pca ica fda monte carlo simulated annealing optimal linear representation principal component analysis fisher discriminant analysis optimization image database object recognition performance maximization;object recognition;monte carlo simulated annealing;image databases;dimension reduction;object recognition performance maximization;visual databases monte carlo methods simulated annealing image representation object recognition principal component analysis;image database;object recognition principal component analysis image analysis image classification independent component analysis performance analysis image databases monte carlo methods computational modeling simulated annealing;image classification;imaging analysis;independent component analysis;indexing terms;simulated annealing;optimization problem;fisher discriminant analysis;computational modeling;grassmann manifold;image representation;principal component analysis;nearest neighbor;performance analysis;optimal linear representation;stochastic gradient;image analysis;optimization;fixed rank subspace;optimal component analysis;ica;pca;monte carlo methods;fda;visual databases	Although linear representations are frequently used in image analysis, their performances are seldom optimal in specific applications. This paper proposes a stochastic gradient algorithm for finding optimal linear representations of images for use in appearance-based object recognition. Using the nearest neighbor classifier, a recognition performance function is specified and linear representations that maximize this performance are sought. For solving this optimization problem on a Grassmann manifold, a stochastic gradient algorithm utilizing intrinsic flows is introduced. Several experimental results are presented to demonstrate this algorithm.	flow;gradient;image analysis;manifold alignment;mathematical optimization;nearest neighbour algorithm;optimization problem;outline of object recognition;performance;single linkage cluster analysis	Xiuwen Liu;Anuj Srivastava;Kyle A. Gallivan	2003	IEEE transactions on pattern analysis and machine intelligence	10.1109/CVPR.2003.1211358	computer vision;image analysis;computer science;machine learning;pattern recognition;mathematics;principal component analysis	Vision	27.84017070402526	-42.76607715225496	149247
8d53885adc5b41753230c0d57a4e77149d241f9d	efficiently downdating, composing and splitting singular value decompositions preserving the mean information	subspace learning;singular value decomposition;exact results;computer vision;latent semantic analysis	Three methods for the efficient downdating, composition and splitting of low rank singular value decompositions are proposed. They are formulated in a closed form, considering the mean information and providing exact results. Although these methods are presented in the context of computer vision, they can be used in any field forgetting information, combining different eigenspaces in one or ignoring particular dimensions of the column space of the data. Application examples on face subspace learning and latent semantic analysis are given and performance results are provided.	computer vision;latent semantic analysis;singular value decomposition	Javier Melenchón;Elisa Martínez Marroquín	2007		10.1007/978-3-540-72849-8_55	computer vision;combinatorics;discrete mathematics;latent semantic indexing;latent semantic analysis;computer science;machine learning;mathematics;singular value decomposition	Vision	27.139207618573074	-40.074456559468445	149378
2e3c9c587d7ff1db6510ca460279587c779fccc7	kernel fused representation-based classifier for hyperspectral imagery	kernel;training;collaboration;classifier fusion collaborative representation cr hyperspectral image hsi classification kernel trick sparse representation sr;dictionaries;hyperspectral imaging image classification image representation;kernel dictionaries training hyperspectral imaging collaboration gallium nitride;hyperspectral imaging;kernel fused method kernel representation coefficients kcr ksr kernel sr kernel cr unified kernel representation based classification framework collaborative representation sparse representation hsi hyperspectral images kfrc hyperspectral imagery kernel fused representation based classifier;gallium nitride	In this letter, we propose a kernel fused representation-based classifier (KFRC) for hyperspectral images (HSIs), which combines sparse representation (SR) and collaborative representation (CR) into a unified kernel representation-based classification framework. First, we present two individual kernel methods, i.e., kernel SR (KSR) and kernel CR (KCR), which kernelize the representation methods by projecting the samples into a high-dimensional kernel space to improve the samples separability between different classes. Once obtaining the two kernel representation coefficients, KFRC attempts to achieve a balance between KSR and KCR via an adjusting parameter  $\theta $  in the kernel residual domain. Subsequently, the class label of each test sample is determined by the minimum residual for each class. Experimental results on two HSIs demonstrate the proposed kernel fused method performs better than the other state-of-the-art representation-based classifiers.	coefficient;cyclic redundancy check;decision boundary;dictionary;feature vector;frame rate control;horizontal situation indicator;kernel (operating system);kernel method;linear separability;pixel;semiconductor research corporation;sparse approximation;sparse matrix;user space	Le Gan;Peijun Du;Junshi Xia;Yaping Meng	2017	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2017.2671852	computer vision;kernel method;kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;hyperspectral imaging;machine learning;pattern recognition;mathematics;polynomial kernel;remote sensing;collaboration	Vision	29.74568411418336	-43.969644479505604	149540
110ae61f56d62f017cd7b98dec828971bb758996	joint multiple dictionary learning for tensor sparse coding	endnotes;tensile stress dictionaries joints vectors correlation databases encoding;image coding joint multiple dictionary learning tensor sparse coding sparse representation high dimensional data one dimensional vector id vector id model data spatial structure property tensor decomposition kronecker product tensor sparsity mode dictionaries atom correlations frequent pattern tree mining algorithm fp tree mining algorithm k svd dictionary update method tensor based dictionary learning algorithm image denoising;vectors data mining dictionaries image coding image denoising learning artificial intelligence tensors trees mathematics;pubications	Traditional dictionary learning algorithms are used for finding a sparse representation on high dimensional data by transforming samples into a one-dimensional (ID) vector. This ID model loses the inherent spatial structure property of data. An alternative solution is to employ Tensor Decomposition for dictionary learning on their original structural form - a tensor - by learning multiple dictionaries along each mode and the corresponding sparse representation in respect to the Kronecker product of these dictionaries. To learn tensor dictionaries along each mode, all the existing methods update each dictionary iteratively in an alternating manner. Because atoms from each mode dictionary jointly make contributions to the spar sity of tensor, existing works ignore atoms correlations between different mode dictionaries by treating each mode dictionary independently. In this paper, we propose a joint multiple dictionary learning method for tensor sparse coding, which explores atom correlations for sparse representation and updates multiple atoms from each mode dictionary simultaneously. In this algorithm, the Frequent-Pattern Tree (FP-tree) mining algorithm is employed to exploit frequent atom patterns in the sparse representation. Inspired by the idea of K-SVD, we develop a new dictionary update method that jointly updates elements in each pattern. Experimental results demonstrate our method outperforms other tensor based dictionary learning algorithms.	algorithm;computation;dictionary;iterative reconstruction;k-svd;machine learning;neural coding;noise reduction;quantum decoherence;remote desktop services;simultaneous equations model;singular value decomposition;sparse approximation;sparse matrix;structure mining;synthetic intelligence	Yifan Fu;Junbin Gao;Yanfeng Sun;Xia Hong	2014	2014 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2014.6889490	k-svd;computer science;theoretical computer science;machine learning;pattern recognition	AI	28.568521942952948	-40.35280477351197	149653
b367cfa44c68efadf7043c7b5a81bf6cd2e6d351	unsupervised sub-categorization for object detection: finding cars from a driving vehicle	automobiles;training;image classification;three dimensional;iterative methods;visualization;boosting;stochastic processes;three dimensional displays;merging;traffic engineering computing automobiles image classification iterative methods learning artificial intelligence object detection stochastic processes;traffic engineering computing;optimization;training visualization tv three dimensional displays merging boosting optimization;tv;learning artificial intelligence;object detection;linear classification unsupervised subcategorization object detection driving vehicle object classification background classification sample relabeling iterative split and merge procedure fast stochastic learning algorithm full retraining car detection problem histogram of oriented gradient features	We present a novel algorithm for unsupervised subcategorization of an object class, in the context of object detection. Dividing the detection problem into smaller subproblems simplifies the object vs. background classification.	algorithm;categorization;iteration;iterative method;linear classifier;mathematical optimization;object detection;unsupervised learning	Rob G. J. Wijnhoven;Peter H. N. de With	2011	2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)	10.1109/ICCVW.2011.6130504	stochastic process;three-dimensional space;computer vision;contextual image classification;object-class detection;visualization;computer science;viola–jones object detection framework;machine learning;pattern recognition;mathematics;iterative method;boosting	Vision	36.88117544263624	-46.87692793287029	149728
7c18965f5573020f32b151a08178ee4906b5bf4c	recursive coarse-to-fine localization for fast object detection	average precision;prior knowledge;machine learning;svm;sliding window;object detection	Cascading techniques are commonly used to speed-up the scan of an image for object detection. However, cascades of detectors are slow to train due to the high number of detectors and corresponding thresholds to learn. Furthermore, they do not use any prior knowledge about the scene structure to decide where to focus the search. To handle these problems, we propose a new way to scan an image, where we couple a recursive coarse-to-fine refinement together with spatial constraints of the object location. For doing that we split an image into a set of uniformly distributed neighborhood regions, and for each of these we apply a local greedy search over feature resolutions. The neighborhood is defined as a scanning region that only one object can occupy. Therefore the best hypothesis is obtained as the location with maximum score and no thresholds are needed. We present an implementation of our method using a pyramid of HOG features and we evaluate it on two standard databases, VOC2007 and INRIA dataset. Results show that the Recursive Coarse-to-Fine Localization (RCFL) achieves a 12x speed-up compared to standard sliding windows. Compared with a cascade of multiple resolutions approach our method has slightly better performance in speed and Average-Precision. Furthermore, in contrast to cascading approach, the speed-up is independent of image conditions, the number of detected objects and clutter.	bag-of-words model;clutter;computation;database;greedy algorithm;image scanner;information retrieval;microsoft windows;object detection;recursion (computer science);refinement (computing);sensor;speedup	Marco Pedersoli;Jordi Gonzàlez;Andrew D. Bagdanov;Juan José Villanueva	2010		10.1007/978-3-642-15567-3_21	sliding window protocol;support vector machine;computer vision;computer science;machine learning;pattern recognition;mathematics	Vision	31.504654798030284	-48.478875288846034	149768
c638fd26b1934005b7957ff99a4a2f0172b09a9a	accurate and scalable image clustering based on sparse representation of camera fingerprint		Clustering images according to their acquisition devices is a well-known problem in multimedia forensics, which is typically faced by means of camera sensor pattern noise (SPN). Such an issue is challenging since SPN is a noise-like signal, hard to be estimated, and easy to be attenuated or destroyed by many factors. Moreover, the high dimensionality of SPN hinders large-scale applications. Existing approaches are typically based on the correlation among SPNs in the pixel domain, which might not be able to capture intrinsic data structure in the union of vector subspaces. In this paper, we propose an accurate clustering framework, which exploits linear dependences among SPNs in their intrinsic vector subspaces. Such dependences are encoded under sparse representations, which are obtained by solving an LASSO problem with non-negativity constraint. The proposed framework is highly accurate in a number of clusters’ estimation and image association. Moreover, our framework is scalable to the number of images and robust against double JPEG compression as well as the presence of outliers, owning big potential for real-world applications. Experimental results on Dresden and Vision database show that our proposed framework can adapt well to both medium-scale and large-scale contexts and outperforms the state-of-the-art methods.		Quoc-Tin Phan;Giulia Boato;Francesco G. B. De Natale	2018	CoRR		pixel;artificial intelligence;computer vision;computer science;transform coding;pattern recognition;image sensor;cluster analysis;data structure;jpeg;fingerprint recognition;sparse approximation	Vision	29.343885935368032	-46.3448517444407	150104
e557698f31f18c4a7b203560f05e24af5fdcea70	urban land cover classification with missing data using deep convolutional neural networks		Fusing different sensors with different data modalities is a common technique to improve land cover classification performance in remote sensing. However, all modalities are rarely available for all test data, and this missing data problem poses severe challenges for multi-modal learning. Inspired by recent successes in deep learning, we propose as a remedy a convolutional neural network architecture for urban remote sensing image segmentation trained on data modalities which are not all available at test time. We train our architecture with a cost function particularly suited for imbalanced classes, as this is a frequent problem in remote sensing. We demonstrate the method using a benchmark dataset containing RGB and DSM images. Assuming that the DSM images are missing during testing, our method outperforms both a CNN trained on RGB images as well as an ensemble of two CNNs trained on the RGB images, by exploiting the training time information of the missing modality.	artificial neural network;benchmark (computing);convolutional neural network;deep learning;image segmentation;loss function;missing data;modal logic;modality (human–computer interaction);network architecture;sensor;test data	Michael Kampffmeyer;Arnt-Børre Salberg;Robert Jenssen	2017	2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2017.8128164	missing data;pattern recognition;artificial intelligence;deep learning;data integration;machine learning;convolutional neural network;data modeling;test data;computer science;rgb color model;image segmentation	Robotics	28.214670266821035	-50.35940163702435	150311
623b62d1ce395bfa71404895cc1603d99c392590	supervised classification of multiple view images in object space for seismic damage assessment	learning;fusion;performance;classification;adaboost;feature;point cloud;random trees	Classification of remote sensing image and range data is normally done in 2D space, because anyhow most sensors capture the surface of the earth from a close-to vertical direction and thus vertical structures, e.g. at building facades are not visible anyways. However, when the objects of interest are photographed from off-nadir directions, like in oblique airborne images, the question on how to efficiently classify those scenes arises. In this paper a study on classification in 3D object space is presented: image features from individual oblique airborne images, and 3D geometric features derived from matching in those images are projected onto voxels. Those are segmented and classified. The study area is Port-Au-Prince (Haiti), where images have been acquired after the earthquakes in January 2010. Results show that through the combination of image evidence as realized by the projection into object space the classification becomes more accurate compared to single image classification. This contribution was selected in a double blind review process to be published within the Lecture Notes in Computer Science series (Springer-Verlag, Heidelberg). Photogrammetric Image Analysis Volume Editors: Stilla U, Rottensteiner F, Mayer H, Jutzi B, Butenuth M LNCS Volume: 6952 Series Editors: Hutchison D, Kanade T, Kittler J, Kleinberg JM, Kobsa A, Mattern F, Mitchell JC, Naor M, Nierstrasz O, Pandu Rangan C, Steffen B, Sudan M, Terzopoulos D, Tygar D, Weikum G ISSN: 0302-9743 The article is accessible online through www.springerlink.com. 45 In: Stilla U et al (Eds) PIA11. International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences 38 (3/W22)	airborne ranger;autostereogram;computer vision;friedrich kittler;image analysis;international standard serial number;kerry mitchell;lecture notes in computer science;lithosphere;machine learning;oblique projection;photogrammetry;prince;sensor;springer (tank);tuple space;voxel	Markus Gerke	2011		10.1007/978-3-642-24393-6_19	computer vision;geography;pattern recognition;remote sensing	Vision	35.49616756026152	-41.662397696508435	150379
44bb6cb6771387f6024a7725b94dcf18ce3432d7	sparse embedding: a framework for sparsity promoting dimensionality reduction	efficient optimization algorithm;dictionary learning;original signal domain;signal recovery;optimization problem;lower-dimensional space;kernel method;meaningful structure;dimensionality reduction;sparse structure;sparse embedding	We introduce a novel framework, called sparse embedding (SE), for simultaneous dimensionality reduction and dictionary learning. We formulate an optimization problem for learning a transformation from the original signal domain to a lower-dimensional one in a way that preserves the sparse structure of data. We propose an efficient optimization algorithm and present its non-linear extension based on the kernel methods. One of the key features of our method is that it is computationally efficient as the learning is done in the lower-dimensional space and it discards the irrelevant part of the signal that derails the dictionary learning process. Various experiments show that our method is able to capture the meaningful structure of data and can perform significantly better than many competitive algorithms on signal recovery and object classification tasks.	algorithm;algorithmic efficiency;cellular automaton;computation;detection theory;dictionary;dimensionality reduction;experiment;kernel method;machine learning;mathematical optimization;nonlinear system;optimization problem;parallel virtual machine;relevance;sparse matrix;synthetic data	Hien Van Nguyen;Vishal M. Patel;Nasser M. Nasrabadi;Rama Chellappa	2012		10.1007/978-3-642-33783-3_30	k-svd;computer science;theoretical computer science;machine learning;pattern recognition	Vision	26.576553048608286	-39.778033904156	150393
7af292248cbc3d3890575c7ae151960d9799b984	metric learning for image steganalysis	measurement signal processing algorithms image reconstruction training manifolds laplace equations support vector machines;metric learning;data hiding method image steganalysis supervised distance metric learning similarity measure image features cover images stego images reduced dimensional space weight metric learning image distribution discrepancy analysis steganalysis metric clean images;image steganalysis;steganography image processing learning artificial intelligence;image steganalysis metric learning	Image steganalysis based on supervised distance metric learning is to find an appropriate measure of similarity between image features where the distribution discrepancy between cover-images and stego-images are analyzed in the reduced dimensional space. Our approach is novel in that it combines the merits of weight metric learning and image distribution analysis in reduced dimension space. By this learning metrics, we exploit a new steganalysis metric to discriminate stego-images from clean images. The experiment results show the effectiveness of the propose approach for some data hiding method.	discrepancy function;experiment;steganalysis;steganography	Guoming Chen;Qiang Chen;Dong Zhang	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7382062	computer vision;machine learning;pattern recognition;mathematics	Robotics	28.298662548852203	-42.453010035280826	150405
9b52a25be980761966192817d27553c722cc0c60	semi-paired probabilistic canonical correlation analysis	canonical correlation analysis;semi paired multi view data;probabilistic canonical correlation analysis	CCA is a powerful tool for analyzing paired multi-view data. However, when facing semi-paired multi-view data which widely exist in real-world problems, CCA usually performs poorly due to its requirement of data pairing between different views in nature. To cope with this problem, we propose a semi-paired variant of CCA named SemiPCCA based on the probabilistic model for CCA. Experiments with artificially generated samples demonstrate the effectiveness of the proposed method.	experiment;free viewpoint television;semiconductor industry;statistical model;visual basic[.net]	Bo Zhang;Jie Hao;Gang Ma;Jinpeng Yue;Zhongzhi Shi	2014		10.1007/978-3-662-44980-6_1	canonical correspondence analysis;computer science;machine learning;data mining;statistics	ML	24.6720825563209	-43.98259347803893	150445
db84ec3d1794178442e12b09ffd058f341bee6ed	representing pairwise spatial and temporal relations for action recognition	action recognition;bag of words	The popular bag-of-words paradigm for action recognition tasks is based on building histograms of quantized features, typically at the cost of discarding all information about relationships between them. However, although the beneficial nature of including these relationships seems obvious, in practice finding good representations for feature relationships in video is difficult. We propose a simple and computationally efficient method for expressing pairwise relationships between quantized features that combines the power of discriminative representations with key aspects of Näıve Bayes. We demonstrate how our technique can augment both appearanceand motion-based features, and that it significantly improves performance on both types of features.	algorithmic efficiency;angularjs;bag-of-words model;code word;key (cryptography);map;naive bayes classifier;overfitting;product binning;programming paradigm	Pyry Matikainen;Martial Hebert;Rahul Sukthankar	2010		10.1007/978-3-642-15549-9_37	computer vision;computer science;bag-of-words model;machine learning;pattern recognition;mathematics	Vision	32.288965509302045	-49.665419548017056	150835
4df889b10a13021928007ef32dc3f38548e5ee56	multi-stage optimal component analysis	learning process;object recognition;face classification;image classification;stochastic gradient optimization process;stochastic processes face recognition gradient methods image classification image representation object recognition;optimization problem;face recognition;stochastic processes;image representation;stochastic gradient;gradient estimate;gradient methods;independent component analysis principal component analysis scattering stochastic processes linear discriminant analysis object recognition image analysis vectors statistical analysis neural networks;face classification optimal component analysis multistage oca learning process stochastic gradient optimization process object recognition application linear representation updating;optimal component analysis;object recognition application;linear representation updating;multistage oca learning process	Optimal component analysis (OCA) uses a stochastic gradient optimization process to find optimal representations for general criteria and shows good performance in object recognition applications. However, OCA often requires extensive computation for gradient estimation and linear representation updating. To significantly reduce the required computation, in this paper, a multi-stage learning process is proposed which decomposes the original optimization problem into several levels. As the learning process at each level starts with a good initial point obtained from next level, the multistage OCA algorithm can speed up the original algorithm significantly and make OCA learning feasible for many applications. We illustrate the effectiveness of the proposed method on the application of face classification.	algorithm;computation;gradient;mathematical optimization;multistage interconnection networks;optimization problem;outline of object recognition	Yiming Wu;Xiuwen Liu;Washington Mio	2007	2007 International Joint Conference on Neural Networks	10.1109/IJCNN.2007.4371359	facial recognition system;optimization problem;stochastic process;mathematical optimization;contextual image classification;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics;statistics	Vision	27.730987274625157	-42.80490054708892	150869
f9c431f58565f874f76a024add2aa80717ec5cf5	disentangling factors of variation for facial expression recognition	convolution;contractive;emotion recognition;deep learning;tfd;auto encoder	We propose a semi-supervised approach to solve the task of emotion recognition in 2D face images using recent ideas in deep learning for handling the factors of variation present in data. An emotion classification algorithm should be both robust to (1) remaining variations due to the pose of the face in the image after centering and alignment, (2) the identity or morphology of the face. In order to achieve this invariance, we propose to learn a hierarchy of features in which we gradually filter the factors of variation arising from both (1) and (2). We address (1) by using a multi-scale contractive convolutional network (CCNET) in order to obtain invariance to translations of the facial traits in the image. Using the feature representation produced by the CCNET, we train a Contractive Discriminative Analysis (CDA) feature extractor, a novel variant of the Contractive Auto-Encoder (CAE), designed to learn a representation separating out the emotion-related factors from the others (which mostly capture the subject identity, and what is left of pose after the CCNET). This system beats the state-of-the-art on a recently proposed dataset for facial expression recognition, the Toronto Face Database, moving the state-of-art accuracy from 82.4% to 85.0%, while the CCNET and CDA improve accuracy of a standard CAE by 8%.	.cda file;algorithm;deep learning;emotion recognition;encoder;galaxy morphological classification;randomness extractor;semi-supervised learning;semiconductor industry	Salah Rifai;Yoshua Bengio;Aaron C. Courville;Pascal Vincent;Mehdi Mirza	2012		10.1007/978-3-642-33783-3_58	computer vision;computer science;artificial intelligence;machine learning;mathematics;deep learning;convolution;autoencoder	Vision	29.693546141675814	-51.06886247812329	150922
6ab0f61310771e72651d611f69fd0283262c2967	hyperspectral image classification based on multiple improved particle swarm cooperative optimization and svm	geophysical image processing;support vector machines geophysical image processing hyperspectral imaging image classification particle swarm optimisation redundancy;extremal optimization hyperspectral image classification hyperspectral data dimensionality information redundancy automatic band selection multiple improved particle swarm cooperative optimization support vector machine mipso svm kernel classifier parameter optimization cooperative evolution;support vector machines;image classification;redundancy;hyperspectral imaging;support vector machines hyperspectral imaging particle swarm optimization optimization accuracy kernel;particle swarm optimisation	The huge increase of hyperspectral data dimensionality and information redundancy has brought high computational cost as well as the over-fitting risk of classification. In this paper, we present an automatic band selection and classification method based on a novel wrapper Multiple Improved particle swarm cooperative optimization and support vector machine model (MIPSO-SVM). The MIPSO-SVM model optimizes both the band subset and SVM kernel parameters simultaneously. In the proposed model, the particle swarm is divided into two sub-swarms. And PSO is improved firstly, by the new update strategy of position and velocity. Then the sub-swarms perform the improved PSO (IPSO) for band selection and classifier parameters optimization independently. Finally, in the process of cooperative evolution, extremal optimization (EO) is incorporated to maintain the diversity of swarms and enhance the space exploration ability of the proposed model. Experimental results demonstrate the effectiveness of the proposed method for band selection and classification of hyperspectral images.	algorithm;algorithmic efficiency;computer vision;extremal optimization;ipso alliance;mathematical optimization;overfitting;particle swarm optimization;premature convergence;redundancy (information theory);support vector machine;velocity (software development)	Yuemei Ren;Yanning Zhang;Qingjie Meng;Lei Zhang	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		support vector machine;computer vision;multi-swarm optimization;contextual image classification;computer science;hyperspectral imaging;machine learning;pattern recognition;mathematics;redundancy	Robotics	30.170871159968264	-43.697850677976746	150993
b9c3e6e54e9a062dc2ff1d8a68b4bef0a0d70c85	a context based tracking for similar and deformable objects				B. Kaushik;Manoj Kumar;Charul Bhatnagar;Anand Singh Jalal	2018	IJCVIP	10.4018/IJCVIP.2018100101	computer vision;machine learning;artificial intelligence;computer science	Vision	36.93432025774887	-47.73320812646886	151060
a08ad0a87c35d9538498dbccd788aa00867fa58c	temporal segmentation of traffic videos based on traffic phase discovery	image motion analysis;temporal video segmentation traffic phase detection topic model fully sparse topic model fstm;training;fstm traffic videos segmentation traffic phase discovery video sequence phase detection video clip traffic light sequence traffic signals fully sparse topic model;computer vision;videos context training image motion analysis computer vision feature extraction visualization;visualization;feature extraction;video signal processing image segmentation image sequences intelligent transportation systems road traffic;context;videos	In this paper, the topic model is adopted to learn traffic phases from video sequence. Phase detection is applied to determine where a video clip is in the traffic light sequence. Each video clip is labeled by a certain traffic phase, based on which, videos are segmented clip by clip. Using topic models, without any prior knowledge of the traffic rules, activities are detected as distributions over quantized optical flow vectors. Then, traffic phases are discovered as clusters over activities according to the traffic signals. We employ the Fully Sparse Topic Model (FSTM) as the topic model. The results show that our method can successfully discover both activities and traffic phases which make veracious description and perception of traffic scenes.	optical flow;quantization (signal processing);sparse;topic model;video clip	Parvin Ahmadi;Razie Kaviani;Iman Gholampour;Mahmoud Tabandeh	2016	NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium	10.1109/NOMS.2016.7502987	computer vision;simulation;visualization;feature extraction;computer science;multimedia	AI	38.40265552237318	-47.208568815818694	151298
1bc51262e027b02643d39076747eefeb45fa2bae	object discovery in depth images	object recognition;neural networks;training;two dimensional displays;three dimensional displays;solid modeling;proposals	We present an unsupervised method for discovering objects from depth information. Our method can identify new common objects appearing in different depth images. We use 2D bounding box proposals to detect candidate locations of objects in each depth image, and then retrieve the corresponding 3D bounding boxes using the depth information. Invalid object proposals can be further removed by analyzing the point cloud distribution inside the 3D bounding box. We measure the similarity between each pair of the object proposals in different images to identify co-occurrences of the same instance. The similarity measure is automatically learned by a Siamese convolutional neural network. Our method is unsupervised in a sense that we do not need human labeled data to train the Siamese network. We use 3D CAD models to synthesize a large set of similar and dissimilar pairs of depth images as the positive and negative data. Our experiments on synthetic data show that the proposed method is able to discover the co-occurrences of the common objects in different depth images.	2.5d;artificial neural network;cloud computing;computer-aided design;convolutional neural network;experiment;minimum bounding box;point cloud;sensor;similarity measure;synthetic data;unsupervised learning	Tzu-Wei Huang;Yu-An Wei;Hwann-Tzong Chen;JenChi Liu	2016	2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)	10.1109/APSIPA.2016.7820700	computer vision;machine learning;data mining;mathematics	ML	30.35046516594345	-48.12767564570467	151302
cdaeaade2a5434fd9775819c2a1e027995e210cf	real-time estimation of human visual attention with dynamic bayesian network and mcmc-based particle filter	focusing;belief networks;saliency based human visual attention;particle filter saliency based human visual attention dynamic bayesian network stream processing markov chain monte carlo mcmc;visual display;particle filtering numerical methods belief networks computer vision markov processes monte carlo methods object detection;probability density function;real time;biological system modeling;humans bayesian methods particle filters stochastic processes biological system modeling hardware computational efficiency signal detection sampling methods object detection;computer vision;markov chain monte carlo sampling;visualization;hidden markov models;real time estimation;stochastic processes;markov chain monte carlo;dynamic bayesian network;particle filter;signal detection theory;humans;markov processes;stochastic model;stream processing;human visual attention;stream processing real time estimation human visual attention dynamic bayesian network signal detection theory visual display stochastic model markov chain monte carlo sampling;markov chain monte carlo mcmc;visual attention;monte carlo methods;object detection;particle filtering numerical methods	Recent studies in signal detection theory suggest that the human responses to the stimuli on a visual display are nondeterministic. People may attend to different locations on the same visual input at the same time. Constructing a stochastic model of human visual attention would be promising to tackle the above problem. This paper proposes a new method to achieve a quick and precise estimation of human visual attention based on our previous stochastic model with a dynamic Bayesian network. A particle filter with Markov chain Monte-Carlo (MCMC) sampling make it possible to achieve a quick and precise estimation through stream processing. Experimental results indicate that the proposed method can estimate human visual attention in real time and more precisely than previous methods.	detection theory;dynamic bayesian network;markov chain monte carlo;nondeterministic algorithm;particle filter;real-time transcription;sampling (signal processing);stream processing	Kouji Miyazato;Akisato Kimura;Shigeru Takagi;Junji Yamato	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202483	stochastic process;computer vision;probability density function;stream processing;visualization;particle filter;markov chain monte carlo;computer science;stochastic modelling;machine learning;markov process;human visual system model;hidden markov model;dynamic bayesian network;statistics;detection theory;monte carlo method	Robotics	38.129240301347146	-41.3622063481738	151323
2cad7cbd96df91cad3d98d667c66cfe61555d5d8	multisource data fusion with multiple self-organizing maps	kohonen self organizing map;complex characteristics;tratamiento datos;teledetection;self organizing feature maps artificial neural networks brain modeling australia biological system modeling statistical distributions neural networks remote sensing solid modeling;high dimensionality;image processing;neural networks;multisource data fusion;building block;helium;biological system modeling;joint exploration;disparity;data processing;imagerie;image classification;traitement donnee;data fusion;compound classification;classification;traitement image;deteccion a distancia;geophysical measurement technique;artificial neural networks;brain modeling;statistical distributions;imagery;neural net;geophysics computing;self organising feature maps;geophysical signal processing;design framework;self organizing feature maps;terrain mapping geophysical techniques geophysical signal processing geophysics computing self organising feature maps remote sensing image classification image processing sensor fusion;fusion donnee;remote sensing;solid modeling;self organization;self organized map;land surface;imagineria;joint exploration geophysical measurement technique land surface terrain mapping remote sensing image processing multisource data fusion sensor fusion multiple self organizing map neural network neural net compound classification image classification kohonen self organizing map complex characteristics disparity;terrain mapping;sensor fusion;reseau neuronal;clasificacion;red neuronal;geophysical techniques;australia;neural network;multiple self organizing map	This paper presents a self-organizing neural network approach, known as multiple self-organizing maps (MSOM’s), to multisource data fusion and compound classification. We use the Kohonen SOM as a building block to set up a design framework for a range of classifiers. We demonstrate that the MSOM is suitable for multisource fusion, where the issues of high dimensionality, complex characteristics and disparity, and joint exploration of spatiality and temporality of mixed data can be adequately addressed. Experiments with a bitemporal data set show the effectiveness of our approach.	artificial neural network;binocular disparity;experiment;organizing (structure);self-organization;self-organizing map;temporal database	Weijian Wan;Donald Fraser	1999	IEEE Trans. Geoscience and Remote Sensing	10.1109/36.763298	computer vision;machine learning;sensor fusion;artificial neural network;remote sensing	Robotics	32.37911350971644	-43.37733856957808	151357
5cb5207759597a5d293a331af73e1ace574a4ba4	plug-and-play correlation filters for visual tracking		Visual tracking is a challenging vision task due to the complex scenes in real scenarios. Recently, correlation filter (CF) based trackers have achieved impressive performance, showing high frame rates. An increasing research focuses on designing complex prior regularization to enforce constraints on filters. However, it is hard to design a single CF formulations to address different complex tracking scenarios. In this paper, we present a Plug-and-Play Correlation Filters (PPCF) framework that iteratively integrate different off-the-shelf CF trackers. Specifically, we incorporate plug-and-play mechanism into standard numerical optimization scheme and then introduce different CF trackers at each iteration. Extensive experiments show that our method achieves much better performance than existing single CF-based tracker and also outperforms the state-of-the-art trackers.	benchmark (computing);bittorrent tracker;compactflash;experiment;iteration;mathematical optimization;plug and play;video tracking	Qianru Chen;Risheng Liu;Xin Fan;Haojie Li	2018		10.1145/3240876.3240899	frame rate;bittorrent tracker;computer vision;correlation;eye tracking;computer science;artificial intelligence;plug and play	Vision	33.56379761067626	-47.4123472743065	151717
4ed2d7ecb34a13e12474f75d803547ad2ad811b2	common action discovery and localization in unconstrained videos		Similar to common object discovery in images or videos, it is of great interests to discover and locate common actions in videos, which can benefit many video analytics applications such as video summarization, search, and understanding. In this work, we tackle the problem of common action discovery and localization in unconstrained videos, where we do not assume to know the types, numbers or locations of the common actions in the videos. Furthermore, each video can contain zero, one or several common action instances. To perform automatic discovery and localization in such challenging scenarios, we first generate action proposals using human prior. By building an affinity graph among all action proposals, we formulate the common action discovery as a subgraph density maximization problem to select the proposals containing common actions. To avoid enumerating in the exponentially large solution space, we propose an efficient polynomial time optimization algorithm. It solves the problem up to a user specified error bound with respect to the global optimal solution. The experimental results on several datasets show that even without any prior knowledge of common actions, our method can robustly locate the common actions in a collection of videos.	affinity analysis;entropy maximization;expectation–maximization algorithm;feasible region;internationalization and localization;mathematical optimization;p (complexity);time complexity;video content analysis	Jiong Yang;Junsong Yuan	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.237	automatic summarization;time complexity;artificial intelligence;robustness (computer science);pattern recognition;computer science;maximization;graph;analytics	Vision	33.68989646347268	-41.028972573605245	151757
62a20e0181bcf16b6836fb60c28c1dd4e8163cee	estimating the lecturer's head pose in seminar scenarios - a multi-view approach	head pose estimation;automatic detection;neural network	In this paper, we present a system to track the horizontal head orientation of a lecturer in a smart seminar room, which is equipped with several cameras. We automatically detect and track the face of the lecturer and use neural networks to classify his or her face orientation in each camera view. By combining the single estimates of the speaker’s head orientation from multiple cameras into one joint hypothesis, we improve overall head pose estimation accuracy. We conducted experiments on annotated recordings from real seminars. Using the proposed fully automatic system we are able to correctly determine the lecturer’s head pose in 59% of the time and for 8 orientation classes. In 92% of the time, the correct pose class or a neighbouring pose class (i.e. a 45 degree error) were estimated.	3d pose estimation;artificial neural network;experiment;multi-user;sensor	Michael Voit;Kai Nickel;Rainer Stiefelhagen	2005		10.1007/11677482_20	computer vision;simulation;speech recognition;pose;3d pose estimation;computer science;machine learning;artificial neural network	Vision	36.58774477351614	-48.46860291299958	151792
2b5e29fec3def8b18e6208643c13fc50ad1ac8d7	a new generative feature set based on entropy distance for discriminative classification	score function;generic model;hidden markov model;information space;feature vector;experimental validation;mixture of gaussians;support vector machine	Score functions induced by generative models extract fixeddimensions feature vectors from different-length data observations by subsuming the process of data generation, projecting them in highly informative spaces called score spaces. In this way, standard discriminative classifiers such as support vector machines, or logistic regressors are proved to achieve higher performances than a solely generative or discriminative approach. In this paper, we present a novel score space that capture the generative process encoding it in an entropic feature vector. In this way, both uncertainty in the generative model learning step and “local” compliance of data observations with respect to the generative process can be represented. The proposed score space is presented for hidden Markov models and mixture of gaussian and is experimentally validated on standard benchmark datasets; moreover it can be applied to any generative model. Results show how it achieves compelling classification accuracies.	benchmark (computing);discriminative model;encode;experiment;feature vector;generative model;hidden markov model;information;markov chain;performance;support vector machine	Alessandro Perina;Marco Cristani;Umberto Castellani;Vittorio Murino	2009		10.1007/978-3-642-04146-4_23	support vector machine;generative topographic map;feature vector;computer science;machine learning;pattern recognition;mixture model;mathematics;score;generative model;hidden markov model;discriminative model;statistics	ML	25.007431745931722	-46.61794409064525	151932
1402061de3982b8d217d91441ed316141e7d5fb1	extended common molecular and discriminative atom dictionary based sparse representation for face recognition	extended dictionary;face recognition;principal component analysis;maximum probability;sparse representation	An extended common molecular and discriminative atom dictionary based sparse representation method is proposed.We propose a very simple way to produce a multiple view representation dictionary.Face recognition is performed by combination of common information and discriminative information.We demonstrated that adding the common information is effective. The employed dictionary plays an important role in sparse representation classification, however how to build the relationship between dictionary atoms and class labels is still an important open question. Many existing sparse representation classification dictionary models exploit only the discriminative information either in the representation coefficients or in the representation residual, which limits their performance. To address this issue, we introduce a novel dictionary building method which is constructed by two parts: the common molecular dictionary and the discriminative atom dictionary. More specifically, the discriminative atom dictionary builds its relationship to class labels and the extended molecular dictionary can reduce the representation residual for all the classes. Therefore, the new dictionary not only has correspondence to the class labels, but also has the perfect representation ability. Besides, the maximum probability representation is used for the final classification. In conclusion, the sparse coefficient of our method is sparser than the sparse representation-based classification (SRC), and our method can achieve better performance. Experiments on the AR, Extended Yale B and CMU PIE face datasets verify that our algorithm outperforms many recently proposed methods.	atom;dictionary;facial recognition system;sparse approximation;sparse matrix	Zheng-ping Hu;Fan Bai;Shuhuan Zhao;Meng Wang;Zhe Sun	2016	J. Visual Communication and Image Representation	10.1016/j.jvcir.2016.05.019	facial recognition system;speech recognition;k-svd;computer science;machine learning;pattern recognition;sparse approximation;principal component analysis	Vision	26.34867767423983	-43.906731640683134	151953
1a48f35ff904ff9b4e21f98e7535684c106e8a7c	tensor representation and manifold learning methods for remote sensing images	null	One of the main purposes of earth observation is to extract in terested information and knowledge from remote sensing (RS) images with high efficiency and accuracy. Howev er, with the development of RS technologies, RS system provide images with higher spatial and temporal reso lution and more spectral channels than before, and it is inefficient and almost impossible to manually interpret the se images. Thus, it is of great interests to explore automati c and intelligent algorithms to quickly process such massive RS data with high accuracy. This thesis targets to develop some efficient information extraction algorithms for RS ima ges, by relying on the advanced technologies in machine learning. More precisely, we adopt the manifold learning al orithms as the mainline and unify the regularization theory, tensor-based method, sparse learning and transfer learning into the same framework. The main contributions of this thesis are as follows.	algorithm;information extraction;machine learning;nonlinear dimensionality reduction;reed–solomon error correction;sparse matrix	Lefei Zhang	2014	CoRR		computer vision;computer science;machine learning;pattern recognition;data mining	ML	30.170193991277095	-44.640728308342105	151999
670ceab14f1db641be2433cc3758f3c130f20390	semi-supervised multiview feature selection with label learning for vhr remote sensing images	image segmentation;support vector machines indexes feature extraction shape spatial resolution image segmentation;support vector machines;indexes;shape;feature extraction;remote sensing feature selection image processing learning artificial intelligence;worldview 2 vhr satellite image semisupervised multiview feature selection semimfs method label learning vhr remote sensing images very high resolution images;view generation semisupervised multiview feature selection l 1 2 norm;spatial resolution	The very high resolution (VHR) images can be seen as multiview data. For better organizing and highlighting similarities and differences between the multiple views of data, a semisupervised multiview feature selection (SemiMFS) method is proposed in this paper, based on consensus and complementary principles. In SemiMFS, feature views are generated by decomposing features into multiple disjoint and meaningful groups. Each feature group represents a view, and each view describes a data characteristic. Then features are evaluated and selected within each view. The experiments on a Worldview-2 VHR satellite image verify the effectiveness and practicability of the method, compared with traditional single-view algorithms.	algorithm;experiment;feature selection;image resolution;organizing (structure);semi-supervised learning;semiconductor industry	Xi Chen;Wei Liu;Fulin Su;Guofan Shao	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7729612	database index;support vector machine;computer vision;image resolution;feature extraction;shape;computer science;machine learning;pattern recognition;image segmentation;feature;remote sensing	Robotics	30.562342908108707	-44.337637250883034	152003
9bd9050c53d90dfa86cb22501812afe6fc897406	fine-grained and layered object recognition	object recognition;fine grained and layered recognition;online learning;structured prediction;object detection	This paper presents a novel research on promoting the performance and enriching the functionalities of object recognition. Instead of simply ̄tting various data to a few prede ̄ned semantic object categories, we propose to generate proper results for di®erent object instances based on their actual visual appearances. The results can be ̄ne-grained and layered categorization along with absolute or relative localization. We present a generic model based on structured prediction and an e±cient online learning algorithm to solve it. Experiments on a new benchmark dataset demonstrate the e®ectiveness of our model and its superiority against traditional recognition methods.	algorithm;benchmark (computing);categorization;emoticon;instance (computer science);loss function;object detection;open road tolling;outline of object recognition;structured prediction;top-down and bottom-up design	Yang Wu;Nanning Zheng;Yuanliu Liu;Zejian Yuan	2012	IJPRAI	10.1142/S0218001412550063	computer vision;method;object model;computer science;viola–jones object detection framework;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;structured prediction;3d single-object recognition	Vision	26.170152590807096	-48.415115949615675	152450
f25b9aed37614aae007fc876f31eed0595ab9cd0	modeling context between objects for referring expression understanding		Referring expressions usually describe an object using properties of the object and relationships of the object with other objects. We propose a technique that integrates context between objects to understand referring expressions. Our approach uses an LSTM to learn the probability of a referring expression, with input features from a region and a context region. The context regions are discovered using multipleinstance learning (MIL) since annotations for context objects are generally not available for training. We utilize max-margin based MIL objective functions for training the LSTM. Experiments on the Google RefExp and UNC RefExp datasets show that modeling context between objects provides better performance than modeling only object properties. We also qualitatively show that our technique can ground a referring expression to its referred region along with the supporting context region.	long short-term memory;regular expression	Varun K. Nagaraja;Vlad I. Morariu;Larry S. Davis	2016		10.1007/978-3-319-46493-0_48	computer vision;computer science;multimedia;context model	Vision	32.256364996231575	-48.33145577763073	152684
b73dacc588fde30c856c8634d0e770ed80cdf639	denoising prior driven deep neural network for image restoration		Deep neural networks (DNNs) have shown very promising results for various image restoration (IR) tasks. However, the design of network architectures remains a major challenging for achieving further improvements. While most existing DNN-based methods solve the IR problems by directly mapping low quality images to desirable high-quality images, the observation models characterizing the image degradation processes have been largely ignored. In this paper, we first propose a denoising-based IR algorithm, whose iterative steps can be computed efficiently. Then, the iterative process is unfolded into a deep neural network, which is composed of multiple denoisers modules interleaved with back-projection (BP) modules that ensure the observation consistencies. A convolutional neural network (CNN) based denoiser that can exploit the multi-scale redundancies of natural images is proposed. As such, the proposed network not only exploits the powerful denoising ability of DNNs, but also leverages the prior of the observation model. Through end-to-end training, both the denoisers and the BP modules can be jointly optimized. Experimental results on several IR tasks, e.g., image denoisig, super-resolution and deblurring show that the proposed method can lead to very competitive and often state-of-the-art results on several IR tasks, including image denoising, deblurring and super-resolution.	algorithm;architecture as topic;artificial neural network;basis pursuit denoising;biological neural networks;circuit restoration;convergence;convolutional neural network;deblurring;elegant degradation;emoticon;end-to-end principle;expanded memory;gradient;image noise;image restoration;iterative method;mathematical optimization;network architecture;noise (electronics);noise reduction;numerous;peak signal-to-noise ratio;projection defense mechanism;scott continuity;stationary process;subderivative;subgradient method;super-resolution imaging	Weisheng Dong;Peiyao Wang;Wotao Yin;Guangming Shi;Fangfang Wu;Xiaotong Lu	2018	IEEE transactions on pattern analysis and machine intelligence	10.1109/TPAMI.2018.2873610	artificial intelligence;network architecture;convolutional neural network;image restoration;computer vision;noise reduction;artificial neural network;iterative and incremental development;computer science;deblurring	Vision	25.2452381031629	-51.28435012068181	152815
19ae769aa067b5bd781882cd18551f7abe01bcf1	data generation for improving person re-identification		In this paper, we explore ways to address the challenges such as data bias caused by the lack of data on person re-identification problem. We propose a data generation framework from both intra- and inter-view aspects for data augmentation to advance the performance of the existing person re-identification algorithms. Specifically, for intra-view data generation, the proposed method generates useful predicted sequences within a camera view for certain person data expansion. The generated sequences well preserve the movement information of the camera and objects, which expands the original data with longer sequence length to tackle the problem caused by insufficient data from the root. For more challenging datasets which suffer from background clutters, we propose an inter-view image generation with automatic end-to-end background substitution to eliminate the influence by the background and increase the diversity of the training data as well, which makes the recognition system learn to focus on the regions of objects and image features related to identity. We then propose a flexible data augmentation method based on our data generation approaches to improve the performance of the person re-identification and analyze the advantages and applicability of these approaches respectively. Evaluated on the challenging re-id datasets, our method outperforms existing state-of-the-art approaches without any network structure modification on the baseline neural network. Cross-datasets evaluation results show that our method has favorable generalization ability and is potentially helpful for solving similar recognition tasks due to the common issue of insufficient data.	algorithm;artificial neural network;baseline (configuration management);convergence insufficiency;convolutional neural network;end-to-end principle;feature extraction;glossary of computer graphics;linux intrusion detection system	Lin Chen;Hua Yang;Shuang Wu;Zhiyong Gao	2017		10.1145/3123266.3123302	feature (computer vision);artificial neural network;test data generation;computer science;training set;machine learning;artificial intelligence	AI	30.010224341737715	-51.968369897697954	152907
d2f0b7392be6ec4d753c9414d57bfc18dcc407a0	human arm pose modeling with learned features using joint convolutional neural network	elbow;training;joints;computational modeling;estimation;feature extraction;graphics processing units;dynamic programming inference human arm pose modeling learned features joint convolutional neural network arm pose configuration color images arm part structure constraints energy model multiscaled images feature extraction;joints training estimation computational modeling feature extraction elbow graphics processing units;pose estimation dynamic programming feature extraction image colour analysis neural nets	This paper proposes a new approach to model human arm pose configuration from still images based on learned features and arm part structure constraints. The subjects in still images have no assumption with regards to clothing style, action category and background, so our model has to accommodate these uncertainties. Proposed approach uses an energy model that incorporates the dependence relationships among arm joints and arm parts, where the potentials represent their occurrence probabilities. Positive and negative instances are computed from input image, using multi-scale image patches to capture the details of arm joints and arm parts. A joint convolutional neural network is then developed for feature extraction. Local rigidity of arm part is used to constrain occurrence of arm joints and arm parts, and these constraints can be efficiently incorporated in dynamic programming for human arm pose inference. Our experimental results show better performance than alternative approaches using hand-crafted features for various still images.	artificial neural network;central processing unit;convolutional neural network;dynamic programming;flic (file format);feature extraction;feature vector;graphics processing unit;mathematical optimization;tesla (microarchitecture)	Chongguo Li;Nelson H. C. Yung;Xing Sun;Edmund Y. Lam	2016	Machine Vision and Applications	10.1007/s00138-016-0796-0	computer vision;estimation;feature extraction;computer science;machine learning;pattern recognition;articulated body pose estimation;computational model;statistics	AI	35.48729250597798	-47.09897821978364	152923
3aa5865f1c0059512b0977004ba5de2c81e90e9a	neighborhood preserving non-negative tensor factorization for image representation	image clustering non negative matrix and tensor factorization locally linear embedding manifold regularization image representation;locally linear embedding;vectors geometry image representation learning artificial intelligence tensors;tensile stress;manifolds;tensile stress manifolds vectors image representation matrix decomposition image reconstruction accuracy;geometry;会议论文;image clustering;accuracy;non negative matrix and tensor factorization;vectors;matrix decomposition;image representation;image reconstruction;learning artificial intelligence;npntf neighborhood preserving nonnegative tensor factorization image representation nonnegative matrix factorization nmf semantic interpretability vectorization local geometrical information manifold learning;manifold regularization;tensors	Non-negative Matrix Factorization (NMF) has become a powerful tool for image representation due to its enhanced semantic interpretability under non-negativity. Unfortunately, two types of neighborhood information essential to representation are lost in NMF. For individual image, the local structure information is missing in the vectorization, which can then be avoided by Non-negative Tensor Factorization (NTF). For image data points, they often reside on a low dimensional submanifold embedded in a high dimensional ambient space. NMF and NTF are incapable of encoding the local geometrical information, which can nevertheless be resuscitated by manifold learning. To simultaneously model both of the neighborhood relationship within and among image data, this paper proposes a novel algorithm called Neighborhood Preserving Non-negative Tensor Factorization (NPNTF) by incorporating locally linear embedding regularization into tensor factorization. Experimental results on image clustering show the superior performance of NPNTF with more natural and discriminating representation ability.	algorithm;arc diagram;automatic vectorization;cluster analysis;data point;embedded system;matrix regularization;national transfer format;negativity (quantum mechanics);non-negative matrix factorization;nonlinear dimensionality reduction	Yu-Xiong Wang;Liang-Yan Gui;Yu-Jin Zhang	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288643	iterative reconstruction;combinatorics;discrete mathematics;topology;tensor;manifold;mathematics;accuracy and precision;stress;matrix decomposition	Vision	26.783066028875698	-40.8661017272348	153024
8ca3cfb9595ebc5b36a25659f6bbf362f0b14ae3	spectral clustering based null space linear discriminant analysis (snlda)	linear discriminate analysis;spectral clustering;discriminant analysis;automatic detection;number of clusters;covariance matrix	While null space based linear discriminant analysis (NLDA) obtains a good discriminant performance, the ability easily suffers from an implicit assumption of Gaussian model with same covariance each class. Meanwhile, mixture model discriminant analysis, which is a good way for processing issues on multiple subclasses in each class, depends on human experience on the number of subclasses and has a highly complex iterative process. Considering the cons and pros of the two mentioned approaches, we therefore propose a new algorithm, called Spectral clustering based Null space Linear Discriminant Analysis (SNLDA). The main contributions of the algorithm include the following three aspects: 1) Employing a new spectral clustering method which can automatically detect the number of clusters in each class. 2) Finding a unified null space for processing multi-subclasses issues with eigen-solution technique. 3) Refining the calculation of the covariance matrix in a single sample subclass. The experimental results show the promising of the proposed SNLDA algorithm.	algorithm;cluster analysis;eigen (c++ library);feret (facial recognition technology);feret database;iterative method;kernel (linear algebra);linear discriminant analysis;mixture model;spectral clustering	Wenxin Yang;Junping Zhang	2007		10.1007/978-3-540-71701-0_34	covariance matrix;kernel fisher discriminant analysis;computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis;spectral clustering;multiple discriminant analysis;statistics	ML	25.02156553167988	-40.572086986157466	153038
f7c50d2be9fba0e4527fd9fbe3095e9d9a94fdd3	large margin multi-metric learning for face and kinship verification in the wild		Metric learning has been widely used in face and kinship verification and a number of such algorithms have been proposed over the past decade. However, most existing metric learning methods only learn one Mahalanobis distance metric from a single feature representation for each face image and cannot deal with multiple feature representations directly. In many face verification applications, we have access to extract multiple features for each face image to extract more complementary information, and it is desirable to learn distance metrics from these multiple features so that more discriminative information can be exploited than those learned from individual features. To achieve this, we propose a new large margin multi-metric learning (LML) method for face and kinship verification in the wild. Our method jointly learns multiple distance metrics under which the correlations of different feature representations of each sample are maximized, and the distance of each positive is less than a low threshold and that of each negative pair is greater than a high threshold, simultaneously. Experimental results show that our method can achieve competitive results compared with the state-of-the-art methods.	algorithm;computer vision;digital media;rose;video tracking	Junlin Hu;Jiwen Lu;Junsong Yuan;Yap-Peng Tan	2014		10.1007/978-3-319-16811-1_17	speech recognition;machine learning	AI	27.60241352590948	-48.34441421477089	153201
8b6a8fc33c4cc562af322827989b67fc37b71aa9	two specific multiple-level-set models for high-resolution remote-sensing image classification	bayesian theory;remote sensing image;belief networks;support vector machines belief networks feature extraction geophysical techniques geophysics computing image classification image fusion image segmentation remote sensing;high resolution;model combination;image segmentation;image resolution;support vector machines;object oriented classification;image fusion;level set;mansouri method image classification remote sensing image segmentation multiple level set framework tsmls model image fusion feature extraction geometrical characteristics achieve effective classification quadratic image energy gmmls model bayesian theory support vector machine;high resolution imagery;bayesian methods;image classification;multiple level set mls;texture features;data mining;multilevel systems;geophysics computing;tsmls model;feature extraction;remote sensing image classification multilevel systems bayesian methods image segmentation image resolution solid modeling feature extraction data mining support vector machines;very high resolution;remote sensing;solid modeling;unsupervised classification;feature fusion;support vector machine;high resolution remote sensing image;point of view;multiple level set framework;quadratic image energy;level set method;object oriented classification high resolution remote sensing image image segmentation multiple level set mls;geophysical techniques;geometrical characteristics;achieve effective classification;gmmls model;mansouri method	This letter adopts level-set methods in order to seek a novel classification strategy in which classification is free of segmentation. A region-driven multiple-level-set (MLS) framework is used to perform very high resolution image classification. Two specific unsupervised classification models are presented. First, from the point of view of feature fusion, an MLS model is suggested by fusing texture features and spectral information (TSMLS model). The model combines spectral information, texture features extracted from the image, and geometrical characteristics of closed curves to achieve effective classification for high-resolution imagery. Second, an alternative MLS model with quadratic image energy (GMMLS model) is presented, which can efficiently integrate the level-set method with Bayesian theory. The model benefits from both the level-set method and Bayesian theory and performs satisfactory classifications. The experiments have demonstrated that our methods can obtain better or similar classification results as compared to support vector machine and Mansouri's method.	algorithm;computer vision;experiment;image resolution;image segmentation;support vector machine;unsupervised learning	Hongchao Ma;Yun Yang	2009	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2009.2021166	support vector machine;computer vision;image resolution;bayesian probability;computer science;machine learning;pattern recognition	ML	31.374248641335186	-43.7211773393922	153259
23fe2ab6aa5f0a19e5dd6fabe1a589c8bcb67808	deep neural network for structural prediction and lane detection in traffic scene	biological patents;image recognition;biological neural networks visualization neurons convolution image recognition feature extraction image analysis;biomedical journals;pedestrian safety;text mining;poison control;europe pubmed central;convolution;injury prevention;edge detection recurrent neural nets;citation search;safety literature;traffic safety;injury control;citation networks;home safety;visualization;injury research;safety abstracts;human factors;research articles;abstracts;feature extraction;open access;occupational safety;safety;life sciences;clinical guidelines;recurrent neural networks image recognition pattern analysis;safety research;accident prevention;image analysis;violence prevention;bicycle safety;neurons;full text;image recognition pattern analysis recurrent neural networks;recurrent neural network multitask convolutional neural network traffic scenes lane boundary detection structured visual detection recurrent neuron layer region of interest geometric attributes multitask deep convolutional network traffic scene structural prediction;poisoning prevention;falls;ergonomics;rest apis;biological neural networks;suicide prevention;orcids;europe pmc;biomedical research;bioinformatics;literature search	Hierarchical neural networks have been shown to be effective in learning representative image features and recognizing object classes. However, most existing networks combine the low/middle level cues for classification without accounting for any spatial structures. For applications such as understanding a scene, how the visual cues are spatially distributed in an image becomes essential for successful analysis. This paper extends the framework of deep neural networks by accounting for the structural cues in the visual signals. In particular, two kinds of neural networks have been proposed. First, we develop a multitask deep convolutional network, which simultaneously detects the presence of the target and the geometric attributes (location and orientation) of the target with respect to the region of interest. Second, a recurrent neuron layer is adopted for structured visual detection. The recurrent neurons can deal with the spatial distribution of visible cues belonging to an object whose shape or structure is difficult to explicitly define. Both the networks are demonstrated by the practical task of detecting lane boundaries in traffic scenes. The multitask convolutional neural network provides auxiliary geometric information to help the subsequent modeling of the given lane structures. The recurrent neural network automatically detects lane boundaries, including those areas containing no marks, without any explicit prior knowledge or secondary modeling.	artificial neural network;biological neural networks;class;computer multitasking;convolutional neural network;deep learning;detector device component;detectors;drug vehicle;entity name part qualifier - adopted;feature extraction;mathematical induction;memory cell (binary);neural network simulation;neuron;neurons;random neural network;recurrent neural network;region of interest;sensor;vision;visual analytics	Jun Li;Xue Mei;Danil V. Prokhorov;Dacheng Tao	2017	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2016.2522428	visualization;feature extraction;computer science;suicide prevention;artificial intelligence;human factors and ergonomics;injury prevention;machine learning;data mining;deep learning;convolution	ML	31.145426717536733	-49.74366915886826	153408
a10ffa9164f18e0025a0403e9b13c5f9817a0496	video copy detection using multiple visual cues and mpeg-7 descriptors	visual ques;content based copy detection;subsequence matching;research paper;video copy detection;activity matching;time series analysis;visual cues;face detection;mpeg 7	1047-3203/$ see front matter Published by Elsevier doi:10.1016/j.jvcir.2010.07.001 * Corresponding author. E-mail addresses: kucuktunc.1@osu.edu (O. Kü edu.tr (M. Bas tan), gudukbay@cs.bilkent.edu.tr (U. Gü edu.tr (Ö. Ulusoy). 1 This work was done while the author was at Bilke We propose a video copy detection framework that detects copy segments by fusing the results of three different techniques: facial shot matching, activity subsequence matching, and non-facial shot matching using low-level features. In facial shot matching part, a high-level face detector identifies facial frames/ shots in a video clip. Matching faces with extended body regions gives the flexibility to discriminate the same person (e.g., an anchor man or a political leader) in different events or scenes. In activity subsequence matching part, a spatio-temporal sequence matching technique is employed to match video clips/segments that are similar in terms of activity. Lastly, the non-facial shots are matched using lowlevel MPEG-7 descriptors and dynamic-weighted feature similarity calculation. The proposed framework is tested on the query and reference dataset of CBCD task of TRECVID 2008. Our results are compared with the results of top-8 most successful techniques submitted to this task. Promising results are obtained in terms of both effectiveness and efficiency. Published by Elsevier Inc.	activity recognition;high- and low-level;logo;mpeg-7;microsoft windows;os-tan;picture-in-picture;video clip;video copy detection	Onur Küçüktunç;Muhammet Bastan;Ugur Güdükbay;Özgür Ulusoy	2010	J. Visual Communication and Image Representation	10.1016/j.jvcir.2010.07.001	computer vision;face detection;speech recognition;sensory cue;computer science;time series;pattern recognition;mathematics;statistics	Vision	35.40543294153255	-50.98179383288737	153432
790aa543151312aef3f7102d64ea699a1d15cb29	confidence-weighted local expression predictions for occlusion handling in expression recognition and action unit detection	facial expressions;action unit;random forest;occlusions;autoencoder;real-time	Fully-automatic facial expression recognition (FER) is a key component of human behavior analysis. Performing FER from still images is a challenging task as it involves handling large interpersonal morphological differences, and as partial occlusions can occasionally happen. Furthermore, labelling expressions is a time-consuming process that is prone to subjectivity, thus the variability may not be fully covered by the training data. In this work, we propose to train random forests upon spatially-constrained random local subspaces of the face. The output local predictions form a categorical expression-driven high-level representation that we call local expression predictions (LEPs). LEPs can be combined to describe categorical facial expressions as well as action units (AUs). Furthermore, LEPs can be weighted by confidence scores provided by an autoencoder network. Such network is trained to locally capture the manifold of the non-occluded training data in a hierarchical way. Extensive experiments show that the proposed LEP representation yields high descriptive power for categorical expressions and AU occurrence prediction, and leads to interesting perspectives towards the design of occlusion-robust and confidence-aware FER systems.	autoencoder;experiment;high- and low-level;random forest;spatial variability	Arnaud Dapogny;Kevin Bailly;Séverine Dubuisson	2017	International Journal of Computer Vision	10.1007/s11263-017-1010-1	artificial intelligence;machine learning;mathematics;statistics	Vision	31.95487201274467	-48.91414028671353	153467
b1b4b030b405b35689f6405e400697a2701e8119	facial action unit intensity prediction via hard multi-task metric learning for kernel regression	icc facial action unit intensity prediction hard multitask metric learning kernel regression multitask formulation optimal linear subspace squared error reduction nadaraya watson estimator mlkr algorithm hard mt mlkr same complexity predictors overfitting issues facial expression recognition and analysis challenge fera 15 intraclass correlation coefficient;regression analysis emotion recognition face recognition learning artificial intelligence;kernel training support vector machines feature extraction measurement databases mouth	The problem of learning several related tasks has recently been addressed with success by the so-called multi-task formulation, that discovers underlying common structure between tasks. Metric Learning for Kernel Regression (MLKR) aims at finding the optimal linear subspace for reducing the squared error of a Nadaraya-Watson estimator. In this paper, we propose two Multi-Task extensions of MLKR. The first one is a direct application of multi-task formulation to MLKR algorithm and the second one, the so-called Hard-MT-MLKR, lets us learn same-complexity predictors with fewer parameters, reducing overfitting issues. We apply the proposed method to Action Unit (AU) intensity prediction as a response to the Facial Expression Recognition and Analysis challenge (FERA'15). Our system improves the baseline results on the test set by 24% in terms of Intraclass Correlation Coefficient (ICC).	algorithm;ambiguous name resolution;baseline (configuration management);coefficient;computer multitasking;concatenation;correlation does not imply causation;machine learning;overfitting;relevance;smart;test set	Jérémie Nicolle;Kevin Bailly;Mohamed Chetouani	2015	2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)	10.1109/FG.2015.7284868	machine learning;pattern recognition;mathematics;statistics	Vision	24.81058755959894	-45.9627407301991	153495
5fd898c0f913d872c6e66398a62512b29ab3bd2e	signature verification based on fusion of on-line and off-line kernels	signature database svc2004 signature verification online kernels off line kernels kernel based methodology pattern recognition svm principle machine learning;kernel based methodology;online kernels;handwriting recognition;signature verification;image fusion;handwriting recognition kernel error analysis testing pattern recognition support vector machines machine learning databases symmetric matrices biometrics;machine learning;off line kernels;visual databases handwriting recognition image fusion pattern recognition;svm principle;pattern recognition;error rate;signature database svc2004;visual databases	The problem of signature verification is considered within the bounds of the kernel-based methodology of pattern recognition, more specifically, SVM principle of machine learning. A kernel in the set of signatures can be defined in different ways and it is impossible to choose the most appropriate kernel a priori. We propose a principle of fusing several on-line and off-line kernels into an entire training and verification technique. Experiments with signature database SVC2004 have shown that the multi-kernel approach essentially decreases the error rate in comparison with verification based on single kernels.	antivirus software;kernel (operating system);machine learning;online and offline;pattern recognition	Vadim Mottl;Mikhail Lange;Valentina Sulimova;Alexey Yermakov	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761208	kernel method;speech recognition;word error rate;computer science;machine learning;pattern recognition;handwriting recognition;tree kernel;image fusion;signature recognition	Vision	31.155303153974593	-41.642528914145636	153649
60678e1c9727a1f67fbe8bb46a914cc5bf6f39aa	is my object in this video? reconstruction-based object search in videos		This paper addresses the problem of video-level object instance search, which aims to retrieve the videos in the database that contain a given query object instance. Without prior knowledge about “when” and “where” an object of interest may appear in a video, determining “whether” a video contains the target object is computationally prohibitive, as it requires exhaustively matching the query against all possible spatial-temporal locations in each video that an object may appear. To alleviate the computational and memory cost, we propose the Reconstruction-based Object SEarch (ROSE) method. It characterizes a huge corpus of features of possible spatial-temporal locations in the video into the parameters of the reconstruction model. Since the memory cost of storing reconstruction model is much less than that of storing features of possible spatial-temporal locations in the video, the efficiency of the search is significantly boosted. Comprehensive experiments on three benchmark datasets demonstrate the promising performance of the proposed ROSE method.	benchmark (computing);digital media;experiment;futures studies;rose;text corpus;tier 2 network	Tan Yu;Jingjing Meng;Junsong Yuan	2017		10.24963/ijcai.2017/635	video tracking;computer science;artificial intelligence;computer vision;deep-sky object	AI	32.565322461913766	-51.382930571256686	153658
631483c15641c3652377f66c8380ff684f3e365c	sync-draw: automatic video generation using deep recurrent attentive architectures		This paper introduces a novel approach for generating videos called Synchronized Deep Recurrent Attentive Writer (Sync-DRAW). Sync-DRAW can also perform text-to-video generation which, to the best of our knowledge, makes it the first approach of its kind. It combines a Variational Autoencoder(VAE) with a Recurrent Attention Mechanism in a novel manner to create a temporally dependent sequence of frames that are gradually formed over time. The recurrent attention mechanism in Sync-DRAW attends to each individual frame of the video in sychronization, while the VAE learns a latent distribution for the entire video at the global level. Our experiments with Bouncing MNIST, KTH and UCF-101 suggest that Sync-DRAW is efficient in learning the spatial and temporal information of the videos and generates frames with high structural integrity, and can generate videos from simple captions on these datasets.	encoder;experiment;mnist database;recurrent word;structural integrity and failure;variational principle	Gaurav Mittal;Tanya Marwah;Vineeth N. Balasubramanian	2017		10.1145/3123266.3123309	pattern recognition;sync;artificial intelligence;machine learning;autoencoder;computer vision;multimedia;big data;deep learning;computer science;mnist database	ML	27.468199972924	-52.06254526185998	153670
13aac86217231a7d118ecdff444ee07234fcff50	classification via incoherent subspaces	subspace learning;indexing terms;classification;alternating projections;theoretical analysis;dictionary learning;feature selection;grassmannian manifolds;lts2;alternate projections	This article presents a new classification framework that can extract individual features per class. The scheme is based on a model of incoherent subspaces, each one associated to one class, and a model on how the elements in a class are represented in this subspace. After the theoretical analysis an alternate projection algorithm to find such a collection is developed. The classification performance and speed of the proposed method is tested on the AR and YaleB databases and compared to that of Fisher’s LDA and a recent approach based on on `1 minimisation. Finally connections of the presented scheme to already existing work are discussed and possible ways of extensions are pointed out.	algorithm;ar (unix);database	Karin Schnass;Pierre Vandergheynst	2010	CoRR		index term;biological classification;computer science;machine learning;pattern recognition;feature selection;world wide web	Vision	24.880833512883548	-41.58649959264654	153695
e5e6c4200ecf594afc0dbb51dc56bdc8d69817aa	fast human activity recognition based on structure and motion	surveillance;activity;recognition;structure and motion;human activity recognition;article;human activity;activity recognition	In this paper, we present a method for the recognition of human activities. The proposed approach is based on the construction of a set of templates for each activity as well as on the measurement of the motion in each activity. Templates are designed so that they capture the structural and motion information that is most discriminative among activities. The direct motion measurements capture the amount of translational motion in each activity. The two features are fused at the recognition stage. Recognition is achieved in two steps by calculating the similarity between the templates and the motion features of the test and reference activities. The proposed methodology yields excellent results when applied on the INRIA database.	activity recognition	Jinhui Hu;Nikolaos V. Boulgouris	2011	Pattern Recognition Letters	10.1016/j.patrec.2011.07.013	computer vision;speech recognition;thermodynamic activity;activity recognition	Vision	36.68704565818029	-49.79546247512534	154004
f827c2d22bf74ba6047e265680a48f7279ea24e4	multi-camera trajectory mining: database and evaluation	pedestrian trajectory multicamera trajectory mining large scale video search large scale video mining nonoverlapping multicamera network visual surveillance criminal investigation pedestrian transition macroscopic analysis tmin database image extraction video surveillance data collection;video surveillance cameras data mining pedestrians;cameras trajectory databases image color analysis data mining topology network topology	In recent years, large-scale video search and mining has been an active research area. Exploring the trajectory of pedestrian of interest in non-overlapping multi-camera network, namely the trajectory mining, is very useful for visual surveillance and criminal investigation. The trajectory mentioned in our work describes the transition of pedestrian among cameras from a macroscopic perspective which is different from the concept in conventional tracking field. In this paper, we collect a database called TMin to promote research and development of trajectory mining. This release of Version 1 contains 1680 images from 30 subjects, all the images are extracted from 6 surveillance videos over two hours, and each subject appears in at least two different cameras. We describe the apparatuses, environments and procedure of the data collection and present baseline performance on the TMin database.	baseline (configuration management);bidirectional texture function;loss function;optimization problem;principle of good enough	Yang Hu;Shengcai Liao;Dong Yi;Zhen Lei;Stan Z. Li	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.801	computer vision;simulation;data mining	Vision	38.567286279192054	-46.04856188869305	154067
f3abaf8a9e6e7450d37d5c2b180d3500292bb938	tangent space discriminant analysis for feature extraction	databases;eigenvalues and eigenfunctions;local tangent space;tsda;manifolds;local tangent space alignment;face recogntion manifold learning local tangent space alignment feature extraction;geometry;manifold learning;feature space;discriminant analysis;face recognition;feature extraction;principal component analysis;tangent space discriminant analysis;geometry manifolds face principal component analysis databases feature extraction eigenvalues and eigenfunctions;face;recognition rates;feature extraction face recognition;recognition rates tangent space discriminant analysis feature extraction tsda local tangent space;dimensional reduction;face recogntion	In this paper, a novel method called tangent space discriminant analysis is proposed for dimensionality reduction and feature extraction. Differing from the recently proposed manifold learning methods completely operating on raw feature space, TSDA completely uses the local tangent space to represent the local within-class geometry and local between-class geometry. Assume that the face images of different people reside on different intrinsically low-dimensional sub-manifolds, TSDA is developed to preserve the locality of each sub-manifold and simultaneously maximize the local separability of different sub-manifolds by using local tangent space alignment. Experimental results show that TSDA achieves higher recognition rates than a few the state-of-the-art techniques.	database;feature extraction;feature vector;linear discriminant analysis;linear separability;local tangent space alignment;locality of reference;nonlinear dimensionality reduction;return loss;supervised learning	Zhong Jin;Wai Keung Wong	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653530	facial recognition system;face;local tangent space alignment;topology;feature vector;manifold;feature extraction;computer science;machine learning;pattern recognition;mathematics;geometry;nonlinear dimensionality reduction;linear discriminant analysis;principal component analysis	Vision	27.16803303440029	-43.54860052305214	154139
24830e3979d4ed01b9fd0feebf4a8fd22e0c35fd	high-resolution comprehensive 3-d dynamic database for facial articulation analysis	databases;high resolution;image resolution;visual databases face recognition image resolution image sequences;clinical diagnosis;observers;facial expression analysis;psychophysical experiment high resolution comprehensive 3d dynamic database facial articulation analysis complex nonverbal communication emotional state awareness state cognitive activity 3d imaging technology 3d sequence 3d dynamic facial articulation database clinical diagnosis facial dysfunction facial sequence age gender ethnicity computer based experiment;face recognition;civil engineering;solid modeling;databases observers humans face algorithm design and analysis solid modeling cameras;face;humans;algorithm design;algorithm design and analysis;cameras;everyday life;image sequences;visual databases;non verbal communication	Face is an essential medium for complex non-verbal communication between humans. Its articulation plays an important role in everyday life, including: manifestation of emotional and awareness states, cognitive activity, personality or wellbeing. With the advances in 3-D imaging technology and ever increasing computing power, automatic analysis of facial articulation using 3-D sequences has become viable. This paper describes a newly developed high-resolution, comprehensive 3-D dynamic facial articulation database which is designed not only for comparative studies of facial expression analysis but also to aid research into clinical diagnosis of facial dysfunctions. The database currently contains 3,360 facial sequences captured from 80 volunteers of various age, gender and ethnicity and will be made available to the wider research community in early autumn 2011. It has been validated using computer-based experiments as well as psychophysical experiments that were used to formally evaluate the accuracy of the recorded expressions. This database is believed to be one of the most comprehensive repositories of facial 3D dynamic articulations to date.	biconnected component;experiment;image resolution;imaging technology	Bogdan J. Matuszewski;Wei Quan;Lik-Kwan Shark	2011	2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)	10.1109/ICCVW.2011.6130511	facial recognition system;algorithm design;computer vision;image resolution;computer science;multimedia	Vision	37.74114958268789	-44.43021406337511	154157
a6aca4e90e4be99e64c42051de7e6c1abdc15aba	a psychological adaptive model for video analysis	iterative key frame selection;content based indexing;video signal processing;video shot;real time;heuristic method;video analysis;psychological adaptive model;feature extraction;key frame extraction;visual features;psychology motion pictures humans visual perception data mining iterative methods feature extraction videoconference frequency intelligent systems;video signal processing feature extraction;iterative key frame selection psychological adaptive model video analysis key frame extraction video shot visual features	"""Extracting key-frames is the first step for efficient content-based indexing, browsing and retrieval of the video data in commercial movies. Most of the existing research deals with """"how to extract representative frames?"""" However the unaddressed question is """"how many key-frames are required to represent a video shot properly?"""" Generally, the user defines this number a priori or some heuristic methods are used. In this paper, we propose a psychological model, which computes this number adaptively and online, from variation of visual features in a video-shot. We incorporate it with an iterative key-frame selection method to automatically select the key-frames. We compare the results of this method with two other well-known approaches, based on a novel effectiveness measure that scores each approach based on its representational power. Movie-clips of varying complexity are used to underscore the success of the proposed model in real-time"""	cognitive model;heuristic;iterative method;key frame;real-time clock	Nirmalya Ghosh;Bir Bhanu	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.156	video compression picture types;computer vision;speech recognition;feature extraction;computer science;video quality;machine learning;video tracking;block-matching algorithm;multimedia	Vision	38.62834855765891	-51.87071713959886	154175
6b5f0840c8b0b61642f55eaf4f00b83a6b270a7c	sparse observation (so) alignment for sign language recognition	sign language recognition;rgb d data;hidden markov model;期刊论文;stable marriage problem;dynamic time warping	"""In this paper, we propose a method for robust Sign Language Recognition from RGB-D data. A Sparse Observation (SO) description is proposed to character each sign in terms of the typical hand postures. Concretely speaking, the SOs are generated by considering the typical posture fragments, where hand motions are relatively slow and hand shapes are stable. Thus the matching between two sign words is converted to measure the similarity computing between two aligned SO sequences. The alignment is formulated as a variation of Stable Marriage Problem (SMP). The classical """"propose-engage"""" idea is extended to get the order preserving matched SO pairs. In the training stage, the multiple instances from one sign are fused to generate single SO template. In the recognition stage, SOs of each probe sign """"propose"""" to SOs of the templates for the purpose of reasonable similarity computing. To further speed up the SO alignment, hand posture relationship map is constructed as a strong prior to generate the distinguished low-dimensional feature of SO. Moreover, to get much better performance, the motion trajectory feature is integrated. Experiments on two large datasets and an extra Chalearn Multi-modal Gesture Dataset demonstrate that our algorithm has much higher accuracy with only 1/10 time cost compared with the HMM and DTW based methods."""	language identification;sparse	Hanjie Wang;Xiujuan Chai;Xilin Chen	2016	Neurocomputing	10.1016/j.neucom.2015.10.112	speech recognition;stable marriage problem;computer science;machine learning;dynamic time warping;hidden markov model	Vision	34.87999640682344	-48.70922912772461	154192
e83808340bf51c68102688635a0589fa6c11c6d9	a symbolic approach for classification of moving vehicles in traffic videos		In this paper, a symbolic approach is proposed to classify moving vehicles in traffic videos. A corner-based tracking method is presented to track and detect moving vehicles. We propose to overlap the boundary curves of each vehicle while tracking it in sequence of frames to reconstruct a complete boundary shape of the vehicle. The reconstructed boundary shape is normalized and then a set of efficient shape features are extracted. The extracted shape features are used to form interval-valued feature vector representation of vehicles. Vehicles are categorized into 4 different types of vehicle classes using a symbolic similarity measure. To corroborate the efficacy of the proposed method, experiment is conducted on 21,239 frames of roadway traffic videos taken in an uncontrolled environment during day time. The proposed method has 95.16% classification accuracy. Moreover, experiments reveal that the proposed method can be well adopted for on-line classification of moving vehicles as it is based on a simple matching scheme.	categorization;experiment;feature vector;online and offline;similarity measure;uncontrolled format string	D. S. Guru;Elham Dallalzadeh;S. Manjunath	2012			artificial intelligence;computer science;machine learning	Vision	37.68525873266591	-49.57867460116123	154498
1a008bb310e8e98fa6aa9c2bf6ab451c69ae8aa4	indoor scene recognition using task and saliency-driven feature pooling	indoor scene recognition	Indoor scene recognition is as of today one of the most challenging open problems in visual place categorization. Since the seminal works of Oliva and Torralba [5] and Lazebnik et al. [3], the mainstream approach to scene recognition has been based on global, appearance-based image representations, enriched with spatial information. This approach, in various forms, has given good results for the outdoor place recognition problem, but proved to be inadequate when dealing with indoor scenes [6]. In indoor environments, indeed, the location of meaningful regions and objects varies drastically within each category. Also, the close-up distance between the camera and the subject makes the variations due to viewpoint changes even more severe. In this scenario, it becomes crucial how low-level features are spatially pooled to get the final image description, especially for the robustness of the representation. In this work we investigate this issue and propose to combine a simple spatial encoding, with a saliency-driven perceptual pooling designed to capture structural properties of the scenes, independently from their position in the image.	categorization;high- and low-level	Marco Fornoni;Barbara Caputo	2012		10.5244/C.26.98	computer vision;computer science;machine learning	Vision	33.150427939866304	-51.09069770703078	154507
bdd8c3ca68a8eab1d8b256687fd1aef794141c20	a unifying framework for relational structure matching	minimisation;evolutionary computation;game theory;evolutionary game theory;maximum clique problem;replicator dynamic;discrete time;evolutionary computation computer vision pattern matching minimisation game theory;computer vision;continuous optimization;pattern matching;state space;pattern recognition;system development;computer vision pattern recognition minimization methods game theory pattern matching artificial intelligence solids hardware;energy minimization;evolutionary game theory relational structure matching computer vision pattern recognition state space search large maximal clique relational matching problem energy minimization continuous optimization replicator dynamical systems	The matching of relational structures is a problem that pervades computer vision and pattern recognition research. During the past few decades, two radically distinct approaches have been pursued to tackle it. The first views the matching problem as one of explicit search in state-space. The most popular method within this class consists of transforming it in the equivalent problem of finding a large maximal clique in a derived “association graph.” In the second approach, the relational matching problem is viewed as one of energy minimization. In this paper, we provide a unifying framework for relational structure matching which does unify the two existing approaches. The work is centered around a remarkable result proved by Motzkin and Straus which allows us to formulate the maximum clique problem in terms of a continuous optimization problem. We present a class of continuousand discrete-time “replicator” dynamical systems developed in evolutionary game theory and show how they can naturally be employed to solve our relational matching problem. Experiments are presented which demonstrate the effectiveness of the proposed approach.	clique (graph theory);clique problem;computer vision;continuous optimization;dynamical system;energy minimization;experiment;game theory;mathematical optimization;maximal set;optimization problem;pattern recognition;state space	Marcello Pelillo	1998		10.1109/ICPR.1998.711944	game theory;computer vision;minimisation;mathematical optimization;evolutionary game theory;discrete time and continuous time;combinatorics;computer science;state space;3-dimensional matching;machine learning;pattern matching;mathematics;continuous optimization;stable roommates problem;energy minimization;statistics;evolutionary computation	Vision	33.65900092277251	-40.71370317231397	154515
28610a00fa8d44b0f861fe2aa5a68d6249a57e2f	a novel iris weight map method for less constrained iris recognition based on bit stability and discriminability	intelligent control systems	In this paper, we propose and investigate a novel iris weight map method for iris matching stage to improve less constrained iris recognition. The proposed iris weight map considers both intra-class bit stability and inter-class bit discriminability of iris codes. We model the intra-class bit stability in a stability map to improve the intra-class matching. The stability map assigns more weight to the bits that have values more consistent with their noiseless and stable estimates obtained using a low rank approximation from a set of noisy training images. Also, we express the inter-class bit discriminability in a discriminability map to enhance the inter-class separation. We calculate the discriminability map using a 1-to-N strategy, emphasizing the bits with more discriminative power in iris codes. The final iris weight map is the combination of the stability map and the discriminability map. We conduct experimental analysis on four publicly available datasets captured in varying less constrained conditions. The experimental results demonstrate that the proposed iris weight map achieves generally improved identification and verification performance compared to stateof-the-art methods.	algorithmic efficiency;code;computation;experiment;glossary of computer graphics;iris recognition;linear model;low-rank approximation;map;nonlinear system	Yang Hu;Konstantinos Sirlantzis;W. Gareth J. Howells	2017	Image Vision Comput.	10.1016/j.imavis.2016.05.003	speech recognition;computer science;machine learning;pattern recognition	Vision	28.17815972625826	-45.99924191475238	154610
061e29eae705f318eee703b9e17dc0989547ba0c	enhancing expression recognition in the wild with unlabeled reference data	real-world data;original data;large scale data;unlabeled reference data;real-world expression recognition;enhancing expression recognition;wild expression data;robust expression recognition;facial expression recognition;unlabeled data;large scale;latest wild expression database	Facial expression recognition is an important task in humancomputer interaction. Some methods work well on ”lab-controlled” data. However, their performances degenerate dramatically on real-world data as expression covers large variations, including pose, illumination, occlusion, and even culture change. To deal with this problem, large scale data is definitely needed. On the other hand, collecting and labeling wild expression data can be difficult and time consuming. In this paper, aiming at robust expression recognition in wild which suffers from the mentioned problems, we propose a semi-supervised method to make use of the large scale unlabeled data in two steps: 1) We enrich reference manifolds using selected unlabeled data which are closed to certain kind of expression. The learned manifolds can help smooth the variation of original data and provide reliable metric to maintain semantic similarity of expression; 2) To elevate the original labeled set for enhanced training, we iteratively employ the semi-supervised clustering to assign labels for unlabeled data and add the most discriminant ones into the labeled set. Experiments on the latest wild expression database SFEW and GENKI show that the proposed method can effectively exploit unlabeled data to improve the performance on real-world expression recognition.	cluster analysis;database;discriminant;experiment;facial recognition system;nos;parsing expression grammar;performance;semantic similarity;semi-supervised learning;semiconductor industry	Mengyi Liu;Shaoxin Li;Shiguang Shan;Xilin Chen	2012		10.1007/978-3-642-37444-9_45	computer science;machine learning;pattern recognition;data mining	AI	28.13898638000689	-46.67407345463139	154611
8e79ae25736ac4953737c9a77957db661cd1d605	linear image coding for regression and classification using the tensor-rank principle	matrix representations;tensor rank coding;image recognition;matrix representation;image coding;tensile stress;images collection;tensor rank principle;svd decomposition;singular value decomposition;image classification;independent component analysis;matrix algebra;computer vision;functional approximation;training examples linear image coding tensor rank principle images collection image space matrix representations tensor rank approximation svd decomposition functional approximation functional classification tensor rank coding dimensionality reduction;training examples;face recognition;dimensionality reduction;image space;image representation singular value decomposition image coding matrix algebra image classification;matrix decomposition;tensor rank;image coding principal component analysis matrix decomposition computer science face recognition image recognition independent component analysis tensile stress decorrelation computer vision;image representation;functional classification;principal component analysis;linear image coding;tensor rank approximation;decorrelation;computer science	"""Given a collection of images (matrices) representing a """"class"""" of objects we present a method for extracting the commonalities of the image space directly from the matrix representations (rather than from the vectorized representation which one would normally do in a PCA approach, for example). The general idea is to consider the collection of matrices as a tensor and to look for an approximation of its tensor-rank. The tensor-rank approximation is designed such that the SVD decomposition emerges in the special case where all the input matrices are the repeatition of a single matrix. We evaluate the coding technique both in terms of regression, i.e., the efficiency of the technique for functional approximation, and classification. We find that for regression the tensor-rank coding, as a dimensionality reduction technique, significantly outperforms other techniques like PCA. As for classification, the tensor-rank coding is at is best when the number of training examples is very small."""		Amnon Shashua;Anat Levin	2001		10.1109/CVPR.2001.990454	facial recognition system;computer vision;discrete mathematics;computer science;machine learning;pattern recognition;mathematics;singular value decomposition	Vision	26.79948726939409	-41.556883760263	154803
cb5f9b1a436771b65bdf4e8fa3e82a4404e8d3c8	multilinear sparse principal component analysis	obj;yale;face databases;visual databases face recognition feature extraction principal component analysis;journal;face recognition technology;tensile stress principal component analysis vectors feature extraction optimization learning systems face recognition;face recognition;feature extraction;principal component analysis;multilinear sparse principal component analysis;weizmann action database;weizmann action database multilinear sparse principal component analysis mspca feature extraction tensor data sparse pca yale face recognition technology face databases coil 20 object database object images;mspca;journal magazine article;coil 20 object database;sparse projections dimensionality reduction face recognition feature extraction principal component analysis pca;visual databases	In this brief, multilinear sparse principal component analysis (MSPCA) is proposed for feature extraction from the tensor data. MSPCA can be viewed as a further extension of the classical principal component analysis (PCA), sparse PCA (SPCA) and the recently proposed multilinear PCA (MPCA). The key operation of MSPCA is to rewrite the MPCA into multilinear regression forms and relax it for sparse regression. Differing from the recently proposed MPCA, MSPCA inherits the sparsity from the SPCA and iteratively learns a series of sparse projections that capture most of the variation of the tensor data. Each nonzero element in the sparse projections is selected from the most important variables/factors using the elastic net. Extensive experiments on Yale, Face Recognition Technology face databases, and COIL-20 object database encoded the object images as second-order tensors, and Weizmann action database as third-order tensors demonstrate that the proposed MSPCA algorithm has the potential to outperform the existing PCA-based subspace learning algorithms.	algorithm;database;elastic net regularization;ephrin type-b receptor 1, human;experiment;f7 wt allele;feature extraction;machine learning;multilinear principal component analysis;projections and predictions;rewrite (programming);sparse matrix	Yong Xu;Qingcai Chen;Jian Yang;David Zhang	2014	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2013.2297381	facial recognition system;multilinear principal component analysis;feature extraction;computer science;machine learning;pattern recognition;data mining;principal component analysis	ML	25.93084098133735	-42.07761293883367	154956
2d8394d63e180417b717a5e1329853a6a43cca46	multitemporal urban area characterization through fuzzy neural networks	kernel based refinement;urban areas fuzzy neural networks neural networks subspace constraints spatial resolution satellites electronic mail image retrieval image resolution image classification;fuzzy neural network;art;fuzzy neural nets;electronic mail;fuzzy artmap;pixel by pixel classification;image processing;image resolution;neural networks;artmap;pixel classification;multitemporal characterization;image classification;neural classifier;city;geophysical measurement technique;subspace constraints;classifier;urban areas;neural net;geophysical signal processing;remote sensing;satellites;image sequence;town;fuzzy neural nets geophysical techniques geophysical signal processing terrain mapping image sequences image classification art neural nets;two step approach;urban area;land surface;terrain mapping;art neural nets;fuzzy neural networks;adaptive resonance theory geophysical measurement technique land surface terrain mapping remote sensing image sequence image processing urban area town city multitemporal characterization fuzzy neural network neural net artmap classifier image classification two step approach pixel by pixel classification kernel based refinement neural classifier art;geophysical techniques;adaptive resonance theory;image sequences;spatial resolution;image retrieval	This paper is devoted to the introduction of a fuzzy ARTMAP classifier based on a two-step approach. First, a pixel-by-pixel classification is performed, then a kernel-based refinement is applied to the output of the first step. The approach is applied to a multitemporal data set of a urban area, both to improve any single classification and to exploit the extra information carried by more images of the same area in different dates. We show that the fuzzy ARTMAP neural classifier is well suited to handle this kind of data. We discuss also the advantages deriving from the use of a multitemporal data set in urban areas, where many features are stable and so more data make their detection and recognition easier.	artificial neural network	Fabio Dell'Acqua;Paolo Gamba	2002		10.1109/IGARSS.2002.1025020	computer vision;image resolution;computer science;machine learning;data mining;artificial neural network	ML	32.2901174085536	-43.94315185487026	155077
cba07ba5830aaa9daaf1ed7fecf3298f28adba1e	face recognition using evolutionary pursuit	reconnaissance visage;analisis imagen;vision ordenador;algoritmo adaptativo;algoritmo genetico;computer vision;adaptive algorithm;face recognition;algorithme adaptatif;algorithme genetique;genetic algorithm;image analysis;vision ordinateur;vision active;analyse image;active vision	This paper describes a novel and adaptive dictionary method for face recognition using genetic algorithms (GAs) in determining the optimal basis for encoding human faces. In analogy to pursuit methods, our novel method is called Evolutionary Pursuit (EP), and it allows for diierent types of (non-orthogonal) bases. EP processes face images in a lower dimensional whitened PCA subspace. Directed but random rotations of the basis vectors in this subspace are searched by GAs where evolution is driven by a tness function deened in terms of performance accuracy and class separation (scatter index). Accuracy indicates the extent to which learning has been successful so far, while the scatter index gives an indication of the expected tness on future trials. As a result, our approach improves the face recognition performance compared to PCA, and shows better generalization abilities than the Fisher Linear Discriminant (FLD) based methods.	basis (linear algebra);dictionary;expectation propagation;facial recognition system;genetic algorithm;linear discriminant analysis	Chengjun Liu;Harry Wechsler	1998		10.1007/BFb0054767	computer vision;image analysis;genetic algorithm;active vision;computer science;artificial intelligence;machine learning	Vision	30.62051567581877	-40.780338365613744	155279
90aff4541b0f66f3427be52f3ef10d4015657c4b	a class of learning algorithms for principal component analysis and minor component analysis	eigenvalues;pattern recognition;learning (artificial intelligence);eigenvalue;convergence;differential equation;convergence of numerical methods;minor component analysis;eigenvectors;learning algorithms;differential equations;eigenvalues and eigenfunctions;principal component analysis;neural nets	Principal component analysis (PCA) and minor component analysis (MCA) are a powerful methodology for a wide variety of applications such as pattern recognition and signal processing. In this paper, we first propose a differential equation for the generalized eigenvalue problem.We prove that the stable points of this differential equation are the eigenvectors corresponding to the largest eigenvalue. Based on this generalized differential equation, a class of PCA and MCA learning algorithms can be obtained. We demonstrate that many existing PCA and MCA learning algorithms are special cases of this class, and this class includes some new and simpler MCA learning algorithms. Our results show that all the learning algorithms of this class have the same order of convergence speed, and they are robust to implementation error.		Qingfu Zhang;Yiu-Wing Leung	2000	IEEE transactions on neural networks	10.1109/72.839022	mathematical optimization;combinatorics;eigenvalues and eigenvectors;computer science;machine learning;signal processing;mathematics;differential equation	ML	24.621062096347806	-39.51411591503783	155288
11d1a1a5c427dbc443a242fbf84bace8e96b788e	large-scale video hashing via structure learning	minimisation;video structure;video signal processing;large scale;cryptography;gradient methods;video hashing;learning artificial intelligence	Recently, learning based hashing methods have become popular for indexing large-scale media data. Hashing methods map high-dimensional features to compact binary codes that are efficient to match and robust in preserving original similarity. However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. The structure information of videos, e.g., discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. We show that the minimization objective can be efficiently solved by an Accelerated Proximal Gradient (APG) method. Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-of-the-art hashing methods.	benchmark (computing);binary code;experiment;hash function;proximal gradient methods for learning;stochastic gradient descent;supervised learning;video clip	Guangnan Ye;Dong Liu;Jun Wang;Shih-Fu Chang	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.282	video compression picture types;feature hashing;computer vision;minimisation;hash table;dynamic perfect hashing;open addressing;computer science;cryptography;theoretical computer science;machine learning;universal hashing;video tracking;mathematics;locality preserving hashing;locality-sensitive hashing;statistics	Vision	24.72186612872452	-44.6200409011526	155298
81e82d0373e95274ed39dd4b6717609bf1d10a49	local deep hashing matching of aerial images based on relative distance and absolute distance constraints		Aerial images have features of high resolution, complex background, and usually require large amounts of calculation, however, most algorithms used in matching of aerial images adopt the shallow hand-crafted features expressed as floating-point descriptors (e.g., SIFT (Scale-invariant Feature Transform), SURF (Speeded Up Robust Features)), which may suffer from poor matching speed and are not well represented in the literature. Here, we propose a novel Local Deep Hashing Matching (LDHM) method for matching of aerial images with large size and with lower complexity or fast matching speed. The basic idea of the proposed algorithm is to utilize the deep network model in the local area of the aerial images, and study the local features, as well as the hash function of the images. Firstly, according to the course overlap rate of aerial images, the algorithm extracts the local areas for matching to avoid the processing of redundant information. Secondly, a triplet network structure is proposed to mine the deep features of the patches of the local image, and the learned features are imported to the hash layer, thus obtaining the representation of a binary hash code. Thirdly, the constraints of the positive samples to the absolute distance are added on the basis of the triplet loss, a new objective function is constructed to optimize the parameters of the network and enhance the discriminating capabilities of image patch features. Finally, the obtained deep hash code of each image patch is used for the similarity comparison of the image patches in the Hamming space to complete the matching of aerial images. The proposed LDHM algorithm evaluates the UltraCam-D dataset and a set of actual aerial images, simulation result demonstrates that it may significantly outperform the state-of-the-art algorithm in terms of the efficiency and performance.	aerial photography;algorithm;hamming space;hash function;image registration;image resolution;loss function;network model;optimization problem;overlap–add method;quantization (signal processing);real-time clock;scale-invariant feature transform;simulation;speeded up robust features;triplet state;window function	Suting Chen;Xin Li;Yanyan Zhang;Rui Feng;Chuang Zhang	2017	Remote Sensing	10.3390/rs9121244	network model;artificial intelligence;computer vision;geology;hamming space;deep learning;hash function;scale-invariant feature transform;binary number	Vision	26.116920300338673	-52.054561520650935	155376
2781d91e2497e635f54f5272890b32383c4347e0	blockwise coordinate descent schemes for sparse representation	parabolic equations encoding;会议论文;parabolic function blockwise coordinate descent schemes sparse representation framework sparse coding dictionary learning optimization learning optimizations learning process;sparse coding dictionary learning coordinate descent;dictionaries encoding optimization convergence sparse matrices minimization linear programming	The current sparse representation framework is to decouple it as two subproblems, i.e., alternate sparse coding and dictionary learning using different optimizers, treating elements in bases and codes separately. In this paper, we treat elements both in bases and codes ho-mogenously. The original optimization is directly decoupled as several blockwise alternate subproblems rather than above two. Hence, sparse coding and bases learning optimizations are coupled together. And the variables involved in the optimization problems are partitioned into several suitable blocks with convexity preserved, making it possible to perform an exact block coordinate descent. For each separable subproblem, based on the convexity and monotonic property of the parabolic function, a closed-form solution is obtained. Thus the algorithm is simple, efficient and effective. Experimental results show that our algorithm significantly accelerates the learning process.	algorithm;code;coordinate descent;dictionary;machine learning;mathematical optimization;neural coding;parabolic antenna;sparse approximation;sparse matrix	Bao-Di Liu;Yu-Xiong Wang;Bin Shen;Yu-Jin Zhang;Yanjiang Wang	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854608	coordinate descent;mathematical optimization;k-svd;theoretical computer science;machine learning;sparse approximation;mathematics	Vision	26.822489583719527	-40.217994384177196	155421
0ddf22f73971277549ab5ba8632890aa0a4369d3	face recognition using kernel direct discriminant analysis algorithms	classification error rate performance;eigenvalues and eigenfunctions;kernel;discriminatory power;small sample size;multiview umist face database;testing;linear discriminate analysis;kernel direct discriminant analysis algorithms;generalized discriminant analysis face recognition kernel direct discriminant analysis algorithms low dimensional feature representation discriminatory power principle component analysis linear discriminant analysis classification error rate performance multiview umist face database;principal component analysis face recognition eigenvalues and eigenfunctions;discriminant analysis;error analysis;kernel machine;face recognition;principal component analysis;classification error;error rate;face recognition kernel algorithm design and analysis linear discriminant analysis error analysis lighting principal component analysis robustness pattern analysis testing;kernel pca;robustness;generalized discriminant analysis;principle component analysis;pattern analysis;lighting;facial expression;low dimensional feature representation;linear discriminant analysis;algorithm design and analysis	"""Techniques that can introduce low-dimensional feature representation with enhanced discriminatory power is of paramount importance in face recognition (FR) systems. It is well known that the distribution of face images, under a perceivable variation in viewpoint, illumination or facial expression, is highly nonlinear and complex. It is, therefore, not surprising that linear techniques, such as those based on principle component analysis (PCA) or linear discriminant analysis (LDA), cannot provide reliable and robust solutions to those FR problems with complex face variations. In this paper, we propose a kernel machine-based discriminant analysis method, which deals with the nonlinearity of the face patterns' distribution. The proposed method also effectively solves the so-called """"small sample size"""" (SSS) problem, which exists in most FR tasks. The new algorithm has been tested, in terms of classification error rate performance, on the multiview UMIST face database. Results indicate that the proposed methodology is able to achieve excellent performance with only a very small set of features being used, and its error rate is approximately 34% and 48% of those of two other commonly used kernel FR approaches, the kernel-PCA (KPCA) and the generalized discriminant analysis (GDA), respectively."""	algorithm;anterior descending branch of left coronary artery;gnome-db;illumination (image);kernel (operating system);kernel method;kernel principal component analysis;linear discriminant analysis;nonlinear system;face recognition;synovial sarcoma	Juwei Lu;Konstantinos N. Plataniotis;Anastasios N. Venetsanopoulos	2003	IEEE transactions on neural networks	10.1109/TNN.2002.806629	computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis;statistics;principal component analysis	Vision	27.31577174543675	-42.79433250128834	155445
e5687f9584deca1fafb68b50fa79b9fcfbd1d379	zero-shot object recognition using semantic label vectors	computers;object recognition;zero shot learning;standards;vectors learning artificial intelligence natural language processing object recognition;training;semantics;transfer learning zero shot object recognition semantic label vectors object categories known object classes training images unknown object classes natural language processing object classes vector representations words vector representations;computer vision;training data;transfer learning;object recognition semantics training computers computer vision standards training data;transfer learning object recognition zero shot learning	"""We consider the problem of zero-shot recognition of object categories from images. Given a set of object categories (called """"known classes"""") with training images, our goal is to learn a system to recognize another non-overlapping set of object categories (called """"unknown classes"""") for which there are no training images. Our proposed approach exploits the recent work in natural language processing which has produced vector representations of words. Using the vector representations of object classes, we develop a method for transferring the appearance models from known object classes to unknown object classes. Our experimental results on three benchmark datasets show that our proposed method outperforms other competing approaches."""	attribute-value system;baseline (configuration management);benchmark (computing);confusion matrix;graphics processing unit;natural language processing;outline of object recognition;word embedding	Shujon Naha;Yang Wang	2015	2015 12th Conference on Computer and Robot Vision	10.1109/CRV.2015.21	natural language processing;computer vision;training set;method;object model;transfer of learning;computer science;viola–jones object detection framework;object;cognitive neuroscience of visual object recognition;machine learning;semantics;3d single-object recognition	Vision	26.017223115514668	-49.46258417667826	155471
f8ab8e4317e24a3887120c750ac3abacb216f85d	detection and localization of crowd behavior using a novel tracklet-based model		In this paper, two novel descriptors are introduced to detect and localize abnormal behaviors in crowded scenes. The first proposed descriptor is based on the orientation and magnitude of short trajectories extracted by tracking interest points in spatio-temporal 3D patches. The proposed descriptor employs a novel simplified Histogram of Oriented Tracklets (sHOT), which is shown to be very effective in the task of crowd abnormal behavior detection. In this scheme, abnormal behaviors are detected at different levels, namely spatio-temporal level and frame level. By combining the first proposed descriptor and the dense optical flow model, we propose our second framework which is able to localize the abnormal behavior areas in video sequences. The evaluation of our simple but yet effective descriptors on different state-of-the-art datasets, namely UCSD, UMN and Violence in Crowds yields very promising results in abnormality detection and outperforming different former state-of-the-art descriptors.		Hamid R. Rabiee;Hossein Mousavi;Moin Nabi;Mahdyar Ravanbakhsh	2018	Int. J. Machine Learning & Cybernetics	10.1007/s13042-017-0682-8	crowd psychology;crowds;magnitude (mathematics);computer vision;histogram;artificial intelligence;abnormality;computer science;optical flow	AI	35.50323736727347	-49.822934051891174	155521
072fc7bb3155ab0a7af2d80006df801a27f127a3	motion blur kernel estimation via deep learning		The success of the state-of-the-art deblurring methods mainly depends on the restoration of sharp edges in a coarse-to-fine kernel estimation process. In this paper, we propose to learn a deep convolutional neural network for extracting sharp edges from blurred images. Motivated by the success of the existing filtering-based deblurring methods, the proposed model consists of two stages: suppressing extraneous details and enhancing sharp edges. We show that the two-stage model simplifies the learning process and effectively restores sharp edges. Facilitated by the learned sharp edges, the proposed deblurring algorithm does not require any coarse-to-fine strategy or edge selection, thereby significantly simplifying kernel estimation and reducing computation load. Extensive experimental results on challenging blurry images demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods on both synthetic and real-world images in terms of visual quality and run-time.	algorithm;artificial neural network;biological neural networks;circuit restoration;computation (action);convolutional neural network;deblurring;deep learning;evaluation;gaussian blur;generic drugs;heuristic;heuristics;ibm notes;kernel;nephrogenic systemic fibrosis;synthetic intelligence	Xiangyu Xu;Jinshan Pan;Yu-Jin Zhang;Ming-Hsuan Yang	2018	IEEE Transactions on Image Processing	10.1109/TIP.2017.2753658	artificial intelligence;computer vision;filter (signal processing);deep learning;kernel (linear algebra);motion blur;image restoration;convolutional neural network;pattern recognition;deblurring;kernel density estimation;computer science	Vision	24.993704987399493	-51.26563480872467	155577
56c200786d89d820f6106587fb0b84efb3e48f9e	locally adjusted robust regression for human age estimation	robustness aging human computer interaction face application software multimedia communication internet image retrieval shape measurement training data;human computer interaction;locally adjusted robust regressor;face recognition;multimedia communication;age estimation;robust regression;regression analysis;locally adjusted robust regressor human age estimation human computer interaction multimedia communication;regression analysis face recognition human computer interaction;human age estimation	Automatic human age estimation has considerable potential applications in human computer interaction and multimedia communication. However, the age estimation problem is challenging. We design a locally adjusted robust regressor (LARR) for learning and prediction of human ages. The novel approach reduces the age estimation errors significantly over all previous methods. Experiments on two aging databases show the success of the proposed method for human aging estimation.	automatic control;database;experiment;human computer;human-based computation;human–computer interaction;job control (unix);support vector machine	Guodong Guo;Yun Fu;Thomas S. Huang;Charles R. Dyer	2008	2008 IEEE Workshop on Applications of Computer Vision	10.1109/WACV.2008.4544009	facial recognition system;computer vision;speech recognition;computer science;machine learning;robust regression;regression analysis;statistics	Vision	35.36885221559406	-43.63094674819325	155683
6c1af761211d380d931362e3b495898f1feaaf78	boosting graph embedding with application to facial expression recognition	graph theory;locality preserving projection;facial expression recognition;locality preserving projections g boosting method graph embedding facial expression recognition;manifolds;boosting graph embedding;graph weighting method;graph theory face recognition feature extraction;classification graph;classification graph boosting graph embedding facial expression recognition feature extract technology graph weighting method;boosting face recognition linear discriminant analysis feature extraction principal component analysis laplace equations information security bismuth data mining kernel;data mining;boosting method;boosting;face recognition;feature extraction;locality preserving projections g;classification algorithms;graph embedding;feature extract technology	As a general framework in feature extract technology, Graph embedding has been paid much attention. Integrate graph embedding with boosting method is a potential developing way. We are the first successfully attempt to incorporate these two technology together. Further more, a new adjacency graph weighting method called “classification graph” was proposed. By using this more suitable graph, the performance of the boosting graph embedding was improved. Experiment results demonstrate the efficient of our approach.	boosting (machine learning);graph embedding	Lei Xiong;Duyan Bi;Xu Zhou;Shiping Ma	2009	2009 Fifth International Conference on Information Assurance and Security	10.1109/IAS.2009.63	machine learning;pattern recognition;data mining;mathematics;graph	Vision	25.37191158348397	-42.65887872129171	155889
70ff5cd33a096e250162741d002c0cdedfea8b43	large margin linear projection and face recognition	quadratic programming;quadratic program;high dimensionality;support vector machines;fisher discriminant;linear discriminate analysis;small samples;large margin linear projection;face recognition;theoretical analysis;pattern recognition;scattering matrix;support vector machine	Unlike linear discriminant analysis, the large margin linear projection (LMLP) classi2er presented in this paper, which also roots in linear Fisher discriminant, takes full advantage of the singularity of within-class scatter matrix, and classi2es projected points in one-dimensional space by itself. Theoretical analysis and experimental results both reveal that LMLP is well suited for high-dimensional small-sample pattern recognition problems such as face recognition. ? 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	facial recognition system;fisher information;linear discriminant analysis;pattern recognition;technological singularity	Fengxi Song;Jingyu Yang;Shuhai Liu	2004	Pattern Recognition	10.1016/j.patcog.2004.01.016	support vector machine;mathematical optimization;kernel fisher discriminant analysis;computer science;machine learning;pattern recognition;mathematics;linear discriminant analysis;quadratic programming	Vision	24.712198114855536	-40.23764886092494	155910
23826835597e3737dfce4ede0c0606f410721468	spatial-temporal texture features for 3d human activity recognition using laser-based rgb-d videos		The IR camera and laser-based IR projector provide an effective solution for real-time collection of moving targets in RGB-D videos. Different from the traditional RGB videos, the captured depth videos are not affected by the illumination variation. In this paper, we propose a novel feature extraction framework to describe human activities based on the above optical video capturing method, namely spatial-temporal texture features for 3D human activity recognition. Spatial-temporal texture feature with depth information is insensitive to illumination and occlusions, and efficient for fine-motion description. The framework of our proposed algorithm begins with video acquisition based on laser projection, video preprocessing with visual background extraction and obtains spatial-temporal key images. Then, the texture features encoded from key images are used to generate discriminative features for human activity information. The experimental results based on the different databases and practical scenarios demonstrate the effectiveness of our proposed algorithm for the large-scale data sets.	activity recognition	Yue Ming;Guangchao Wang;Xiaopeng Hong	2017	TIIS	10.3837/tiis.2017.03.019	video capture;discriminative model;computer science;preprocessor;feature extraction;projector;activity recognition;computer vision;data set;rgb color model;pattern recognition;artificial intelligence	Vision	36.96961512341783	-50.485408346074614	155987
d52630f1146fae44015559afe44fb68bcdc7e693	face recognition using discriminant sparsity neighborhood preserving embedding	face recognition;dimensionality reduction;feature extraction;graph embedding;sparse representation	In this paper, we propose an effective supervised dimensionality reduction technique, namely discriminant sparsity neighborhood preserving embedding (DSNPE), for face recognition. DSNPE constructs graph and corresponding edge weights simultaneously through sparse representation (SR). DSNPE explicitly takes into account the within-neighboring information and between-neighboring information. Further, by taking the advantage of the maximum margin criterion (MMC), the discriminating power of DSNPE is further boosted. Experiments on the ORL, Yale, AR and FERET face databases show the effectiveness of the proposed DSNPE.	algorithm;database;dimensionality reduction;discriminant;experiment;feret (facial recognition technology);facial recognition system;local-density approximation;memory management controller;norton power eraser;pattern recognition;principal component analysis;return loss;self-propelled particles;sparse approximation;sparse matrix	Gui-Fu Lu;Zhong Jin;Jian Zou	2012	Knowl.-Based Syst.	10.1016/j.knosys.2012.02.014	facial recognition system;computer vision;graph embedding;feature extraction;computer science;machine learning;pattern recognition;sparse approximation;dimensionality reduction	Vision	25.578743504050188	-42.37545474215815	155995
de72bf54d08d21fa687d6ec4c3dadf3fe2b17ca1	investigating drivers' head and glance correspondence		The relationship between a driver's glance pattern and corresponding head rotation is highly complex due to its nonlinear dependence on the individual, task, and driving context. This study explores the ability of head pose to serve as an estimator for driver gaze by connecting head rotation data with manually coded gaze region data using both a statistical analysis approach and a predictive (i.e., machine learning) approach. For the latter, classification accuracy increased as visual angles between two glance locations increased. In other words, the greater the shift in gaze, the higher the accuracy of classification. This is an intuitive but important concept that we make explicit through our analysis. The highest accuracy achieved was 83% using the method of Hidden Markov Models (HMM) for the binary gaze classification problem of (1) the forward roadway versus (2) the center stack. Results suggest that although there are individual differences in head-glance correspondence while driving, classifier models based on head-rotation data may be robust to these differences and therefore can serve as reasonable estimators for glance location. The results suggest that driver head pose can be used as a surrogate for eye gaze in several key conditions including the identification of high-eccentricity glances. Inexpensive driver head pose tracking may be a key element in detection systems developed to mitigate driver distraction and inattention.		Joonbum Lee;Mauricio Muñoz;Alex Fridman;Trent Victor;Bryan Reimer;Bruce Mehler	2016	CoRR		computer vision;simulation;computer science	NLP	37.474246270983535	-43.26873492094681	155997
aa62a6f3b693c617b2122e29310374fbb55b7358	refinable kernels	wavelet-like reproducing kernel;refinable kernels;riesz bases;refinable function;mathematical learning;refinable kernels;dual ker- nels;learning with kernels;training data;reproducing kernel hilbert spaces;various characterization;refinable feature maps;reproducing kernel hilbert space;riesz basis;refinable translation invariant kernel;refinable kernel;wavelet-like reproducing kernels	Motivated by mathematical learning from training data, we introduce the notion of refinable kernels. Various characterizations of refinable kernels are presented. The concept of refinable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a refinable kernel that forms a Riesz basis. In particular, we characterize refinable translation invariant kernels, and refinable kernels defined by refinable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces.	hilbert space;kernel (operating system);multiresolution analysis;refinable function;separation kernel;wavelet	Yuesheng Xu;Haizhang Zhang	2007	Journal of Machine Learning Research		mathematical optimization;mathematical analysis;discrete mathematics;transfer matrix;mathematics	ML	30.867205298805516	-38.92382803905725	156070
f4e2f1743befd5d5e14ed89e4e8357fb7ab7880a	large-scale machine learning and evaluation platform for real-time traffic surveillance	sensors;mining;machine learning;video	In traffic engineering, vehicle detectors are trained on limited datasets, resulting in poor accuracy when deployed in real-world surveillance applications. Annotating large-scale high-quality datasets is challenging. Typically, these datasets have limited diversity; they do not reflect the real-world operating environment. There is a need for a large-scale, cloud-based positive and negative mining process and a large-scale learning and evaluation system for the application of automatic traffic measurements and classification. The proposed positive and negative mining process addresses the quality of crowd sourced ground truth data through machine learning review and human feedback mechanisms. The proposed learning and evaluation system uses a distributed cloud computing framework to handle data-scaling issues associated with large numbers of samples and a high-dimensional feature space. The system is trained using AdaBoost on 1,000,000 Haar-like features extracted from 70,000 annotated video frames. The trained real-time vehicle detector achieves an accuracy of at least 95% for 1∕2 and about 78% for 19∕20 of the time when tested on ∼7;500;000 video frames. At the end of 2016, the dataset is expected to have over 1 billion annotated video frames.©2016SPIE and IS&T [DOI: 10.1117/1.JEI	adaboost;cloud computing;crowdsourcing;feature vector;ground truth;haar wavelet;image scaling;machine learning;operating environment;real-time clock;sensor	Justin A. Eichel;Akshaya Kumar Mishra;Nicholas Miller;Nicholas Jankovic;Mohan A. Thomas;Tyler Abbott;Douglas Swanson;Joel Keller	2016	J. Electronic Imaging	10.1117/1.JEI.25.5.051204	mining;video;computer science;sensor;data science;machine learning;data mining	ML	31.986337156255274	-46.86156616364385	156137
9a7b470d810234d337fcd0ed72091cebc945195e	semantic parsing of human manipulation activities using on-line learned models for robot imitation	manipulators concurrent engineering control engineering computing human robot interaction;object recognition;manipulators;image segmentation;semantics;semantics manipulators image segmentation object recognition computational modeling spatiotemporal phenomena;computational modeling;semantic parsing concurrent manipulation human manipulation activity recognition robot imitation online learned models;spatiotemporal phenomena	Human manipulation activity recognition is an important yet challenging task in robot imitation. In this paper, we introduce, for the first time, a novel method for semantic decomposition and recognition of continuous human manipulation activities by using on-line learned individual manipulation models. Solely based on the spatiotemporal interactions between objects and hands in the scene, the proposed framework can parse not only sequential and concurrent (overlapping) manipulation streams but also basic primitive elements of each detected manipulation. Without requiring any prior object knowledge, the framework can furthermore extract object-like scene entities that are performing the same role in the detected manipulations. The framework was evaluated on our new egocentric activity dataset which contains 120 different samples of 8 single atomic manipulations (e.g. Cutting and Stirring) and 20 long and complex activity demonstrations such as “making a sandwich” and “preparing a breakfast”. We finally show that parsed manipulation actions can be imitated by robots even in various scene contexts with novel objects.	activity recognition;discriminative model;entity;high- and low-level;interaction;object type (object-oriented programming);online and offline;parsing;reflections of signals on conducting lines;robot;sensor;shlaer–mellor method;unsupervised learning;velocity (software development)	Eren Erdal Aksoy;Mohamad Javad Aein;Minija Tamosiunaite;Florentin Wörgötter	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353773	computer vision;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;semantics;image segmentation;computational model	Robotics	35.48087557202379	-46.79141443628247	156282
25297ec4c3a0a32a9dca9076f53f3d26ad909820	a family of modified projective nonnegative matrix factorization algorithms	approximation algorithms;pearson distances projective nonnegative matrix factorization subspace representations positive linear approximation iterative positive projection algorithms euclidean distance hellinger distances;subspace representations;linear approximation;projective nonnegative matrix factorization;nonnegative matrix;euclidean distance;approximation theory;distance measurement;hellinger distances;matrix decomposition;nonnegative matrix factorization;image representation;principal component analysis;iterative positive projection algorithms;non negative matrix factorization;positive linear approximation;matrix decomposition approximation theory image representation;face;approximation methods;spatial locality;sparse representation;principal component analysis sparse matrices covariance matrix neural networks matrix decomposition iterative algorithms linear approximation projection algorithms euclidean distance image coding;pearson distances;kullback leibler	We propose here new variants of the Non-negative Matrix Factorization (NMF) method for learning spatially localized, sparse, part-based subspace representations of visual or other patterns. The algorithms are based on positively constrained projections and are related both to NMF and to the conventional SVD or PCA decomposition. A crucial question is how to measure the difference between the original data and its positive linear approximation. Each difference measure gives a different solution. Several iterative positive projection algorithms are suggested here, one based on minimizing Euclidean distance and the others on minimizing the divergence of the original data matrix and its non-negative approximation. Several versions of divergence such as the Kullback-Leibler, Csiszar, and Amari divergence are considered, as well as the Hellinger and Pearson distances. Experimental results show that versions of P-NMF derive bases which are somewhat better suitable for a localized and sparse representation than NMF, as well as being more orthogonal.	algorithm;euclidean distance;iterative method;kullback–leibler divergence;linear approximation;non-negative matrix factorization;singular value decomposition;sparse approximation;sparse matrix	Zhijian Yuan;Erkki Oja	2007	2007 9th International Symposium on Signal Processing and Its Applications	10.1109/ISSPA.2007.4555631	mathematical optimization;combinatorics;mathematical analysis;machine learning;mathematics;non-negative matrix factorization;approximation algorithm	ML	26.694431287361258	-40.224302145519516	156440
17e25b13f359029a063a9dea74b3645ac747abe9	motion trend patterns for action modelling and recognition	semi dense trajectory field;local binary pattern;bag of features;action recognition	A new method for action modelling is proposed, which combines the trajectory beam obtained by semi-dense point tracking and a local binary trend description inspired from the Local Binary Patterns (LBP). The semi dense trajectory approach represents a good trade-off between reliability and density of the motion field, whereas the LBP component allows to capture relevant elementary motion elements along each trajectory, which are encoded into mixed descriptors called Motion Trend Patterns (MTP). The combination of those two fast operators allows a real-time, on line computation of the action descriptors, composed of space-time blockwise histograms of MTP values, which are classified using a fast SVM classifier. An encoding scheme is proposed and compared with the state-of-the-art through an evaluation performed on two academic action video datasets.	belief propagation;computation;computational complexity theory;line code;local binary patterns;motion field;real-time clock;semiconductor industry;uncontrolled format string	Thanh Phuong Nguyen;Antoine Manzanera;Matthieu Garrigues	2013		10.1007/978-3-642-40261-6_43	computer vision;local binary patterns;computer science;machine learning;pattern recognition;mathematics	Vision	37.23471661617477	-50.448122869299105	156456
41df6dc55917c1c2ab8802a6ea6cfc0bd7cea2e6	integration of 3d cad, reverse engineering and rapid prototyping in fabrication of invisible tooth aligner	dentistry;reverse engineering invisible tooth aligner cad rapid prototyping;software prototyping;cad;reverse engineering prototypes fabrication teeth dentistry medical treatment production optical devices mechanical engineering disruption tolerant networking;medical computing;post orthodontic treatment tracking 3d cad reverse engineering rapid prototyping invisible tooth aligner dental plaster model;rapid prototyping;patient treatment;patient treatment reverse engineering cad software prototyping dentistry medical computing;reverse engineering	Current fabrication of invisible tooth aligner follows the standard procedure to first form a dental plaster model manually adjusted per the dentist's empirical judgment, and then replicate the model to make an invisible tooth aligner fit onto the specific patient. Such a process requires meticulous hands-on experience and time. The objective of this paper is to use digitalized tools to improve orthodontic efficiency. The tool will enable the dentist to directly modify a dental model in a computer, output the post-modification file to speedily produce a prototype, and finally fabricate a ready-to-wear invisible tooth aligner. This tool will also provide clear evaluation for post orthodontic treatment tracking.	codoncode aligner;computer-aided design;hands-on computing;prototype;rapid prototyping;reverse engineering;self-replication	Alan C. Lin	2005	2005 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2005.1571513	computer science;cad;reverse engineering	Visualization	39.07056169443234	-38.376050913299544	156498
514062c37ba2aad013a16e5b15ae7c7d468f36db	superpixel-based unsupervised band selection for classification of hyperspectral images		This paper presents an unsupervised approach to band selection in hyperspectral images that considers both spectral and spatial information in data dimensionality reduction. The approach exploits the concepts of superpixel and chunklets for identifying the spectral channels most suitable to be used in classification for discriminating land-cover classes. The segmented superpixels can be regarded as many small spectral homogeneous and spatial neighboring pixel chunklets. Based on the observation that the superpixel chunklets achieve high homogeneity and consistency within land-cover classes, a series of band criteria (BC) is identified by learning the optimal band transformation that results in low within-class variability and high total variability. Then, the learned BC, which are called band measures, are given in input to an efficient clustering algorithm, i.e., the affinity propagation, for selecting highly separable bands with low redundancy. The effectiveness of proposed approach was assessed on three hyperspectral data sets. The results point out the advantages of the proposed methods over five state-of-the-art unsupervised methods.	acclimatization;affinity propagation;algorithm;anion exchange protein 1, erythrocyte;bands;class;classification;cluster analysis;dental whitening;dimensionality reduction;domain adaptation;eigen (c++ library);genetic selection;heart rate variability;horizontal situation indicator;new type;performance;pixel;reln gene;rl (complexity);sirolimus;software propagation;spatial variability;unsupervised learning;whitening transformation;statistical cluster	Chen Yang;Lorenzo Bruzzone;Haishi Zhao;Yulei Tan;Renchu Guan	2018	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2018.2849443	computer vision;cluster analysis;spatial analysis;image segmentation;homogeneity (statistics);dimensionality reduction;mathematics;hyperspectral imaging;affinity propagation;pattern recognition;artificial intelligence;data set	Vision	30.52699522233285	-43.91845013217848	156505
8debdbaa55c121ee39b466f1c60f5823de58adad	uncorrelated multilinear nearest feature line analysis		In this paper, we propose a new subspace learning method, called uncorrelated multilinear nearest feature line analysis (UMNFLA), for the recognition of multidimensional objects, known as tensor objects. Motivated by the fact that existing nearest feature line (NFL) can effectively characterize the geometrical information of limited samples, and uncorrelated features are desirable for many pattern analysis applications since they contain minimum redundancy and ensure independence of features, we propose using the NFL metric to seek a feature subspace such that the within-class feature line (FL) distances are minimized and between-class FL distances are maximized simultaneously in the reduced subspace, and impose an uncorrelated constraint to extract statistically uncorrelated features directly from tensorial data. UMNFLA seeks a tensor-to-vector projection (TVP) that captures most of the variation in the original tensorial input, and employs sequential iterative steps based on the alternating projection method. Experimental results on the task of single trial electroencephalography (EEG) recognition suggest that UMN-FLA is particularly effective in determining the low-dimensional projection space needed in such recognition tasks.		Ye Liu;Liqing Zhang	2014		10.3233/978-1-61499-419-0-531	speech recognition;machine learning;pattern recognition	Vision	26.817139969531034	-43.12373788630447	156609
26f5bb752c33a3ebb41e8cc3f1294ecd3944cfae	human action recognition using associated depth and skeleton information	databases;image recognition image colour analysis;image recognition;kernel;skeleton;offline collected database;kinect;computer vision;2d rgb camera;image colour analysis;rgb representation;action recognition;skeleton association;human action recognition;rgb representation human action recognition associated depth information skeleton information computer vision next generation depth camera next generation binocular camera 2d rgb camera multimodal imaging device rgb video recognition kinect offline collected database interdatabase variation;pattern recognition;multimodal imaging device;skeleton information;depth association;next generation depth camera;interdatabase variation;next generation binocular camera;rgb video recognition;cameras;associated depth information;videos;skeleton association action recognition depth association;cameras videos skeleton computer vision databases kernel pattern recognition	The recent advances in imaging devices have opened the opportunity of better solving computer vision tasks. The next-generation cameras, such as the depth or binocular cameras, capture diverse information, and complement the conventional 2D RGB cameras. Thus, investigating the yielded multi-modal images generally facilitates the accomplishment of related applications. However, the limitations of these devices, such as short effective distances, expensive costs, or long response time, degrade their applicability in practical use. Addressing this problem in this work, we aim at action recognition in RGB videos with the aid of Kinect. We improve recognition accuracy by leveraging information derived from an offline collected database, in which not only the RGB but also the depth and skeleton images of actions are available. Our approach adapts the inter-database variations, and enables the sharing of visual knowledge across different image modalities. Each action instance for recognition in RGB representation is then augmented with the borrowed depth and skeleton features.	binocular vision;computer vision;digital camera;kinect;modal logic;online and offline;response time (technology)	Nick C. Tang;Yen-Yu Lin;Ju-Hsuan Hua;Ming-Fang Weng;Hong-Yuan Mark Liao	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854475	computer vision;kernel;computer science;multimedia;skeleton;computer graphics (images)	Vision	35.78668249198236	-48.73503638110943	156829
017d9eb123b505b1eade8ecd2382ad93386dc52e	pattern recognition		We present a new approach for contextual semantic segmentation and introduce a new tree-based framework, which combines local information and context knowledge in a single model. The method itself is also suitable for anytime classification scenarios, where the challenge is to estimate a label for each pixel in an image while allowing an interruption of the estimation at any time. This offers the application of the introduced method in time-critical tasks, like automotive applications, with limited computational resources unknown in advance. Label estimation is done in an iterative manner and includes spatial context right from the beginning. Our approach is evaluated in extensive experiments showing its state-of-the-art performance on challenging street scene datasets with anytime classification abilities.	align (company);anytime algorithm;computation;computational resource;decision tree;entropy (information theory);experiment;interrupt;iteration;map;pattern recognition;pixel;requirement;rényi entropy;unsupervised learning;window of opportunity	Axel Pinz Thomas Pock;Horst Bischof Franz Leberl	2012		10.1007/978-3-642-32717-9		Vision	31.098403045260472	-48.884259633422474	156992
3935cabb73d75939ade5fc8839cfd946fbdc8057	learning image representations equivariant to ego-motion		Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance i.e. they respond systematically to transformations associated with distinct ego-motions. With three datasets, we show that our unsupervised feature learning system significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in a disjoint domain.	artificial neural network;autonomous car;convolutional neural network;feature learning;unsupervised learning;visual learning	Dinesh Jayaraman;Kristen Grauman	2015	CoRR		computer vision;machine learning;pattern recognition;mathematics	Vision	26.317747445669998	-49.35845804610855	157045
63730fcb2e31ec9418c82bb2138700de7468855a	robust deep auto-encoder for occluded face recognition	occlusion;stacked sparse denoising autoencoder;face recognition;deep neural network	Occlusions by sunglasses, scarf, hats, beard, shadow etc, can significantly reduce the performance of face recognition systems. Although there exists a rich literature of researches focusing on face recognition with illuminations, poses and facial expression variations, there is very limited work reported for occlusion robust face recognition. In this paper, we present a method to restore occluded facial regions using deep learning technique to improve face recognition performance. Inspired by SSDA for facial occlusion removal with known occlusion type and explicit occlusion location detection from a preprocessing step, this paper further introduces Double Channel SSDA (DC-SSDA) which requires no prior knowledge of the types and the locations of occlusions. Experimental results based on CMU-PIE face database have showed that, the proposed method is robust to a variety of occlusion types and locations, and the restored faces could yield significant recognition performance improvements over occluded ones.	deep learning;encoder;facial recognition system;preprocessor	Lele Cheng;Jinjun Wang;Yihong Gong;Qiqi Hou	2015		10.1145/2733373.2806291	facial recognition system;computer vision;speech recognition;computer science;three-dimensional face recognition	Vision	29.387243063578282	-50.65587487865527	157053
11356cd6bb0f2776a88cd584ff108470414c6594	submanifold sparse convolutional networks		Convolutional network are the de-facto standard for analysing spatio-temporal data such as images, videos, 3D shapes, etc. Whilst some of this data is naturally dense (for instance, photos), many other data sources are inherently sparse. Examples include penstrokes forming on a piece of paper, or (colored) 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard “dense” implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce a sparse convolutional operation tailored to processing sparse data that differs from prior work on sparse convolutional networks in that it operates strictly on submanifolds, rather than “dilating” the observation with every layer in the network. Our empirical analysis of the resulting submanifold sparse convolutional networks shows that they perform on par with state-of-the-art methods whilst requiring substantially less computation.	computation;convolution;convolutional neural network;embedded system;experiment;image;point cloud;shadow copy;sparse matrix;while	Benjamin Graham;Laurens van der Maaten	2017	CoRR		machine learning;artificial intelligence;point cloud;sparse matrix;submanifold;computation;computer science;lidar;sparse approximation;rgb color model	Vision	27.995069647538017	-49.301540097760125	157124
ab34efed8e2cc4e85cde9b0be134d376eddc3c31	horror video scene recognition based on multi-view multi-instance learning	会议论文	Comparing with the research of pornographic content filtering on Web, Web horror content filtering, especially horror video scene recognition is still on the stage of exploration. Most existing methods identify horror scene only from independent frames, ignoring the context cues among frames in a video scene. In this paper, we propose a Multi-view Multi-Instance Leaning (MIL) model based on joint sparse coding technique that takes the bag of instances from independent view and contextual view into account simultaneously and apply it on horror scene recognition. Experiments on a horror video dataset collected from internet demonstrate that our method’s performance is superior to the other existing algorithms.	algorithm;content-control software;emotion recognition;experiment;internet;neural coding;sparse matrix;unified framework;world wide web	Xinmiao Ding;Bing Li;Weiming Hu;Weihua Xiong;Zhenchong Wang	2012		10.1007/978-3-642-37431-9_46	computer vision;computer science;multimedia;computer graphics (images)	Vision	33.22707128498937	-49.70662610285098	157148
26f408645c26a608600909f23ee0d58a658c4ec4	the kosko subsethood fuzzy associative memory (ks-fam): mathematical background and applications in computer vision	mathematical morphology;fuzzy associative memory;mobile robotics;vision based localization;kosko subsethood measure;gray scale image;erosion;morphological neural network;pattern recognition	Many well-known fuzzy associative memory (FAM) models can be viewed as (fuzzy) morphological neural networks (MNNs) because they perform an operation of (fuzzy) mathematical morphology at every node, possibly followed by the application of an activation function. The vast majority of these FAMs represent distributive models given by single-layer matrix memories. Although the Kosko subsethood FAM (KS-FAM) can also be classified as a fuzzy morphological associative memory (FMAM), the KS-FAM constitutes a two-layer non-distributive model. In this paper, we prove several theorems concerning the conditions of perfect recall, the absolute storage capacity, and the output patterns produced by the KS-FAM. In addition, we propose a normalization strategy for the training and recall phases of the KS-FAM. We employ this strategy to compare the error correction capabilities of the KS-FAM and other fuzzy and gray-scale associative memories in terms of some experimental results concerning gray-scale image reconstruction. Finally, we apply the KS-FAM to the task of vision-based self-localization in robotics.	activation function;artificial neural network;computer vision;content-addressable memory;database normalization;error detection and correction;flash memory;fuzzy associative matrix;grayscale;iterative reconstruction;mathematical morphology;robotics;windows legacy audio components	Peter Sussner;Estevão Laureano Esmi;Ivan Villaverde;Manuel Graña	2011	Journal of Mathematical Imaging and Vision	10.1007/s10851-011-0292-0	computer vision;mathematical morphology;erosion;computer science;artificial intelligence;machine learning;mathematics;fuzzy associative matrix;bidirectional associative memory	AI	35.04415144065599	-39.67969926311193	157160
f1e44e64957397d167d13f8f551cae99e5c16c75	face detection and facial expression recognition using simultaneous clustering and feature selection via an expectation propagation statistical learning framework	facial expression recognition;dirichlet process;generalized dirichlet mixture;expectation propagation;feature selection;face detection	In this paper, we focus on developing a novel framework which can be effectively used for both face detection (i.e. discriminate faces from non-face patterns) and facial expression recognition. The proposed statistical framework is based on a Dirichlet process mixture of generalized Dirichlet (GD) distributions used to model local binary pattern (LBP) features. Our method is built on nonparametric Bayesian analysis where the determination of the number of clusters is sidestepped by assuming an infinite number of mixture components. An unsupervised feature selection scheme is also integrated with the proposed nonparametric framework to improve modeling performance and generalization capabilities. By learning the proposed model using an expectation propagation (EP) inference approach, all the involved model parameters and feature saliencies can be evaluated simultaneously in a single optimization framework. Furthermore, the proposed framework is extended by adopting a localized feature selection scheme which has shown, according to our results, superior performance, to determine the most important facial features, as compared to the global one. The effectiveness and utility of the proposed method is illustrated through extensive empirical results using both synthetic data and two challenging applications involving face detection, and facial expression recognition.	binary pattern (image generation);binary prefix;cluster analysis;expectation propagation;face detection;feature selection;local binary patterns;machine learning;mathematical optimization;mixture model;simulation;software propagation;synthetic data	Wentao Fan;Nizar Bouguila	2013	Multimedia Tools and Applications	10.1007/s11042-013-1548-z	face detection;computer science;machine learning;pattern recognition;feature selection;statistics	Vision	35.07780878773075	-47.46935230404081	157264
06843ae4c5f6da14b0b623de6d87cb325f97faed	semisupervised adversarial discriminative domain adaptation, with applicationto remote sensing data		Recently, convolutional neural networks (CNNs) have received substantial attention in the literature for object recognition (e.g., buildings and roads) in several remote sensing data modalities (e.g., aerial color imagery). Although CNNs have exhibited excellent recognition performance, recent research suggests that trained CNNs can often perform very poorly when applied to data collected over new geographic regions, and for which little labeled training data is available. In this work, we consider the adversarial discriminative domain adaptation (ADDA) approach to address this limitation, due its recent success on related problems. A limitation of ADDA is that it is unsupervised, so in this work we extend ADDA to a semi-supervised algorithm, in which we assume that both labeled and unlabeled data are available in the new domain (e.g., in new geographic region to be evaluated). We compare semi-supervised ADDA to ADDA and a standard fine-tuning approach wherein available labeled data is used for standard CNN training. We perform experiments on two remote sensing datasets and the results indicate that semi-supervised ADDA consistently improves over the other approaches when small amounts of labeled training data are available in the new domain.	aerial photography;algorithm;artificial neural network;convolutional neural network;domain adaptation;experiment;outline of object recognition;regular expression;semi-continuity;semi-supervised learning;semiconductor industry;unsupervised learning	Rui Wang;Leslie M. Collins;Kyle Bradbury;Jordan M. Malof	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518096	convolutional neural network;remote sensing;discriminative model;labeled data;domain adaptation;data modeling;training set;computer science	Vision	28.146911805008514	-50.51842098744203	157325
f9d82f6711dc42a5808997a4c411b5b86c29fa22	detection of infrared point targets with linear eigentargets and nonlinear eigentargets	second order;eigenvalues and eigenfunctions;pca target detection infrared point target subspace;kernel;linear principal component analysis;nonlinear subspace algorithm;optical filters;training;infrared point target detection;principal component analysis eigenvalues and eigenfunctions gaussian distribution object detection;gabor filters;higher order statistics;subspace;nonlinear pca;infrared detectors principal component analysis gabor filters kernel higher order statistics neural networks pattern recognition support vector machines independent component analysis target recognition;nonlinear pca infrared point target detection nonlinear eigentarget nonlinear subspace algorithm linear principal component analysis gaussian distribution linear eigentarget detection;nonlinear eigentarget;principal component analysis;infrared point target;matched filters;infrared;target detection;pca;linear eigentarget detection;gaussian distribution;filtering theory;object detection	The linear subspace algorithm and nonlinear subspace algorithm is explored to detect point targets. We call them as linear Eigentargets and nonlinear Eigentargets. Linear principal component analysis (LPCA) is based on the second-order correlations without taking higher-order statistics into account. So LPCA is only appropriate to represent the data with a Gaussian distribution. That results in the performance limitation of linear Eigentargets detection based on LPCA. For improving detection performance, we extend linear Eigentargets to its nonlinear version, nonlinear Eigentargets, in this paper. Because the nonlinear PCA is capable of capturing the part of higher-order statistics, the better detection performance can be achieved.	algorithm;experiment;nonlinear system;pattern recognition;principal component analysis;whole earth 'lectronic link	Ruiming Liu	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.378	machine learning;pattern recognition;mathematics;statistics;principal component analysis	ML	31.64290505037063	-41.640381172873035	157364
23a2107d5653235911d69312b6f1b82fad42ddf1	meta-tracking for video scene understanding	histograms;video surveillance;image motion analysis;surveillance;video surveillance feature extraction image motion analysis;chaotic scenes particle meta tracking procedure video scene understanding dominant motion pattern surveillance video motion histogram orientation distribution function odf particle trajectory low level motion feature brain fiber tractography;layout;trajectory;vectors;feature extraction;vectors tracking histograms layout surveillance vehicles trajectory;vehicles;scene understanding;tracking	This paper presents a novel method to extract dominant motion patterns (MPs) and the main entry/exit areas from a surveillance video. The method first computes motion histograms for each pixel and then converts it into orientation distribution functions (ODFs). Given these ODFs, a novel particle meta-tracking procedure is launched which produces meta-tracks, i.e. particle trajectories. As opposed to conventional tracking which focuses on individual moving objects, meta-tracking uses particles to follow the dominant flow of the traffic. In a last step, a novel method is used to simultaneously identify the main entry/exit areas and recover the predominant MPs. The meta-tracking procedure is a unique way to connect low-level motion features to long-range MPs. This kind of tracking is inspired by brain fiber tractography which has long been used to find dominant connections in the brain. Our method is fast, simple to implement, and works both on sparse and extremely crowded scenes. It also works on highly structured scenes (highways, traffic-light corners, etc.) as well as on chaotic scenes.	chaos theory;closed-circuit television;experiment;high- and low-level;pixel;sparse matrix	Pierre-Marc Jodoin;Yannick Benezeth;Yi Wang	2013	2013 10th IEEE International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2013.6636607	layout;computer vision;simulation;feature extraction;computer science;trajectory;machine learning;video tracking;motion estimation;histogram;tracking;statistics;computer graphics (images)	Vision	38.93476915042967	-47.143419501403464	157566
2008d5bcc1022402a574c092f0a1c8055acda8fa	vehicle type classification using bagging and convolutional neural network on multi view surveillance image		This paper aims to introduce a new vehicle type classification scheme on the images from multi-view surveillance camera. We propose four concepts to increase the performance on the images which have various resolutions from multi-view point. The Deep Learning method is essential to multi-view point image, bagging method makes system robust, data augmentation help to grow the classification capability, and post-processing compensate for imbalanced data. We combine these schemes and build a novel vehicle type classification system. Our system shows 97.84% classification accuracy on the 103,833 images in classification challenge dataset.	closed-circuit television;comparison and contrast of classification schemes in linguistics and metadata;convolutional neural network;deep learning;sampling (signal processing);video post-processing	Pyong-Kun Kim;Kil-Taek Lim	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.126	pattern recognition;artificial intelligence;convolutional neural network;deep learning;computer science;classification scheme;machine learning	Vision	28.373556879464633	-50.63863865909634	157626
f414a4f51748d548c2e72971f1e428ebb34754bd	visual attribute-augmented three-dimensional convolutional neural network for enhanced human action recognition		Visual attributes in individual video frames, such as the presence of characteristic objects and scenes, offer substantial information for action recognition in videos. With individual 2D video frame as input, visual attributes extraction could be achieved effectively and efficiently with more sophisticated convolutional neural network than current 3D CNNs with spatio-temporal filters, thanks to fewer parameters in 2D CNNs. In this paper, the integration of visual attributes (including detection, encoding and classification) into multistream 3D CNN is proposed for action recognition in trimmed videos, with the proposed visual Attribute-augmented 3D CNN (A3D) framework. The visual attribute pipeline includes an object detection network, an attributes encoding network and a classification network. Our proposed A3D framework achieves state-of-the-art performance on both the HMDB51 and the UCF101 datasets.	a3d;artificial neural network;convolutional neural network;object detection	Yunfeng Wang;Wengang Zhou;Qilin Zhang;Houqiang Li	2018	CoRR		computer science;pattern recognition;artificial intelligence;convolutional neural network;machine learning;encoding (memory);object detection	Vision	27.22457656719932	-52.02701936161496	157664
de8cd616d90964147dd0af622b76b5caf322ed96	exemplar-based recognition of human–object interactions	human object interactions;image recognition;object recognition;exemplar representation;exemplar based recognition;image matching;torso;video action datasets;pose estimation image matching object recognition;journal article;spatial pose object interaction exemplars;videos image recognition object detection human factors;vectors;estimation;spatial structure information;exemplar based approach;hoi recognition;action recognition;dictionaries;human object interactions hois;exemplar based hoi descriptor;video action datasets exemplar based recognition human object interactions spatial structure information human pose estimation exemplar based approach spatial pose object interaction exemplars exemplar based hoi descriptor matching model hoi recognition exemplar representation;matching model;object detection;exemplar modeling;videos;human pose estimation;pose estimation	Human action can be recognized from a single still image by modeling human-object interactions (HOIs), which infers the mutual spatial structure information between human and the manipulated object as well as their appearance. Existing approaches rely heavily on accurate detection of human and object and estimation of human pose; they are thus sensitive to large variations of human poses, occlusion, and unsatisfactory detection of small size objects. To overcome this limitation, a novel exemplar-based approach is proposed in this paper. Our approach learns a set of spatial pose-object interaction exemplars, which are probabilistic density functions describing spatially how a person is interacting with a manipulated object for different activities. Specifically, a new framework consisting of an exemplar-based HOI descriptor and an associated matching model is formulated for robust human action recognition in still images. In addition, the framework is extended to perform HOI recognition in videos, where the proposed exemplar representation is used for implicit frame selection to negate irrelevant or noisy frames by temporal structured HOI modeling. Extensive experiments are carried out on two image action datasets and two video action datasets. The results demonstrate the effectiveness of our proposed methods and show that our approach is able to achieve state-of-the-art performance, compared with several recently proposed competitors.		Jianfang Hu;Wei-Shi Zheng;Jianhuang Lai;Shaogang Gong;Tao Xiang	2016	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2015.2397200	computer vision;estimation;pose;torso;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics;statistics	Vision	34.19831910606997	-48.9005680313833	157673
70b846beb7a500723baceeb614bad03583741642	human tracking & visual spatio-temporal statistical analysis	spatio temporal analysis;tracking object detection spatiotemporal phenomena statistical analysis;real time systems cameras monitoring visualization buildings servers statistical analysis;visual analysis human tracking human presence statistics spatio temporal analysis;human tracking;human presence statistics;visual analysis;visual spatio temporal statistical analysis visual interactive tools semantic information energy consumption architectural map virtual top cameras partial occlusions multispace dynamic environment multicamera network monitoring human tracking system	In this work, a novel, multi-space, real-time and robust human tracking system is going to be presented. The system exploits a multi-camera network monitoring the multi-space dynamic environment under interest, detecting and tracking the humans in it. The system is able to handle the dynamic changes of the environment, as well as partial occlusions utilizing virtual top cameras. Furthermore, the system is able to real-time visualize the detection and tracking results on the architectural map of the dynamic environment, as well as a variety of statistics. The visual spatio-temporal analysis of the tracked data are presented in a consolidated form for the overall monitoring area and analytically for each space separately and for each tracked human. These statistics could be also combined with the energy consumption in the area, as well as with other environmental data providing semantic information such as comfort. The overall system is equipped with a number of visual interactive tools providing real-time spatio-temporal human presence analysis offering to the user the opportunity to capture and isolate the areas/spaces with high human presence, the days and times of high human presence, to correlate this information with the potential energy consumption and indicators such as comfort.	real-time clock;real-time locating system;sensor;tracking system	Dimosthenis Ioannidis;Stelios Krinidis;Dimitrios Tzovaras;Spiridon D. Likothanassis	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025693	computer vision;visual analytics;simulation;computer science;multimedia	Robotics	38.906366476964656	-44.636032377416704	157741
6ace8183e0cb020b93f808474eefdc5131cc8b63	deception detection via blob motion pattern analysis	proof of concept;pattern analysis	Deception dectection is one of the most difficult problems in affect recognition and expression research area. Recently, non-verbal methods of detecting deception have appeared to be promising. Thomas[1] presented a proof-of-concept study based on the blob analysis of some suspects’ interviews and mock experiments video clips. In this paper, we present our recent research work in the direction of developing an automated deception detection system. We propose a blob motion pattern analysis approach to solve this problem. Our approach consists of the following steps: (a) using skin-color based technology to detect body blobs, i.e. head and hands, and calculating the in-frame and cross-frame features. (b) segmenting the training videos into small clips with a fixed duration where each clip contains only one blob motion pattern, and automatically clustering these patterns into groups. (c) using HMM-based method to model the pattern sequences and estimating the latent subject’s state. During the body blobs detection process, we first take an off-line training phase to set up a lookup table to determine the probability of each color vector ci being skin-colored. Then a standard connected components labeling algorithm is applied to yield different skin-colored regions. Size filtering on the derived connected components is performed to eliminate small, isolate blobs that do not correspond to body blobs. An ellipse fitting algorithm is also applied to find the right position of body blobs. We calculate the in-frame and cross-frame features include position, shape, distance between blobs, velocity, etc. and denote each blob as a vector b = {position, size, velocity, . . .}. Due to the space-time nature of blob motion patterns, we adopt a discrete scene event based feature representation approach considering that each clip contains only one blob motion pattern. The blob motion pattern in a clip can be represented as P = {f1, f2, . . . , fT }, where T is the number of frames in the video clip and fi = {bhead, bleft−hand, bright−hand}. Now the deception detection problem can be redefined formally. Consider the training data set D consists of N patterns with a fixed duration, D = {P1, P2, . . . , PN}, where Pi is the pattern vector defined as above. The subject’s behavioral state can be considered as a discrete time series of patterns. Using these patterns as observations we can train an HMM to estimate the latent subject’s state. We assume that there are 3 hidden states in the model, denoted as S = {agitation, normal, over − control} This assumption comes from the	algorithm;cluster analysis;connected component (graph theory);connected-component labeling;curve fitting;definition;emoticon;experiment;hidden markov model;lookup table;mock object;online and offline;pattern recognition;sensor;skin (computing);test set;time series;uncharted 3: drake's deception;velocity (software development);video clip	Fan Xia;Hong Wang;Junxian Huang	2007		10.1007/978-3-540-74889-2_70	psychology;computer vision;computer science;artificial intelligence;social psychology;proof of concept	Vision	38.26425949801638	-48.877956342054084	157826
57427e39f0116c90a7f8255bf8e50bf57143594b	learning spatially regularized similarity for robust visual tracking		Matching visual appearances of the target object over consecutive frames is a critical step in visual tracking. The accuracy performance of a practical tracking system highly depends on the similarity metric used for visual matching. Recent attempts to integrate discriminative metric learned by sequential visual data (instead of a predefined metric) in visual tracking have demonstrated more robust and accurate results. However, a global similarity metric is often suboptimal for visual matching when the target object experiences large appearance variation or occlusion. To address this issue, we propose in this paper a spatially weighted similarity fusion (SWSF) method for robust visual tracking. In our SWSF, a part-based model is employed as the object representation, and the local similarity metric and spatially regularized weights are jointly learned in a coherent process, such that the total matching accuracy between visual target and candidates can be effectively enhanced. Empirically, we evaluate our proposed tracker on various challenging sequences against several state-of-the-art methods, and the results demonstrate that our method can achieve competitive or better tracking performance in various challenging tracking scenarios.	robustness (computer science);video tracking	Xiuzhuang Zhou;Qirun Huo;Yuanyuan Shang;Min Xu;Hui Ding	2017	Image Vision Comput.	10.1016/j.imavis.2016.11.016	computer vision;machine learning;pattern recognition;mathematics	Vision	33.574569084511204	-48.003091778414415	157874
1da5a11105933c82bdd5c834475a38f13a75c6a8	alternating direction method for approximating smooth feature vectors in nonnegative matrix factorization	linear spectral unmixing problem smooth feature vector nonnegative matrix factorization nmf large scale qp problem alternating direction method of multipliers;vectors matrix decomposition quadratic programming;spectral unmixing nonnegative matrix factorization smoothness constrains alternating direction method of multipliers b splines;vectors splines mathematics optimization matrix decomposition computational modeling sparse matrices standards	In many applications of Nonnegative Matrix Factorization (NMF), the features vectors can be approximated by linear combinations of some basis functions that reflect the prior knowledge on the estimated factors. This approach is useful for modeling smoothness or unimodality. However, to estimate the coefficients of these linear combinations, a large-scale QP problem needs to be formulated and solved in each alternating optimization step. To alleviate a huge computational complexity of this approach, we applied the fast alternating direction method of multipliers to our model. As a result, our algorithm outperforms the well-known NMF algorithms in terms of efficiency for solving linear spectral unmixing problems.	approximation algorithm;augmented lagrangian method;basis function;coefficient;computational complexity theory;mathematical optimization;non-negative matrix factorization;whole earth 'lectronic link	Rafal Zdunek	2014	2014 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2014.6958865	mathematical optimization;combinatorics;mathematical analysis;nonnegative matrix;mathematics;matrix decomposition	ML	26.677009377763614	-38.96362720157247	158047
9e8382aa1de8f2012fd013d3b39838c6dad8fb4d	learning object-centric transformation for video prediction		Future frame prediction for video sequences is a challenging task and worth exploring problem in computer vision. Existing methods often learn motion information for the entire image to predict next frames. However, different objects in the same scene often move and deform in different ways intuitively. Considering the human visual system, one often pays attention to the key objects that contain crucial motion signals, rather than compress an entire image into a static representation. Motivated by this property of human perception, in this work, we develop a novel object-centric video prediction model that learns local motion transformation dynamically for key object regions with visual attention. By transforming objects iteratively to the original input frames, next frame can be produced. Specifically, we design an attention module with replaceable strategies to attend to objects in video frames automatically. Our method does not require any annotated data during training procedure. To produce sharp predictions, adversarial training is adopted in our work. We evaluate our model on the Moving MNIST and UCF101 datasets and report competitive results, compared to prior methods. The generated frames demonstrate that our model can characterize motion for different objects and produce plausible future frames.	adversary (cryptography);artificial neural network;computer vision;frame language;human visual system model;mnist database;recurrent neural network	Xiongtao Chen;Wenmin Wang;Jinzhuo Wang;Weimian Li	2017		10.1145/3123266.3123349	computer vision;artificial intelligence;multimedia;learning object;block-matching algorithm;video tracking;mnist database;machine learning;computer science;motion estimation;perception;motion compensation;human visual system model	Vision	27.17439322487971	-51.698612020215656	158068
0310e003aa05624cbae86186c2f029f4928bfec1	recognition of composite human activities through context-free grammar based representation	computer society;atomic action;logic;bayesian methods;humans hidden markov models pattern recognition computer vision pixel logic feature extraction computer society bayesian methods production systems;computer vision;hidden markov models;context free grammar;feature extraction;pixel;image sequence;pattern recognition;production systems;humans;human activity	This paper describes a general methodology for automated recognition of complex human activities. The methodology uses a context-free grammar (CFG) based representation scheme to represent composite actions and interactions. The CFG-based representation enables us to formally define complex human activities based on simple actions or movements. Human activities are classified into three categories: atomic action, composite action, and interaction. Our system is not only able to represent complex human activities formally, but also able to recognize represented actions and interactions with high accuracy. Image sequences are processed to extract poses and gestures. Based on gestures, the system detects actions and interactions occurring in a sequence of image frames. Our results show that the system is able to represent composite actions and interactions naturally. The system was tested to represent and recognize eight types of interactions: approach, depart, point, shake-hands, hug, punch, kick, and push. The experiments show that the system can recognize sequences of represented composite actions and interactions with a high recognition rate.	context-free grammar;context-free language;experiment;high- and low-level;interaction;linearizability;raw image format	Michael S. Ryoo;Jake K. Aggarwal	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.242	computer vision;feature extraction;computer science;machine learning;logic;pixel;hidden markov model	Vision	38.88788488681321	-48.286722976409024	158276
6562a50439ff5a144d48535adcf8bc82d070057b	two-stage object detection based on deep pruning for remote sensing image		In this paper, we concentrate on tackling the problems of object detection in very-high-resolution (VHR) remote sensing images. The main challenges of object detection in VHR remote sensing images are: (1) VHR images are usually too large and it will consume too much time when locating objects; (2) high false alarm because background dominate and is complex in VHR images. To address the above challenges, a new method is proposed to build two-stage object detection model. Our proposed method can be divided into two processes: (1) we use twice pruning to get region proposal convolutional neural network which is used to predict region proposals; (2) and we use once pruning to get classification convolutional neural network which is used to analyze the result of the first stage and output the class labels of proposals. The experimental results show that the proposed method has high precision and is significantly faster than the state-of-the-art methods on NWPU VHR-10 remote sensing dataset.	object detection	Sheng-sheng Wang;Meng Wang;Xin Zhao;Dong Liu	2018		10.1007/978-3-319-99365-2_12	pruning;false alarm;convolutional neural network;remote sensing;object detection;deep learning;computer science;artificial intelligence	Vision	32.50153560506249	-45.305585971487446	158279
9e84a580765261964b860c65a9ccc843fa8be708	multi-task learning using uncertainty to weigh losses for scene geometry and semantics		Numerous deep learning applications benefit from multitask learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.		Alex Kendall;Yarin Gal;Roberto Cipolla	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00781	task analysis;computer vision;machine learning;pattern recognition;computer science;homoscedasticity;deep learning;linear regression;artificial intelligence;image segmentation;multi-task learning;semantics;weighting	Vision	27.961723519948485	-48.50298730836147	158436
99388a2aae0b8c78eabdf9d10ae8f6c7dff54cd4	visual tracking via graph regularized kernel correlation filer and multi-memory voting		Abstract Correlation filter based tracking approach has been an important branch of visual tracking. However, most correlation filter based trackers fail to work under occlusion due to their frame-by-frame model update strategy, and the tracking performance can be further enhanced by optimizing the energy equation. The target appearance during tracking is nearly moving on a manifold. So, the classification scores should be similar on the target manifold. K Nearest Neighbor graphs are constructed and the classification scores on the neighborhood are regularized to have similar values. Through the local score propagation on the graph, the learned Graph Regularized Kernel Correlation Filer can represent different appearances of the object. Furthermore, in the proposed Multi-Memory Voting scheme, occlusion problem is addressed by voting from multiple target snapshots in the memory pool. An extensive evaluation on two recent benchmarks shows that the proposed tracker achieves competitive performance compared to nine other state-of-the-art trackers.	kernel (operating system);netapp filer	Weiwei Zheng;Huimin Yu;Wei Huang	2018	J. Visual Communication and Image Representation	10.1016/j.jvcir.2018.08.004	artificial intelligence;kernel (linear algebra);manifold;pattern recognition;mathematics;eye tracking;bittorrent tracker;correlation;k-nearest neighbors algorithm;voting;memory pool	Vision	33.68317393121226	-48.82819772172922	158620
f826e810f6819c273addf1d7d7cfe09738fc808d	illumination-robust face recognition via sparse representation	minimization;training;camera illumination robust face recognition signal representation method sparse representation based classification;training data;face recognition;signal representation;signal classification;minimization training data lighting face recognition training sparse matrices face;face;lighting;signal representation cameras face recognition signal classification;sparse representation;sparse matrices;cameras	Sparse representation receives a lot of attention as a representation method of signals in recent years. The sparse representation-based classification (SRC) applies sparse representation to face recognition. Camera parameters and/or illumination change may occur in real situations of face recognition. This study aims to propose a illumination robust SRC. The proposed method modifies the bases which are added to training bases derived from the training data set to detect noise contained in the recognized facial image. In the case where a few training data set is prepared, this method succeeds to obtain the higher recognition rate than the conventional SRC.	facial recognition system;image;mathematical optimization;sample rate conversion;sparse approximation;sparse matrix;test set	Koji Inoue;Yoshimitsu Kuroki	2011	2011 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2011.6115969	facial recognition system;face;computer vision;training set;sparse matrix;computer science;machine learning;pattern recognition;sparse approximation;three-dimensional face recognition;lighting	Vision	29.45982002340014	-46.922400118087964	158747
4c2618e5fb39ec35a57b399a19b8dbdc16589b47	zigzagnet: efficient deep learning for real object recognition based on 3d models		Effective utilization on texture-less 3D models for deep learning is significant to recognition on real photos. We eliminate the reliance on massive real training data by modifying convolutional neural network in 3 aspects: synthetic data rendering for training data generation in large quantities, multi-triplet cost function modification for multi-task learning and compact micro architecture design for producing tiny parametric model while overcoming over-fit problem in texture-less models. Network is initiated with multi-triplet cost function establishing sphere-like distribution of descriptors in each category which is helpful for recognition on regular photos according to pose, lighting condition, background and category information of rendered images. Fine-tuning with additional data further meets the aim of classification on special real photos based on initial model. We propose a 6.2 MB compact parametric model called ZigzagNet based on SqueezeNet to improve the performance for recognition by applying moving normalization inside micro architecture and adding channel wise convolutional bypass through macro architecture. Moving batch normalization is used to get a good performance on both convergence speed and recognition accuracy. Accuracy of our compact parametric model in experiment on ImageNet and PASCAL samples provided by PASCAL3D+ based on simple Nearest Neighbor classifier is close to the result of 240 MB AlexNet trained with real images. Model trained on texture-less models which consumes less time for rendering and collecting outperforms the result of training with more textured models from ShapeNet.	3d modeling;deep learning;outline of object recognition	Yida Wang;Can Cui;Xiuzhuang Zhou;Weihong Deng	2016		10.1007/978-3-319-54190-7_28	computer vision;pattern recognition;rendering (computer graphics);convolutional neural network;artificial intelligence;parametric model;architecture;normalization (statistics);computer science;deep learning;3d single-object recognition;machine learning;k-nearest neighbors algorithm	ML	29.384393557837292	-49.2019706540442	158792
0180cfc57a0f6f54cd9049cf9b512c3d484cc633	integrated sensing and processing decision trees	back end performance metric;pattern clustering;local dimensionality reduction classification clustering adaptive sensing sequential sensing;adaptive sensing;sensor systems;decision tree;image resolution;misclassification rate;algorithms artificial intelligence decision support techniques pattern recognition automated systems integration;hyperspectral sensors;sequential sensing;layout;indexing terms;classification;performance metric;partitioning metric;classification setting;integrated sensing and processing decision trees;partitioning metric integrated sensing and processing decision trees adaptive sequential sensing classification setting sensor optimization back end performance metric misclassification rate local dimensionality reduction;local dimensionality reduction;clustering;decision support techniques;magnetic resonance imaging;pattern classification;decision trees pattern recognition layout sensor systems hyperspectral sensors spatial resolution magnetic resonance imaging hyperspectral imaging image resolution classification tree analysis;pattern recognition;artificial intelligence;algorithms;pattern recognition automated;classification tree analysis;adaptive sequential sensing;hyperspectral imaging;sensor optimization;decision trees;systems integration;dimensional reduction;pattern clustering decision trees pattern classification;spatial resolution	We introduce a methodology for adaptive sequential sensing and processing in a classification setting. Our objective for sensor optimization is the back-end performance metric-in this case, misclassification rate. Our methodology, which we dub Integrated Sensing and Processing Decision Trees (ISPDT), optimizes adaptive sequential sensing for scenarios in which sensor and/or throughput constraints dictate that only a small subset of all measurable attributes can be measured at any one time. Our decision trees optimize misclassification rate by invoking a local dimensionality reduction-based partitioning metric in the early stages, focusing on classification only in the leaves of the tree. We present the ISPDT methodology and illustrative theoretical, simulation, and experimental results.	decision trees;decision tree;dimensionality reduction;jamie wilkinson;mathematical optimization;plant leaves;simulation;subgroup;throughput;trees (plant)	Carey E. Priebe;David J. Marchette;Dennis M. Healy	2004	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2004.12	image resolution;computer science;hyperspectral imaging;magnetic resonance imaging;machine learning;decision tree;pattern recognition;data mining	ML	31.921790258774344	-42.10624988350697	158805
516a014f4654c90a22ae3d363b6e80bda68a084d	adaptive human-centered representation for activity recognition of multiple individuals from 3d point cloud sequences	clutter;multipeople simon says games adaptive human centered representation activity recognition multiple individuals 3d point cloud sequences human centered robotics applications childhood education adhuc representation local spatio temporal features lst features armi human centered detector affiliation regions depth data mining human localization dynamic background clutter mobile robots moving cameras linear perspective view variations multichannel information encoding meka humanoid robot;robot vision data mining feature extraction humanoid robots image colour analysis image representation image sensors image sequences mobile robots object recognition;three dimensional displays;image color analysis;feature extraction;robot vision systems;feature extraction three dimensional displays cameras robot vision systems clutter image color analysis;cameras	Activity recognition of multi-individuals (ARMI) within a group, which is essential to practical human-centered robotics applications such as childhood education, is a particularly challenging and previously not well studied problem. We present a novel adaptive human-centered (AdHuC) representation based on local spatio-temporal features (LST) to address ARMI in a sequence of 3D point clouds. Our human-centered detector constructs affiliation regions to associate LST features with humans by mining depth data and using a cascade of rejectors to localize humans in 3D space. Then, features are detected within each affiliation region, which avoids extracting irrelevant features from dynamic background clutter and addresses moving cameras on mobile robots. Our feature descriptor is able to adapt its support region to linear perspective view variations and encode multi-channel information (i.e., color and depth) to construct the final representation. Empirical studies validate that the AdHuC representation obtains promising performance on ARMI using a Meka humanoid robot to play multi-people Simon Says games. Experiments on benchmark datasets further demonstrate that our adaptive human-centered representation outperforms previous approaches for activity recognition from color-depth data.	activity recognition;algorithm;benchmark (computing);clutter;color depth;encode;human-centered computing;humanoid robot;mobile robot;point cloud;relevance;robotics;visual descriptor	Hao Zhang;Christopher M. Reardon;Chi Zhang;Lynne E. Parker	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139459	computer vision;simulation;feature extraction;computer science;clutter	Robotics	36.9587791044223	-49.29033401677052	158823
ee2c4c0bc3b2b4d6ad4378dedc55479cc59e3ca7	face hallucination and recognition using kernel canonical correlation analysis		Canonical correlation analysis (CCA) is a classical but powerful tool for image super-resolution tasks. Since CCA in essence is a linear projection learning method, it usually fails to uncover the nonlinear relationships between high-resolution (HR) and low-resolution (LR) facial image features. In order to solve this issue, we propose a new face hallucination and recognition algorithm based on kernel CCA, where the nonlinear correlation between HR and LR face features can be well depicted by implicit high-dimensional nonlinear mappings determined by specific kernels. First, our proposed method respectively extracts the principal component features from high-resolution and low-resolution facial images for computational efficiency and noise removal. Then, it makes use of kernel CCA to learn the nonlinear consistency of HR and LR facial features. The proposed approach is compared with existing face hallucination algorithms. A number of experimental results on LR face recognition have demonstrated the effectiveness and robustness of our proposed method.	face hallucination;kernel (operating system)	Zhao Zhang;Yunhao Yuan;Yun Li;Bin Li;Ji-Peng Qiang	2017		10.1007/978-3-319-70136-3_67	machine learning;feature (computer vision);kernel (linear algebra);robustness (computer science);pattern recognition;artificial intelligence;facial recognition system;principal component analysis;canonical correlation;computer science;face hallucination;projection (linear algebra)	Vision	25.410154265438926	-41.88838299404918	158863
