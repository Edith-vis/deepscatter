id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
498005bd75f33b1ad5bdaf5254daa559393141a6	combined force display system of emg sensor for interactive performance		"""This is a report of research and some experimental applications of human-computer interaction in computer music and interactive media arts. In general, many sensors are used for the interactive communication as interfaces, and the performer receives the output of the system via graphics, sounds and physical reactions of interfaces like musical instruments. I have produced many types of interfaces, not only with mechanical/electrical sensors but also with biological/physiological sensors. This paper is intended as an investigation of some special approaches: (1) 16-channel electromyogram sensing system called """"MiniBioMuse-III"""" and its applications, (2) 8-channel electric-feedback system and its applications, (3) combination of EMG sensor and biofeedback system sharing same electrode to construct the """"force display"""" effect of live control with EMG sensors."""	electromyography;graphics;human–computer interaction;interactive media;sensor	Yoichi Nagashima	2003			multimedia;the arts;interactive media;computer music;graphics;computer science	HCI	-46.2876044420054	-36.585937195299046	43103
7969288576c072e55a8e57c105eb9e8c388ea33a	special session 2 computer vision and image analysis		Computer vision and Image analysis have taken advantage of five decades of intense development. The complexity of the algorithms in this field can now benefit from dramatically increasing computational power to offer viable applications going from embedded systems to cloud computing. This special session proposes three papers dealing with the implementation of computer vision and image analysis applications on embedded systems.	algorithm;cloud computing;computation;computer vision;embedded system;image analysis	Patrice Delmas;Rachel Ababou	2016		10.1109/DASIP.2016.7853819	computer vision;computer science;theoretical computer science;multimedia;computer graphics (images)	Vision	-46.96724561128854	-28.63156957481584	43121
7707439d565a4ca8a9ec150e2c73c9c05b38ea6d	gavrinis: the raising of digital stones	photogrammetry;art;geometric analysis digital stones neolithic art extraordinary engravings digital multiscalar technology photogrammetric technology low tech scanning methods clean 3d model;reconstruction;solid modelling art;real time visits;passage grave;art joints vegetation databases three dimensional displays solid modeling data models;public presentation;neolithic;low tech techniques;lasergrammetry;archeology;realtime visits gavrinis neolithic cairn passage grave archeology lasergrammetry photogrammetry low tech techniques reconstruction public presentation;cairn;gavrinis;solid modelling	The Tumulus of Gavrinis, which is located in Brittany, is one of the best known monuments of Neolithic art because of its extraordinary engravings. A joint team of archaeologists and architects began recording the monument using digital multiscalar technology in 2011. The scanning was complicated by the narrowness of the inner spaces, uncomfortable conditions, granulometry of the stone, and extensive vegetation coverage. A huge database was created by using a combination of photogrammetric technology and low-tech scanning methods. A lot of post-production work was needed to turn the cloud of data into a clean 3D model, since the original tumulus is irregular and lacks any repetitive patterns or predictive geometry. The goal was to obtain a detailed geometric analysis of the tumulus in order to develop a better understanding of its architecture and engravings, which are sometimes less than a millimeter deep. This article covers the technological solutions that we applied while confronting situations ranging from the need to enhance a carved figure to our effort to reconstruct the entire monument by collating the results of several different recording techniques.	geometric analysis;granulometry (morphology);photogrammetry;polygonal modeling	Laurent Lescop;Serge Cassen	2013	2013 Digital Heritage International Congress (DigitalHeritage)	10.1109/DigitalHeritage.2013.6744825	visual arts;geography;archaeology;cartography	Visualization	-35.696002218244224	-31.924895988688757	43169
053370f59aa87815eb858ddbba432b451bf4a11f	dynamic, flexible and multi-dimensional visualization of digital photos and their metadata		Digital photos and their metadata get explosively growth, requiring better support to browsing them. We propose a dynamic and flexible visualization of digital photos and their metadata. A prototype is designed and implemented based on the algorithm of D-FLIP, our previous work for flexibly displaying a large photo collection. We design various dynamic photo visualizations with up to four-dimensional meta-information by using Bertin's visual variables and three-dimensional representation. It allows users to dynamically and effectively manage photo visualizations by selecting meta-information of user's current needs and interest.	algorithm;prototype	Xin Huang;Kazuki Takashima;Kazuyuki Fujita;Yoshifumi Kitamura	2018		10.1145/3279778.3279923	computer graphics (images);metadata;visualization;computer science	HCI	-34.01429231570852	-34.044072764990354	43197
b701ca22851158390127e69bda4dde8a4fe4f5ad	a digital waveguide-based approach for clavinet modeling and synthesis	signal image and speech processing;quantum information technology spintronics	The Clavinet is an electromechanical musical instrument produced in the mid-twentieth century. As is the case for other vintage instruments, it is subject to aging and requires great effort to be maintained or restored. This paper reports analyses conducted on a Hohner Clavinet D6 and proposes a computational model to faithfully reproduce the Clavinet sound in real time, from tone generation to the emulation of the electronic components. The string excitation signal model is physically inspired and represents a cheap solution in terms of both computational resources and especially memory requirements (compared, e.g., to sample playback systems). Pickups and amplifier models have been implemented which enhance the natural character of the sound with respect to previous work. A model has been implemented on a real-time software platform, Pure Data, capable of a 10-voice polyphony with low latency on an embedded device. Finally, subjective listening tests conducted using the current model are compared to previous tests showing slightly improved results.	amplifier;computation;computational model;computational resource;electronic component;embedded system;emulator;pure data;real-time transcription;requirement;speech synthesis	Leonardo Gabrielli;Vesa Välimäki;Henri Penttinen;Stefano Squartini;Stefan Bilbao	2013	EURASIP J. Adv. Sig. Proc.	10.1186/1687-6180-2013-103	computer vision;simulation;telecommunications;computer science;electrical engineering;machine learning;algorithm	Embedded	-45.9828917214923	-34.25054697336811	43237
903a5f3d0a7285c1b32259d88f787b5708e3af75	rapid prototyping of medical graphic interfaces	graphical interface	Rapid prototyping is an adequate methodology for the development of window based graphic interfaces because it allows an effective integration of experts in human factors and potential users in the process of the production of the interaction software. We have implemented an environment for the rapid prototyping of medical graphic interfaces based on the characteristic architecture of knowledge based systems. It provides a specification language for the declarative representation of the elementary dialogue components, including the visual aspect, behavior and content of the windows. This representation is directly executable and permits an incremental approach to the final interface. The environment provides a universal control mechanism and facilities for the integration of the interface with relational data bases and expert systems. The paper present a general description of our system and details of the method for the representation of the interaction.	computer graphics;rapid prototyping	Roque Marín;Maria Taboada;Ramón P. Otero;Alvaro Barreiro;José Mira Mira;Ana E. Delgado	1992			simulation;human–computer interaction;computer science;theoretical computer science;graphical user interface	HCI	-40.36293826916282	-29.330639666886995	43264
93ada3d3195cc925919a6d0b598c02031e81fb3c	afasia: the ultimate homeric one-man-multimedia-band	interactive music;real time;real time musical systems;multimedia interaction;musical robots	In this paper we present Afasia, an interactive multimedia performance based in Homer’s Odyssey [2]. Afasia is a one-man digital theater play in which a lone performer fitted with a sensor-suit conducts, like Homer, the whole show by himself, controlling 2D animations, DVD video and conducting the music mechanically performed by a robot quartet. After contextualizing the piece, all of its technical elements, starting with the hardware input and output components, are described. A special emphasis is given to the interactivity strategies and the subsequent software design. Since its first version premiered in Barcelona in 1998, Afasia has been performed in many European and American countries and has received several international awards.	dynamic music;input/output;interactivity;midi;software design	Sergi Jordà	2002			visual arts;real-time computing;simulation;human–computer interaction;computer science;artificial intelligence;multimedia;computer graphics (images)	Robotics	-47.57064256683615	-32.725386614869635	43330
6d2d39f597a4f7ba6c73ef24f6fb3b30133f4883	modeling and simulation with augmented reality	objet synthetique;rairo ro;aplicacion militar;sequence video;application militaire;medical simulation;modeling and simulation;simulation;technique video;simulacion;tecnica video;journal;rairo;realite augmentee;realidad aumentada;recherche operationnelle;imaging;image sequence;rairo operations research;military application;realite augmentee visuelle;video technique;formation image;secuencia imagen;formacion imagen;augmented reality;edp sciences;esaim;sequence image	In applications such as airport operations, military simulations, and medical simulations, conducting simulations in accurate and realistic settings that are represented by real video imaging sequences becomes essential. This paper surveys recent work that enables visually realistic model constructions and the simulation of synthetic objects which are inserted in video sequences, and illustrates how synthetic objects can conduct intelligent behavior within a visual augmented reality.	augmented reality;autonomous robot;database;entity;graphical user interface;graphics;image segmentation;real-time locating system;real-time transcription;rendering (computer graphics);simulation;synthetic intelligence	Khaled Hussain;Varol Kaptan	2004	RAIRO - Operations Research	10.1051/ro:2004014	medical simulation;augmented reality;simulation;computer science;modeling and simulation;computer graphics (images)	Graphics	-34.57942357983479	-28.47898636536723	43331
000f046ab02afa1fee6a0d94926a43649174c0ca	haystack: a platform for creating, organizing and visualizing semistructured information	user interface;context menus;agents;semantic web;drag and drop;informal user interfaces;semistructured information;rdf	1. OVERVIEW In this demonstration we present Haystack, an environment that allows users to easily manage their documents, e-mail, appointments, tasks, and other information [1]. Haystack uses a semistructured data model to describe the connections between different documents in a user’s corpus as well as the metadata concerning each document. This amalgamation provides users with a unified framework for managing all of their information, e.g., documents, emails, etc., through a single interface. Furthermore, Haystack’s user interface exposes general tools for navigating the various kinds of information found in users’ corpora.	data model;email;organizing (structure);text corpus;unified framework;user interface	David Huynh;David R. Karger;Dennis Quan;Vineet Sinha	2003		10.1145/604045.604116	computer science;artificial intelligence;software agent;semantic web;rdf;database;internet privacy;user interface;world wide web	DB	-41.40386455416379	-24.51501860610558	43486
bf349f517cdcd36cc9758a9a2891e07f7a567095	thear: development of a mobile multimodal audiometry application on a cross-platform framework	auditory system;speech;servers;mobile communication;mobile handsets;speech recognition;calibration	This paper presents our ongoing research in the field of audiometry application development. There are many researches on the development of audiometry application but they are all platform specific. In this paper, we developed a new multimodal framework that enabled cross-platform development by using open standards such as HTML5, CSS5 and JavaScript. We will describe the architecture of the new multimodal framework which supports the conduction of the automatic audiometry. Based on the framework, we further propose three multimodal speech audiometry approaches for supporting automatic speech audiometry on mobile devices. By using the framework combined with the three multimodal speech audiometry approaches, we developed an application called THear (TsingHua ear) to support the conduction of pure-tone audiometry and speech audiometry on mobile devices. A preliminary performance evaluation on THear will also be reported.	html5;javascript;mobile device;multimodal interaction;performance evaluation	Wai-Kim Leung;Jia Jia;Yu-Hao Wu;Jiayu Long;Lianhong Cai	2016	2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)	10.1109/ISCSLP.2016.7918397	calibration;speech recognition;mobile telephony;computer science;speech;linguistics;server	SE	-46.22501183479865	-37.274701482653455	43525
7cb99b2449e108c10840e1ba79a94b332e2ad996	spatio-temporal visualization of battlefield entities and events	interfase usuario;aplicacion militar;application militaire;usability testing;guerra;user interface;computer graphics;base donnee temporelle;software systems;war;spatio temporal data;data visualization;military application;utilisabilite;interface utilisateur;visualisation donnee;temporal databases;usabilidad;usability;grafico computadora;infographie;guerre	In this work, we address visualization of spatio-temporal data for military application. Four different visualization prototypes have been developed to track the movement of military entities across a land surface over time; three more have been developed to track the occurrences of numerous war events. We have implemented the prototypes in a software system with a novel clock face GUI. Usability tests have been carried out and confirmed the effectiveness of the solution.	entity;graphical user interface;human factors and ergonomics;software system;usability	Qiyue Fong;Foo Meng Ng;Zhiyong Huang	2006		10.1007/11784203_59	simulation;usability;human–computer interaction;computer science;operating system;database;computer security;data visualization;computer graphics (images)	Visualization	-34.283759516785835	-28.3914745057551	43537
34d7707875809686bae5630f7c9e9df2ecb81e33	support tools for graphs in computer science education	graph theory;computer science education visualization computer science application software tree graphs animation shape control user interfaces informatics software systems;computer graphics;software systems;graphs;computer science education;dictionaries;electronic version computer science education graph support tools software systems graph models complex situations visual processing books graph algorithms dictionary;courseware graphs graph theory computer science education computer graphics dictionaries;graph algorithm;computer education;graph model;courseware;visual processing	The main thesis of this paper is that intricate nature of software systems and computer education systems can, and in our opinion should, be represented by graph models. Graphs are used in computer science and computer education almost everywhere, since graph is a very natural way of explaining complex situations on an	computer science;software system	Victor N. Kasyanov	2001		10.1109/ICALT.2001.943930	natural language processing;adjacency list;wait-for graph;computer science;graph theory;theoretical computer science;graph;graph;graph drawing;computer graphics;software system;graph rewriting	Logic	-33.92763852379988	-30.3344438857311	43772
06d13a5515a815a7ca32ee8cb562e1a1c879c20f	enveloping users and computers in a collaborative 3d augmented reality	groupware;input device;2d window managers collaborative 3d augmented reality emmie environment management multiuser information environments experimental user interface collaborative augmented environment 3d virtual space hybrid user interface 3d widgets physical objects tracked displays input devices see through head worn displays virtual ether 2d displays 3d displays drag and drop;user interface;information privacy;hybrid user interfaces;window manager;groupware augmented reality three dimensional displays;operating system;3d environment;three dimensional displays;3d widget;physical environment;collaboration environmental management prototypes three dimensional displays user interfaces space technology information management virtual environment visualization two dimensional displays;virtual environment;augmented reality;drag and drop;virtual space;3d display	We present EMMIE (Environment Management for Multiuser Information Environments), a prototype experimental user interface to a collaborative augmented environment. Users share a 3D virtual space and manipulate virtual objects that represent information to be discussed. We refer to EMMIE as ahybrid user interfacebecause it combines a variety of different technologies and techniques, including virtual elements such as 3D widgets, and physical objects such as tracked displays and input devices. See-through headworn displays overlay the virtual environment on the physical environment, visualizing the pervasive “virtual ether” within which all interaction occurs. Our prototype includes additional 2D and 3D displays, ranging from palm-sized to wall-sized, allowing the most appropriate one to be used for any task. Objects can be moved among displays (including across dimensionalities) through drag & drop. In analogy to 2D window managers, we describe a prototype implementation of a shared 3D environment manager that is distributed across displays, machines, and operating systems. We also discuss two methods we are exploring for handling information privacy in such an environment.	2d computer graphics;3d computer graphics;augmented reality;drag and drop;information privacy;input device;operating system;pervasive informatics;prototype;user interface;virtual reality;window manager	Andreas Butz;Tobias Höllerer;Steven K. Feiner;Blair MacIntyre;Clifford Beshers	1999		10.1109/IWAR.1999.803804	augmented reality;simulation;stereo display;human–computer interaction;information privacy;computer science;virtual machine;operating system;multimedia;user interface;input device	HCI	-43.8509453888385	-37.57727221615737	43797
7a539f39a310004cdaaad68ea128133da8665d04	the sigcse 2001 maze demonstration program	object oriented design;introductory courses;introductory computer science;large courses;graphic user interface;course management	This article will describe the SIGCSE 2001 Maze Demo program that may be used as a CS2 laboratory exercise on traversal algorithms. The article will also describe the object-oriented design of the program and the Java Power Tools that were used to enable rapid development of its graphical user interface. Finally, the quality of the program and the speed of its development shows that it is now practical to teach freshmen using full graphical user interfaces rather than interfaces that use the console or a small restricted set of interface widgets.	algorithm;graphical user interface;java;sigcse;tree traversal	Richard Rasala;Jeff Raab;Viera K. Proulx	2002		10.1145/563340.563455	simulation;human–computer interaction;computer science;object-oriented design;software engineering;graphical user interface;programming language;user interface;computer graphics (images)	HCI	-48.05423499775377	-27.82016538337154	43886
509bd9ec057802a38ee955c27aec5716f163a1b7	the actuated guitar: implementation and user test on children with hemiplegia		People with a physical handicap are often not able to engage and embrace the world of music on the same terms as normal functioning people. Traditional musical instruments have been refined over the last centuries, developing highly specialized and powerful interfaces; but nearly all require two functioning hands. In this study we try to enable people with Hemiplegia to play a real electrical guitar, by modifying it in a way that allows people with Hemiplegia able to actually use the instrument. We developed a guitar platform utilizing sensors to capture the rhythmic motion of alternate fully functioning limbs, such as a foot, knee or the head to activate a motorized fader moving a pick back and forth across the strings. This approach employs the flexibility of a programmable digital system which allows us to scale and map different ranges of data from various sensors to the motion of the actuator, thereby making it easier to adapt to individual users. To validate and test the instrument platform we collaborated with the Helena Elsass Center in Copenhagen, Denmark during their 2013 Summer Camp, to see if we actually succeeded in creating an electrical guitar that children with Hemiplegia could play. The initial user studies showed that children with Hemiplegia were able to play the actuated guitar by producing rhythmical movement across the strings, enabling them to enter a world of music they so often see as closed.	dos;digital electronics;sensor	Jeppe Madura Larsen;Dan Overholt;Thomas B. Moeslund	2014			embedded system;simulation;computer hardware	HCI	-46.441065970583054	-36.542916057413954	43915
6e11704a55ef501e0c17896518f0a25c0c4c86d1	the synthesis of non-photorealistic motion effects for cartoon	image motion analysis;animators;stylistic motion impressions;parametric control;realistic images computer animation image motion analysis;realistic images;nonphotorealistic motion effects;computer animation;animation painting motion estimation atmosphere couplings motion control video sequences feature extraction communications technology displays;parametric control nonphotorealistic motion effects cartoon videos animators stylistic motion impressions;cartoon videos	In cartoon videos, animators use stylistic motion impressions to present fast moving called non-photorealistic motion effects. These motion effects not only represent the direction of motion but also set off the atmosphere by contrast. But, for drawing these effects animators need to spend extra time. According to these, we propose a system to generate these effects automatically. In order to systematically display various effects, we classify them and analyze their linkage and difference. Furthermore, we establish a proper procedure to accomplish painting and animators can create different motion expressions through parametric control	algorithm;linkage (software);motion capture	Tian-ding Chen	2006	Sixth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2006.253717	computer vision;computer science;computer animation;multimedia;computer graphics (images)	Robotics	-39.68037060789091	-37.798256132673274	43967
1619b220ebb237424216d15af1cc48a353a3fc09	haptic feedback in a computer music performance interface	arte		haptic technology	Lonny Chu	1996			simulation;human–computer interaction;multimedia	HCI	-48.11650570873909	-33.921200893704395	44041
3672fe5b713d25069e8d44563f3e0358034f672c	practical 3d animation of multiply articulated construction equipment	vitascope visualization system;software tool;multiply-articulated construction equipment;discrete-event construction process model;specific task;multiply-articulated virtual construction equipment;instantiate specific piece;animation method;external software process;common construction task;3d animation;data visualisation;inverse kinematics;software process;discrete event simulation;kinematics;visual system	"""This paper presents research that led to the design and implementation of practical 3D animation methods to visualize multiply-articulated construction equipment in 3D animations of discrete-event construction process models. Using principles of forward and inverse kinematics, we designed and implemented generic pieces of multiply-articulated virtual construction equipment that accept task-level instructions from external software processes. Discrete event simulation models can configure and instantiate specific pieces of such equipment and instruct them to perform common construction tasks using simple, parametric statements of text. Once instructed to perform specific tasks (e.g. Load dirt), these """"smart"""" pieces of equipment (e.g. Backhoes) automatically decipher the sequence and amplitudes of the elemental motions their components (e.g. Boom) must undergo to accomplish those tasks. The animation methods are implemented in a software tool called KineMach that integrates as an add-on with the VITASCOPE visualization system."""	add-ons for firefox;computer animation;elemental;inverse kinematics;organizing (structure);programming tool;relevance;simulation;vitascope	Vineet R. Kamat;Julio C. Martínez	2004	Proceedings of the 2004 Winter Simulation Conference, 2004.		computer simulation;simulation;visualization;computer science;engineering;engineering drawing;computer graphics (images)	Graphics	-38.573655856189816	-30.645511027316534	44062
0cfac0612df8cb6090dc1f96b93be8216c182d71	a natural language model for managing tv-anytime information in mobile environments	modelizacion;television;documento electronico;lenguaje natural;informacion numerica;ontologie;informatique mobile;mobile device;metadata;telephone portable;pervasive computing;digital tv;specification;langage naturel;mobile phone;document electronique;informatica difusa;modelisation;digital information;mobile environment;user profile;telefono movil;especificacion;informatique diffuse;natural language;comportement utilisateur;metadonnee;ambiguity;ontologia;information numerique;interaction model;metadatos;user behavior;mobile computing;ambiguedad;modeling;ontology;comportamiento usuario;electronic document;ambiguite	The TV-Anytime standard describes structures of categories of digital TV program metadata, as well as User Profile metadata for TV programs. We describe a natural language model for the users to interact with the TVAnytime metadata and preview TV programs from their mobile devices. The language utilizes completely the TV-Anytime metadata specifications and it can accommodate future metadata extensions. The interaction model does not use clarification dialogues, but it uses the user profiles to rank the possible answers in case of ambiguities, as well as TV-Anytime Metadata information and ontologies with information concerning digital TV. We describe an implementation of the language that runs on a PDA and a mobile phone and manages the metadata on a remote TV-Anytime compatible TV set.	anytime algorithm;language model;natural language;tv-anytime	Anastasia Karanastasi;Fotis G. Kazasis;Stavros Christodoulakis	2004		10.1007/978-3-540-30188-2_2	geospatial metadata;systems modeling;computer science;operating system;marker interface pattern;ontology;mobile device;database;multimedia;database catalog;natural language;television;mobile computing;metadata;world wide web;computer security;specification;data element;meta data services;metadata repository	NLP	-37.80413946966123	-26.18617224690718	44154
617dd2bf92f1fc9f9166ea8c541027abc066ee71	generative choreography: animating in real-time dancing avatars	novel approach;virtual world;critical aspect;choreographic movement;computational ecosystem;evolutionary art history;generative choreography;real-time dancing avatar;ai model;technical aspect	In this paper we introduce a novel approach to dance choreographies in virtual worlds. We present a dance performed by avatars in a virtual world, where a computational ecosystem provides a mechanism driving the actions and movements of the avatars. First, we discuss the background and motivations, and describe the performance. Then, we describe the technical aspects of the algorithm driving the choreographic movements. Finally we discuss its critical aspects and contextualize the work with regards to dance practice and evolutionary art history. In the process of this discussion, we emphasize the advantages of the AI model of computational ecosystems for the animation of non-player-characters.	avatar (computing);generative science;real-time transcription	Rui Filipe Antunes;Frederic Fol Leymarie	2012		10.1007/978-3-642-29142-5_1	simulation;art;multimedia;communication	HCI	-47.770730973015255	-34.709531698097	44227
a541dcce98c418f6f91f57f3cd1af284022f0f8e	towards the third generation: the case for ikbh (intelligent knowledge-based hypermedia) environments	knowledge base		hypermedia	Roderick I. Nicolson	1989			computer science;multimedia;hypermedia;human–computer interaction;knowledge base	Robotics	-45.025743942803	-26.050955541424763	44332
9db8035a4739d98305507db5e5edaba04c92659f	aframe: a domain specific language for virtual reality: extended abstract	html5;dsls;virtual reality	Do Domain Specific Languages (DSLs) have anything to contribute towards the building and animating of three-dimensional scenes for Virtual Reality (VR)? Yes! AFrame is a DSL, and a widely-supported HTML5 extension for building virtual reality experiences. AFrame is open-source, actively developed by Mozilla, and works inside most modern browsers, including mobile browsers. Customizable web-based delivery of VR experiences is possible, because AFrame is a DSL.	digital subscriber line;domain-specific language;html5;open-source software;virtual reality;web application	Andy Gill	2017		10.1145/3039895.3039899	simulation;computer science;multimedia;world wide web	Visualization	-45.304942090477056	-30.791956184469864	44398
ebb5f2c7c582176f52f6d1cdf2b23c7789c501ae	a system to restructure hypertext networks into valid user models	restructuration;lien hypertexte;hipertexto;navegacion informacion;learning;navigation information;web;information browsing;restructuracion;hyperlink;automatisation;automatizacion;dynamic linking;systeme adaptatif;aprendizaje;apprentissage;internet;comportement utilisateur;adaptive system;mathematical model;sistema adaptativo;user behavior;network structure;hypertexte;hypertext;comportamiento usuario;user model;automation	We have implemented an experimental system that automatically restructures hypertext networks according to their users’ browsing behavior. The system applies link weights to the hyperlinks in the networks and updates these link weights according to three learning rules. The learning rules are based on how often a particular hyperlink is being traversed and operate on strictly local information of link traversals. Changes in network structure are fed back to users by dynamic link ordering according to descending link weight. The system has been shown to be able to structure random hypertext networks into valid representations of their users’ browsing preferences in two WWW experiments and a simulation using a mathematical model of user navigation.	dynamic-link library;experiment;experimental system;hyperlink;hypertext;mathematical model;simulation;www	Johan Bollen;Francis Heylighen	1998	The New Review of Hypermedia and Multimedia	10.1080/13614569808914701	the internet;simulation;user modeling;hypertext;computer science;adaptive system;automation;mathematical model;hyperlink;multimedia;world wide web	Web+IR	-38.254630501432814	-25.53732486221538	44430
f874c5ada2b45704dcb8fd19dade8d3597ceafa6	3d authoring for content experts: a collaborative approach	groupware;computer graphics;virtual reality;product model;information content;authoring systems;computer graphic;computer graphics authoring systems virtual reality groupware;3d model;interactive application;design and implementation;interaction model;interactive graphics;virtual worlds 3d authoring content experts collaborative approach production model 3d scenes computer graphics interactivity graphic complexity intermediate actor application domain;collaboration buildings production natural languages user interfaces layout computer graphics tail navigation cultural differences;virtual worlds	The current production model for 3D scenes, inherited from the early years of computer graphics, prevents a real collaboration among the subjects involved and, as a consequence, the full exploitation of the new potentialities of the 3D media. In this scenario the content expert is only a consultant of the 3d modeler, who is directly responsible for designing the interaction model. In complex 3D applications interactivity, graphic complexity and content play different important roles that should correspond to different well defined skills: authoring is a collaborative activity that involves different experts. The content expert is the key figure of the extended team of authors that collaborate to the design and implementation of a complex interactive application. He/she is an intermediate actor between the final users and the experts devoted to the design of the low level graphic details: is an expert in the application domain and understands the needs of the final users with respect to orientation, navigation and access to the information content; is able to speak the technical language needed to describe the interaction but needs to be provided with high level interaction interface, built by the graphic experts, because is not necessarily skilled in the fine-grained language of interaction with the technology used to build virtual worlds.	application domain;computer graphics;high-level programming language;interactivity;jargon;self-information;virtual world	Fabio Pittarello;Augusto Celentano	2001		10.1109/HCC.2001.995288	simulation;human–computer interaction;computer science;multimedia	HCI	-39.22181819563733	-32.4181056038436	44440
bfd3b7404aa67ac36ce4bb10353413043f181a18	armida tm: multimedia applications across atm-based networks accessed via internet navigation	multimedia;davic;internet;atm	ARMIDA TM (Applications Retrieving Multimedia Information Distributed over ATM) is a prototypical system for experimental applications based on the interactive retrieval of multimedia information from remote data bases over ATM networks. ARMIDA TM aims at being compliant with the specifications issued by the “Digital Audio Visual Council” (DAVIC) for interactive multimedia services and applications, as well as at representing a sample implementation of the system specified in DAVIC 1.0. The present paper illustrates the way ARMIDA TM works to provide these applications, by describing the system hardware and software architecture and its main components, with a particular concern towards the way they interact to implement the required functionality.	atm turbo;database;reference implementation;software architecture;xml:tm	Stefano Dal Lago;Guido Franceschini;Pietro Marchisio;Marco Mesturino;Enrico Polese;Giovanni Venuti	1997	Multimedia Tools and Applications	10.1023/A:1009637130801	embedded system;the internet;computer science;operating system;atmosphere;multimedia;world wide web;computer network	HPC	-46.96723476256846	-25.577072887307583	44538
93a526cc054052dc107e6312fc0fb8993ec910b6	practical computer graphics for scientific users : philosophy and implementation	computer graphic	Abstract   The scientific computer user presents a unique challenge to the graphics system designer. If the graphics system is properly designed and implemented, computer graphics becomes a vital research tool for the scientific user. These users' desires and expectations play an important role in every phase of the design process.  This paper describes the philosophy of the graphics system at the National Center for Atmospheric Research (NCAR) and its implementation. Details of algorithms are not presented; rather the purpose has been to describe a successful computer graphics system which may serve as a guide for other designers who desire to provide practical computer graphics. This graphics system operates under a batch-mode computer system without using interactive terminals.	computer graphics	Thomas J. Wright	1975	Computers & Graphics	10.1016/0097-8493(75)90002-3	graphics pipeline;scientific visualization;human–computer interaction;computer science;graphics software;computer graphics;software rendering;computer graphics (images)	Visualization	-46.91125933102147	-28.36196097532928	45241
10b28ac5e5c3d06adc1a514bc3d22fa99acd138c	interest modeling in games: the case of dead reckoning	multi player online games;dead reckoning;ant colonies;interest modelling	In games, the goals and interests of players are key factors in their behavior. However, techniques used by networked games to cope with infrequent updates and message loss, such as dead reckoning, estimate a player’s movements based mainly on previous observations. The estimations are typically made by using dynamics of motion, taking only inertia and some external factors (e.g., gravity, wind) into account while completely ignoring the player’s goals (e.g., chasing other players or collecting objects). This paper proposes AntReckoning: a dead reckoning algorithm, inspired from ant colonies, which models the players’ interests to predict their movements. AntReckoning incorporates a player’s interest in specific locations, objects, and avatars in the equations of motion in the form of attraction forces. In practice, these points of interest generate pheromones, which spread and fade in the game world, and are a source of attraction. To motivate and validate our approach we collected traces from Quake III. We conducted specific experiments that demonstrate the effect of game-related goals, map features, objects, and other players on the mobility of avatars. Our simulations using traces from Quake III and World of Warcraft show that AntReckoning improves the accuracy by up to 44 % over traditional dead reckoning techniques and can decrease the upload bandwidth by up to 32 %.	algorithm;ant colony;dead reckoning;experiment;online and offline;overhead (computing);point of interest;quake engine;simulation;tracing (software);upload;world of warcraft	Amir Yahyavi;Kévin Huguenin;Bettina Kemme	2012	Multimedia Systems	10.1007/s00530-012-0275-z	dead reckoning;simulation;computer science;ant colony;multimedia;algorithm	HCI	-38.384765408942904	-36.66772599031436	45328
dde7bd9e273f802ea98d1ab70bb42fcec90b1de3	design of plastic spur gears using virtual reality	computer aided design;vr;cad;computer model;simulation;virtual reality;plastic mould;plastic spur gears	The research is a study into the design process of plastic spur gears using virtual reality (VR). It is aimed at developing a functional model and further validating the model for the production of plastic spur gears. It involves the development of a software application which will automatically generate gear design parameters. At the conclusion of the research, a flawless simulated computer model for the calculation of all parameters of plastic spur gears was developed in both DOS environment and in an enhanced interface using macromedia flash. The complete plastic spur gear profile was generated upon feeding in the major gear designing parameters. In conclusion, virtual reality can be applied as a useful teaching tool for the improvement of engineering learning processes especially in areas with limited access to required learning infrastructure.	virtual reality	O. T. Laseinde;S. B. Adejuyigbe	2014	IJCAET	10.1504/IJCAET.2014.058001	simulation;computer science;engineering;computer aided design;cad;virtual reality;engineering drawing;manufacturing engineering	Visualization	-36.913066636389765	-31.26876350074096	45381
9750317f42877e4380de403895d67aeac60823e6	room#81 - agent-based instrument for experiencing architectural and vocal cues		ROOM#81 is a digital art installation which explores how visitors can interact with architectural and vocal cues to intimately collaborate. The main space is split into two distinct areas separated by a soft wall, i.e. a large piece of fabric tensed vertically. Movement within these spaces and interaction with the soft wall is captured by various kinds of sensors. People’s activity is constantly used by an agent in order to predict their actions. Machine learning is then achieved by such agent to incrementally modify the nature of light in the room and some laryngeal aspects of synthesized vocal spasms. The combination of people closely collaborating together, light changes and vocal responses creates an intimate experience of touch, space and sound.	machine learning;sensor	Nicolas D'Alessandro;Roberto Calderon;Stefanie Müller	2011			simulation;acoustics;artificial intelligence	HCI	-48.11529377605132	-35.83719159861646	45388
dd00121129bb3e55a9a2a347908500b2dce010d8	modeling multimedia displays using action based temporal logic	temporal logic;presentations;multimedia;temporal presentations.;semantics;modeling;action languages;modeling multimedia;graphic user interface;system modeling;action language;complex dynamics	We present a metalanguage, named Alan that can be used to model dynamic multimedia displays, particularly those that display multimedia database query results. Alan is an action language that uses temporal logic to model nonMarkovian systems. We show how it can be used for specifying the behavior of fairly complex dynamic multimedia display systems, modeling all graphical user interface elements on the display plus the effects of actions and of the passage of time on media such as video and audio.	action language;database;graphical user interface;temporal logic	Graciela Gonzalez;Chitta Baral;Peter A. Cooper	2002			computer vision;computer science;theoretical computer science;multimedia	HCI	-36.01319249047773	-27.65924052223108	45454
176b35cce9d61f2e0198f72af6f90ab0efc45a18	a user interface for the online elucidation of natural language search statements			natural language user interface	V. Aragón Ramírez	1986			user interface design;human–computer interaction;natural user interface;user experience design;natural language user interface;monolithic application;computer science;interactive systems engineering;user interface;user requirements document	ML	-42.52127243660584	-29.312026691397552	45554
46ebbc3bd4f41ddf054894b9ccc7b165295d106e	an end user tool for customising personal spaces in ubiquitous computing environments	personal computing;user evaluation;informatique mobile;programming environment;pervasive computing;customization;personnalisation;intelligence artificielle;informatica difusa;medio ambiente programacion;space use;informatique diffuse;personalizacion;artificial intelligence;inteligencia artificial;mobile computing;informatique personnelle;environnement programmation;physical interaction;end user programming;ubiquitous computing environment	We present a variant of end-user programming targeting ubiquitous computing environments that allows non-technical users to create “programs” to customise their personal living spaces. Using this end-users do not need to write program code, or follow a rigid sequential list of actions in order to achieve results Rather they only need to show the system the required behaviour via physical interactions with the environment. Finally, we report on a user evaluation that indicates end-users find this approach to be a useful and enjoyable experience.	spaces;ubiquitous computing	Jeannette Shiaw-Yuan Chin;Victor Callaghan;Graham Clarke	2006		10.1007/11833529_109	embedded system;simulation;human–computer interaction;computer science;artificial intelligence;operating system;personalization;database;multimedia;mobile computing;ubiquitous computing	HCI	-36.04704521382079	-25.63812044416123	45624
9cbd0e24c3bbc00f3329720579fe52fe7d4df99a	dynamic optimization for web page based on user's estimated browsing intention		When a web designer designs a web page, they decide the order and layout of the elements for the user. Multiple users who use the same web page have varying purposes. The order and layout decided by the designer is not necessarily effective for all users' intentions. The purpose of this study is to estimate the mental model of the user visiting a web page and to change the functionality of the web browser to optimize the order and layout of the web components in real time, based on the estimation result. In this paper, we propose a feature-modeling of the page elements (the basis for realizing this function) and an optimization method using a simple mental model.	browsing;mathematical optimization;mental model;web components;web design;web page	Shogo Kusumure;Taketoshi Ushiama	2018		10.1145/3164541.3164633	world wide web;real-time computing;computer science;web page;responsive web design	DB	-40.28720994517651	-28.373741543586707	45644
2cf82b208a2375c29558cd52ffbacf946b279055	realizing 3d visual programming environments within a virtual environment	3d visualization;programming environment;virtual environments;visual programming;virtual environment;user interfacing	In the visual programming community, many interesting graphical metaphors have been reported upon for representing computer programs graphically. Most of them have a 2D or 2.5D appearance on the screen in order to reflect the inherent multi-dimensionality of the programming constructs being represented. By going into a three-dimensional representation, this reflection can go a step further. With ever increasing 3D graphics rendering capabilities on todays computers, it moreover becomes feasible to extend the dimensionality of the program (and data structure) depiction. We follow this approach by realizing 3D graphical programming techniques within CAEL, our interactive Computer Animation Environment Language. The paper elucidates how several concepts, traditionally found within the Virtual Environments area, can be utilized in the realization of three-dimensional Programming Environments.	2.5d;3d computer graphics;computer animation;computer program;data structure;graphical user interface;rendering (computer graphics);virtual reality;visual programming language	Frank Van Reeth;Karin Coninx;Steve De Backer;Eddy Flerackers	1995	Comput. Graph. Forum	10.1111/j.1467-8659.1995.cgf143_0361.x	visualization;human–computer interaction;reactive programming;functional reactive programming;computer science;programming language implementation;virtual machine;extensible programming;operating system;multimedia;programming paradigm;event-driven programming;symbolic programming;inductive programming;visual programming language;concurrent object-oriented programming;computer graphics (images)	Graphics	-40.21362584246112	-31.736541052893035	45898
19b6920637c2dd678b0f44d1507c66fcc100f8a3	setting the scene: playing digital director in interactive storytelling and creation	digital storytelling;game design;autonomous agent;interactive storytelling	Interactive digital storytelling promises to be a new artistic discipline. But what does the artwork look like? And how much control does the artist have over the final result? By searching for the right answers, we draw several approaches from diverse fields such as filmmaking, game design, autonomous agents, psychology, and narrative intelligence. We first give a definition of interactive storytelling that includes a scope between authorship and emergent narrative, and present two example projects with different emphases. Then, we introduce a multi-level concept for an experimental stage that can be used to explore interactive storytelling at varying degrees of flexibility versus predetermination. Instead of inventing the ultimate virtual narrator, we suggest developing layers of run-time engines that allow authors to work with directions on each single layer separatelyFfrom selecting high-level dramatic structures down to directing animated actors. The concept of underlying models at each level is explained in detail by the example of a story model implementation. Further, we show variations of level adjustments causing graded degrees of semi-autonomy. First results of this experimental stage are presented, and the primary future tasks are pointed out. r 2002 Published by Elsevier Science Ltd.	interactive storytelling	Ulrike Spierling;Dieter Grasbon;Norbert Braun;Ido Iurgel	2002	Computers & Graphics	10.1016/S0097-8493(01)00176-5	game design;computer vision;simulation;computer science;artificial intelligence;autonomous agent;multimedia;algorithm;computer graphics (images);mechanical engineering	HCI	-39.6114858316418	-32.735384308791836	45917
47a3784c5584926f3d3f16859bd59e7f34a56875	a 3d collaborative editor using webgl and webrtc	traitement du signal et de l image;p2p;intelligence artificielle;vision par ordinateur et reconnaissance de formes;webrtc;traitement des images;synthese d image et realite virtuelle;webgl;collaboration application	In 3D collaborative environments, users needs interactivity and real-time updates. With web-based applications, such requirement implies that conventional client-server -alone- is no longer enough. To overcome this unmet need, we propose a hybrid client server peer-to-peer (P2P) communication model based on pluginless web standards enabling users to design collaboratively 3D scenes. The client part includes a WebGL editor to visualize and edit 3D scenes while the server side provides data and ensure persistence. Using the WebRTC protocol, a P2P mesh is generated to transmit directly the updates through a scenes working group. The feasibility of our approach is demonstrated with a web-based prototype submitted to a qualitative evaluation highlighting the usage of WebRTC for direct 3D data transmission with low latency and high throughput, and WebGL for 3D rendering.	3d rendering;client–server model;collaborative real-time editor;diskless node;interactivity;peer-to-peer;persistence (computer science);prototype;real-time web;server (computing);server-side;throughput;web application;web standards;webgl;webrtc	Caroline Desprat;Jean-Pierre Jessel;Hervé Luga	2015		10.1145/2775292.2778297	simulation;computer science;multimedia;world wide web	Networks	-44.540740214088586	-26.97539972127217	45996
875d475e2b7fab1dffa638dc9760fc811eafbf02	mathbrush: a case study for pen-based interactive mathematics	categories and subject descriptors according to acm ccs g 4 mathematical software user interfaces;pen based interface;computer algebra system;system design	Current generations of computer algebra systems require users to transform two dimensional math expressions into one dimensional strings, to master complex sets of commands, and to analyze lengthy output strings for relevant information. MathBrush is a system, designed based on research in education pedagogy, that provides a pen-based interface to many of the features of computer algebra systems. We describe relevant work in education pedagogy as a motivation for MathBrush's design. We highlight aspects of MathBrush that are unique from other contemporary pen-math systems. Finally, we present the results of a thinkaloud evaluation of the MathBrush system. Together, these observations validate aspects of the current design of MathBrush, suggest areas for refinement, and inform the design of future pen-math systems.	computer algebra system;refinement (computing)	George Labahn;Edward Lank;Mirette S. Marzouk;Andrea Bunt;Scott MacLean;David Tausky	2008		10.2312/SBM/SBM08/143-150	simulation;computer science;theoretical computer science	HCI	-45.529359667538074	-30.18397969796147	46212
bf854c5a492161487a94a29be898c1570938d837	natural language interfaces to conceptual models		Accessing structured data in the form of ontologies currently requires the use of formal query languages (e.g., SeRQL or SPARQL) which pose significant difficulties for non-expert users. One way to lower the learning overhead and make ontology queries more straightforward is through a Natural Language Interface (NLI). While there are existing NLIs to structured data with reasonable performance, they tend to require expensive customisation to each new domain. Additionally, they often require specific adherence to a pre-defined syntax which, in turn, means that users still have to undergo training. In this thesis, we study the usability of NLIs from two perspectives: that of the developer who is customising the NLI system, and that of the end-user who uses it for querying. We investigate whether usability methods such as feedback and clarification dialogs can increase the usability for end users and reduce the customisation effort for the developers. To that end, we have developed two systems, QuestIO and FREyA, whose design, evaluation and comparison with similar systems form the core of the contribution of this thesis.	native-language identification;natural language user interface;ontology (information science);overhead (computing);personalization;query language;sparql;usability	Danica Damljanovic	2011			computer science;database;communication;world wide web	DB	-41.12907425919055	-26.241251712204726	46481
73c81f024ce88c41aef8571956f3fde16be8979f	remote collaboration using augmented reality videoconferencing	videoconferencing;video streaming;cutting plane;volume rendering;optical tracking;computer supported collaborative work;system design;application sharing;augmented reality;face to face;remote collaboration	This paper describes an Augmented Reality Videoconferencing System, which is a novel remote collaboration tool combining a desktop-based AR system and a videoconference module. The novelty of our system is the combination of these tools with AR applications superimposed on live video background displaying the conference parties’ real environment, merging the advantages of the natural face-to-face communication of videoconferencing and AR’s interaction capabilities with distributed virtual objects using tangible physical artifacts. The simplicity of the system makes it affordable for everyday use. We explain our system design based on concurrent video streaming, optical tracking and 3D application sharing, and provide experimental proof that it yields superior quality compared to pure video streaming with successive optical tracking from the compressed streams. We demonstrate the system's collaborative features with a volume rendering application that allows users to display and examine volumetric data simultaneously and to highlight or explore slices of the volume by manipulating an optical marker as a cutting plane interaction device.	3d computer graphics;augmented reality;bmc remedy action request system;cutting-plane method;desktop computer;streaming media;systems design;volume rendering	István Barakonyi;Tamer Fahmy;Dieter Schmalstieg	2004			computer vision;augmented reality;simulation;computer science;multimedia;videoconferencing;volume rendering;cutting-plane method;systems design;computer graphics (images)	HCI	-42.42851058737817	-37.114370555354235	46492
616e95cee16d87b1dd5d7455c521372b833c68c1	future 3d audio technologies for consumer use	ambisonics;binaural reproduction;3d audio;sound field reproduction	Audio engineers and acousticians have been putting lots of effort on realizing three-dimensional (3D) audio technologies aiming at realization of truly high-fidelity audio, highly realistic virtual reality environment, and next-generation telecommunications. Fortunately, such efforts resulted in establishment of some theories providing physical and mathematical backgrounds for 3D audio technologies and some have been realized in laboratories. However, to apply them to consumer use, there remain lots of practical issues to be solved. This paper reviews state-of-the-art 3D audio techniques and prospects for applying them to a consumer use.	3d computer graphics;audio engineer;audio signal processing;theory;virtual reality	Makoto Otani	2015	2015 IEEE 4th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2015.7398713	simulation;engineering;multimedia;communication	Visualization	-46.33505123883245	-34.02417471433119	46550
7af912de82b6aae1e8a22f965ef9d2a2d07e6986	user verification of the frbr conceptual model and testing of frbr prototypes		FRBR is a conceptual model of the bibliographic universe. While FRBR is focused on end-users, no user studies were performed for its development. Since its release, two research groups, one at University of Ljubljana, Slovenia, and another at Kent State University, USA, started to systematically verify the FRBR model with users. This paper will provide an overview of several studies performed so far that focus on the exploration of mental models of end-users, the verification of attributes needed to support user tasks and the testing of FRBR prototypes. The results validate the FRBR conceptual model and confirm that FRBR can be the basis for the development of bibliographic information systems that provide more effective user information seeking than the traditional catalogs.	functional requirements for bibliographic records;information seeking;information system;mental model;usability testing	Maja Zumer;Athena Salaba;Yin Zhang	2012		10.1007/978-3-642-34752-8_20	conceptual model;computer science;information retrieval;data mining;user information;information system;functional requirements for bibliographic records	SE	-41.422217453501425	-26.1974719057865	46640
8c316dd8fae0b5113ee3595f57381f584d0b327e	gradient-based steering for vision-based crowd simulation algorithms		Most recent crowd simulation algorithms equip agents with a synthetic vision component for steering. They offer promising perspectives through a more realistic simulation of the way humans navigate according to their perception of the surrounding environment. In this paper, we propose a new perception/motion loop to steering agents along collision free trajectories that significantly improves the quality of vision-based crowd simulators. In contrast with solutions where agents avoid collisions in a purely reactive (binary) way, we suggest exploring the full range of possible adaptations and retaining the locally optimal one. To this end, we introduce a cost function, based on perceptual variables, which estimates an agent’s situation considering both the risks of future collision and a desired destination. We then compute the partial derivatives of that function with respect to all possible motion adaptations. The agent then adapts its motion by following the gradient. This paper has thus two main contributions: the definition of a general purpose control scheme for steering synthetic vision-based agents; and the proposition of cost functions for evaluating the perceived danger of the current situation. We demonstrate improvements in several cases.	algorithm;binary file;crowd simulation;gradient;local optimum;loss function;synthetic vision system	Teofilo Bezerra Dutra	2015	Comput. Graph. Forum	10.1111/cgf.13130	anime;computer vision;simulation;computer facial animation;computer science;artificial intelligence;crowd simulation;real-time computer graphics;computer animation;computer graphics;3d computer graphics;computer graphics (images)	AI	-38.487702256095474	-37.13376064701732	46802
5162bf5d4421034977217a3302d7cb87bed7a9c4	virtual clay: a real-time sculpting system with haptic toolkits	3d;non photorealistic rendering;boundary representation;physics based modeling;real time;subdivision surface;interactive;stylized;physical properties	In this paper we systematically develop a novel, interactive sculpting framework founded upon subdivision solidsand physics-based modeling. In contrast with popular subdivision surfaces, subdivision solids have the unique advantage offering both the boundary representation and the interior material of a solid object. We unify the geometry of subdivision solids with the principle of physicsbased models and formulate dynamic subdivision solids . Dynamic subdivision solids respond to applied forces in a natural and predictive manner and give the user the illusion of manipulating semielasticvirtual clay. We have developed a real-time sculpting system that provides the user with a wide array of intuitive sculpting toolkits. The flexibility of the subdivision solid approach allows users to easily modify the topology of sculpted objects, while the inherent physical properties are exploited to provide a natural interface for direct, force-based deformation. More importantly, our sculpting system is equipped with natural, haptic-based interaction to provide the user with a realistic sculpting experience. CR Categories: I.3.5 [Computer Graphics]: Physics-based modeling; I.3.3 [Computer Graphics]: Modeling packages; I.3.6 [Computer Graphics]: Interaction techniques; H.5.2 [User Interfaces]: Haptic I/O; I.3.7 [Computer Graphics]: Virtual reality; I.3.7 [Computer Graphics]: Animation.	boundary representation;clay animation;computer graphics;haptic technology;input/output;interaction technique;list of toolkits;real-time locating system;real-time transcription;subdivision surface;virtual reality;visuo-haptic mixed reality	Kevin T. McDonnell;Hong Qin;Robert A. Wlodarczyk	2001		10.1145/364338.364395	real-time computing;simulation;computer science;non-photorealistic rendering;multimedia;interactivity;subdivision surface;boundary representation;physical property;stylized fact;quantum mechanics;3d computer graphics;computer graphics (images)	Graphics	-42.06949107967861	-36.42895390392989	46810
95b4b1f9e530b17e1ea7d28af518fbc56a35b2a2	distributed augmented reality for collaborative design applications	distribution;computer aided design;interaction;group communication;cscw;augmented reality;collaborative design;user interaction;user interface separation	This paper presents a system for constructing collaborative design applications based on distributed augmented reality. Augmented reality interfaces are a natural method for presenting computer-based design by merging graphics with a view of the real world. Distribution enables users at remote sites to collaborate on design tasks. The users interactively control their local view, try out design options, and communicate design proposals. They share virtual graphical objects that substitute for real objects which are not yet physically created or are not yet placed into the real design environment. We describe the underlying augmented reality system and in particular how it has been extended in order to support multi-user collaboration. The construction of distributed augmented reality applications is made easier by a separation of interface, interaction and distribution issues. An interior design application is used as an example to demonstrate the advantages of our approach.	ar (unix);augmented reality;computer-supported cooperative work;distributed computing;graphical user interface;graphics;interactivity;multi-user;session (computer science)	Klaus H. Ahlers;André Kramer;David E. Breen;Pierre-Yves Chevalier;Chris Crampton;Eric Rose;Mihran Tuceryan;Ross T. Whitaker;Douglas S. Greer	1995	Comput. Graph. Forum	10.1111/j.1467-8659.1995.cgf143_0003.x	distribution;augmented reality;computer-mediated reality;user experience design;interaction;simulation;human–computer interaction;communication in small groups;computer science;computer aided design;computer-supported cooperative work;mixed reality;multimedia;design education	HCI	-43.9321588026232	-37.3931050449199	46875
a4331bf4619f5a2ad67d0c408856a105bb1148f1	applying 'second life' to a cave#8482;-like system for the elaboration of interaction methods with programmable interfaces	interaction;interactive method;computer vision	With its multiple types of interaction scenarios both between people and between people and objects in virtual space, ‘Second Life’ can be used as a platform for both social and technological research. Set up in a stereoscopic environment, the Open Source Viewer is implemented in a technological framework for iteratively creating, evaluating and optimizing appropriate input/output patterns and interface metaphors for immersive environments. Focus of R&D are the real-time interaction methods rendered possible by a programmable Interface. Thus, the Ars Electronica Futurelab has developed a technological framework based on own R&D results. The framework consists of an interaction wrapper (Palmist), a distribution engine (ARSBOX) [Berger et al. 2004] and a render unit (VRizer) [Berger et al. 2005].	input/output;interface metaphor;real-time transcription;second life;stereoscopy;virtual reality	Daniela Kuka;Christopher Lindinger;Horst Hörtner;Florian Berger;Doris Zachhuber	2007		10.1145/1280720.1280891	computer vision;interaction;interactive systems engineering;human–computer interaction;computer science;computer graphics (images)	HCI	-43.71184841146031	-34.78784003899262	47010
ed9f0dcaa6e620e0837370097fcd10c06826f551	graphical editors and graphic grammars	graphical editors;graphic grammars	A graphical editor is a software tool for computeraided construction of various graphical structures (schemes, diagrams, etc.). In what follows, we discuss construction of plane (2D) graphical structures. Today, such editors are widely used in computer work. As a rule, a general-purpose text editor incorporates a graphical editor used for creation of illustrations of various kinds. However, in many such editors, construction of graphic structures is founded solely on the skills of the user employing a given set of basic operations. It is rather difficult, if not impossible, to verify semantics of these structures by means of a computer. These graphic structures are designed to be perceived and interpreted by a human.	diagram;general-purpose markup language;graphical user interface;programming tool;text editor	E. A. Zhogolev	2001	Programming and Computer Software	10.1023/A:1010934315721	theoretical computer science;computer science;rule-based machine translation	Graphics	-39.08699936818164	-30.663625805705916	47016
bed44e4949df19a2d96ef4c1120cc7c6637ad397	breaking paragraphs into lines	spacing;dynamic programming;shortest paths;justification;box glue penalty algebra;composition;layout;tex tau epsilon chi;history of printing;typesetting;line breaking;word processing	This paper discusses a new approach to the problem of dividing the text of a paragraph into lines of approximately equal length. Instead of simply making decisions one line at a time, the method considers the paragraph as a whole, so that the final appearance of a given line might be influenced by the text on succeeding lines. A system based on three simple primitive concepts called ‘boxes’, ‘glue’, and ‘penalties’ provides the ability to deal satisfactorily with a wide variety of typesetting problems in a unified framework, using a single algorithm that determines optimum breakpoints. The algorithm avoids backtracking by a judicious use of the techniques of dynamic programming. Extensive computational experience confirms that the approach is both efficient and effective in producing high-quality output. The paper concludes with a brief history of line-breaking methods, and an appendix presents a simplified algorithm that requires comparatively few resources.	algorithm;approximation;backtracking;breakpoint;computation;dynamic programming;unified framework	Donald E. Knuth;Michael F. Plass	1981	Softw., Pract. Exper.	10.1002/spe.4380111102	layout;composition;computer science;artificial intelligence;dynamic programming;engineering drawing;algorithm	Vision	-37.08378632485893	-34.28425118152466	47076
58bb7789af384576da72c146ac40b900f46d0e0b	dynamic query visualisations on world wide web clients: a dhtml solution for maps and scattergrams	choropleth maps;personal computer;visual design;information visualization;scattergrams;dynamic html;sliders;dynamic query;dynamic queries;world wide web;technical report;information visualisation	Dynamic queries are gaining popularity as a method for interactive information visualization. Many implementations have been made on personal computers, and there is increasing interest in web-based designs. While Java and Flash strategies have been developed, we believe that a Dynamic HTML implementation could help promote more widespread use. We implemented double-box range sliders with choropleth maps and scattergrams, which are two popular visualizations, using HTML layers and tables. This paper describes our methods for slider control, visual presentation, and displaying/updating results for these visualizations. Visual design issues innovations and performance enhancements were necessary to create viable designs.	dynamic html;information visualization;java;map;personal computer;web application;world wide web	Evan Golub;Ben Shneiderman	2003	Int. J. Web Eng. Technol.	10.1504/IJWET.2003.003320	information visualization;choropleth map;computer science;technical report;operating system;database;multimedia;dynamic html;world wide web	HCI	-42.772088922805	-27.363832536403415	47245
6319bd8a054e146592735fd5ccbed8a6b7559021	puppet modeling for real-time and interactive virtual shadow puppet play	real time performance shadow puppet play virtual puppet virtual storytelling puppet modeling;bone and bind tools function;art;puppet modeling;texture mapping technique;real time virtual shadow puppet play;smoother arm movement;wrist;real time;realistic arm movement puppet modeling interactive virtual shadow puppet play real time virtual shadow puppet play south east asia professional puppeteers off line mode shadow puppet movement flash development platform bone and bind tools function adobe flash cs4 texture mapping technique h anim standard model virtual shadow puppets smoother arm movement;off line mode;texture mapping;virtual reality;professional puppeteers;realistic arm movement;virtual reality humanities;joints;arm movement;visualization;standard model;bones;shadow puppet movement;humanities;adobe flash cs4;animation real time systems bones joints wrist art visualization;h anim standard model;animation;flash development platform;performing art;virtual shadow puppets;interactive virtual shadow puppet play;virtual puppet;south east asia;shadow puppet play;real time performance;real time systems;virtual storytelling	Traditional shadow puppet play has been a popular local performing art and storytelling tradition in many regions of South East Asia. Currently, this traditional show is slowly becoming less popular due to the fact that the show can only be performed by professional puppeteers and there are not many of them around nowadays. Furthermore, the theater requires high cost of maintenance, and a long time and arduous task in preparing for a show. Therefore, various applications have been developed in order to allow the users to perform the shadow puppet play virtually. Most previous related works involved creating a show or storyline in off-line mode, and those that allow interactive and real-time performance provide limited interactivity and the arm movement of the shadow puppet is not realistic and free enough like an original shadow puppet movement. In this paper, we propose a method that provides more interactivity and more realistic arm movement in real-time by focusing on the holder that is attached to the wrist of the shadow puppet which is a key element in moving the arm of the traditional shadow puppet during a performance. The method uses the Bone and Bind tools function of the Flash-development platform, Adobe® Flash® CS4 or higher, together with texture mapping technique and the H-Anim standard model. In the preliminary evaluation of the method, both the experts and nonexperts gave encouraging responses and feedbacks with regard to faster and better control, and more realistic and smoother arm movement of the virtual shadow puppets.	anim;adobe creative suite;coat of arms;feedback;interactivity;joystick;online and offline;real-time locating system;real-time transcription;shadow copy;shadow volume;texture mapping	Sirot Piman;Abdullah Zawawi Talib	2012	2012 Second International Conference on Digital Information and Communication Technology and it's Applications (DICTAP)	10.1109/DICTAP.2012.6215364	anime;texture mapping;standard model;simulation;visualization;computer science;virtual reality;multimedia;computer graphics (images)	Graphics	-40.609516631577165	-35.50660287970326	47370
111767607f760d048f1e3f26b59a6ef001236a95	algorithmic musical improvisation from 2d board games		ScrabbleTM-to-MIDI is a computer-emulated two dimensional board game that generates MIDI music from game rules and state as the game is played. Software modules include the board graphical user interface, rule engine and game state, a translator that maps game state to musical events, a second graphical user interface for manipulating translator configuration parameters, and a software synthesizer for rendering MIDI events. The plugin translator and its configuration parameters comprise a composition. Improvisation consists of playing a unique game and of making ongoing adjustments to mapping parameters during play. Statistical distributions of letters and words provide a basis for mapping structures from word lists to notes, chords and phrases. While pseudorandom tile selection provides a stochastic aspect to the instrument, players utilize knowledge of vocabulary to impose structure on this sequence of pseudo-random selections, and a conductor uses mapping parameters to variegate this structure in up to sixteen instrument voices.	basis (linear algebra);business rules engine;dictionary attack;emulator;graphical user interface;midi;map;pseudorandomness;vocabulary	Dale Parson	2010			simulation;rendering (computer graphics);software;midi;plug-in;chord (music);vocabulary;musical improvisation;graphical user interface;computer science	HCI	-44.721899818042495	-31.61499712447871	47458
4c6373e9e5a2974adca187575c1b5fb1a4f19066	design of a pen-based electric diagram editor based on context-driven constraint multiset grammars	pen based interfaces;pen based interface;user interface;structured document analysis;on line interpretation;incremental parsing;visual languages;visual language;structured documents;software assessing	This paper deals with the computer-aided design of pen-based interfaces for structured document composition. In order to take advantage of the interaction with the user, the goal is to interpret the user hand-drawn strokes incrementally, i.e. directly as the document is being drawn. We present a generic approach for such purpose: it is based on a new formalism, ContextDriven Constraint Multiset Grammars (CDCMG), and its associated incremental parser. CDCMG model how documents of a given nature are composed; they can be applied on various natures of documents. We demonstrate how it has been exploited to develop, in collaboration with a society that spreads out industrial pen-based solutions, a prototype for electric diagram composition and editing. We also present an evaluation of the system. Experimental results first emphasize the gain of time in comparison with more classical user interfaces. They also demonstrate its user-friendliness and its usability.	computer-aided design;context-sensitive grammar;diagram;formal system;generic programming;pen computing;prototype;semantics (computer science);usability;user interface	Sébastien Macé;Éric Anquetil	2007		10.1007/978-3-540-73107-8_47	natural language processing;computer science;operating system;database;programming language;user interface	HCI	-39.14642023966343	-28.74915639915631	47507
f1235b4190113aa66d0aeebe5a6f9c5ec4c01533	towards a collaborative 3d interaction model for cooperative design in virtual environments	3d interaction;groupware;virtual reality groupware;collaborative work;3d interaction techniques;application software;collaborative interaction;computer supported cooperative work;collaboration;virtual reality;3d interaction technique;adaptive workflow;semiimmersive virtual environment;multi user;collaborative 3d interaction model;navigation;computational modeling;complex system;support group;group awareness;group awareness concept collaborative 3d interaction model cooperative design virtual reality technology computer supported cooperative work cscw technology semiimmersive virtual environment;group awareness concept;space technology;collaboration virtual environment collaborative work virtual reality navigation space technology military computing environmental management computational modeling application software;virtual environment;cooperative work 3d interaction techniques collaborative interaction collaborative virtual environment;environmental management;collaborative virtual environment;military computing;cooperative work;virtual reality technology;cscw technology;immersive virtual environment;cooperative design	Design of complex systems requires multi-user cooperation with virtual reality technologies to provide 3D interactions between users and virtual objects. Recent advances in both virtual reality (VR) systems and computer-supported cooperative work (CSCW) technologies have resulted in a convergence of the appearance of the collaborative virtual environments (CVEs) systems supporting different forms of collaboration and interaction between users. The collaboration in these systems refers to the simultaneous interactions (collaborative interaction) of multiple users on a virtual object in an immersive or semi-immersive virtual environment. In this paper, we propose a method to modeling collaborative 3D interactions that supports group interaction in CVEs. The proposed method is based on group awareness concepts (focus and nimbus) combined astutely with 3D interaction paradigms (navigation, selection and manipulation) to provide us a collaborative 3D interaction model needed to design an adaptive workflow for coordination of multi-user 3D interactions in CVEs.	3d interaction;collaborative virtual environment;complex systems;computer-supported cooperative work;immersion (virtual reality);multi-user;programming paradigm;semiconductor industry;virtual reality	Samir Otmane;Nassima Ouramdane;Malik Mallem	2007	2007 11th International Conference on Computer Supported Cooperative Work in Design	10.1109/CSCWD.2007.4281434	navigation;application software;simulation;human–computer interaction;computer science;knowledge management;virtual machine;operating system;computer-supported cooperative work;virtual reality;space technology;management;computational model;collaboration	Visualization	-48.08298925344258	-31.90270478086808	47594
ff017852a13cab1f20a714e4e1dbf819abcba317	control and autonomy for intelligent virtual agent behaviour	animacion por computador;interfase usuario;control motor;motion control;virtual characters;engine control;user interface;autonomous system;control inteligente;cooperation;duality;intelligence artificielle;cooperacion;intelligent control;sistema autonomo;commande moteur;commande mouvement;control movimiento;dualite;agent intelligent;systeme autonome;intelligent agent;artificial intelligence;interface utilisateur;dualidad;commande intelligente;agente inteligente;inteligencia artificial;computer animation;virtual agent;animation par ordinateur	This paper discusses some issues for animating Virtual Characters. It emphasizes the duality between control and autonomy. Specific problems and examples are addressed like generic motion engine, environment-based motion control, autonomous behaviors, and cooperation between the user and an autonomous agent.	autonomy;intelligent agent	Daniel Thalmann	2004		10.1007/978-3-540-24674-9_54	motion control;duality;simulation;computer science;autonomous system;artificial intelligence;computer animation;user interface;intelligent agent;cooperation;intelligent control	AI	-35.18381990141355	-26.41376015239028	47798
3627928810dd6be7dd35a0e9b1c8478de8815094	multiple coordinated views for searching and navigating web content repositories	contextualized information space;information exploration;web documents;controlled vocabulary;context information;information retrieval;climate change;contextual information;information visualization;multiple views;media monitoring;qa75 electronic computers computer science;ontology learning;document enrichment;just in time;domain ontology;new media;domain specificity;multiple coordinated view;qa76 computer software	The advantages and positive effects of multiple coordinated views on search performance have been documented in several studies. This paper describes the implementation of multiple coordinated views within the Media Watch on Climate Change, a domain-specific news aggregation portal available at www.ecoresearch.net/climate that combines a portfolio of semantic services with a visual information exploration and retrieval interface. The system builds contextualized information spaces by enriching the content repository with geospatial, semantic and temporal annotations, and by applying semi-automated ontology learning to create a controlled vocabulary for structuring the stored information. Portlets visualize the different dimensions of the contextualized information spaces, providing the user with multiple views on the latest news media coverage. Context information facilitates access to complex datasets and helps users navigate large repositories of Web documents. Currently, the system synchronizes information landscapes, domain ontologies, geographic maps, tag clouds and just-in-time information retrieval agents that suggest similar topics and nearby locations.	controlled vocabulary;information retrieval;just-in-time compilation;map;news aggregator;ontology (information science);ontology learning;portlet;semiconductor industry;tag cloud;web content;web page	Alexander Hubmann-Haidvogel;Arno Scharl;Albert Weichselbraun	2009	Inf. Sci.	10.1016/j.ins.2009.01.030	controlled vocabulary;information visualization;new media;computer science;data mining;climate change;world wide web;information retrieval	Web+IR	-40.640014163696456	-24.075433167208363	47906
db10b8d9cc9b44a636eca35a8486c588a539c6dc	vrml possibilities: the evolution of the glasgow model [virtual city]	town and country planning;virtual reality languages;software systems;computer architecture buildings solid modeling spatial databases surfaces context modeling hardware software systems application software virtual reality;large scale;very large databases virtual reality languages town and country planning architecture geographic information systems;geographic information systems;large scale multi layered urban models vrml virtual reality modelling language abacus glasgow model model evolution virtual city architecture building aids geometrical databases urban topography 3d data sets graphical functionality model manipulation contextual data;virtual reality modelling language;urban modeling;very large databases;architecture	and archaeological findings. Among the Web site’s most important features are virtual reconstructions, which let Internet users explore the archaeological site and see reconstructed buildings superimposed upon their still existing foundations. In addition, the Web site shows, by means of a virtual walk, major parts of the nearby Bos t’Ename Forest Preserve, which are only accessible to a limited number of visitors through guided tours. This, in effect, gives Internet visitors open access to otherwise restricted areas without the danger of physical damage to this precious and delicate ecological area. At the present time, CD-ROM adaptations of these VR technologies are under active development. MM	cd-rom;internet;vrml;world wide web	Gareth Ennis;Malcolm Lindsay	2000	IEEE MultiMedia	10.1109/93.848428	computer vision;simulation;human–computer interaction;computer science;artificial intelligence;architecture;operating system;geographic information system;multimedia;world wide web;computer graphics (images)	Visualization	-35.135811406464434	-31.707473825189574	48058
ed8621d8e09e2f0ac3f055fae08cd623d860d297	umea: translating interaction histories into project contexts	personal information management;interaction histories;virtual environment;activity theory;empirical evaluation	Virtual environments based on the desktop metaphor provide limited support for creating and managing project-specific work contexts. The paper discusses existing approaches to supporting higher-level user activities and presents a system named UMEA (User-Monitoring Environment for Activities). The design of the system is informed by activity theory. The system: (a) organizes resources into project-related pools consisting of documents, folders, URLs, and contacts, (b) monitors user activities, (c) automatically adds new resources to pools associated with active projects, and (d) provides personal information management tools linked to individual projects. An empirical evaluation of the system is reported.	desktop computer;desktop metaphor;emoticon;personal information management	Victor Kaptelinin	2003		10.1145/642611.642673	simulation;activity theory;human–computer interaction;computer science;virtual machine;personal information management;world wide web	HCI	-41.706260179510686	-25.471442435836906	48171
a49d445bcc68013f35e09640faafbaa7417100c7	human-centered design for vr interactions	shortest paths;path planning;discrete search;path finding;navigation;navigation meshes;anytime dynamic search	VR has the potential to provide experiences and deliver results that cannot be otherwise achieved. However, interacting with immersive applications is not always straightforward and it is not just about an interface for the user to reach their goals. It is also about users working in an intuitive manner that is a pleasurable experience and devoid of frustration. Although VR systems and applications are incredibly complex, it is up to designers to take on the challenge of having the VR application intuitively communicate to users how the virtual world and its tools work so that those users can achieve their goals in an elegant and comfortable manner.	interaction;virtual world	Jason Jerald;Richard Marks	2016		10.1145/2897826.2927320	navigation;simulation;human–computer interaction;computer science;pathfinding;artificial intelligence;motion planning;multimedia	HCI	-38.1165397639746	-37.75815319868138	48334
5a46854ba76aa319594b05966adbd5398ff9fb33	poster: icexplorer: studying great lakes ice cover	great lakes ice atlas;atmospheric science;histograms;ice cover;earth and atmospheric sciences h 4 3 information systems applications communications applications;lakes data analysis data visualisation geophysics computing glaciology ice;great lakes ice cover;interaction styles;icexplorer;j 2 physical sciences and engineering;user interface;advancing scientific research;lakes;glaciology;interaction style;visual interaction;advancing scientific research icexplorer great lakes ice cover lake erie great lakes ice atlas visualization technique interaction technique automated data analysis;automated data analysis;data visualisation;data analysis;graphical user interfaces;geophysics computing;visualization technique;time series analysis;information browsers h 5 2 information interfaces and presentation user interfaces;data visualization;great lake;graphic user interface;lake erie;information system;information interfaces and presentation;lakes ice data analysis data visualization user interfaces animation solids histograms educational institutions information systems;scientific research;meteorology;ice;graphical user interfaces interaction styles j 2 physical sciences and engineering earth and atmospheric sciences h 4 3 information systems applications communications applications information browsers h 5 2 information interfaces and presentation user interfaces;interaction technique	IceXplorer is a tool for analyzing variations in ice cover on Lake Erie. It enhances the data and pre-packaged analysis currently available in the Great Lakes Ice Atlas and serves as an example of a small, focused application where simple but carefully-chosen visualizations, interaction techniques, and automated data analysis are combined to create an effective tool for advancing scientific research.	interaction technique	Stina S. Bridgeman	2009	2009 IEEE Symposium on Visual Analytics Science and Technology	10.1109/VAST.2009.5333082	human–computer interaction;computer science;graphical user interface;data visualization;statistics	Logic	-33.90500860361779	-31.099538080406855	48477
0f7b452e21a2a181562f9dc9176caec75da336f8	visualizing and assessing hypotheses for marine archaeology in a vr cave environment	vr systems and toolkits;immersion;article;marine archaeology	The understanding and reconstruction of a wrecks formation process can be a complicated procedure that needs to take into account many interrelated components. The team of the University of Cyprus investigating the 4th-century BC Mazotos shipwreck are unable to interact easily and intuitively with the recorded data, a fact that impedes visualization and reconstruction and subsequently delays the evaluation of their hypotheses. An immersive 3D visualization application that utilizes a VR CAVE was developed, with the intent to enable researchers to mine the wealth of information this ancient shipwreck has to offer. Through the implementation and evaluation of the proposed application, this research seeks to investigate whether such an environment can aid the interpretation and analysis process and ultimately serve as an additional scientific tool for underwater archaeology.	software archaeology;visualization (graphics)	Irene Katsouri;Aimilia Tzanavari;Kyriakos Herakleous;Charalambos Poullis	2015	JOCCH	10.1145/2665072	computer science;archaeology;maritime archaeology;immersion	Visualization	-36.0959620891033	-32.67392717942577	48492
0a81c2e34640bcfd9c7a571a94e3bd338571935a	the animal algorithm animation tool	algorithm animation;data structure;scripting language	In this paper, we present Animal, a new tool for developing animations to be used in lectures. Animal offers a small but powerful set of graphical operators. Animations are generated using a visual editor, by scripting or via API calls. All animations can be edited visually. Animal supports source and pseudo code inclusion and highlighting as well as precise user-defined delays between actions. The paper evaluates the functionality of Animal in comparison to other animation tools.	algorithm;application programming interface;graphical user interface;pseudocode;visual editor	Guido Rößling;Markus Schüer;Bernd Freisleben	2000		10.1145/343048.343069	data structure;human–computer interaction;computer science;scripting language;programming language;computer graphics (images)	HCI	-40.96557984505164	-29.67146617644551	48630
b976b683623d50f702c6fb3466f13f56de762112	computer-aided design of user interfaces by example	interface builder;computer aided design;programming by example;user interface;software agent;interface design;programming by demonstration;machine learning;user interface design	A promising approach to Computer-Aided Design of User Interfaces (CADUI) is Programming by Example, where an interface designer demonstrates the behavior of an interface by presenting concrete examples and demonstrating how the system should behave on those examples. It lets the user interface designer “play end-user”, simulating what an end-user would see and do. A software agent records the steps of the user interface and generalizes a program that can be used in analogous situations in the future. The popular genre of so-called Interface Builders can be seen as a “poor-man’s” Programming by Example. It is now time to extend such systems so that behavior as well as appearance can be specified by example.	browsing;cognitive dimensions of notations;computer-aided design;programming by demonstration;programming by example;r language;simulation;software agent;user interface;user interface design;world wide web	Henry Lieberman	2002		10.1007/978-94-010-0421-3_1	user interface design;look and feel;user;interface description language;10-foot user interface;user experience design;human action cycle;simulation;interface metaphor;shell;human–computer interaction;natural language user interface;computer science;interface;interface control document;adapter pattern;multimedia;event-driven programming;natural user interface;user interface;graphical user interface testing;multiple document interface	HCI	-42.39587227466764	-30.045212536546757	49176
49358915ae259271238c7690694e6a887b16f7ed	synthesis of expressive facial animations: a multimodal caricatural mirror	multimodal interface;research outputs;research publications;facial animation;audio visual;spoken language processing;facial expression	This paper describes a natural and intuitive way to create expressive facial animations, using a novel approach based on the so-called ‘multimodal caricatural mirror’ (MCM). Taking as an input an audio-visual video sequence of the user’s face, the MCM generates a facial animation, in which the prosody and the facial expressions of emotions can either be reproduced or amplified. The user can thus simulate an emotion and see almost instantly the animation it produced, like with a regular mirror. In addition, the MCM also enables to amplify the emotions of selected parts of the input video sequence, leaving other parts unchanged. It therefore constitutes a novel approach to the design of very expressive facial animation, as the affective content of the animation can be modified by post-processing operations.	facial recognition system;multi-chip module;multimodal interaction;semantic prosody;simulation;video post-processing	Olivier Martin;Irene Kotsia;Ioannis Pitas;Arman Savran;Jordi Adell;Ana Huerta;Raphaël Sebbe	2007	Journal on Multimodal User Interfaces	10.1007/BF02884429	speech recognition;computer facial animation;computer science;computer animation;multimedia;communication;facial expression	Graphics	-44.153213187981734	-33.444903001340954	49179
7feb4e9ed8f9d08038b451b4dd52662f66eb1b09	virtual workbench-a non-immersive virtual environment for visualizing and interacting with 3d objects for scientific visualization	virtual reality;scientific visualization	The Virtual Workbench (VW) is a non-immersive virtual environment that allows users to view and interact with stereoscopic objects displayed on a workspace similar to a tabletop workspace used in day-to-day life. A VW is an ideal environment for collaborative work where several colleagues can gather around the table to study 3D virtual objects. The Virtual Reality laboratory at the Naval Research Laboratory has implemented the VW using a concept similar to (Froehlich et al., 1994). This paper investigates how the VW can be used as a non-immersive display device for understanding and interpreting complex objects encountered in the scientific visualization field. Different techniques for interacting with 3D visualization objects on the table and using VW as a display device for visualization are evaluated using several cases.	display device;interaction;scientific visualization;stereoscopy;virtual reality;volume rendering;workbench;workspace	Upul Obeysekare;Chas Williams;Jim Durbin;Lawrence J. Rosenblum;Robert Rosenberg;Fernando Grinstein;Ravi Ramamurthi;Alexandra Landsberg;William Sandberg	1996	Proceedings of Seventh Annual IEEE Visualization '96		computer vision;cave automatic virtual environment;human–computer interaction;computer science;computer graphics (images)	Visualization	-42.04671724380678	-36.09182097165993	49265
6adf5efce94501869c4a2955f25d7648cb2f7a7f	development of an earth environmental digital library system for soil and land-atmospheric data	3d visualization;user interface;digital library;vrml	We propose and examine new methods for automatic data loading system and flexible user interface system with many features such as 3D visualization. We implement the earth environmental digital library and operate it on the Web. Though our system is focusing the limited users like earth environmental researchers, more than 8000 hits per month describe the practical usefulness of it.	digital library;library classification;organic user interface;world wide web	Eiji Ikoma;Taikan Oki;Masaru Kitsuregawa	2001		10.1145/379437.379799	digital library;vrml;visualization;human–computer interaction;computer science;multimedia;user interface;world wide web	HCI	-42.65206998246197	-27.285733893950052	49288
6101aa33592a9de5667cb1a2c4eaccd443f03195	the biolin: a current-based musical interface	musical interface;interface design;media art	"""In this paper, we describe a media artwork that features the Biolin, a musical device that produces different sounds depending on the target object that it is being played on. Shaped like an ordinary violin bow, the Biolin analyzes the target using a weak electric current to produce a timbre that matches the target. The user can then perform by """"playing"""" the target object using the Biolin. The Biolin was designed by modifying a violin bow in order to make it conductive and connected to a computer for the transmission of data, resulting in visual and auditory output. We showcased Biolin in front of a small audience, in which the users showed positive reactions to the new approach of making sounds from everyday objects and people around us."""		Sungjae Hwang;Kibeom Lee;Daham Park;Woonseung Yeo	2009		10.1145/1690388.1690483	acoustics;engineering;multimedia;communication	HCI	-47.17819794843108	-37.134752890803966	49608
89f72ab743a68d4303aef7bef9d13d5cc67a68e3	synthesis of problems for shaded area geometry reasoning		A shaded area problem in high school geometry consists of a figure annotated with facts such as lengths of line segments or angle measures, and asks to compute the area of a shaded portion of the figure. We describe a technique to generate fresh figures for these problems. Given a figure, we describe a technique to automatically synthesize shaded area problems. We demonstrate the efficacy of our synthesis techniques by synthesizing problems from fresh figures as well as figures from a corpus of problems from high-school geometry textbooks.		Chris Alvin;Sumit Gulwani;Rupak Majumdar;Supratik Mukhopadhyay	2017		10.1007/978-3-319-61425-0_39	computer science;line segment;geometry	Theory	-36.515452999886065	-34.83110816252976	49672
7d7a8b8925b131c350674061b2bc238dada9881e	a handwritten japanese historical kana reprint support system: development of a graphical user interface		Reprint of Japanese historical manuscripts is time-consuming and requires training because they are hand-written, and may contain characters different from those currently used. We proposed a framework for assisting the human process for reading Japanese historical manuscripts and implemented a part of a system based on the framework as a Web service. In this paper, we present a graphical user interface (GUI) for the system and reprint process through the GUI. We conducted a user test to evaluate the system with the GUI by a questionnaire. From the results of the experiment, we confirmed that the GUI can be used intuitively but we also found points to be improved in the GUI.	constraint satisfaction problem;experiment;graphical user interface;historical document;optical character recognition;web service	Atsushi Yamazaki;Kazuki Sando;Tetsuya Suzuki;Akira Aiba	2018		10.1145/3209280.3229117	reprint;database;kana;web service;computer science;constraint satisfaction problem;user interface;graphical user interface	HCI	-41.40825773660258	-27.411841740153974	49685
5414e0020632a4a538a819a4eb343af6b4432622	webgl-based streaming and presentation of objects with bidirectional texture functions	public dissemination;cultural heritage;moulage;btf compression;progressive transmission;btf streaming;btf rendering;webgl	Museums and Cultural Heritage institutions have a growing interest in presenting their collections to a broader community via the Internet. The photo-realistic presentation of interactively inspectable virtual surrogates is one of the most challenging problems in this field. For this purpose, we seek to employ not only a 3D geometry but also a powerful material representation capable of reproducing the full visual appeal of an object. In this article, we propose a WebGL-based presentation framework in which reflectance information is represented via Bidirectional Texture Functions (BTF). Our approach works out-of-the-box in modern Web browsers and allows for the progressive transmission and interactive rendering of digitized artifacts consisting of 3D geometry and reflectance information. We handle the huge amount of data needed for this representation by employing a novel progressive streaming approach for BTFs, which allows for the smooth interactive inspection of a steadily improving version during the download. We demonstrate an interesting use-case of this technique at a cross section of Cultural Heritage, medical education, and research and provide an evaluation of the capabilities of our framework in the scope of BTF compression and transmission.	bidirectional texture function;cross section (geometry);download;interactivity;internet;out of the box (feature);surrogates;webgl	Christopher Schwartz;Roland Ruiters;Michael Weinmann;Reinhard Klein	2013	JOCCH	10.1145/2499931.2499932	simulation;computer science;cultural heritage;archaeology;multimedia;computer graphics (images)	Graphics	-36.2440257945768	-32.478622964501724	49779
ee15dbb2f9b3a01c14f53df1941af330f087dbf0	multidimensional data visualization for decay study in cultural heritage: an object-oriented implementation	colour graphics;data visualisation;object-oriented programming;software libraries;solid modelling;3d computer graphics;italian research project;vtk;ancient monument restoration;cultural heritage;free c++ class library;multidimensional data visualization;object-oriented software program;visualization toolkit	The scope of this work has been to apply multidimensional data visualization techniques to the study of the ancient building decay events. With this aim we have created new visual tools based on the shape variation (glyph), that with more traditional techniques based on the color have been implemented in object-oriented software programs using the innovative and powerful visualization toolkit (VTK), a free C++ class library for visualization and 3D computer graphics. The data used are climatic, chemical and environmental measurements detected on/near the Roman theatre in the city of Aosta, acquired in the ambit of an Italian research project focused on the restoration of ancient monuments	3d computer graphics;ambit;c++;circuit restoration;data visualization;glyph;library (computing);vtk	Laura Chiappini;Rossella Cossu;Marco Di Lorenzo	2004	Proceedings Computer Graphics International, 2004.	10.1109/CGI.2004.1309254	information visualization;visualization;human–computer interaction;computer science;multimedia;programming language;object-oriented programming;3d computer graphics;computer graphics (images)	Visualization	-35.847755489676366	-31.32223367354007	49886
bb628dd4ce6c9d8199278af110f8c03f91986031	multiart — an api specification to support production and distribuction of telematic art	videos telematic art robot robotic dance virtual environment;3d virtual environment;art;formal specification;robots streaming media art virtual environments multimedia communication three dimensional displays real time systems;video streaming;3d virtual environment multiart api specification telematic art production telematic art distribution telematic art presentation robots choreography programming video streams;virtual reality;virtual environments;three dimensional;robotic;telematic;streaming media;three dimensional displays;environment;application program interfaces;robots;multimedia communication;virtual environment;virtual reality application program interfaces art formal specification;virtual;robot;videos;dance;real time systems	This paper describes the specification of an API called MultiArt, and its role is to support the production and presentation of telematic art in its many forms of expressions. This API abstracts specific art's applications that can be used together. We present an initial architecture and a case study demonstrating the use of MultiArt. Our case study makes use of a predefined functions that allow us to program robots choreography in an easy way, manage video streams captured during the art presentations and provide communication and interactivity to viewers through a 3D virtual environment.	application programming interface;interactivity;robot;streaming media;telematics;virtual reality	Ricardo Dias;Hugo T. A. Sena;Igor Rosberg de Medeiros Silva;Haroldo Teodosio;Samuel O. Azevedo;Aquiles M. F. Burlamaqui	2011	2011 IEEE International Conference on Virtual Environments, Human-Computer Interfaces and Measurement Systems Proceedings	10.1109/VECIMS.2011.6053848	robot;simulation;human–computer interaction;computer science;artificial intelligence;virtual reality;multimedia	Visualization	-43.77716201802274	-34.10470928671299	50032
f179f03c96d576c5bb9a6bbd89d8a21aa7a4e6fe	augmented reality environment using a web browser - content presentation with a two-layer display	augmented reality	We developed a prototype system to make an augmented reality (AR) environment using a Web browser. Although AR technology has the potential to be used in various applications, authoring/editing AR contents is a problem on the wide spread. Graphics expertise and knowledge of computer programming are necessary for creating contents for AR applications. We used a Web browser as a presentation tool so that users who had experience in creating Web contents could reuse the multimedia data as AR contents and modify the AR contents by themselves. A two-layer display was used in the system in order to superimpose virtual objects onto a real scene for creating an AR environment. Another problem is about identifying objects in the real scene. An open-source library, ARToolkit, is often used as an image-processing tool for detecting the position and orientation of markers as well as identifying them. However, the markers must be registered in the system in advance, and the registration becomes tedious work when many markers are used such as Japanese kanji characters. The optical character reader (OCR) middleware was implemented as a character-recognition function for Japanese character markers. This paper describes the system design and software architecture for constructing an AR environment using a Web browser display and the demonstration of the prototype system.	artoolkit;augmented reality;computer programming;graphics;image processing;middleware;open-source software;optical character recognition;prototype;sensor;software architecture;systems design	Kikuo Asai;Hideaki Kobayashi	2006			world wide web;computer science;mixed reality;computer-mediated reality;human–computer interaction;augmented reality;multimedia	Web+IR	-43.387808800876655	-36.07278908760623	50127
39d912c383b4e62baaa0c3db5064ce0b645a3f79	representation of linguistic and domain knowledge for second language learning in virtual worlds	virtual environments;natural language generation;ontology lexicon interface	There has been much debate, both theoretical and practical, on how to link ontologies and lexicons in natural language processing (NLP) applications. In this paper, we focus on an application in which lexicon and o ntology are used to generate teaching material. We briefly describe the application (a serious game for language learning). We then zoom in on the representation and interlinking of the lexicon and of the ontology. We show how the use of existing standards and of goo d practice principles facilitates the design of our resources while satisfying the expressivity requirements set by natural language g en ration.	grey goo;lexicon;natural language processing;ontology (information science);requirement;virtual world	Alexandre Denis;Ingrid Falk;Claire Gardent;Laura Perez-Beltrachini	2012			natural language processing;upper ontology;language identification;natural language programming;computer science;ontology;linguistics	NLP	-39.6592664970007	-26.7542758792159	50656
28b006ae87e8d1a7bde17a2cd06cf29e57c8c617	predicting user actions using interface agents with individual user models	eficacia sistema;user modelling;multiagent system;systeme intelligent;intelligent interfaces;multi agent system;hidden markov model;relacion hombre maquina;sistema inteligente;performance systeme;man machine relation;user preferences;time series;interface agent;system performance;modelisation par utilisateur;intelligent system;relation homme machine;user behavior;sistema multiagente;user model;systeme multiagent	The incompleteness and uncertainty about the state of the world and about the consequences of actions are unavoidable. If we want to predict the performance of multiuser computing systems, we have the uncertainty of what the users are going to do, and how that affects system performance. Intelligent interface agent development is one way to mitigate the uncertainty about user behaviors by predicting what users will do based on learned users' behaviors, preferences, and intentions. This work focuses on developing user models that can analyze and predict user behavior in multi-agent systems. We have developed a formal theory of user behavior prediction based on hidden Markov models. This work learns the user model through a time-series action analysis and abstraction by taking users' preferences and intentions into account in order to formally define user modeling.		Jung Jin Lee;Robert McCartney	1999		10.1007/3-540-46693-2_12	user;simulation;user modeling;computer user satisfaction;interface metaphor;human–computer interaction;user journey;computer science;artificial intelligence;user requirements document;machine learning;time series;multi-agent system;hidden markov model;statistics	HCI	-35.05391335887908	-25.59739863242552	50886
481d6c025aa05f12f022a889bebe12268072311a	motion analysis for folk dance evaluation	i 3 7 computer graphics;animation;three dimensional graphics and realism	Motion capture techniques are becoming a popular method for digitizing folk dances for preservation and dissemination. Although technically the captured data can be of very high quality, folk dancing, in contrast to choreographed performances, allow for stylistic variations and improvisations that cannot be easily captured by the data themselves. The majority of motion analysis and comparison algorithms are explicitly based on quantitative metrics and thus do not usually provide any insight on style qualities of a performance. In this work, we introduce a motion analysis and comparison framework that is based on Laban Movement Analysis (LMA); these algorithms are particularly useful in the context of teaching folk dances. We present a prototype virtual reality simulator in which users can preview segments of folk dance performed by a 3D avatar and repeat them. The users’ performances are captured and subsequently compared to the folk dance template motions. The system then provides intuitive feedback about their performance, which is based on the four LMA components (BODY,EFFORT,SHAPE,SPACE) and provides both a quantitative and qualitative evaluation of the performance.	algorithm;display resolution;electronic circuit simulation;eurographics;experiment;level of measurement;motion capture;performance;poor posture;prototype;real-time transcription;virtual reality simulator	Andreas Aristidou;Efstathios Stavrakis;Yiorgos Chrysanthou	2014		10.2312/gch.20141304	simulation;computer science;multimedia;computer graphics (images)	Visualization	-39.355609043334134	-36.11453840727424	50973
32cc2dc66de8db9bdf107971185e2909385b16e6	rule-based crowds: generation, animation, cloth and rendering of 15.000 unique human characters	rule based;crowd simulation;ancient egypt;virtual archaeology;high fidelity graphics;character animation	"""Scanline is currently developing the proprietary crowd-simulation tool """"Sheep"""". It is designed to generate mass scenes with 15.000 different characters, animate them by an unlimited amount of rules, supply presimulated cloth and render a whole shot in a single layer taking less than 45 minutes. This will disclose completely new dimensions for the realization of Visual Effects with mass sequences."""	crowd simulation;scan line;visual effects	Stephan Trojansky;Florian Hu;Kolja Kähler;Christoph Sprenger;Stephan Stapel	2003		10.1145/965400.965518	rule-based system;character animation;computer vision;simulation;computer science;artificial intelligence;crowd simulation;computer graphics (images)	Graphics	-39.411436257391244	-33.86552221435056	51136
454c9eb42024441e7c15d9ec662c07525d9826e5	from participatory design to participating problem solving: enhancing system adaptability through user modelling	computadora;modelizacion;adaptacion;user modelling;human computer interaction;knowledge based system;system adaptability;ergonomia;flexible inference control;ordinateur;relacion hombre maquina;usuario;conceptual model;hombre;man machine relation;utilisateur;ergonomie;controle;computer;resolucion problema;modelisation;adaptive interface;inference control;adaptation;inferencia;human;user;relation homme machine;participatory design;modeling;ergonomics;inference;problem solving;resolution probleme;user model;system functionality;homme;knowledge base	The issue on the role of users in knowledge-based systems can be investigated from two aspects: the design aspect and the functionality aspect. Participatory design is an important approach for the first aspect while system adaptability supported by user modelling is crucial to the second aspect. In the article, we discuss the second aspect. We view a knowledge-based computer system as the partner of users' problem-solving process, and we argue that the system functionality can be enhanced by adapting the behaviour of the system to fit the needs of users with different profiles. We emphasise that the notion of user modelling is crucial to realise such kind of flexibility. User modelling will be beneficial to the user, not only through adaptive interfaces, but also through the enhanced system adaptability. In a knowledge-based system, by incorporating user models, searching can be reduced to a smaller portion in the knowledge-base, thus enhancing system functionality. In other words, user modelling is incorporated to realise flexible inference control to achieve system adaptability. An example is provided, and a general conceptual model is sketched. We conclude this paper by emphasising that the design aspect and functionality aspect are complementary. Achieving enhanced functionality through joint efforts of computers and human users indicates a kind of participatory execution of computerised problem-solving or participatory problem-solving.	complex adaptive system;computer;knowledge-based systems;problem solving	Zhengxin Chen	1993	AI & SOCIETY	10.1007/BF01901819	user;knowledge base;simulation;user modeling;systems modeling;computer science;knowledge management;artificial intelligence;conceptual model;adaptation	AI	-37.571395135917236	-26.370400505629465	51152
3c7ef1355deb97347075f75c3080b80969a221aa	an integrated approach for steering, visualization, and analysis of atmospheric simulations	integrated approach	In the research described here, we have constructed at tightly coupled set of methods for monitoring, steering, and applying visual analysis to large scale simulations. This work shows how a collaborative, interdisciplinary process that teams application and computer scientists can result in a powerful integrated approach. The integrated design allows great flexibility in the development and use of analysis tools. This work also shows that visual analysis is a necessary component for full understanding of spatially complex, timedependent atmospheric processes. Overview For large scale atmospheric simulations one would like a tight coupling between the simulation, the observational database on which it is based, and the visualization/ analysis process by which it is understood. In fact there should be feedback, in the form of steering, between the latter and the simulation, since this will yield much more accurate representations of atmospheric processes and a significantly more focused investigation of behavior relevant to questions being asked. Since the data have complicated 3D structures and are highly time-dependent, the visualization approach must handle this dynamic data in a highly interactive fashion. In the research described here, we have combined all these aspects into a single, integrated approach. This has required a collaborative, interdisciplinary process involving atmospheric scientists, experts in parallel high performance computing, visualization specialists, and experts in user interfaces. In particular, we find that it is important to have the scientists involved from the beginning in defining the steps of the project and evaluating its results. This constant evaluation allows an iterative refinement of the approach and aids everybody (including the scientists) in discovering new aspects of the problem that they did not foresee. We think that the process used here could serve as a template for building highly effective and powerful applications (and tools supporting them), a process where the developer comes away with a deeper understanding of user needs. The Scientific Problem The ultimate goal in climate modeling is the simultaneous simulation on a global scale of physical and chemical interactions in the ocean and atmosphere. This goal is still far from reach since, in addition to the problem's enormous complexity, parameters must be chosen to simulate processes that are not well understood or whose influence can only be approximated at the scale of current models. Earth and atmospheric scientists at Georgia Tech have developed a global chemical transport model (GCTM) [1] that uses assimilated windfields for the transport calculations. These models are important tools to answer scientific questions about the stratospheric-tropospheric exchange mechanism or the distribution of species such as ch lorof luorocarbons (CFC's ) , hydrochlorofluorocarbon (HCFC's), and ozone. This model uses a spectral approach, which is common to global models [2], to solve the transport equation for each species. In a spectral model, all variables are expanded into a set of orthogonal spherical basis functions. In a typical run our model contains 37 layers, which represent segments of the earth's atmosphere from the surface to approximately 50 km, with a horizontal	approximation algorithm;atmospheric chemistry;basis function;chemical transport model;climate model;computation;computer scientist;computer simulation;dynamic data;feedback;ibm notes;interaction;iterative method;iterative refinement;rapid prototyping;refinement (computing);spherical basis;supercomputer;user interface	Yves Jean;Thomas Kindler;William Ribarsky;Weiming Gu;Greg Eisenhauer;Karsten Schwan;Fred Alyea	1995			computational science;simulation;computer science	HPC	-33.84472505057866	-29.46610360164813	51566
c987f800c4e66197757179ff3569cec18909323f	codestrates: literate computing with webstrates		We introduce Codestrates, a literate computing approach to developing interactive software. Codestrates blurs the distinction between the use and development of applications. It builds on the literate computing approach, commonly found in interactive notebooks such as Jupyter notebook. Literate computing weaves together prose and live computation in the same document. However, literate computing in interactive notebooks are limited to computation and it is challenging to extend their user interface, reprogram their functionality, or develop stand-alone applications. Codestrates builds literate computing capabilities on top of Webstrates and demonstrates how it can be used for (i) collaborative interactive notebooks, (ii) extending its functionality from within itself, and (iii) developing reprogrammable applications.	computation;ipython;user interface	Roman Rädle;Midas Nouwens;Kristian Antonsen;James R. Eagan;Clemens Nylandsted Klokmose	2017		10.1145/3126594.3126642	software;multimedia;human–computer interaction;computation;computer science;user interface	HCI	-43.23734856933915	-31.681084021756146	52070
b66447802530b4755eff325abdc368f607ca86f6	a study for image-based integrated virtual environment	databases;image processing;image resolution;sensors;photorealistic large scale environment;rendered image reliability;prototypes;virtual environment rendering computer graphics displays virtual reality solid modeling image resolution prototypes buildings power system modeling data visualization;virtual reality;rendering methods;data accuracy;rendering errors;user interactions;large scale;displays;solid modeling;data resolution;data visualization;image processing rendering computer graphics virtual reality;depth information;image based integrated virtual environment;scalability image based integrated virtual environment sensors databases user interactions rendering methods user viewpoint depth information rendered image reliability environment manager display image data accuracy data resolution display resolution rendering errors photorealistic large scale environment cabin;scalability;display resolution;virtual environment;power system modeling;cabin;rendering computer graphics;user interaction;environment manager;user viewpoint;buildings;display image	In thi.f papel; we propo.fe a new approach to building a virtual environment by u.fing raw data from a real environment. To lnanage and analyze djJferent type.f if datasets from many sensors and databases, we build and integrate djJferent types if environments with dijferent types if datasets, user interactions and rendering method.f. To integrate the environments, we merge rendered images Gf each virtual environment according to the user's viewpoint, depth i71.formation, and reliability G.f rendered images, and display the resultant image to the usel: In addition, we propose an integration environment by using an environment lnanagel; }j,'hich determine.f the U.fer's vie'-\-point with re.~pect to the integrant environment.~ according to the u.~er'.f input, and compose a display image from rendered images Gf the integrant environments. The reliability Gf the rendered images is detennined by data accuracy and resolution, display resolution and rendering errors from rendering methodologies. By integratingfour constructed environments according to the user's vie}j,point, we can constroct a photorealistic large-.fcale environment from diJferent types Gf dataset.f. Furthermore, we implemented the prototype .ry.ftem in CABlN and demon.ftrated the .fcalability if thi.f architecture.	2d-plus-depth;database;display resolution;image resolution;interaction;prototype;resultant;sensor;virtual reality	Tomohiro Tanikawa;Koichi Hirota;Michitaka Hirose	2002		10.1109/ISMAR.2002.1115092	computer vision;display resolution;scalability;image resolution;image processing;computer science;sensor;virtual machine;operating system;virtual reality;prototype;multimedia;solid modeling;data visualization;computer graphics (images)	Visualization	-34.50661391837209	-33.73102583701599	52295
6f2140b1fec239d4d0de0590cedca9975e161816	interactive multimedia interface and multimodal analysis for technology-enhanced learning and performance preservation	3d motion visualization;instruments;interactive music performance preservation;multimodal interface;multimedia;sonification;technology enhanced learning;i maestro project;interactive music;gestural interface;string instruments;preservation multimedia interactive multimodal technology enahnced learning education motion capture visualisation sensor;computer aided instruction;training;music computer aided instruction data analysis data visualisation gesture recognition interactive systems multimedia computing;performance analysis instruments multimedia computing computer interfaces data analysis feedback graphics educational technology visualization testing;interactive multimedia interface;interactive multimedia;multimodal data analysis;multimedia computing;motion capture;data visualisation;data analysis;interactive;research and development;level of detail;visualisation;sensor;three dimensional displays;interactive feedback;multimedia communication;performance analysis;digital preservation;production;preservation;multimodal;technology enahnced learning;performing art;performing arts preservation;interactive feedback interactive multimedia interface technology enhanced learning i maestro project 3d motion visualization multimodal data analysis 3d graphics sonification string instruments interactive music performance preservation performing arts preservation gestural interface;interactive systems;music;gesture recognition;3d graphics	This paper presents one of the interactive multimedia tools resulted from a research and development project in the context of technology-enhanced learning for music, and the usage of the multimodal interface for performing arts preservation. Firstly, the paper briefly introduces the i-Maestro project (see www.i-maestro.org). With particular focuses on the interactive multimedia interface for 3D motion visualization, the paper discusses the applications and usages of this gestural interface for technology-enhanced learning for string instruments. The interface utilizes online and offline multimodal data analysis and provide interactive feedback using 3D graphics and sonification. It provides additional level of details and data for stylistic and performance analysis. Next, the paper presents an ongoing research project which is working on digital preservation of interactive music performance using this interface.	3d computer graphics;dynamic music;gesture recognition;multi media interface;multimodal interaction;online and offline;profiling (computer programming);sonification	Kia Ng	2009	2009 Fifth International Joint Conference on INC, IMS and IDC	10.1109/NCM.2009.107	motion capture;sonification;human–computer interaction;computer science;sensor;level of detail;music;gesture recognition;multimedia;data analysis;preservation;computer graphics (images)	HCI	-45.062215225780996	-33.772566728194384	52379
b698d37082a9e7b7b24a5e1e03f5a889114bb4c4	new perspectives in end-user development		When end users approach a development task, they bring with them a set of techniques, expressions, and knowledge, which can be leveraged in order to make the process easier. The Natural Programming Project has been working for over twenty years to better understand how end users think about their tasks, and to develop new ways for users to express those tasks that will be more “natural,” by which we mean closer to the way they think. Our chapter in the previous book covered the first 10 years of this research; and here we summarize the most recent 10 years. This includes studies on barriers that impede EUD, and a new tool that helps with the understanding and debugging barriers by showing developers why their program has its current behavior. We also describe a tool that we created to B.A. Myers (✉) · M.B. Kery · T.J.-J. Li Carnegie Mellon University, Pittsburgh, PA, United States e-mail: bam@cs.cmu.edu M.B. Kery e-mail: mkery@andrew.cmu.edu T.J.-J. Li e-mail: tobyli@cs.cmu.edu A.J. Ko University of Washington, Seattle, WA, United States e-mail: ajko@uw.edu C. Scaffidi Oregon State University, Corvallis, OR, United States e-mail: scaffidc@eecs.oregonstate.edu S. Oney University of Michigan, Ann Arbor, MI, United States e-mail: soney@umich.edu Y. Yoon Google, Mountain View, CA, United States e-mail: youngseokyoon@google.com K. Chang IBM, Armonk, NY, United States e-mail: kerry.chang@ibm.com 1 © Springer International Publishing AG 2017 F. Paternò, V. Wulf (eds.), New Perspectives in End-User Development, DOI 10.1007/978-3-319-60291-2_1 help EUDs input, process, and transform data in the context of spreadsheets and web pages. Interaction designers are a class of EUDs that may need to program interactive behaviors, so we studied how they naturally express those behaviors, and then built a spreadsheet-like tool to allow them to author new behaviors. Another spreadsheet tool we created helps EUDs access web service data without writing code, and extends the familiar spreadsheet to support analyzing the acquired web-based hierarchical data and programming data-driven GUI applications. Finally, EUDs often need to engage in exploratory programming, where the goals and tasks are not well-formed in advance. We describe new tools to help users selectively undo past actions, along with on-going research to help EUDs create more efficient behaviors on smartphones and facilitate variations when performing data analysis.	debugging;email;end-user development;exploratory programming;graphical user interface;hierarchical database model;smartphone;spreadsheet;springer (tank);undo;web application;web page;web service;well-formed element	Fabio Paternò;Volker Wulf	2017		10.1007/978-3-319-60291-2	end-user development;human–computer interaction;engineering	HCI	-47.47745211042294	-27.60030614007588	52998
9ac04423c5955871823ce1076b97d687d8e11f6b	realnav: exploring natural user interfaces for locomotion in video games	3d interface;user evaluation;video games;data interpretation;computer graphics;wiimotes;kalman filters;gesture recognition realnav natural user interfaces reality based locomotion video game interfaces data interpretation 3d interface data kalman filter;kalman filter;locomotion;video game;acceleration;natural user interfaces;hybrid approach;graphical user interfaces;realnav;kalman filters computer games computer graphics gesture recognition graphical user interfaces;football;games;user interfaces games hardware cameras accelerometers costs robustness centralized control spinning leg;natural user interface;video game interfaces;football wiimotes locomotion video games;infrared;computer games;accelerometers;gesture recognition;3d interface data;tracking;quaternions;hardware;reality based locomotion	We present a reality-based locomotion study directly applicable to video game interfaces; specifically, locomotion control of the quarterback in American football. Focusing on American football drives requirements and ecologically grounds the interface tasks of: running down the field, maneuvering in a small area, and evasive gestures such as spinning, jumping, and the “juke.”	ecology;natural user interface;requirement	Brian Williamson;Chadwick A. Wingrave;Joseph J. LaViola	2010	2010 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2010.5444737	simulation;human–computer interaction;computer science;multimedia	Embedded	-39.37767776216173	-37.840794660914376	53044
0475ed6f9b540b7b985179a72e9c64d42cf07aab	enhancing embodied intelligent agents with affective user modelling	user modelling;usuario;exploracion;besoin utilisateur;necesidad usuario;utilisateur;information presentation;user need;informacion publica;agent intelligent;information public;intelligent agent;exploration;user;agente inteligente;information system;public information;systeme information;sistema informacion	The objective of this research is the exploration how affective knowledge used in global controlling mechanisms for public information systems with lifelike presentation agents will increase the effectiveness of such systems in terms of information presentation, but also help the user to better explain his/her needs by adopting a more natural conversational style through interactive dialogue.	information system;intelligent agent	Patrick Gebhard	2001		10.1007/3-540-44566-8_42	user;simulation;exploration;human–computer interaction;computer science;artificial intelligence;multimedia;intelligent agent;information system	AI	-36.799638798429	-25.89093872577497	53142
c6138b490664e6a49edd4900e67613789a72bf4a	graphical interaction management	graphical interaction management	Graphical interfaces and interactive graphical programmes are awkward to write because of a lack of top-down structure. A methodology for constructing graphical programs will be described, together with a system that generates the basic interaction requirements for such applications.	graphical user interface	Balbir S. Barn;Philip J. Willis	1987	Comput. Graph. Forum	10.1111/j.1467-8659.1987.tb00358.x	simulation;human–computer interaction;computer science;theoretical computer science;post-wimp;graphical user interface testing	HCI	-41.67030640229045	-29.63312513803408	53229
6a833475d051a2280d887326756cb48be46cbc83	highly realistic 3d presentation agents with visual attention capability	context aware;real time;eye movement;eye tracking;visual attention;3d graphics;eye gaze	This research proposes 3D graphical agents in the role of virtual presenters with a new type of functionality – the capability to process and respond to visual attention of users communicated by their eye movements. Eye gaze is an excellent clue to users’ attention, visual interest, and visual preference. Using state-of-the-art non-contact eye tracking technology, eye movements can be assessed in a unobtrusive way. By analyzing and interpreting eye behavior in real-time, our proposed system can adapt to the current (visual) interest state of the user, and thus provide a more personalized, context-aware, and ‘attentive’ experience of the presentation. The system implements a virtual presentation room, where research content of our institute is presented by a team of two highly realistic 3D agents in a dynamic and interactive way. A small preliminary study was conducted to investigate users’ gaze behavior with a non-interactive version of the system. A demo video based on our system was awarded as the best application of life-like agents at the GALA event in 2006.	eye tracking;graphical user interface;interactivity;personalization;real-time clock	Arjen Hoekstra;Helmut Prendinger;Nikolaus Bee;Dirk Heylen;Mitsuru Ishizuka	2007		10.1007/978-3-540-73214-3_7	computer vision;simulation;visual search;eye tracking;computer science;linguistics;multimedia;gaze-contingency paradigm;3d computer graphics	HCI	-46.894049719721885	-37.77461845717078	53450
14c7032ea4ce95d9412d5e22439cea8487e1d986	legibility enhancement for information visualisation	dimensional space;specific application;future development;information visualisation;legibility enhancement;urban landscape;cognitive map;world wide web;information space;internet;user interfaces;data visualisation;three dimensional	Navigation in computer generated information spaces may be difficult, resulting in users getting “lost in hyperspace.” This work aims to build on research from the area of city planning to try to solve this problem. We will introduce the concepts of legibility and cognitive maps and the five features of urban landscape with which they are associated. Following this will be descriptions of techniques and algorithms which we have developed to allow these features to be introduced to three dimensional spaces for information visualisation. Next we will describe a specific application of these techniques in the visualisation of the World Wide Web and conclude with a look at future development of the system.	algorithm;cognitive map;information visualization;lost in hyperspace;world wide web	Rob Ingram;Steve Benford	1995			three-dimensional space;computer vision;the internet;simulation;human–computer interaction;cognitive map;computer science;web navigation;multimedia;user interface;data visualization	HCI	-34.95463081412167	-31.870718360714367	53460
0bc2e46c19afef0b3af3a23849820d3ba435d36c	preventing knowledge transfer errors: probabilistic decision support systems through the users' eyes	modelizacion;interfase usuario;representacion conocimientos;computacion informatica;systeme aide decision;user interface;heuristic method;user centered design;metodo heuristico;sistema ayuda decision;probabilistic approach;modelisation;transferencia conocimiento;decision support system;transfert des connaissances;ciencias basicas y experimentales;enfoque probabilista;approche probabiliste;comportement utilisateur;representation connaissance;knowledge transfer;interface utilisateur;peritaje;methode heuristique;user behavior;expertise;knowledge representation;grupo a;user interaction;modeling;comportamiento usuario	Development and use of probabilistic decision support systems benefit by a good communication between the developer on the one hand, and the user and the domain expert on the other hand. Communication is difficult because large differences in training and experience exist between the two. This necessitates user-centered design of the representations used in this communication, and attention to the translation of user terms to model terms. A systematic approach to developing user-centered representations and preventing knowledge transfer errors is outlined in this paper. We demonstrate how five heuristic guidelines can be fruitfully applied in different developer-user interaction situations in different phases of decision-support system construction.	decision support system;heuristic;human–computer interaction;interaction information;knowledge base;knowledge management;linda (coordination language);mathematical model;mental model;statistical model;subject-matter expert;user-centered design	Hermina J. M. Tabachneck-Schijf;Petra L. Geenen	2009	Int. J. Approx. Reasoning	10.1016/j.ijar.2008.04.010	knowledge representation and reasoning;user-centered design;simulation;systems modeling;decision support system;computer science;artificial intelligence;user interface	AI	-36.841371320236405	-26.671909000443794	53535
3ee5020fdd21e99daee2ebe87564afe2c49b17d1	irida: a real-time wireless sensor network visualization feedback protocol	real time visualization;visualization protocol;protocols;video projector irida real time wireless sensor network visualization feedback protocol real time visualization feedback system wireless sensor network algorithm fixed hardware test bed vertical flat surface feedback loop system;real time;test bed;visual feedback wireless sensor networks data visualization tools visualization protocol;wireless sensor network;wireless communication;data visualisation;visualization;feedback loop;data visualization;visual feedback;wireless sensor networks data visualisation real time systems;wireless sensor networks protocols visualization data visualization hardware heart beat wireless communication;feedback system;wireless sensor networks;data visualization tools;heart beat;hardware;real time systems	In this paper, we describe the implementation of a real time visualization and feedback system for Wireless Sensor Network algorithms. The system is based on a fixed hardware test bed, which is deployed on a vertical flat surface and a feedback loop system that takes information about the current state of the network and projects this state, in a visual way, on the surface itself using a video projector. The protocol used is open and simple to use, and can be easily adapted for different hardware configurations. We call our system Irida.	algorithm;feedback;real-time web;testbed;video projector	Marios Karagiannis;Laetitia Dallinge;José D. P. Rolim	2012	2012 IEEE 8th International Conference on Distributed Computing in Sensor Systems	10.1109/DCOSS.2012.54	embedded system;real-time computing;wireless sensor network;computer science;distributed computing;data visualization;statistics	Robotics	-35.17383471718472	-34.81003886741694	53650
3df645ba240d692bb24b1a48f92404b287f1c687	gestural control in electronic music performance: sound design based on the 'striking' and 'bowing' movement metaphors	dmi;musical interface;gestural control	Following a call for clear movement-sound relationships in motion-controlled digital musical instruments (DMIs), we developed a sound design concept and a DMI implementation with a focus on transparency through intuitive control metaphors. In order to benefit from the listener's and performer's natural understanding of physical processes around them, we use gestures with strong physical associations as control metaphors, which are then mapped to sound modules specifically designed to represent these associations sonically. The required motion data can be captured by any low-latency sensor device worn on the hand or wrist, that has an inertial measurement unit with six degrees of freedom. A dimension space analysis was applied on the current implementation in order to compare it to existing DMIs and illustrate its characteristics. In conclusion, our approach resulted in a DMI with strong results in transparency, intuitive control metaphors, and a coherent audio-visual link.	coherence (physics);six degrees of separation	Frederic Robinson;Cedric Spindler;Volker Boehm;Erik Oña	2015		10.1145/2814895.2814901	simulation;direct media interface;human–computer interaction;computer science;artificial intelligence;multimedia	Robotics	-46.234072526864026	-36.484162175609136	53670
a6ef498b8a40dabc383acd350d5eee61d8180ff7	iterative design of an interface for easy 3-d direct manipulation	moving object;personal computer;direct manipulation;3 d manipulation;handle box;requirement engineering;user testing;iterative design;hand gestures;narrative handles;bounding box;space planning	Although computer tools for 3-D design applications are now widely available for use on personal computers, they are unnecessarily difficult to use. Conventions for establishing and manipulating views of 3-D objects require engineering-oriented dialogues that are foreign to most users. This paper describes the iterative design and testing of a new mechanism for moving 3-D objects with a mouse-controlled cursor in a space planning application prototype. Emphasis was placed on developing a design which would make 3-D interaction more intuitive by preserving users' experiences with moving objects in the real, physical world. Results of an informal user test of the current interface prototype are presented and implications for the development of a more general direct manipulation mechanism are discussed.	accessibility;cursor (databases);direct manipulation interface;institute for operations research and the management sciences;iteration;iterative design;iterative method;personal computer;prototype;thinking outside the box;usability	Stephanie Houde	1992		10.1145/142750.142772	iterative design;simulation;human–computer interaction;computer science;minimum bounding box;multimedia;requirements engineering	HCI	-44.07998886874321	-36.90846741405638	53801
8bbb9c47b68605ef1c6e551eaf27026829b36595	modeling dynamic aspects in virtual interactive environments	3d user interface;virtual reality;medical image;graphics hardware;3d environment;software industry;interactive environment;real time rendering;virtual worlds	Virtual 3D environments are found in a wide range of applications, like entertainment software, industrial simulations and virtual reality in medical imaging. Modern graphics hardware supports real-time rendering of highly complex virtual worlds; at the same time, extensive interaction capabilities are required to make an application useful. Graphical scenes are commonly composed of many reusable and recurring elements in a well-defined hierarchical way. In this work, a similar approach is proposed for modeling interaction capabilities. Comparable to the elements of a graphical scene, interaction mechanisms often consist of multiple components. By modeling these components as reusable objects, complex interaction mechanisms can easily be realized. The usefulness of this method is demonstrated in an application for visualizing 3D medical data, in which it has been applied to design both the 3D user interface and several interactive tools for manipulating medical datasets.		Mathias Seitel;Vivek Vaidya;Raghu Kokku;Rakesh Mullick	2006		10.1007/3-540-32137-3_62	human–computer interaction;computer science;instructional simulation;multimedia;computer graphics (images)	Visualization	-41.35300464158058	-33.74865022760301	53842
6c7160533968807857ebc1a072507c3c8f7de476	possibilities for the digital baton as a general-purpose gestural interface	hand held device;gestural interface;real time;controller;gestural input;musical instruments;musical instrument;computer music;conducting;gestural control;gesture recognition	This paper describes issues and results from the design and use of the Digital Baton, a new interface for real-time gestural control. Its construction was originally motivated by the need for a new instrument on which to perform computer music, and it was designed to replicate as closely as possible the feel of a traditional conducting baton. However, it has unexpectedly become a model for the design of new interfaces and digital objects, and is currently being used to record data for analysis in gesture-recognition research. Some preliminary results and future research areas are discussed at the end.	baton;general-purpose markup language;gesture recognition;real-time transcription;self-replicating machine	Teresa Marrin Nakra	1997		10.1145/1120212.1120409	speech recognition;controller;human–computer interaction;computer science;gesture recognition;conductor;computer music	HCI	-46.23400076181819	-36.13224795410777	53878
35b56924574c2af082ca0d7dcfefcd23656ed4a1	enhancing interactive television news	television news;network video;interactive television	A prototype system for interactive television news is described. It supports the full production cycle for interactive news, including assembly of clips into stories and stories into newscasts. A variety of interactive techniques are offered to the viewer. These include expressing likes and dislikes of headlines, skipping out of stories, requesting additional content and selection of stories from a menu. This system was deployed into homes for two weeks using fresh television content. User control events were logged and evaluated to understand interactive viewing behavior.	prototype	Dan R. Olsen;Benjamin Sellers;Trent Boulter	2014		10.1145/2602299.2602300	internet television;computer science;television director;multimedia;advertising;interactive television;computer graphics (images)	HCI	-45.385116806807005	-27.578664702018386	54028
6967510650d058c001cf355bd324cb761b2ffd21	industrial robots: computer interfacing and control by wesley e. snyder 325 pages, black/white photographs and line drawings, prentice-hall 1985 (£15.95)	line drawings;industrial robots		industrial robot;jim hall (programmer)	Peter B. Scott	1988	Robotica	10.1017/S026357470000374X	computer science;artificial intelligence	Robotics	-46.369688363156136	-29.624394452317887	54194
48719387663d97935cf28ae47799826a09bb8a83	the application of matlab platform in the teaching of digital signal processing	digital signal processing;teaching matlab digital signal processing;mathematics computing;word processing software word;computer aided instruction;finite impulse response filter;data processing;presses;data visualisation;virtual machines;signal processing;educational courses;multimedia communication;relevance theory;virtual machines computer aided instruction data visualisation digital signal processing chips educational courses mathematics computing signal processing teaching;visualization software;matlab digital signal processing discrete fourier transforms education finite impulse response filter multimedia communication presses;digital signal processing chips;computer simulation digital signal processing teaching data processing visualization software word processing software word matlab notebook;discrete fourier transforms;computer simulation;matlab;word processing;matlab notebook;teaching	Aiming at the course characteristics of Digital Signal Processing (DSP), the powerful data-processing and visualization software MATLAB is introduced into the course teaching. To this end, the MATLAB is seamlessly linked with the powerful word-processing software WORD with the help of MATLAB notebook, and then the teaching content combined with text and diagram, static and dynamic are realized in the teaching, which overcomes the cumbersome processes in the traditional multi-media teaching, in which the switches from one application program to another one is needed in order to present the results. Meanwhile, along with the course teaching, a course experimental system of DSP is designed based on the MATLAB platform. Through the computer simulation of the experimental system, the students can further apprehend and master the basic concepts, relevant theories and analysis methods of DSP. These will help improve the course teaching progress and effectiveness of DSP.	diagram;digital signal processing;experimental system;matlab;network switch;theory;visualization software	Yaxun Zhou;Jianfeng Dong	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.1190	computer simulation;software visualization;data processing;computer science;virtual machine;electrical engineering;theoretical computer science;digital signal processing;finite impulse response;signal processing;computer engineering	Robotics	-44.2595797822074	-28.450024951584638	54287
516661064aa2767c202769409b75b28b5cd0e13a	making it up as you go along - improvising stories for pedagogical purposes	animacion por computador;teoria cognitiva;interfase usuario;narrative;realite virtuelle;realidad virtual;cognitive appraisal;user interface;autonomous system;narration;virtual reality;cognitive theory;sistema autonomo;theorie cognitive;action selection;narracion;systeme autonome;interface utilisateur;enseignement;virtual environment;computer animation;teaching;role play;ensenanza;animation par ordinateur	We consider the issues involved in taking educational roleplay into a virtual environment with intelligent graphical characters, who implement a cognitive appraisal system and autonomous action selection. Issues in organizing emergent narratives are discussed with respect to a Story Facilitator as well as the impact on the authoring process.	action selection;autonomous robot;emergence;emergent democracy;graphical user interface;high- and low-level;level structure;microsoft lumia;noise shaping;nondeterministic algorithm;nonlinear gameplay;organizing (structure);pedagogical agent;virtual reality	Ruth Aylett;Rui Figueiredo;Sandy Louchart;João Dias;Ana Paiva	2006		10.1007/11821830_25	psychology;simulation;computer science;artificial intelligence;virtual reality;multimedia;narrative;communication	AI	-35.56077908752089	-26.399810380991543	54337
3bda3b2316798981002b1da3ca44274f8b167625	from scientific computation to decision support	decision support;scientific computing	Abstract   Research institutes and universities have developed many programs which have been used up until now mainly by specialists. The input preparation, the interpretation of the output, and the use of the program are often too complicated to be suitable for a wider group of users. A part of the knowledge necessary for the use of a scientific program can be included in a user interface. The development of such user interfaces is a mixture of the development of an expert system and a normal user interface. The impact of the new user interface on the changing user task has to be taken into account in the development process.	computation;computational science;decision support system	Henk J. van Zuylen	1993	Knowl.-Based Syst.	10.1016/0950-7051(93)90003-C	user interface design;user;user modeling;computer user satisfaction;interface metaphor;decision support system;human–computer interaction;computer science;knowledge management;data mining;user interface	DB	-46.409874919138	-28.010589537446016	54355
074f7448c838f24dbad1f9398e979eeecc937699	the chemical table: an open dialog between visualization and design	data visualisation chemistry computing;periodic table information visualization chemical tables;chemical tables;chemistry computing;information visualization;periodic table;data visualisation;chemical information visualization;chemical table;chemical information visualization chemical table	With a history of over 300 years, the chemical table is the textbook case of information visualization. This paper explores the development of the chemical table as a tool designed for chemical information visualization. It uses a historical context to investigate the purpose of chemical tables and charts, and suggests reasons why the two-dimensional periodic table remains the de facto standard for chemical information display.	chart;cheminformatics;decision table;display device;information visualization;dialog	Francis T. Marchese	2008	2008 12th International Conference Information Visualisation	10.1109/IV.2008.79	information visualization;computer science;theoretical computer science;database;data visualization	Visualization	-41.02292056129203	-28.160338369763664	54357
287d19b17744942e2ddd9a963763fe9963a287e1	integrating video into an application framework	video object;application framework;motion video;data type;object oriented	Object-oriented application frameworks help software engineers develop sophisticated interactive applications quickly and reduce maintenance costs significantly. An application framework provides, among others, predefined visual classes for text or graphics display and for user interaction elements, as well as integrated control of these elements. Designed before motion video display was technically feasible on desktop computers, these frameworks’ architectures are built around the assumption of relatively slowly changing bitmap images to be displayed on a computer screen. Motion video screen update rates were not anticipated. We extended a typical application framework in a multitasking environment to support motion video display in full generality. The internal architecture of the framework was changed to remove the subtle obstacles for high screen update rates and a data type ‘video’ was designed and integrated seamlessly with the existing visual classes. Our video objects display motion video generated by autonomous hardware or software processes. They may appear in any shape or number mixed with other visual objects in scrollable views, as building blocks in graphics editors, as characters in text editors, as items in list or pop-up menus, etc. Video objects support the full visual class protocol for client-transparent double buffering, cut/copy/paste/undo operations, and output to or input from files. In this paper, we describe the features of these video objects and our changes to the architecture of the application framework necessary to support the new paradigms of motion video.	application framework;autonomous robot;bitmap;computer monitor;computer multitasking;cut, copy, and paste;data compression;desktop computer;digital video;display device;framebuffer;graphics software;hard disk drive;jpeg;moving picture experts group;multiple buffering;programming paradigm;software development process;software engineer;streaming media;text editor;undo;usability;video file format;visual objects	Peter Schnorf	1993		10.1145/166266.168439	computer vision;data type;computer science;video tracking;multimedia;object-oriented programming;motion compensation;computer graphics (images)	HCI	-41.4735680873794	-31.224821515217318	54544
16191df9c2205fce340967183c86255907e962e7	the implementation of the virtual tourism teaching system	interactive roaming function;virtual tourism teaching system;software platform;user interface;computer aided instruction;virtual roaming virtual reality three dimensional modeling openscenegraph;software 3ds max 7 0;virtual reality;three dimensional modeling;real time simulation;three dimensional;realtime simulation;computational modeling;education object oriented modeling layout virtual reality solid modeling educational institutions application software shape rendering computer graphics operating systems;three dimensional displays;user interface virtual tourism teaching system virtual reality technology software 3ds max 7 0 super3d editor realtime simulation interactive roaming function super3d object group items;solid modeling;openscenegraph;super3d object group items;user interfaces;power modeling;object oriented modeling;virtual reality computer aided instruction real time systems user interfaces;super3d editor;data models;virtual reality technology;virtual roaming;real time systems	over the past few years virtual reality technology has developed very rapidly, it has been widely used in many areas. However, there are very few applications in education in ordinary schools. Because of virtual reality technology can make up for the inadequacy of traditional teaching, there are broad application prospects in the teaching. In this paper, a powerful modeling software-3DS MAX 7.0 is chosen to create three-dimensional model for tourist attractions; then, virtual software platform --Super3D Editor is used to realize the real-time simulation and interactive roaming function; finally, we realize rapid development with Super3D Object Group items. We design the user interface and applied to the tourism teaching, expand the function to achieve the purpose of tourism teaching.	3d modeling;max;real-time transcription;simulation;software development kit;systems design;user interface;virtual reality;virtual tour	Hongli Liu;Feng Qiu;Linlin Wang;Haipeng Shi	2009	2009 Second International Symposium on Computational Intelligence and Design	10.1109/ISCID.2009.203	simulation;human–computer interaction;computer science;virtual reality;multimedia;user interface	Arch	-46.19117837292672	-29.05010952702481	54583
095f02fb96bb8df1fc13eba4cb77e67f6becd720	a taxonomy of see-through tools	user interface;button;menu;macro;multihand;transparent;lens;viewing filter;control panel	In current interfaces, users select objects, apply operations, and change viewing parameters in distinct steps that require switching attention among several screen areas. Our SeeThrough InterfaceTht software reduces steps by locating tools on a transparent sheet that can be moved over applications with one hand using a trackball, while the other hand controls a mouse cursor. The user clicks through a tool onto application objects, simultaneously selecting an operation and an operand. Toois may include graphical filters that display a customized view of application objects. Compared to traditional interactors, these tools save steps, require no permanent screen space, reduce temporal modes, apply to multiple applications, and facilitate customization. This paper presents a taxonomy of see-through tools that considers variations in each of the steps they perform. As examples, we describe particular see-through tools that perform graphical editing and text editing operations. CR	apply;cursor (databases);glossary of computer graphics;graphical user interface;operand;pointer (user interface);taxonomy (general);text editor;trackball	Eric A. Bier;Maureen C. Stone;Kenneth P. Fishkin;William Buxton;Thomas Baudel	1994		10.1145/191666.191786	human–computer interaction;computer hardware;computer science;operating system;macro;lens;multimedia;user interface	HCI	-41.392052232735864	-30.868051203632575	54692
446e04b6fae52f16c84cf0397dae591f6acc96dd	computational design of mechanical characters	fabrication;mechanical characters;interactive design;animation	We present an interactive design system that allows non-expert users to create animated mechanical characters. Given an articulated character as input, the user iteratively creates an animation by sketching motion curves indicating how different parts of the character should move. For each motion curve, our framework creates an optimized mechanism that reproduces it as closely as possible. The resulting mechanisms are attached to the character and then connected to each other using gear trains, which are created in a semi-automated fashion. The mechanical assemblies generated with our system can be driven with a single input driver, such as a hand-operated crank or an electric motor, and they can be fabricated using rapid prototyping devices. We demonstrate the versatility of our approach by designing a wide range of mechanical characters, several of which we manufactured using 3D printing. While our pipeline is designed for characters driven by planar mechanisms, significant parts of it extend directly to non-planar mechanisms, allowing us to create characters with compelling 3D motions.	3d printing;computation;crank (person);interactive design;rapid prototyping;semiconductor industry	Stelian Coros;Bernhard Thomaszewski;Gioacchino Noris;Shinjiro Sueda;Moira Forberg;Robert W. Sumner;Wojciech Matusik;Bernd Bickel	2013	ACM Trans. Graph.	10.1145/2461912.2461953	anime;simulation;computer science;fabrication;interactive design;computer graphics (images)	Graphics	-38.71943182602345	-33.78592683647376	54812
5c7bf38cf25a6ef80c4c3b6e1dddb7439fb80ef2	\alphatrellis: a system for writing and browsing petri-net-based hypertext	document structure;pilot study;directed graph;authoring system;user interface design;access control;petri net	We have developed a new model of hypertext in pilot studies. The traditional hypertext model resembles a directed graph, representing information fragments and the relationships that tie the fragments together. Our model, based on Petri nets, also represents the hypertext's browsing semantics (i.e., how the information is to be visited). The Petri net model is a generalization of traditional directed graph models. It permits development of browsing and authoring systems that can incorporate the analytical techniques that have been developed for Petri nets and also incorporate the user interface designs that have been developed for hypertext systems. The Petri net base also permits powerful specification of how the hypertext is to be browsed. New abilities include synchronization of simultaneous traversals of separate paths through a hypertext as well as of security/access control considerations into a hypertext (specifying nodes that can be proven accessible only to certain classes of browsers). In addition, different tailored versions can be generated from a single document structure in the Petri net-based model.	hypertext;petri net	P. David Stotts;Richard Furuta	1989		10.1007/3-540-53863-1_35	user interface design;directed graph;computer science;access control;document structure description;database;multimedia;world wide web;petri net	NLP	-39.64786251940286	-26.154373790713606	54844
0a3f938910dcde6a9d801372fe21470b08843773	integrated development environment for computer music composition	arte		integrated development environment	Jon Drummond	1997			human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-47.16119793953189	-32.101659726781705	54864
af87f9c216f59259e9a781b646c66924a8ff297f	wiring cracker: the mechanics of a non-anthropomorphic, real-time, performance animation puppet	real time	"""At Protozoa we've coined the term """"wiring"""" to describe the art of procedurally connecting data from various input devices to a real-time character or CG puppet. Mapping a human's motions onto a non-humanoid character is one of the more fascinating aspects of wiring. One such character, """"Cracker,"""" a genetically mutated """"Lizard Cow,"""" was designed and modeled by Bay Raitt and wired by myself for an in-house short animation called """"Meat."""" It's the nonanthropomorphic mapping between the performer and the CG character as well as the techniques used to accomplish this that are described in this sketch."""	cg artist;digital puppetry;input device;motion capture;old red cracker;real-time computing;real-time locating system;real-time transcription;wiring	Mike Morasky	1998		10.1145/280953.282445	simulation;computer science;artificial intelligence;computer graphics (images)	Graphics	-43.84299665710098	-35.19202471389547	54899
b4c307289648b7baba3d24fbd7ac4555ff37ace2	immersive sound field simulation in multi-screen projection displays		This paper describes the immersive sound field simulation technology that represents an interactive sound field in the multi-screen projection display. In this method, convolution filters, that were calculated based on the wave equation, are replaced in real-time using the multi-channel digital signal processor, and the simulated sounds are displayed using the 16-channel speakers. In addition, compensation filters are used to reduce the influence of the screen attenuation. This system was applied to the video avatar communication, and the effectiveness of this method was evaluated.	simulation	Tetsuro Ogi;Takuro Kayahara;Masafumi Kato;Hiroshi Asayama;Michitaka Hirose	2003		10.2312/EGVE/IPT_EGVE2003/135-142	computer vision;simulation;computer science;computer graphics (images)	HCI	-45.56186607586732	-34.374289162380656	55761
911c1a0a085d062bee51c290d090462bb52d0bcb	real-time 3d fire simulation using a spring-mass model	silk torch;mass spring system;real time control;real time interactions;real time;real time simulation;3d fire simulation;augmented reality 3d fire simulation spring mass model real time simulation silk torch flames kinematics turbulent visual dynamics texture sequencing real time interactions;turbulent visual dynamics;flames kinematics;texture sequencing;3 dimensional;rendering computer graphics augmented reality digital simulation fires;fires computational modeling computer simulation fuels rendering computer graphics computational efficiency application software gases computer graphics augmented reality;augmented reality;rendering computer graphics;fires;spring mass model;digital simulation	"""We present a method for real-time simulation of 3D fire inspired by an old mechanical trick known as the """"silk torch"""". Motivated by the proven illusive effect of silk torch, we model the kinematics of flames by a mass-spring system and its turbulent visual dynamics by texture sequencing with variable speeds and transparencies that depend on the speed of the vaporized fuel. The approach allows for incorporating external forces such as the gravity, and the wind force for added realism. Also, a specific characteristic of our method is that any object inserted in aflame can be modeled simply as an external force in the mass-spring system, making real-time interactions with fire a simple addition to the overall system. While the approach maintains an extremely low computational cost, these flexibilities increase the realism of our 3D fire by allowing for real-time control and the interactivity, which is a highly desirable requirement in applications such as augmented reality"""	algorithmic efficiency;augmented reality;evaporation;interaction;interactivity;real-time clock;real-time transcription;silk road;simulation;torch;transparency (projection);turbulence	Murat Balci;Hassan Foroosh	2006	2006 12th International Multi-Media Modelling Conference	10.1109/MMMC.2006.1651309	three-dimensional space;augmented reality;simulation;real-time control system;computer science;multimedia;effective mass;computer graphics (images)	Robotics	-40.91482694802722	-34.72985908452682	55972
7db137d45dd43f8f59819cc2cd1ee9b9324ad56f	easysnap: real-time audio feedback for blind photography	real time;photography;blind users;computer vision;low vision;camera phone;visual interfaces;non visual interfaces	This demonstration presents EasySnap, an application that enables blind and low-vision users to take high-quality photos by providing real-time audio feedback as they point their existing camera phones. Users can readily follow the audio instructions to adjust their framing, zoom level and subject lighting appropriately. Real-time feedback is achieved on current hardware using computer vision in conjunction with use patterns drawn from current blind photographers.	audio feedback;camera phone;computer vision;framing (world wide web);norm (social);real-time locating system;real-time transcription	Samuel White;Hanjie Ji;Jeffrey P. Bigham	2010		10.1145/1866218.1866244	computer vision;computer science;photography;multimedia;camera phone;computer graphics (images)	HCI	-42.34147106965513	-37.92944913623321	56001
a02ce1d18eada6c266615b4751f6f8ead0339c05	towards accessible automatically generated interfaces part 1: an input model that bridges the needs of users and product functionality		Automatic model-based generation of user interfaces is a potential strategy to enable individuals with disabilities to control products and services with an interface that fits their specific needs. Most of the existing work and models have been focused on mainstream users and have evolved into complex, multilayered approaches. In this paper, we describe a new, simpler input model, the FIN-USI model, which is a bridge between the basic input a system/device needs for functionality (the Functionality Input Needs; FINs) and the basic input that users provide as input in an abstract, modality independent manner (User-Sensible Inputs; USIs). An abstract model of a user interface can be made up of FIN and USI elements. Each FIN and USI element consists of a type of input and characteristics that are applied to that input type. Input elements may be grouped for functionality or usability reasons. All the components of the model are described and examples of application are given in this paper.		J. Bern Jordan;Gregg C. Vanderheiden	2017		10.1007/978-3-319-58530-7_9	personalization;universal design;usability;human–computer interaction;computer science;user interface	DB	-40.95111779119185	-29.063291931030548	56071
07bd1c2b5d8b0f83ec7c0997fbbffec52ed23122	visualization of water quality data for the chesapeake bay	water pollution;3d visualization;center for computational field simulation;chesapeake bay;mississippi state university;scirt;site characterization interactive research toolkit;interactive system;model-data comparisons;rendering;simulation;water quality data visualization;water quality measurement	We discuss a visualization system for the comparison of simulated and measured water quality. The system extends SCIRT (Site Characterization Interactive Research Toolkit), an interactive system originally developed at the NSF Engineering Research Center for Computational Field Simulation at Mississippi State University. The ongoing study of the Chesapeake Bay presents research in 3D visualization of model-data comparisons.	computation;ibm notes;interactivity;simulation;volume rendering	Adam B. Forgang;Bernd Hamann;Carl F. Cerco	1996	Proceedings of Seventh Annual IEEE Visualization '96		visualization;rendering;computer science;water pollution;computer graphics (images)	Visualization	-34.35562970516978	-29.877569231049282	56096
78b655145081cde6937ab0227bd46f238c48e4a2	an adaptive window management system	adaptive user interfaces;adaptive window management;user model;adaptive window management system	Modern complex and information intensive computer applications invol ve multi-window operations. To simultaneously visualize all the necessary information for a task requ ires time–consuming window management operations by the user. Since these activities are not directly related to th e user’s task domain, time spent on window management results in loss of user’s mental context and an increase in the actual task completion time. We propose an Adaptive Window Manager (AWM) which automates the layout of the window s n the display screen according to the current user and his current task domain and learns the user’s layout requirem ents. In this paper, we shortly address what user model we used for building AWM.	computer;management system;user modeling;window manager	Stefan Stille;Shailey Minocha;Rolf Ernst	1997			user interface design;embedded system;real-time computing;user modeling;title bar;shell;human–computer interaction;computer science;operating system;user interface	HCI	-40.29171757706329	-28.09535133967687	56233
4e5d839af8e6044b61e46d781b5e5ce4f80e11e0	artificial intelligence and dynamic design: adaptive real time 3d characters	human computer interaction;human computer interfaces;real time;artificial intelligence humans internet pipelines art computer interfaces virtual environment intelligent agent coaxial components databases;dynamic design;artificial intelligent;adaptive real time 3d characters;software agents;internet;artificial intelligence;reactive visually dynamic web based agents;software agents artificial intelligence human computer interaction interactive systems internet real time systems;interactive systems;human computer interface;real time systems;human computer interfaces artificial intelligence dynamic design adaptive real time 3d characters reactive visually dynamic web based agents	Creating design that relates to the dynamic nature of a conversation is what this project explores. As a focus of intent the exploration pursues the interaction that people have with reactive visually dynamic Web based agents. This is in an attempt to expand on the concept on human computer interfaces by creating design specific, emotive and responsive. The current focus is on a single real-time 3D character with the design concept encompassing elements of conversational artificial intelligence	artificial intelligence;human computer;kinetic data structure;real-time clock	Mark J. Chavez	2006	Tenth International Conference on Information Visualisation (IV'06)	10.1109/IV.2006.24	simulation;artificial architecture;human–computer interaction;computer science;multimedia	Robotics	-47.92374605277607	-34.422041976759935	56318
76345777258baddc392babec8007b8910f2eabcb	data-driven object manipulation in images	chen goldberg;object completion;wiley periodicals;supported manipulation;user study;candidate object;data-driven object manipulation;tsinghua university;related object;similar shape;selected object	We present a framework for interactively manipulating objects in a photograph using related objects obtained from internet images. Given an image, the user selects an object to modify, and provides keywords to describe it. Objects with a similar shape are retrieved and segmented from online images matching the keywords, and deformed to correspond with the selected object. By matching the candidate object and adjusting manipulation parameters, our method appropriately modifies candidate objects and composites them into the scene. Supported manipulations include transferring texture, color and shape from the matched object to the target in a seamless manner. We demonstrate the versatility of our framework using several inputs of varying complexity, for object completion, augmentation, replacement and revealing. Our results are evaluated using a user study.	alpha compositing;blackwell (series);complexity;computer;display resolution;entity–relationship model;eurographics;interactivity;internet;norm (social);seamless3d;thin plate spline;usability testing	Chen Goldberg;Tao Chen;Fang-Lue Zhang;Ariel Shamir;Shi-Min Hu	2012	Comput. Graph. Forum	10.1111/j.1467-8659.2012.03005.x	computer vision;method;computer science;artificial intelligence;multimedia;computer graphics (images)	Vision	-38.756127825872134	-34.745650083025865	56432
e3f311bb9ac67c606ac51e83c499f0a8e8da02d3	application of augmented reality to industrial tele-training	collaborative work;application software;wearable computers;virtual reality;telecommunication computing;computer networks;computer vision;computer displays;wearable computer;augmented reality;mobile computing;user equipment;augmented reality wearable computers mobile computing computer networks collaborative work computer displays telecommunication computing computer vision application software virtual reality;virtual worlds;head mounted display	Augmented Reality (AR) is a departure from standard virtual reality in a sense that it allows users to see computer generated virtual objects superimposed over the real world through the use of see-through head-mounted display. Users of such system can interact in the real/virtual world using additional information, such as 3D virtual models and instructions on how to perform these tasks in the form of video clips, annotations, speech instructions, and images. In this paper, we describe a prototype of a collaborative industrial Tele-training system. The distributed aspect of this system will enable users on remote sites to collaborate on training tasks by sharing the view of the local user equipped with a wearable computer. The users can interactively manipulate virtual objects that substitute real objects allowing the trainee to try out and discuss the various tasks that needs to be performed. Experimental results are presented.	algorithm;augmented reality;binary code;computer vision;error detection and correction;geforce 4 series;graphics processing unit;head-mounted display;interactivity;item unique identification;laptop;prototype;real-time clock;requirement;stereoscopy;television;user interface;video card;video clip;virtual reality;virtual world;wearable computer	Pierre Boulanger	2004	First Canadian Conference on Computer and Robot Vision, 2004. Proceedings.	10.1109/CCCRV.2004.1301462	augmented reality;computer-mediated reality;simulation;wearable computer;human–computer interaction;computer science;metaverse;virtual reality;mixed reality;multimedia;mobile computing	Visualization	-43.15418153290758	-36.39849206531029	56450
3c7d4f2493e2e81ff86b3580699b4a42f983bcd8	tactus: toolkit-level support for synchronized interactive multimedia	multimedia;real time;audio video;interactive multimedia;interactive;synchronization;interface;graphic user interface;toolkit	Tactus addresses problems of synchronizing and controlling various interactive continuous-time media. The Tactus system consists of two main parts. The first is a server that synchronizes the presentation of multiple media, including audio, video, graphics, and MIDI at a workstation. The second is a set of extensions to a graphical user interface toolkit to help compute and/or control temporal streams of information and deliver them to the Tactus Server. Temporal toolkit objects schedule computation events that generate media. Computation is scheduled in advance of real time to overcome system latency, and timestamps are used to allow accurate synchronization by the server in spite of computation and transmission delays. Tactus supports precomputing branches of media streams to minimize latency in interactive applications.	computation;graphical user interface;graphics;midi;precomputation;server (computing);user interface toolkit;widget toolkit;workstation	Roger B. Dannenberg;Thomas P. Neuendorffer;Joseph M. Newcomer;Dean Rubine;David B. Anderson	1993	Multimedia Systems	10.1007/BF01213486	synchronization;real-time computing;telecommunications;computer science;operating system;interface;graphical user interface;distributed computing;multimedia;interactive media;interactivity;world wide web	OS	-42.68966155135028	-34.57788046874789	56474
97d52a08b6bc1edcc18e0c9c9f5dd6358b13ae14	a collaborative design framework in a distributed virtual environment.	distributed virtual environment;collaborative design			Hiroaki Nishino;Yoichi Mori;Kazuyoshi Korida	1999			computer science;instructional simulation;distributed design patterns	HPC	-48.12949795611732	-31.804867313513277	56703
e7be089f543ebba106af1fe0bfb8a5631c21f2f3	integrating interactive 3d-graphics into an object-oriented application framework	application framework;3d user interface;building block;user interface;direct manipulation;interactive 3d graphics;object oriented;interactive graphics;3d graphics;interaction technique	This paper describes the integration of 3D graphics into the visual 2D part of an application framework. Most object-oriented application frame-works are built to ease the development of interactive graphical applications that use direct manipulation techniques. This study is based on the object-oriented application framework ET++ that provides predefined visual classes for text, 2D graphics display, and standard user interface components. We extended ET++ to support 3D graphics in a general way. It is now possible to integrate 3D graphics objects which may be placed wherever other visual objects can go, i.e. in scrollable views, as building blocks in dialogs or graphics editors, as characters in text editors, as items in lists or pop-up menus, etc. As a consequence 2D and 3D graphics are dealt in a uniform way. Interaction techniques on 3D graphics objects and 3D user interface components are supported.	3d computer graphics;application framework	Dominik Eichelberg;Philipp Ackermann	1993		10.1007/3-540-57312-7_54	human–computer interaction;computer science;real-time computer graphics;multimedia;graphics software;interactive media;computer graphics;user interface;3d computer graphics;computer graphics (images)	NLP	-41.58093436674472	-30.775912980985765	56707
517d7e896383ce7b88d2d7fabc03c31ca88c3f40	formulating efficient software solution for digital image processing system	opencv;image processing;software solution;sustainability;plugin;matlab	Digital image processing systems are complex, being usually composed of different computer vision libraries. Algorithm implementations cannot be directly used in conjunction with algorithms developed using other computer vision libraries. This paper formulates a software solution by proposing a processor with the capability of handling different types of image processing algorithms, which allow the end users to install new image processing algorithms from any library. This approach has other functionalities like capability to process one or more images, manage multiple processing jobs simultaneously and maintain the manner in which an image was processed for later use. It is a computational efficient and promising technique to handle variety of image processing algorithms. To promote the reusability and adaptation of the package for new types of analysis, a feature of sustainability is established. The framework is integrated and tested on a medical imaging application, and the software is made freely available for the reader. Future work involves introducing the capability to connect to another instance of processing service with better performance. Copyright © 2015 John Wiley u0026 Sons, Ltd.	digital image processing	Thomas Sherwood;Ezak Ahmad;Moi Hoon Yap	2016	Softw., Pract. Exper.	10.1002/spe.2339	embedded system;plug-in;image processing;computer science;backporting;theoretical computer science;software development;operating system;software construction;sustainability;computer graphics (images)	EDA	-44.61238529999589	-29.615695909184947	56924
e4703f69dd0f2970495bb1314e5f2e8bed50eec7	towards augmented choreography	settore inf 01 informatica	Choreographers are interested in enriched performances where virtual actants play together with live performers. Augmented Choreography can be viewed as the definition of how perceptions generated from the environment turn into commands that influence the environment itself and, in particular, virtual actants. This paper introduces a modular and extensible architecture that supports the flexible and dynamic definition of augmented choreographies and presents an experimental application.	actant;augmented reality;performance	Diego Bernini;Giorgio De Michelis;Mauro Plumari;Francesco Tisato;Michele Cremaschi	2011		10.1007/978-3-642-33329-3_2	computer science	HCI	-45.01066242070332	-35.55783164216242	57030
2fa5f55a697f729923e43086f1751018a8649f8b	a realtime and direct-touch interaction system for the 3d cultural artifact exhibition	computer graphics;virtual reality;haptics;digital museum	"""We propose a realtime and direct-touch interaction system for 3D cultural artifact exhibition based on a texture-based haptic rendering technique. In the field of digital archive, it is important to archive and exhibit the cultural artifact at the high-definition. To archive the shape, color and texture of the cultural artifact, it is important to archive and represent not only visual effect but haptic impression. Therefore, multimodal digital archiving, realtime multisensory rendering, and intuitive and immersive exhibition system are necessary. Therefore, we develop a realtime and direct-touch interaction system for the 3D cultural artifact exhibition based on a texture-based haptic rendering technique. In our system, the viewer can directly touch a stereoscopic vision of 3D digital archived cultural artifact with the string-based and scalable haptic interface device """"SPIDAR"""" and vibration motor."""		Wataru Wakita;Katsuhito Akahane;Masaharu Isshiki;Hiromi T. Tanaka	2011		10.1007/978-3-642-22024-1_22	computer vision;art;multimedia;computer graphics (images)	HCI	-41.89882583106814	-36.45080094588027	57032
c518468eeb4bda76eadab6971d7d020711bb615f	a tool to estimate usability of web 2.0 applications	web pages;usability visualization;website;web sites ergonomics software reusability;web usage mining;software reusability;web sites;data visualization;xml;data usability web 2 0 applications website;web server;usability web pages web server data visualization xml ergonomics;web 2 0 applications;usability;ergonomics;data usability;usability visualization web usage mining dynamic analysis;dynamic analysis	Nowadays, companies and home users use websites offering services ranging from web sites up to complex web applications. The ergonomics of these applications often remain unconsidered and the applications turn out to be hard to use. In this paper, a tool is presented to facilitate the examination of usability. Web 2.0 applications in particular are supported, because they are more flexible and require other techniques than traditional web applications. The paper explains how to collect, analyze, process and visualize usability data for Web 2.0 applications.	ajax (programming);human factors and ergonomics;usability;usage data;web 2.0;web application;web design;web search engine;web server;xml	Ludger Martin	2009	2009 11th IEEE International Symposium on Web Systems Evolution	10.1109/WSE.2009.5631254	web service;web usability;web application security;web development;web modeling;xml;web analytics;web mapping;web-based simulation;usability;web design;human–computer interaction;web accessibility initiative;web standards;computer science;web navigation;web page;database;dynamic program analysis;web intelligence;web engineering;programming language;web 2.0;world wide web;data visualization;web server;mashup;usability inspection	Web+IR	-43.513298608072525	-24.769837041314904	57257
23f9ea2ffc61abe1be75dd5c7bf707a8715dbe39	assembling the planetary computer	hand held device;interpersonal communication;data stream;wireless internet;growth rate;sensors and actuators;ubiquitous computing;wireless lan;peer to peer computing;switching network;embedded processor	Geographic Information Systems (GIS) are computerized systems for the storage, retrieval, manipulation, analysis, and display of geographically referenced data (Mark et al., 1996). The components of a GIS project fall into three categories; computer hardware and software, spatial data, and trained personnel. A GIS can also be considered a visual language that allows the user to visualize and easily interact with digital data. This paper will discuss the basics behind defining and assembling a planetary GIS project.	computer hardware;digital data;geographic information system;online and offline;planetary scanner;spatial analysis;visual language	Larry Smarr	2001		10.1007/3-540-45427-6_1	embedded system;simulation;human–computer interaction;telecommunications;computer science;operating system;computer security;ubiquitous computing;computer network;interpersonal communication	HCI	-34.00187563258487	-36.06334450613972	57322
4d359ba450836d5d36e4e62ea7d158ceba0a6a62	placid: a planner for dynamically composing user interfaces services	formalization;user interface;ui composition;algorithm;dynamic composition;ui service	Dynamic Services Composition (DSC) aims at composing interactive systems from a set of available services corresponding to the available components. A component consists of a Functional Core and/or of a User Interface (UI) respectively providing computation and/or representation functions. In software engineering, a part of the literature focuses on the dynamic composition of computation services. Making the hypothesis that UI services can also be composed leads to a new research area in Human Computer Interaction: the dynamic composition of UI services. This paper presents two main contributions: the formalization of the problem and its solving by planning.	automated planning and scheduling;computation;human computer;human–computer interaction;software engineering;user interface	Yoann Gabillon;Gaëlle Calvary;Humbert Fiorino	2014		10.1145/2607023.2610277	human–computer interaction;computer science;database;world wide web	SE	-40.6942137930108	-27.17055044617281	57460
e932d528eee43f7de54ec33d261c1b255770fbf7	introduction to the special session on visual languages for human-to-human communication	visual language	T h e world-wide connectivity of the Internet and the high-quality graphzcal capabilities of today’s personal computers are fostering a growth of new software that permits people t o communicate with others i n a varie t y of visual ways. T h e papers i n this session allustrate several examples of these approaches. I n some cases the focus of the communicat ion is on the computational properties of objects or systems. I n other cases, the communicat ion is not about computation, but the computer technology supports that communication.	computation;personal computer	Steven L. Tanimoto	1997		10.1109/VL.1997.626604	natural language processing;computer science;multimedia;programming language;visual communication	Theory	-47.24112369677919	-26.51374761129091	57572
5af3a7cb348c044ad54c4d1740196cf370f1eec8	interactive focus+context analysis of large, time-dependent flow simulation data	methode domaine temps;modelizacion;sistema interactivo;base dato multidimensional;vision ordenador;time dependent;time dependent flow visualization;traitement flux donnee;analisis datos;multidimensional data visualization;flux donnee;analyse temporelle;flujo datos;fluid mechanics;metodo dominio tiempo;multidimensional database;analisis temporal;time dependent features;mecanique fluide;time analysis;computer vision;systeme conversationnel;multiple view;computational fluid dynamics;modelisation;interactive feature specification;data analysis;dependance du temps;time dependence;visualisation ecoulement;interactive system;simulation ecoulement;data flow processing;data visualization;data flow analysis;vue multiple;analyse donnee;analyse flux donnee;visualisation donnee;vision ordinateur;base donnee multidimensionnelle;time domain method;focus context visualization;flow simulation;data flow;mecanique fluide numerique;mecanica fluido numerica;modeling;dependencia del tiempo;mecanica fluido;flow visualization;vista multiple	Visualization of time-dependent simulation data, such as datasets from CFD simulation, still is a very challen ging task. In this paper, we present a new approach to the interact ive visual analysis of flow simulation data which is especially t argeted at the analysis of time-dependent data. It supports the flexi ble specification and visualization of flow features in an interactive setup of multiple linked views. Special emphasis is put on new mechanisms to capturetime-dependent features, i.e., flow features which are inherently dependent on time. We proposethe integration of attribute derivation into the process of interactive visual analysis to enable the subsequent user access to othe rwise implicit properties of the unsteady data in our interactive feature specification framework. All views of this flow analysis setup are linked in the sense that the features in focus are consistent ly emphasized in the visualization (more colorful, less trans parent) whereas the rest of the data is only shown as context in reduce d style. In addition to introducing our new approach, we also demonstrate its use in the context of several application ex amples.	3d rendering;authorization;autostereogram;brushing and linking;computational fluid dynamics;data-flow analysis;diesel;feature extraction;flexible-fuel vehicle;focus-plus-context screen;freedom of information laws by country;gnu variants;interactive visual analysis;simulation;source-to-source compiler;visualization (graphics)	Helmut Doleisch;Helwig Hauser;Martin Gasser;Robert Kosara	2006	Simulation	10.1177/0037549707078278	data flow diagram;simulation;systems modeling;flow visualization;computational fluid dynamics;computer science;data-flow analysis;data analysis;data visualization;algorithm;fluid mechanics;computer graphics (images)	Visualization	-34.815613392180495	-28.374543965588728	57611
8b703748344451d8dbd2c3e0e9375ca18b66c3ca	intelligent analysis of user interactions with web applications	task models;task model;remote evaluation;remote usability evaluation;intelligence analysis;tools;usability;user interaction	In this paper, we describe a tool able to perform intelligent analysis of Web browser logs using the information contained in the task model of the application. We show how this approach supports remote usability evaluation of Web sites.	data logger;interaction;java;javascript;source-to-source compiler;usability;web application	Laila Paganelli;Fabio Paternò	2002		10.1145/502716.502735	pluralistic walkthrough;web usability;component-based usability testing;cognitive walkthrough;intelligence analysis;usability;web design;human–computer interaction;computer science;usability engineering;web navigation;task analysis;multimedia;heuristic evaluation;world wide web;usability lab;usability inspection;remote evaluation	Web+IR	-42.62638186008842	-28.978332943206823	57614
a9f1eb1ae012c28951742e3d615674a7b208d357	towards user-friendly projectional editors		Today’s challenges for language development include language extension and composition, as well as the use of diverse notations. A promising approach is projectional editing, a technique to directly manipulate the abstract syntax tree of a program, without relying on parsers. Its potential lies in the ability to combine diverse notational styles – such as text, symbols, tables, and graphics – and the support for a wide range of composition techniques. However, projectional editing is often perceived as problematic for developers. Expressed drawbacks include the unfamiliar editing experience and challenges in the integration with existing infrastructure. In this paper we investigate the usability of projectional editors. We systematically identify usability issues resulting from the architecture. We use JetBrains Meta Programming System (MPS) as a case study. The case study discusses the concepts that MPS incorporates to address the identified issues, evaluates effectiveness of these concepts by surveying professional developers, and reports industrial experiences from realizing large-scale systems. Our results show that the benefits of flexible language composition and diverse notations come at the cost of serious usability issues – which, however, can be effectively mitigated with facilities that emulate editing experience of parser-based editors.	antlr;abstract syntax tree;baseline (configuration management);bioinformatics;categorization;context-free grammar;context-free language;cybernetics;eclipse modeling framework;electronic notes in theoretical computer science;embedded software;embedded system;experience;graphical user interface;graphics;handbook;hierarchical editing language for macromolecules;instance (computer science);ipke wachsmuth;ll parser;language workbench;lecture notes in computer science;material point method;meta programming system;metaprogramming;parse tree;parser combinator;parsing;problem solving;programming paradigm;projection screen;pure;real-time transcription;requirement;sigchi;semantics (computer science);software engineering;springer (tank);structure editor;symbol (formal);syntactic pattern recognition;syntax definition formalism;table (database);text editor;usability;usability engineering;visual language	Markus Völter;Janet Siegmund;Thorsten Berger;Bernd Kolb	2014		10.1007/978-3-319-11245-9_3	artificial intelligence;natural language processing;computer science;usability;parsing;architecture;abstract syntax tree;notation;graphics;user friendly;metaprogramming	SE	-39.95318724993915	-28.672581665643285	57681
f01d7744f594bfbb23ebb0200b7ee16412afeac5	user interface tools	user interface	A user inte~ace tool is any software that helps user interfaee designers or programmers design, implement and test user interfaces and user interfaee software. Whereas five years ago, user interface tools were primarily research projeets, today there are literally hundreds of successful commercial user interface tools. In addition, research into new techniques and tools is extremely active, with one or two sessions at each CHI conference, and an entire sepamte conference (UIST) devoted to this topic every year. This tutorial provides an overview of both the commercial and research segments of this area.	acm symposium on user interface software and technology;automatic computing engine;chi;programmer	Brad A. Myers;Dan R. Olsen	1994		10.1145/259963.260535	user interface design;look and feel;user;10-foot user interface;interface metaphor;shell;natural language user interface;magic pushbutton;computer science;network interface;console application;natural user interface;debug menu;interactivity;user interface;graphical user interface testing;multiple document interface	EDA	-43.15529932690556	-30.285196196565963	57731
44cd85e4e6db85d66600713e428506e1c77ba25b	arrangements: flexibly adapting music data for live performance		Human-Computer Music Performance for popular music – where musical structure is important, but where musicians often decide on the spur of the moment exactly what the musical form will be – presents many challenges to make computer systems that are flexible and adaptable to human musicians. One particular challenge is that humans easily follow scores and chord charts, adapt these to new performance plans, and understand media locations in musical terms (beats and measures), while computer music systems often use rigid and even numerical representations that are difficult to work with. We present new formalisms and representations, and a corresponding implementation, where musical material in various media is synchronized, where musicians can quickly alter the performance order by specifying (re-)arrangements of the material, and where interfaces are supported in a natural way by music notation.	chart;computer;numerical analysis;order by	Roger B. Dannenberg;Andrew Russell	2015			programming;simulation;acoustics;computer science;artificial intelligence;multimedia;musicality	HCI	-47.005352305290444	-35.7967822518942	57741
6b58909e523cbbcad6c154f26be7416e68c7fb7d	an intelligent movie production system	rule-based approach;electronic moviemaking;video retrieval;cinematic knowledge representation;virtual director;knowledge representation;rule based;production system;character animation;software systems;three dimensional;motion pictures;user interface	We are developing an intelligent approach of motion picture generation for desktop software system EMM (Electronic MovieMaker) that aims at automating the production of digital movies with various visual effects like three-dimensional animation, real image, and their composition. This paper describes our efforts focusing on automatic synthesis of character animation and automatic video retrieval from video database/Web video library dependent on filmmaking rules. Two kinds of screenplay’ user interface are introduced along with their theory basis of design. Then expounds how to visualize the screenplay by using cinematic ‘rules of thumb’ to make a scene. The implementation of building knowledge representation includes scene’s layout and shooting and character’s action written in CLIPS language.	clips;computer animation;desktop computer;educational entertainment;knowledge base;knowledge representation and reasoning;production system (computer science);software system;user interface;video clip;visual effects	Jinhong Shen;Terumasa Aoki;Hiroshi Yasuda;Seiya Miyazaki	2005			software system;rule-based system;knowledge representation and reasoning;computer graphics (images);multimedia;character animation;user interface;computer science	Graphics	-39.557275084275176	-32.64975615107059	57886
9d6faf6bbb8cd8fe526d9cd94f66b218369b9ff1	designing user interfaces tailored to the current user’s requirements in real time	teoria cognitiva;interfase usuario;user interface management system;user interface;real time;exigence usager;exigencia usuario;base connaissance;besoin utilisateur;necesidad usuario;cognitive theory;satisfiability;theorie cognitive;user assistance;scenario;assistance utilisateur;argumento;user need;user requirement;temps reel;asistencia usuario;script;on the fly;tiempo real;base conocimiento;interface utilisateur;user interface design;knowledge base	Traditional design of user interfaces is based on a perfect knowledge of the user's interaction requirements for the target audience. This approach leads to user interfaces designed for a generic ideal user who doesn't exist at all. As a result, every user or the interface has to adapt his/her own user's interaction requirements to those of this ideal user. In a ideal scenario, there should be as many versions of the user interface as final users. Each of those versions would be designed to satisfy the user's interaction requirements of a single user. Under this approach, we have designed GADEA, a user interface management system able to design different versions of a user interface, on the fly, depending on the cognitive, perceptive and motive skills of each user of the application. This system observes the users as they perform common tasks, analyzing their behavior in order to determine their interaction requirements.		Martin Gonzalez-Rodriguez;Juan Ramón Pérez Pérez;María del Puerto Paule Ruíz	2004		10.1007/978-3-540-27817-7_10	user interface design;look and feel;user;knowledge base;user experience design;simulation;user modeling;computer user satisfaction;interactive systems engineering;interface metaphor;human–computer interaction;user journey;computer science;artificial intelligence;scenario;user requirements document;unique user;user guide;post-wimp;natural user interface;interactivity;user interface;world wide web;graphical user interface testing;user story;satisfiability	Networks	-36.84791712129956	-26.380372778946292	57946
5f894e9bb7098b65bac73bb608dd041f5137f86a	génération de plan de site web pour les non-voyants par des fourmis artificielles	navegacion;site web;artificial ants;invertebrata;vision disorder;swarm intelligence;hipertexto;intelligence en essaim;red www;tree;arthropoda;insecto social;trouble de la vision;accesibilidad;reseau web;minimum capacitated;capacidad memoria;navigation;visually impaired people;web sitemap;capacite memoire;internet;memory capacity;accessibility;aculeata;insecta;graph;world wide web;hymenoptera;insecte social;sitio web;social insect;formicoidea;trastorno vision;inteligencia de enjambre;hypertexte;hypertext;accessibilite;web site	The internet user can meet navigation problems in particular because of the great number of web pages and links. In order to solve this problem, we propose in this article an automatic sitemap generator. The obtained sitemap is both usable by the internet user and webmaster, but it is especially adapted to visual handicapped person or impaired memory capacities. In this paper, we present our method of sitemap generation which uses artificial ants to generate clusters of similar pages, then we use Prim algorithm on each cluster generated by ants.		Sonia Colas;Nicolas Monmarché;Mohamed Slimane	2008	Revue d'Intelligence Artificielle	10.3166/ria.22.137-159	navigation;the internet;hypertext;swarm intelligence;computer science;artificial intelligence;accessibility;artificial ants;tree;graph;world wide web	Crypto	-36.5084116908878	-24.309724748658653	58042
6a7ff0a8852a6278a9b34b69f527ce4ab68801a4	"""the """"authoring on the fly"""" system for automatic presentation recording"""	user interface;information retrieval;browsing;information retrieval interfaces;searching browsing and interacting with multimedia data;searching;automatic presentation recording;authoring on the fly;multimedia data;ubiquitous computing;and interacting with multimedia data;classroom teaching and ubiquitous computing	In the presentation recording scenario there are two aspects regarding the user interface: How the recording is done (i.e., how the presenter or his/her assistants have to interact with the recording tools (hardware and software)), and how users can access the produced multimedia documents (i.e., how replay, as well as search and navigation in the files can be done comfortably and efficiently). In this demonstration, we will illustrate Authoring on the Fly (AOF), a system for presentation recording which provides new approaches and solutions for both of these issues.	on the fly;user interface	Wolfgang Hürst;Gabriela Maass;Rainer Müller;Thomas Ottmann	2001		10.1145/634067.634072	human–computer interaction;computer science;multimedia;user interface;world wide web;ubiquitous computing;information retrieval	HCI	-45.59813390058389	-33.090399743763605	58144
94a9ed103c57a283b4011b77c00296fb59ab0e10	end user mobile task automation using multimodal programming by demonstration		Conversational agents are often used to perform tasks on smartphones, but existing conversational agents are limited in capabilities and lack of customizability. My work explores using the programming-by-demonstration approach to enable end users to program new tasks for conversational agents by demonstrating using the familiar graphical user interfaces of third-party apps. I propose to use a multi-modal (demonstration and verbal instruction) interface to support generalization, editing, error handling as well as creating control structures in creating such smartphone automation.	automation;bespoke;control flow;dialog system;exception handling;graphical user interface;modal logic;multimodal interaction;programming by demonstration;smartphone	Toby Jia-Jun Li	2017	2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)	10.1109/VLHCC.2017.8103491	human–computer interaction;automation;end user;multimedia;programming by demonstration;mobile telephony;computer science;graphical user interface	HCI	-46.45487983741438	-37.66090604709235	58176
46e3e08cbac1b0e30d3f8c57dd3387cac2204408	virtual laboratory for experimental structural dynamics	earthquake engineering;test bed;virtual laboratory;structural dynamics;nonlinear dynamic analysis;experimental structural dynamics;graduate education	This paper presents a Java-Powered Virtual Laboratories (VL) which has been developed to provide a means for on-line interactive structural dynamics experiments for undergraduate and graduate education. This VL intends to provide a conceptual and practical understanding of a wide range of topics related to the collection, analysis, and interpretation of data from dynamic testing, including sensor type and placement, aliasing, windowing, nonlinearities, etc. A multistory shear building is employed as a test bed and the responses of the structure are obtained through the linear/nonlinear dynamic analysis. The VL is available on-line at http://sstl.cee.uiuc.edu/java/esd/VirtualLab.html. This paper presents the unique features and usage of this VL.	aliasing;dynamic testing;experiment;java;nonlinear system;online and offline;placement (eda);programming language;simulation;structural dynamics;testbed	Sung-Han Sim;B. F. Spencer;G. C. Lee	2009	Comp. Applic. in Engineering Education	10.1002/cae.20162	structural dynamics;simulation;earthquake engineering;computer science;engineering;electrical engineering;artificial intelligence;world wide web;testbed;mechanical engineering	SE	-44.16024269767938	-28.592294571731866	58450
aa4e221fadc1d68552875bc022ef8a7b46d789d6	impact of visual cues on climate perception in virtual urban environments: a user study	climate;weather;virtual reality;virtual reality climatology town and country planning;virtual reality experiment visual cue climate perception virtual urban environment user study virtual reality urban projects urban space perception urban space use sky aspect shadows sun location light effects;weather virtual reality urban studies perception climate;urban studies;meteorology visualization sun springs urban areas virtual environments atmosphere;perception	Virtual reality is a good tool to design and assess urban projects and to study perception in cities. Climate perception significantly influences the perception and use of urban spaces, however, virtual urban environments are scarcely represented with different climatic aspects. In this paper, we study the role that visual cues (sky aspect, shadows, sun location, and light effects) specifically play in climate perception (season, daytime and temperature) in virtual urban environments. We present and discuss the data we collected from a recent virtual reality experiment in which ten variations of the climatic context in the same urban space were assessed.	coherence (physics);usability testing;virtual reality	Toinon Vigier;Guillaume Moreau;Daniel Siret	2015	2015 19th International Conference on Information Visualisation	10.1109/iV.2015.89	simulation;geography;remote sensing	Visualization	-35.52639624603594	-36.37486710962859	58584
6cd5072e016f0146892ec79058a7cf9189f5d823	sea of images: a dense sampling approach for rendering large indoor environments	image features;large environments;image based rendering walk through system sea of images approach dense sampling approach computer simulation large indoor environments photorealistic models;walkthroughs;real time;image sampling rendering computer graphics indoor environments solid modeling layout computer vision sea surface computer graphics hardware reflectivity;indoor environment;omnidirectional;digital simulation rendering computer graphics;compression;image based rendering;rendering computer graphics;warping;digital simulation	"""omputer simulation of real-world environments is one of the great challenges of computer graphics. Ultimately, computer simulation technologies should let an untrained operator walk through a city or building with a handheld device that captures a digital model, which can provide the realistic visual experience of walking through the environment interactively. Applications include remote education, virtual heritage, specialist training, electronic commerce, and entertainment. For instance, students can """" visit """" famous historical sites, archeologists can capture excavation sites as they evolve over time, and soldiers can train in simulated environments. The research challenge is to develop methods to capture, represent , and render large environments with photorealistic imagery from arbitrary viewpoints at interactive rates. Navigation in these environments is critical, and, in many cases, the application should convey the environment's scale and duplicate its visual richness. For example, a virtual visit to the Louvre is not particularly interesting if only one room is available, if the user is constrained to move along a preset sequence of viewpoints, or if the sculptures are displayed as synthetic renderings of coarsely detailed 3D models. Current computer graphics techniques fall short of meeting these challenges. (See the """" Related Work in Simulated Environments """" sidebar for a discussion of some current methods.) Our sea of images approach provides new methods for acquiring, analyzing, representing, and rendering photorealistic models of complex indoor environments. Figure 1 gives an overview of our image-based rendering (IBR) walk-through system based on the sea of images approach. In this article, we describe the system and give results for its implementation in three environments of different sizes and types. Although we've described several of the components in conference proceedings, 1–4 this is the first article to provide a beginning-to-end description of the entire system. A sea of images is a collection of images captured every couple of inches in a large environment. The representation provides a densely sampled 4D approximation to the plenoptic function, which describes the radiance leaving or arriving any point (x,y) from any direction (φ,θ). To provide real-time rendering, we address several data-management issues. In particular, we compress the acquired data into a multiresolution hierarchy so users can access it efficiently for continuous sequences of viewpoints , and we use time-critical algorithms to prefetch relevant image data and feature-based warping methods to reconstruct novel images. These techniques enable interactive walk-throughs of large environments with subtle viewpoint-dependent visual effects. …"""	360-degree video;3d modeling;3d reconstruction;alpha compositing;approximation algorithm;computer graphics;computer simulation;e-commerce;emoticon;fiducial marker;image resolution;image-based modeling and rendering;interactivity;mobile device;motion estimation;multiresolution analysis;omnidirectional camera;proxy server;real-time transcription;rendering (computer graphics);requirement;sampling (signal processing);scalability;software deployment;specular highlight;synthetic data;system deployment;virtual heritage;visual effects;window of opportunity	Daniel G. Aliaga;Thomas A. Funkhouser;Dimah Yanovsky;Ingrid Carlbom	2003	IEEE Computer Graphics and Applications	10.1109/MCG.2003.1242379	image warping;omnidirectional antenna;terrain rendering;computer vision;tiled rendering;scientific visualization;2d computer graphics;image-based modeling and rendering;3d rendering;rendering;computer science;parallel rendering;real-time computer graphics;multimedia;real-time rendering;texture memory;computer graphics;alternate frame rendering;volume rendering;compression;feature;software rendering;3d computer graphics;computer graphics (images)	Visualization	-38.37924491740965	-34.90194032673961	58655
4d2221860e45543d0a5003cde0eec17511fb5b1f	mcipa: a music content information player and annotator for discovering music	signal processing	In this paper, we present a new tool for intra-document browsing of musical pieces. This tool is a multimedia player which represents the content of a musical piece visually. Each type of musical content (structure, chords, downbeats/ beats, notes, events) is associated which a distinct visual representation. The user sees what he/ she is listening too. He can also browse inside the music according to the visual content. For this, each type of visual object has a dedicated feedback, either as an audio-feedback or as a playhead feedback. Content information can be extracted automatically from audio (using signal processing algorithms) or annotated by hand by the user. This multimedia player can also be used as an annotator tool guided by the content.	algorithm;audio feedback;browsing;signal processing	Geoffroy Peeters;David Fenech;Xavier Rodet	2008			speech recognition;signal processing;chord (music);multimedia;active listening;computer science	HCI	-45.13572363877888	-33.09311987264518	58869
130b513e20bf067141b8f24bb5a77b19a7f38517	enhancing interface design using attentive interaction design toolkit	attentive interaction;design toolkit;camera based interaction;interactive spaces;interface design;machine vision;interactive environment;user interface design;facial expression;short period;interactive space;interaction design;graphic design	This paper shows how a software toolkit enables graphic designers to make camera-based interactive environments in a short period of time without requiring experience in user interface design or machine vision. The Attentive Interaction Design Toolkit, a vision-based input toolkit, gives users an analysis of faces found in a given image stream, including facial expression, body motion, and attentive activities. This data is fed to a text file that can be easily understood by humans and programs alike. A four-day workshop demonstrated that some Flash-savvy architecture students could construct interactive spaces (e.g. Eat-Eat-Eat, TaiKer-KTV and ScreamMarket) based on a group of people's body and their head motions.	interaction design;machine vision;user interface design	Chia-Hsun Jackie Lee;Jon Wetzel;Ted Selker	2006		10.1145/1179295.1179314	graphic design;user interface design;computer vision;user experience design;machine vision;human–computer interaction;sonic interaction design;computer science;interface design;interaction design;multimedia;facial expression;computer graphics (images)	HCI	-44.66828551092171	-35.35859421320685	59028
a3f980893071eee91550cf83ecbe43114fe6bbbc	labalive - a java toolbox for the simulation of systems	information resources;communication system;animation and computer graphics;interactive systems java information resources inheritance software tools data visualisation computer animation;educational applications;oscillographs labalive java toolbox animated simulations communication systems world wide web user interaction inheritance hierarchy sine wave generator;data visualisation;interactive animation;on line teaching;graphical representation;world wide web;software tools;computer animation;inheritance;user interaction;interactive systems;java animation finite impulse response filter web sites wiring wires visualization displays joining processes layout;java	The labAlive toolbox allows the development of interactive, animated si mulations and demonstrations of communication systems. Students can explore complex learning contents, and since the toolbox is implemented in Java, t he training material can seamlessly be integrated into the World Wide Web. To make the dev lopment of customized wirings as easy as possible, a framework is provided for the simulation task, user interaction and display. An inheritance hierarchy wit h base classes implements these tasks, and a set of ready to use objects is provided, e.g . wires, standard systems like sine-wave generator and oscillographs. Developing a n ew wiring mainly consists of creating the required systems and wires, connecting up an d defining the graphical layout. Special focus has been put on the extendability of th e toolbox. New systems and also their graphical representation can easily be imple ented.	as-easy-as;extensibility;graphical user interface;java;simulation;wiring;world wide web	Erwin Riederer;Rolf Matzner	1999		10.1109/MMCS.1999.778595	telecommunications;computer science;theoretical computer science;operating system;computer animation;programming language;java;world wide web;data visualization;communications system;computer graphics (images)	Graphics	-43.980704751496646	-28.508592483357603	59041
452d9301c849b837f58cdbd55cfa30575c1988b8	dynamic floating window: new creative tool for three-dimensional movies	printing;eye;computing systems;vision;cameras	Unlike the real world, stereoscopic cinemas and display devices have a bordered frame, which can unnaturally cut off our view, and create conflicting visual cues. These perceptual conflicts can diminish the three-dimensional (3-D) effect and cause visual fatigue for the audience. Presented is a method to resolve these issues, by developing a controllable articulated 3-D border. © 2012 SPIE and IS&T. [DOI: 10.1117/1.JEI.21.1.011009]	3d film;cinema 4d;stereographer;stereoscopy	Brian R. Gardner	2012	J. Electronic Imaging	10.1117/1.JEI.21.1.011009	vision;computer vision;computer science;multimedia;computer graphics (images)	HCI	-40.759579904240894	-36.878235515076376	59142
7ac7ad211207b74603ef695a28ba83e3fffa68bc	virtual hairy brush for digital painting and calligraphy	non photorealistic rendering;human computer interaction;digital painting and calligraphy simulation algorithm solid modeling non photorealistic rendering virtual hairy brush;virtual hairy brush digital painting and calligraphy simulation algorithm solid modeling non photorealistic rendering;simulation framework;simulation algorithm;computer graphic;digital painting and calligraphy;solid modeling;virtual hairy brush;market value;article;algorithm design	The design of user friendly and expressive virtuaI brush systems for interactive digital painting and caligraphy has atracted a Iot of attention and efort in both computer graphics and human-computer interaction circles for a Iong time.Providing a digital environment for paper-less artwork creation is not only chalenging in terms of algorithmic design,but also promising for its potential market values.This paper proposes a novel algorithmic framework for interactive digital painting and caligraphy based a noveI virtuaI hairy brush modeI.The algorithms in the kerneI of our simulation framework are built upon solid modeling techniques.Implementing the algorithms,we have developed a virtuaI hairy brush prototype system with which end users can interactively produce high-quality digital paintings and caligraphic artwork.(The latest progress of our virtuaI brush project is reported at the website“htp://www.cs.hku.hk/̂,songhua/e-brush/'’.1	algorithm;computer graphics;digital environment;hairy ball theorem;human–computer interaction;interactivity;prototype;simulation;solid modeling;usability	Songhua Xu;Francis Chi-Moon Lau;Congfu Xu;Yunhe Pan	2005	Science in China Series F: Information Sciences	10.1360/03yf0389	algorithm design;computer science;market value;non-photorealistic rendering;multimedia;solid modeling;algorithm;computer graphics (images)	Graphics	-40.402960667771374	-34.62426136793797	59668
9705fd1d48abf31cb86e69c826939647d63ee46d	high quality-oriented product cooperate design in virtual environments	computer aided design;design automation;history;gestural interface;virtual reality;assembly;feedback;surface deformation;tactile feedback;signal synthesis;auditory feedback;product design;product design virtual environment feedback haptic interfaces virtual reality signal synthesis augmented reality assembly design automation history;virtual environment;augmented reality;haptic interfaces;haptic interaction;cooperative design	The integration of virtual reality and computer-aided design technologies is a revolution in the history of design. Virtual environments provide more information and feedback to designers than traditional desktop systems. Such information and feedback include immersive and stereoscopic visual information, haptic or tactile feedback, and auditory feedback. The multimodal information is in general rather intuitive and inspiring to designers, resulting in globally enhanced insights, creativity and productivity. Moreover, multimodal virtual reality technology excels at the visualization of complex scene and information, which is particularly difficult in desktop systems. In this talk, we present our recent research achievements on: (1) online synthesis of multi-channel visual signals, and the construction of multimodal virtual environments; (2) fusion of graphic, image and video signals in augmented reality environments; (3) synthesis of visual and haptic signals in virtual assembly applications; (4) virtual design and simulation in multimodal virtual environments, including surface deformation based on hand gesture interface, virtual assembly with haptic interactions, virtual maintenance in augmented reality environments; (5) demonstrations and examples.	virtual reality	Jianrong Tan	2010		10.1109/CSCWD.2010.5472014	augmented reality;computer-mediated reality;simulation;human–computer interaction;computer science;virtual machine;instructional simulation;feedback;assembly;virtual reality;mixed reality;multimedia;product design;mechanical engineering	Visualization	-47.94381155400587	-32.98244449690757	59676
431e9fe126b19ed5185b1bae8655aef677933af9	raydiance: a tangible interface for teaching computer vision	limited programming experience;token connection;novel paradigm;computer vision algorithm;prototyping computer vision algorithm;computer vision technique;teaching computer vision;novel light ray metaphor;tangible interface;particular task;physical token;case study	This paper presents a novel paradigm for prototyping Computer Vision algorithms; this paradigm is suitable for students with very limited programming experience. Raydiance includes a tangible user interface controlled by a spatial arrangement of physical tokens which are detected using computer vision techniques. Constructing an algorithm is accomplished by creating a directed graph of token connections. Data is processed, then propagated from one token to another by using a novel Light Ray metaphor. Our case study shows how Raydiance can be used to construct a computer vision algorithm for a particular task. Imagine you are an undergraduate student registered in a Computer Vision class. You need to prototype a multi-step computer vision process for your class project. You have limited experience with programming environments such as Matlab and C++. For each processing step, many algorithms are available through the Matlab Image Processing Toolbox and OpenCV[2]. You need to test all these algorithms in order to make an informed choice. You also need to write the software that integrates all selected algorithms into a computer vision system. Each algorithm typically works with several parameters, thus when the complexity of the computer vision task increases, the combinatorial difficulty of selecting the best algorithms and optimizing their parameters may easily grow out of control. The scenario described above represents a typical bottleneck in project-based undergraduate and even Masters-level Computer Vision classes. This raises the following questions: Can we teach Computer Vision with less emphasis on the low-level programming tasks? Can we teach Computer Vision to students with limited experience in programming? During the last two decades, significant progress has been made in major areas of computer vision, with numerous robust algorithms being developed for image enhancement, segmentation, motion tracking and object recognition. Implementations of such algorithms are available through the MATLAB Image Processing Toolbox and the OpenCV library[2]. However, the task of integrating existing algorithms into a functional system is not trivial, since one needs to program the glue code to link these algorithms. G. Bebis et al. (Eds.): ISVC 2011, Part II, LNCS 6939, pp. 259–269, 2011. c © Springer-Verlag Berlin Heidelberg 2011 260 P. Reimer, A. Branzan Albu, and G. Tzanetakis This paper proposes a new paradigm called Raydiance to assist novice programmers in the design, testing, and visualization of Computer Vision algorithms. Raydiance includes a tangible user interface controlled by a spatial arrangement of physical tokens which are detected using computer vision techniques. Constructing an algorithm is accomplished by creating a directed graph of token connections. Data is processed, then propagated from one token to another by using a novel Light Ray metaphor. We show how Raydiance can be used to construct a computer vision algorithm for a particular task. Raydiance makes use of image processing techniques in OpenCV[2], and libCVD[1]. The remainder of our paper is structured as follows. Section 1 discusses similar approaches and implementations of visual programming interfaces used for rapid software prototyping, and the foundations of tangible computing interfaces using fiducial markers. Section 2 describes the proposed approach for the design of Raydiance. Section 3 presents a case study which consists of a detection task implemented in Raydiance. Section 4 draws conclusions and outlines future work directions.	algorithm;c++;computer vision;coupling (computer programming);directed graph;fiducial marker;glue code;high- and low-level;image editing;image processing;lecture notes in computer science;low-level programming language;matlab;opencv;outline of object recognition;programmer;programming paradigm;prototype;ray (optics);scalability;software prototyping;springer (tank);tangible user interface;visual programming language	Paul Reimer;Alexandra Branzan Albu;George Tzanetakis	2011		10.1007/978-3-642-24031-7_26	computer vision;simulation;computer science;machine learning;multimedia;computer graphics (images)	Vision	-40.13978844223029	-31.72719520221262	59761
33dccffbdbca6d3e87165217a2a48fa833e60244	water, image, gesture and sound: composing and performing an interactive audiovisual work		ABSTRACTPerforming and composing for interactive audiovisual system presents many challenges to the performer. Working with visual, sonic and gestural components requires new skills and new ways of thinking about performance. However, there are few studies that focus on performer experience with interactive systems. We present the work Blue Space for oboe and interactive audiovisual system, highlighting the evolving process of the collaborative development of the work. We consider how musical and technical demands interact in this process, and outline the challenges of performing with interactive systems. Using the development of Blue Space as a self-reflective case study, we examine the role of gestures in interactive audiovisual works and identify new modes of performance.	gesture recognition	Linda Walsh;Andrew Bluff;Andrew Johnston	2017	Digital Creativity	10.1080/14626268.2017.1353524	human–computer interaction;multimedia;visual music;computer science;performing arts;gesture	HCI	-47.33792856390768	-34.095979237623276	59821
1bc82980c0af1c5a662532cdbddb44cf8abb996b	training agents to recognize text by example	programming by example;non-expert user;agents;grammars interactively;parsers;important function;training agents;iterative process;programming by demonstration.;training agent;end user;recognize text;grammex heuristically;direct manipulation interface;concrete example;editing grammar;grammars;unstructured information;interface design	An important function of an agent is to be “on the lookout” for bits of information that are interesting to its user, even if these items appear in the midst of a larger body of unstructured information. But how to tell these agents which patterns are meaningful and what to do with the result? Especially when agents are used to recognize text, they are usually driven by parsers which require input in the form of textual grammar rules. Editing grammars is difficult and error-prone for end users. Grammex [“Grammars by Example”] is the first direct manipulation interface designed to allow non-expert users to define grammars interactively. The user presents concrete examples of text that he or she would like the agent to recognize. Rules are constructed by an iterative process, where Grammex heuristically parses the example, displays a set of hypotheses, and the user critiques the system's suggestions. Actions to take upon recognition are also demonstrated by example.	cognitive dimensions of notations;direct manipulation interface;heuristic;interactivity;iterative method;parsing	Henry Lieberman;Bonnie A. Nardi;David J. Wright	1999	Autonomous Agents and Multi-Agent Systems	10.1023/A:1010018830260	fuzzy logic;natural language processing;computer science;artificial intelligence;interface design;software agent;adaptive behavior;parsing	HCI	-38.49084149219136	-30.52878325394596	59824
6b1d742c63203530794ca631afcc79fbdddfb3cc	real time animated facial expression transfer	face recognition cameras computer animation;performance driven animation expression transfer facial animation facial tracking;partially automated key framing technique real time animated facial expression transfer human face expression computer graphic human perception three stage method realistic facial animation 2d video 3d model joint based system joint based rig real time tracking camera 2d position facial landmark 3d relative movement data;videos face real time systems facial animation three dimensional displays solid modeling;three dimensional displays;solid modeling;facial animation;face;videos;real time systems	Realistic animation of an expressive human face has been a great challenge in computer graphics due to the human perception of human faces. It is a complex, costly and time-consuming process requiring a great detail in many aspects. In this paper, a three-stage method is presented that simplifies the creation of realistic facial animations by transferring the expressions from 2D videos onto 3D models with a joint-based system on a real-time basis. The first stage covers the preparation of the model with a joint-based rig for the transfer. The second includes the real time tracking of a human face with a single camera to obtain 2D positions of facial landmarks. Also it covers the transfer of 3D relative movement data to animate the prepared model by moving the respective joints. The last stage covers the recording of animation using a partially automated key-framing technique. The presented method provides a fast, easy to use and affordable system that produces visually satisfying facial animations.	3d computer graphics;3d modeling;framing (world wide web);real-time transcription	Beste Ekmen;Hazim Kemal Ekenel	2016	2016 24th Signal Processing and Communication Application Conference (SIU)	10.1109/SIU.2016.7495959	computer vision;facial motion capture;computer facial animation;computer science;interactive skeleton-driven simulation;computer animation;multimedia;computer graphics (images)	Graphics	-39.42193684953085	-36.3252425817895	59980
cff33e0d43ebb391db3d08a0b4cf6c7e9c04dec0	hybridspace: integrating 3d freehand input and stereo viewing into traditional desktop applications	single coherent interface hybridspace 3d freehand input stereo viewing desktop applications home environment workplace environment stereoscopic display eye strain 3d spatial interactions 2d tasks user task hybrid interaction technique 2d display 3d display 2d input modality 3d input modality 2d output modality 3d output modality;graphical user interfaces;gestures stereoscopic 2d 3d fish tank vr;visual perception;interactive systems;visual perception graphical user interfaces interactive systems;three dimensional displays mice two dimensional displays stereo image processing accuracy tracking animation	Technologies for 3D input and output are rapidly advancing, and are becoming more common in home and workplace environments. However, viewing a stereoscopic display can cause eye strain and fully relying on 3D spatial interactions can be fatiguing and may be less efficient for common 2D tasks. In this paper we explore the design possibilities of transitioning between 2D and 3D modalities to best support the user's current task. In a formal study, we demonstrate that a Hybrid interaction technique, that transitions between 2D display and input, to 3D, mid-task, outperforms 2D only and 3D only techniques. Guided by this result, we present HybridSpace, a proof-of-concept modeling environment that combines the benefits of 2D and 3D input and output modalities within a single coherent interface.	adobe freehand;coherence (physics);desktop computer;input/output;interaction technique;stereoscopy	Natalia Bogdan;Tovi Grossman;George W. Fitzmaurice	2014	2014 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2014.6798842	computer vision;computer science;multimedia;computer graphics (images)	HCI	-43.30333512019587	-37.59159042402771	60335
b67f8167a53c6af4b42ea4a9cedf8a046eda776b	mrtspace - multi-user 3d environments using vrml			multi-user;vrml	Dieter W. Fellner;Oliver Jucknath	1996			vrml;multi-user;human–computer interaction;computer science	HCI	-47.96993015868169	-31.95346742234902	60560
c7ea6337d85fd8e5ba46378fec4c0588785e9da9	the patchworks code editor: toward faster navigation with less code arranging and fewer navigation mistakes	user study;navigation;integrated development environment ide;code editor	Increasingly, people are faced with navigating large information spaces, and making such navigation efficient is of paramount concern. In this paper, we focus on the problems programmers face in navigating large code bases, and propose a novel code editor, Patchworks, that addresses the problems. In particular, Patchworks leverages two new interface idioms - the patch grid and the ribbon - to help programmers navigate more quickly, make fewer navigation errors, and spend less time arranging their code. To validate Patchworks, we conducted a user study that compared Patchworks to two existing code editors: the traditional file-based editor, Eclipse, and the newer canvas-based editor, Code Bubbles. Our results showed (1) that programmers using Patchworks were able to navigate significantly faster than with Eclipse (and comparably with Code Bubbles), (2) that programmers using Patchworks made significantly fewer navigation errors than with Code Bubbles or Eclipse, and (3) that programmers using Patchworks spent significantly less time arranging their code than with Code Bubbles (and comparably with Eclipse).	eclipse;programmer;source code editor;usability testing	Austin Z. Henley;Scott D. Fleming	2014		10.1145/2556288.2557073	navigation;simulation;human–computer interaction;computer science;operating system;world wide web;computer graphics (images)	HCI	-44.18954982210566	-30.791317204498437	60582
296744de4ddcad8b8280bf2a1ae7b794b02223b4	3d modeling of trees from freehand sketches	interpersonal communication;learning algorithm;user interface;rule based system;underground public art;tangible user interface;three dimensional;3d model;gesture recognition	We will present a user interface for quickly and easily modeling three-dimensional (3D) botanical trees from freehand sketches. The system generates 3D geometry from a two-dimensional (2D) sketch based on the assumption that trees spread their branches uniformly. Several editing operations are implemented, such as adding, cutting, and erasing branches. Our system also predicts, and generates, branches automatically. The user can continue editing a 3D tree by selecting the predicted branches. The interface of our system allows novices to design reasonably natural-looking trees quickly and interactively as compared with the interfaces of rule-based systems designed for expert users (e.g., L-systems).	3d modeling;adobe freehand;interactivity;l-system;logic programming;rule-based system;user interface	Makoto Okabe;Takeo Igarashi	2003		10.1145/965400.965565	three-dimensional space;computer vision;simulation;computer science;artificial intelligence;operating system;machine learning;gesture recognition;multimedia;user interface;interpersonal communication;computer graphics (images)	Graphics	-38.27619871934552	-32.604219500675654	60778
7ff0bb7d0d6db26d50a97b02bfdebb95ffe85828	high level data fusion on a multimodal interactive applications platform	institutional repositories;human computer interaction;fedora;multimodal fusion;software prototyping;human computer interaction data fusion multimodal interactive application rapid development rapid prototyping user centered design reusable components;speech;user centered design;natural languages;data fusion;joints;vital;feature extraction;software reusability;speech analysis signal processing humans merging robustness remote sensing prototypes user centered design speech recognition image analysis;merging;multimodal interaction;robustness;user centred design;tool integration;humans;sensor fusion;vtls;reusable component;interactive systems;user centred design human computer interaction interactive systems sensor fusion software prototyping software reusability;ils	We demonstrate a multimodal high-level data fusion tooling integrated on a platform aimed at the rapid development and prototyping of multimodal applications through user-centered design. The platform embeds a set of pure and combined modalities as reusable components and generic mechanisms for combining modalities with a rich support for multimodal fusion in order to improve the human-computer interaction.	high- and low-level;human–computer interaction;multimodal interaction;oracle fusion architecture;user-centered design	Olga Vybornova;Hildeberto Mendonça Filho;Jean-Yves Lionel Lawson;Benoit M. Macq	2008	2008 Tenth IEEE International Symposium on Multimedia	10.1109/ISM.2008.21	computer vision;simulation;human–computer interaction;computer science;machine learning;sensor fusion	Embedded	-45.79052499152868	-37.16138738864362	61022
26a0da832857877c90d436ffd76271c204833592	the integration of subjective and objective data in the animation of human movement	human movement;labanotation;animation;movement abnormalities;movement notation;dance notation	Animation of human movement can be based either on analog inputs derived directly from actual movements or on symbolic inputs chosen to produce the desired movement. The former type of input can be quite accurate and objective but is a description of the required movement whereas the latter is often quite imprecise and subjective but provides an analysis of the required movements. Two existing systems for a computer based animation are being used to explore the problems involved in integrating such inputs. Specifically, animation driven by analog signals from electro-goniometers is integrated with animation derived from Labanotation commands; the results are illustrated with a short movie.	analog signal	Thomas W. Calvert;John Chapman;Aftab E. Patla	1980		10.1145/800250.807492	anime;dance notation;computer vision;simulation;computer facial animation;skeletal animation;computer science	Graphics	-39.89526979800501	-36.900454452662046	61134
9a17f11695ad34e491496c46634e6bb9d8581b84	designspace: a manual interaction environment for computer-aided design	cad;teleconference;presence;manual and gestural communication;collaboration.;spatial acoustics;inter- active simulation;dexterous manipulation;manual interaction environment;virtual environment;design process;conceptual design;mechanism design;design research;pointing device;design theory;computer aided design	DesignSpace is a computer-aided-design (CAD) system that facilitates dexterous manipulation of mechanical design representations. The system consists of an interactive simulation programmed with a seamless extended model of the designer’s physical environment and driven with continuous instrumentation of the designer’s physical actions. The simulation displays consistent visual and aurat images of the virtuat environment without occluding the designer’s sensation of the physical surroundings. Developed at Stanford University’s Center for Design Research (CDR), DesignSpace serves as an experimental testbed for design theory and methodology research. DesignSpace includes significant contributions from recent CDR development projects: TalkingGlove, CutPlane, VirtualHand, TeleSign, and VirtualGrasp. The current DesignSpace prototype provides modeling facility for only crude conceptual design and assembly, but can network multiple systems to share a common virtual space and arbitrate the collaborative interaction. The DesignSpace prototype employs three head-tracked rear projection images, head-coupled binaural audio, hand instrumentation, and electromagnetic position tracking.	computer-aided design	William L. Chapin;Timothy A. Lacey;Larry J. Leifer	1994		10.1145/259963.260018		EDA	-43.46939680973224	-37.04140563006785	61334
6df869b4a8b424cac20aeb18ba187fb497b80ee5	art directed rendering & shading using control images	gaze;alternative splicing;art;graphic novel;branching;virtual reality;attention;cave2;belief states;interactive;storytelling;fixation;digital humanities;eye tracking;interactive installation	In this work, we present a simple mathematical approach to art directed shader development. We have tested this approach over two semesters in an introductory level graduate rendering & shading class at Texas A&M University. The students in the class each chose an artist's style to mimic, and then easily created rendered images strongly resembling that style (see Figures 1). The method provides shader developers an intuitive process, giving them a high level of visual control in the creation of stylized depictions.	high-level programming language;shader;shading	Ergun Akleman;Siran Liu;Donald H. House	2015		10.1145/2785585.2792524	fixation;computer vision;digital humanities;attention;eye tracking;branching;computer science;artificial intelligence;alternative splicing;virtual reality;multimedia;interactivity;computer graphics (images)	Vision	-39.71983592728005	-32.195711510767836	61447
056b55750ea7ea2cbf8d0c9c3a276ecced4b0e64	a novel way to study muscle anatomy of the beef animal		In an academic and industrial setting, it is difficult to teach the anatomy of a beef animal. It has required a beef carcass fabricated into wholesale and retail cuts or dissection of individual muscles. This could only happen in a laboratory, and substantial cost would be incurred for each lab session. Books or manuals can assist somewhat but these are only twodimensional in nature, making it difficult to understand some of the spatial relationships between muscles. It is now possible to use a web site (http://bovine.unl.edu) as a resource for the muscular anatomy of the beef animal. The site helps users understand bovine muscular and skeletal anatomy through the use of interactive 3D and 2D graphics simulations, pictures, drawings, and navigation through a series of well-defined information modules. The sections below describe the procedures and methodology followed to develop the content, material, and online infrastructure.	2d computer graphics;book;simulation	Vishal Singh;Ashu Guru;Bucky L. Gwartney;Steven J. Jones	2004		10.1145/1186107.1186134	computer vision;computer graphics (images);artificial intelligence;computer science	HCI	-39.1230208953422	-33.68399029988144	61550
6c29948024b9cbe130d7f070768031e11ee43119	creativity enhancement of painterly rendering using a suggestive interface		Abstract Non-photorealistic rendering (NPR) can use various parameters and techniques to automatically generate a wide range of images with different painterly appearances. However, a real painting is the result of a creative process, and traditionally, many artists have developed their own media, styles, and techniques. Fortunately, the computer can also be used in different ways, for instance, as a tool to support and enhance creativity. We propose a system to interactively generate a painterly image through a suggestive interface. After the artist intuitively selects a region, our system immediately offers different suggestions of NPR modifications. The multidimensional NPR parameter space is transformed in a more perceptual space by optimization, which is validated by a user study. The suggested images generated from sampled parametric variations are displayed according to our perceptual distance. Through the same interface, our system allows for refinement or exploration without any need to understand the various parameters involved in generating NPR effects. The interest of our suggestive interface is supported by another user study, where participants felt that it stimulated their creativity as they interactively and iteratively created painterly images by choosing suggested images for the selected regions. Another experiment showed that different viewers found the resulting images more creative than their initial automatically generated images.		Shuhei Kodama;Pierre Poulin;Tomoaki Moriya;Tokiichiro Takahashi	2018	Computers & Graphics	10.1016/j.cag.2017.11.001	rendering (computer graphics);artificial intelligence;computer vision;parameter space;computer science;perception;computer graphics (images);parametric statistics;non-photorealistic rendering;creativity	Visualization	-38.408397952293925	-35.10171746406159	61579
8b99a2ed652f9eb3af28b039149830e3524fc56b	procedural approach to volumetric terrain generation	procedural generation;terrain modeling;volumetric terrain;virtual worlds	The recent outbreak of indie games has popularized volumetric terrains to a new level, although video games have used them for decades. These terrains contain geological data, such as materials or cave systems. To improve the exploration experience and due to the large amount of data needed to construct volumetric terrains, industry uses procedural methods to generate them. However, they use their own methods, which are focused on their specific problem domains, lacking customization features. Besides, the evaluation of the procedural terrain generators remains an open issue in this field since no standard metrics have been established yet. In this paper, we propose a new approach to procedural volumetric terrains. It generates completely customizable volumetric terrains with layered materials and other features (e.g., mineral veins, underground caves, material mixtures and underground material flow). The method allows the designer to specify the characteristics of the terrain using intuitive parameters. Additionally, it uses a specific representation for the terrain based on stacked material structures, reducing memory requirements. To overcome the problem in the evaluation of the generators, we propose a new set of metrics for the generated content.	material flow;problem domain;procedural programming;requirement;scenery generator;volumetric display	Aitor Santamaría-Ibirika;Xabier Cantero;Mikel Salazar;Jaime Devesa;Igor Santos;Sergio Huerta;Pablo García Bringas	2013	The Visual Computer	10.1007/s00371-013-0909-y	computer vision;simulation;computer science;metaverse;multimedia	Robotics	-37.55312644846881	-33.15129072678218	61641
569296aa226408165fe3b291ed76a5669260722b	the correlation between the ability to read and manually reproduce a 3d image: some implications for 3d information visualisation	individual 3d drawing ability;3d image;object recognition;art;mrt;pediatrics;psychology experiment 3d image 3d information visualisation object recognition computer graphics individual 3d drawing ability;3d imaging;computer graphics;training;construction industry;information visualization;object recognition art data visualisation;chiaroscuro;computer graphic;conference paper;data visualisation;visualization;three dimensional displays;3d information visualisation;analysis of variance;psychology experiment;spatial ability;perspective drawing 3d images mrt 3d information visualisation chiaroscuro;correlation;3d images;mental rotation;article;perspective drawing;information visualisation	Most of us can recognize common 3D objects depicted in drawings, photographs and computer graphics. But, few of us are able to manually reproduce them in a convincing manner. This paper discusses a psychology experiment that investigates the variability in individual drawing ability and the ability to read 3D images. Currently, Shepard and Metzler’s Mental Rotation Test is the most popular test for spatial ability. This paper discusses the need to further investigate the correlation of 3D drawing ability and recognition and its potential effect on the legibility of the 3D information visualization application. This paper reports ongoing research in this field.	computer graphics;information visualization;spatial variability	Theodor G. Wyeld	2009	2009 13th International Conference Information Visualisation	10.1109/IV.2009.79	computer vision;computer science;multimedia;computer graphics (images)	HCI	-36.699658616855785	-36.12297057125489	61715
58a10e46094c9c4a581cd3d9e6fd5b16f2e07ab3	experimence: considerations for composing a rock song for interactive audience participation		In popular music genres, typical songs are pre-composed and leave little or no space for improvisation during a live performance. That applies for the performing musicians as well as for the spectators in terms of interactive audience participation. In this study we question these improvisational limits and try to identify strategies for involving the audience as an additional and unpredictable factor in a pre-composed rock song. To do so we composed “Experimence” guided by the standard practice of song writing. The song was premiered at a public live concert where the audience could collaboratively participate in real-time by playing with a balloon together throughout the song. Using a wizard of oz technique, the movements of the balloon influenced the live music played by the pianist. We reflect across this experience and present notable issues raised during the composition, rehearsals and the actual performance. We then classify these aspects as abstract variables of consideration for a composition meant to promote such audience participation. We propose this proof of concept as a starting point for further discussion, suggesting that a song such as Experimence can be a unique and individual piece of music every time it is played although largely pre-composed.	interactivity;real-time computing;real-time transcription;wizard (software)	Oliver Hödl;Geraldine Fitzpatrick;Simon D Holland	2014			humanities;art;multimedia;communication	HCI	-48.20979089785283	-35.09617897824788	61814
4f416d40ecb3534ba29a0aaf11c007fd425529de	mobile melody recognition system with voice-only user interface.	user interface;mobile phone;speech recognition	A melody recognition system with a voice-only user interface is presented in this paper. By integrating speech recognition and melody recognition technology we have built an end-to-end melody retrieval system that allows a users to do voice controlled melodic queries and melody generation using a dial-in service with a mobile phone.	end-to-end principle;mobile phone;speech recognition;user interface	Timo Sorsa;Katriina Halonen	2002			user;10-foot user interface;speech recognition;computer science;multimedia;natural user interface;user interface	Mobile	-46.69387681812731	-34.464947824921985	61818
8d82a8234a76df80b490884e7bb0611382eb684b	talking about data: sharing richly structured information through blogs and wikis	data sharing;linked data;visualization;data semantics;semantic web;wikis;article;blogs;authoring tool	The web has dramatically enhanced people's ability to communicate ideas, knowledge, and opinions. But the authoring tools that most people understand, blogs and wikis, primarily guide users toward authoring text. In this work, we show that substantial gains in expressivity and communication would accrue if people could easily share richly structured information in meaningful visualizations. We then describe several extensions we have created for blogs and wikis that enable users to publish, share, and aggregate such structured information using the same workflows they apply to text. In particular, we aim to preserve those attributes that make blogs and wikis so effective: one-click access to the information, one-click publishing of content, natural authoring interfaces, and the ability to easily copy-and-paste information and visualizations from other sources.	1-click;aggregate data;blog;cut, copy, and paste;wiki;world wide web	Edward Benson;Adam Marcus;Fabian Howahl;David R. Karger	2010		10.1145/1772690.1772802	visualization;many-to-many;computer science;semantic web;linked data;multimedia;world wide web;information retrieval	HCI	-41.849294905921205	-25.26366125444164	61979
25c2ff63a3ab4811554e1c4a061c24cc29e12fea	a haptic interaction method for volume visualization	user interfaces;phantom haptic interface;accelerated ray casting method;haptic feedback;haptic interaction method;interactive visual feedback rates;isosurface rendering;modeling applications;point contact forces;three-dimensional filters;virtual objects;visual data exploration;visual feedback;volume rendering;volume visualization	Volume visualization techniques typically provide support for visual exploration of data, however additional information can be conveyed by allowing a user to see as well as feel virtual objects. We present a haptic interaction method that is suitable for both volume visualization and modeling applications. Point contact forces are computed directly from the volume data and are consistent with the isosurface and volume rendering methods, providing a strong correspondence between visual and haptic feedback. Virtual tools are simulated by applying three-dimensional filters to some properties of the data within the extent of the tool, and interactive visual feedback rates are obtained by using an accelerated ray casting method. This haptic interaction method was implemented using a PHANToM haptic interface.	haptic technology;isosurface;ray casting;scientific visualization;volume rendering	Ricardo S. Avila;Lisa M. Sobierajski	1996	Proceedings of Seventh Annual IEEE Visualization '96		computer vision;computer science;parallel rendering;multimedia;computer graphics (images)	Visualization	-41.68297421265084	-36.4042301452214	62212
43619d6d0f79e42c7d7493dd07b343e424bbcd37	bayesphone: precomputation of context-sensitive policies for inquiry and action in mobile devices	sensibilidad contexto;bayes estimation;modelizacion;distributed system;systeme reparti;context aware;informatique mobile;mobile device;systeme aide decision;telephone portable;sistema ayuda decision;prise decision;probabilistic approach;mobile phone;modelisation;estimacion bayes;decision support system;telefono movil;sistema repartido;enfoque probabilista;approche probabiliste;comportement utilisateur;inferencia;user behavior;sensibilite contexte;mobile computing;toma decision;modeling;inference;comportamiento usuario;user model;estimation bayes	Inference and decision making with probabilistic user models may be infeasible on portable devices such as cell phones. We highlight the opportunity for storing and using precomputed inferences about ideal actions for future situations, based on offline learning and reasoning with the user models. As a motivating example, we focus on the use precomputation of call-handling policies for cell phones. The methods hinge on the learning of Bayesian user models for predicting whether users will attend meetings on their calendar and the cost of being interrupted by incoming calls should a meeting be attended.	adaptive behavior;bayesian network;computation;interrupt;mobile device;mobile phone;offline learning;online and offline;personalization;precomputation;prototype;statistical model;dialog	Eric Horvitz;Paul Koch;Raman Sarin;Johnson Apacible;Muru Subramani	2005		10.1007/11527886_33	simulation;user modeling;systems modeling;decision support system;computer science;artificial intelligence;operating system;mobile device;mobile computing;computer security	HCI	-35.482984155642164	-25.483558362065736	62276
9336c813a72b66220b248c39a84fc7541b5a55f5	inventions on displaying and resizing windows	software;patent analysis;bepress selected works;inventions;triz;gui;triz software inventions graphical user interface software inventions software patents inventive problem solving gui windows resizing window border window scrolling window resizing window selection window positioning window size	Windows are used quite frequently in a GUI environment. The greatest advantage of using windows is that each window creates a virtual screen space. Hence, although the physical screen space is limited to a few inches, use of windows can create unlimited screen space to display innumerable items. The use of windows facilitates the user to open and interact with multiple programs or documents simultaneously in different windows. Sometimes a single program may also open multiple windows to display various items. The user can resize the windows and move their location time to time as desired. However, there are several concerns of a window relating to its size, appearance, positioning, color, visibility, resizability etc. The following are some of the issues faced while using windows. ⇛ There is a concern regarding the size of windows, i.e., whether the window size should be allowed to be bigger than the physical screen size. If the window size cannot be bigger than screen size, then it cannot display larger items. But if the windows are expanded bigger than the screen size, then its borders may go beyond the screen and become inaccessible. ⇛ If we limit the size of the windows then there are issues like, what should be the maximum size of windows, how do the user know that the window cannot expand further, whether the size of one window should affect the size of other windows? ⇛ Resizing windows should not go beyond a minimum size, as otherwise, the user cannot know the content of the window. ⇛ Conventionally the user has to move the mouse pointer to the corner of the window to drag or resize the window. This is felt to be a stressful process when repeated again and again. ⇛ Holding the mouse pointer exactly on the window border or corner is felt to be difficult especially if you have a shaky hand, a fast pointer speed or a narrow border. In such cases there is possibility of clicking the mouse when the pointer is moved out of the window, which can cause unpredictable results. ⇛ When there are multiple windows, some windows partially or fully block other windows and hinder their visibility. ⇛ When a window is completely covered by other windows, it is difficult to select that window, as no part of the window is visible for the user to click and select. ⇛ Sometimes the …	display size;glossary of computer graphics;graphical user interface;microsoft windows;pointer (computer programming);pointer (user interface);virtual desktop;window function	Umakant Mishra	2014	CoRR	10.2139/ssrn.1264693	directx video acceleration;desktop window manager;title bar;wallpaper;engineering;z-order;commit charge;compositing window manager;world wide web;engineering drawing;computer graphics (images)	HCI	-41.25161526050572	-32.002408966954576	62506
e2c7a33e7cb304f8f504db1fc0d7b8d04aeafdf4	a web interface for a sound database and processing system	arte	The World Wide Web has brought the possibility of distributing multimedia objects through the Internet, becoming a very interesting platform for the development of innovative music and audio applications. Apart from the explosion of commercial plug-ins and compression techniques aimed at real-time transfer of music and audio data, there are some projects that go beyond that by attempting to use Internet as a musical production environment. In this article we will discuss our view on the technological requirements for an ideal Web-based studio and as a particular application we will show a Web front-end to the Spectral Modeling Synthesis (SMS) system.	deployment environment;esther drummond;internet;plug-in (computing);production system (computer science);real-time cmix;requirement;spectral modeling synthesis;way to go;world wide web	Ramon Loureiro;Xavier Serra	1997			human–computer interaction;computer science;database;world wide web	DB	-45.07530114038843	-32.315172020705816	62762
1476805f957a26026fb07a3c215965932600f78a	the integration of a virtual environment and 3d modeling tools in a networked robot system	robotics;image based modeling;3d model;electronic and computer engineering;telerobotics;virtual environment	This paper presents interfacing techniques to integrate a virtual environment (VE), computer graphics, image-based modelling, and other technologies in a networked system for robot control. Technologies such as networked robotics have advanced rapidly in the past decade, bringing a physical aspect to the usage of the Internet. Various applications of Internet telerobotics have been investigated and a variety of techniques have been proposed to increase the control robustness and efficiency of such systems. In this paper, we introduce a teleoperated robot manipulator that uses VE and other supporting technologies as a human-system interface to suppress the variable communication latency of the Internet. The paper focuses on the control techniques and the system structure that supports the implementations of the techniques.	3d modeling;computer graphics;internet;robot control;robotics;telerobotics;virtual reality	JiaCheng Tan;Gordon Clapworthy;Igor R. Belousov	2006	Int. J. Image Graphics	10.1142/S0219467806002136	telerobotics;embedded system;computer vision;simulation;computer science;virtual machine;robotics	Robotics	-40.46743176330099	-37.44494892681487	62800
40dd9cf02f11e9a79dde2778761684199d75d52f	exploring uddi registries using modified ofdav browser	conference proceeding	This paper introduces a framework of interactive navigating through the web-based UDDI Business Registries. By creating a 2D interactive visual interface, service requesters are able to visually navigate through the large information hierarchies that are usually used to present the UDDI Business Registries. Instead of using the traditional keywords or click-through searching methodologies, which are often inefficient and less obvious, we use a visualization technique, namely Online Force-Directed Animated Visualization (OFDA VJ, to create our visual interface. A simplified prototype with visual interface has been developed for the purpose of demonstration.	prototype;viewtiful joe;web services discovery;web application	Biao Jiang;Mao Lin Huang	2003			computer science	HCI	-42.30142872006842	-24.648302123847824	63149
3682ed1bd4602b1c69541ef720996bee6e9b1ccb	reality portals	video textures;augmented virtuality;collborative virtual environments;environment visualization;teleoperation	Through interactive augmented virtuality we provide the ability to interactively explore a remote space inside a virtual environment. This paper presents a tool and technique that can be used to create such virtual worlds that are augmented by video textures taken of real world objects. The system constructs and updates, in near real-time, a representation of the user-defined salient and relevant features of the real world. This technique has the advantage of constructing a virtual world that contains the relevant video-data of the real world, while maintaining the flexibility of a virtual world. The virtual-real world representation is not dependent on physical location and can be manipulated in a way not subject to the temporal, spatial, and physical constraints found in the real world. Another advantage is that spatializing the video-data may afford more intuitive examination.	interactivity;mixed reality;portals;real-time computing;real-time locating system;virtual reality;virtual world;virtuality (gaming)	Karl-Petter Åkesson;Kristian T. Simsarian	1999		10.1145/323663.323665	computer vision;augmented reality;teleoperation;simulation;computer science;mixed reality;multimedia;augmented virtuality	HCI	-41.84952869325303	-36.66389849083092	63184
edfc90cef4872faa942136c5f824bbe4c6839c57	improvasher: a real-time mashup system for live musical input		In this paper we present Improvasher a real-time musical accompaniment system which creates an automatic mashup to accompany live musical input. Improvasher is built around two music processing modules, the first, a performance following technique, makes beat-synchronous predictions of chroma features from a live musical input. The second, a music mashup system, determines the compatibility between beat-synchronous chromagrams from different pieces of music. Through the combination of these two techniques, a real-time predictive mashup can be generated towards a new form of automatic accompaniment for interactive musical performance.	linear predictive coding;mashup (web application hybrid);real-time clock;real-time locating system;real-time transcription	Matthew E. P. Davies;Adam M. Stark;Fabien Gouyon;Masataka Goto	2014			musical;human–computer interaction;multimedia;mashup;computer science	AI	-44.778772701025176	-33.39944738987963	63209
8c1794e241e63519fa2a08bb4b7319a6bbf9a25d	simulation of visual assessment for the given deployment of graphical user interface elements		We observe a constant growth of methods and models enabling automated generation of graphical data, derived either from physical world or taken from simulations. Good information design is crucial for humancomputer interface, improving productivity and enhancing human understanding. This paper proposes two novel algorithms to simulate the visual assessment of deployment of GUI elements. GUI elements are treated as the uniform rectangular blocks on the layout. Those blocks eventually are replaced by the system dependent object representations, e.g. visual metaphors of the heating system. The results returned by the algorithms were compared with results obtained by a survey. The proposed metrics can be used in visual evaluation of various simulation models, as long as they consist of logically separable elements. The paper discusses the theoretical background, the properties of proposed algorithms alongside with the sample application prototype outputs.	algorithm;graphical user interface;information design;prototype;simulation;software deployment	Daniel Skiera;Mark Hoenig;Juergen Hoetzel;Pawel Dabrowski;Slawomir Nikiel	2012		10.7148/2012-0216-0221	10-foot user interface;simulation;shell;human–computer interaction;natural user interface;user interface;world wide web;graphical user interface testing;multiple document interface	HCI	-42.86057038900182	-30.028836417608154	63348
0b8de861897c128baeb6ce4be7e930252808c832	a computational model of skilled use of a graphical user interface	action slips;user models;display based problem solving;computer model;graphical user interfaces;graphic user interface;problem solving;user model	This paper describes a computational model of skilled use of a graphical user interface based on Kintsch's construction-integration theory [4, 8]. The model uses knowledge of a detailed representation of information on the display, a user's goals and expectations, knowledge about the interface, and knowledge about the application domain to compute actions necessary to accomplish the user's current goal. The model provides a well-motivated account of one kind of errors, action slips [14], made by skilled users. We show how information about the intermediate state of a task on the display plays a critical role in skilled performance, i.e., display-based problem solving.	application domain;computation;computational model;experiment;graphical user interface;problem solving;sampling (signal processing);simulation	Muneo Kitajima;Peter G. Polson	1992		10.1145/142750.142803	user interface design;computer simulation;user;simulation;user modeling;interface metaphor;shell;human–computer interaction;computer science;knowledge management;operating system;graphical user interface;natural user interface;user interface;graphical user interface testing	AI	-36.96267388399329	-28.184614519562217	63477
6e638ae1ec2945bd450ec6df6af04a24e2f5eab3	isymphony: an adaptive interactive orchestral conducting system for digital audio and video streams	video streaming;interaction style;music interfaces;gestures;video recording;conducting;adaptive gesture recognition;exhibits;gesture recognition	We present iSymphony, an interactive orchestral conducting system for digital audio and video that adaptively adjusts to the user's conducting style. Using a digital baton, users may control the tempo, volume, and instrument emphasis of a digital audio and video recording of an orchestra. The system adaptively recognizes three gesture profiles: the four-beat neutral-legato pattern, an up-down pattern, and random gestures. The system uses an audio time-stretching algorithm we developed that allows the playback speed of a digital audio recording to be arbitrarily adjusted without changing its pitch. iSymphony is an example of how computers can enable more people to experience an interaction style normally limited to a few people (conductors), and is installed as part of the It's Artastic! exhibit at the Betty Brinn Children's Museum in Milwaukee, USA.	algorithm;baton;computer;digital recording;pitch (music);streaming media;video	Eric Lee;Henning Kiel;Saskia Dedenbach;Ingo Grüll;Thorsten Karrer;Marius Wolf;Jan O. Borchers	2006		10.1145/1125451.1125507	speech recognition;digital audio;computer science;gesture recognition;conductor;multimedia;gesture;world wide web;computer graphics (images)	HCI	-44.43959066553213	-35.54077206862309	63626
e79340dce83359b82ee251b8026a08330ac81f5c	model-driven generation of collaborative virtual environments for cultural heritage		Collaborative Virtual Environments are experiencing a large interest in cultural heritage field mostly due to the strong opportunity given by novel augmented reality applications. There are already several examples of collaborative augmented visit to museums or historical sites. Anyway the traditional and static approach to computer graphics is very limiting because it often requires the development of a Virtual Environment for every new application. In this paper, we propose a technique for model-driven generation of mixed reality virtual environments, where every modification in contents, visit path and in interactions with the physical surrounding environment don't require a great re-coding effort, enabling fast deployment of collaborative virtual environments for cultural explorations only providing new contents and a small set of parameters.	collaborative virtual environment;model-driven integration	Alberto Bucciero;Luca Mainetti	2013		10.1007/978-3-642-41190-8_29	collaborative virtual environment;mixed reality;human–computer interaction;conceptual design;multimedia;computer science;virtual machine;computer graphics;cultural heritage;augmented reality;instructional simulation	HPC	-40.07313701361732	-33.14335126776649	63721
ec42a1bb50590deaa5c16fa163acd899c97b2033	realizing the compute potential of the mobile gpu	graphics processors;miscellaneous;portable devices	Computing is evolving as smartphones and tablets increasingly become primary entertainment devices. This shift requires greater performance from mobile processors to deliver the same quality experiences that a PC or gaming console does, but without compromising battery life in a more compact mobile device form factor.  The mobile GPU is offers tremendous computing potential to enable unprecedented experiences from vision for augmented reality, to game physics, geometry creation and image processing. This panel, composed of leading mobile graphics experts from Qualcomm, ARM, Google, Intel, Lytro and Pelican Imaging will cover the newest and best ways advanced programmers can take advantage of the massive computing power of today's mobile GPUs. The discussion will cover the GPU and system architectures, as well as compute APIs like OpenCL and Renderscript to enable GPGPU. The panel will address the opportunities and challenges of GPU compute, the architecture required to support it, and their vision for where it will take us.  Designed for advanced programmers, as well as thought leaders in the industry, attendees will leave this session with a clear understanding of the latest developments in mobile GPU compute. Beyond what is possible today, industry experts on this panel will explore where mobile graphics are headed and discuss their companies' respective visions for the future of mobile compute.  Attendees will have an opportunity at the end of the panel to pose questions.	arm architecture;application programming interface;augmented reality;central processing unit;experience;game physics;general-purpose computing on graphics processing units;graphics processing unit;image processing;mobile device;opencl api;programmer;renderscript;smartphone;tablet computer	Patrick Moorhead;Kurt Akeley;Jason Sams;David Blythe;Dave Shreiner;Eric Demers;Kartik Venkataraman	2014		10.1145/2614208.2615553	embedded system;computer vision;real-time computing;mobile search;simulation;computer science;artificial intelligence;operating system;mobile technology;computer graphics (images)	Arch	-48.19421467441833	-30.13418801700049	63740
b716e7d2152abcaafca675d99c9a78de5b1a7fc7	generation of test samples for construction of dashboard design guidelines: impact of color on layout balance		The metric-based evaluation of user interfaces is a promising way to quickly evaluate their usability and other various design aspects. However, development of such metrics usually requires a sufficiently large training set of realistic-looking user interface samples, which might not be always easy to find. This paper describes a workflow of the preparation of such samples. It presents a configurable generator based on the composition of simple widgets into a screen according to a predefined model. It also describes a reusable library for simple creation of widgets using capabilities of the JavaScript framework Vue.js. The application of the implemented generator is then demonstrated on the generation of dashboard samples which are used to show the significance of color in the measuring of the layout balance.	dashboard	Olena Pastushenko;Jiri Hynek;Tomás Hruska	2018		10.1007/978-3-319-77712-2_93	database;dashboard (business);javascript;usability;workflow;training set;user interface;computer science	EDA	-40.701545996349395	-31.03087585978965	63773
2be36c049b3eaba29e07b70a4c615938547eb2e1	sonic visualiser: an open source application for viewing, analysing, and annotating music audio files	user interface;visualisation;audio analysis;open source;recorded music	"""Sonic Visualiser is a friendly and flexible end-user desktop application for analysis, visualisation, and annotation of music audio files. Its stated goal is to be """"the first program you reach for when want to study a musical recording rather than simply listen to it"""". To this end, it has a user interface that resembles familiar audio editing applications, a set of useful standard visualisation facilities, and support for a plugin format for additional automated analysis methods."""	audio engineer;desktop computer;open-source software;plug-in (computing);sonic visualiser;user interface	Chris Cannam;Christian Landone;Mark B. Sandler	2010		10.1145/1873951.1874248	speech recognition;computer science;operating system;multimedia;user interface;world wide web;audio analyzer	HCI	-44.72815137261631	-32.48767436171039	63819
c19cb09dd6fa1d2a5414c85ad2228dc3a991538c	pegasus: a simulation tool to support design of progression games		The process of designing a game involves many phases. We can summarize the work of the game designer as satisfactorily converting the idea in their mind to a digital game, which is not a simple task. Therefore, game designers should have a variety of tools to assist them. However, there are not that many specialized tools to support the game design process. Herein, we describe the experience of using Pegasus to design a part of a game. We propose an environment to simulate progression games based on game design patterns. Thus, we described the interaction of the game designer with Pegasus in such an environment, in order to support the process of creating, testing, and refining game elements before proceeding to the programming phase. Each configuration of the game elements corresponded to a simulation that could be performed multiple times, like in discrete event simulation. The results showed that Pegasus has the potential to support game design. Additionally, we presented some support components that were created to facilitate the use of the tool.		Marcelo A. R. da Silva;Geraldo Xexéo	2018	Int. J. Computer Games Technology	10.1155/2018/9341032	discrete event simulation;human–computer interaction;video game design;simulation;computer science;game design	EDA	-42.68679665402272	-31.66489614373182	64065
7b42c56686116c14c30c1b58b364134bb48036d0	an enactive approach to facilitate interactive machine learning for co-creative agents	cognitive science;computational creativity;collaboration;human computation;creativity support tools	This paper introduces a novel approach to developing co-creative agents that collaborate in real time creative contexts, such as art and pretend play. Our approach builds upon recent work in computational creativity called interactive machine learning (IML). In IML, agents learn through demonstration, interaction, and real time feedback from a human user (as opposed to offline training). To apply IML to open-ended creative collaboration, we developed an enactive model of creativity (EMC) based upon the cognitive science theories of enaction. This paper introduces our enactive approach to building co-creative agents within the broader field of interactive machine learning by describing the theory, design, and initial prototypes of two co-creative agents.	cognitive science;computational creativity;enactive interfaces;enactivism;machine learning;nonlinear gameplay;online and offline	Nicholas Davis	2015		10.1145/2757226.2764773	computational creativity;simulation;human–computer interaction;computer science;artificial intelligence;management;collaboration	AI	-48.21565783931657	-34.260415415205586	64175
87d318ed4ad51160feee5114a58dff493fe470de	a sat based scheduler for tournament schedules	scheduling problem;sat solver;search engine;web interface	We present a scheduler with a web interface for generating fair game schedules of a tournament. The tournament can be either single or double round-robin or something in between. The search engine inside the scheduler is a SAT solver which can handle a mix of ordinary and TL (True-Literal) clauses. The latter are the formulas using the function TL which counts the number of true literals in a clause. By using TL clauses, we could solve a typical scheduling problem in a few of seconds. If we convert them into ordinary clauses, the state-of-the-art SAT solvers could not solve them in one week. We showed how to integrate TL clauses into a SAT solver and take advantages of the advanced SAT techniques. Our scheduler provides a free service to all the people who are interested in fair sports schedules.	backtracking;boolean satisfiability problem;literal (mathematical logic);round-robin scheduling;schedule (computer science);scheduling (computing);solver;sorting;transform, clipping, and lighting;user interface;web search engine	Hantao Zhang;Dapeng Li;Haiou Shen	2004			job shop scheduling;search engine;tournament;computer science;boolean satisfiability problem;user interface;distributed computing;schedule	AI	-45.6409863813388	-27.439752966384823	64219
ebf51b034eab523466185a9cd3bdc18dab39a5d5	vers la maîtrise du virtuel à travers le réel: un nouvel usage de l'informatique en design	computer aided design;3d modeling;design process;virtual reality;3d model;vu;manual media	This paper proposes a new paradigm in computer-aided design. Considering, on one hand the traditional sketches and mock-ups, and digital techniques on the other, this approach fuses the two and proposes new techniques that use the performance of the digital with the capacities of the manual, without replacing or imitating one or the other. In the development of design computer solutions, it is important to know the user well. However, most researchers propose systems that do not consider how designers actually work. This new approach is presented through two new innovative techniques.	computer-aided design;linear algebra;mock object;programming paradigm	Tomás Dorta	2006		10.1145/1132736.1132753	simulation;engineering;artificial intelligence;engineering drawing	EDA	-37.58669762373232	-31.159553924681664	64821
c1b89d29e0696ed3510e5c380c5a5051fc827cdc	a two-view approach to creating computer animation	object-oriented data model;shape view;structural view;animation sequence;new interactive animation system;two-view approach;creating computer animation	In this paper, we report about the design and implementation of a new interactive animation system. The system is based on the idea that all objects occurring in animation sequences should have two views:the structural view and the shape view. Using our system, the animator can create the two views simultaneously without programming and can manipulate objects by sending messages. Thus, the system is based on an interactive, object-oriented data model implemented on the Macintosh.	bentley–ottmann algorithm;collision detection;compositing;computer animation;data model	Wenhui Guo	1995			computer vision;computer facial animation;computer science;computer animation;multimedia;computer graphics (images)	Graphics	-41.21115881672416	-32.956507857209736	64869
6efe9e71102c78e0ed49a022af73b529feba3ef5	afreeca: extending the spatial model of interaction	analytical models;human computer interfaces;collaboration;virtual reality;interaction techniques;collaborative virtual environment afreeca spatial model cve system;shared virtual worlds;receivers;visualization;afreeca;three dimensional displays;solid modeling;receivers collaboration context solid modeling visualization three dimensional displays analytical models;interaction techniques shared virtual worlds human computer interfaces;collaborative virtual environment;context;spatial model;interaction technique;human computer interface;virtual worlds;cve system	This paper analyses the Spatial Model of Interaction, a model that rules the possible interactions among two objects and that has been widely accepted and used in many CVE systems. As a result of this analysis, some deficiencies are revealed and a new model of interaction is proposed. Additionally, a prototype illustrating some of the best features of this model of interaction is detailed.	common vulnerabilities and exposures;interaction;prototype;scalability;self-replicating machine	Diego Martínez;José Pascual Molina;Arturo S. García;Jonatan Martínez;Pascual González	2010	2010 International Conference on Cyberworlds	10.1109/CW.2010.63	simulation;visualization;human–computer interaction;computer science;virtual reality;multimedia;solid modeling;interaction technique;collaboration	Robotics	-43.87530873052319	-37.4899500875726	64901
1e5dbe3bd0f5567302f679a100cd7f9632236549	visual and textual consistency checking tools for graphical user interfaces	textual consistency checking tools;developpement logiciel;visual consistency checking tools;software metrics;assessment tools;human computer interaction;usability testing;user performance;software prototyping;user interface;dialog box summary table;color;prototypes;visual design;metrics;metric;testing;human factors software tools graphical user interfaces user interface management systems software metrics;indexing terms;consistencia;output feedback;software metrics textual consistency checking tools visual consistency checking tools graphical user interfaces experiment inconsistent interface terminology user performance software tools sherlock graphical analysis tools dialog box summary table terminology analysis tools interface spell checker button analysis tools interface color software architecture;development tool;assessment tool;software architecture;graphical user interfaces;textual and visual style;human factors;terminology user interfaces software tools color software design software architecture testing software prototyping prototypes output feedback;evaluation tools;desarrollo logicial;consistance;software development;consistency checking;graphic user interface;inconsistent interface terminology;button analysis tools;metrico;terminology;interface utilisateur;software tools;user interface management systems;technical report;experiment;software design;systems integration methodology;terminology analysis tools;interface color;interface graphique;user interfaces;sherlock;consistency;metrique;interface spell checker;graphical analysis tools	Designing a user interface with a consistent visual design and textual properties with current generation GUI development tools is cumbersome. SHERLOCK, a family of consistency checking tools, has been d signed to evaluate visual design and textual properties of interface, make the GUI evaluation process less arduous, and aid usability testing. SHERLOCK includes a dialog box summary table to provide a compact overview of visual properties of hundreds of dialog boxes of the interface. Terminology specific tools, like Interface Concordance, Terminology Baskets and Interface Speller have been developed. Button specific tools including Button Concordance and Button Layout Table have been created to detect variant capitalization, distinct typefaces, distinct colors, variant button sizes and inconsistent button placements. This paper describes the design, software architecture, and the use of SHERLOCK. An experiment with 60 subjects to study the effects of inconsistent interface terminology on user's performance showed 10-25% speedup for consistent interfaces. SHERLOCK was tested with four commercial prototypes; the corresponding outputs, analysis and feedback from designers of these applications is presented.	color;concordance (publishing);feedback;graphical user interface;programming tool;software architecture;software prototyping;speedup;usability testing;dialog	Rohit Mahajan;Ben Shneiderman	1997	IEEE Trans. Software Eng.	10.1109/32.637386	human–computer interaction;computer science;human factors and ergonomics;operating system;software engineering;graphical user interface;database;programming language;user interface;world wide web	HCI	-40.97001339946453	-28.8963198171042	64950
76f566c8193133020be5e9fe983c7bdbf38b326f	performance-led design of computationally generated audio for interactive applications	interactive sound synthesis;embodied sound design;foley	Stylized temporal behaviour is difficult to incorporate into computational models of environmental sound, requiring either a specialised algorithm or a potentially large amount of scripted automation. For decades Foley artists have relied on embodied performance to rapidly generate expressive sound effects for the moving image. This research project explores performance-led techniques in the design of computational audio for interactive applications.	algorithm;computation;computational model;interactivity	Christian Heinrichs;Andrew W McPherson	2016		10.1145/2839462.2854109	simulation;computer science;multimedia;communication	HCI	-46.08967694506991	-34.48721675983814	65512
d67b306c92c17611ac2ff036f45561ba82a60e1e	natural language interface framework for spatial object composition systems.	communication homme machine;traitement automatique des langues naturelles;human computer communication;computacion informatica;order;imperative;filologias;linguistique appliquee;solicitudes de accion;info eu repo semantics article;informacion documentacion;dialogue interaction;agencement;linguistica;traduction automatique;imperatif;espace virtuel;ciencias basicas y experimentales;spatial language;ordre;lenguaje espacial;interaccion con dialogos;computational linguistics;espagnol;grupo a;configuration;action queries;virtual space;ciencias sociales;linguistique informatique;grupo b;natural language processing;machine translation;applied linguistics	Spatial Object Composition (SOC) systems involve tasks of virtual combination of physical objects (such as furniture parts) with the purpose of creating new, complex objects or arranging the objects in space. In this paper we present a framework for implementing Natural Language Interfaces focused on SOC systems. We propose the use of a rewriting rules grammar (which we call Direct Translation Grammar) to translate action queries in natural language to computational procedures interpretable by the SOC engine. Examples are given for imperative commands in Spanish.	imperative programming;natural language user interface;object composition;rewriting	Hiram Calvo;Alexander F. Gelbukh	2003	Procesamiento del Lenguaje Natural		natural language processing;order;computer science;artificial intelligence;computational linguistics;applied linguistics;linguistics;machine translation;configuration	HCI	-38.193684077264614	-27.743056644993803	65740
024b2aa7f86f9224472a489bdfa34559cd15c85c	non-invasive adaptation of black-box user interfaces	ui prototyping;user interface;handheld computer;user interface toolkit;ui retargeting;real world application;menu navigation;ui evaluation;user interfaces;3d graphics;interaction technique	In this paper a new method for the non-invasive adaptation of user interfaces is presented. The main idea is not to implement the user interface toolkit as an API, but instead as an object file that redefines the functionality of the API of an already existing toolkit in a generic way based on a so-called preloading technique. Compared to common approaches, the presented method allows us to evaluate prototypical user interfaces with a large number of real-world applications with very little effort. If the prototype proves useful, the new user interface is instantly available for all applications based on the redefined toolkit, which significantly simplifies the spreading of new interaction techniques and ideas. We illustrate the procedure by means of adapting the user interface of a 3D graphical application based on a widely used toolkit for use on handheld computers.	application programming interface;black box;computer;definition;desktop computer;graphical user interface;hooking;interaction technique;list of toolkits;microsoft foundation class library;microsoft windows;mobile device;object file;prototype;unix;user interface toolkit	Dirc Rose;Simon Stegmaier;Guido Reina;Daniel Weiskopf;Thomas Ertl	2003			user interface design;look and feel;user;10-foot user interface;user modeling;shell;human–computer interaction;computer science;operating system;multimedia;natural user interface;interactivity;user interface;world wide web;graphical user interface testing;3d computer graphics	HCI	-42.955700548412544	-30.70669595285473	65869
4fa7a4a14958970415ec95ebc21fb4aedc3d7e5e	landing place: remapping motion capture of dance movement to objects and environments	dynamic reflectometry;motion capture;relighting;3d video	Typically motion capture data is used to drive characters. To accomplish this, the actor has to mimic the body shape of the targeted character via posture or using props. The actor also has to move with a predetermined style. A skeletal system is then applied to the point marker data, in order to attach movement to the model. This whole process is oriented towards specific result and leaves little room for the exploration of artistic process. In the collaboration on multimedia dance performance Landing Place we used motion capture to inspire a variety of visuals without preliminary planning and used marker point data directly to produce the animations in Maya.	autodesk maya;motion capture;poor posture;skeletal animation	Vita Berezina-Blackburn;Bebe Miller;Brian Windsor	2005		10.1145/1187112.1187245	computer vision;motion capture;simulation;computer science;computer graphics (images)	Graphics	-44.10373678527961	-35.33087324906227	65917
52ab5a9bc50e1538022f62215bdc17e461f7d070	a multi agent approach to interest profiling of users	television;multiagent system;multi agent system;modele agrege;customization;personnalisation;modelo agregado;recommandation;intelligence artificielle;service utilisateur;multiple classifiers;user profile;recommender system;comportement utilisateur;personalizacion;aggregate model;recomendacion;artificial intelligence;recommendation;inteligencia artificial;user behavior;servicio usuario;user service;sistema multiagente;comportamiento usuario;wiskunde en informatica wiin;systeme multiagent	Intelligent applications deliver personalized experiences and services to the user. This is done by creating and using a profile of the user: user profiling. Several approaches and algorithms are developed for user profiling. This paper describes a multi agent approach that allows multiple algorithms to be combined dynamically to generate a knowledge and interest profile of a user. IBM's ABLE environment was used for the implementation of the multi agent system. To test the system, the user interest profile is build on browse behavior and this profile is applied in a TV program recommender system. The results of implemented system show that multi agent systems provide an excellent platform for an extendible user profiling system that can use multiple classifiers.		P. H. H. Rongen;J. Schröder;Frank Dignum;J. Moorman	2005		10.1007/11559221_33	user;simulation;user modeling;computer science;artificial intelligence;operating system;database;multimedia;television;world wide web;recommender system	AI	-37.10391432989882	-25.152636077976744	65952
440da01268483df945943e7809f04f195fbd11a8	management system integration supported on virtual reality technology: the building lighting devices		A virtual model to support decision-making in the planning of construction maintenance was developed. It gives the capacity to transmit, visually and interactively, information related to the physical behaviour of a specific component of a building, defined as a function of the time variable. The model helps to identify needs related to the lighting equipment of a building, namely, the cost involved in lamp replacement and control of stock, warnings of the need for periodic inspections and minimization of times when broken bulbs are left in place.	management system;system integration;virtual reality	Alcínia Zita Sampaio;Miguel M. Ferreira;Daniel P. Rosário	2010		10.1007/978-3-642-16402-6_34	simulation;multimedia;computer graphics (images)	Visualization	-37.711940354909984	-34.930555986650475	66269
2da01ae6a41a476fee9c6c2e7e5a0e5cc6987b0a	construction of a distributed learning resource management system based on rss technology	busqueda informacion;lettre alphabet;distributed system;really simple syndication;teleenseignement;systeme reparti;abreviation;information retrieval;resource management;abreviacion;indexing terms;user assistance;gestion recursos;sistema repartido;assistance utilisateur;internet;distributed learning;recherche information;asistencia usuario;learning object;gestion ressources;letra alfabeto;teleensenanza;enseignement;abbreviation;remote teaching;letter;resource management system;teaching;ensenanza	RSS is the first letter abbreviations of English Rich Site Summary (enriches the Web site summary ) or Really Simple Syndication (really simple merger ), it is a kind of simple and easy way to share the content among different Web sites, usually used in news and other Web sites arranged in order, such as blog. We designed and implemented a distributed learning resource management system based on RSS technology. In this paper, we firstly introduced the basic principles of RSS technology, and then described the three components of the system: the cluster of the distributed learning resource Web sites, the content aggregator and the content reader. This system will help us to manage the distributed learning resources simply and efficiently	blog;management system;rss;web syndication;world wide web	Chengling Zhao;Liyong Wan;Ying Yu;Qi Luo	2006	Proceedings. Frontiers in Education. 36th Annual Conference	10.1007/11906070_30	the internet;simulation;index term;letter;computer science;artificial intelligence;resource management;rss;database;multimedia;world wide web;news aggregator	Web+IR	-38.05969292785363	-24.527766038174015	66290
4a3195bb81252ca956e653a4df6a0b2229cb7033	assisting end-users in understanding and programming simulations	content management;visual interaction;humans animation visualization virtual environment user centered design computer languages prototypes automation;authoring systems;software agents;visual programming;data visualisation;visualization technique;football;sport;virtual environment;computer animation;visual programming digital simulation sport content management authoring systems data visualisation computer animation software agents;digital simulation;virtual agent;assistive visualization simulation understanding simulation programming content authoring virtual environment development end user programming american football coach digital playbooks animated content virtual agents spatial location play formation;end user programming	We live in a media-rich environment. Today’s technology user is accustomed to visual, interactive content but faced with challenges for authoring such content. The divide between the user and the creator grows with the greater complexity of the media. We seek to enable end-users to develop their own virtual environments through end-user programming. In particular, we are interested in helping American football coaches create digital playbooks with animated content. Such an environment requires the presentation of a large amount of information. This information can be associated with virtual agents, spatial locations, particular play formations, etc. We are currently focusing on assistive visualization techniques for both the programming and presentation of the strategy information.	assistive technology;computer simulation;digital rights management;end-user development;intelligent agent;virtual reality	Christoph Neumann	2005		10.1109/VLHCC.2005.21	simulation;human–computer interaction;computer science;multimedia	HCI	-42.4165534565759	-33.789145224213186	66473
ece54996e5ae26aa32ea07acf00f3a1d0d4eeec2	ecat: event capture annotation tool		This paper introduces the Event Capture Annotation Tool (ECAT), a user-friendly, open-source interface tool for annotating events and their participants in video, capable of extracting the 3D positions and orientations of objects in video captured by Microsoft’s Kinectr hardware. The modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT’s object, program, and attribute representations, although ECAT uses its own spec for explicit labeling of motion instances. The demonstration will show the tool’s workflow and the options available for capturing event-participant relations and browsing visual data. Mapping ECAT’s output to VoxML will also be addressed.	james pustejovsky;modeling language;open-source software;spec#;usability	Tuan Do;Nikhil Krishnaswamy;James Pustejovsky	2016	CoRR		computer vision;computer science;database;world wide web	HCI	-40.67004068335299	-29.420828131775735	66569
0937c4c9780d4ebd8e684e3ea9bc7d76f1d47299	discovering, visualizing, and sharing knowledge through personalized learning knowledge maps	hand;occupation time;representacion conocimientos;fiabilidad;reliability;group structure;ingenierie connaissances;implementation;real time;semantics;hombre;intelligence artificielle;semantica;semantique;sharing;particion;paradigm;temps occupation;modelo 2 dimensiones;fiabilite;temps reel;decouverte connaissance;comportement utilisateur;human;tiempo ocupacion;modele 2 dimensions;paradigme;mano;tiempo real;artificial intelligence;descubrimiento conocimiento;inteligencia artificial;user behavior;partage;main;knowledge representation;implementacion;paradigma;representation connaissances;structure groupe;estructura grupo;two dimensional model;comportamiento usuario;homme;knowledge discovery;knowledge engineering	This paper presents an agent-based approach to semantic exploration and knowledge discovery in large information spaces by means of capturing, visualizing and making usable implicit knowledge structures of a group of users. The focus is on the developed conceptual model and system for creation and collaborative use of personalized learning knowledge maps. We use the paradigm of agents on the one hand as model for our approach, on the other hand it serves as a basis for an efficient implementation of the system. We present an unobtrusive model for profiling personalised user agents based on two dimensional semantic maps that provide 1) a medium of implicit communication between human users and the agents, 2) form of visual representation of resulting knowledge structures. Concerning the issues of implementation we present an agent architecture, consisting of two sets of asynchronously operating agents, which enables both sophisticated processing, as well as short respond times necessary for enabling interactive use in real-time.	agent architecture;agent-based model;cartography;cognitive map;collaborative filtering;concept map;embedded system;emergence;information source;personalization;programming paradigm;real-time clock;semantic mapper;supervised learning;unsupervised learning;user agent	Jasminko Novak;Michael Wurst;Monika Fleischmann;Wolfgang Strauss	2003		10.1007/978-3-540-24612-1_15	simulation;computer science;artificial intelligence;knowledge engineering;reliability;semantics;implementation	AI	-36.40918664294104	-25.238676481908005	66668
e7fca35e0d07faf1d828d8baf4136cfdb77690d5	temporality-based user interface design approaches for desktop and small screen environment	context based approach;context aware computing;small screens;uis;navigational burden;computational science and engineering;form based user interfaces;mobile interface forms;ca;temporality;interface tailoring;user interfaces	User interface (UI) forms are used to enter data in information systems. The improper UI increases navigational burden, data entry errors and reduces the efficiency of the user. Further, in small screen environments, it is difficult to display full interface on a single screen due to the limitation of small screen. Research efforts are going on to investigate the improved user interface methods for desktop and small screen environments. Normally, context-based approach is followed to design user interfaces. In this approach, the input attributes are divided into several contexts based on certain criteria and UI form is designed for each context. We have exploited the fact that several attributes in the UI form may not receive values only during certain time period. We term this as ‘temporality’ property of an attribute. In this paper, we propose a methodology to improve UI for desktop environments by exploiting the notion of ‘temporality’ property of attribute. Further, we also propose an improved UI design methodology by extending notion of temporality to interface tailoring methods for small screens. The experiment results on a real dataset show that the proposed method improves the performance as compared to existing approaches.	desktop computer;information system;software development process;systems development life cycle;television;user interface design	Mittapally Kumara Swamy;P. Krishna Reddy;R. Uday Kiran;M. Venugopal Reddy	2012	IJCSE	10.1504/IJCSE.2012.046180	user interface design;temporality;simulation;human–computer interaction;computer science;computational science and engineering;database;multimedia;user interface;world wide web	HCI	-40.25800648104045	-27.461957386112257	66688
ba4d0b8352de910c309180eee59f5f5c141eb08a	towards an interactive, generative design system: integrating a 'build and evolve' approach with machine learning for complex freeform design	agency;evolutionary design;design representation;user centered evolutionary design;machine learning;system integration;user interaction	The research presented in this paper deals concerns interactive evolutionary design systems and specifically with the Interactive Evolutionary Design Environment (IEDE) developed by the authors. We describe the IEDE concentrating upon the three major components: Component-based Representation; Construction and Repair Agents (providing build and evolve services) and a machine learning sub-system. We also describe the clustering technique utilized within the IEDE to improve the user interactivity of the system.	freeform surface modelling;interactivity;machine learning	Azahar Tekchand Machwe;Ian C. Parmee	2007		10.1007/978-3-540-71805-5_50	user experience design;systems engineering;engineering;knowledge management;machine learning;design education;active learning	Graphics	-41.096397296136224	-26.92343731135295	66965
42637373904acef11ecd545c09a6b74b1ec292ee	interactive programming in agda - objects and graphical user interfaces			agda;graphical user interface;interactive programming	Andreas Abel;Stephan Adelsberger;Anton Setzer	2017	J. Funct. Program.	10.1017/S0956796816000319	human–computer interaction;event-driven programming;post-wimp	PL	-42.17863256548862	-29.6219806706718	67123
26cebeb2c88727e358ad635c47e6599d685cbbff	an open protocol for wide-area multi-user x3d	user agent;multi user 3d;online game;virtual reality;multi user;x3d;web3d;online game programming	This paper describes work to create an open protocol for wide-area multi-user X3D, incorporating many aspects of prior academic, experimental, and proprietary systems, but emphasizing simplicity and practicality for use among heterogeneous Internet user agents. In the Internet tradition, it is documented as a protocol (rather than a framework), and backed by freely-available reference implementations. The hope is that this protocol is useful to those working on new X3D networking nodes as well as to those building multi-user world systems.	internet;multi-user;user agent;world-system;x3d	Jay C. Weber;Tony Parisi	2007		10.1145/1229390.1229413	user agent;computer science;distributed computing;multimedia;world wide web	Networks	-46.04589033198749	-26.410564575600265	67196
7866034a34ffb77fc2892282d971024a3fcb9683	evolution analysis with animated and 3d-visualizations	software metrics;empirical study;software systems evolution evolution analysis animation 3d visualizations;3d visualizations;3d visualization;software systems;large scale system;software metrics computer animation data visualisation;layout;data mining;animation software systems data visualization cities and towns software engineering computer graphics multimedia systems large scale systems software metrics software standards;data visualisation;evolution analysis;visualization;software systems evolution;animation;data visualization;3 dimensional;computer animation	Large software systems typically exist in many revisions. In order to analyze such systems, their evolution needs to be analyzed, too. The main challenge in this context is to cope with the large volume of data and to visualize the system and its evolution as a whole. This paper presents an approach for visualizing the evolution of large-scale systems which uses three different, tightly integrated visualizations that are 3-dimensional and/or animated and which support different analysis tasks. According to a first empirical study, all tasks are supported well by at least one visualization.	evolution;software system	Sven Wenzel;Jens Koch;Udo Kelter;Andreas Kolb	2009	2009 IEEE International Conference on Software Maintenance	10.1109/ICSM.2009.5306279	visualization;human–computer interaction;computer science;theoretical computer science;software engineering;data visualization;computer graphics (images)	Visualization	-33.95785832399774	-30.506773946953075	67828
707bc3e3fcacb2f1b676192f25ebbb34f034b9d7	a personalized walk through the museum: the chip interactive tour guide	mobile device;cultural heritage;user modeling;semantic web technology;chip;mobile museum guide;recommender system;museum collection;semantic web;interactive museum tours;recommender systems;user model	More and more museums aim at enhancing their visitors' museum experiences in a personalized, intensive and engaging way inside the museum. The CHIP1 (Cultural Heritage Information Personalization) project offers various online and mobile tools to the users to be their own curators, e.g. browsing the online collections, planning personalized museum tours, getting recommendations about interesting artworks to see, and quickly finding their ways in the museum. In this paper we present the new version of the personalized museum guide2 offered on a mobile device in the physical museum space. We maintain a dynamic user model to ensure high relevance of recommended artworks and museum tours and in this way (1) support personalized interaction both online and in the museum and (2) provide an intuitive bridge between the online and on-site experiences. We apply semantic Web technologies to enrich the museum collection and guarantee serendipity, novelty and relevance of the recommendations.	experience;mobile device;personalization;relevance;semantic web	Ivo Roes;Natalia Stash;Yiwen Wang;Lora Aroyo	2009		10.1145/1520340.1520479	user modeling;human–computer interaction;computer science;multimedia;world wide web;recommender system	HCI	-40.79235425445337	-25.3581256385261	68149
3e55534ea8356967690b7a573e8b059a33b53736	the virtual reality modeling language explained	high level languages;coherent model virtual reality modeling language vrml immersive 3d experience geometric modeling primitives geometric modeling features 3d interchange format commonly used semantics 3d applications hierarchical transformations light sources texture mapping 3d analog html multiplatform language 3d web page publishing animation user participation multimedia;web pages;user participation;texture mapping;game engine;virtual reality;modeling language;scientific visualization;multimedia computing;virtual reality modeling language;interactive animation;multimedia computing virtual reality high level languages electronic data interchange;interchange format;geometric model;virtual reality solid modeling animation head three dimensional displays light sources geometry material properties html publishing;3d input device;electronic data interchange;material properties;head mounted display	VRML stands for Virtual Reality Modeling Language. Technically, VRML is neither virtual reality nor a modeling language. Virtual reality generally implies an immersive 3D experience, which typically requires a head mounted display (HMD) and 3D input devices, such as digital gloves. VRML neither requires nor imposes immersion. Furthermore, a true modeling language would contain richer geometric modeling primitives and mechanisms. VRML provides a bare minimum of geometric modeling features but contains numerous features unavailable in a modeling language. If VRML is not virtual reality or a modeling language, what is it? This question has several answers. At its core, VRML serves as a 3D interchange format. It defines most of the commonly used semantics found in today's 3D applications such as hierarchical transformations, light sources, viewpoints, geometry, animation, fog, material properties, and texture mapping. Here's a second answer to: what is VRML? It's a 3D analog to HTML. This means that VRML serves as a simple, multiplatform language for publishing 3D Web pages. The fact that some information, including games, engineering models, scientific visualizations, educational experiences, and architecture, can best be experienced in 3D has motivated this language. Typically, these types of projects require intensive interaction, animation, and user participation and exploration beyond what a page, text, or image based format can handle. Another answer is that VRML provides the technology to integrate 3D, 2D, text, and multimedia into a coherent model.	modeling language;vrml;virtual reality	Rikk Carey	1998	IEEE MultiMedia	10.1109/93.713310	material properties;texture mapping;computer vision;scientific visualization;simulation;human–computer interaction;computer science;artificial intelligence;optical head-mounted display;geometric modeling;operating system;electronic data interchange;web page;virtual reality;multimedia;modeling language;world wide web;high-level programming language;computer graphics (images)	Visualization	-39.69106482972002	-33.20762590542431	68152
3078cf695968dd67966020cf0f768b934a30e143	realtime constraint-based cinematography for complex interactive 3d worlds	3d interaction	"""In 3D interactive ction systems, a virtual camera must \ lm"""" the behaviors of multiple autonomous characters as they unpredictably interact with one another, are modi ed by the viewer, and manipulate artifacts in 3D worlds with complex scene geometries. It must continuously plan camera movements to clearly shoot the salient visual features of each relevant character. To address these issues, we have developed a 3D interactive ction system with a narrative planner that, together with a bank of autonomous character directors, creates cinematic goals for a constraintbased realtime 3D virtual cinematography planner. As interactive narratives unfold, a cinematic goal selector creates view constraints to lm the most salient activities performed by the characters. These constraints are then passed to a camera planner, which employs a partial constraintbased approach to compute the position and orientation of the virtual camera. This framework has been implemented in a prototype 3D interactive ction system, Cops&Robbers, a testbed with multiple characters interacting in an intricate cityscape."""	3d computer graphics;autonomous robot;interaction;natural language generation;prototype;testbed;virtual camera system	William H. Bares;Joël P. Grégoire;James C. Lester	1998			computer vision;simulation;computer science;artificial intelligence;computer graphics (images)	AI	-39.951370663986246	-36.401927506986056	68457
aea9effb6bce46960ee01ac92164ae5c3c0b78eb	global illumination for interactive applications and high-quality animations		One of the main obstacles to the use of global illumination in image synthesis industry is the considerable amount of time needed to compute the lighting for a single image. Until now, this computational cost has prevented its widespread use in interactive design applications as well as in computer animations. Several algorithms have been proposed to address these issues. In this report, we present a much needed survey and classification of the most up-to-date of these methods. Roughly, two families of algorithms can be distinguished. The first one aims at providing interactive feedback for lighting design applications. The second one gives higher priority to the quality of results, and therefore relies on offline computations. Recently, impressive advances have been made in both categories. Indeed, with the steady progress of computing resources and graphics hardware, and the current trend of new algorithms for animated scenes, common use of global illumination seems closer than ever.	global illumination;interactivity	Cyrille Damez;Kirill Dmitriev;Karol Myszkowski	2002		10.2312/egst.20021050	simulation;computer science;multimedia;computer graphics (images)	HCI	-39.63414219929127	-33.95057633948258	68571
4c60cbf9734e9803a860bbcfebb4f3bf66ccb2d2	the imagetcl multimedia algorithm development system	tk-based development environment;complex file format;tk environment;algorithm development system;testing algorithm;high-performance multimedia data analysis;dartmouth experimental visualization laboratory;multimedia algorithm development;imagetcl multimedia development system;imagetcl approach;new tcl	The IMAGETCL multimedia development system is a new Tcl/Tk-based development environment specifically targeting development of high-performance multimedia data analysis algorithms. Multimedia algorithm development is complicated by large volumes of data, complex le formats, compression and decompression, and temporal synchronization. Testing algorithms requires elaborate user interfaces which can display intermediate and result images, play audio, and adjust parameters. IMAGETCL uses the features of the Tcl/Tk environment as a base on which to build a system that signi cantly aids this process. This paper describes the IMAGETCL approach to algorithm development and describes several applications of IMAGETCL in the Dartmouth Experimental Visualization Laboratory (DEVLAB).	algorithm;data compression;tcl;user interface	Charles B. Owen	1997			simulation;computer science;multimedia;computer graphics (images)	SE	-43.710068816553125	-32.910864723066354	68572
5eba097e48b60e06a1e29a977dcdf62d13dd80f7	conversion of jpg image into dicom image format with one click tagging		DICOM images are the centerpiece of radiological imaging. They contain a lot of metadata information about the patient, procedure, sequence of images, device and location. To modify, annotate or simply anonymize images for distribution, we often need to convert DICOM images to another format like jpeg since there are a number of image manipulation tools available for jpeg images compared to DICOM. As part of a research at our institution to customize radiology images to assess cognitive ability of multiple user groups, we created an open-source tool called Jpg2DicomTags, which is able to extract DICOM metadata tags, convert images to lossless jpg that can be manipulated and subsequently reconvert jpg images to DICOM by adding back the metadata tags. This tool provides a simple, easy to use user-interface for a tedious manual task that providers, researchers and patients might often need to do.	1-click;dicom;jpeg	Olakunle Oladiran;Judy Gichoya;Saptarshi Purkayastha	2017		10.1007/978-3-319-58466-9_6	dicom;metadata;information retrieval;jpeg;lossless compression;computer science	Vision	-44.33974334179963	-26.69617859996416	68793
0a33219b704ec39f3b8063bbd77426a1ab7e4990	a sketch-based interface for collaborative design	non photorealistic rendering;engineering design;direct manipulation;line drawing interpretation;conceptual design;rapid prototyping;3d model;visual feedback;virtual environment;collaborative design;sketching	We present an interface for collaborative conceptual design that combines sketch elements, direct manipulation of 3D objects and non-photorealistic rendering. Such a combination results in a simple and intuitive 2D-sketch-to- 3D modeling system suitable for novice users. It allows users potentially located in geographically distant areas to cooperate by sketching, exploring and modifying their ideas interactively, with immediate visual feedback. Our system prototype supports several modeling primitives and can be extended to handle user-defined objects. Potential applications of our system include early stages of urban and landscape design, rapid prototype of virtual environments, animation, education and recreational use.	3d modeling;direct manipulation interface;glossary of computer graphics;interactivity;non-photorealistic rendering;prototype;rapid prototyping;sketch;unbiased rendering;virtual reality	Zhe Fan;Ma Chi;Arie E. Kaufman;Manuel Menezes de Oliveira Neto	2004		10.2312/SBM/SBM04/143-150	simulation;engineering;multimedia;computer graphics (images)	HCI	-37.5308037017074	-33.1537028423372	68845
43bfba18bf87a9d30973c9038dd4b80a3f6932c8	xfaceed: authoring tool for embodied conversational agents	embodied conversational agents;real time;3d model;talking heads;facial animation;mpeg 4;talking head;embodied conversational agent;3d facial animation;authoring tool;open source	In this paper, XfaceEd, our open source, platform independent tool for authoring 3D embodied conversational agents (ECAs) is presented. Following MPEG-4 Facial Animation (FA) standard, XfaceEd provides an easy to use interface to generate MPEG-4 ready ECAs from static 3D models. Users can set MPEG-4 Facial Definition Points (FDP) and Facial Animation Parameter Units (FAPU), define the zone of influence of each feature point and how this influence is propagated among the neighboring vertices. As an alternative to MPEG-4, one can also specify morph targets for different categories such as visemes, emotions and expressions, in order to achieve facial animation using the keyframe interpolation technique. Morph targets from different categories are blended to create more lifelike behaviour.Results can be previewed and parameters can be tweaked real time within the application for fine tuning. Changes made take into effect immediately, which in turn ensures rapid production. The final output is a configuration file XML format and can be interpreted by XfacePlayer or other applications for easy authoring of embodied conversational agents for multimodal environments.	3d modeling;dialog system;embodied agent;facial recognition system;interpolation;key frame;morph target animation;multimodal interaction;open-source software;xml	Koray Balci	2005		10.1145/1088463.1088500	computer vision;speech recognition;computer facial animation;embodied agent;computer science;multimedia;mpeg-4;computer graphics (images)	Graphics	-42.19468097196461	-33.24097030515659	69098
6cb394dbb2d2c2e69fcbfea885de4982f44dfc80	a style controller for generating virtual human behaviors	restricted boltzmann machines;style content separation;motion capture;animation;virtual agent	Creating a virtual character that exhibits realistic physical behaviors requires a rich set of animations. To mimic the variety as well as the subtlety of human behavior, we may need to animate not only a wide range of behaviors but also variations of the same type of behavior influenced by the environment and the state of the character, including the emotional and physiological state. A general approach to this challenge is to gather a set of animations produced by artists or motion capture. However, this approach can be extremely costly in time and effort. In this work, we propose a model that can learn styled motion generation and an algorithm that produce new styles of motions via style interpolation. The model takes a set of styled motions as training samples and creates new motions that are the generalization among the given styles. Our style interpolation algorithm can blend together motions with distinct styles, and improves on the performance of previous work. We verify our algorithm using walking motions of different styles, and the experimental results show that our method is significantly better than previous work.	algorithm;game controller;interpolation;motion capture;virtual actor	Chung-Cheng Chiu;Stacy Marsella	2011			boltzmann machine;anime;motion capture;simulation;computer science;artificial intelligence;computer graphics (images)	Graphics	-39.21768972081304	-35.92208041535014	69293
0ce1dc16ef12fb65c48e0db18025c3bb2beae3fe	mapbot: a web based map information retrieval system	agent interaction;computacion informatica;geographic information system;information retrieval system;geographic information;user interface;grupo de excelencia;visual information retrieval;web mapping;interactive system;ciencias basicas y experimentales;qa0076 75 computer software;graphic user interface;natural user interface;scalable vector graphics	Abstract   Many types of information are geographically referenced and interactive maps provide a natural user interface to such data. However, map presentation in geographical information systems and on the Web is closed related to traditional cartography and provides a very limited interactive experience. In this paper, we present MAPBOT, an interactive Web based map information retrieval system in which Web users can easily and efficiently search geographical information with the assistance of a user interface agent (UIA). Each kind of map feature such as a building or a motorway works as an agent called a Maplet. Each Maplet has a user interface level to assist the user to find information of interest and a graphic display level that controls the presence and the appearance of the feature on the map. The semantic relationships of Maplets are defined in an Ontology Repository provided by the system which is used by the UIA to assist a user to semantically and efficiently search map information interested. An Ontology Editor with a graphic user interface has been implemented to update the Ontology Repository. Visualization on the client is based on Scalable Vector Graphics which provides a high quality Web map.	information retrieval	Maozhen Li;Man Qi	2003	Information & Software Technology	10.1016/S0950-5849(03)00082-X	user interface design;web service;user;web mapping;computer science;web navigation;scalable vector graphics;graphical user interface;database;geographic information system;multimedia;natural user interface;user interface;world wide web;information retrieval	AI	-40.39825765805713	-25.71997409318374	69317
c330ea14f73e2e53538e844f62066f5beb93a325	embedded interactive concept maps in web documents	concept map		concept map;web page	Robert Kremer;Brian R. Gaines	1996			social semantic web;web development;web modeling;web standards;world wide web;web mapping;web page;web design;web navigation;computer science	NLP	-43.420151867535765	-23.969435871811438	69412
99e5ecef69661d3e969e924d192ba03f39fa99a1	haptic visualization of computational fluid dynamics data using reactive forces	sistema interactivo;interfaz grafica;interfaces;acoplamiento modo;graphical interface;volume rendering;man machine dialogue;implementation;computation fluid dynamics;couplage mode;flux donnee;aeronef;flujo datos;carta de datos;aeronave;systeme conversationnel;computational fluid dynamics;visualization;engineering and technology;feedback;teknik och teknologier;estudio caso;interactive system;mappage;data visualization;etude cas;graphic user interface;dialogo hombre maquina;visualisation donnee;haptic technology;mapping;interactive graphics;data flow;implementacion;mode coupling;interface graphique;haptic interaction;aircraft;dialogue homme machine	Datasets from Computational Fluid Dynamics (CFD) can be post-processed and visualized to aid understanding of the flow phenomena present. Visualization of CFD data, however, often suffers from problems such as occlusion and cluttering when methods such as glyphing and volume rendering are applied. In this paper we present a case study where new modes for haptic interaction are used to enhance the exploration of CFD data. A VR environment with interactive graphics and an integrated graphical user interface has been implemented. In contrast to previous work on haptic interaction with CFD data we employ a ‘reactive’ haptic scheme as opposed to direct force mapping. The reactive approach not only generates more stable feedback but also provides clearer and more intuitive cues about the underlying data. Two haptic modes are used to enhance the understanding of different features in the flow data: One presents the orientation of the data and also guides the user to follow the stream as it flows around the aircraft fuselage. The other provides a haptic representation of vortex data. This mode enables the user to perceive and so follow tendencies of vorticity and vortices.	computation;computational fluid dynamics;feedback;graphical user interface;graphics;haptic technology;quantum vortex;volume rendering	Karljohan E. Lundin Palmerius;Mattias Sillén;Matthew D. Cooper;Anders Ynnerman	2005		10.1117/12.587029	simulation;flow visualization;human–computer interaction;computer science;computer graphics (images)	Visualization	-35.63725993145989	-28.6817936102769	69461
86f8531348e13ef27ffb1ab58bf003aaf7527fe5	a study of annotations for a consumer health portal	portals;web sites cataloguing digital libraries health care medical information systems portals;information systems;software libraries;digital library;digital libraries;digital library consumer health portal cataloguer annotations consumer health web sites web site cataloging process;annotation;cataloguing;web services annotation cataloging;portals navigation software tools medical services information systems computer applications human factors web services software libraries web page design;web service;consumer health portal;computer applications;navigation;human factors;medical services;medical information systems;web site cataloging process;consumer health web sites;web page design;web services;web sites;software tools;cataloging;cataloguer annotations;health care	Catalogers of Web sites for a digital library face unique challenges. There are no well-established rules for cataloging the less structured and constantly-changing information object. Identifying problems arising from Web site cataloging process will provide insights in designing better cataloging systems to support the process. One of the approaches of exploring the problems and how they are dealt with is examining the notes that catalogers made during the Web site cataloging process. The paper presents a study of annotations made by cataloguers of consumer health Web sites in order to better understand the Web site cataloging process		Lili Luo;David West;Gary Marchionini;Catherine Blake	2005	Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '05)	10.1145/1065385.1065491	web service;digital library;computer science;multimedia;world wide web;information retrieval	Visualization	-43.13913435151973	-25.01828572719704	69602
42cb31389b2af97517d65ae616ad7158674308d4	description of a modeling, simulation, animation, and real-time control (mosart) environment for a class of electromechanical systems	faster than real time simulation;control engineering education;electric actuators;system dynamics modeling;controls education;system modeling;model system;real time control;stand alone application electromechanical systems modeling simulation animation and real time control controls education cart pendulum control3d lab microsoft windows visual c matlab simulink graphical user interface toolboxes visualization three dimensional animation features;animation control;computer aided instruction;analysis and design;simulation;control engineering education actuators electrical engineering education modeling simulation animation real time systems visual languages computer aided instruction graphical user interfaces visualization;mathematical analysis control engineering education electric actuators electrical engineering education modelling simulation computer animation real time systems visual languages c language computer aided instruction graphical user interfaces data visualisation;modeling simulation animation and real time control mosart;direct 3d;mathematical analysis;three dimensional;and real time control mosart;data visualisation;visualization;control system;c language;development environment;visual languages;graphical user interfaces;animation;initial condition;mathematical model;electrical engineering education;graphic user interface;design;electromechanical systems;interaction model;near real time;computer animation;control education;modeling;animal model;pendulum;model simulation;real time systems	"""This paper describes an Interactive Modeling, Simulation, Animation, and Real-Time Control (MoSART) Environment that is useful for controls education and research. The described MoSART environment is shown to be useful for analyzing, designing, visualizing, and evaluating control systems for a class of """"cart-pendulum"""" electromechanical systems. The environment-referred to as Cart-Pendulum Control3D-Lab-is based on Microsoft Windows, Visual C++, Direct-3D, and MATLAB/Simulink. The environment can be used as a stand-alone application or together with MATLAB, Simulink, and toolboxes. When used as a stand-alone application, a friendly graphical user interface permits easy interaction. Users may select (via pull-down menus) systems, dynamical models, control laws, exogenous signals (including joystick inputs) and associated parameters, initial conditions, integration routines, and associated parameters. When used with MATLAB, Simulink, and toolboxes, the previously mentioned nominal features are significantly enhanced. In either case, the interface permits users to access the following (via pull-down menus): animation models, mesh properties, texture and lighting models, system-specific visual indicators, graphics to be displayed, animation/data display/storage rates, simulation control buttons, and extensive documentation. When Simulink is present, users can exploit extensive visualization and three-dimensional (3-D) animation features through provided and/or user-generated Simulink diagrams. This capability makes the developed environment very extensible with respect to mathematical models and control laws. In addition, users may readily export simulation data to MATLAB/toolboxes for postprocessing and further analysis. The environment also contains a suite of well-documented (easy-to-modify) models and control laws that are implemented within the provided Simulink block diagrams. Provided (special) blocks enable animation, joystick inputs, and (near) real-time simulation and animation (when possible). (Near real-time-or faster-than-real-time-simulation and animation are possible whenever the mathematical and animation models are sufficiently simple and data manipulation requirements, e.g. storage and display, are sufficiently mild. For the systems considered, (near) real-time simulation and animation is readily achievable.) Associated with each block diagram are system-specific, menu-accessed m-files that permit detailed analysis and design. A hardware module permits real-time control of actual hardware experiments. The developed environment is shown to be a valuable tool for enhancing both controls education in a variety of classes as well as research. Examples are presented to illustrate the utility of the environment."""	c++;control system;diagram;documentation;excite;experiment;file spanning;graphical user interface;graphics;initial condition;joystick;matlab;mathematical model;microsoft windows;nonlinear gameplay;problem solving;real-time clock;real-time transcription;real-time web;requirement;simulation;simulink;texture mapping;user-generated content	Armando A. Rodriguez;Richard P. Metzger;Oguzhan Cifdaloz;Thanate Dhirasakdanon	2005	IEEE Transactions on Education	10.1109/TE.2004.842915	simulation;systems modeling;human–computer interaction;computer science;engineering;control system;graphical user interface;computer graphics (images)	Graphics	-41.635398125487	-32.649384922873736	69703
0196097e55de1975eed3d3ff946abb7dbc83b7a9	scenocosme: sphèraléas		Artist Statement Scenocosme use interactive art, music, and architecture to create evolutionary and interactive artwork. With multiple forms of expression, they invite the spectator to participate in the center of collective musical or choreographic performances. SphèrAléas is made of a half-spherical structure and an evolutionary device for producing visual and sonorous shapes. This space is ideal for collective performances; it accommodates in its center a constellation of visible shapes, spinning sound loops, and luminous vibratory elements. The dome-bubble hosts universes and objects endowed with life. They are articulated as subtle microcosms in conversation. Visitors become actively engaged in this matrix space and create sonorous and visual interstices that awaken their senses and open unexpected territories to their imaginations. Visitors (creators) are offered ephemeral face-to-face opportunities for dialogue, and they give themselves up to this sensitive complicity, cradled by flows of random emotions. The show is ever changing. SphèrAléas allows visitors to create moments of interchange and immerses them in spaces that feel like daydreams. Aléas, an original virtual music software/instrument, arose from a reflection on how to materialise or draw sound with 3D images. It's a synthesis software that manipulates sounds and abstract images to create a dialogue with reality. It creates an interactive, sensitive relationship with the audience, and it allows visitors to create, modify, observe, and manipulate moving 3D shapes. By manipulating the sensors, visitors can continuously change the whole structure by playing with the different variables: order, side-by-side positioning, overlapping, speed, rhythm, harmonic pitch, etc. Thus, as they position new materials, visitors create sustained tunes made of rhythmic relationships animated by subtle temporal intervals. Each visitor appropriates the system to create particular processes of repetitive polyphony, where parts overlap then disappear into hypnotic swirls. The audience determines the future of the work by experimenting with infinite orchestrations.	dot-com bubble;experiment;interactive art;luminous studio;performance;sensor	Gregory Lasserre;Anais met den Ancxt	2007		10.1145/1280120.1280174		Graphics	-48.076469297280845	-35.75634741647774	69809
be199cee6f66307b869bfb47b846754d468822bb	kara: a bci approach to composition		Kara is a greek word that could be translated as head. In the Kara series of pieces, the musicians wear braincomputer interfaces (BCI) in order to capture their EEG waves while performing. The information from these waves is sent to a computer, where it is processed in order to generate a real-time score, computer generated sounds and a visual display of the data. A closed-loop is formed between the musicians mental activity and the music they generate. As they perform the real-time score generated by their EEG waves, more mental activity is generated, which in turn generates the next portion of the score, and so on. This loop continues for the whole piece, although the score generation algorithms vary along different sections of the musical discourse. This article is presented for the piece+paper modality.	algorithm;brain–computer interface;electroencephalography;energy (psychological);modality (human–computer interaction);real-time locating system	Rodrigo F. Cádiz;Patricio de la Cuadra	2014			speech recognition;brain–computer interface;composition (visual arts);electroencephalography;computer science	Graphics	-47.52471145350156	-35.88789579451338	69848
b0cbb6b3aeb34c07974f9a72df8bf9800a139cd1	digit: a digital foley system to generate footstep sounds		We present DIGItal sTeps (DIGIT), a system for assisting in the creation of footstep sounds in a post-production foley context—a practice that recreates all diegetic sounds for a moving image. The novelty behind DIGIT is the use of the acoustic (haptic) response of a gesture on a tangible interface as means for navigating and retrieving similar matches from a large database of annotated footstep sounds. While capturing the tactile expressiveness of the traditional sound foley practice in the exploration of physical objects, DIGIT streamlines the workflow of the audio post production environment for film or games by reducing its costly and time-consuming requirements.		Luis Aly;Rui Penha;Gilberto Bernardes	2017		10.1007/978-3-030-01692-0_28	post-production;sound design;speech recognition;novelty;haptic technology;workflow;foley;computer science;gesture;numerical digit	EDA	-44.962763291418625	-34.56438779875002	70161
01a19de5910120d2965f678d3c6cf4def385b0df	a survey of procedural content generation techniques suitable to game development	3d;rivers roads games sociology statistics shape humans;rivers;procedural generation;human guidance game development context procedural content generation techniques content creation cost reduction assisted techniques nonassisted techniques human intervention;game development;shape;roads;games;statistics;humans;survey;rendering computer graphics computer games;sociology;3d survey procedural generation game development	The development of a complex game is a time consuming task that requires a significant amount of content generation, including terrains, objects, characters, etc that requires a lot of effort from the a designing team. The quality of such content impacts the project costs and budget. One of the biggest challenges concerning the content is how to improve its details and at the same time lower the creation costs. In this context procedural content generation techniques can help to reduce the costs associated with content creation. This paper presents a survey of classical and modern techniques focused on procedural content generation suitable for game development. They can be used to produce terrains, coastlines, rivers, roads and cities. All techniques are classified as assisted (require human intervention/guidance in order to produce results) or non-assisted (require few or no human intervention/guidance to produce the desired results).	procedural generation;video game development	Daniel Michelon De Carli;Fernando Bevilacqua;Cesar Tadeu Pozzer;Marcos Cordeiro d'Ornellas	2011	2011 Brazilian Symposium on Games and Digital Entertainment	10.1109/SBGAMES.2011.15	simulation;engineering;operations management;multimedia	HCI	-36.07467004461714	-32.21498365877044	70231
2245bbb32b7714046c591fc4ea85fd2324cf1323	3d topographic map generation of fukushima daiichi power plant: visualization of the reconstruction plan for effective information sharing	design automation;topographs 3d topographic map generation fukushima daiichi power plant reconstruction plan visualization information sharing great east japan earthquake fukushima daiichi nuclear power plant tokyo electric power company tsunami 3d virtual map cad topographic map aerial photographs;earthquakes;three dimensional displays;image reconstruction;solid modeling;cities and towns;power generation;tsunami cad cartography data visualisation earthquakes nuclear power stations;three dimensional displays power generation solid modeling earthquakes cities and towns image reconstruction design automation;computer graphics fukushima daiichi nuclear power plant tsunami reconstruction plan visualization	The Great East Japan Earthquake that occurred on March 11, 2011 resulted in unprecedented damage in various parts of Japan. In particular, the Fukushima Daiichi Nuclear Power Plant of Tokyo Electric Power Company received extensive damage due to the tsunami generated by the earthquake. In our paper, we propose the creation of a 3D virtual map that is a combination of a CAD topographic map near the Fukushima Daiichi plant, aerial photographs, and topographs.	aerial photography;computer-aided design;topography	Akio Doi;Kenji Oshida;Kenji Sakakibara;Sachio Kurose;Tomoya Itoh	2014	2014 IEEE International Symposium on Independent Computing (ISIC)	10.1109/INDCOMP.2014.7011758	engineering;civil engineering;nuclear medicine;cartography	Arch	-34.91310298489404	-30.84778872969883	70278
445f0b154a2142a7ebe6a0e3e01b4b02abd9165c	a novel human computer interaction paradigm for volume visualization in projection-based virtual environments	human computer interaction;object interaction;direct manipulation;institute for integrated and intelligent systems;conference output;faculty of science environment engineering and technology;virtual reality;computer human interaction;280104;object manipulation;volume visualization;virtual environment;immersive virtual environment	We propose a novel Human Computer Interaction (HCI) paradigm for volume visualization in projection-based immersive virtual environments (VEs). This paradigm is intuitive, highly efficient and allows accurate control over the virtual objects. A fine control mode for direct manipulation is proposed to address the low accuracy problem of virtual object manipulation in VEs. An agent object interaction method is proposed to provide more flexibility in manipulating the volume objects. A two-handed scaling method is proposed to conveniently scale the volume object along one, two, or three axes. Finally, a ghost object paradigm is proposed to address the motion constraint problem for virtual objects. An implementation using a 3-state tracked glove setup as the input interface is discussed. How basic functionality for volume visualization can be transferred from the 2D WIMP (Window, Icon, Menu, and Pointer) interface to a 3D VR interface is also systematically discussed.	direct manipulation interface;effective method;human computer;human–computer interaction;image scaling;input device;interaction technique;programming paradigm;scientific visualization;three-state logic;usability;virtual reality;wimp (computing)	Changming He;Andrew Lewis;Jun Jo	2007		10.1007/978-3-540-73214-3_5	simulation;human–computer interaction;computer science;virtual machine;virtual reality;computer graphics (images)	Visualization	-42.38132150763508	-37.16424412452782	70351
4107ec5ea3b2255f82c23734113f13f3031e7622	inside interactive video cutout	dynamic reflectometry;interactive video;relighting;3d video	This sketch shows many of the implementation details that support the Video Cutout system described in the papers session at SIGGRAPH 2005. Video Cutout is an interactive system for efficiently extracting foreground objects from a video. We extend the min-cut based segmentation approach from image to video. A novel user interface allows the user to indicate the foreground object simultaneously across space and time. A hierarchical mean-shift preprocess minimizes the number of nodes that min-cut must operate on. Within min-cut we define new local cost functions to augment the more global costs defined in earlier work. Finally, we extend previous 2D alpha matting methods to the 3D spatio-temporal video object preserving both spatial and temporal smoothness. In this sketch we will go into many of the details that could not fit within the short presentation in the papers session. We assume the reader and viewer of the session will have seen that paper and talk. Here we will delve into details such as: • Pre-Processing	interactivity;mean shift;minimum cut;preprocessor;siggraph;user interface	Pravin Bhat;Jue Wang;Alex Colburn;Maneesh Agrawala;Michael F. Cohen	2005		10.1145/1187112.1187212	computer vision;multimedia;computer graphics (images)	Graphics	-34.66176693466822	-34.82494466899491	70491
09d4b8bf63e3ef7c2626758f3c2e59a396182778	sound production and modeling	digital signal processing;modeling technique;production computer interfaces personal digital assistants multimedia systems motion pictures virtual reality digital signal processing physics speech processing multiple signal classification;graphics intensive real time applications sound production sound modeling computer representations human sound perception multimedia multispeaker surround sound entertainment systems augmented reality virtual reality real time sound manipulation multimodal computer human interfaces games;real time;virtual reality;acoustic signal processing;multimedia systems;multimedia computing;human interface;multimedia computing acoustic signal processing hearing virtual reality user interfaces real time systems;music perception;augmented reality;system architecture;real time application;user interfaces;hearing;human computer interface;real time systems	23 S ound in multimedia, movies, games, virtual reality, and human–computer interfaces is a growing field that encompasses the disciplines of analog and digital signal processing, physics, speech, music, perception, and computer systems architecture. This overview of sound production and modeling techniques surveys the state of the art in sound technology. Sound has become a critical component in modern multimedia systems, especially multispeaker surround-sound entertainment systems. Many of the components and techniques in these systems are also applicable in virtual and augmented reality systems. With basic sound hardware now available for most (but not all, as in some palmtops) computer systems , and increasing processor speeds allowing direct real-time sound manipulation, enhancing multimodal computer–human interfaces by using the sonic channel is becoming commonplace. Sound in games and other graphics-intensive real-time applications is achieving higher levels of sophistication and promises to undergo even further advancements in realism and responsiveness. Sound is a wave phenomenon created by vibrations of physical objects or fluids. For any given medium, sound travels at a constant rate determined by the properties of that medium such as the density and bending/compression modulus. In stiff media (such as rigid bars and plates), the speed of sound is sometimes a function of the frequency of oscillation, with greater speed for increasing frequency. Some propagating sound waves are transverse, where the disturbance is orthogonal to the direction of propagation. This is evident in water waves, which displace the surface of the water up and down, yet travel perpendicular to the disturbance motion. Figure 1 shows a transverse wave in the wave traveling on a string. The transverse wave also occurs in many other solids such as membranes and plates. Other types of propagating sound waves are longitudinal, in which the disturbance is in the same direction as the propagation. This happens in air, as Figure 2 shows. Waves propagate in physical objects, but the vibration can sometimes appear much differently to observers. For example, if we lift up a string under tension at the center and release it, it appears to flop up and down, rather than the appearance of waves traveling down and back along the string. Similarly, if we strike a drumhead in the center, it appears to flop up and down (like the string, but in 2D). If we were to pick up a string near the end, and if we could observe the vibration in slow …	augmented reality;computer architecture;digital signal processing;flops;graphics;modulus robot;multimodal interaction;photographic plate;real-time clock;real-time computing;real-time transcription;responsiveness;software propagation;surround sound;systems architecture;transverse wave;virtual reality	Perry R. Cook	2002	IEEE Computer Graphics and Applications	10.1109/MCG.2002.1016695	augmented reality;simulation;human–computer interaction;computer science;operating system;digital signal processing;virtual reality;multimedia;user interface;human interface device;computer graphics (images)	Graphics	-45.614950241650284	-35.32248516661968	70536
7c3b4ac51d9d55b9572f04b145a93449ad6755bc	reconfigurable three-dimensional prototype system using digital materials	gaze;motion capture;analysis;bilateral filter	"""Digital materials are discrete elements such as LEGO Blocks that it can be a kind of reconfigurable 3D matters. There are two advantages of using digital material rather than a continuous material. Firstly, it is easy to change the form after shaping by assembling and disassembling the elements. Secondly, There is never that the error of the part impacts the whole form in the shaping because the elements can be connected exactly by the joint system. There are many researches of digital material focus on the modular connection by press fitting or bonding. Such a digital material can't be assembled and disassembled smoothly after shaped. In our research, we designed the digital material """"Kelvin Block"""" (figure 1a) that specialized in smoothly reconfiguring, and we developed the machine """"3D Assembler"""" (figure 1b) to arrange Kelvin Blocks automatically. The size of Kelvin Block is 40mmx40mmx40mm that is optimized to the volume of the joint system."""	assembly language;noise shaping;prototype;smoothing	Keita Sekijima;Hiroya Tanaka	2015		10.1145/2787626.2792657	computer vision;real-time computing;motion capture;simulation;computer science;artificial intelligence;analysis;geometry;bilateral filter;computer graphics (images)	Graphics	-43.54838112425787	-35.27313314058776	70546
0f3f443670efcdcade1ab7e5d9970b83a3c939a3	developing virtual reality applications with unity	virtual reality;consumer level virtual reality hardware virtual reality applications unity game engine vr components fully immersive vr experience multiplatform game engine interactive 3d content intuitive interface 3d software;virtual reality computer games;abstracts;computer games	This tutorial will provide an introduction to Unity (http://www.unity3D.com) and several VR components that are designed to work with Unity. These VR components can be used in isolation or pieced together to provide fully immersive VR experiences. Unity is a feature rich multi-platform game engine for the creation of interactive 3D content. It includes an intuitive interface while at the same time allowing low-level access for developers. Thousands of assets provided by other content creators can be reused to quickly develop immersive experiences. Because of its intuitive interface, well designed architecture, and ability to easily reuse assets, 3D software can be developed in a fraction of time compared to traditional development. Consumer-level virtual-reality hardware combined with Unity have recently empowered hobbyists, professionals, and academics to quickly create virtual reality applications. Because of Unity's widespread use and ease of use, several virtual reality companies now fully support Unity. During this tutorial, participants will learn how to quickly build virtual reality applications from some of the leaders of Unity virtual reality development. Attendees will gain an understanding of how to use multiple VR components with Unity and will have enough knowledge to start building VR applications using Unity by the end of the tutorial.	unity;virtual reality	Jason Jerald;Peter Giokaris;Danny Woodall;Arno Hartbolt;Anish Chandak;Sebastien Kuntz	2014		10.1109/VR.2014.6802117	computer-mediated reality;artificial reality;simulation;human–computer interaction;computer science;artificial intelligence;operating system;virtual reality;mixed reality;multimedia;immersion	Visualization	-42.838986161423875	-33.37141506967076	70837
898bf5fff6aa0a79f89863d0dcb1eab3a8310910	presentation system of still picture teaching material using the implant-jpeg	indicated image;various image information;related image;additional information;picture information;presentation system;high-level concept;image information;related image information;image teaching material	The authors developed a presentation system for still image teaching materials, which enables various image information embedded in still images on the World Wide Web to be displayed while synchronizing that information with audio explanations and also enables other related image information to be referenced interactively. This paper presents an overview of the presentation system and reports on a creation tool for still image teaching materials, which supports the creation of still image teaching materials to be used in the presentation system. The presentation system's viewer, which is provided to enable the user to accurately understand the image information, has functions not only for changing the display of the image that was pointed to so that the indicated image can be differentiated from other images but also for displaying explanatory text for the image and displaying additional information such as information for other related images. In addition, the system's search function enables the user to find a high-level concept to which the image information belongs and easily retrieve other picture information having the same high-level concept. The presentation data can be easily maintained and managed by putting together and distributing the information for the still image and each partial still image in an implant-JPEG format file called a capsule. © 2004 Wiley Periodicals, Inc. Syst Comp Jpn, 35(14): 96–105, 2004; Published online in Wiley InterScience (). DOI 10.1002&sol;scj.10042	jpeg	Nobuyoshi Yonezawa;Hitoshi Sasaki;Zenju Otsuki;Masahiro Ukigai;Youzou Miyadera;Takashi Okunuki;Shoji Yoshihara;Takateru Kamei	2004	Systems and Computers in Japan	10.1002/scj.10042	computer vision;computer science;artificial intelligence;multimedia;computer graphics (images)	Robotics	-44.19822603474849	-27.389078956425067	71065
caca5991333f01c404eaf4e33718a6f7869845e4	erasing, digging and clipping in volumetric datasets with one or two hands	3d interaction;large dataset;volume rendering;interactive volume sculpting;real time;efficient implementation;graphics hardware;real time volume rendering;volume visualization;two handed input;interaction technique	Visualization of volumetric datasets is common in many fields and has been an active area of research in the past two decades. In spite of developments in volume visualization techniques, interacting with large datasets still demands research efforts due to perceptual and performance issues. The support of graphics hardware for texture-based visualization allows efficient implementation of rendering techniques that can be combined with interactive sculpting tools to enable interactive inspection of 3D datasets. In this paper we report the development of three 3D interactive tools, eraser, digger and clipper, which specify regions within the volume to be discarded from rendering. Sculpting is accomplished by running special fragment programs that discard fragments based on geometric predicates. The interaction techniques we proposed were implemented using the virtual hand metaphor. The tools were evaluated by comparing the use of a 3D mouse against a conventional wheel-mouse for guiding volume and tools manipulation. Two-handed input was tested with both types of mouse and the results obtained indicate a preference for a combination of 2D and 3D mouse.	clipper;clipping (computer graphics);graphics hardware;interaction technique;scientific visualization;scroll wheel;volumetric display	Rafael Huff;Carlos A. Dietrich;Luciana Porcher Nedel;Carla Maria Dal Sasso Freitas;João Luiz Dihl Comba;Sílvia Delgado Olabarriaga	2006		10.1145/1128923.1128967	computer vision;simulation;computer science;computer graphics (images)	Visualization	-40.45676468390261	-32.645586589359695	71221
c3a7c0aa0416749eae7ff1778a2e506a41d49a24	opennero: a game platform for ai research and education	machine learning;ease of use;3d graphics	OpenNERO is an open source game platform designed for game AI research. The software package combines features commonly available in modern game engines (such as 3D graphics, physics simulation, 3D audio rendering, networked play, and a powerful scripting interface) with an easy to use API and tools for defining machine learning tasks, environments, and agents. Flexibility and ease of use of the system are demonstrated by following the process of creating a machine learning game from scratch. The scalability of the platform is tested through the implementation of the existing NERO machine learning game using the new tools.	3d computer graphics;application programming interface;artificial intelligence;dynamical simulation;game engine;machine learning;open-source video game;scalability;usability	Igor Karpov;John Sheblak;Risto Miikkulainen	2008			machine learning;artificial intelligence;first playable demo;simulation;human–computer interaction;game programming;game testing;computer science;game design;level design;game developer;game development tool;game design document	AI	-42.18137638816211	-33.624777237575834	71542
45dbf48677bf71ec4b947f0b8006fafbd19685d3	computational design and motion control for characters in the real world	mechanical characters;robotics;computational design;animation;control	Computer graphics techniques allow artists to realize their imaginative visions, leading to immersive virtual worlds that capture the imagination of audiences world-wide. And now, thanks to advancements in rapid manufacturing devices, tangible links between these vivid virtual worlds and our own can be created. But in order to unleash the full potential of this technology, a key challenge lies in determining the fundamental principles and design paradigms that allow digital content to be processed into forms that are suitable for fabrication. A particularly challenging task is that of creating physical representations of animated virtual characters. This paper discusses several techniques that can be applied towards this goal. In particular, a method for controlling the deformation behavior of real-world objects is described, and a computational design system that allows casual users to create animated mechanical characters is presented. In addition, this paper shows that control algorithms developed for physics-based character animation can also be applied to legged robots, allowing them to move with skill and purpose.	3d printing;algorithm;computation;computer graphics;digital recording;robot;virtual world	Stelian Coros	2013		10.1145/2522628.2541251	anime;simulation;computer science;artificial intelligence;multimedia;robotics;scientific control;computer graphics (images)	Graphics	-40.51988719525072	-35.20718181245936	71580
e527f141e0be80339ef4fcc35428b396eef843b5	information relationships in prolog programs: how do programmers comprehend functionality?	language comprehension;lenguaje programacion;programming language;prolog;relacion hombre maquina;man machine relation;comportement utilisateur;langage programmation;comprension lenguaje;comprehension langage;relation homme machine;user behavior;comportamiento usuario	Within the context of software development, psychological complexity is a measure of the difficulty a programmer experiences when interacting with a program. To date there has been little research to investigate models of program comprehension as a basis for developing psychological complexity metrics. The purpose of this study is to identify information relationships that reflect the organization of programmersu0027 cognitive models during the comprehension of unfamiliar PROLOG programs. An analysis of frequency and temporal ordering of subject protocols provides support for a two-model theory of PROLOG comprehension. During comprehension, programmers construct both a program model based on the detection of data structure relationships and a domain or real-world model based on the detection of function relationships.	programmer;prolog	David Bergantz;Johnette Hassell	1991	International Journal of Man-Machine Studies	10.1016/S0020-7373(05)80131-2	natural language processing;computer science;programming language;prolog;algorithm	Arch	-39.70729612522101	-28.282494799535563	71619
65a8845e0e524e0c71b18f9b96966d26ec49fcff	the visualization of 3d terrain based on vrml	virtual reality data visualisation geographic information systems rendering computer graphics;geographic information system;geographic information;3d terrain;prototypes;information technology;virtual reality;layout;3d rendering techniques;geographical information system;data visualisation;navigation;visualization;webgis;network servers;3d terrain visualization;gis;3d rendering techniques 3d terrain visualization geographical information system virtual reality gis;geographic information systems;three dimensional displays;image color analysis;solid modeling;ray tracing;vrml;dem 3d terrain visualization vrml webgis;digital elevation models;dem;terrain visualization;rendering computer graphics;3d graphics;java;visualization layout geographic information systems information technology prototypes rendering computer graphics java virtual reality network servers navigation	3D terrain visualization is always a hot topic for GIS (Geographical Information System) and VR (Virtual Realiy). The emergence of WebGIS makes the integration between network and GIS closer, and makes the sharing of geographic information more convenient and faster. However, it is still an urgent problem for 3D terrain visualization based on network for a long time. This paper puts forward an effective visualized method for 3D terrain based on network, combined with VRML and its specific 3D rendering techniques. Firstly, we introduced some basic concepts of terrain visualization. Secondly, described the traits and principles of VRML. Thirdly, discussed some techniques of 3D graphic rendering. Lastly, realized the 3D terrain visualization on network based on browser plug-in. all the work provided a reference for the sharing and the visualization of 3D terrain.	3d computer graphics;3d rendering;emergence;geographic information system;plug-in (computing);rendering (computer graphics);terrain rendering;vrml	Huirong Chen;Rencan Peng;Shujun Li;Caixia Yu	2009	2009 International Forum on Information Technology and Applications	10.1109/IFITA.2009.219	terrain rendering;computer vision;information visualization;computer science;multimedia;computer graphics (images)	Visualization	-35.384995455154616	-31.839011830731838	71687
b28ae44e3a69f8f822230bdc5fdcad0633eaf26f	kidpen: a stroke-based method for kid-style sketches synthesis from photos		Drawings of children usually have a unique charm due to their naïve and untutored styles. To easily produce the kid-style art, we proposed KidPen, a method that can transform realistic photos into kid-style sketches. Synthesizing kid-style sketches is challenging because children often draw objects with large shape changes and content simplification. We propose a stroke composition method based on a general cognitive process of human copy-drawing, thus the system is not restricted to specific object categories. The perceptual study shows that there is no significant difference of naturalness between the synthesized sketches of our method and the children's drawings.	cognition;list of minor characters in the matrix series;naivety;symbolic computation	Wan-Ling Yang;Mei-Yun Chen;Hong-Shiang Ling;Yu-Hsuan Huang;Ming Ouhyoung	2017		10.1145/3145749.3149434	computer vision;naturalness;artificial intelligence;perception;cognition;composition (visual arts);non-photorealistic rendering;computer science	AI	-39.36490522347394	-35.86617917145813	71776
c3417a6ce9be79fb90db6c66936c9ad15a3e7586	a survey of sketch based modeling systems	sketch based modeling interactive design sketch comprehension	As 3D technology, including computer graphics, virtual reality and 3D printing, has been rapidly developed in the past years, 3D models are gaining an increasingly huge demand. Traditional 3D modeling platforms such as Maya and ZBrush, utilize “windows, icons, menus, pointers” (WIMP) interface paradigms for fine-grained control to construct detailed models. However, the modeling progress can be tedious and frustrating and thus too hard for a novice user or even a well trained artist. Therefore, a more intuitive interface is needed. Sketch, an intuitive communication and modeling tool for human beings, becomes the first choice of modeling community. So far, various sketch-based modeling systems have been created and studied. In this paper, we attempt to show how these systems work and give a comprehensive survey.We review and categorize the systems in four aspects: the input, the knowledge they use, the modeling approach and the output. We also discuss about inherent challenges and open problems for researchers in the future.	3d modeling;3d printing;academy;autodesk maya;categorization;computer graphics;graph coloring;microsoft windows;pixologic zbrush;sketch;sketch-based modeling;super bit mapping;virtual reality;wimp (computing)	Chao Ding;Ligang Liu	2016	Frontiers of Computer Science	10.1007/s11704-016-5422-9	simulation;human–computer interaction;computer science;artificial intelligence;machine learning;database;multimedia;sketch recognition;algorithm	Graphics	-40.41266411633608	-33.8988532537736	71938
a7b3353b09cb70d3abe2919c634f44b23ed5edb5	contextualized preview of image map links	dual use of image space;hypertext navigation;smooth transition;multiple links;link preview;image maps	Previewing links in hypertext navigation helps reduce the cognitive overhead associated with deciding whether or not to follow a link. In this paper we introduce a new concept called Dual-Use of Image Space (DUIS) and we show how it is used provide preview information of image map links. In DUIS the pixels in the image space are used both as shading information as well as characters which can be read. This concept provides a mechanism for placing the text information related to images in context, that is, the text is placed within the corresponding objects. Prior to DUIS contextualized preview of links was only possible with text links. The following are the advantages of contextualized preview image map links: (1) Readers can benefit from both the text and the image without making visual saccades between the two. (2) The text does not obstruct the image as is the case in the existing techniques. (3) It is easy for the readers to associate the image and its corresponding image since the two are presented close to each other. The text in the image space may also contain links, and for this reason, it is possible to introduce multiple links for image maps.	correctness (computer science);hypertext;image map;overhead (computing);pixel;shading;the current;usability	Wallace Chigona;Thomas Strothotte	2002		10.1145/513338.513379	computer vision;image map;computer science;theoretical computer science;multimedia;world wide web	Vision	-34.57489262671197	-34.850854604451115	72010
4cd64acbe7d27e7d69a4b7990630e08f472c055f	actionscript cookbook - solutions and examples for flash developers				Joey Lott	2003			computer science;computer engineering;computer graphics (images)	SE	-47.020543602936065	-29.641605918592017	72146
280c0ccf83e345a0621dbab48e5967bb021c89f9	an augmented reality interface for visualizing and interacting with virtual content	tratamiento datos;systeme temps reel;image tridimensionnelle;interfase usuario;pistage;human computer interaction;donnee textuelle;procesamiento informacion;realite virtuelle;realidad virtual;modelo autorregresivo;dato textual;user interface;real time;rastreo;virtual reality;data processing;traitement donnee;tangible interface;natural interaction;autoregressive model;virtual heritage;man machine system;feasibility;learning systems;realite augmentee;learning system;realidad aumentada;general solution;analisis regresion;learning by example;senal video;signal video;information processing;textual data;video signal;tridimensional image;analyse regression;sistema hombre maquina;interface utilisateur;augmented reality human computer interaction tangible interfaces virtual heritage;real time system;regression analysis;sistema tiempo real;augmented reality;traitement information;modele autoregressif;indoor installation;instalacion interior;installation interieure;tangible interfaces;practicabilidad;faisabilite;apprentissage a partir d exemple;tracking;imagen tridimensional;systeme homme machine	In this paper, a novel AR interface is proposed that provides generic solutions to the tasks involved in augmenting simultaneously different types of virtual information and processing of tracking data for natural interaction. Participants within the system can experience a real-time mixture of 3D objects, static video, images, textual information and 3D sound with the real environment. The user-friendly AR interface can achieve maximum interaction using simple but effective forms of collaboration based on the combinations of human–computer interaction techniques. To prove the feasibility of the interface, the use of indoor AR techniques are employed to construct innovative applications and demonstrate examples from heritage to learning systems. Finally, an initial evaluation of the AR interface including some initial results is presented.	augmented reality;human–computer interaction;interaction technique;real-time locating system;surround sound;usability	Fotis Liarokapis	2006	Virtual Reality	10.1007/s10055-006-0055-1	augmented reality;simulation;data processing;information processing;computer science;virtual reality;tracking;multimedia;autoregressive model;user interface;regression analysis;computer graphics (images)	Visualization	-35.423306312757546	-27.619227287929995	72217
0261acdf2d66b8e157f1fdfabf6bc6545afc3815	interactions haptiques au sein de simulations dynamiques traitement dynamique des contacts et des chocs entre objets rigides	modelo dinamico;sistema interactivo;contacto mecanico;rendu image;penalised dynamics;realite virtuelle;realidad virtual;restitucion imagen;dynamic model;virtual reality;haptics;simulator;systeme conversationnel;dynamic contact;mechanical contact;contact mecanique;simulador;contact dynamique;interactive system;sensibilidad tactil;modele dynamique;image rendering;simulateur;contact force computation;non smooth dynamic;contacto dinamico;sensibilite tactile;tactile sensitivity	In this paper, we propose a state of the art on the processing of contacts and impacts in dynamic simulators. We point out the main principle of haptic rendering in interactive simulations in the context of virtual reality. We briefly describe the coupling problematic between the different modules that compose a haptic simulation. Then we propose a survey of the methods that process the interactions between rigid objects. For that we dissociate the methods of interaction detection, from the methods that compute the collision efforts. Then, we discuss the use penalized dynamics and non-smooth dynamics methods for haptic-based simulations.	interaction;simulation	Loïc Tching;Georges Dumont;Bruno Arnaldi;Jérôme Perret	2009	Technique et Science Informatiques	10.3166/tsi.28.953-981	simulation;computer science;artificial intelligence;virtual reality;haptic technology;computer graphics (images)	Logic	-35.227153411759446	-27.7857901858875	72572
6b9dba0c85dd6a62fbfef36faf5f74c904a8dd7e	special issue on hardware-accelerated rendering tephniques	hardware accelerator			Wolfgang Heidrich;Wolfgang Stuerzlinger	2002	J. Graphics, GPU, & Game Tools	10.1080/10867651.2002.10487565	embedded system;hardware acceleration;computer hardware;computer science;operating system;computer graphics (images)	Graphics	-47.17279231227007	-29.20789325020016	72663
b2e00bfb7af8c6bff89c8a3a05fdf1a5f4d2be61	a design and implementation of transcoder for displaying diagram data of multimedia title on web browser	web pages;diagram transcoder;design and implementation;vml;css layer	This thesis intends to suggest a way to convert it to VML code to play back diagram in internet browser to generate diagram contents used in production of multimedia title as Internet contents, and play it back. By converting multimedia title including excellent contents to web contents, web contents of high quality is intended to be generated. For this, it is rearranged to maintain spatial synchronization of object used inside title in web page by using the CSS Layer and generating VML code, location of playback of diagram contents and shape of the diagram are played back on web page through generation VML code. Through this research, diagram representation that is a difficult problem in converting multimedia title to web contents becomes possible. And furthermore, by using writing tool, diagram of web page can become designed by operating mouse without complex VML code.		DaeHyuck Park;Euisun Kang;Younghwan Lim	2007		10.1007/978-3-540-74477-1_34	static web page;vector markup language;computer science;operating system;web page;database;multimedia;world wide web	DB	-43.89009631399308	-27.511057999765963	72849
136406a63106658f6518dab67995af0d3315b647	a browser user interface for digital television	digital television;user interface	This paper discusses a process of designing and implementing a graphical user interface (GUI) for an XML browser. The process consists of four steps: a) a concept of a multimedia browser for television is de ned; b) the GUI requirements are de ned; c) a prototype is designed and tested with multimedia authoring tools; and d) nally, the prototype is implemented, which is done in Java, and integrated with an existing XML browser. The result is a browser application that can be run on digital television.	browser user interface;emoticon;exception handling;graphical user interface;handheld game console;java;one-way function;prototype;remote control;requirement;web content;xml	Juha P. Vierinen;Petri Vuorimaa	2001			user interface design;world wide web;10-foot user interface;internet television;shell (computing);multiple document interface;natural user interface;digital television;computer science;user interface	SE	-42.50559300604798	-29.623559127660858	72854
dcc2a4d782a293b286b63e88f4b2ce7f64e1c20e	asehm: a new transmission control mechanism for remote rendering system	remote rendering systems;3d digital models;mobile applications	Digital 3D models have emerged as a new type of multimedia following sound, image and video. This media type has been distributed and processed widely on desktop PCs. However, processing 3D models on mobile devices is more difficult, mainly due to their physical constraints. Though the remote rendering framework is able to make up for some deficiencies, previous methods based on this framework suffered from high transmission frequency, which just imposes a high power demand on mobile devices’ already limited battery life. In this paper, a new transmission control method, Adaptive Splitting and Error Handling Mechanism, is proposed to be integrated with canonical remote rendering system. Our mechanism is able to reduce transmission frequency by trading transmission with splitting operations. According to relevant research findings, reducing frequency will result in a decline in power consumption. Finally, the effectiveness of our method in terms of frequency reduction is validated by comparison with state of the art method.	3d modeling;algorithm;computation;coupling (computer programming);desktop computer;digital 3d;download;exception handling;hausdorff dimension;mobile device;personal digital assistant;rendering (computer graphics);retransmission (data networks);server (computing);viewport	Yajie Yan;Xiaohui Liang;Ke Xie;Qinping Zhao	2012	Multimedia Tools and Applications	10.1007/s11042-012-1116-y	embedded system;computer vision;real-time computing;simulation;telecommunications;operating system;computer security		-42.332067545112814	-35.290483801787055	72870
cadb674878a531196b8cd69cd4fe4156da48e5f7	key-music: an expert system environment for music composition			expert system	Antonio Camurri;Mauro Giacomini;A. Ponassi;Renato Zaccaria	1988			multimedia;expert system;musical composition;computer science	AI	-47.77320722288093	-33.26225140019472	72929
24999ab2ac2c2ce5fccf4a82f369291111826025	a knowledge-based tool for user interface evaluation and its integration in a uims	user interface evaluation;knowledge base		knowledge-based systems;user interface management systems	Jonas Löwgren;Tommy Nordqvist	1990			user interface design;user;human–computer interaction;computer science;knowledge management;user interface;graphical user interface testing	Logic	-42.278823299740495	-28.813588012770712	73010
fd089f7594bbe4d6b9d8a0f38dac7680ae94e3cb	real-time data-driven interactive rough sketch inking		"""We present an interactive approach for inking, which is the process of turning a pencil rough sketch into a clean line drawing. The approach, which we call the Smart Inker, consists of several """"smart"""" tools that intuitively react to user input, while guided by the input rough sketch, to efficiently and naturally connect lines, erase shading, and fine-tune the line drawing output. Our approach is data-driven: the tools are based on fully convolutional networks, which we train to exploit both the user edits and inaccurate rough sketch to produce accurate line drawings, allowing high-performance interactive editing in real-time on a variety of challenging rough sketch images. For the training of the tools, we developed two key techniques: one is the creation of training data by simulation of vague and quick user edits; the other is a line normalization based on learning from vector data. These techniques, in combination with our sketch-specific data augmentation, allow us to train the tools on heterogeneous data without actual user interaction. We validate our approach with an in-depth user study, comparing it with professional illustration software, and show that our approach is able to reduce inking time by a factor of 1.8X, while improving the results of amateur users."""	convolutional neural network;line drawing algorithm;real-time data;real-time locating system;shading;simulation;usability testing;vagueness	Edgar Simo-Serra;Satoshi Iizuka;Hiroshi Ishikawa	2018	ACM Trans. Graph.	10.1145/3197517.3201370	real-time data;computer vision;pencil (mathematics);computer graphics (images);computer science;artificial intelligence;sketch	Graphics	-38.241578680594756	-32.707208693068466	73229
0ec60c4bcaa9314cfad2555ce0c4818e0bba2ac6	teaching, analyzing, designing and interactively simulating sliding mode control		This paper introduces an interactive methodology to analize, design, and simulate sliding model controllers for  $\mathbb {R}^{2}$  linear systems. This paper reviews sliding mode basic concepts and design methodologies and describes an interactive tool which has been developed to support teaching in this field. The tool helps students by generating a nice graphical and interactive display of most relevant concepts. This fact can be used so that students build their own intuition about the role of different parameters in a sliding mode controller. Described application has been coded with Sysquake using an event-driven solver technique. The Sysquake allows using precise integration methods in real time and handling interactivity in a simple manner.	control theory;event-driven programming;graphical user interface;interactivity;linear system;simulation;solver;sysquake;vii	Ramon Costa-Castell&#x00F3;;Niliana Carrero;Sebastián Dormido;Enric Fossas	2018	IEEE Access	10.1109/ACCESS.2018.2815043	control theory;interactivity;sliding mode control;nice;visualization;computer science;control engineering;linear system;trajectory;distributed computing;solver	Robotics	-37.18073614611006	-37.2905689361673	73433
30e0b849b7cfc980d0b1423eee00a80d12110aa8	lattice: strategies for and against control in an improvisation instrument		Lattice is a laptop improvisation instrument designed to balance user control with unpredictable behavior. The operator triggers synthesis events, specifying their timbre, and upper and lower boundaries on their frequency and duration. The specific frequency envelope and duration of the event are then algorithmically determined, as are all other synthesis parameters relevant to the chosen timbral type. The resulting tension between performer control and algorithmic specification leads to unfamiliar and interesting improvisational situations. The implementation of Lattice also responds to design goals including portability, learnability, and expressive sound synthesis. This paper describes the instrument and its implementation.	algorithm;laptop;learnability;software portability;user interface	Christopher Burns	2005			psychology;simulation;management science;communication	PL	-46.693536245377516	-36.12485518540179	73449
9be0d9e3bf9058d188cd4450b7009fbe7dd7b5e1	interactive 3-d indoor modeler for virtualizing service fields		This paper describes an interactive 3-D indoor modeler that effectively creates photo-realistic 3-D indoor models from multiple photographs. This modeler supports the creation of 3-D models from photographs by implementing interaction techniques that use geometric constraints estimated from photographs and visualization techniques that help to easily understand shapes of 3-D models. We evaluated the availability and usability by applying the modeler to model service fields where actual workers provide services and an experience-based exhibit. Our results confirmed that the modeler enables the creation of large-scale indoor environments such as hot-spring inns and event sites at a relatively modest cost. We also confirmed that school children could learn modeling operations and create 3-D models from a photograph for approximately 20 min because of the easy operations. In addition, we describe additional functions that increase the effectiveness of 3-D modeling based on knowledge from service-field modeling. We present applications for behavior analysis of service workers and for 3-D indoor navigation using augmented virtuality (AV)-based visualization realized by photo-realistic 3-D models.	3d modeling;experiment;first-person (video games);interaction technique;maxima and minima;mixed reality;relevance;tracking system;usability;virtuality (gaming)	Tomoya Ishikawa;Kalaivani Thangamani;Masakatsu Kourogi;Andrew P. Gee;Walterio W. Mayol-Cuevas;Jungwoo Hyun;Takeshi Kurata	2011	Virtual Reality	10.1007/s10055-011-0202-1	computer vision;simulation;multimedia	HCI	-39.18985246788039	-35.21039418131226	73458
6df6e4145264f68dba127b6c23e72868619ec072	an advanced platform to speed up the design of multilingual dialog applications for multiple modalities	grammar;systeme temps reel;methode semi automatique;adaptability;adaptabilite;base donnee;haute performance;design process;voicexml;multiple modalities;semiautomatic method;automatic system;normalisation;flexibilidad;mixed initiative;generacion automatica;diagramme etat;xml language;metodo semi automatico;database;base dato;portability;automatic generation;automatic dialog systems generation;adaptabilidad;data model;data center;generation automatique;evaluation subjective;diagrama estado;sistema automatico;grammaire;multilinguality;robustesse;portabilite;normalizacion;xml;systeme automatique;alto rendimiento;state diagram;robustness;flexibilite;real time system;management tool;dialog management tools;sistema tiempo real;multilinguisme;subjective evaluation;high performance;gramatica;standardization;langage xml;lenguaje xml;multilingualism;flexibility;portabilidad;multilinguismo;robustez;evaluacion subjetiva;real time systems	In this paper, we present a complete platform for the semiautomatic and simultaneous generation of human–machine dialog applications in two different and separate modalities (Voice and Web) and several languages to provide services oriented to obtaining or modifying the information from a database (data-centered). Given that one of the main objectives of the platform is to unify the application design process regardless of its modality or language and then to complete it with the specific details of each one, the design process begins with a general description of the application, the data model, the database access functions, and a generic finite state diagram consisting of the application flow. With this information, the actions to be carried out in each state of the dialog are defined. Then, the specific characteristics of each modality and language (grammars, prompts, presentation aspects, user levels, etc.) are specified in later assistants. Finally, the scripts that execute the application in the real-time system are automatically generated. We describe each assistant in detail, emphasizing the methodologies followed to ease the design process, especially in its critical aspects. We also describe different strategies and characteristics that we have applied to provide portability, robustness, adaptability and high performance to the platform. We also address important issues in dialog applications such as mixed initiative and over-answering, confirmation handling or providing long lists of information to the user. Finally, the results obtained in a subjective evaluation with different designers and in the creation of two full applications that confirm the usability, flexibility and standardization of the platform, and provide new research directions. 2005 Elsevier B.V. All rights reserved. 0167-6393/$ see front matter 2005 Elsevier B.V. All rights reserved. doi:10.1016/j.specom.2005.11.001 q This work was partly supported by the European Commission’s Information Society Technologies Programme under contract no. IST2001-32343. The authors are solely responsible for the contents of this publication. * Corresponding author. Tel.: +34 91 3367366x4209; fax: +34 91 3367323. E-mail addresses: lfdharo@die.upm.es (L.F. D’Haro), cordoba@die.upm.es (R. de Córdoba), jfl@die.upm.es (J. Ferreiros), shamerich@harmanbecker.com (S.W. Hamerich), vschless@harmanbecker.com (V. Schless), bkladis@knowledge-speech.gr (B. Kladis), vschubert@harmanbecker.com (V. Schubert), okocsis@knowledge-speech.gr (O. Kocsis), sigel@faw.uni-ulm.de (S. Igel), pardo@ die.upm.es (J.M. Pardo). 864 L.F. D’Haro et al. / Speech Communication 48 (2006) 863–887	command-line interface;data model;fax;modality (human–computer interaction);real-time computing;real-time locating system;software portability;state diagram;usability;dialog	Luis Fernando D'Haro;Ricardo de Córdoba;Javier Ferreiros;Stefan W. Hamerich;Volker Schless;Basilis Kladis;Volker Schubert;Otilia Kocsis;Stefan Igel;José Manuel Pardo	2006	Speech Communication	10.1016/j.specom.2005.11.001	xml;simulation;real-time operating system;speech recognition;computer science;artificial intelligence;dialog system	AI	-37.41071740068333	-27.041423087843278	73658
ea5ec5b6678fb721fd7e3c332a9b724e8638fd8d	beyond stereoscopic: combining spatial acquisition technologies in real-time engines to produce immersive virtual reality expereinces for the dissemination of archaeological research	virtual studio production;vr 360 film;pitoti valcamonica;3d scanning;rock art	The foundation of this paper was the 3D-Pitoti.eu project 20132016. Its aim was to acquire archaeological rock-art from Val-camonica in the Southern Alps by using modern 3D scanning technologies, building on previous work with 2D animation of rock art[4]. The project also aimed to capture multi-scaled 3D data from the macroscopic level of airborne surveillance down to the sub millimetre of coloured laser scans. These static scans where used to formulate ways of (re-)animating the figures that are depicted in these ancient rock-art artefacts. The use of motion-capture, hand animation and novel volumetric recording systems lead to complex datasets that we combined in a novel workflow utilizing a real-time game engine as our virtual production studio. The final challenge was to create Pitoti Prometheus a narrated 3D 360° VR film experience for final dissemination of our work for the public and archaeological research sector.	3d film;3d scanner;airborne ranger;game engine;immersion (virtual reality);motion capture;prometheus;real-time transcription;stereoscopy;virtual reality	Frederick Baker;Marcel Karnapke	2016	2016 International Conference on 3D Imaging (IC3D)	10.1109/IC3D.2016.7823466	simulation;engineering;multimedia;computer graphics (images)	Visualization	-35.76656417641886	-31.87948471255793	73901
af09ea73921cdb63d32bbb3ced76c619848e4e3c	combining physical modeling and additive synthesis as a mapping strategy for realtime control		This paper presents a mapping strategy for the control of a signal-based digital synthesis model of the clarinet. This strategy combines the use of a synthesis model based on a physical model with the use of a signal model based on additive synthesis. From the output of the physical model, specific sound descriptors are extracted and used to control an additive synthesis model based on the analysis of natural sounds. This approach allows to benefit both from the control quality of the physical modeling with the sound quality provided by the additive resynthesis of natural sounds. However, some improvements are still required to better interface and set the two models, particularly concerning transients.		Philippe Guillemain;Vincent Verfaille	2007			control engineering;real-time computing;simulation	Graphics	-45.46847080032661	-34.835419490793576	73942
c2b4b7e59aa9a409006e3864c2a59c9a8b7714b4	ez3.js: a robust and flexible webgl-based 3d engine	ez3 js;web browser;webgl;javascript;3d graphics	We present EZ3.js, a modern, robust and flexible WebGL-based JavaScript engine for developing interactive and real-time 3D graphics applications on the web. This engine tackles the problem of rendering 3D models with complex instructions, non-portable code, and well documented code focus on software engineers which works over web browsers. EZ3.js offers a simple code-structure to load, transform and render 3D models applying different effects over them, based on efficient data structures and rendering algorithms. Template models for diffuse and specular reflection are included, joined with local lighting and texture mapping techniques. The full management of resources is optimized to obtain an outstanding performance, also reducing the memory consumption. Our engine is an open source library which provides an available entire documentation that can be extended in future for the graphics community of developers, being a great contribution in comparison to similar existing web 3D engines. Tests executed evaluated the performance, the memory consumption, and the number of required lines of code to render complex models, showing remarkable aspects of EZ3.js as a WebGL-based 3D engine.	3d computer graphics;3d modeling;algorithm;data structure;documentation;game engine;javascript engine;open-source software;real-time web;software engineer;source lines of code;texture mapping;webgl	Andres Alvarez;Carlos Zapata;Esmitt Ramírez	2016	2016 XLII Latin American Computing Conference (CLEI)	10.1109/CLEI.2016.7833404	embedded system;computer science;world wide web;computer graphics (images)	Graphics	-40.89350063623192	-32.52411958684452	74145
fab150c8298302cba8ee7c2ef978a1a0bbb15fb2	the virtual mail system	electronic mail;user interfaces electronic mail virtual reality;user interface;virtual reality;virtual friend virtual mail system v mail cave virtual environment mail messages user interface;postal services;virtual environment;user interfaces	"""Companies conduct world-wide collaborations which often involve asynchronous work with group members in di erent time-zones. In asynchronous work, the participants can easily waste days just clarifying questions because feedback is slow. For e cient collaborations, accurate \handing-o """" of work between participants is crucial. Therefore, messages exchanged between collaborators should not contain ambiguities. One cause for ambiguity may be in the mismatch between the content domain and the medium used to convey the message regarding that domain. For example, if the task is to design buildings, the explanation of ideas usually involves the description of objects and features in a spatial context. This is di cult to accomplish even with advanced e-mail systems that allow the inclusion of pictures, animation and audio as attachments. In these systems, the e-mail messages are still detached from the environment in which they were initially recorded. In VR we have the opportunity to bridge this by allowing the same environment to be the medium over which a recording was originally created and played back. When a play-back involves the recording of the remote participant's virtual presence (an avatar) and actions, the avatar can be regarded as a surrogate of the original recorder of the message."""	asynchronous i/o;attachments;email	Tomoko Imai;Andrew E. Johnson;Jason Leigh;David E. Pape;Thomas A. DeFanti	1999		10.1109/VR.1999.756930	human–computer interaction;computer science;operating system;virtual reality;multimedia;push email;user interface;world wide web	HCI	-48.11482217981022	-36.45712142559349	74187
f3e34165969a615409eac0a69fc52bbc867c5cef	interaction et contexte dans les interfaces zoomables	vues focus contexte;visualisation d information vues focus contexte interfaces zoomables transparence menus contextuels control menus en francais;transparence;interfaces zoomables;menus contextuels;interaction homme machine;visualisation d information;control menus	Many interactive computer systems use menus as an important part of their interface. Menus allow users to select operations but not to control their execution. A second interactor, such as a dialog box, has to be used to control the chosen operation and thus complete the interaction. This decomposition of what is a single action from the user’s point of view into two distinct steps slows down interaction with computer systems. This thesis proposes a new contextual pop-up menu, called aControl Menu, that includes proportional control of the chosen operation with immediate feedback. Using this menu gives a more fluid control of complex interfaces and has the advantage of an expert form of use that is very similar to, and thus easily learnt from, the novice usage. Interaction with databases and navigation within large information spaces are important tasks in many applications. Many visualization systems cause user disorientation as users find it difficult to understand their position within the information space and to locate desired information. This thesis proposes several new contextual aids for Zoomable User Interfaces that address these issues. The first aid, a hierarchical view of the information space, helps users understand their current position and the location of the desired information, and accelerates navigation. The second type of aid uses dynamically generated transparent and temporary views that are created and controlled by users in a single gesture. These interactive views overlay the current view of the focus with contextual or historical information which shows users what surrounds the current view or the route taken to arrive at that view. The effective use of these new aids requires a tight coupling between interaction and presentation which is achieved via the use of Control Menus.	database;digital zoom;user interface;dialog	Stuart Pook	2001			art;performance art	HCI	-36.72884074840733	-28.531239580357244	74529
0a3db1e904ed72993ebdc601405d61459b0201a4	kahip v0.53 - karlsruhe high quality partitioning - user guide		This paper severs as a user guide to the graph partitioning framework KaHIP (Karlsruhe High Quality Partitioning). We give a rough overview of the techniques used within the framework and describe the user interface as well as the file formats used. Moreover, we provide a short description of the current library functions provided within the framework.	graph partition;user interface	Peter Sanders;Christian Schulz	2013	CoRR		graph partition;computer science;database;user interface;file format	HPC	-43.23859081219427	-26.81828183862644	74572
ee6288d289d200b2e221634c6344083e04bb7554	sound analyser: a plug-in for real-time audio analysis in live performances and installations		Real-time audio analysis has great potential for being used to create musically responsive applications in live performances. There have been many examples of such use, including sound-responsive visualisations, adaptive audio effects and machine musicianship. However, at present, using audio analysis algorithms in live performance requires either some detailed knowledge about the algorithms themselves, or programming – or both. Those wishing to use audio analysis in live performances may not have either of these as their strengths. Rather, they may instead wish to focus upon systems that respond to audio analysis data, such as visual projections or sound generators. In response, this paper introduces the Sound Analyser – an audio plug-in allowing users to a) select a custom set of audio analyses to be performed in real-time and b) send that information via OSC so that it can easily be used by other systems to develop responsive applications for live performances and installations. A description of the system architecture and audio analysis algorithms implemented in the plug-in is presented before moving on to two case studies where the plug-in has been used in the field with artists.	algorithm;audio and video interfaces and connectors;audio plug-in;emoticon;performance;plug-in (computing);real-time cmix;real-time transcription;systems architecture	Adam M. Stark	2014			human–computer interaction;multimedia;analyser;systems architecture;computer science;plug-in;audio analyzer	HCI	-44.878199581923056	-32.764070278274794	74774
fc69fe361376acaa8c229ee00adb78f96723aedb	argos: an advanced in-vehicle data recorder on a massively sensorized vehicle for car driver behavior experimentation	gaze;automatic driving;traffic accident;facteur securite;pedestrian safety;advanced vehicle control and safety systems;accident circulation;superviseur;automovil;advanced driver assistance system;poison control;behavioral analysis;conduccion vehiculo;etude experimentale;injury prevention;behavioural sciences computing;in vehicle data recorder ivdr;ivdr;conduccion automatica;biological system modeling;automated highways;conduite vehicule;technique video;safety literature;enregistreur donnee;conduite automatique;vehicle driving;mirada;invehicle data recorder;traffic safety;tecnica video;injury control;in vehicle data recorder ivdr advanced driver assistance systems adas advanced vehicle control and safety systems avcss driver behavior feedback;vehicle driver;alphanumerical data;incar light;car driver behavior experimentation;user assistance;factor seguridad;home safety;regard;injury research;safety abstracts;feedback;accidente trafico;road safety automated highways behavioural sciences computing;supervisor;human factors;assistance utilisateur;senal video;signal video;route;accidents;global positioning system;automobile;motor car;environmental data;computer aided manufacturing;analyse comportementale;occupational safety;audio stimuli;seguridad trafico;asistencia usuario;safety;audio stimuli argos invehicle data recorder massively sensorized vehicle car driver behavior experimentation traffic safety ivdr alphanumerical data environmental data incar light;advanced vehicle control and safety systems avcss;advanced driver assistance systems adas;carretera;video signal;conductor vehiculo;safety research;video technique;accident prevention;analisis conductual;violence prevention;safety factor;registrador datos;humans;highway;vehicle safety;bicycle safety;securite trafic;driver behavior;power system modeling;road safety;poisoning prevention;falls;estudio experimental;ergonomics;suicide prevention;argos;massively sensorized vehicle	A crucial factor in traffic safety is driver behavior. A better understanding of driver actions will help in determining the most common reasons for car accidents. Therefore, research in this field helps to reduce accidents due to driver distraction. This paper presents Argos, which is a complex and powerfully computerized car to help researchers in the study of car driver behavior. The Argos system is an improved in-vehicle data recorder (IVDR) that allows recording many kinds of alphanumerical data such as the speed (vehicle data), the point of gaze (driver data), or the current distance to lateral road marks (environmental data). In addition, Argos can record up to nine simultaneous video images which are synchronized with the alphanumerical data. Argos can also generate and record different kinds of in-car light and audio stimuli, allowing an experiment supervisor to interact or to schedule specific actions to take place during an experiment.	algorithm;baseline (configuration management);complex system;data logger;device driver;experiment;ivdr;image compression;image processing;lateral thinking;open-source software;operating system;parallel computing;pattern recognition;requirement;sensor;software prototyping;source lines of code;user interface	Antonio Pérez;Maria Isabel García;Manuel Nieto;José Luis Pedraza;Santiago Rodríguez;Juan Zamorano	2010	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2010.2046323	route;embedded system;simulation;global positioning system;factor of safety;computer science;engineering;suicide prevention;human factors and ergonomics;injury prevention;data logger;feedback;computer security	Robotics	-34.37632616937178	-28.59153840164478	74807
fd8f8d95f23558ffd2ebb8988f2721aec2b0532c	computer graphics in textiles and apparel modeling	computer graphics;computer graphics textile industry robotics and automation humans fabrics clothing industry computer aided manufacturing shape composite materials animation;computer graphic;shape;computer aided manufacturing;animation;fabrics;humans;composite materials;clothing industry;robotics and automation;textile industry	oven cloth has been part of the human experience for thousands of years. The oldest known relic of human industry is a piece of hemp fabric dating back approximately 8,000 years. Given the scope of human exposure to cloth, it’s surprising how little is known about its properties. This “old-fashioned” material, which we take for granted every day, is actually quite complex. Its complexity brings a versatility that makes it useful not only for clothing and sails, but also as an industrial material. Woven cloth is an integral component of certain composite materials used to reinforce structural elements and provide a sturdy “skin” for many advanced aircraft and vehicles. The reasons for modeling textiles and apparel are numerous. In the computer animatiodentertainment field, the demand for more realistic virtual actors increases the need for faster, better, and easier tools to clothe them. The tools are being developed. It is now time to move them out of the research labs into the entertainment studios for use with the computer-generated characters we see on television and in the movies. The need for textile modeling technologies is even greater within the CAD/CAM field. Certainly, many of the clothing modeling tools developed for the entertainment industry can also be used to design real clothing. Future apparel CAD systems should allow fashion designers to experiment easilywith a variety of fabrics and patterns on a 3D dynamic virtual mannequin before the actual garment is manufactured. Once the design is complete, it can be sent to an automated loom that weaves the fabric and then to a computer-controlled cutting table that nests and cuts the appropriate patterns. Customers might even try on a virtual garment in an augmented reality environment before having it custom sewn. The textile modeling requirements for industrial applications are quite different from the requirements for entertainment and apparel design applications. Woven cloth is often used within composite materials because it can be formed into highly complex shapes while still maintaining its outstanding mechanical and structural properties. In the composites domain, cloth is not allowed to freelydrape over another object. Instead David E. Breen California Institute of Technology	augmented reality;cloth modeling;complexity;computer graphics;computer-aided design;computer-generated holography;loom;requirement;television;the movies;virtual actor	David E. Breen	1996	IEEE Computer Graphics and Applications	10.1109/MCG.1996.536272	anime;simulation;textile industry;shape;computer science;geometry;computer graphics;computer-aided manufacturing;computer graphics (images)	Graphics	-40.18831104127471	-34.45711782213509	74879
361197db34c30174bca38aa35659afb765481bd1	augvox: an augmented voice instrument	voice;arduino;computer music;interaction design	As a musical instrument, the human voice offers a significant range in terms of sonic results. Control of this particular instrument is aided by a natural familiarity, and it takes little concentration to quickly produce sounds of vastly differing musical characteristics. The advent of computer music has afforded manipulative processing that delivers auditory results that would never be attainable in the natural world. This project attempts to bridge the gap between natural generation of vocal sounds, and complex digital processing, by developing a simple, ergonomic instrument that places control of the sonic output in the hands of the artist.	digital data;human factors and ergonomics;interaction;jaquet-droz automata	Brian Tuohy	2013		10.1145/2460625.2460697	speech recognition;acoustics;engineering;communication	ML	-46.43404910690628	-36.153898969913676	74946
9720a963451143edb492b8a067dc59f0a61d752f	utilising context ontology in mobile device application personalisation	context awareness;mobile device;user interface;context studio;application personalization;rule;ontology	Context Studio, an application personalisation tool for semi-automated context-based adaptation, has been proposed to provide a flexible means of implementing context-aware features. In this paper, Context Studio is further developed for the end users of small-screen mobile devices. Navigating and information presentation are designed for small screens, especially for the Series 60 mobile phone user interface. Context ontology, with an enhanced vocabulary model, is utilized to offer scalable representation and easy navigation of context and action information in the UI. The ontology vocabulary hierarchy is transformed into a folder-file model representation in the graphical user interface. UI elements can be directly updated, according to the extensions and modifications to ontology vocabularies, automatically in an online system. A rule model is utilized to allow systematic management and presentation of context-action rules in the user interface. The chosen ontology-based UI model is evaluated with a usability study.	graphical user interface;mobile device;mobile phone;personalization;s60 (software platform);scalability;semiconductor industry;usability testing;vocabulary;on-line system	Panu Korpipää;Jonna Häkkilä;Juha Kela;Sami Ronkainen;Ilkka Känsälä	2004		10.1145/1052380.1052399	human–computer interaction;computer science;ontology;operating system;ontology;mobile device;multimedia;context model;ontology-based data integration;user interface;world wide web;process ontology;suggested upper merged ontology	HCI	-41.02591860771338	-25.783149497276366	74953
414ec5be820c68abfdbf61728a3ffb2ac0a296a9	physically-based audio rendering of contact	audio user interfaces;rendering computer graphics audio signal processing audio user interfaces digital simulation computer animation signal synthesis;real time sound processing software physically based audio rendering real time contact sounds synthesis interactive simulations interactive animation physically based impact model acoustic characteristics colliding objects model physical parameters spatial dynamics resonating object position dependent interaction simulation sound synthesis module low cost platforms human computer interfaces;audio signal processing;algorithm design and analysis shape finite element methods costs monitoring usability facial animation multimedia systems reverberation production;sound synthesis;real time;spatial dynamic;interactive simulation;signal synthesis;computer animation;rendering computer graphics;digital simulation	This paper describes an algorithm for real-time synthesis of contact sounds for interactive simulations and animation. The algorithm is derived from a physically-based impact model, and the acoustic characteristics of colliding objects can be realistically simulated by properly adjusting the physical parameters of the model. A technique for describing the spatial dynamics of a resonating object is proposed, which allows simulation of position-dependent interaction. It is shown that the numerical implementation leads to an efficient sound synthesis module, that runs in real-time on low cost platforms. The effectiveness of the model is demonstrated, and its applications are discussed.	acoustic cryptanalysis;algorithm;numerical analysis;real-time clock;real-time locating system;simulation	Federico Avanzini;Matthias Rath;Davide Rocchesso	2002		10.1109/ICME.2002.1035636	physically based animation;computer vision;simulation;audio signal processing;computer science;interactive skeleton-driven simulation;computer animation;multimedia;computer graphics (images)	Graphics	-45.01915563095827	-34.7365029337674	75143
d70a01566d287447d51cf8b54238f5516e6c71be	edwordle: consistency-preserving word cloud editing		We present EdWordle, a method for consistently editing word clouds. At its heart, EdWordle allows users to move and edit words while preserving the neighborhoods of other words. To do so, we combine a constrained rigid body simulation with a neighborhood-aware local Wordle algorithm to update the cloud and to create very compact layouts. The consistent and stable behavior of EdWordle enables users to create new forms of word clouds such as storytelling clouds in which the position of words is carefully edited. We compare our approach with state-of-the-art methods and show that we can improve user performance, user satisfaction, as well as the layout itself.	algorithm;alignment;borg scale rating of perceived exertion score 17;customize;digitorenocerebral syndrome;experiment;inline linking;interaction;interactivity;lasso;microsoft word for mac;multi-touch;muscle rigidity;numerous;simulation;tag cloud;usability	Yunhai Wang;Xiaowei Chu;Chen Bao;Lifeng Zhu;Oliver Deussen;Baoquan Chen;Michael Sedlmair	2018	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2017.2745859	data visualization;visualization;theoretical computer science;storytelling;tag cloud;cloud computing;semantics;rigid body;computer science	Visualization	-36.16988057112479	-34.56298625342766	75442
67f3bf500a729d979c51805dbbb816cbf6eff1e8	a constraint satisfaction approach to predicting skilled interactive cognition	finite domain constraints;usability evaluation;time course;constraint satisfaction;user modeling;cognitive architecture;tools for usability evaluation;user model	In this paper we report a new approach to generating predictions about skilled interactive cognition. The approach, which we call Cognitive Constraint Modeling, takes as input a description of the constraints on a task environment, on user strategies, and on the human cognitive architecture and generates as output a prediction of the time course of interaction. In the Cognitive Constraint Models that we have built this is achieved by encoding the assumptions inherent in CPM-GOMS as a set of constraints and reasoning about them using finite domain constraint satisfaction.	cpm-goms;cognition;cognitive architecture;constraint logic programming;constraint satisfaction;goms	Alonso H. Vera;Andrew Howes;Michael McCurdy;Richard L. Lewis	2004		10.1145/985692.985708	constraint programming;simulation;user modeling;computer user satisfaction;constraint satisfaction;human–computer interaction;computer science;knowledge management;machine learning	AI	-36.90710546342088	-27.267232032663788	75520
99161e20dced327cd69657b3a53e34c0eb05e178	an adaptation method by feedback in an evolutionary hypermedia system	hypermedia systems;user adaptation;software evolution;adaptive method	In this paper, we describe a model for hypermedia systems that aims to reduce the main problems present in the development of these systems. This model, called SEM-HP, is defined as systemic, semantic, adaptive and evolutionary. It is systemic because it separates the aspects of representation, presentation, navigation and adaptation of the hypermedia into four subsystems; semantic because it makes explicit the meaning of the offered information; adaptive because its functioning varies according to the features of the user navigating it; and evolutionary because it supports mechanisms that allow the author to restructure the hypermedia in a flexible and consistent way. The paper also presents and details a method called Adaptation by Feedback, which analyzes and integrates the navigational behaviour of the users browsing the hypermedia, compares the structures traced by the users with those previously defined by the author, and suggests the necessary modifications so that by using the evolutionary capacity of the system the author can bring both structures closer.	hypermedia;systemics	Nuria Medina-Medina;Fernando Molina-Ortiz;Lina García-Cabrera	2006		10.1145/1145581.1145617	simulation;computer science;software evolution;software engineering;multimedia;world wide web	Robotics	-39.00224616946481	-26.109080690840695	75985
5b07c48b8e8b82586ce3a9a64ba6b12564b5423a	experiences integrating cooperative multimedia applications into www			www	David Fernández;Encarna Pastor;Luis Bellido	1996			human–computer interaction;multimedia;computer science	DB	-45.529200972758844	-25.80064670978673	76068
b0c25e8fd88aec18f2d58fa39ec52ee0c2b144db	a pda-based system for recognizing buildings from user-supplied images	image generation;information service;before present;mobile user	  This paper reports on research into and development of portable hardware that will enable users in the field to send images,  and associated positional data from a PDA to a server for processing. The central aim is to provide navigational and informational  services to an urban mobile user based on building recognition. The paper begins by describing the hardware before presenting  research into server-side building recognition methods that operate by comparing user-supplied images with images generated  by an existing 3d virtual model.    	personal digital assistant	Wanji Mai;Gordon Dodds;Chris Tweed	2003		10.1007/978-3-540-24641-1_11	computer vision;simulation;geography;multimedia	Robotics	-33.98721562284059	-36.064121789368876	76118
0beff6a04477fec91c67959c88bd09140e7e7e22	chirp: the computer human interface rapid prototyping and design assistant toolkit	human interface;rapid prototyping	This presentation includes a description and a demonstration video of the Computer Human Interface Rapid Prototyping (CHIRP) Toolkit. The CHIRP Toolkit architecture, planned capabilities, and the way in which computer human interface (CHI) designers interact with it to build and mtilf y functional interactive scenarios are described. The presentation includes a sample of case studies that illustrate how the evolving CHIRP toolkit is being used to support CHI design for real world systems. INTRODUCTION Briefly, the CHIRP Toolkit is an integrated set of tools supporting the rapid development of modem gmphical user interfaces (GUIS), concept of operations demonstrations, and training simulations. It provides quick access to basic OSF Motif widgets and gadgets (e.g., containers, buttons, icons, etc.), as well as higher-level prefabricated reusable interface modules such as a panel of buttons, a complete menu structure, a scrollable help window. Reusable graphical objects (e.g., maps, globes, images, graphs, etc.) stored in application libraries, and all the available interactive graphic application creation utilities (e.g., 2D mercator and globe mapping) can be quickly summoned to the “work area” via a simple point-and-click action. A form of visual programming is supported by providing the designer with a direct manipulation interface to position and size objects to create the “look of the user interface. In many cases, the designer can lay out a screen, assign a window for display from an application, assign menu areas for selectable options, each of which call an application function which results in an alteration of the display (i.e., create and test-drive interactive graphical interface without writing any actual code). DESIGN ASSISTANT Commercially available X Window System GUI development tools do not actively assist developers in the design of quality user interfaces. As a resul~ many of the Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercmi advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copy!ng IS by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee andlor specific permission. CH194 Compan!on-4/94 Boston, Massachusetts USA Q 1994 ACM 0-89791-651-4/94/01 13... $3.50 X Windows-based user interfaces produced to-date are characterized by (1) operator fatigue due to the use illegible fonts and color combinations, (2) operator confusion due to graphical clutter and pm screen layout, and (3) excessive operator error due to poor design and inconsistent implementation of critical interaction techniques. There is a real need for an embedded CHI Design Assistant that can take advantage of a library of “human factored reusable GUI design components, as well as a new CHI standards and guidelines knowledgebase to actively assist the u[ser interface designer during critical aspects of the design process. CHIRP development activily is currently concentrated on the creation of a Design Assistant that will actively assist the interface designer in making use of the best available CHI knowledge to ensure compliance with the principles of good design. Early Design Assistant implementation is focusing on the encoding of knowledge found in documented standards andl guidelines related to color usage, screen layout, window management and action vocabulary. A search of the technical literature failed to uncover developments directly related to computer-assisted design of X Windows/Motif GUIS. It di~ however, yield useful information for the design and implementation of a cooperative design assistant, Previous attempts to provide designers in various technical domains with computerbased assistance have shown the prime importance of system design driven by the needs of the designers. Therefore, group discussions were conducted with more than a dozen professiomds responsible for designing GUIS, before actual design and implementation commenced. Based upon the findings of this activity it was decided to focus early Design Assistant implementation on the encoding of knowledge derived from previous GUI design efforts, and from well-defined and documented guidelines related to basic screen layout and color usage. It was (concluded that the return on investment from developing a system that effectively deals with screen layout and color alone could be considerable if one takes into account the sheer loss of	chi;clutter;computer-aided design;direct manipulation interface;embedded system;graph (discrete mathematics);graphical user interface;image;integrated development environment;interaction technique;knowledge base;library (computing);map;microsoft windows;modem;motif;point and click;rapid prototyping;simulation;systems design;user error;visual programming language;vocabulary;window manager;world-system;x window system	Robert J. Remington	1994		10.1145/259963.260091	embedded system;human–computer interaction;computer science;operating system;human interface device	HCI	-41.32181599158059	-30.402752537256983	76186
55a6fdaf2fa2488d3799d50dbd96e43e1735f6e1	art-directable procedural vegetation in disney's zootopia	procedural animation;procedural geometry;vegetation	With six distinct habitats integrated into one mammalian metropolis, early artwork for Zootopia displayed a large appetite for diverse vegetation. Our flexible toolset for procedural vegetation allowed us to easily customize the system to provide the required variation through new growth and animation module features, while maintaining art-directable control. We enriched the vegetation animation tools and workflows to support various levels of interaction with the characters and environment. To address the geometric complexity produced by all of this variation, we implemented various instancing schemes to allow the renderer to re-use as much geometry as possible.	geometry instancing;habitat;procedural programming	Hans Keim;Maryann Simmons;Daniel Teece;Jared Reisweber;Sara Drakeley	2016		10.1145/2897839.2927469	simulation;vegetation;computer graphics (images)	Graphics	-37.56863804597163	-32.67204762649453	76240
3c66c8f2435aaffc0198438c14fa418d5c062e15	geopod: using a game-style interface to explore a serious meteorological database		This paper discusses the human-computer interface component of the GEOPOD project, a software system that implements an interactive, intuitive interface – the GEOpod – that allows student users to probe a 3-D immersive environment of authentic geophysical data (i.e. based on real observations, assimilated data, and/or simulated output from physically consistent, numerical weather prediction modeling systems), actuate virtual atmospheric devices to collect data, and record observations. The system provides a guided instructional environment in which meteorology undergraduate students can explore a given atmospheric volume in a “shuttlepod-like” virtual flying machine. Because the atmospheric data consist of real-time observations and imagery, along with simulated data from numerical models based on actual physics, the exploration environment naturally exhibits technical accuracy, scientific soundness, physical consistency, authenticity, and high fidelity.	human–computer interaction;immersion (virtual reality);numerical analysis;numerical weather prediction;real-time locating system;software system;widget (gui)	Blaise W. Liffick;Gary M. Zoppetti;Sepi Yalda;Richard E. Clark	2016		10.1007/978-3-319-40355-7_26	flight simulator;software system;simulation;visualization;immersion (virtual reality);numerical weather prediction;soundness	Visualization	-41.84024590280182	-35.56183202437032	76282
84bc341385878ce7704e41247edadab0fb6124c7	examination of arrangement support functions for emphasizing user's intentions on keyword map	information retrieval information visualization intention emphasizing intelligent web interaction;keyword space visualization;graph drawing;information retrieval;intelligent web interaction;information visualization;data visualisation;internet;visualization springs reflection information retrieval production facilities testing;user intentions;internet data visualisation information retrieval;arrangement support functions;information retrieval arrangement support functions user intentions keyword map information visualization system keyword space visualization keyword relationship graph drawing interactive features intelligent web interaction;intention emphasizing;keyword relationship;information visualization system;interactive features;keyword map;support function	The keyword map, which is a kind of information visualization systems, has been studied for visualizing keyword space. This system shows keywords and relationships between keywords by a graph drawing method, and is equipped with interactive features. However, existing keyword map has not considered positively the reflection of user's intention in keyword space. In this paper, the keyword map equipped with arrangement supporting functions is proposed for emphasizing user's intention. The relation between user's intentions and their usages of arrangement supporting functions is examined through experiments	experiment;graph drawing;information retrieval;information visualization;interactivity;relevance;sinc function;turing test	Tomoki Kajinami;Yasufumi Takama	2006	2006 9th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2006.345372	support function;the internet;information visualization;computer science;multimedia;graph drawing;world wide web;information retrieval	Robotics	-42.96373323820425	-24.89939397522208	76395
3188e39896deedb48b813087dbcfd39fe423c68c	[d95] haptic training for stem education using point-cloud based haptic rendering and light-based illusion	haptic interfaces three dimensional displays rendering computer graphics training solid modeling software	Haptic 3D exploration is a fascinating experience, but learning to use the haptic interface and getting accustomed to the haptic stimulus have been always challenging to novice users. Besides, the development cost of 3D models and software is another issue. In this demo, we present a method to utilize a depth camera or 2D images to enable haptic 3D rendering through our previously developed point-cloud based haptic 3D rendering algorithm. We envision that this approach can bring new modality and possibilities for STEM education, especially for students with disabilities. Furthermore, we study the discrepancies between haptic perception and visual perception, since the illumination plays a big role in vision. We anticipate the haptic representation of light-based illusion can play a role in deepening the understanding of haptic perception during training.	3d modeling;3d rendering;algorithm;haptic technology;modality (human–computer interaction);point cloud	Chung Hyuk Park	2014	2014 IEEE Haptics Symposium (HAPTICS)	10.1109/HAPTICS.2014.6775574	stereotaxy;computer vision;computer science;multimedia;computer graphics (images)	Robotics	-42.16059756871915	-36.949432194424716	76505
fce925f42df573b17fdc15681d7d11658263b5a2	hydronet: an intelligent hydroponics web service environment	web service	In this work an intelligent web service environment for managing hydroponics cultivation processes is proposed. The environment is called HydroNet and includes information and personalized support to hydroponics’ interested groups. The aim is to give the producer the opportunity to access for the first time hydroponics consulting services that meet his/her particular needs over the web. The environment consists of an underlying web services infrastructure. It supports training, support and recommendation services, adaptive web and user interaction services, remote online access services and GIS support services. The web services are coupled with smart mechanisms such as dynamic information flow, interface adaptation and intelligent web structure reorganization. The exploitation of various state of the art technologies takes place, in order to achieve a user centric approach for the collection, presentation and dissemination of data. Overall the environment aims at encouraging the development of hydroponics in Greece.	geographic information system;personalization;web service;world wide web	A. Liopa-Tsakalidis;Evangelos Sakkopoulos;D. Savvs;Alexander B. Sideridis;Giannis Tzimas	2005	Neural Parallel & Scientific Comp.		world wide web;web service;multimedia;user-centered design;business	Web+IR	-40.200477533466	-24.134150879271115	76506
957c37033378f9b583fbfa13f6d779511ac85661	human-computer interaction for the generation of image processing applications	modelizacion;sistema interactivo;interfase usuario;vision ordenador;ontologie;human computer interaction;image processing;user interface;customization;procesamiento imagen;personnalisation;semantics;acquisition connaissances;semantica;semantique;traitement image;processing time;computer vision;systeme conversationnel;man machine system;modelisation;interactive application;interactive system;knowledge acquisition;personalizacion;image processing and computer vision;temps traitement;sistema hombre maquina;ontologia;interface utilisateur;vision ordinateur;interaction model;adquisicion de conocimientos;modeling;domain ontology;ontology;tiempo proceso;ontology design;symbolic representation;systeme homme machine	The development of customized image processing applications is time consuming and requires high level skills. This paper describes the design of an interactive application generation system oriented towards producing image processing software programs. The description is focused on two models which constitute the core of the human-computer interaction. First, the formulation model identifies and organizes information that is assumed necessary and sufficient for developing image processing applications. This model is represented as a domain ontology which provides primitives for the formulation language. Second, the interaction model defines ways to acquire such information from end-users. The result of the interaction is an application ontology from which a suitable software is generated. This model emphases the gradual emergence of a semantics of the problem through purely symbolic representations. Based on these two models, a prototype system has been implemented to conduct experiments.		Régis Clouard;Arnaud Renouf;Marinette Revenu	2011	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2010.12.002	computer vision;simulation;systems modeling;image processing;computer science;artificial intelligence;ontology;semantics;user interface	AI	-36.76603400968514	-26.892740924595923	76702
6f0465574debd4519206e76e841d348486e8fe73	the ambulant annotator: empowering viewer-side enrichment of multimedia content	structured;smil;content enrichment;multimedia documents	This paper presents a set of demos that allow viewer-side enrichment of multimedia content in a home setting. The most relevant features of our system are the following: passive authoring of content in contraposition to the traditional active PC authoring, preservation of the base content, and collaborative authoring (e.g., to share the enriched material with a peer group). These requirements are met by modelling television content as structured multimedia documents using SMIL 2.1.	case preservation;demo (computer programming);gene ontology term enrichment;requirement;synchronized multimedia integration language	Pablo César;Dick C. A. Bulterman;Jack Jansen	2006		10.1145/1166160.1166209	synchronized multimedia integration language;computer science;multimedia;internet privacy;world wide web	Web+IR	-41.9746462018977	-24.538351194713794	76750
20451df4926c989a2abdc2fab01b0a336e1bce57	rendering mathematics for the web using madoko	mathematics;latex;markdown;madoko;rendering	Madoko [6-8] is a novel authoring system for writing complex documents. It is especially well suited for complex academic or industrial documents, like scientific articles, reference manuals, or math-heavy presentations. One particular important aspect of Madoko is to write a document in high-level Markdown [5] with a focus on semantic content. From this document specification we can generate both high-quality PDF output (via LATEX) but also generate highquality HTML that can re-scale and re-flow dynamically. Styling is done through standard CSS attributes and can be done orthogonal to the content.	cascading style sheets;device independent file format;html;high- and low-level;latex;portable document format;scientific literature;world wide web	Daan Leijen	2016		10.1145/2960811.2967168	markdown;rendering;computer science;database;multimedia;world wide web;computer graphics (images)	Web+IR	-42.385684162687525	-26.7277258088274	76834
79fd755521be3b35cf6a6d4db931b754a0a090d4	systematic measurement of human map-reading ability with street-view based navigation systems	user map reading ability;navigation system;digital map;user operation;digital mapping	Recently, the various navigation systems spread in many aspects of our daily life. It becomes necessary for the user to choose the one which is suitable for oneself from among these systems. However, it is difficult that users understand all these systems, because ability which understands map and guidance disagree by each user. Besides, there will be the user who cannot understand it even if a system more easily informed guidance for a user. We think that a uniform guidance is a problem for some people, because the ability to understand a map is different by a user. Therefore, it is necessary for us to investigate ability to understand guidance, that is, human map-reading ability. Moreover, we consider that it is important that a system analyzes the ability of user and shows weak points of user. In this paper, we developed a system to simulate whether we seem to do way-finding in real space, and first of all we performed outdoor experiments about pedestrian way-finding for measuring by the system. Next, we proposed three major indicators from pedestrian behaviors, and finally measured human map-reading ability in our system. Consequently, we think that will open up a new vista of the future for personal guidance services by our approach.	cognitive science;correctness (computer science);erika enterprise;experiment;geographic information system;geoinformatics;george sugihara;lakes of wada;simulation;visit;weak value;web search engine	Kaori Kobayashi;Ryong Lee;Kazutoshi Sumiya	2010		10.1145/2108616.2108666	user;computer vision;simulation;user modeling;digital mapping;computer user satisfaction;multimedia	AI	-37.41284904193578	-35.904219641166335	77064
a8186c8d847814ac1b46a454636f722769c7c120	foa: an xsl-fo authoring tool	authoring tool	XML, XSL, XSL-FO, FO, authoring, FOA, XSL-T, presentation, rendering, document FOA (Formatting Object Authoring) is an authoring tool that applies rich styling to XML content. It allows the styling to be re-used across multiple documents. It also allows the author to build or import a library of style components. It is based on XSL-FO, the W3C-defined markup language, whose aim is to add rich styling to XML content, especially for paginated documents.	docbook xsl;markup language;xml;xsl formatting objects	Fabio Giannetti	2002			mathematics	Web+IR	-42.54941139242869	-26.572597336840033	77083
de52f38ac73d1c2683f0ddfe1e614827b44f6027	example based processing for image and video synthesis	learning from example;markov random fields;function approximation;filter approximation;dissertation;image based rendering	painting In this experiment, we used an abstract painting consisting of globs of paint on a canvas. Our algorithm synthesizes an image that resembles the painting, yet contains no visibly copied patches from the painting. This data set presents a challenge for image analogies because there are only large scale features to copy from, and switching from one glob of paint to another could create visible seams and other artifacts. Image analogies and quilting produce interesting results that differ from ours and from each other. As a result, abstract painting rendering appears to be a problem domain where having all three algorithms as an option would be valuable to a user. Impressionist painting Figure 28 shows results run on an impressionist painting with pastel colors. Our algorithm synthesizes an image that has similar brush strokes to the painting, but loses some of the detail at the bottom of the image (e.g. the houses). When we use the painting’s original colors, our synthesized image looks like a mix of the training painting and the input image. Post-impressionist painting 1 Figure 29 shows results run on a post-impressionist painting. Our algorithm synthesizes an image that has similar brush strokes to the painting, particularly in the shrubbery to the left and especially the right of the door. These strokes are very similar to those in the tree/plant areas of the painting. Similarly, the strokes used for the path along the bottom leading up to the door resemble the much smaller strokes used on the road in the painting.		Antonio Haro	2003			computer vision;computer science;theoretical computer science;machine learning	Graphics	-39.39653357083363	-33.03043025089693	77534
dae965d88827101dcd5ec335ce3f0ce820d1b914	active layout engine: algorithms and applications in variable data printing	tecnologia electronica telecomunicaciones;computacion informatica;grupo de excelencia;ciencias basicas y experimentales;simplex;constraint solving;tecnologias;table formatting;automatic document layout;variable data printing	Variable Data Printing (VDP) refers to the process of generating and printing dynamic or personalized contents. A core technology required by highly customized VDP applications is the automatic document layout design engine, whose task is to adjust the original design or generate a new layout to present variable contents. This paper presents a novel document layout design engine, called Active Layout Engine (ALE). “Active” reflects several unique features of the engine: First, through linear text block modeling and two-pass constraint solving algorithm, it supports a rich set of layout operations, such as simultaneous optimization of text block width and height, integrated image cropping, and non-rectangular text wrapping. Second, it does not rely on a particular layout description language and thus can actively pursue emerging formats and standards. This paper describes the various technical aspects of ALE: linearization of the text block modeling, two-pass constraint solving algorithm, format-neutral Active Layout Template (ALT), system optimization, and typical VDP applications around the core engine.	algorithm;anna-brita stenström;approximation algorithm;brian;chao (sonic);computer monitor;constraint satisfaction problem;graphical user interface;layout engine;mathematical optimization;mobile phone;page (computer memory);personalization;program optimization;solver;variable data printing;wrapping (graphics)	Xiaofan Lin	2006	Computer-Aided Design	10.1016/j.cad.2005.11.006	layout;ic layout editor;computer science;engineering;theoretical computer science;document layout analysis;mathematics;geometry;comprehensive layout;engineering drawing;simplex;computer graphics (images)	EDA	-43.382140462370295	-27.26680359515741	77560
84448cde48bcce8a44ad941b1fd0631142f5be38	the macronode approach: mediating between adaptive and dynamic hypermedia	navegacion;interfase usuario;adaptive hypermedia;sistema experto;hipertexto;user interface;system dynamics;reutilizacion;rule based system;base connaissance;reuse;hypermedia;navigation;vinculo;base conocimiento;interface utilisateur;systeme expert;link;hypertexte;hipermedia;hypertext;lien;reutilisation;knowledge base;expert system	In this paper, we discuss an approach that tries to blur the distinction between adaptive hypermedia and ynamic hypermedia. The approach aims at finding an optimal trade-off between resource reuse and flexibility: existing atomic pieces of data are collected and properly annotated; at the interaction time, the system dynamically builds the nodes of the hypermedia composing different pieces together. The proposed annotation formalism is illustrated and a rule-based system to compose hypermedia nodes exploiting different knowledge sources is presented. Finally, the advantages of this approach with respect to adaptation and dynamic generation are discussed.	adaptive hypermedia;content adaptation;logic programming;rule-based system;semantics (computer science)	Elena Not;Massimo Zancanaro	2000		10.1007/3-540-44595-1_16	knowledge base;navigation;hypertext;link;human–computer interaction;lien;computer science;artificial intelligence;reuse;database;multimedia;system dynamics;user interface;world wide web;expert system	AI	-37.76130833229845	-26.07117320030665	77817
9b05d0faebf7649eeed70f8fe9b1302d083aa868	interactive visualization for linguistic structure		We provide a visualization library and web interface for interactively exploring a parse tree or a forest of parses. The library is not tied to any particular linguistic representation, but provides a generalpurpose API for the interactive exploration of hierarchical linguistic structure. To facilitate rapid understanding of a complex structure, the API offers several important features, including expand/collapse functionality, positional and color cues, explicit visual support for sequential structure, and dynamic highlighting to convey node-to-text correspondence.	application programming interface;interactive visualization;interactivity;library (computing);open-source software;parse tree;parsing;user interface;visualization library;web design	Aaron Sarnat;Vidur Joshi;Cristian Petrescu-Prahova;Alvaro Herrasti;Brandon Stilson;Mark Hopkins	2017			artificial intelligence;natural language processing;visualization;visual analytics;computer science;interactive visualization;information visualization	HCI	-38.71017935374715	-29.38986532484967	77975
13596490e10eb4949b890bfefd3e006163d360f5	16-cds: a surface controller for the simultaneous manipulation of multiple analog components		This project presents a control surface that combines a grid of photoresistors with a microcontroller to allow a musician to simultaneously manipulate multiple analog components. A brief background on past uses of photosensitive components for music and film composition and instrument-building introduces a few contexts for the controller. Topics such as implementation, construction, performance scenarios of the controller are also discussed.	audio control surface;microcontroller	Carlos Domínguez	2014			microcontroller;grid;human–computer interaction;computer science;control engineering;control theory	Robotics	-46.03208211682273	-36.195916284190275	78038
8c3b0cfdd0db6f74e401a29f026f40a773d325e0	comic-crowd: interactive comic creation that supports multiple storylines, visualizations, and platforms	contents creation;branching narratives;multiple storylines;multi hop derivation;information visualization;interactive system;comic and manga	Comic-Crowd is an interactive digital comic system for developing and presenting multiple comic-book storylines and multiple visualizations. To date, digital comic books have had the same format and level of interaction since they first appeared. Thus, such comics have one storyline, and the interaction is akin to turning pages. In contrast, our approach supports multiple storylines, so that readers can select a storyline, with say, a happy or sad ending, depending on their preferences. Thus, our format and level of interaction are more dynamic than those of conventional digital comics. Moreover, since our format can provide multiple visualizations, it would be useful for a wide variety of applications. In this paper, we describe our Comic-Crowd concept and an implementation that expresses its unique interaction features and format.	book	Hiroaki Tobita	2015		10.1145/2836041.2836057	simulation;information visualization;computer science;multimedia;computer graphics (images)	HCI	-43.957068632533456	-32.563129831878776	78374
88671e8100886baa6e3267deab93e8d9e1bbb572	implementation of 3d simulator for golf putting green	polynomials;sport;computer animation;digital simulation	In this paper, we implemented the 3D simulator for golf putting green by using the Simulink data of golf ball motion with OpenGL. A golf putting green surface formula is approximated using polynomial equations. In the 3D simulator, golf ball and other objects are designed in 3ds Max software. For a communication between 3D simulator and Matlab Simulink model, we used virtual com port software. A result of animation can be seen in 3D by using Samsung 3D LED monitor and shutter glasses.	active shutter 3d system;approximation algorithm;autodesk 3ds max;com (hardware interface);led display;matlab;movie projector;opengl;polynomial;serial port;simulation;simulink	Bayanjargal Baasandorj;Hyeokjae Kwon;Ganduulga Gankhuyag;Woonchul Ham	2013	2013 10th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2013.6677391	embedded system;simulation;computer science;sport;computer graphics (images)	Robotics	-38.35409355072323	-30.382957521535666	78535
379418ccfd987bbdce7c51a72cfff8d7e8099ce3	comprehensive architecture for simulation of the human body based on functional anatomy	model view controller;virtual human;human body;virtual environment;software design	In this paper we propose a structured approach for the simulation of the human body which is comprehensive and extendable. Our architecture resembles the human organism as defined by the systematic and functional anatomy to integrate a broad range of simulation algorithms. To share common data and to create interlinks between algorithms without modifying the algorithms themselves we introduce abstract control entities that mimic the basic setup of physiological systems. We utilize the model-view-controller pattern to establish a separation of algorithms and data. The structure was designed for the purpose of interactive simulation of the human body in virtual environments.	algorithm;computational anatomy;entity;extensibility;global variable;interactive storytelling;model–view–controller;simulation;virtual reality	Sebastian Ullrich;Jakob Valvoda;Andreas Prescher;Torsten Kuhlen	2007		10.1007/978-3-540-71091-2_66	computer vision;simulation;human–computer interaction;computer science	Graphics	-41.75031493623219	-34.816096888949	78738
33475c51153470de9ad0f1fd7360778009a7fad9	separating semantics from rendering: a scene graph based architecture for graphics applications	application development;hierarchical structure;model view controller;user interface;semantics;software architecture;scene graph;systems architecture;design pattern;system architecture;dynamic scenes;rendering	A large number of rendering and graphics applications developed in research and industry are based on scene graphs. Traditionally, scene graphs encapsulate the hierarchical structure of a complete 3D scene, and combine both semantic and rendering aspects. In this paper, we propose a clean separation of the semantic and rendering parts of the scene graph. This leads to a generally applicable architecture for graphics applications that is loosely based on the well-known Model-View-Controller (MVC) design pattern for separating the user interface and computation parts of an application. We explore the benefits of this new design for various rendering and modeling tasks, such as rendering dynamic scenes, out-of-core rendering of large scenes, generation of geometry for trees and vegetation, and multi-view rendering. Finally, we show some of the implementation details that have been solved in the process of using this software architecture in a large framework for rapid development of visualization and rendering applications.	aardvark;computation;constant folding;design pattern;graphics;just-in-time compilation;mathematical optimization;model–view–controller;on the fly;optimizing compiler;out-of-core algorithm;parallel computing;parsing;rendering (computer graphics);scene graph;semantic html;software architecture;user interface;whole earth 'lectronic link	Robert F. Tobler	2011	The Visual Computer	10.1007/s00371-011-0572-0	software architecture;terrain rendering;computer vision;tiled rendering;image-based modeling and rendering;rendering;computer science;theoretical computer science;parallel rendering;scene graph;design pattern;real-time rendering;texture memory;model–view–controller;user interface;rapid application development;alternate frame rendering;software rendering;systems architecture;image-based lighting;computer graphics (images)	Graphics	-36.8348168692222	-30.677206438731314	78980
48a58e4f3ca1f99a0860be54572740e845374c15	real-time character animation using puppet metaphor	character animation;real time	For computer animation creation, character design is a very important factor but very hard work. Especially its motion design is very laborious work. So the authors propose new motion design method using a puppet metaphor and this paper introduces its prospective application examples. The puppet show is one of very popular entertainments. Puppets are not real characters like human actors/actresses and they cannot take real actions. However the puppet show has enough entertainment aspects. Moreover, the puppet is also children toy and it is easy to manipulate for even children. Then the authors employ the puppet metaphor for motion design of computer animation creation.	computer animation;prospective search;real-time transcription	Yoshihiro Okada	2002			animation;computer facial animation;skeletal animation;computer animation;multimedia;communication;animation director;computer graphics (images)	Graphics	-40.38099267923558	-35.587467657768386	79147
61672668d9c193c1085d65c8453eef0811cf3c8e	color-space cad: direct gamut editing in 3d	bitmap and frame buffer operations;interactive image processing;design automation;3d navigation;image processing;graphics hardware color space cad direct gamut editing interactive image processing 3d perceptual color usability direct 3d navigation;color space;computer graphics;cad;user interfaces cad colour graphics computer graphics image colour analysis;operations research;hardware visualization monitoring digital photography usability feedback software packages optimization methods navigation computer graphics;color space cad;monitoring;graphics hardware;three dimensional displays;image color analysis;image colour analysis;transforms;direct gamut editing;aerospace electronics;image processing software bitmap and frame buffer operations;3d perceptual color usability;tools;colour graphics;indium;direct 3d navigation;image processing software;user interfaces;graphics;arsenic	Color-space CAD is an interactive image-processing framework that lets users manipulate colors directly in 3D perceptual color space. Unlike traditional 2D color- manipulation tools, which often require multiple iterations, color-space CAD allows direct 3D navigation of the solution space. The framework uses graphics hardware to accelerate the computation-intensive mapping operations.	color space;computation;computer-aided design;feasible region;graphics hardware;image processing;iteration	Neophytos Neophytou;Klaus Mueller	2008	IEEE Computer Graphics and Applications	10.1109/MCG.2008.49	arsenic;computer vision;image processing;computer science;graphics;cad;multimedia;indium;color space;computer graphics;user interface;graphics hardware;computer graphics (images)	Visualization	-37.89423457983759	-34.691410901118175	79226
42ded74d4858bea1070dadb08b037115d9d15db5	exigent: an automatic avatar generation system		Avatars are pervasive in video games and virtual worlds. The automatic generation of these avatar promises to reduce player effort and provide system-defined mappings between “real” (physical) player characteristics and virtual identities. We present an avatar generation system called Exigent; given a photograph of a human face, Exigent creates an avatar. Exigent leverages two recent computer vision packages and extends them with a custom facial analysis and generation sub-module. The generated avatars incorporate the user’s gender, face shape, hair color, facial expression, eye shape, and more. This system is different from most avatar creators in its consideration of finer-grained details such as curvature of the face, distance between eyes, size of features, etc.	avatar (computing);computer vision;pervasive informatics;virtual world	Dominic Kao;D. Fox Harrell	2015			multimedia;simulation;computer science	HCI	-38.01895140677706	-36.021009985142385	79442
49fa7c745ea70fff78f9a0d9594f93b8f19678e7	stepwise development of distributed interactive simulation systems	sistema interactivo;distributed system;model based reasoning;raisonnement base sur modele;analisis escena;analyse scene;trajectoire;systeme reparti;modelo 3 dimensiones;generic model;modele 3 dimensions;three dimensional model;remote operation;systeme conversationnel;sistema repartido;trajectory;interactive system;object oriented;teleaccion;operating characteristic;oriente objet;trayectoria;distributed interactive simulation;orientado objeto;high level architecture;teleoperation;scene analysis;dynamic scenes	Objects in distributed interactive simulation systems are unpredictable, i.e., are controlled by their local operators, and are remote, i.e., must rely on some transmission media to visualize dynamic scene from their local perspectives. The paper reports on the ongoing project 1  aimed at a methodology for developing distributed interactive simulation systems for real and simulated objects in a 3D dynamic scene, based on a generic model of a material point, operational characteristics of participating objects, and their trajectories. Suitability of the proposed methodology is briefly discussed using the example of a High Level Architecture (HLA) standard.		Tomasz Orlowski;Bogdan Wiszniewski	2003		10.1007/978-3-540-24669-5_64	computer vision;teleoperation;simulation;computer science;artificial intelligence;trajectory;model-based reasoning;object-oriented programming	Robotics	-34.63290307039405	-28.316766086937317	79584
f2e3019f35777d0ffd0e3fd8cface1f32ecbe9ca	virtual remote inspection — a new concept for virtual reality enhanced real-time maintenance		Virtual Reality (VR) technology has found its way from entertainment and gaming applications into industrial environments. Only few approaches discuss the application of VR in the inspection and maintenance domain. Having reliable maintenance processes is however of significant importance for the economic success of manufacturing companies. In order to optimize manual maintenance processes, we developed a concept for visual remote inspections of manufacturing machines based on Virtual Reality technology. The approach focusses on the usage of real-world recordings that enable the viewer to virtually move to another place. We realized and demonstrated the developed concept in a prototypical implementation.	emoticon;prototype;real-time clock;real-time locating system;streaming media;virtual reality	Christian Linn;Simon Bender;Joshua Prosser;Kevin Schmitt;Dirk Werth	2017	2017 23rd International Conference on Virtual System & Multimedia (VSMM)	10.1109/VSMM.2017.8346304	multimedia;simulation;maintenance engineering;computer science;virtual reality	Visualization	-42.53291336553638	-36.09095475640916	79610
ef4c4bf4f2c5e980dd2d9d6f497251af53138a63	real-time crowd simulation integrating potential fields and agent method	crowd simulation;potential field;human behavior;agent;types of simulation;animation;crowd dynamics	Crowd simulation is studied extensively in computer graphics, animation, and safety. A real-time crowd simulator has been developed based on potential fields and agent approach in this article. This simulator produces realistic complex heterogeneous motion and improves the simulation rates by at least 32% in comparison with the potential field results. The model of this simulator can efficiently tackle the problems in global optimal navigation, collision avoidance, and dynamic interaction; furthermore, it allows an agent to make independent decisions.	addendum;agent-based model;apache continuum;computer graphics;crowd simulation;digital library;image resolution;real-time locating system;real-time transcription;relevance;rendering (computer graphics)	Guanghui Lu;Leiting Chen;Weiping Luo	2016	ACM Trans. Model. Comput. Simul.	10.1145/2885496	anime;computer vision;simulation;computer science;crowd simulation;human behavior;computer graphics (images)	Graphics	-37.95481541488139	-37.25880518147021	79700
c60ca9d4b36ef86652cc7680e7e2d1877b72c232	real-time online interactive applications on the grid (roia 2008)	collaborative work;graphical interface;real time;online interaction;multimedia application;scientific visualization;graphic user interface;networked virtual environment;web technology;network computing;everyday life;dislocations	Through the recent advancements in network technology, graphics cards and displays, a new type of application -- real-time online interactive applications (ROIAs) -- has become more and more popular. Everyday life has been affected and transformed not only by the use of Web technology, but also by collaborative multimedia applications, networked computer games, cooperative scientific visualizations, networked virtual environments and real-time graphical displays. Users and organizations dislocated all over the globe are enabled to work with a variety of tools and graphical user interfaces to communicate and jointly solve problems using sophisticated graphical interfaces. The grid and its technologies are known to provide a sophisticated basis for ROIAs and collaborative work.	real-time transcription	Christoph Anthes;Thomas Fahringer;Dieter Kranzlmüller	2008		10.1007/978-3-642-00955-6_37	scientific visualization;human–computer interaction;computer science;operating system;graphical user interface;database;distributed computing;multimedia;world wide web	HPC	-44.43644857925756	-36.13395034608609	79739
99484701de5986bf9e2dc1aa21c7eadfe31b15fe	assisting two-way mapping generation in hypermedia workspace	spatial hypertext;two way mappings;information visualization;information workspace;editable visualizations;projective structure	This paper reports our study of a two-way mapping generation tool called Mapping Assistant, as an extension to the Spatial Hypermedia system VITE. Mapping Assistant has been designed to overcome the problem arising due to the difficulty of users in generating an initial two-way mapping for VITE. We have developed VITE to allow users to interact with information in a semi-formal workspace. Creating two-way mapping profiles is a vital step for projecting structured information into a spatial hypermedia system. A previous study of VITE indicated that users spent much of their time developing an initial mapping before working on the information task. We designed the Mapping Assistant to assist users by generating a quick initial mapping from the data entered by the user and reduce the cognitive and mental load on the user. This research studies users' impression of the Mapping Assistant. The results indicate that the users liked the Mapping Assistant and found it useful, but comments from users also reveal possible directions for further improvement of the tool and its design.	hypermedia;semiconductor industry;workspace	Hao-wei Hsieh;Katherine Pauls;Amber Jansen;Gautam Nimmagadda;Frank M. Shipman	2010		10.1145/1810617.1810636	simulation;information visualization;human–computer interaction;computer science;multimedia;world wide web	HCI	-40.72597071644913	-27.335331162768853	79910
f9712866a22d2e6f90c022c52850ccc766bcdd21	räumliche gestenerkennung und natural user interfaces mit microsoft kinect			kinect	Björn Oltmanns;Denis Kruschinski;Dieter Wallach	2011			human–computer interaction;computer graphics (images);user interface;computer science	Crypto	-47.104936644348484	-31.44547450097671	80294
8a02b9f6e533758e0edefc93d787097aafab96a3	magic boards	art;haptics;design tools	Augmenting a video with special effects can produce visually compelling and interesting results. For example, the television newscast of a weatherman often contains graphics and animation. To create such mixes of live and synthetic media, the presentation must be carefully planned. By contrast an interaction using a marker or chalkboard (eg. a lecture or meeting) can be more spontaneous and requires a lesser amount of planning. While it is simple to capture such an interaction with a video camera, that video is often unsatisfactory. For example the board may be difficult to read because of low resolution or sloppy writing. Or the video itself may just be boring. We want to be able to augment the board with special effects (such as in the weatherman example) without giving up the flexibility of traditional board interaction. In order to achieve our goal, we propose creating and adding the special effects as an off-line/post-process after the interaction has been recorded. One direct approach is to repaint the board in every frame. This method is unattractive, because it is tedious and time consuming. Instead, our Magic Boardssystem is designed to augment video of an event where one or more people write on a board, without requiring nearly as much work. The system allows the spontaneity and collaboration found in this type of interaction while producing a video that contains special effects on the board, such as videos and type written text. The input to Magic Boards is a pre-recorded video of some interaction around a chalk or marker board, where the camera is not moving. Someone who is familiar with the interaction, who we refer to as the “author,” is presented with a small set of images retrieved from the video. Each image represents a single idea written on the board. The author then replaces each image with a picture, video or animation, to better represent each idea. The author also indicates when to expose (or hide) different parts of each new image. A new video is rendered using the replaced pictures and videos in place of the writing on the board. The new material is kept at the bottom layer of the video, meaning that anyone who walks in front of the board will always remain in front.	automated planning and scheduling;emergence;graphics;image resolution;mike lesser;online and offline;spontaneous order;streaming media;synthetic data	Michael N. Wallick;Michael Gleicher	2005		10.1145/1186954.1187013	computer science;artificial intelligence;haptic technology	HCI	-39.29861390867652	-34.28459794891862	80338
0bd60528f4b2ae0322c7de472f908cae929dc0f8	presentation layer modeling for shareable multimedia data				Gerhard A. Schloss;Michael J. Wynblatt	1995			database;data mining;computer science;presentation layer;multimedia	EDA	-45.158440783982215	-26.083541883667444	80548
4fae0147b7884ee401a6dda9f1fc024d32cc2c1b	an intelligent instructional tool for puppeteering in virtual shadow puppet play		Shadow puppet play has been a popular storytelling tradition for many centuries in many parts of Asia. In this paper, we present an initial idea and architecture of a software tool that allows people to experience shadow puppet play in the virtual world. Normally, a virtual puppet show is controlled automatically by the application. However, our tool allows the user to create storyline and control the puppets directly in real-time with a special device that can improve the skill of a puppeteer. This paper focuses in detail on the design and issues of a component of the software tool which is the intelligent instructional tool for puppeteering of virtual shadow puppet play. The result of the preliminary evaluation has shown that the tool is able to help users more beneficially and a higher degree of satisfaction among the respondents which include professional puppeteers and potential users.	programming tool;puppeteer;real-time locating system;shadow copy;shadow volume;virtual world	Sirot Piman;Abdullah Zawawi Talib	2011		10.1007/978-3-642-30214-5_13	simulation;multimedia;computer graphics (images)	HCI	-40.680587779933546	-35.131073680785576	80609
b84081eeb8bb6ece7be17ea5a4c64568b3f7aff2	wikipublisher: a print-on-demand wiki	web pages;web publishing;printing the web;wikis;word processing;wiki markup	Web and print exist as two solitudes: printed web pages often disappoint and converting print documents into good web pages is hard. A wiki makes it easy for authors to create rich web content, but is little help if readers wish to print the results. Wikipublisher lets readers turn wiki pages or page collections into print, with a quality better than most word processing documents. This lowers the time and cost of creating online and print versions of the same content, with no loss of quality in either medium.	printed web;web content;web page;wiki	John Rankin;Craig Anslow;James W Noble;Brenda Chawner;Donald Gordon	2009		10.1145/1641309.1641346	web service;personal wiki;static web page;web development;html;web design;web standards;computer science;web page;multimedia;web 2.0;world wide web;information retrieval;web server	Web+IR	-45.238609777873776	-24.584207910171735	80679
07923904614ad217ab35c545e13132f02738d6ab	reification, polymorphism and reuse: three principles for designing visual interfaces	design principle;marking menu;coloured petri net;user interface;direct manipulation;reuse;large scale;instrumental interaction;polymorphism;interaction model;visual interfaces;reification;interaction technique;design principles	This paper presents three design principles to support the development of large-scale applications and take advantage of recent research in new interaction techniques: Reification turns concepts into first class objects, polymorphism permits commands to be applied to objects of different types, and reuse makes both user input and system output accessible for later use. We show that the power of these principles lies in their combination. Reification creates new objects that can be acted upon by a small set of polymorphic commands, creating more opportunities for reuse. The result is a simpler yet more powerful interface.  To validate these principles, we describe their application in the redesign of a complex interface for editing and simulating Coloured Petri Nets. The cpn2000 interface integrates floating palettes, toolglasses and marking menus in a consistent manner with a new metaphor for managing the workspace. It challenges traditional ideas about user interfaces, getting rid of pull-down menus, scrollbars, and even selection, while providing the same or greater functionality. Preliminary tests with users show that they find the new system both easier to use and more efficient.	coloured petri net;first-class function;interaction technique;item unique identification;reification (computer science);simulation;user interface;workspace	Michel Beaudouin-Lafon;Wendy E. Mackay	2000		10.1145/345513.345267	polymorphism;simulation;reification;human–computer interaction;computer science;artificial intelligence;design elements and principles;reification;operating system;data mining;reuse;programming language;user interface;world wide web;interaction technique	HCI	-41.611479616503495	-30.508484498784874	81236
6e65bad2b7538fed8cf695c34594a06491e514ee	dali! - drawing animated lines!		We present an animation system for the creation of non-photorealistic 3D animations. Our system daLi! is able to render images using a common 3D model as input. This model may be enriched with additional information concerning for instance hierarchy, structure and presentation style. The rendering is done analytically, which means the output is resolution independent. It results in a series of images that depict the animation using line-drawings. The implementation of daLi! was completely done in Smalltalk.	3d computer graphics;graphical user interface;image resolution;lars bak (computer programmer);magdeburg;non-photorealistic rendering;smalltalk;unbiased rendering	Maic Masuch;Stefan Schlechtweg-Dorendorf;Bert Freudenberg	1997			computer animation;smalltalk;rendering (computer graphics);computer graphics (images);animation;non-photorealistic rendering;hierarchy;computer science	Graphics	-39.45070679745776	-32.416544039666746	81499
d3c091e79cc820773dd42605182447e8dfa0bd1e	mobile image browsing on a 3d globe: demo paper	hierarchical data structure;mobile device;3d visualization;image search and browsing;visual search;image browsing;image search;user interaction	With users increasingly using their mobile devices such as smartphones as digital photo albums, effective methods for managing these collections are becoming increasingly important. Standard solutions provide only limited facilities for organising, browsing and searching image collections on mobile devices, making it challenging and time-consuming to locate images of interest.  In this demo paper, we present an intuitive interface for organising and browsing image collections on mobile devices. Images are arranged on a 3D globe according to colour similarity. To avoid image overlap image thumbnails are placed on a regular grid structure while large image collections are organised using a hierarchical data structure. Through multi-touch user interaction image browsing can be performed in an intuitive and effective manner.	data structure;digital photography;hierarchical database model;image viewer;mobile device;multi-touch;regular grid;smartphone;thumbnail	Klaus Schöffmann;Marco A. Hudelist;Manfred del Fabro;Gerald Schaefer	2012		10.1145/2324796.2324866	computer vision;visualization;visual search;computer science;mobile device;multimedia;world wide web	HCI	-33.87401520667844	-34.90827414762109	81584
7242df4b278d06d515dd62dd05c17ab940a0a5d7	introduction and description of the interactive image format (.iif) for the world wide web.	image formation;world wide web			Jeffrey Ferguson	1996			vrml;web mapping;web-based simulation;computer science;web navigation;web page;multimedia;world wide web;computer graphics (images)	Vision	-43.70471912352541	-26.8984656758518	81640
91f334f32f8ad3c6b0a1d84d698b0ae544f1c371	a collaborative client participant fusion system for realistic remote conferences	gpu;human centric communication;foreground segmentation;mixed reality;remote conferencing systems	Remote conferencing systems provide a shared environment where people in different locations can communicate and collaborate in real time. Currently, remote video conferencing systems present separate video images of the individual participants. To achieve a more realistic conference experience, we enhance video conferencing by integrating the remote images into a shared virtual environment. This paper proposes a collaborative client participant fusion system using a real-time foreground segmentation method. In each client system, the foreground pixels are extracted from the participant images using a feedback background modeling method. Because the segmentation results often contain noise and holes caused by adverse environmental lighting conditions and substandard camera resolution, a Markov Random Field model is applied in the morphological operations of dilation and erosion. This foreground segmentation refining process is implemented using graphics processing unit programming, to facilitate real-time image processing. Subsequently, segmented foreground pixels are transmitted to a server, which fuses the remote images of the participants into a shared virtual environment. The fused conference scene is represented by a realistic holographic projection.	algorithm;computer graphics;dilation (morphology);embedded system;erosion (morphology);graphics processing unit;holography;image processing;internet protocol suite;lossless compression;markov chain;markov random field;mathematical morphology;pixel;real-time clock;real-time transcription;s-video;server (computing);synthetic intelligence;telecommunications network;virtual reality;visual effects;webcam	Wei Song;Mingyun Wen;Yulong Xi;Phuong Minh Chu;Hoang Vu;Shokh-Jakhon Kayumiy;Kyungeun Cho	2015	The Journal of Supercomputing	10.1007/s11227-015-1580-z	computer vision;computer science;mixed reality;multimedia;computer graphics (images)	Visualization	-35.31259310101919	-35.207731534284406	81650
49e29dc066c829479d420c3cdf4ef4cf0ee6ffdd	enterprise simulation laboratory for simulation games in virtual reality	virtual reality;enterprise simulation laboratory;simulation game	The paper presents the Enterprise Simulation Laboratory concept, which is currently being developed at Helsinki University of Technology. At first, the need for visual, interactive enterprise simulation models is put forward. Thereafter, a classification of enterprise simulation models is presented, and the state of the art in enterprise simulation games is analyzed. To fill the gaps in both manual and computer supported enterprise simulations, the idea of a “virtual enterprise space” for simulation is presented. It enables the interactive, visual simulation and experimentation of enterprise systems in a group mode. The Enterprise Simulation Laboratory creates unique research possibilities to apply and study enterprise simulation techniques and organizational learning in a laboratory setting.	simulation;virtual reality	Riitta Smeds	1997			simulation;human–computer interaction;instructional simulation;metaverse;mixed reality;multimedia	EDA	-48.12018881468976	-31.014254391301144	81667
0fd9a8b236a55fc548e63536246c5ecbe18e8285	glsv: graphics library stereo vision for opengl	task performance;image tridimensionnelle;stereoscopic;vision ordenador;representation graphique;paper;realite virtuelle;realidad virtual;software libraries;traitement image stereoscopique;vision estereoscopica;vision stereoscopique;virtual reality;stereoscopy;program library;display algorithms;computer vision;stereo image processing;stereo vision;general public license;bibliotheque programme;grafo curva;stereoscopie;tridimensional image;vision ordinateur;opengl;estereoscopia;computer science;bibliotheque logiciel;stereopsis;graphics;biblioteca programa;imagen tridimensional	This work proposes the development of an auxiliary library for use with OpenGL, to facilitate the creation of graphic applications incorporating stereoscopic representation. This library, christened graphics library stereo vision (GLSV), is designed to remove all calculations involving knowledge of stereo vision theory from the task performed by the programmer without the latter having to change the way he/she has been working with the OpenGL library. The GLSV is distributed under the terms of the GNU Library General Public License agreement.	gnu;graphics library;opengl;operating system;programmer;software portability;stereopsis;stereoscopy	Santiago Martín;Javier Suárez;R. Orea;Ramón Rubio;Ramón Gallego	2008	Virtual Reality	10.1007/s10055-008-0105-y	stereoscopy;computer vision;simulation;computer science;stereopsis;virtual reality;computer graphics (images)	HCI	-35.98905526995737	-27.53802188247471	81769
dcff0c26cfbe7f4cbeabf4b018a16412546be274	word expansion supports posix shell interactivity		The POSIX shell is the standard tool to deploy, control, and maintain systems of all kinds; the shell is used on a sliding scale from one-off commands in an interactive mode all the way to complex scripts managing, e.g., system boot sequences. For all of its utility, the POSIX shell is feared and maligned as a programming language: the shell is feared because of its incredible power, where a single command can destroy not just local but also remote systems; the shell is maligned because its semantics are non-standard, using word expansion where other languages would use evaluation.   I conjecture that word expansion is in fact an essential piece of the POSIX shell’s interactivity; word expansion is well adapted to the shell’s use cases and contributes critically to the shell’s interactive feel.	apl;booting;complex text layout;emoticon;graphical user interface;interactivity;iterative method;posix;programmer;programming language;variadic function	Michael Greenberg	2018		10.1145/3191697.3214336	programming language;conjecture;interactivity;semantics;scripting language;command-line interface;interactive programming;use case;computer science;posix	PL	-44.5618657252792	-31.213001318631182	81891
0790ac18b4441e6fba009ad009344a7bc65b6bd1	i-room: a virtual space for intelligent interaction	emergency response;groupware;collaborative work artificial intelligence intelligent systems humans production informatics game theory technological innovation virtual environment decision making;realite virtuelle;realidad virtual;mixed initiative;collaborative application;public safety;creative industries;collaboration;virtual reality;prise de decision;intelligence artificielle;video game;i room;virtual worlds technology;computer supported collaborative work;intelligent systems computer supported collaborative work virtual worlds;three dimensional displays;games;intelligent systems;intelligent system;avatars;artificial intelligence;space technology;virtual reality groupware knowledge based systems;inteligencia artificial;virtual environment;interactive space;toma decision;collaborative activities;intelligent systems i room virtual environment collaborative activities virtual worlds technology interaction space knowledge based systems;virtual space;interaction space;context;knowledge based systems;virtual worlds;knowledge base	The I-Room is a virtual environment intended to support a range of collaborative activities, especially those that involve sense making, deliberation, and decision making. The I-Room case studies described in this paper all employ virtual worlds technology to provide this interaction space and show how this can be augmented with external knowledge-based and intelligent systems.		Austin Tate;Yun-Heh Chen-Burger;Jeff Dalton;Steffen Potter;D. Richardson;Jussi Stader;Gerhard Wickler;I. Bankier;C. Walton;P. Williams	2010	IEEE Intelligent Systems	10.1109/MIS.2010.5	games;knowledge base;simulation;intelligent decision support system;human–computer interaction;computer science;knowledge management;virtual machine;artificial intelligence;virtual reality;space technology;collaboration	Visualization	-33.927847020284325	-24.68517132769185	82014
1bd71436eb1daa1e89022d90d9993ff8f8bf1360	method to generate disaster-damage map using 3d photometry and crowd sourcing		Thanks to the rapid progress of the Internet and mobile devices, information related to disaster areas can be collected through the Internet. To grasp the degree of damage in a disaster situation, the use of crowdsourcing for coordinating the individual efforts (micro tasks) of an enormous number of users (workers) on the Internet has been drawing attention as a means of quickly solving problems. However, the information gathered from the Internet is huge and diverse, so it is difficult to formulate as a crowdsourcing task. This paper proposes a conversion platform for the images of a disaster site photographed by various users as information about the site, integrating the images into a single map using 3D image processing, and providing the map to crowdsourcing as a micro task.	crowdsourcing;image processing;internet;lambda calculus;mobile device;stereoscopy	Koyo Kobayashi;Hidehiko Shishido;Yoshinari Kameda;Itaru Kitahara	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8258473	data mining;image processing;the internet;computer science;geographic information system;mobile device;solid modeling;grasp;crowdsourcing	Robotics	-34.72641651894195	-36.52502482310827	82190
a28bb826ca4299734428be7c1816758dd111d644	ab-hci: an interface multi-agent system to support human-centred computing	artefacto;metodo adaptativo;interfase usuario;multiagent system;streaming;interaction design capabilities;architecture systeme;human computer interaction;multi agent system;user interface;man machine dialogue;integration information;mixed initiative approach;methode adaptative;intelligence artificielle;human centred computing;artefact;software architecture human computer interaction multi agent systems software agents;software agents;captador medida;software architecture;mixed initiative approach interface multi agent system human centred computing software artefacts interaction design capabilities user interaction;multi agent systems;information integration;transmission en continu;measurement sensor;capteur mesure;adaptive method;comportement utilisateur;integracion informacion;artificial intelligence;dialogo hombre maquina;arquitectura sistema;interface utilisateur;interface multi agent system;inteligencia artificial;user behavior;transmision fluyente;system architecture;sistema multiagente;software artefacts;user interaction;comportamiento usuario;systeme multiagent;dialogue homme machine	Currently, new types of information are available to applications. This incoming stream of data can be processed to improve user experience. User experience can be enhanced to present contents to users in a better manner, or to provide software artefacts to improve their interaction with the environment. The wide range of interaction design capabilities offered by this data requires methods and run-time architectures able to cope with the integration of the incoming data from the interaction environment into the applications. Human-centred computing takes into account what the state of user interaction is and how this data can be used to actually make it useful. A multi-agent system architecture is described. This architecture is able to tackle the arrival of context data by means of a set of sensors and interface agents that filter and process the context-of-use data to produce new presentations. The context of use provides information about the user, the platform, the physical environment and the current task the user is performing. These new presentations produced are adapted according to the context– of-use information gathered by applying a mixed-initiative approach to overcome some of the usual flaws found in full-adaptive applications.	human–computer interaction;interaction design;multi-agent system;sensor;systems architecture;user experience	Víctor López-Jaquero;Francisco Montero Simarro;Pascual González	2009	IET Software	10.1049/iet-sen:20070108	user interface design;embedded system;user;software architecture;simulation;human–computer interaction;computer science;engineering;artificial intelligence;information integration;software agent;multi-agent system;user interface	HCI	-36.94618681455157	-26.52138576450816	82274
b7c6a359dadd35c4a219e236cfe99e5a862b0ef7	towards interactive authoring tools for composing spatialization	trajectory three dimensional displays visualization music interviews aerospace electronics prototypes;prototypes;visualization;trajectory;user interfaces authoring systems computer aided engineering data visualisation interactive systems music;three dimensional displays;aerospace electronics;temporal dimension interactive authoring tools music composers sound spatialization processes computer aided composition environment compositional interfaces three dimensional trajectory visualization three dimensional trajectory edition spatial scene descriptions;interviews;music	We present interactive tools designed to help music composers controlling sound spatialization processes in a computer-aided composition environment. We conducted interviews with composers to understand their needs and inform the design of new compositional interfaces. These interfaces support quick input, visualization and edition of three-dimensional trajectories, as well as the control of the temporal dimension in spatial scene descriptions.	surround sound	Jérémie Garcia;Jean Bresson;Thibaut Carpentier	2015	2015 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2015.7131745	human–computer interaction;computer science;multimedia;computer graphics (images)	Visualization	-44.26841578368907	-34.23624582179507	82324
455eff6d743e9c00d29235d2c29b68cc0cd47b54	temporal magic lens: combined spatial and temporal query and presentation	occupation time;interfase usuario;navegacion informacion;user interface;surveillance;navigation information;real time;base donnee temporelle;relacion hombre maquina;interrogation base donnee;information browsing;interrogacion base datos;man machine relation;vigilancia;senal video;temps occupation;signal video;monitoring;temps reel;tiempo ocupacion;video signal;tiempo real;interface utilisateur;temporal databases;relation homme machine;monitorage;monitoreo;database query;interaction technique	We introduce the concept of a temporal Magic Lens, a novel interaction technique that supports querying and browsing for video data. Video data is available from an increasingly number of sources, and yet analyzing and processing it is still often a manual, tedious task. A Temporal Magic Lens is an interactive tool that combines spatial and temporal components of video, creating a unified mechanism for analyzing video data; it can be used for viewing real-time video data, as well as for browsing and searching archival data. In this paper, we define the Temporal Magic Lens concept and identify its four key components. We present a sample implementation for each component, and then describe two usage scenarios for a prototype surveillance application.	interaction technique;prototype;real-time locating system;reference implementation	Kathy Ryall;Qing Li;Alan Esenther	2005		10.1007/11555261_64	simulation;human–computer interaction;computer science;database;temporal database;user interface;world wide web;interaction technique;computer graphics (images)	HCI	-34.26201283730105	-28.114413759831585	82379
7fab1c6d5223fd9518bcb2ecd1da8738b2b167d0	a multi agent system for 3d media spaces assistance	internet multi agent systems avatars;multi agent system;user interface;multiagent systems internet cultural differences global communication virtual reality avatars user interfaces proposals space technology information technology;multi agent systems;internet;3d environment;media space;avatars;multi agent architecture;internet cultural space environment multiagent system 3d media spaces assistance attractive user interface resource 3d environment avatar personal agent communication service;communication service	Nowadays we have several cultural spaces accessible in the Internet. These synthetic worlds mix different medias (text, audio, image, video) and attractive user interface resources (as 3D ones). Not closed to presentation environments these applications offer to user other facilities: personalization, interactive views, additional information and communication tools. In this paper we describe a multi-agent architecture used to enhance 3D media spaces assistance. Our proposal is based on a 3D environment where users are represented by avatars. Each avatar has a personal agent that captures all the relevant user actions. We also have other types of agents, which are responsible for providing communication services. Also, we present a case study showing how these concepts were implemented in the ICSpace (Internet cultural space) environment.	agent architecture;avatar (computing);intelligent agent;internet;media space;multi-agent system;personalization;synthetic intelligence;user interface	Tatiana A. Tavares;Samuel A. Oliveira;Anne M. P. Canuto;Luiz Marcos Garcia Gonçalves;Guido Lemos de Souza Filho	2005	Third Latin American Web Congress (LA-WEB'2005)	10.1109/LAWEB.2005.6	the internet;simulation;human–computer interaction;computer science;artificial intelligence;multi-agent system;multimedia;user interface	HCI	-34.058123388277956	-24.587904309705078	82463
08e05335d05bf2999e14cc690ecc2b5d1545140d	virtual character within mpeg-4 animation framework extension	animacion por computador;working group;h anim;humanoid animation;interpolation;skeleton muscle and skin;interpolation methods;image coding;generic skeleton representation;multimedia;standards;realite virtuelle;realidad virtual;virtual characters;virtual reality languages;virtual reading modeling language;bba;and skin;interpolation methods virtual character animation mpeg 4 animation framework extension interactive multimedia interoperability virtual reading modeling language hybrid coding standardization face and body animation humanoid animation bone based animation generic skeleton representation curve based deformations;specification;h anim 1 1;hybrid coding standardization;biological system modeling;virtual reality;mpeg 4 animation framework extension;multimedia application;low bitrate animation compression;layout;curve based deformations;mpeg 4 synthetic and natural hybrid coding;1 1;bone based animation bba;multimedia systems;skeleton;interactive multimedia;sms;1 1 2001;codage image;virtual reality modeling language;fba;compression image;mpeg 4 standard;virtual character animation;image compression;especificacion;2001;interpolation method;facial animation;norma;interpolation virtual reality languages computer animation multimedia systems open systems image coding;snhc;vrml;model definition and animation;and skin sms model definition and animation;mpeg 4 standard facial animation layout videos virtual reality biological system modeling standardization face skeleton interpolation;face;virtual reality modeling language vrml;interoperability;synthetic data;animation framework extension afx;face and body animation fba;computer animation;open systems;afx;user interaction;norme;standardization;bone based animation;muscle;face and body animation;videos;mpeg 4 synthetic and natural hybrid coding snhc;compresion imagen;animation framework extension;animation par ordinateur	Enriched multimedia applications and services aim at combining images, sounds, videos, and synthetic objects into hybrid and interactive scenes. The core technologies discussed here deal with the representation and integration within such complex scenes of a specific kind of synthetic data, namely virtual character animation. This paper analyzes how an integrated and standardized framework is currently emerging in order to ensure application interoperability, universal content access, and user interactivity. We first compare how virtual character animation has been addressed within virtual reality modeling language (VRML) and MPEG-4 synthetic and natural hybrid coding standardization processes. A comparative synthesis between the objectives and the capabilities of each framework is exposed, specifically MPEG-4 Face and Body Animation (FBA) versus H-Anim (Humanoid Animation Working Group, WEB3D Consortium, h-anim.org.) 1.1, and MPEG-4 Bone-Based Animation (BBA) versus H-Anim 2001. The Animation Framework eXtension (AFX) specifications that are a part of MPEG-4 Systems Part 16 include the BBA framework. The BBA animation concepts, based on generic skeleton representation and curve-based deformations, are introduced. The definition of the related nodes and how they successfully address the BBA concepts are as well discussed. Some comments with respect to the rotation representation, the interpolation methods, the animation mask, and value parameters of the animation stream are made. This set of specifications provides an efficient framework, which is appropriate to real-time and animation realistic applications within networked environments.	afx windows rootkit 2003;anim;form-based authentication;interactivity;interoperability;interpolation;modeling language;real-time clock;synthetic data;synthetic intelligence;vrml;virtual reality;web3d consortium	Marius Preda;Françoise J. Prêteux	2004	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2004.830661	physically based animation;computer vision;vrml;computer facial animation;skeletal animation;telecommunications;computer science;interactive skeleton-driven simulation;virtual reality;computer animation;multimedia;statistics;computer graphics (images)	Graphics	-41.813550616331035	-33.26120174280102	82891
2f441cd8e0aa1f5193ec6fb6e79697a4b1b5186c	how to teach a fish to swim	highlevel behaviors virtual fish tank computer users animated fish programming environment fish tank metaphor programming by demonstration programming by conditioning;mouth;programming by conditioning;computer users;mice;programming environments;marine animals;paints;automatic programming virtual reality computer animation programming environments;programming environment;feeds;virtual reality;automatic programming;virtual fish tank;marine animals animation feeds mice paints large screen displays educational institutions mouth programming profession;programming by demonstration;highlevel behaviors;programming profession;animation;computer animation;large screen displays;fish tank metaphor;animated fish	Wehavedevelopeda virtual fishtankin which computer users are representedby animatedfish. The actionsand interactionsof the fish in the tank are meantto reflectthe actionsof users in thereal world. Our first attemptat creating a programmingenvironmentthat allowed peopleto customizetheir own fish did not work very well because users did not want to explicitly write programsto control their fish.Maintainingthefishtankmetaphor , weattempted to solvethis problemby havingusers teachfishratherthan write code. We borrowedideasfromthe literature on programmingby demonstr ation and developeda methodof programmingby conditioningin which users demonstr ate behaviors andalso reward (or feed)fish that are behaving appropriately. Rewardsgiveuserstheability to definehighlevel behaviors (setsof specificmovements)and complex relationshipsbetweensituationsandresponses.	norm (social);user (computing)	Stephen Farrell;Paul P. Maglio;Christopher S. Campbell	2001		10.1109/HCC.2001.995254	simulation;engineering;multimedia;communication	HCI	-42.90120994043809	-32.36562707672336	83048
c4bb8abd0be9ccb3d76749e53032624aa73d9a39	defining a control standard for easily integrating haptic virtual environments with existing audio / visual systems	multi modal;haptics;force feedback;audio;audio visual;control;virtual environment	This paper presents an approach to audio-haptic integration that utilizes Open Sound Control, an increasingly well-supported standard for audio communication, to initialize and communicate with dynamic virtual environments that work with off-the-shelf force-feedback devices.	haptic technology;virtual reality	Stephen Sinclair;Marcelo M. Wanderley	2007		10.1145/1279740.1279781	computer vision;simulation;aes11;computer science;artificial intelligence;multimedia;haptic technology	Visualization	-45.29264288242947	-36.68095116072331	83365
867bc31be44e489f448fe7cb745c35706a1c0db2	implementation of 3d visualization aplications based on physical-haptics principles to perform rehabilitation tasks	haptic interfaces engines three dimensional displays solid modeling glass rendering computer graphics;virtual reality data visualisation patient rehabilitation rendering computer graphics software tools;glass;engines;three dimensional displays;solid modeling;virtual environment 3d visualization physical haptics principles rehabilitation tasks virtual reality software tools ogre3d rendering graphics engine nvidia physx;haptic interfaces;rendering computer graphics	Nowadays, There are a lot of tools and procedures for the development of computer applications for teaching, entertainment, telecommunications, marketing, design and other more. This paper present a implementation method for developing applications based on virtual reality and procedures physical-haptics, in order to perform rehabilitation tasks, describing the used software tools. The first one is Ogre3D which is used as rendering graphics engine to add realistic 3D visualization features. Then, the physical engine NVIDIA PhysX is used to incorporate accurate physics simulation and to implement collision detection between objects in the virtual environment. The third one is OpenHaptics which is used to generate a force feedback in the haptic device Sensable Phantom. Using the developed applications, the user's immersion sense in the virtual environment is increased and improved, since the user can manipulate virtual objects with realistic physical behaviour. Finally, two examples of implementation in a rehabilitation environment are shown to demonstrate the main features of the developed tool.	3d computer graphics;collision detection;dynamical simulation;game engine;haptic technology;imaging phantom;immersion (virtual reality);object-oriented graphics rendering engine;physx;virtual reality	Luis Daniel Lledó;Santiago Ezquerro;Francisco J. Badesa;Raymond Morales;Nicolás García Aracil;José María Sabater	2014	5th IEEE RAS/EMBS International Conference on Biomedical Robotics and Biomechatronics	10.1109/BIOROB.2014.6913813	scientific visualization;simulation;image-based modeling and rendering;rendering;computer science;parallel rendering;multimedia;real-time rendering;computer graphics;alternate frame rendering;software rendering;3d computer graphics;computer graphics (images)	Visualization	-41.24358254946404	-34.7361455696437	83574
4e15fdbe2fe92855c73e0f465e759611bf70a625	blendervr: open-source framework for interactive and immersive vr	i 3 2 graphics systems distributed network graphics;virtual reality;i 3 2 graphics systems distributed network graphics h 5 1 multimedia information systems artificial augmented and virtual realities;vrpn protocol blendervr virtual reality open source framework interactive application immersive application blender game engine graphics rendering physics engine synchronization process osc protocol;augmented;rendering computer graphics synchronization virtual reality engines games navigation;navigation;engines;synchronization;games;and virtual realities;h 5 1 multimedia information systems artificial;virtual reality protocols public domain software rendering computer graphics synchronisation;rendering computer graphics	BlenderVR is an open-source project framework for interactive and immersive applications based on an extension of the Blender Game Engine to Virtual Reality applications. BlenderVR is a generalization of the BlenderCAVE project, accounting for alternate platforms (e.g., HMD, video-walls). The goal is to provide a flexible and easy to use framework for the creation of VR applications for various platforms, making use of the existing power of the BGE's graphics rendering and physics engine. Compatible with 3 major Operating Systems, BlenderVR has been developed by VR researchers with support from the Blender Community. BlenderVR currently handles multi-screen/multi-user tracked stereoscopic rendering through efficient low-level master/slave synchronization process with multimodal interactions via OSC and VRPN protocols.	ambiguous name resolution;blender game engine;head-mounted display;high- and low-level;linux;master/slave (technology);microsoft windows;multi-user;multimodal interaction;open-source software;physics engine;rendering (computer graphics);scene graph;stereoscopy;virtual reality	Brian F. G. Katz;Dalai Felinto;Damien Touraine;David Poirier-Quinot;Patrick Bourdot	2015	2015 IEEE Virtual Reality (VR)	10.1109/VR.2015.7223366	games;synchronization;navigation;simulation;rendering;computer science;parallel rendering;real-time computer graphics;virtual reality;multimedia;graphics software;computer graphics;alternate frame rendering;software rendering;3d computer graphics;computer graphics (images)	Visualization	-43.12159050818289	-36.469437737902744	83593
c26209722d9e0f881eb3f4b90727ed4bf0e08350	nonlinear perspective projections and magic lenses: 3d view deformation	3d visualization;3d view deformation nonlinear projections magic lenses;perspective projection;computer graphics;lenses optical distortion engines cameras visual effects nonlinear optics pixel nonlinear distortion focusing image converters;real time;3d virtual worlds nonlinear perspective projections magic lenses 3d view deformation deformed 3d visual effects real time navigation 3d nonlinear perspective projections camera position parameters partially linear magnification;3d virtual world;virtual reality;graphical user interfaces;magic lenses;3d environment;nonlinear projections;computer graphics virtual reality graphical user interfaces real time systems;3d view deformation;real time systems	Nonlinear projections and implementations are largely unexplored in 3D environments. A technique for generating deformed 3D visual effects using nonlinear perspective projections allows real-time navigation of 3D environments. The authors derive their deformation algorithms from 3D nonlinear perspective projections, which consider factors such as depth, view angle, and camera position-parameters 2D distortion algorithms don't consider. Distortion from the methods is more realistic than that of the 2D image distortion. In addition, the algorithms allow partially linear magnification or nonlinear deformation of 3D views in real time with less performance degradation. The authors also developed an experimental system that lets one deform 3D virtual worlds for real-time navigation. This article is available with a short video documentary on CD-ROM.	algorithm;cd-rom;distortion;elegant degradation;experimental system;lens (device);navigation;nonlinear system;projections and predictions;real-time locating system;video clip;virtual world;visual effects	Yonggao Yang;Jim X. Chen;Mohsen Beheshti	2005	IEEE Computer Graphics and Applications	10.1109/MCG.2005.29	computer vision;perspective;simulation;visualization;computer science;graphical user interface;virtual reality;computer graphics;computer graphics (images)	Visualization	-40.37065542246668	-36.46539312521991	83895
029547267853c16d6d30500b80e20d34c24c2f3b	generating remote control interfaces for complex appliances	speech interfaces;universal speech interface usi;appliances;remote control;personal universal controller;personal digital assistant;handheld computer;personal digital assistants pdas;personal universal controller puc;state dependence;specification language;pebbles;handheld computers	The personal universal controller (PUC) is an approach for improving the interfaces to complex appliances by introducing an intermediary graphical or speech interface. A PUC engages in two-way communication with everyday appliances, first downloading a specification of the appliance's functions, and then automatically creating an interface for controlling that appliance. The specification of each appliance includes a high-level description of every function, a hierarchical grouping of those functions, and dependency information, which relates the availability of each function to the appliance's state. Dependency information makes it easier for designers to create specifications and helps the automatic interface generators produce a higher quality result. We describe the architecture that supports the PUC, and the interface generators that use our specification language to build high-quality graphical and speech interfaces.	download;graphical user interface;high- and low-level;personal and ubiquitous computing;remote control;software appliance;specification language	Jeffrey Nichols;Brad A. Myers;Michael Higgins;Joseph Hughes;Thomas K. Harris;Ronald Rosenfeld;Mathilde Pignol	2002		10.1145/571985.572008	embedded system;specification language;human–computer interaction;computer hardware;computer science;operating system;programming language;remote control	HCI	-46.42642905808661	-37.615732515963934	83922
b9c3db4036f1132d354b1d31d271a8ce66037cbe	"""lessons from a """"living in a database"""" graphical query interface"""	join algorithms;database system;recovery mechanisms;user interface;access methods;main memory databases;interactive environment;access planning;entity relationship	The Living In a Database system (LID) is a user-friendly interface to an entity-relationship database Its underlying ideas are similar to Cattell's PDB [Cattell 80], but its presentation is significantly different LID uses a bit-mapped graphics terminal with mouse pointer to create an attractive interaction environment. Experience from the implementation suggests that dynamic graphic displays --- those which have graphic symbols that change as the data they present change --- are an important feature in user interfaces but are difficult to implement with current technology The implementation also uncovers an important inadequacy in the PDB/LID idea the inability to operate on sets of data instances in the same way as individual data instances An extension to LID is suggested to alleviate the problem.	computer terminal;database;entity–relationship model;graphical user interface;pointer (computer programming);pointer (user interface);protein data bank;raster graphics;usability	Dennis Fogg	1984		10.1145/602259.602273	entity–relationship model;computer science;data mining;database;programming language;user interface;access method;world wide web	DB	-41.60785255504967	-30.361928305945902	84207
d5ebd957441038c9b93d427019590c1fb073bdf7	applications of the responsive workbench	logistics data processing;3d image;virtual wind tunnel;virtual environments;view angle;hand movement tracking;computer aided production planning;speech recognition;virtual reality;virtual workbench;interface devices;three-dimensional displays;head movement tracking;fine-granularity interaction;stereoscopic shuttered glasses;responsive workbench;collaborative production modeling;table surface;fine-granularity visualization;wind tunnels;data visualisation;simulated laser pointer;virtual objects;gesture recognition;interactive systems;collaborative production planning;mirror;groupware;interactive devices;situational awareness;computer-generated stereoscopic image projection;stylus	Many interface devices for virtual reality provide full immersion inside the virtual environment. This is appropriate for numerous applications that emphasize navigating through a virtual space. However, a large class of problems exists for which navigation is not the critical issue. Rather, these applications demand a fine-granularity visualization and interaction with virtual objects and scenes. This applies to a host of other applications typically performed on a desktop, table or workbench. Responsive Workbench technology offers a new way to develop virtual environments for this rather sizable class of applications. The Responsive Workbench operates by projecting a computer-generated, stereoscopic image off a mirror and through a table surface. Using stereoscopic shuttered glasses, users observe a 3D image displayed above the tabletop. By tracking the group leader's head and hand movements, the Responsive Workbench permits changing the view angle and interacting with the 3D scene. Other group members observe the scene as the group leader manipulates it, facilitating communication among observers. Typical methods for interacting with virtual objects on the workbench include speech recognition, gesture recognition and a simulated laser pointer (stylus). This article features Responsive Workbench applications from four institutions that have pioneered this technology. The four applications are: visualization, situational awareness, collaborative production modeling and planning, and a virtual windtunnel	responsive web design;workbench		1997	IEEE Computer Graphics and Applications	10.1109/MCG.1997.10010	stereoscopy;situation awareness;computer vision;simulation;computer science;virtual machine;wind tunnel;operating system;gesture recognition;virtual reality;data visualization;computer graphics (images)	Visualization	-42.59471670391845	-37.12624296921987	84418
1eb3b2cbd6a9fe06c1afc0f6a48c1dab3c472f84	performance capture with physical interaction	animated scene;motion data;dynamic aspect;motion capture;final motion;physical interaction;complex interaction;final integrated scene;intelligent response;dynamics-based response;performance capture;dynamic motion synthesis;intermittency;physical model;turbulence;fluid simulation;virtual worlds;real time	This paper introduces a technique for combining performance-based animation with a physical model in order to synthesize complex interactions in an animated scene. The approach is to previsualize interaction of final integrated scene, online, while the performance is being recorded. To accomplish this goal, we propose a framework which unifies kinematic playback of motion capture and dynamic motion synthesis. The proposed method augments a real-time recording of a human actor with dynamics-based response in order to modify motion data based on the conditions of the character. The system unifies kinematic and dynamic aspects of the final motion while allowing user control over the outcome both temporally and spatially across the character's body. Examples of complex interactions interleaved with intelligent response underscore the power of the technique along with multi-person captures in which remote users interact physically in a shared virtual world.	interaction;motion capture;real-time clock;temporal logic;user interface;virtual world	Nam Nguyeny;Nkenge Wheatland;David F. Brown;Brian Parise;C. Karen Liu;Victor B. Zordan	2010			fluid simulation;turbulence;computer vision;real-time computing;motion capture;simulation;physical model;computer science;artificial intelligence;operating system;multimedia;intermittent energy source;computer graphics (images)	Graphics	-39.53088274051848	-36.64318011834388	84644
bb780581420f0b3b1d6ae96a724c5d7d02b468c1	px: supporting voice in workstations	workstations telephony graphics computer interfaces isdn digital signal processing chips intelligent networks merging computer applications application software;telecommunications computing;voice communication;workstations;speech analysis and processing;px voice editor personal exchange research project personal workstation users px concepts telephone circuit server workstation adaptor software call processing toolkit voice storage toolkit voice processing toolkits;telephone systems;workstations speech analysis and processing telecommunications computing telephone systems voice communication	The Personal Exchange (PX) research project, which explores an architecture to provide personal workstation users with dexterity in manipulating voice, is discussed. PX concepts and an initial implementation of the architecture are described. Both the hardware (workstation, telephone, circuit server, and workstation adaptor) and software (call-processing toolkit, voice-storage toolkit, voice-processing toolkits, and PX voice editor) are examined.<<ETX>>	list of toolkits;pixel;server (computing);workstation	Ragui Kamel;Kamyar Emami;Robert Eckert	1990	Computer	10.1109/2.56874	embedded system;speech recognition;workstation;computer science;operating system;software engineering;multimedia	Arch	-46.90264048481185	-27.014169289499343	84651
f8605ef9bbb63566241132422a68751c7c9e7156	x3dom virtual reality book store	e commerce;level of detail lod;virtual reality;level of detail;x3d;virtual reality vr store;webgl;x3dom;virtual worlds	Virtual reality commer1ce (VR-commerce) is a potential candidate for a wide adoption of the next wave of the e-commerce platform, since its advantages and novel shopping experience derived from the VR interface. Currently, platform dependence and downloading and installing a huge virtual world before cruising into the VR-store are examples among many cumbersome shoppers facing in using such platform. WebGL is a front runner for implementing this kind of innovative shopping experience, which just enters its matured phase. This study is the first implementation of a full-scale virtual reality store by X3D. It focused on the technical feasibility aspects of implementing a virtual reality book store by the X3D and WebGL standard.	download;e-commerce;full scale;virtual reality;virtual world;webgl;x3d	Hassadee Pimsuwan;Satidchoke Phosaard;Pimmanee Rattanawicha;Wachara Chantatub	2012		10.1145/2338714.2338750	computer-mediated reality;artificial reality;simulation;engineering;metaverse;mixed reality;multimedia;computer graphics (images)	Visualization	-41.35085647504396	-33.76827712573718	84792
6d0295cbb82119524605e0c6e65d0a4e0ea83b11	understanding and predicting the affordances of visual logics	g400 computing	We compare the affordances of two visual logics, one from the Euler family of notations, spider diagrams, and one which takes a significantly different approach to representing logical concepts, existential graphs. We identify strengths and weaknesses of each notation and present these features as being related to the idea that each notation is, to a greater or lesser degree, biased towards objects or predicates, and that such biases make a notation more or less effective in a given context. We then introduce a framework for understanding and predicting those affordances, which can help guide us towards better use of existing graphical notations and the design of more effective new notations. The framework links research in semiotics and linguistics with insights provided by the HCI and diagrams communities.	degree (graph theory);diagram;effective method;emulator;euler;euler–lagrange equation;eurographics;existential graph;eye tracking;graphical user interface;human–computer interaction;image;mike lesser;predicate (mathematical logic);reasoning system;secure digital;semiotics;simulation	Jim Burton;Peter Coppin	2012			natural language processing;computer science;communication	SE	-39.6849304285137	-29.71720452121418	84802
2d0cf7146ef5c259047a16953c78165e1b366741	animated talking head with personalized 3d head model	range data;generic model;texture mapping;talking head;laser range scanner;human computer interface	Natural Human-Computer Interface requires integration of realistic audio and visual information for perception and display. An example of realistic audio and visual information for perception and display. An example of such an interface is an animated talking head displayed on the computer screen in the form of a human-like computer agent. This system converts text to acoustic speech with synchronized animation of mouth movements. The talking head is based on a generic 3D human head model, but to improve realism, natural looking personalized models are necessary. In this paper we report results in adapting a generic head model to 3D range data of a human head obtained from a 3D laser range scanner. This personalized model is incorporated into the talking head system. With texture mapping, the personalized model offers a more natural and realistic look than the generic model.	acoustic cryptanalysis;computer monitor;human–computer interaction;machine perception;personalization;software agent;texture mapping;while;wire-frame model	Jörn Ostermann;Lawrence S. Chen;Thomas S. Huang	1998	VLSI Signal Processing	10.1023/A:1008070323952	texture mapping;computer vision;computer science;multimedia;computer graphics (images)	Graphics	-45.282539207565186	-36.09978048599393	85009
62bc9d9c3b186d6fec7a9758390d262a5b29b542	euclidean vectors in physics education using augmented reality		Augmented reality (AR) is one of the latest technologies that have demonstrated to be an efficient tool to improve pedagogical techniques. In this work, we present preliminary results of ongoing research in the development of an augmented reality system to facilitate learning of Euclidean vectors properties in physics. The system aids the user to understand physical concepts, such as magnitude and direction, along with operations like addition, subtraction and cross product of vectors, by visualizing augmented virtual components merged in a user-interaction environment.	augmented reality	Angel Chi-Poot;Anabel Martín-González	2014		10.1007/978-3-319-13969-2_30	computer vision;simulation;computer graphics (images)	HCI	-43.969888685744046	-36.233624245367686	85122
006ae27e99f7610dcb0e7d7119dad381a225cc79	non-photorealistic rendering in context: an observational study	non photorealistic rendering;non photorealistic rendering npr;line drawings;observational study;evaluation of npr and traditional scientific illustration;pen and ink illustration;computer science	Pen-and-ink line drawing techniques are frequently used to depict form, tone, and texture in artistic, technical, and scientific illustration. In non-photorealistic rendering (NPR), considerable progress has been made towards reproducing traditional pen-and-ink techniques for rendering 3D objects. However, formal evaluation and validation of these NPR images remain an important open research problem. In this paper we present an observational study with three groups of users to examine their understanding and assessment of hand-drawn pen-and-ink illustrations of objects in comparison with NPR renditions of the same 3D objects. The results show that people perceive differences between those two types of illustration but that those that look computer-generated are still highly valued as scientific illustrations.	computer-generated holography;line drawing algorithm;non-photorealistic rendering;open research;unbiased rendering	Tobias Isenberg;Petra Isenberg;M. Sheelagh T. Carpendale;Mario Costa Sousa;Joaquim A. Jorge	2006		10.1145/1124728.1124747	visual arts;computer vision;computer science;non-photorealistic rendering;multimedia;observational study;statistics;computer graphics (images)	HCI	-36.76482440580369	-35.87166012956559	85124
7157c41c8f93e6167776fb0c358f8132bf0df103	musical interaction with hand posture and orientation: a toolbox of gestural control mechanisms		This paper presents a toolbox of gestural control mechanisms which are available when the input sensing apparatus is a pair of data gloves fitted with orientation sensors. The toolbox was developed in advance of a live music performance in which the mapping from gestural input to audio output was to be developed rapidly in collaboration with the performer. The paper begins with an introduction to the associated literature before introducing a range of continuous, discrete and combined control mechanisms, enabling a flexible range of mappings to be explored and modified easily. An application of the toolbox within a live music performance is then described with an evaluation of the system with ideas for future developments.	control system;poor posture;sensor	Thomas Mitchell;Sebastian Madgwick;Imogen Heap	2012			computer vision;simulation;human–computer interaction;computer science;artificial intelligence;multimedia	HCI	-45.280616257536735	-35.820591100230054	85226
cc4453aaee0ff049247d719feba5e1b8a23323e2	integration of hypertext into a decision support system			decision support system;hypertext	Peter Dambon;Fahri Yetim	1990			computer science;human–computer interaction;decision support system;knowledge management;hypertext	Robotics	-45.112022710152125	-25.913920236231856	85300
55f2e90b5166a691d104f7592d4329575b65c1b5	a three-layer virtual director model for supporting automated multi-site distributed education	video streaming;user layer;human friendly;machine friendly;language translation;authoring languages;user preferences;video streaming authoring languages education language translation;timed automaton;three layer virtual director model;execution layer;automated display management system;msde;video stream;cameras switches automata streaming media displays virtual colonoscopy tiles automatic control humans;display layer;display layer three layer virtual director model automated multisite distributed education msde video stream automated display management system human friendly machine friendly user layer automatic translation execution layer;automated multisite distributed education;automatic translation	In multi-site distributed education (MSDE), video streams from multiple sites are available. To best utilize the limited screen space at each site, we develop a customizable, automated display management system in this paper, i.e., only user-preferred streams will be shown as triggered by events and timers. The configuration of such user preference, however, is challenging because it has to be both human-friendly and machine-friendly. To address this challenge, we propose a three-layer virtual director model. In the user layer, we identify three categories of parameters that can represent a wide range of user preferences yet are easy to use. These preferences are then automatically translated into a machine-friendly timed automaton in the execution layer. The automaton is simulated dynamically, which selects a subset of streams to show on the screen through a display layer. Evaluation results demonstrate the correctness and efficiency of the proposed framework	access grid;automata theory;computer science;correctness (computer science);formal verification;glossary of computer graphics;lecture recording;msde;multitier architecture;spaces;springer (tank);streaming media;timed automaton;timer;uppaal;usability;user (computing);user experience	Bin Yu;Cha Zhang;Yong Rui;Klara Nahrstedt	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262526	simulation;computer science;operating system;multimedia;world wide web;timed automaton;algorithm	DB	-43.8992750247766	-33.25902257728346	85733
a494191eed61da992579d67412d504f5b3104816	a gesture-free geometric approach for mid-air expression of design intent in 3d virtual pottery	natural user interfaces;mid air interactions;gestures;hand based shape modeling;virtual pottery;mesh deformation	The advent of depth cameras has enabled mid-air interactions for shape modeling with bare hands. Typically, these interactions employ a finite set of pre-defined hand gestures to allow users to specify modeling operations in virtual space. However, human interactions in real world shaping processes (such as pottery or sculpting) are complex, iterative, and continuous. In this paper, we show that the expression of user intent in shaping processes can be derived from the geometry of contact between the hand and the manipulated object. Specifically, we describe the design and evaluation of a geometric interaction technique for bare-hand mid-air virtual pottery. We model the shaping of a pot as a gradual and progressive convergence of the pot’s profile to the shape of the user’s hand represented as a point-cloud (PCL). Thus, a user does not need to learn, know, or remember any gestures to interact with our system. Our choice of pottery simplifies the geometric representation, allowing us to systematically study how users use their hands and fingers to express the intent of deformation during a shaping process. Our evaluations demonstrate that it is possible to enable users to express their intent for shape deformation without the need for a fixed set of gestures for clutching and deforming a shape.	digital camera;interaction technique;iterative method;noise shaping;point cloud;virtual reality	Vinayak;Karthik Ramani	2015	Computer-Aided Design	10.1016/j.cad.2015.06.006	simulation;engineering;gesture;engineering drawing	HCI	-38.07560750842825	-33.80996160767452	85793
85930ede6ca1e1ee7bb725d63d037772dd3b0c13	interactive design and simulation of net sculptures	3d graphical user interfaces;mass spring particle systems;user interface;net sculptures;particle system;graphic user interface;drag and drop;interaction design;3d graphics	We present a graphical user interface that allows an artist to virtually design and visualize net sculptures. Net sculptures consist of net pieces that are seamlessly connected to each other and to fixed rails. They are flexible and hence dynamic under external forces such as gravity and wind. The interface that we describe allows an artist to create net sculptures made up of multiple net pieces. Simple operations such as clicking on points and click-and-drag gestures are used to create and modify individual net pieces, and drag-and-drop gestures are used to connect net pieces to multiple rails. The effect of gravity on the net sculpture is simulated, allowing the artist to simultaneously design and visualize net sculptures as they would appear once installed in a real setting.	drag and drop;graphical user interface;interactive design;interactivity;simulation	Grigore D. Pintilie;Peter Heppel;Janet Echelman	2010		10.1007/978-3-642-13544-6_7	simulation;human–computer interaction;computer science;operating system;interaction design;particle system;graphical user interface;user interface;3d computer graphics;computer graphics (images)	Graphics	-42.52165477039634	-37.72299986035024	85843
5d265ddac536b3c3a5246362843be9facfbdc970	web-based animation of data structures using jawaa	command language;web pages;teaching techniques;introductory courses;large courses;course management;data structure	JAWAA is a simple command language for creating animations of data structures and displaying them with a Web browser. Commands are stored in a script file that is retrieved and run by the JAWAA applet when the applet's Web page is accessed through the Web. JAWAA commands allow for creation and movement of primitive objects (circles, lines, text, rectangles) and data structure objects (arrays, stacks, queues, lists, trees and graphs). A JAWAA script can be generated as the output of a program written in any language.	applet;command language;data structure;shell script;web page;world wide web	Willard C. Pierson;Susan H. Rodger	1998		10.1145/273133.274310	static web page;web development;data web;data structure;computer science;operating system;software engineering;teaching method;web page;database;programming language;world wide web	Theory	-43.49998329417604	-25.599899256605436	85878
4bccc06d713e0f8997bf4c8e2a16c198ed10d374	interaction and the epistemic potential of digital libraries	digital library;information visualization;knowledge environment;epistemic action	This article presents a framework of micro-level interactions with visual representations of information in digital libraries. The framework is comprised of three basic interactions—conversing, manipulating, and navigating—and 13 task-based interactions: animating, annotating, chunking, cloning, collecting, composing, cutting, filtering, fragmenting, probing, rearranging, repicturing, and searching. In a typical digital library, the purpose of interaction is to locate and access relevant information. In this framework, the purpose of interaction is to help people create knowledge, develop understanding, solve problems, and acquire insight from the resources in a collection. In other words, interaction can have epistemic benefits and, consequently, it can be used to leverage the epistemic potential of digital libraries.	automated planning and scheduling;content-control software;digital library;ip fragmentation;information;instruction scheduling;interaction;library (computing);organizing (structure);problem solving;shallow parsing;vocabulary	Karl Fast;Kamran Sedig	2010	International Journal on Digital Libraries	10.1007/s00799-011-0066-8	digital library;information visualization;computer science;knowledge management;data mining;multimedia;world wide web	HCI	-41.52080366458656	-25.37543857969703	86081
eb6fba50f23141a435412cc5a617325dbe5ee456	conversational text input for modifying graphics facial images	short term memory;text input;line drawings;hash table;facial features;data structure	This paper reports on a text interpretation program for a minicomputer with 8K memory to facilitate modifying line drawings of faces. An interactive language is described that allows conversational dialogues between the user and image modification routines. We present an implementation that retains context during a dialogue and makes possible relational adjustments of facial features. Imprecise feature judgements issued by the user are used to modify images. Ambiguities encountered by the program are resolved by interrogating short term memory buffers and hash table data. Language and hardware features are combined by data structures that interface display processor instructions and requests generated by the interpreted text. The updating of these structures is discussed in terms of relationships between structure elements.	8k resolution;data structure;graphics;hash table;image editing;interpreted language;minicomputer	Michael L. Rhodes;Allen Klinger	1977	SIGART Newsletter	10.1145/1045283.1045334	natural language processing;hash table;speech recognition;data structure;computer science;artificial intelligence;short-term memory;programming language	Graphics	-38.8167768979548	-30.422523529077946	86400
2c4af1325cf20bb5b9ae7ebbd6495586afbf66e8	smooth video hopping for surveillance cameras	expressive imagery;non realistic modeling	As more and more surveillance cameras are installed in towns and cities, they are going to be more open to the public, and people may want to see these videos. In such a scenario, people may find it difficult to understand which place they are seeing because it is almost impossible to remember which camera images are from which place and from which direction. This problem is particularly important when a viewer changes from one camera to another to see a scene from a different angle. This happens because the viewer sometimes does not understand the spatial relationship among cameras in a real space. When the image is instantly switched from one camera to the other, the viewer may feel disoriented because he/she has to place the location of the new camera in his/her mind to recognize what he/she is seeing. Therefore, a sophisticated camera switching method is needed that enables viewers understand how camera viewpoints change.	closed-circuit television;frequency-hopping spread spectrum;towns	Takahiro Tsuda;Itaru Kitahara;Yoshinari Kameda;Yuichi Ohta	2006		10.1145/1179849.1180010	computer vision;simulation;computer science;computer graphics (images)	Vision	-40.47330972380989	-36.46276458151264	86533
8d2b2e05d262832a9423e7c60fdf9a19b72de8c5	on-the-fly time scaling for compressed audio streams	time scale;real time;data format;linear time;on the fly;media synchronization	Time scaling is a technique used to modify media-object presentation duration. This paper proposes an audio time-scaling algorithm focused on supporting applications that need: to maintain the original data format for storage or immediate presentation on any legacy audio player; to perform linear time scaling in real time, allowing the adjustment factor to vary along the audio presentation; and to perform time mark-up maintenance, that is, to compute new time values for original marked audio time instants. The proposed algorithm is appropriate for those applications that do not need a great adjustment factor variation. The integration with content rendering tools is presented in the paper and also an example of using these tools in a hypermedia presentation formatter.	2.5d;algorithm;hypermedia;image scaling;time complexity	Suzana Maranhão;Rogério Rodrigues;Luiz Fernando Soares	2006		10.1007/978-3-540-70760-8_17	time complexity;real-time computing;computer hardware;computer science;multimedia	PL	-42.61648544739332	-34.534457286231465	86539
82cb0ddad847e96da8176eae56ec12d4d78ebb99	vcs: a virtual collaborative space based on immersive teleconferencing	modelizacion;sistema interactivo;video object;teletravail;interfase usuario;concepcion ingenieria;engineering design;collaborative work;realite virtuelle;realidad virtual;teleconference;user interface;conception ingenierie;virtual reality;technique video;tecnica video;natural interaction;integrated design;concepcion integrada;systeme conversationnel;teleconferencia;modelisation;realite augmentee;realidad aumentada;senal video;signal video;teletrabajo;interactive system;remote work;video signal;video technique;interface utilisateur;ingenierie simultanee;ingenieria simultanea;augmented reality;modeling;virtual space;conception integree;spatial model;concurrent engineering;cooperative work	The traditional video teleconferencing systems provide a video in a window interface paradigm, it's not sufficient for naturally interactive interface for the collaborative work. We design a collaborative space called VCS. In VCS, a virtual collaborative space is built based on immersive teleconferencing. The remote conferees can discuss in the virtual space as similar to do it in the local rooms. VCS can be used to provide a new paradigm for the remote cooperative work. This paper presents the spatial model and video object extraction technique of VCS.	veritas cluster server	Weidong Bao;Maojun Zhang;Wang Chen	2006		10.1007/11941354_36	augmented reality;simulation;teleconference;systems modeling;computer science;artificial intelligence;virtual reality;multimedia;user interface;concurrent engineering;computer graphics (images)	Visualization	-35.53378002236045	-27.286217756210537	86572
216777da15b1b309d3e16d24586b197923450859	follow-me!: conducting a virtual concert	conducting;gesture recognition	In this paper, we present a real-time continuous gesture recognition system for conducting a virtual concert. Our systems allow the user control over beat, by conducting four different beat-pattern gestures; tempo, by making faster or slower gestures; volume, by making larger or smaller gestures; and instrument emphasis, by directing the gestures towards specific areas of the orchestra on a large display. A recognition accuracy of up to 95% could be achieved for the conducting gestures (beat, tempo, and volume).	gesture recognition;real-time clock;user interface;virtual concert	Seungju Han;Jung-Bae Kim;James D. K. Kim	2012		10.1145/2380296.2380324	speech recognition;computer science;gesture recognition;conductor	HCI	-46.00130271825405	-36.09918882455952	86621
5550ab50ffabe55620c316fe000b71797bdf671d	a hand drumming dataset for physical modeling		Physical modeling is a proven technique for creating sounds with rich expressive potential, but the state of the art in control does not offer access to the whole of this potential. New developments in modeling algorithms are typically presented with single-point, idealized excitations where more complex ones would add vitality to the sounds produced. The 2D waveguide mesh, in particular, can be excited simultaneously at multiple points on a surface, like a physical drum by a hand. The authors present a synthesis system in which this control has been implemented using a 2D pressure sensor, resulting in sounds that capture the some of the salient qualities of hand drumming. A dataset of 2D force measurements from various hand drumming techniques is presented, to be used by researchers in physical modeling synthesis.	algorithm	Randy Jones;Mathieu Lagrange;W. Andrew Schloss	2007			waveguide;pressure sensor;computer vision;artificial intelligence;computer science	Graphics	-45.574660076362434	-35.0938979686414	86898
9714062de869e00c2ab31289b049b2f21c11e5fd	a next generation browsing environment for large image repositories	image databases;e commerce;image database browsing;image database;multimedia data;next generation;content based image retrieval;hue sphere	Next generation environments will change the way people work and live as they will provide new advances in areas ranging from remote work and education, e-commerce, gaming to information-on-demand. In many of these applications intelligent interpretation of multimedia data such as image, video and audio resources is necessary. In this paper we present an effective approach to handling image repositories providing the user with an intuitive interface of visualising and browsing large collections of pictures. Based on the idea of similarity-based organisation of images where images that are visually similar are located close to each other in visualisation space, images are projected onto a sphere with which the user can interact. Rotating the sphere reveals images of different colours while tilting operations focus on brighter or darker images. Large image collections are handled through a hierarchical approach that brings up similar, previously hidden, images when zooming in on an area. Furthermore, the way images are organised can be interactively changed by the user. Our next generation browsing environment has been successfully tested on a large database of several thousand images.	angularjs;color;database;e-commerce;interactivity;telecommuting;tree structure;zooming user interface	Gerald Schaefer	2009	Multimedia Tools and Applications	10.1007/s11042-009-0409-2	e-commerce;computer vision;image retrieval;computer science;multimedia;world wide web	Graphics	-39.516697922792105	-34.241741952245945	86917
bdd96ccc5bb39bcca9e106980ae8b5d02e57437d	vr-cad integration: multimodal immersive interaction and advanced haptic paradigms for implicit edition of cad models	3d interaction;computer aided design;naming;boundary representation;real time;virtual reality;construction history graph;haptics;multimodal interaction;virtual environment;gesture recognition	This paper presents an approach for the integration of Virtual Reality (VR) and Computer-Aided Design (CAD). Our general goal is to develop a VR–CAD framework making possible intuitive and direct 3D edition on CAD objects within Virtual Environments (VE). Such a framework can be applied to collaborative part design activities and to immersive project reviews. The cornerstone of our approach is a model that manages implicit editing of CAD objects. This model uses a naming technique of B-Rep components and a set of logical rules to provide straight access to the operators of Construction History Graphs (CHG). Another set of logical rules and the replay capacities of CHG make it possible to modify in real-time the parameters of these operators according to the user's 3D interactions. A demonstrator of our model has been developed on the OpenCASCADE geometric kernel, but we explain how it can be applied to more standard CAD systems such as CATIA. We combined our VR–CAD framework with multimodal immersive interaction (using 6 DoF tracking, speech and gesture recognition systems) to gain direct and intuitive deformation of the objects' shapes within a VE, thus avoiding explicit interactions with the CHG within a classical WIMP interface. In addition, we present several haptic paradigms specially conceptualized and evaluated to provide an accurate perception of B-Rep components and to help the user during his/her 3D interactions. Finally, we conclude on some issues for future researches in the field of VR–CAD integration.	catia;computer-aided design;geometric modeling kernel;gesture recognition;haptic technology;multimodal interaction;open cascade technology;real-time transcription;virtual reality;wimp (computing)	Patrick Bourdot;Thomas Convard;Flavien Picon;Mehdi Ammi;Damien Touraine;Jean-Marc Vézien	2010	Computer-Aided Design	10.1016/j.cad.2008.10.014	simulation;human–computer interaction;computer science;virtual machine;computer aided design;multimodal interaction;gesture recognition;virtual reality;multimedia;haptic technology;boundary representation	Visualization	-42.9436358743281	-36.64900356251664	86964
d7b436a14d5ad57db5098c3bb4e067e8cd0dcbec	dynamically reconfigurable vision-based user interfaces	vision system;image processing;dynamic reconfiguration;user interface;vision based user interface;perceptual user interfaces;vision based interaction;interactive display;interactive system;field of view;typical development;physical environment	We describe a system that supports practical, vision-based user interfaces, addressing the issues of a usable interaction paradigm, support for application developers, and support for application deployment in real-world environments. Interfaces are defined as configurations of predefined interactive widgets that can be moved from one surface to another. Complex interfaces can be dynamically reconfigured, changing both form and location on the fly, because the functional definition of the interface is decoupled from the specification of its location in the environment. We illustrate the power of such an architecture in the context of projected interactive displays.	code reuse;computer vision;image processing;input device;interaction technique;link/cut tree;on the fly;programming paradigm;prototype;reconfigurability;reconfigurable computing;software deployment;user interface;xml protocol	Rick Kjeldsen;Anthony Levas;Claudio S. Pinhanez	2004	Machine Vision and Applications	10.1007/s00138-004-0145-6	user interface design;computer vision;machine vision;field of view;human–computer interaction;image processing;computer science;multimedia;post-wimp;natural user interface;user interface	HCI	-43.454481877041864	-34.50498840097464	86972
22565b64d8a6ed249148b5b89a110003674b6b1d	toward an agent-based platform for learning objects management	busqueda informacion;distributed system;outil logiciel;reseau social;distance education;software tool;multiagent system;teleenseignement;systeme reparti;enseignement superieur;agent based;information retrieval;articulo;customization;personnalisation;higher education;intelligence artificielle;social network;ensenanza superior;sistema repartido;innovation;telecomunicacion;recherche information;object oriented;agent intelligent;telecommunication;comportement utilisateur;learning object;intelligent agent;personalizacion;oriente objet;artificial intelligence;teleensenanza;agente inteligente;inteligencia artificial;user behavior;educacion;remote teaching;sistema multiagente;innovacion;herramienta software;orientado objeto;toward an agent based platform for learning objects management;red social;comportamiento usuario;geographic distribution;systeme multiagent	The current paper proposes a platform for learning objects manage- ment, based on the paradigm of intelligent agents. Such platform offers a per- sonalization of the information based on user's behavior, an intelligent search capability for seeking some information between the objects and their ramifica- tions, as well as the current state of the resources that compose to a certain ob- ject. This will allow the identification of the right objects to create suitable edu- cative courses. The main contributions of the paper are: in first place, the proposal of integrating the technology of intelligent agents to a platform of learning objects, which makes more easy and precise the search of objects in several repositories located, mainly, in geographically distributed places; Sec- ond of all, the improvement of a technological tool which supports the process of education, through a scheme of distance education, utilized by the University of Colima; Finally, we intend to promote the adaptation of these educative schemes to the new technological advances, brought by Computer Science and Telecommunications in last years.		Nicandro Farías Mendoza;Gabriel Cruz;Orvil Ceja;Miguel Díaz;José Macías	2004		10.1007/11553762_11	distance education;innovation;simulation;computer science;artificial intelligence;object-oriented programming;higher education;intelligent agent;social network	AI	-37.68091295520133	-24.84772158205772	87131
7c0368cd0cb32708414736559a9ebef05df56089	interactive mr game based on intelligent agent	character;dynamic environment;chat;intelligent agent;agent systems;handheld device;mixed reality;entertainment	Up to date handheld devices have been developed enough in order to be used as platforms for Mixed Reality (MR). In the previous work, we have discussed incorporating a dynamic environment to offer better experience, which is verified as a new element for the field of digital elements. Providing users with more immersive and realistic experience, we have designed an agent system which regards agents as characters in a game to make them collaborate with each other. The characters are expected to behave more intelligently and naturally thanks to this advance.	artificial intelligence;intelligent agent;mixed reality;mobile device	Jaemin Soh;Gowun Jeong;Kyusung Cho;Hyun Seung Yang	2008		10.1145/1501750.1501861	simulation;human–computer interaction;engineering;multimedia	HCI	-44.55790539460223	-37.35993001142589	87232
3ea8e59e47e0d3a1549968df1de59e5e8351c081	using navcon for conceptual navigation in web documents	sensibilidad contexto;site web;documento electronico;ingenieria del conocimiento;controle acces;web documents;ontologie;context aware;hipertexto;navegacion informacion;web pages;red www;metadata;ingenierie connaissances;navigation information;web semantique;knowledge management;reseau web;information browsing;systeme ouvert;document electronique;internet;website browsing;conceptual navigation;web semantica;metadonnee;semantic web;world wide web;ontologia;access control;metadatos;sitio web;sensibilite contexte;open systems;sistema abierto;context based navigation;ontology;hypertexte;open hypertext system;hypertext;web site;electronic document;knowledge engineering	This paper presents conceptual navigation and NavCon, an architecture that implements this navigation in World Wide Web pages. NavCon architecture makes use of ontology as metadata to contextualise user's search for information. Conceptual navigation is a technique to browse websites within a context. Context filters relevant retrieved information, and it drives user's navigation through paths that meet his needs. Based on ontologies, NavCon automatically inserts conceptual links in web pages. These links permit the users to access a graph representing concepts and their relationships. Browsing this graph, it is possible to reach documents associated with user's desired ontology concept.	web page	José Renato Villela Dantas;Pedro Porfírio Muniz Farias	2009	IJAMC	10.1504/IJAMC.2009.028711	the internet;hypertext;computer science;access control;semantic web;web navigation;knowledge engineering;ontology;web page;database;multimedia;open system;metadata;world wide web	NLP	-38.674833333529676	-25.445958927399943	87251
2391914d88cea16a032f6e71890578ab425022fc	generating musical performances with director musices	music performance;rules	23 Computer M usic Journal, 24:3, pp. 23± 29, Fall 2000 © 2000 M assachusetts Inst itute of Technology . Director Musices is a program that transforms notated scores into musical performances. It implements the performance rules emerging from research projects at the Roy al Institute of Technology (KTH). Rules in the program model performance aspects such as phrasing, articulat ion, and intonation, and they operate on performance v ariables such as tone, inter-onset duration, amplitude, and pitch. By manipulating rule parameters, the user can act as a metaperformer controlling different features of the performance, leav ing the technical execution to the computer. Different interpretations of the same piece can easily be obtained. Features of Director M usices include M IDI file input and output , rule palettes, graphical display of al l performance v ariables (along w ith music notat ion), and user-defined performance rules. The program is implemented in Common Lisp and is av ailable free as a s tand-alone applicat ion both for M acintosh and Window s platforms. Further information, including music examples, publicat ions, and the softw are itself, is located online at http:// w w w .speech.k th.se/music/performance/.	common lisp;graphical user interface;infographic;input/output;onset (audio);performance;pitch (music)	Anders Friberg;Vittorio Colombo;Lars Frydén;Johan Sundberg	2000	Computer Music Journal	10.1162/014892600559407	speech recognition;computer science;multimedia	ML	-47.661670462767894	-25.877869628839235	87350
3496dacb689da6fa89242bbaba1983ab33b3963a	reagent: converting ordinary webpages into interactive software agents		We introduce Reagent, a technology that readily converts ordinary webpages containing structured data into software agents with which one can interact naturally, via a combination of speech and pointing. Previous efforts to make webpage content manipulable by third-party software components in browsers or desktop applications have generally relied upon specialized instrumentation included in the webpages – a practice that neither scales well nor applies to preexisting webpages. In contrast, Reagent automatically captures semantic details and semantically-meaningful mouse events from arbitrary webpages that contain no pre-existing special instrumentation. Reagent combines these events with text transcriptions of user speech to derive and execute parameterized commands representing human intent. Thus, users may request various visualization or analytic operations to be performed on data displayed on a page by speaking to it and/or pointing to elements within it. When unable to infer translations between event labels and human terminology, Reagent proactively asks users for definitions and adds them to its dictionary. We demonstrate Reagent in the context of a collection of pre-existing webpages that contain football team and player statistics.	component-based software engineering;desktop computer;dictionary;software agent;third-party software component;web page	Matthew Peveler;Jeffrey Kephart;Hui Su	2018	CoRR		component-based software engineering;information retrieval;multimedia;web page;computer science;terminology;data model;software agent;visualization;transcription (linguistics);parameterized complexity	HCI	-40.39656799437713	-29.621228020479553	87468
bb8eb01872b0fd3669a0aa59e44402f210aa0c3e	user adaptive answers generation for conversational agent using genetic programming	genetique;site web;metodo adaptativo;interfase usuario;coreano;genetic program;analisis datos;genetica;user interface;interrogation base donnee;interrogacion base datos;user adaptation;methode adaptative;intelligence artificielle;genetics;korean;data analysis;coreen;agent intelligent;adaptive method;intelligent agent;conversational agent;utilisabilite;artificial intelligence;analyse donnee;interface utilisateur;agente inteligente;inteligencia artificial;sitio web;usabilidad;usability;ge netic programming;database query;web site	Recently, it seems to be interested in the conversational agent as an effective and familiar information provider. Most of conversational agents reply to user’s queries based on static answers constructed in advance. Therefore, it cannot respond with flexible answers adjusted to the user, and the stiffness shrinks the usability of conversational agents. In this paper, we propose a method using genetic programming to generate answers adaptive to users. In order to construct answers, Korean grammar structures are defined by BNF (Backus Naur Form), and it generates various grammar structures utilizing genetic programming (GP). We have applied the proposed method to the agent introducing a fashion web site, and certified that it responds more flexibly to user’s queries.	agent-based model;beta normal form;dialog system;genetic programming;usability	Kyoung Min Kim;Sungsoo Lim;Sung-Bae Cho	2004		10.1007/978-3-540-28651-6_121	simulation;usability;computer science;artificial intelligence;database;data analysis;user interface;world wide web;intelligent agent;algorithm;korean	AI	-36.915403573472936	-25.13145329898628	87500
0597b8266b831fbe34d5b0cbae09aecf5214c709	musink: composing music through augmented drawing	musical interfaces;creativity;interactive paper;gestural interface;smooth transition;gesture interfaces;participatory design;end user programming	We focus on the creative use of paper in the music composition process, particularly the interaction between paper and end-user programming. When expressing musical ideas, composers draw in a precise way, not just sketch. Working in close collaboration with composers, we designed Musink to provide them with a smooth transition between paper drawings and OpenMusic, a flexible music composition tool. Musink's built-in recognizers handle common needs, such as scoping and annotation. Users can also define new gestures and associate them with their own or predefined software functions. Musink supports semi-structured, delayed interpretation and serves as a customizable gesture browser, giving composers significant freedom to create their own, individualized composition languages and to experiment with music, on-paper and on-line.	end-user development;finite-state machine;java annotation;online and offline;openmusic;scope (computer science);semiconductor industry	Theophanis Tsandilas;Catherine Letondal;Wendy E. Mackay	2009		10.1145/1518701.1518827	human–computer interaction;multimedia;creativity;world wide web	HCI	-46.598839612899496	-35.713187020859664	87502
9420efb2a2e94a063c46f290c743920d7816143c	haptic carillon - analysis & design of the carillon mechanism		The carillon is one of the few instruments that elicit sophisticated haptic interaction from amateur and professional players alike. Like the piano keyboard, the velocity of a player’s impact on each carillon key, or baton, affects the quality of the resultant tone; unlike the piano, each carillon baton returns a different forcefeedback. Force-feedback varies widely from one baton to the next across the entire range of the instrument and with further idiosyncratic variation from one instrument to another. This makes the carillon an ideal candidate for haptic simulation. The application of synthesized forcefeedback based on an analysis of forces operating in a typical carillon mechanism offers a blueprint for the design of an electronic practice clavier and with it the solution to a problem that has vexed carillonists for centuries, namely the inability to rehearse repertoire in private. This paper will focus on design and implementation of a haptic carillon clavier derived from an analysis of the Australian National Carillon in Canberra.	baton;blueprint;compiler;eclipse modeling framework;feedback;haptic technology;human–computer interaction;input/output;mathematical model;musical keyboard;prototype;resultant;simulation;simulink;switch;the australian;velocity (software development)	Mark Havryliv;Fazel Naghdy;Greg Schiemer;Timothy Hurd	2009				Robotics	-46.64088964228718	-35.60047167796951	87523
a776659c21c9b2b674647ccd927934f3b9301305	render me real?: investigating the effect of render style on the perception of animated virtual humans	uncanny valley;motion capture;facial animation;perception	The realistic depiction of lifelike virtual humans has been the goal of many movie makers in the last decade. Recently, films such as Tron: Legacy and The Curious Case of Benjamin Button have produced highly realistic characters. In the real-time domain, there is also a need to deliver realistic virtual characters, with the increase in popularity of interactive drama video games (such as L.A. Noire™ or Heavy Rain™). There have been mixed reactions from audiences to lifelike characters used in movies and games, with some saying that the increased realism highlights subtle imperfections, which can be disturbing. Some developers opt for a stylized rendering (such as cartoon-shading) to avoid a negative reaction [Thompson 2004]. In this paper, we investigate some of the consequences of choosing realistic or stylized rendering in order to provide guidelines for developers for creating appealing virtual characters. We conducted a series of psychophysical experiments to determine whether render style affects how virtual humans are perceived. Motion capture with synchronized eye-tracked data was used throughout to animate custom-made virtual model replicas of the captured actors.	3d modeling;experiment;humans;interactive storytelling;l.a. noire;motion capture;real-time clock;shading;tron;virtual actor;virtual reality	Rachel McDonnell;Martin Breidt;Heinrich H. Bülthoff	2012	ACM Trans. Graph.	10.1145/2185520.2185587	computer vision;motion capture;simulation;computer facial animation;computer science;uncanny valley;multimedia;perception;computer graphics (images)	Graphics	-40.1188534596709	-35.86118072052572	87710
8bed98ff97f6a529a53eda7de2d44609d3ab84a4	the comprovisador's real-time notation interface (extended version)		Comprovisador is a system designed to enable real-time mediated soloist-ensemble interaction, through machine listening, algorithmic procedures and dynamic staff-based notation. It uses multiple networked computers – one host and several clients – to perform algorithmic compositional procedures with the music material improvised by a soloist and to coordinate the musical response of an ensemble. Algorithmic parameters are manipulated by a conductor/composer who mediates the interaction between soloist and ensemble, making compositional decisions in real-time. The present text, an extended version of a paper presented at CMMR 2018, in Matosinhos, focuses on the notation interface of this system, after overviewing its concept and structure. A discussion is made on how rehearsals and live performances impacted the development of the interface.	real-time transcription	Pedro Louzeiro	2017		10.1007/978-3-030-01692-0_33	musical;multimedia;notation;human–computer interaction;computer science;machine listening;musical improvisation;graphical user interface	Logic	-46.63087047337295	-35.12526476241225	87829
83eb819bde1ca1f4f11cafe7d139c01d30cbb38a	exploring spatial meaning with a tangible map		In this paper, we describe the Tangible Map, an interactive, table-mounted tactile / digital display aimed at facilitating exploration of an environment and the data generated therein. For visitors in an urban setting, translating abstract map symbols into comprehensible information can be a challenging task due to the level of abstraction and the disconnect between flat displays and spatial information. We address this question by combining a tangible user interface (created with 3D-printed buildings) and dynamic spatial information. To explore these concepts, we designed and fabricated the Tangible Map to serve as an interactive centerpiece within the MIT campus. Through application of user-centered design principles, we demonstrate strategies for managing user attention across beyond-field-of-view displays.	3d printing;display device;tangible user interface;user-centered design	Will Walker;Hyungie Sung;Chris Kevin Ong;Federico Casalegno	2017		10.1145/3078810.3078826	display device;human–computer interaction;map symbolization;spatial analysis;multimedia;design elements and principles;tangible user interface;abstraction;computer science;urban informatics	HCI	-35.84832362786508	-33.95844804319062	87849
b3b7aa363058e8c6de766c8c767045f88ccb564b	cpn/tools: a tool for editing and simulating coloured petri nets etaps tool demonstration related to tacas	marking;interfase usuario;plataforma;representation graphique;systeme unix;marking menu;coloured petri net;red petri;user interface;representacion grafica;unix system;espace etat;platform;marcacion;state space;reperage;interface utilisateur;sistema unix;espacio estado;petri net;plateforme;graphics;reseau petri;interaction technique	CPN/Tools is a major redesign of the popular Design/CPN tool for editing, simulation and state space analysis of Coloured Petri Nets. The new interface is based on advanced interaction techniques, including bi-manual interaction, toolglasses and marking menus and a new metaphor for managing the workspace. It challenges traditional ideas about user interfaces, getting rid of pull-down menus, scrollbars, and even selection, while providing the same or greater functionality. CPN/Tools requires an OpenGL graphics accelerator and will run on all major platforms (Windows, Unix/Linux, MacOS). 1 The CPN/Tools Interface Interaction techniques for desktop workstations have changed little since the creation of the Xerox Star in the early eighties. The vast majority of today’s interfaces are still based on a single mouse and keyboard to manipulate windows, icons, menus, dialog boxes, and to drag and drop objects on the screen. While these interfaces are now ubiquitous they are also reaching their limits: as new applications become more powerful, the corresponding interfaces become too complex. CPN/Tools [1] addresses this trade-off between power and ease-of-use by combining new interaction techniques into a consistent and simple interface for editing and simulating Coloured Petri Nets [2, 3]. Coloured Petri Nets frequently contain a large number of pages, which are similar to modules in programming languages. In CPN/Tools we have designed a window manager that makes it easy to manage these modules. The workspace occupies the whole screen and contains window-like objects called binders. Binders contain pages, each equivalent to a window in a traditional environment. Each page has a tab similar to those found in tabbed dialogs. Clicking the tab brings that page to the front of the binder. A page can be dragged to a different binder or to the background to create a new binder for it. Binders reduce the number of windows on the screen and the time spent organizing them. Binders also help users organize their work by grouping related pages together and reducing the time spent looking for hidden windows. CPN/Tools supports multiple views, allowing several binders to contain a representation of the same page. For example one binder can contain a view on a page including simulation information while another binder can contain a view on the same page without simulation information and at a smaller scale (Fig.1). The CPN/Tools interface requires a keyboard and two pointing devices. For a right-handed user we use a mouse for the right hand and a trackball for the left hand. The mouse is used for tasks that may require precision, while the trackball is used for tasks that do not require much precision e.g. moving tools. For simplicity we assume a right-handed user in our description of interaction techniques. The interface has no menu bars, no pull-down menus, no scrollbars and no dialog boxes. Instead, it uses a unique combination of traditional, recent and novel interaction techniques: Direct manipulation (i.e. clicking or dragging objects) is used for frequent operations such as moving objects, panning the content of a view and editing text. When a tool is held in the right hand, e.g. after having selected it in a floating palette, direct manupulation actions are still available via a long click, i.e. pressing the mouse button, waiting for a short delay until the cursor changes, and then either dragging or releasing the mouse button. Bi-manual manipulation is a variant of direct manipulation that involves using both hands for a single task. It is used to resize objects (binders, places, transitions, etc.) and to zoom the view of a page. The interaction is similar to holding an object with two hands and stretching or shrinking it. Unlike traditional window management techniques, using two hands makes it possible to simultaneously resize and move a binder, or pan and zoom the view of a page. Marking Menus[5] are radial, contextual menus that appear when clicking the right button of the mouse. Marking menus offer faster selection than traditional linear menus for two reasons. First, it is easier for the human hand to move the cursor in a given direction than to reach for a target at a given distance. Second, the menu does not appear when the selection gesture is executed quickly, which supports a smooth transition between novice and expert use. Kurtenbach and Buxton [5] have shown that selection times can be more than three times faster than with traditional menus. Keyboard input is used only to edit text. Some navigation commands are available at the keyboard to make it easier to edit several inscriptions in sequence without having to move the hands to the pointing devices. Keyboard modifiers and shortcuts are not necessary since most of the interaction is carried out with the two hands on the pointing devices. Floating palettes contain tools represented by buttons. Clicking a tool with the mouse activates this tool, i.e. the user conceptually holds the tool in the hand. Clicking on an object with the tool in hand applies the tool to that object. Floating palettes are moved with the left hand, making it easy to bring the tools close to the objects being manipulated, and saving the time spent moving the Fig. 1. The CPN/Tools interface. The left column is called the index. The top-left and top-right binders contain different views of the same page. The views are scaled differently and the view in the top-left binder contains simulation information. The bottom binder contains six pages represented by tabs: The page in front shows several magnetic guidelines (dashed lines). In the top right binder a circular marking menu has been popped up on a page. The palette with the VCR-like controls is a floating palette, while a toolglass is positioned over the page in the bottom binder. This toolglass can be used to edit colours, linetypes and linethicknesses. cursor to a traditional menubar or toolbar. Floating palettes can be dropped in the workspace and become a standard toolpalette. In many current interfaces, after a tool is used (especially a creation tool), the system automatically activates a “select” tool. This supports a frequent pattern of use in which the user wants to move an object immediately after it has been created but causes problems when the user wants to create additional objects of the same type. CPN/Tools avoids this automatic changing of the current tool by ensuring that the user can always move an object, even when a tool is active, with a long click of the mouse. This mimics the situation in which one continues holding a physical pen while moving an object out of the way in order to write. Toolglasses[4] like floating palettes, contain a set of tools represented by buttons, and are moved with the left hand, but unlike floating palettes, they are semi-transparent. A tool is applied to an object with a click-through action: The tool is positioned over the object of interest and the user clicks through the tool onto the object. The toolglass disappears when the tool requires a drag interaction, e.g. when creating an arc. This prevents the toolglass from getting in the way and makes it easier to pan the document with the left hand when the target position is not visible. This is a case where the two hands operate simultaneously but independently. Magnetic guidelines are used to align objects and keep them aligned. Moving an object near a guidelinet causes the object to snap to it. Objects can be removed from a guideline by dragging them away from it. Moving a guideline moves all the objects attached to it, maintaining their alignment. Preliminary results from our user studies make it clear that none of the above techniques is always better or worse. Rather, each emphasizes a different, but common pattern of use. Marking menus work well when applying multiple commands to a single object. Floating palettes work well when applying the same command to different objects. Toolglasses work well when the work is driven by the structure of the diagram, such as working around a cycle in a Petri net.	align (company);bi-directional text;color;coloured petri net;cursor (databases);desktop computer;diagram;direct manipulation interface;drag and drop;end-user license agreement;european joint conferences on theory and practice of software;file binder;graphics processing unit;interaction technique;item unique identification;linux;microsoft windows;mouse button;name binding;opengl;organizing (structure);paging;palette (computing);pie menu;pointing device;pro tools;programming language;programming tool;radial (radio);semiconductor industry;simulation;state space;trackball;unix;usability testing;user interface;videocassette recorder;window manager;workspace;workstation;xerox star;dialog	Michel Beaudouin-Lafon;Wendy E. Mackay;Mads Jensen;Peter Andersen;Paul Janecek;Henry Michael Lassen;Kasper Lund;Kjeld Høyer Mortensen;Stephanie Munck;Anne V. Ratzer;Katrine Ravn;Søren Christensen;Kurt Jensen	2001		10.1007/3-540-45319-9_39	embedded system;real-time computing;computer science;state space;graphics;operating system;platform;user interface;petri net;interaction technique	HCI	-41.71627822572728	-30.75286464853992	87927
b80294dd1b71b217c6f0b298dd62f6bddfab036d	the collaborative design platform - a protocol for a mixed reality installation for improved incorporation of laypeople in architecture	j 5 computer applications arts and humanities architecture c 2 2 computer systems organization computer communication networks network protocols h 5 1 information systems information interfaces and presentation multimedia information systems	Presentations and discussions between architects and clients during the early stages of design usually involve sketches, paper and models, with digital information in the form of simulations and analyses used to assess variants and underpin arguments. Laypeople, however, are not used to reading plans or models and find it difficult to relate digital representations to the real world. Immersive environments represent an alternative approach but are laborious and costly to produce, particularly in the early design phases where information and ideas are still vague. Our project shows, how linking analogue design tools and digital VR representation has given rise to a new interactive presentation platform that bridges the gap between analogue design methods and digital architectural presentation. The prototypical platform creates a direct connection between a physical volumetric model and interactive digital content using a large-format multi-touch table as a work surface combined with real-time 3D scanning. Coupling the 3D data from the scanned model with the 3D digital environment model makes it possible to compute design relevant simulations and analyses. These are displayed in real-time on the working model to help architects assess and substantiate their design decisions. Combining this with a 5sided projection installation based on the concepts Carolina Cruz Neiras CAVE Automatic Virtual Environment (CAVE)1 offers an entirely new means of presentation and interaction. The design (physical working model), the surroundings (GIS data) and the simulations and analyses are presented stereoscopically in real-time in the virtual environment. While the architect can work as usual, the observer is presented with an entirely new mode of viewing. Different ideas and scenarios can be tried out spontaneously and new ideas can be developed and viewed directly in three dimensions. The client is involved more directly in the process and can contribute own ideas and changes, and then see these in user-centred stereoscopic 3D. By varying system parameters, the model can be walked through at life size.	3d scanner;cave automatic virtual environment;digital data;digital environment;digital recording;geographic information system;mixed reality;multi-touch;real-time clock;real-time computing;simulation;stereoscopy;vagueness;virtual reality;volume mesh	Tibor Goldschwendt;Christoph Anthes;Gerhard Schubert;Dieter Kranzlmüller;Frank Petzold	2014		10.1109/ISMAR.2014.6948477	human–computer interaction;computer science;applications architecture;multimedia	HCI	-36.86508127419702	-32.907986520949336	88035
3c3f3adf9574bd9c190f230d078ada4c3bcdad85	bringing the web closer: stereoscopic 3d web conversion	3d;stereoscopic;interaction;web;depth;html;interface;javascript;framework	In this paper we present 3DSjQ, a tool used to implement stereoscopic 3D in web pages. It provides HTML developers the possibility to create static and dynamic content that interacts with depth. We uncover the algorithm used for the tool, describe the method of operation and discuss future work including further development and implementations.		Alexey Chistyakov;Diego González-Zúñiga;Jordi Carrabina	2013		10.1007/978-3-319-03068-5_5	ajax;web development;web modeling;html;web design;computer science;multimedia;client-side scripting;world wide web;computer graphics (images)	Web+IR	-42.98582682395331	-27.951943516065445	88270
39007ad20e6d1dc974486db1e44ff05dd9a6b38e	a rule-based approach to animating multi-agent environments	doctorate;computer animation computer software signal processing information theory	This dissertation describes ESCAPE (Expert Systems in Computer Animation Production Environments), a multi-agent animation system for building domain-oriented, rulebased visual programming environments. Much recent work in computer graphics has been concerned with producing behavioural animations of artificial life-forms mainly based on algorithmic approaches. This research indicates how, by adding an inference engine and rules that describe such behaviour, traditional computer animation environments can be enhanced. The comparison between using algorithmic approaches and using a rule-based approach for representing multi-agent worlds is not based upon their respective claims to completeness, but rather on the ease with which end users may express their knowledge and control their animations with a minimum of technical knowledge. An environment for the design of computer animations incorporating an expert system approach is described. In addition to direct manipulation of objects on the screen, the environment allows users to describe behavioural rules based upon both the physical and non-physical attributes of objects. These rules can be interpreted to suggest the transition from stage to stage or to automatically produce a longer animation. The output from the system can be integrated into a commercially available 3D modelling and rendering package. Experience indicates that a hybrid environment, mixing algorithmic and rulebased approaches, would be very promising and offer benefits in application areas such as creating realistic background scenes and modelling human beings or animals either singly or in groups. A prototype evaluation system and three different domains are described and illustrated with preliminary animated images.	3d modeling;algorithm;artificial life;computer animation;computer graphics;direct manipulation interface;expert system;inference engine;logic programming;multi-agent system;prototype;visual programming language	Victor Ye	1996			computing;computer facial animation;human–computer interaction;computer science;computer animation;multimedia;computer graphics (images)	Graphics	-38.776605602203894	-32.04569952115297	88300
0aabee5ced02048b5de6f3cab6a2e13c4987b3d0	poster: rapid development of natural user interaction using kinect sensors and vrpn	virtual reality peripheral network natural user interaction kinect sensors vrpn nui hardware microsoft kinect software tool graphical user interface;skeleton virtual reality software speech recognition servers sensors graphical user interfaces;depth camera;microsoft kinect;virtual reality graphical user interfaces user interface management systems;virtual reality natural user interaction microsoft kinect depth camera;virtual reality;graphical user interfaces;user interface management systems;virtural reality applications center;natural user interaction	The availability of low-cost natural user interaction (NUI) hardware took a significant step forward in 2010 with the introduction of the Microsoft Kinect. Despite significant work on the available software development kits for the Kinect, tasks beyond simple single-Kinect skeleton tracking remain challenging to implement. This paper introduces a software tool that significantly accelerates the prototyping and implementation of NUI in virtual reality, particularly for developers with limited programming skills. This is achieved by creating a graphical user interface to provide a consistent development environment for defining Kinect settings and voice commands. This is coupled with a server to transmit skeleton and voice information using the Virtual Reality Peripheral Network (VRPN). Furthermore, the system is capable of combining data from multiple Kinect sensors into one data stream, abstracting the implementation details so the designer may focus on the environment creation and development.	graphical user interface;kinect;natural user interface;peripheral;programming tool;sensor;server (computing);software development kit;virtual reality	Timothy B. Morgan;Diana Jarrell;Judy M. Vance	2014	2014 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2014.6798871	human–computer interaction;computer science;multimedia;natural user interface;user interface;graphical user interface testing;computer graphics (images)	Mobile	-43.53828353140959	-33.34554455682606	88312
aeeadec9ab47bbe39283f5156dc10322d0b1eb21	3d scene manipulation with 2d devices and constraints	3d manipulation;user interface;interactive 3d environments;computer graphic;3d scene construction;3d environment;constraints	Content creation for computer graphics applications is a laborious process that requires skilled personnel. One fundamental problem is that manipulation of 3D objects with 2D user interfaces is very difficult for nonexperienced users. In this paper, we introduce a new system that uses constraints to restrict object motion in a 3D scene, making interaction much simpler and more intuitive. We compare three different 3D scene manipulation techniques based on a 2D user interface. We show that the presented techniques are significantly more efficient than commonly used solutions. To our knowledge, this is the first evaluation of 3D manipulation techniques with 2D devices and constraints.	3d modeling;3d user interaction;approximation algorithm;autodesk maya;collision detection;computer graphics;data point;input device;interaction technique;object composition;usability testing;user interface	Graham Smith;Wolfgang Stuerzlinger;Tim Salzman	2001			computer vision;simulation;human–computer interaction;computer science;operating system;multimedia;user interface;computer graphics (images)	Graphics	-38.996520635144016	-34.81894417444661	88369
f457354b5a916fe1a3fe6746ab673672f48446d7	digital buddhist image creation by haptic deformation	corea;interfase usuario;image numerique;narrative;teleenseignement;realite virtuelle;realidad virtual;user interface;narration;cultural heritage;asservissement visuel;divertissement;virtual reality;coree;user assistance;patrimoine culturel;assistance utilisateur;asie;sensibilidad tactil;patrimonio cultural;narracion;asistencia usuario;imagen numerica;korea;interface utilisateur;audition;teleensenanza;audicion;digital image;auditory feedback;virtual environment;remote teaching;visual servoing;entertainment;sensibilite tactile;hearing;tactile sensitivity;haptic interaction;servomando visual;asia	In this paper, we describe a haptic deformation algorithm used in a virtual storytelling system known as the Digital Buddhist Image Creation System. This system supports not only haptic interaction, but also visual and auditory feedback and the combination of these three modalities provides a user with an immersive experience of a virtual cultural environment. We believe the proposed bi-directional haptic interaction will encourage users to spontaneously participate and engage with the system. To demonstrate and explore this concept, a virtual environment that tells the legend of Korea's 'Unju Temple' has been constructed. The proposed haptic interaction is expected to play an important role in supporting users as they directly experience and learn about this significant piece of cultural heritage in a digital, virtual environment.		Jong-Phil Kim;Jeung-Chul Park;Beom-Chan Lee;Kwan-Heng Lee;Jeha Ryu	2006		10.1007/11736639_121	stereotaxy;computer vision;simulation;computer science;virtual reality;multimedia;narrative	Vision	-48.18361002655524	-32.79648849024521	88559
2f10e89154f8db838ffa9e31869e5cc27821c8ea	using vrml and java to visualize 3d algorithms in computer graphics education	indonesia;description systeme;system description;representation tridimensionnelle;indonesie;second year;realite virtuelle;realidad virtual;concepcion sistema;computer graphics;reseau ordinateur;virtual reality;classroom;algorithm visualization;computer graphic;computer network;systeme conversationnel;algorithme;algorithm;computer graphics education;3d model;asie;interactive system;system design;sistema conversacional;red ordenador;vrml;three dimensional representation;descripcion sistema;sala clase;salle cours;grafico computadora;infographie;conception systeme;representacion tridimensional;asia;java;algoritmo	In computer graphics education, many concepts are hard to teach using traditional classroom tools. Therefor we are building a system that will be used in our second year computer graphics course to aid the lecturer in explaining the working of a number of fundamental algorithms. This paper describes how VRML and Java are used to create applets with interactive 3D models to visualize these algorithms. Our system structure is described, as well as an elaborate example.	algorithm;computer graphics (computer science);java;vrml	Herbert Baerten;Frank Van Reeth	1998	Computer Networks	10.1016/S0169-7552(98)00205-0	simulation;vrml;computer science;virtual reality;graphics software;programming language;computer graphics;java;computer network;3d computer graphics;systems design;computer graphics (images)	Graphics	-36.04409637935861	-27.56235303446594	88684
918aa1a3d6833c5cb19fd377e3542a1d36cad052	practical computer graphics for scientific users	design process;computer graphic;graphics system;scientific computing;national center for atmospheric research	The scientific computer user presents a unique challenge to the graphics system designer. If the graphics system is properly designed and implemented, computer graphics becomes a vital research tool for the scientific user. These users' desires and expectations play an important role in every phase of the design process.This paper describes the philosophy of the graphics system at the National Center for Atmospheric Research (NCAR) and its implementation. Details of algorithms are not presented; rather the purpose has been to describe a successful computer graphics system which may serve as a guide for other designers who desire to provide practical computer graphics. This graphics system operates under a batch-mode computer system without using interactive terminals.	algorithm;batch processing;computational science;computer graphics;computer terminal;systems design;user (computing)	Thomas J. Wright	1974		10.1145/563182.563187	scientific visualization;design process;human–computer interaction;computer science;graphics software;computer graphics;software rendering;computer graphics (images)	Graphics	-46.92638146575189	-28.325034921271843	88747
635bb909892cfca4707adeedeba3d0b1adb4acab	chip demonstrator: semantics-driven recommendations and museum tour generation	semantic web technology;chip;museum collection	The main objective of the CHIP project is to demonstrate how Semantic Web technologies can be deployed to provide personalized access to digital museum collections. We illustrate our approach with the digital database ARIA of the Rijksmuseum Amsterdam1. For the semantic enrichment of the Rijksmuseum ARIA database we collaborated with the CATCH STITCH project2 to produce mappings to Iconclass3, and with the MultimediaN E-culture project4 to produce the RDF/OWL of the ARIA and Adlib databases. The main focus of CHIP is on exploring the potential of applying adaptation techniques to provide personalized experience for the museum visitors both on the Web site and in the museum. This resulted in three demonstrator components:	recommender system	Lora Aroyo;Natalia Stash;Yiwen Wang;Peter Gorgels;Lloyd Rutledge	2007		10.1007/978-3-540-76298-0_64	chip;human–computer interaction;computer science;multimedia;world wide web	Web+IR	-40.761775249805304	-25.3774169727931	88783
fa9561737f77c1c66b047a3efd83e3f9a05b30fa	analyse de techniques de coopération en environnements virtuels 3d	animacion por computador;distributed system;groupware;systeme reparti;realite virtuelle;realidad virtual;interaction;cooperation;collaboration;virtual reality;cooperacion;synchronisation;sistema repartido;metafora;virtual realit collaborative virtual environments;synchronization;state of the art;sincronizacion;computer animation;metaphor;collecticiel;metaphore;animation par ordinateur	Virtual reality has lead to development of many methods for interacting betwe en real humans and virtual objects in 3D spaces. Nowadays, some new develop ments are made for cooperative interactions which means that several persons can act sim ultaneouly on shared objects. A cooperation requires new metaphors to make several perso ns acting on same objects possible and also to have some new feedbacks to users to explain shared a ctions. Finally, some new technical issues in platforms appear like synchronization and consiste cy b tween sites. MOTS-CLÉS :État de l’art, interaction, coopération, collaboration, réalité virtuelle, envir onnements virtuels collaboratifs.	interaction;virtual reality	Laurent Aguerreche;Thierry Duval;Bruno Arnaldi	2009	Technique et Science Informatiques	10.3166/tsi.28.767-797	synchronization;simulation;human–computer interaction;computer science;virtual reality	Theory	-35.56609028991238	-26.567274804657924	89081
7e8304f6691cb20410c4a573d1be595152b0f1bc	dynamic interactions in physically realistic collaborative virtual environments	index terms artificial;simulation index terms artificial augmented and virtual realities computer supported cooperative work synchronous interaction animation;artificial;groupware;graphical user interfaces augmented reality groupware avatars motion estimation computer animation;rigid body;object interaction;collaboration animation virtual environment collaborative work avatars computational modeling computer simulation application software humans kinematics;computer supported cooperative work;simulation;virtual reality;motion estimation;indexing terms;augmented;graphical user interfaces;computer simulation computer systems cybernetics data display environment humans imaging three dimensional man machine systems models biological online systems touch user computer interface;animation;and virtual realities;avatars;on the fly;inverse kinematics;augmented reality;computer animation;computer animation physically realistic collaborative virtual environment rigid body simulator object movement inverse kinematics realism dynamic avatar interaction augmented virtual reality computer supported cooperative work synchronous interaction;collaborative virtual environment;synchronous interaction;virtual worlds	This work describes our efforts in creating a general object interaction framework for dynamic collaborative virtual environments. Furthermore, we increase the realism of the interactive world by using a rigid body simulator to calculate all actor and object movements. The main idea behind our interactive platform is to construct a virtual world using only objects that contain their own interaction information. As a result, the object interactions are application independent and only a single scheme is required to handle all interactions in the virtual world. In order to have more dynamic interactions, we also created a new and efficient way for human users to dynamically interact within virtual worlds through their avatar. In particular, we show how inverse kinematics can be used to increase the interaction possibilities and realism in collaborative virtual environments. This results in a higher feeling of presence for connected users and allows for easy, on-the-fly creation of new interactions. For the distribution of both the interactive objects and the dynamic avatar interactions, we keep the network load as low as possible. To demonstrate the effectiveness of our techniques, we incorporate them into an existing CVE framework.	animation;avatar (computing);behavior;collaborative virtual environment;common vulnerabilities and exposures;genetic heterogeneity;handling (psychology);haptic technology;interaction design;interaction information;inverse kinematics;movement;muscle rigidity;on the fly;physx;physical phenomenon or property;physical object;qp state machine frameworks;simulation;simulators;virtual reality;virtual world;collision	Pieter Jorissen;Maarten Wijnants;Wim Lamotte	2005	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2005.100	anime;augmented reality;rigid body;simulation;index term;human–computer interaction;computer science;inverse kinematics;computer-supported cooperative work;motion estimation;graphical user interface;virtual reality;computer animation;multimedia	Visualization	-39.128025063250824	-37.26142233146438	89179
634bfb81c9998efaa5b67e82beb3c382aeda1053	a smart assistant for shooting virtual cinematography with motion-tracked cameras	virtual cinematography;cinematography;motion tracked virtual cameras;motion tracking;virtual camera planning	This demonstration shows how an automated assistant encoded with knowledge of cinematography practice can offer suggested viewpoints to a filmmaker operating a hand-held motion-tracked virtual camera device. Our system, called Director's Lens, uses an intelligent cinematography engine to compute, at the request of the filmmaker, a set of suitable camera placements for starting a shot that represent semantically and cinematically distinct choices for visualizing the current narrative. Editing decisions and hand-held camera compositions made by the user in turn influence the system's suggestions for subsequent shots. The result is a novel virtual cinematography workflow that enhances the filmmaker's creative potential by enabling efficient exploration of a wide range of computer-suggested cinematographic possibilities.	applications of artificial intelligence;mobile device;virtual camera system	Christophe Lino;Marc Christie;Roberto Ranon;William H. Bares	2011		10.1145/2072298.2072481	computer vision;multimedia;cinematography;computer graphics (images)	HCI	-41.30211390285646	-36.22738475019756	89538
575ed47c83dc4d31648f36d9ff5ccc9f201784e8	beyond the playlist: seamless playback of structured video clips	ambulant open smil player;open source media player;multimedia presentations;video streaming;structured video audio clips;caching;media rendering computer graphics streaming media prefetching multimedia communication delay servers;media rendering;prefetching;media rendering seamless playback structured video audio clips ambulant open smil player open source media player smil multimedia presentation performance delay service interruption switch delay elimination;media;seamless playback;servers;design and implementation;streaming media;performance delay;multimedia communication;service interruption;seamless playback caching prefetching smil multimedia presentations;smil;spatial relationships;multimedia presentation;rendering computer graphics;smil multimedia presentation;switch delay elimination;open source	In this paper we introduce the design and implementation of seamless playback for video/audio in the Ambulant Player. The Ambulant Open SMIL Player is an open-source media player that supports SMIL 3.0. A typical SMIL multimedia presentation consists of a set of declarative references to video/audio clips, which are relative to each other in terms of temporal and spatial relationships. Unfortunately, the declarative nature of SMIL often imposes performance delays, as individual items are fetched and presented. In this paper, we discuss the design and implementation of a caching and prefetching scheme that avoids service interruption and eliminate switch delay among these clips. A collection of videos is thereby rendered as if they were continuously rendered from one media container on one media source. Experiments are carried out to validate that our techniques can significantly lower the start delay of media rendering and therefore realize the seamless playback of SMIL multimedia presentations.	algorithm;cpu cache;cache (computing);experiment;interactive media;interrupt;open-source software;rendering (computer graphics);response time (technology);seamless3d;synchronized multimedia integration language;usability testing;user experience design;video clip	Bo Gao;Jack Jansen;Pablo César;Dick C. A. Bulterman	2010	IEEE Transactions on Consumer Electronics	10.1109/TCE.2010.5606288	spatial relation;media;synchronized multimedia integration language;computer science;operating system;multimedia;world wide web;server;computer graphics (images)	DB	-42.66437640822696	-34.65686402144834	89610
17f09a7b4407996d4ac920be83809dff354cbe4c	space perception and luminance contrast: investigation and design applications through perceptually based computer simulations	space perception and luminance contrast investigation and design applications through perceptually based computer simulations;articulo;high dynamic range imagery;information gathering;depth perception;lighting simulation;high dynamic range;psychophysical experiments;urban design;computer simulation;space perception;depth cue	Pictorial cues are the visual information gathered from 3D scenes; and they provide depth perception in the physical world. Pictorial cues are also used to create the illusion of depth on planar media. Planar media are a common platform for architects to visually examine the spatial qualities of their designs. Therefore, knowledge of pictorial cues can be used as a design strategy to enrich the spatial experience. In this paper, luminance contrast is proposed as an effective depth cue and design strategy. Lighting based perceptual studies are challenged by the dynamics of the luminous environments in physical experimental settings. Computer simulation allows the study of lighting variability throughout the day and year in a systematic manner. This paper utilizes a computational framework to simulate perceptual reality. Psychophysical experiments are conducted in this alternative environment. 3D scenes and the resulting 2D imagery are utilized to investigate the impact of lighting patterns and luminance contrast on depth perception. The results of the study demonstrate that luminance variations within a space impacts the perceived distance as much as they impact the luminance contrast between the task and the background. Application of this pictorial cue is demonstrated through architectural and urban design examples.	common platform;computer simulation;depth perception;experiment;heart rate variability;image;luminous studio	Nan-Ching Tai;Mehlika Inanici	2010		10.1145/1878537.1878730	computer simulation;computer vision;urban design;depth perception;computer science;multimedia;computer graphics (images)	HCI	-36.56756256636565	-36.83084337702856	89868
94466763ecf14d9cb33375a523c28e651c9e2a32	implementation and evaluation of a background music reactive game	user study;signal analysis;musically controlled games;audioasteroids;video game;briquolo;breakout;musical controller;games;background music reactive games;music signal analysis;mp3;music;open source	"""This paper discusses further work on the authors' """"background music reactive games"""" concept, where background music is used to modify video game parameters and thus actions on the screen. Each song selected by the player makes the game look different and behave variedly. The concept is explored by modifying an open-source game called Briquolo, which is based on the well-known arcade game Breakout. Several audio signal features such as magnitude, energy, centroid, and spectral flux are calculated from the background music MP3 file and mapped to relevant game parameters. In order to verify how well the features work in practice, a user study with 20 participants was arranged. The results suggest strongly that people appreciate the concept of background music reactive games. The selected analysis algorithms and mapping worked nicely, and 90% of participants felt that the music truly affected the game. In addition, 90% of participants also felt that the modified version was more entertaining than the original."""	algorithm;arcade game;breakout box;mp3;online music store;open-source software;spectral flux;usability testing;whole earth 'lectronic link	Khalid Aallouche;Homam Albeiriss;Redouane Zarghoune;Juha Arrasvuori;Antti J. Eronen;Jukka Holm	2007		10.1145/1367956.1367957	video game design;game design;simulation;simultaneous game;computer science;game mechanics;game art design;metagaming;multimedia;sequential game;communication	HCI	-47.708992277521546	-36.032202021974484	90133
8cbdd761999780a27a4d073b822719398d2c7275	an auditory display for exploratory visualization of multidimensional data	multidimensional data;auditory display	This paper describes an auditory display being developed for a workstation designed to facilitate the visualization of multidimensional data. To establish the context of this work, the paper first discusses the psychophysical basis for using sound to represent multidimensional data, surveys the related literature, and describes the novel graphical technique employed by the workstation to display multidimensional data visually. Next, it gives an overview of the auditory display and its relationship to the visual display, emphasizing the roles played by the apparent location of sound sources and the repetition of sounds in time. Finally, the paper discusses the current prototype sound system and some of the difficulties to be faced in implementing the proposed facility. This work is being done as part of the Exploratory Visualization (Exvis) project at the University of Lowell.	auditory display	Stephen Smith	1989		10.1007/978-3-642-75903-1_10	auditory display	Visualization	-47.47668424783102	-33.922881236638105	90191
118e11ee6c00e690864ce6fa5b47f8e6d7590910	a tool for constructing 3d environments with virtual agents	autonomous virtual humans;intelligent virtual environment;physics based modeling;believable agents;user interface;simulation;virtual human;virtual environments;collision detection;3d environment;animation;inverse kinematics;dynamic characteristic;virtual environment;multimedia presentation;educational application;virtual agents;virtual agent;virtual worlds	The use of Virtual Environments as a user interface is essential for certain types of applications, both in education and entertainment. These worlds are even more attractive for the user when they are neither static nor pre-scripted, but have dynamic characteristics and are populated by autonomous entities, also called virtual agents. There has been a lot of research concerning visualization, animation and behavior of virtual agents, but there are no generic architectures, methodologies and tools for the development of intelligent virtual environments, i.e. 3D environments with autonomous virtual agents. In this paper, we present SimHuman, a tool for the construction of virtual worlds with autonomous entities, targeted for a specific group of applications, such as simple simulation systems, virtual environments, educational applications, multimedia presentations, etc. It consists of a programming library and two utilities and it is highly dynamic and configurable, as it is not based on fixed scenes and models. It has embedded characteristics such as Inverse Kinematics, Physically Based Modeling, Collision Detection and Response, and Vision. SimHuman incorporates some important features for designing and building virtual environments and turns out to be an effective tool for interactive 3D applications with virtual agents.	algorithm;autonomous robot;collision detection;computer animation;desktop computer;educational entertainment;embedded system;entity;executable;graphics library;hypermedia;intelligent agent;intensional logic;inverse kinematics;library (computing);opengl;personal computer;population;poser;riva tnt2;simulation;spatial–temporal reasoning;synthetic intelligence;the c++ programming language;the times;user interface;vrml;video card;virtual reality;virtual world	Spyros Vosinakis;Themis Panayiotopoulos	2005	Multimedia Tools and Applications	10.1007/s11042-005-5607-y	anime;simulation;human–computer interaction;computer science;virtual machine;instructional simulation;operating system;inverse kinematics;multimedia;user interface;collision detection	Visualization	-41.87503396259312	-34.07410095250933	90278
1d2d2626b7c4318f0b98074fbee457ac9035d0b0	psycho-physical crowds	crowd simulation;psychology;physics	Human crowds are becoming an increasingly important component of video games and virtual environments. However, as these virtual worlds become more realistic, the simulation of human crowds must become more sophisticated to keep pace. Much recent work in the field of crowd simulation has focused on improving aspects relating to navigation and collision avoidance in order to create complex, collision free trajectories. However, this paper argues that an equally important aspect of creating believable crowds is simulating aspects beyond navigation such as the internal psychological state of a simulated agent and external physical forces. We also discuss how recently proposed methods can be used to simulate these physical and psychological aspects of crowd simulations while maintaining high-quality, collision free motion.	crowd simulation;mental state;virtual reality;virtual world	Stephen J. Guy	2013		10.1145/2522628.2541249	simulation;computer science;artificial intelligence;crowd simulation;multimedia	Robotics	-38.600957644679745	-37.20746146806356	90634
8638ff5788fdfa11da7bc0148eba069360d90736	modeling short-term dynamics and variability for realistic interactive facial animation	image motion analysis;motion control;facial dynamics;graphics and multimedia facial animation motion models facial dynamics interactive animation computer graphics;computer graphics;graphics and multimedia;interactive speed;realistic images computer animation emotion recognition face recognition image motion analysis interactive systems;animation system;emotion recognition;face 1 0 0 1 0 0 1 facial expression 1 0 0 1 0 0 1 humans 1 0 0 1 0 0 1 image processing computer assisted 1 0 0 1 0 0 1 models biological 1 0 0 1 0 0 1 nonlinear dynamics 1 0 0 1 0 0 1 principal component analysis 1 0 0 1 0 0 1 user computer interface 1 0 0 1 0 0 1;motion capture data;computer graphic;short term dynamics;face recognition;interactive animation;synthetic face short term dynamics realistic interactive facial animation animation system realistic expressive facial motion interactive speed motion model facial expression dynamics temporal signature motion capture data long term visual behavior;synthetic face;long term visual behavior;displays;motion model;facial expression dynamics;facial animation displays motion control humans;facial animation;motion models;realistic images;realistic expressive facial motion;humans;temporal signature;realistic interactive facial animation;facial expression;computer animation;interactive systems	A new animation system produces realistic expressive facial motion at interactive speed. It employs motion models that control facial-expression dynamics and retain the expressions' temporal signature learned from motion capture data. A nondeterministic component ensures the variety of the long-term visual behavior. This system can efficiently animate any synthetic face.	animation;face;heart rate variability;interactivity;motion capture;synthetic intelligence	Nicolas Stoiber;Gaspard Breton;Renaud Séguier	2010	IEEE Computer Graphics and Applications	10.1109/MCG.2010.40	facial recognition system;motion control;computer vision;facial motion capture;computer facial animation;computer science;interactive skeleton-driven simulation;computer animation;multimedia;computer graphics;facial expression;computer graphics (images)	Graphics	-39.62824385273233	-36.780702780906	90811
bc5b00fdccf503a84be3fa070a772ef5318132b1	comparison of sound spatialization techniques in mpeg-4 scene description		This paper overviews the recently updated spatial sound features in MPEG-4 international standard. A scene description language present in MPEG-4 Systems can be used for sound composition, and creating immersive audiovisual scenes where the acoustic response is parametrically defined and can be designed to correspond to the visual scene. This interface enables detailed modeling of sound propagation in an acoustic environement, but it can also be used to create dynamic, spatial sound effects for audio-only applications. In this paper we present an overview of the audio part of the scene description, and discuss the different technologies that may be used to reproduce the spatialized sound.		Riitta Väänänen;Jyri Huopaniemi;Ville Pulkki	2000			speech recognition;linguistics;communication	Visualization	-46.608929467884856	-33.67898468014681	90981
4f880223d6a661ebddd5ece9a1f065ca7d059ee3	synthesizing cooperative conversation	interfaz multimodal;animacion por computador;systeme intelligent;architecture systeme;multimodal interface;computer graphics;man machine dialogue;sistema inteligente;automatic generation;intelligent system;dialogo hombre maquina;arquitectura sistema;man machine interface;facial expression;computer animation;system architecture;grafico computadora;infographie;eye gaze;interface multimodale;interface homme machine;dialogue homme machine;animation par ordinateur	We describe an implemented system which automatically generates and animates conversations between multiple human-like agents with appropriate and synchronized speech, intonation, facial expressions, and hand gestures. Conversations are created by a dialogue planner that produces the text as well as the intonation of the utterances. The speaker/listener relationship, the text, and the intonation in turn drive facial expressions, lip motions, eye gaze, head motion, and arm gesture generators. Disciplines Computer Sciences | Engineering | Graphics and Human Computer Interfaces Author(s) Catherine Pelachaud, Justine Cassell, Norman I. Badler, Mark Steedman, Scott Prevost, and Matthew Stone This journal article is available at ScholarlyCommons: http://repository.upenn.edu/hms/193 Synthesizing Cooperative Conversation Catherine Pelachaud Justine Cassell Norman Badler Mark Steedman Scott Prevost Matthew Stone University of Pennsylvania z July 5, 1996 Abstract We describe an implemented system which automatically generates and animates conversations between multiple human-like agents with appropriate and synchronized speech, intonation, facial expressions, and hand gestures. Conversations are created by a dialogue planner that produces the text as well as the intonation of the utterances. The speaker/listener relationship, the text, and the intonation in turn drive facial expressions, lip motions, eye gaze, head motion, and arm gesture generators.We describe an implemented system which automatically generates and animates conversations between multiple human-like agents with appropriate and synchronized speech, intonation, facial expressions, and hand gestures. Conversations are created by a dialogue planner that produces the text as well as the intonation of the utterances. The speaker/listener relationship, the text, and the intonation in turn drive facial expressions, lip motions, eye gaze, head motion, and arm gesture generators.	amnesia: the dark descent;catherine;computer science;computer scientist;graphics;human computer;ibm notes;interaction;mark steedman;norman packard;real-time locating system;refinement (computing)	Catherine Pelachaud;Justine Cassell;Norman I. Badler;Mark Steedman;Scott Prevost;Matthew Stone	1995		10.1007/BFb0052313	human–machine interface;computer vision;speech recognition;eye tracking;computer science;computer animation;computer graphics;facial expression;systems architecture	Graphics	-38.78506691718044	-28.12202686800119	91480
2f0b5e67d17a469ee9dfbdba8ba383d7e5f57bb8	gasp-ii - a geometric algorithm animation system for an electronic classroom		This paper describes GASP-II, an algorithm animation system for geometric computing, which is particularly suitable for a dktributed electronic classroom. The system allows the creation, presentation and interactive exploration of 3D geometric algorithms. GASP-II can also be used as a visual debugger for geometric computing.	algorithm;computational geometry;debugger	Maria Shneerson;Ayellet Tal	1997		10.1145/276884.276930		Graphics	-46.34496501151346	-29.714549695128298	91739
669afba5ea483b4c976ab6e14ac947830db9c832	immps: a multimedia presentation design system	herencia;multimedia databases music multimedia systems weight control artificial intelligence spatial databases educational products mood;interfase usuario;base donnee;sistema experto;architecture systeme;multimedia;learning;user interface;heritage;implementation;database;base dato;business graphics;base connaissance;multimedia systems;intelligent multimedia presentation;authoring systems;interactive multimedia;artificial intelligent;aprendizaje;ejecucion;apprentissage;graphical user interfaces;object oriented;systeme windows;windows 95 immps interactive multimedia presentation development system multimedia presentation design knowledge inheritance relations presentation windows object oriented multimedia database object reuse database browser;authoring;oriente objet;base conocimiento;arquitectura sistema;interface utilisateur;object oriented databases;information system;systeme expert;interactive programming multimedia systems business graphics authoring systems object oriented databases graphical user interfaces;multimedia presentation;system architecture;inheritance;multimedia database;orientado objeto;systeme information;interactive programming;sistema informacion;knowledge base;expert system	Our interactive multimedia presentation development system uses artificial intelligence to specify knowledge inheritance relations between presentation windows. An object-oriented multimedia database organizes resources and presentations, and a database browser facilitates object reuse. The system runs under Windows 95 and can be used for general-purpose presentations or for education, training, or product demonstrations.		Timothy K. Shih;Ruth E. Davis	1997	IEEE MultiMedia	10.1109/93.591175	knowledge base;human–computer interaction;computer science;artificial intelligence;operating system;graphical user interface;database;multimedia;interactive media;object-oriented programming;user interface;implementation;world wide web;expert system;information system	Visualization	-37.41164172390576	-26.057330862981374	91814
5f42aa03af8abc6cdcde74ee8ccda7f06106d41f	3d model dynamic cutting technology based on game engine		Although games is famous for its high interactivity, but in some special area of applications, such as educational games [1], the interactivity of games is still not enough. For example, the 3D objects in game scene cannot be cut off in real. This paper proposes a real-time 3D objects cutting technology in games, which can cut any arbitrary 3D objects at any angle into two parts. Applying with this technology, game players can do a number of more realistic cutting operations. This technology is suitable in areas which need real cutting operations, such as educational games, mechanical simulation, and medical surgery teaching and so on.	3d modeling;algorithm;game demo;game engine;interactive media;interactivity;kinect;real-time clock;real-time locating system;simulation;unity	WenFeng Hu;Shuang Zhao;Yu Ren	2017	2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2017.7960058	interactivity;simulation;algorithm design;theoretical computer science;mathematical game;solid modeling;computer science	Robotics	-40.5321966818629	-34.960217210768164	92246
aa502283fe4fe5c7d225e626596ff7c39027517e	probabilistic models for designing motion and sound relationships	max;mubu;mapping by demonstration;hmm;gmm;nime;motion;sound;machine learning;multimodal;mapping;ismm;mendeley tags gmm;movement;gmr	We present a set of probabilistic models that support the design of movement and sound relationships in interactive sonic systems. We focus on a mapping–by–demonstration approach in which the relationships between motion and sound are defined by a machine learning model that learns from a set of user examples. We describe four probabilistic models with complementary characteristics in terms of multimodality and temporality. We illustrate the practical use of each of the four models with a prototype application for sound control built using our Max implementation.	machine learning;prototype	Jules Françoise;Norbert Schnell;Riccardo Borghesi;Frédéric Bevilacqua	2014			movement;computer vision;speech recognition;giant magnetoresistance;computer science;generalized method of moments;motion;machine learning;multimodal interaction;sound;hidden markov model	ML	-46.146543250218876	-35.57464430135535	92511
e50d2884792bbe3ae5e57e89b1bc77731ff2ffe1	hci metacomputing: universal syntax, structured editing, and deconstructible user interfaces	metadata;abstract syntax tree;structured editing;data metaformat;user interface architecture;usability metadesign	"""There is a classic design tension between user-friendly user interface design and expert-friendly user interface design. There is also a classic design tension between binary data format design and printable data format design. This work attempts to expose both sets of design tensions as having the same cause and solution. We observe an opportunity to redefine the baseline for """"human-readable"""" formats by pairing a universal binary syntax with a universal structured editor and explore the rippling implications that it could have on human-computer interaction and the computing landscape at large. We discuss how this paradigm solves a host of typical bugs and developer pain points as well as making software more flexible, how it can be used to add a self-descriptive capacity to information representations ranging from data formats to user interfaces, and finally, how that creates new outlets for end-users to apply tiers of computational literacy for their own empowerment."""	baseline (configuration management);binary data;human-readable medium;human–computer interaction;metacomputing;programming paradigm;rippling;software bug;structure editor;usability;user interface design	Christopher Hall	2014		10.1145/2660252.2660258	user interface design;user experience design;human–computer interaction;computer science;theoretical computer science;operating system;programming language;user interface;metadata;world wide web;abstract syntax tree	HCI	-41.104069306805954	-29.500597365167266	92521
527295087d30741349027a8430f656aebea0cfc2	an adaptive web content delivery system	sistema experto;architecture systeme;red www;heterogeneous environment;systeme adaptatif;internet;system design;content delivery;adaptive system;sistema adaptativo;world wide web;arquitectura sistema;reseau www;systeme expert;system architecture;expert system	The desktop-centric design of most of the current web contents pose many difficulties for pervasive browsing. In this paper, we present our study on the problem to support pervasive browsing in the heterogeneous environment of today’s Internet. A system solution – Adaptive Web Content Delivery (AWCD), is presented to overcome the problems existed in present web infrastructure. The system designed is extensible for the further development of Internet. The two major subsystems of AWCD, client profile learning and adaptation, are described in detail. Experiment results of our system are also shown.	desktop computer;internet;web content	Jinlin Chen;Yudong Yang;HongJiang Zhang	2000		10.1007/3-540-44595-1_30	web development;web modeling;the internet;simulation;computer science;artificial intelligence;adaptive system;multimedia;world wide web;expert system;systems design	HCI	-38.14254495097019	-25.153152671195627	92623
c259afee1eeaea021f1e9387a92e370634f9af3a	erratum to: grape: a graphical pipeline environment for image analysis in adaptive magnetic resonance imaging			grape flavor;image analysis;magnetic resonance imaging	Refaat E. Gabr;Getaneh B. Tefera;William J. Allen;Amol S. Pednekar;Ponnada A. Narayana	2016	International Journal of Computer Assisted Radiology and Surgery	10.1007/s11548-016-1508-y	computer vision;computer graphics (images)	Vision	-46.81278657558959	-28.89887392518834	92715
e2950c81afe1d8d1929c381753eed6511c997652	experiments in inkjet colour tests for printmaking		The motivation for this research is based on how artists mix and print colour by traditional means (painting and printmaking) and how these differ from colour picker tools, slider bars and methods developed for digital printing, and whether it is possible to incorporate both? Artists have been expert at mixing colour for centuries, yet although the artist and designer has access to a wide range of digital imaging tools and technologies, that on first glance, are dedicated to the creation of colour mixtures, the resulting colours are often disappointing. It appears that hardware, software tools and methods for digital printing are not necessarily suited to the specific requirements of the artist. In fact, they are too generalised to obtain a high degree of quality and too inflexible to allow artists to obtain precision and predictability. Based on existing hardware and software, the paper suggests alternative approaches to custom colour ink mixing and printing. Through the development of alternative ink colours specifically mixed for inkjet printing the paper demonstrates specially designed charts for printing and double printing of custom mixed inks.	chart;color management;color space;computer hardware;digital imaging;digital rights management;lookup table;printing;ramp simulation software for modelling reliability, availability and maintainability;requirement;technical standard;visual effects;while;workspace	Carinna Parraman	2010			visual arts;engineering;digital printing;engineering drawing;computer graphics (images)	HCI	-40.637727057522056	-32.269985755153115	92927
1f8a29f9f69e80763132f3dc13db13983103b0a4	smartcon: a context-aware service discovery and selection mechanism using artificial neural networks	sensibilidad contexto;distributed system;teoria cognitiva;iterative method;red sin hilo;capabilities profile;multiagent system;systeme reparti;context aware;informatique mobile;agent mobile;reseau sans fil;iterative learning;service selection;agente movil;wireless network;m services;service web;anns;ann;cognitive theory;web service;agent logiciel;composite capabilities preference profiles;theorie cognitive;intergiciel publication souscription;cc pp;metodo iterativo;software agents;artificial neural networks;sistema repartido;context aware service;intergicial editor suscriptor;methode iterative;backpropagation algorithm;contexto;preferencia;contexte;algorithme retropropagation;preference;cognitive feedback;preference profile;service discovery;sensibilite contexte;reseau neuronal;mobile agent;sistema multiagente;mobile services;context aware systems;mobile computing;back propagation;publish subscribe middleware;red neuronal;context;servicio web;artificial neural network;systeme multiagent;neural network;composite profiles;algoritmo retropropagacion	In this paper, we present SmartCon, a context-aware system for the discovery and selection of mobile services using Artificial Neural Networks (ANNs). The solution we have developed is a mobile agent-enabled system that adaptively and iteratively learns to select the best available mobile service derived from the extraction of a series of features utilising contextual information such as the Composite Capabilities/Preference Profiles (CC/PP), service-specific and non-uniform user-specific features which are supplied to a Back-Propagation Neural Network. Based on the features provided, the neural network classifies the most relevant mobile service. In the present work, the system is also capable through iterative learning to generalise and gather information using cognitive feedback based on the user's decisions and interactivity with a Mobile Device. SmartCon is evaluated using a series of preliminary empirical data and results show an 87% success rate in the discovery and selection of the best or most relevant mobile service.	artificial neural network;neural network software;service discovery	Eyhab Al-Masri;Qusay H. Mahmoud	2009	IJISTA	10.1504/IJISTA.2009.022693	simulation;telecommunications;computer science;artificial intelligence;backpropagation;machine learning;artificial neural network	HCI	-36.40243235884142	-25.16377535973654	93258
8818aa5b0cf49b557025d2e3018252a2e3761979	an interactive design system for pop-up cards with a physical simulation	mass spring model;interactive system;interactive design system;pop up card;three dimensional structure;user interaction;system simulation;interaction design;physical simulation	We present an interactive system that allows users to design original pop-up cards. A pop-up card is an interesting form of papercraft consisting of folded paper that forms a three-dimensional structure when opened. However, it is very difficult for the average person to design pop-up cards from scratch because it is necessary to understand the mechanism and determine the positions of objects so that pop-up parts do not collide with each other or protrude from the card. In the proposed system, the user interactively sets and edits primitives that are predefined in the system. The system simulates folding and opening of the pop-up card using a mass–spring model that can simply simulate the physical movement of the card. This simulation detects collisions and protrusions and illustrates the movement of the pop-up card. The results of the present study reveal that the user can design a wide range of pop-up cards using the proposed system.	collision detection;dynamical simulation;elasticity (data store);interactive design;interactivity;norm (social)	Satoshi Iizuka;Yuki Endo;Jun Mitani;Yoshihiro Kanamori;Yukio Fukui	2011	The Visual Computer	10.1007/s00371-011-0564-0	simulation;human–computer interaction;card sorting;computer science;interaction design;computer graphics (images)	HCI	-42.11451690258598	-37.75237501795998	93344
3ee9f1c2627e2d146d99909a71126b8e76a5f310	proteus: an approach to interface evaluation	software tool;user needs;user perception;interface design;computing;rapid prototyping;space use;interface evaluation	PROTEUS is a number of software tools which allow the implementation of an iterative, user centred approach to software (particularly interface) design, using rapid prototyping. The tools allow automated collection of questionnaire data, logging of system usage, and the central technique which is the collection of a qualitative representation of users perception of an interface design space, using the Construct Elicitation System. This data is fed back to the designer, and increases understanding user needs in relation to an interface. The paper describes the development of PROTEUS as an integrated evaluation tool, and reports on some of the empirical work underlying the approach embodied by PROTEUS, including its integration into the design of a small but real system. 1: INTRODUCTION	data logger;information needs;iterative method;rapid prototyping	Jonathan Crellin	1990			user interface design;user;computing;simulation;human–computer interaction;computer science;interface design;operating system;natural user interface;user interface	HCI	-42.678299743827885	-29.073001911925765	93515
e22d2bde4483914a91d8f1d1d0a12c9ea67604fd	mobile augmented reality using scalable recognition and tracking	tracking mobile augmented reality visual word recognition remote server mobile phone wi fi;visual word recognition;mobile handsets mobile communication servers target tracking scalability augmented reality visualization;tracking augmented reality;object database;mobile phone;visualization;servers;wireless lan augmented reality mobile computing mobile handsets tracking;mobile communication;mobile handsets;remote server;wireless lan;scalability;augmented reality;target tracking;mobile computing;wi fi;mobile augmented reality;tracking	In this paper, a new mobile Augmented Reality (AR) framework which is scalable to the number of objects being augmented is proposed. The scalability is achieved by a visual word recognition module on the remote server and a mobile phone which detects, tracks, and augments target objects with the received information from the server. The server and the mobile phone are connected through a conventional Wi-Fi. In the experiment, it takes 0.2 seconds for the cold start of an AR service initiation on a 10k object database, which is fairly acceptable in a real-world AR application.	augmented reality;cold start;scalability;server (computing);visual word	Jaewon Ha;Jinki Jung;ByungOk Han;Kyusung Cho;Hyun Seung Yang	2011	2011 IEEE Virtual Reality Conference	10.1109/VR.2011.5759473	embedded system;augmented reality;scalability;visualization;mobile telephony;mobile database;computer science;operating system;tracking;multimedia;internet privacy;mobile computing;server	Visualization	-42.90767109174097	-36.07517673341257	93524
884ce6d12dad78f65cc353f45c8e15abd1003bcc	ivolver: a visual language for constructing visualizations from in-the-wild data		iVoLVER, the Interactive Visual Language for Visualization Extraction and Reconstruction, is a web-based pen-and-touch interface that graphically supports construction of interactive visualizations. iVoLVER is designed to enable data extraction from different types of artifacts (e.g., photos) and to use that data to generate original representations of that data. People can create visualizations from data that is not structured in traditional formats without the need of textual programming or sitting at their desk. This demonstration shows how iVoLVER visualizations are constructed and also demonstrates the possible uses of iVoLVER in several contexts.	touch user interface;visual language;web application	Miguel A. Nacenta;Gonzalo Gabriel Méndez	2017		10.1145/3132272.3132299	interactive visual analysis;visual analytics;multimedia;data extraction;human–computer interaction;visual language;visualization;desk;information visualization;computer science	HCI	-41.14898943327044	-28.403547872923955	93551
7fbfad33dc63d6ca4aeaddf8d2672c957ad89e93	transforming your shadow into colorful visual media: multiprojection of complementary colors	text;media art;colorful pictures;multiprojection techniques;texture animation;live videos in shadows;real time systems	This article proposes a real-time system that transforms shadows on a floor into colorful visual media. This system is based on the effect of complementary colors and multiprojection techniques. By exactly calibrating the system geometrically and photometrically, our system makes it possible to display images such as texture animation, colorful pictures, text, and live videos in shadows without any digitized artifact or latency. Since it can display any image, it is applicable for various purposes such as media art, entertainment, and advertisement.	color;computer animation;image;real-time computing;real-time transcription	Yugo Minomo;Yasuaki Kakehi;Makoto Iida;Takeshi Naemura	2006	Computers in Entertainment	10.1145/1146816.1146832	computer vision;computer science;multimedia;computer graphics (images)	Graphics	-40.88167521763126	-36.58407588967558	93564
bc05829e7a049de472c8839a73c0f98118c7697e	mediating collaborative design for constructing educational virtual reality environments: a case study	construction education;concepcion asistida;interfase usuario;computer aided design;proceso concepcion;multidisciplinaire;concepcion ingenieria;engineering design;design process;realite virtuelle;visualizacion;realidad virtual;systeme aide decision;young children;user interface;computer graphics;conception ingenierie;common ground;virtual reality;sistema ayuda decision;prise decision;multidisciplinary teams;integrated design;concepcion integrada;preparacion serie fabricacion;inquiry learning;visualization;decision support system;visualisation;shared knowledge;conception assistee;multidisciplinary;virtual reality environment;interface utilisateur;multidisciplinar;process planning;ingenierie simultanee;ingenieria simultanea;virtual environment;collaborative design;toma decision;preparation gamme fabrication;grafico computadora;infographie;conception integree;concurrent engineering;processus conception;virtual worlds	This paper presents a case study of a multidisciplinary team designing a virtual environment and instrument for young children's science inquiry learning. The designers had the course of meetings to collaboratively develop a learning unit using CLOVES, a virtual world builder. The study showed the potential of CLOVES as a collaborative medium. The designers actively participated in decision-making at every stage of the design process and shared knowledge among one another, which helped the establishment of common ground.	virtual reality	Yongjoo Cho;Kyoung Shin Park;Thomas G. Moher;Andrew E. Johnson	2004		10.1007/978-3-540-30103-5_4	simulation;visualization;decision support system;human–computer interaction;computer science;engineering;operating system;computer aided design;virtual reality;engineering design process;mechanical engineering	Visualization	-34.124733272513055	-24.799735589790302	93743
c0b9599829c32366b3d5e92d6114a249f02eba08	reducing aircraft noise with computer graphics	air force base;engineering graphics;aerodynamics;cad;virtual reality;noise abatement;computer graphic;flow separation;computational fluid dynamics;data visualisation;flow visualisation;aerospace computing;aerospace computing aircraft helicopters aerodynamics flow separation computational fluid dynamics flow visualisation cad noise pollution noise abatement virtual reality parallel processing data visualisation engineering graphics;noise pollution;parallel computer;noise reduction computer graphics airports military standards military aircraft stress guidelines defense industry government educational institutions;helicopters;parallel processing;aircraft;visualization software aircraft noise reduction computer graphics noise pollution reduction jets helicopters high end parallel computers virtual reality systems	Excessive noise is a part of almost everyone's life, whether it comes from the next-door neighbor running his leaf blower or cars driving by with stereos blasting. But for those who live near airports or air force bases, noise is a constant distraction that can cause stress and other health-related problems. For the military and commercial airlines, more stringent noise emission standards are increasingly becoming an issue, as they must try to meet federal guidelines or face restrictive night-flight rules, increased no-fly zones, and additional airport fees. For these reasons, industries, governments, and educational institutions such as Pennsylvania State University are conducting research to help reduce the noise pollution caused by jets and helicopters. Penn State's aerospace engineering department has been using high-end parallel computers, virtual reality systems, and advanced visualization software to test the causes of aircraft noise and find ways to reduce it.	computer graphics	Anna Turnage	2002	IEEE Computer Graphics and Applications	10.1109/MCG.2002.999783	parallel processing;computer vision;simulation;noise pollution;aerodynamics;computational fluid dynamics;computer science;operating system;cad;virtual reality;noise control;flow separation;data visualization;computer graphics (images);mechanical engineering	Visualization	-34.72263601372471	-30.33620191821928	93746
87e91f2c664d40a5add788c38f5cbc926c6981cc	mind over virtual matter: using virtual environments for neurofeedback training	electrical capacitance tomography;protocols;brain computer interface;eeg signal virtual matter virtual environments neurofeedback training brain computer interface vrml world training protocol parameters interactivity parameters vrml object appearance;virtual reality languages;signal detection;virtual matter;virtual reality;virtual environments;noise measurement;neurofeedback training;qa75 electronic computers computer science;signal processing;interactivity parameters;vrml world;vrml object appearance;user interfaces virtual reality languages electroencephalography;neurofeedback electrical capacitance tomography electroencephalography scalp virtual reality noise measurement signal detection protocols read only memory signal processing;neurofeedback;electroencephalography;virtual environment;training protocol parameters;eeg signal;user interfaces;read only memory;scalp	This paper describes on-going research at Lancaster University to develop a brain-computer interface (BCI) with which to conduct neurofeedback training. We have built a system that translates EEG signals detected from the scalp of a subject into movement and interaction within a VRML world. The training protocol parameters can be set prior to a session commencing. These correspond to signal thresholds within which a subject will be rewarded for maintaining his or her EEG component signal amplitude for a predetermined period. The training environments are constructed from a set of VRML components. Interactivity parameters, in terms of VRML object appearance and behaviour corresponding to changes in the EEG signal, can be chosen to suit the requirements of the session.	neurofeedback	Jennifer Allanson;John A. Mariani	1999		10.1109/VR.1999.756961	brain–computer interface;communications protocol;simulation;electroencephalography;computer science;noise measurement;virtual machine;operating system;signal processing;neurofeedback;virtual reality;multimedia;user interface;read-only memory;detection theory;computer graphics (images)	Visualization	-44.96762861348962	-36.646416206732695	94007
366244c3b8ad0693394c0d3d5b02b4ab17f0de13	three-dimensional metamorphosis: a survey	three dimensional	A metamorphosis or a (3D) morphing is the process of continuously transforming one object into another. 2D and 3D morphing are popular in computer animation, industrial design, and growth simulation. Since there is no intrinsic solution to the morphing problem, user interaction can be a key component of a morphing software. Many morphing techniques have been proposed in recent years for 2D and 3D objects. We present a survey of the various 3D approaches, giving special attention to the user interface. We show how the approaches are intimately related to the object representations. We conclude by sketching some morphing strategies for the future.	3d modeling;capability maturity model;computer animation;correspondence problem;interpolation;morphing;simulation;user interface	Francis Lazarus;Anne Verroust-Blondet	1998	The Visual Computer	10.1007/s003710050149	three-dimensional space;computer science;geometry	Graphics	-47.92919091182661	-29.350102573792302	94201
2217862809d31971b1853f2b4c30f6c3c1175d36	a music video authoring system synchronizing climax of video clips and music via rearrangement of musical bars	art;virtual reality;cave2;storytelling;digital humanities;interactive installation	This paper presents a system that can automatically add a soundtrack to a video clip by replacing and concatenating an existing song's musical bars considering a user's preference. Since a soundtrack makes a video clip attractive, adding a soundtrack to a clip is one of the most important processes in video editing. To make a video clip more attractive, an editor of the clip tends to add a soundtrack considering its timing and climax. For example, editors often add chorus sections to the climax of the clip by replacing and concatenating musical bars in an existing song. However, in the process, editors should take naturalness of rearranged soundtrack into account. Therefore, editors have to decide how to replace musical bars in a song considering its timing, climax, and naturalness of rearranged soundtrack simultaneously. In this case, editors are required to optimize the soundtrack by listening to the rearranged result as well as checking the naturalness and synchronization between the result and the video clip. However, this repetitious work is time-consuming. [Feng et al. 2010] proposed an automatic soundtrack addition method. However, since this method automatically adds soundtrack with data-driven approach, this method cannot consider timing and climax which a user prefers.	chorusos;climax group;climax community;concatenation;video clip	Haruki Sato;Tatsunori Hirai;Tomoyasu Nakano;Masataka Goto;Shigeo Morishima	2015		10.1145/2787626.2792608	digital humanities;computer science;artificial intelligence;operating system;virtual reality;multimedia;computer graphics (images)	AI	-44.04142185741833	-32.44194953824847	94222
b74e0f0a21ec9a367aef077f4e1dcfbfce4268dc	ray, camera, action! a technique for collaborative 3d manipulation	3d interaction;input device;input device collaboration 3d interaction;collaboration	We present a technique to support collaborative 3D manipulation. Our approach is based on two or more users jointly specifying the parameters of each transformation using a point, a ray, and a scalar value. We discuss how this concept can be coupled with a camera system to create a scalable technique that can accommodate both parallel and serial collaboration.	action!;scalability	Wallace Santos Lages	2016	2016 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2016.7460080	simulation;human–computer interaction;computer science;multimedia	Visualization	-42.834109208754334	-37.20047760278079	94467
9919455cca1d92a27995103c5b888cd111fa6d5c	an extensible scripting language for interactive animation in a speech-enabled virtual environment	markup languages;virtual environment;xml;motion capture;natural languages;scripting language;language interpretation;production;computer science;virtual reality;3d animation;animation;java;computer animation;character animation;application software;markup language	Character animations on most virtual environment systems are canned motions created off-line through motion capture techniques. The motions are then encoded and transmitted with a fixed format and played at the client side. In this paper, we have proposed an XML-based scripting language, called eXtensible Animation Markup Language (XAML), to describe interactive dialog-based animations. The language is designed to describe character animations at various command levels and to compose a new animation from existing animation clips. In addition, the language is extended to incorporate other dialog-based scripting language such as VoiceXML. We have implemented such a system in Java that can interpret the language and render 3D animations based on the user's interactive voice commands	canned response;client-side;extensible application markup language;java;motion capture;null character;online and offline;scripting language;virtual reality;voicexml;xml;dialog	Tsai-Yen Li;Mao-Yung Liao;Chun-Feng Liao	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)		computer facial animation;computer science;virtual reality;computer animation;multimedia;programming language;computer graphics (images)	Visualization	-42.34998466322318	-33.19363803679916	94472
85c1a79e8b21743834841ce1ab3435ed392e51db	facial expression recognition and modeling for virtual intelligent tutoring systems	reconnaissance visage;vision ordenador;facial expression recognition;systeme tutoriel intelligent;realite virtuelle;intelligent tutoring system;realidad virtual;relacion hombre maquina;virtual reality;man machine relation;computer vision;face recognition;face animation;intelligent tutoring systems;pattern recognition;vision ordinateur;relation homme machine;reconnaissance forme;facial expression;reconocimiento patron;article	This paper describes ongoing work for developing a new interface for intelligent tutoring systems based on recognition and synthesis of facial expressions. This interface senses the emotional state of the user, or his/her degree of attention, and communicates more naturally through face animation.		Homero Ríos;Ana Luisa Solís;Emilio Aguirre;Lourdes Guerrero;Joaquín Peña;Alberto Santamaría	2000		10.1007/10720076_11	computer vision;computer science;artificial intelligence;virtual reality;multimedia;facial expression	Robotics	-35.48464654354168	-27.10319537514764	94634
4946f9aaf3d284640a0ecc11672ceb489a97223f	3d design reference framework	concepcion asistida;interfase usuario;computer aided design;modele geometrique;user interface;three dimensional system;conception assistee;systeme 3 dimensions;interface utilisateur;sistema 3 dimensiones;geometrical model;modelo geometrico	Abstract   The paper presents a new user-interface method for creating and manipulating 3D geometries with geometric modellers. The method is particularly useful during the initial stages of conceptual design, where designers have rapidly to generate concepts of components and assemblies with no immediate considerations of scale and accuracy. Application examples are included demonstrating how the method is used in conjunction with a solid-modelling system.		E. Sittas	1991	Computer-Aided Design	10.1016/0010-4485(91)90031-Q	simulation;computer science;engineering;computer aided design;user interface;engineering drawing;mechanical engineering	EDA	-37.560520347722395	-31.185821892943753	94754
dcd4ed99c2ed30468ab7eac130a763691927fa2c	an efficient methodology for constructing and managing a 3-d world	spatial indexing;object oriented rendering;real time;virtual reality;spatial index;object oriented programming;tree data structures;satisfiability;r tree;3d world;object oriented;region trees;region tree;moving objects;lens;three dimensional world;near real time;tree data structures virtual reality rendering computer graphics object oriented programming real time systems;rendering computer graphics;three dimensional world 3d world virtual reality camera real time lens moving objects object oriented rendering region trees spatial indexing r tree;object oriented rendering approach;and spatial indexing;camera;real time systems	As the demand for virtual reality systems increases, a new approach is needed to satisfy the following requirements: 1) A world consists of a large number of objects; 2) A camera is allowed to move and a world should be displayed in near real-time depending on the position of the camera as well as the angle of its lens; and 3) Minimum effort should be needed when objects are moving. We satisfy these requirements with the following: 1) An object-oriented rendering approach is developed and 2) An approach based on region trees and spatial indexing (specifically R+-tree) is developed to manage operations to objects (i.e., retrieval, insertion, and deletion) effectively.	r+ tree;real-time clock;real-time computing;requirement;virtual reality	Taehyung Wang;Phillip C.-Y. Sheu	1999		10.1109/WORDS.1999.806571	computer vision;simulation;computer science;computer graphics (images)	DB	-34.66932991336035	-36.34263436060502	94765
05841b3f69fb9598c1fbee413a59ed05f97f9d4b	virtualisation d'interfaces matérielles par l'intermédiaire d'un ordinateur porté	virtual interfaces;wearable computer	After reminding the complementarity of wearable and pervasive computing, this paper describes the principles of hardware interfaces virtualization through wearable computers. We present the benefits of this approach and our current prototype's architecture. Our preliminary results show that, even though virtualizing hardware interfaces does not currently improve user performance, it can enhance user satisfaction.	complementarity (physics);electrical connector;prototype;ubiquitous computing;wearable computer	Alexandre Plouznikoff;Nicolas Plouznikoff;Jean-Marc Robert	2005		10.1145/1148550.1148598	embedded system;simulation;human–computer interaction;computer science	HCI	-46.761115265601745	-37.853899710537384	95213
c9e84b0b157141a95f5f326559bc835d9547489c	real-time generation of populated virtual cities	real time visualization;real time;data management;crowd simulation;virtual human;polygon partitioning and real time visualization;data model;virtual life simulation;city modeling;terrain modeling	This paper presents a new approach for real-time generation of 3D virtual cities. The main goal is to provide of a generic framework which support semi-automatic creation, manage-ment, and visualization of urban complex environments for virtual human simulation, called virtual urban life (VUL). It intends to minimize efforts of designers in the modeling of complex and huge environments. A versatile multi-level data model has been developed to support data management and visualization in an efficient way. Moreover, a polygon partitioning algorithm addresses the city allotment problem in an automatic way, according to input parameters and constraints. In addition, we discuss some results of virtual populated city simulations developed with proposed frame-work.	algorithm;data model;population;real-time transcription;semiconductor industry;simulation;virtual actor;virtual world	Luiz Gonzaga da Silveira;Soraia Raupp Musse	2006		10.1145/1180495.1180527	simulation;human–computer interaction;data model;data management;computer science;crowd simulation;computer graphics (images)	Visualization	-37.33222321434581	-32.944024142232934	95339
9bcb45937a05d769fb8f6c931c4adb23fe0b45bd	physical visualization of geospatial datasets	fabrication;earth;geospatial analysis;computational modeling;three dimensional displays;solid modeling;data visualization	Geospatial datasets are too complex to easily visualize and understand on a computer screen. Combining digital fabrication with a discrete global grid system (DGGS) can produce physical models of the Earth for visualizing multiresolution geospatial datasets. This proposed approach includes a mechanism for attaching a set of 3D printed segments to produce a scalable model of the Earth. The authors have produced two models that support the attachment of different datasets both in 2D and 3D format.	attachments;computer monitor;digital modeling and fabrication;grid (spatial index);printing;scalability	Hessam Djavaherpour;Ali Mahdavi-Amiri;Faramarz F. Samavati	2017	IEEE Computer Graphics and Applications	10.1109/MCG.2017.38	computer science;bioinformatics;data science;geospatial analysis;data mining;earth;solid modeling;fabrication;computational model;data visualization	Visualization	-34.164075940427395	-31.22880029081363	95371
73ce53fdbd96cc62a31027624df9a37f05f36417	sketchnode: intelligent sketching support and formal diagramming	usability testing;graph drawing;computational intelligence;interaction models;interaction model;functional requirement;sketching	The primary motivation for building SketchNode is to provide an environment for exploring how people use tools to create, arrange, edit and interpret graph diagrams. It has two equivalent interfaces: sketching and diagramming, so that the functional requirements and advantages and disadvantages of the differences can be studied. In this paper we describe two iterations of SketchNode, in particular the computational intelligence required to maintain a sketch that appears hand-drawn and the complexity of providing two interfaces that are equivalent in terms of interaction and visualization. The development and usability tests presented here contribute to the understanding of what intelligent sketch diagramming tools can support and the interaction paradigm of dual visualization tools.	computational intelligence;diagram;functional requirement;iteration;list of concept- and mind-mapping software;programming paradigm;usability	Beryl Plimmer;Helen C. Purchase;Hong Yul Yang	2010		10.1145/1952222.1952249	human–computer interaction;computer science;theoretical computer science;computational intelligence;graph drawing;functional requirement	HCI	-39.90432934120294	-29.85406255215102	95400
5bdfe25bba7f7c093cb669f953cb5551c5f6aa48	photosim: tightly integrating image analysis into a photo browsing ui	photo browsing;user feedback;content analysis;photo collection;clustering;zoomable user interface;image analysis;zoomable ui;personal photo collection;image retrieval;image similarity	Current photo browsers for personal photo collections mostly use the folder structure or camera-recorded time stamps as the only ordering principle. Some also allow manually provided meta-data (tags) for organizing and retrieving photos, but currently, only professional tools allow a pre-grouping of photos by image similarity. We believe that similarity is indeed a useful criterion both for image retrieval and casual browsing, and that the tight integration of content analysis techniques in media UIs in general can lead to more powerful UIs. In this paper we present a prototype, in which we have tightly integrated image analysis techniques and user feedback into a zoomable user interface for browsing and sorting digital photos. We have discussed our system with domain experts and received encouragement as well as valuable ideas for future research.	browsing;image analysis;image retrieval;organizing (structure);prototype;sorting;zooming user interface	Ya-Xi Chen;Andreas Butz	2008		10.1007/978-3-540-85412-8_21	computer vision;image analysis;content analysis;image retrieval;computer science;operating system;multimedia;cluster analysis;zoom;world wide web;information retrieval	HCI	-43.01723453123853	-26.94088884656521	95609
1e5e59e3fe3a5bd58d6f0cee53cec3692d11532b	integrating multi-touch in high-resolution display environments	tiled display;software;high resolution;design process;human computer interaction;touch sensitive screens;customized software high resolution display environments control paradigm multitouch gesture interactive hardware tiled display environment texas advanced computing center economical display system commodity hardware;best practice;large scale datasets human computer interaction multi touch gesture recognition tiled display;weak interaction;large scale;visualization;large scale datasets;touch sensitive screens interactive systems;multi touch;monitoring;software monitoring hardware visualization cameras linux;linux;interactive systems;gesture recognition;cameras;hardware	High-resolution display environments consisting of many individual displays arrayed to form a single visible surface are commonly used to present large scale data. Using these displays often involves a control paradigm where interactions become cumbersome and non-intuitive. By combining highresolution displays with multi-touch and gesture interactive hardware, researchers can explore data more naturally, efficiently and collaboratively. This fusion of technology is necessary to effectively use tiled-display environments and mediate their primary weakness - interaction. In order to realize these objectives, a team at the Texas Advanced Computing Center (TACC) developed an economical display system using a combination of commodity hardware and customized software. In this paper we explain the requirements, design process, functions and best practices for constructing such displays. In addition, we explain how these systems can be used effectively with application examples.	best practice;commodity computing;image resolution;interaction;multi-touch;programming paradigm;requirement	Brandt M. Westing;Benjamin Urick;Maria Esteva;Fernando Rojas;Weijia Xu	2011	2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC)	10.1145/2063348.2063359	visualization;design process;image resolution;human–computer interaction;computer hardware;computer science;operating system;weak interaction;gesture recognition;linux kernel;best practice;computer graphics (images)	HPC	-43.480325414000035	-36.594289751927484	95685
fec19ce6d6e2f48539671f180ee67eb48c0c6acb	mobile real-time collaboration for semantic multimedia - a case study with mobile augmented reality systems	multimedia metadata;mobile multimedia;xmpp;real time collaboration;augmented reality;mpeg 7	Advanced mobile applications that enable new ways of interaction with digital objects become increasingly important for on-site professional communities. These new ways of interaction, e.g. in Mobile Augmented Reality (MAR) via position and 3D movement, are real needs for fieldwork domains such as cultural heritage management and the construction industry. In addition, on-site professional communities generate shared knowledge bases with multimedia content and semantic annotations through collaboration. However, current MAR applications lack real-time collaboration features. In practice, blending multimedia semantics in mobile real-time collaboration is challenging due to the limitations of mobile devices, the lack of mature dedicated designed communication infrastructures and the constraints of the remote environments. This paper presents a mobile real-time collaboration system for semantic multimedia annotations with augmented reality features. We use XMPP as a real-time protocol for the secure, scalable and interoperable processing of XML-based semantic multimedia metadata described in MPEG-7. Our prototype was evaluated in the digital documentation of historical sites for cultural heritage management. The evaluation results indicate potential for increased productivity and enhanced mutual awareness in on-site professional communities.	augmented reality;real-time transcription	Dejan Kovachev;Petru Nicolaescu;Ralf Klamma	2014	MONET	10.1007/s11036-013-0453-z	augmented reality;mobile search;human–computer interaction;computer science;multimedia;world wide web	HCI	-47.78781502268802	-23.95081337651208	95824
43fa517da8473c304e4e10386c5e0db8c91f704a	a high-quality high-fidelity visualization of the september 11 attack on the world trade center	smoothed particle hydrodynamics liquid representation high quality high fidelity visualization ad 2001 09 11 world trade center finite element analysis aircraft commercial animation system animation system 3d scene beam elements;pedestrian safety;graphics utilities three dimensional graphics and realism applications;poison control;ad 2001 09 11;injury prevention;terrorism data visualization computational modeling animation poles and towers physics finite element methods analytical models aircraft manufacture buildings;smoothed particle hydrodynamics liquid representation;commercial animation system;beam elements;animation system 3d scene;animation system;safety literature;algorithms computer graphics imaging three dimensional numerical analysis computer assisted september 11 terrorist attacks user computer interface;traffic safety;injury control;graphics utilities;home safety;data visualisation;injury research;safety abstracts;high quality high fidelity visualization;human factors;occupational safety;smooth particle hydrodynamics;safety;world trade center;safety research;accident prevention;violence prevention;finite element analysis;bicycle safety;three dimensional graphics and realism;finite element analysis fea;computer animation;poisoning prevention;finite element analysis computer animation data visualisation;falls;ergonomics;suicide prevention;applications;aircraft	In this application paper, we describe the efforts of a multidisciplinary team toward producing a visualization of the September 11 attack on the North Tower of New York's World Trade Center. The visualization was designed to meet two requirements. First, the visualization had to depict the impact with high fidelity by closely following the laws of physics. Second, the visualization had to be eloquent to a nonexpert user. This was achieved by first designing and computing a finite-element analysis (FEA) simulation of the impact between the aircraft and the top 20 stories of the building and then by visualizing the FEA results with a state-of-the-art commercial animation system. The visualization was enabled by an automatic translator that converts the simulation data into an animation system 3D scene. We built upon a previously developed translator. The translator was substantially extended to enable and control visualization of fire and of disintegrating elements to better scale with the number of nodes and the number of states to handle beam elements with complex profiles and to handle smoothed particle hydrodynamics liquid representation. The resulting translator is a powerful automatic and scalable tool for high-quality visualization of FEA results.	animation;computation (action);device translator device component;f9 embryonic antigen gene;federal enterprise architecture;finite element method;flat ductal epithelial atypia of the breast;imagery;machine translation;requirement;scalability;simulation;smoothed-particle hydrodynamics;smoothing (statistical technique)	Paul Rosen;Voicu Popescu;Christoph M. Hoffmann;Ayhan Irfanoglu	2008	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2008.41	computer vision;smoothed-particle hydrodynamics;simulation;computer science;suicide prevention;human factors and ergonomics;injury prevention;finite element method;data mining;computer animation;computer graphics (images)	Visualization	-34.7582688181576	-30.851667149232785	95987
ce23f76ca61f702f64d065a86835809ce1f826fe	paint desirable subjects with interactive video feedback	painting;fractals;paints;chaos video feedback painting digital paint system interactive art fractals;chaos;video feedback;light emitting diodes;digital paint system;media;painting fractals educational institutions cameras media paints light emitting diodes;camera interactive video feedback interactive fractal paintings paint systems evolving fractal patterns artworks video feedback paint system bare bones vf system vf dynamics 2d composition;interactive art;video signal processing art cameras;cameras	In this paper we describe the creation and the presentation of interactive fractal paintings with paint systems based on VF(Video Feedback). VF occurs when a camera points to its own display. VF is able to generate evolving fractal patterns. But it is difficult to create artworks under specified subjects based on VF. To solve this difficulty, we implemented video feedback paint system by minimally modifying the bare-bones VF system. The system enables the user to draw shapes directly on a display engaged in VF. These shapes geometrically confine the VF dynamics and compose a designed 2D composition which can include specific subject matter. Artworks created using the system can generate evolving patterns with the viewer manipulating anything possible between the camera and the display yet always conform to designed 2D compositions.	artworks;fractal;subject matter expert turing test	Ruimin Lyu;Haotian Wu;Zhongliang Yang	2013	2013 International Conference on Computer-Aided Design and Computer Graphics	10.1109/CADGraphics.2013.82	computer vision;video feedback;media;painting;computer science;computer graphics (images)	EDA	-39.868152744982325	-34.650686750416924	96066
95d0e83f8c6c6574d76db83b83923b5f8e1dfdc5	magic decorator: automatic material suggestion for indoor digital scenes	conference_paper;material suggestion;data driven content creation;computer aided aesthetic design;indoor scene	Assigning textures and materials within 3D scenes is a tedious and labor-intensive task. In this paper, we present Magic Decorator, a system that automatically generates material suggestions for 3D indoor scenes. To achieve this goal, we introduce local material rules, which describe typical material patterns for a small group of objects or parts, and global aesthetic rules, which account for the harmony among the entire set of colors in a specific scene. Both rules are obtained from collections of indoor scene images. We cast the problem of material suggestion as a combinatorial optimization considering both local material and global aesthetic rules. We have tested our system on various complex indoor scenes. A user study indicates that our system can automatically and efficiently produce a series of visually plausible material suggestions which are comparable to those produced by artists.	3d computer graphics;color;combinatorial optimization;decorator pattern;mathematical optimization;usability testing	Kang Chen;Kun Xu;Yizhou Yu;Tian-Yi Wang;Shi-Min Hu	2015	ACM Trans. Graph.	10.1145/2816795.2818096	computer vision;scene statistics;multimedia;computer graphics (images)	Graphics	-37.46346342846347	-34.261904660417684	96219
17383681b5f832adc7c37888972e1c8c9fd30484	personal navigation in semantic wikis		In this paper, we propose a personal navigation approach to Semantic Wikis. In semantic wikis, wikis pages are annotated with semantic data to facilitate research and navigation. The navigation is collaborative designed and shared by every user. However, individuals involved in a collaborative knowledge building activity need to customize the navigation according to her personal needs. In order to overcome this, we extend semantic wikis with personal annotations facilities to support personal navigation. This approach differs from other adaptive navigation approaches, because of the personalization is carried out by the user herself. We have implemented and validated these ideas on the top of a P2P semantic wiki.	peer-to-peer;personalization;wiki	Diego Torres;Alicia Díaz;Hala Skaf-Molli;Pascal Molli	2009			semantic computing;personal wiki;computer science;knowledge management;multimedia;world wide web	Web+IR	-41.75600813380549	-24.91320848713265	96291
bb3719c57cf8cc15cdc546b31d4a717ce914bb82	a survey of level of detail support in current virtual reality solutions	virtual reality;power method;graphics system;level of detail	The technique of Level of Detail (LOD) offers a powerful method of reducing the computational burden of a virtual reality (VR) system. As a result, it represents a valuable and important facility which one would expect to find in all serious VR graphics systems. This survey aims to present many of the rendering engines commonly employed in state-of-the-art VR solutions and details the degree of support which these systems provide for LOD. The investigation reveals a significant lack of support for this facility over the range of packages reviewed. Consequently, a call is made for improved LOD support in future VR products.	erewhon;graphics;level of detail;virtual reality;web browser engine	Martin Reddy	1995	Virtual Reality	10.1007/BF02009725	simulation;power iteration;computer science;artificial intelligence;level of detail;virtual reality;multimedia;computer graphics (images)	Visualization	-40.84269605209416	-32.654225450580064	96296
e1ea02441b8a57c5c271351e450c1b7299b436a5	a collaborative scene editor for vrml worlds	collaboration;multi user environment;vrml;cscw;java	In this paper, we analyze the requirements for a Web-based collaborative infrastructure within a virtual world. Additionally, we combine several tools and methodologies to propose a flexible and fluid collaborative environment using Java language to create a VRML scene graph. The proposed prototype aims at four aspects: a shared workspace of scene editor, an active entity composition algorithm in Java, collaborative control in the multi-user environment, and access control mechanism toward the shared data.	access control;algorithm;collision detection;computer-supported cooperative work;interaction;java;multi-user;prototype;requirement;scene graph;user interface;vrml;virtual reality;virtual world;workspace	Tainchi Lu;Chuanwen Chiang;Ming-Tang Lin;Chungnan Lee	1998	Comput. Graph. Forum	10.1111/1467-8659.00253	vrml;human–computer interaction;computer science;computer-supported cooperative work;multimedia;programming language;java;world wide web;collaboration	HCI	-47.68020997872154	-31.787911553917013	96455
fcda1e784373854a2dc84974b3dcb12ab6b639c2	a music notation construction engine for optical music recognition	graph traversal;music notation construction;optical music recognition;definite clause grammars	Optical music recognition (OMR) systems are used to convert music scanned from paper into a format suitable for playing or editing on a computer. These systems generally have two phases: recognizing the graphical symbols (such as note-heads and lines) and determining the musical meaning and relationships of the symbols (such as the pitch and rhythm of the notes). In this paper we explore the second phase and give a two-step approach that admits an economical representation of the parsing rules for the system. The approach is flexible and allows the system to be extended to new notations with little effort—the current system can parse common music notation, Sacred Harp notation and plainsong. It is based on a string grammar and a customizable graph that specifies relationships between musical objects. We observe that this graph can be related to printing as well as recognizing music notation, bringing the opportunity for cross-fertilization between the two areas of research. Copyright c © 2003 John Wiley & Sons, Ltd.	graphical user interface;john d. wiley;optical mark recognition;parsing;pitch (music);printing;string grammar	David Bainbridge;Timothy C. Bell	2003	Softw., Pract. Exper.	10.1002/spe.502	natural language processing;forsyth–edwards notation;mathematical notation;speech recognition;engineering notation;computer science;graph traversal;notation;pop music automation;programming language	AI	-38.832541081688056	-30.421641461384688	96651
6c62c459a3d820a8dc1560391b13a4eb5425363f	fusion of pose and head tracking data for immersive mixed-reality application development	three dimensional displays sensors solid modeling tracking pose estimation visualization skeleton;virtual reality data visualisation helmet mounted displays image motion analysis pose estimation;sensors;skeleton;visualization;immersive visualization function pose head tracking data fusion immersive mixed reality application development full body control 3d first person perception motion sensor head mounted display virtual scene;three dimensional displays;solid modeling;body pose tracking;tracking;hand tracking virtual reality unity 3d head mounted de vice;pose estimation	This work addresses the creation of a development framework where application developers can create, in a natural way, immersive physical activities where users experience a 3D first-person perception of full body control. The proposed frame-work is based on commercial motion sensors and a Head-Mounted Display (HMD), and a uses Unity 3D as a unifying environment where user pose, virtual scene and immersive visualization functions are coordinated. Our proposal is exemplified by the development of a toy application showing its practical use.	head-mounted display;mixed reality;motion capture;pose (computer vision);sensor;unity	Katarzyna Czesak;Raúl Mohedano;Pablo Carballeira;Julián Cabrera;Narciso N. García	2016	2016 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)	10.1109/3DTV.2016.7548886	computer vision;simulation;engineering;articulated body pose estimation;computer graphics (images)	Visualization	-40.40294765351282	-37.00900430181216	96714
da3f6f155e0a8ad427b391b13da261d3e2122a0c	image-based interactive exploration of real-world environments	personal computer;keywords graphical user interfaces;interactive video;journal article;multimedia systems;three dimensional displays rendering computer graphics solid modelling realistic images data visualisation interactive systems;computer graphic;image based interactive exploration;data visualisation;layout computer graphics application software rendering computer graphics photography image reconstruction microcomputers multimedia communication broadcasting image resolution;three dimensional displays;image reconstruction;three dimensional reconstruction;visually complex environments interactive scene computer graphics application area photorealistic 3d architectural model real world images image based rendering system 3d scene modeling interactive video based tour broadcast video resolution;realistic images;three dimensional scene modelling;image based rendering;rendering computer graphics;interactive computer graphics;interactive systems;3d reconstruction;three dimensional computer graphics;solid modelling	Interactive scene walkthroughs have long been an important computer graphics application area. More recently, researchers have developed techniques for constructing photorealistic 3D architectural models from real-world images. We present an image-based rendering system that brings us a step closer to a compelling sense of being there. Whereas many previous systems have used still photography and 3D scene modeling, we avoid explicit 3D reconstruction because it tends to be brittle. Our system is not the first to propose interactive video-based tours. We believe, however, that our system is the first to deliver fully interactive, photorealistic image-based tours on a personal computer at or above broadcast video resolutions and frame rates. Moreover, to our knowledge, no other tour provides the same rich set of interactions or visually complex environments.	3d reconstruction;computer graphics;graphics software;interaction;personal computer;rendering (computer graphics);scene graph	Matthew Uyttendaele;Antonio Criminisi;Sing Bing Kang;Simon A. J. Winder;Richard Szeliski;Richard I. Hartley	2004	IEEE Computer Graphics and Applications	10.1109/MCG.2004.1297011	3d reconstruction;iterative reconstruction;computer vision;image-based modeling and rendering;computer science;multimedia;data visualization;computer graphics (images)	Graphics	-38.91248104769603	-34.84500190748385	96801
3ede2d847a0336164fc4d6574724c9afc9848db3	scene blocking utilizing forces	natural language processing;hamlet;blocking;forces;theatre	Positioning characters in virtual environments currently requires manual work and human intervention to complete. Many applications focus primarily on producing nonverbal behaviors and interacting one-on-one with humans. The problem is that most applications, especially games, are very interactive experiences. They introduce a human factor where a character (the human) may choose not to follow a predefined script, yet an author needs to be able to accommodate the unexpected movements when blocking their AI characters. Here, we look to our prior work on positioning characters in these types of virtual environments to create an AI Director to pre-block a play-script. In addition, we incorporate a force-directed graph component to assist with positioning the AI characters when there is a human-controlled character involved. Forcedirected graphs have been shown to position objects aesthetically for large and complex graphs. We rely upon this feature to assist with adjusting pre-defined play-script blocking to include the human-controlled character, making the human appear to be moving correctly even when they are not. Finally, we evaluate this approach based on occlusion and clustering analysis to show its effectiveness in balancing a production and incorporating a human-controlled character.	blocking (computing);cluster analysis;directed graph;force-directed graph drawing;global positioning system;hidden surface determination;human factors and ergonomics;humans;interaction;virtual reality	Christine Talbot;Gregory Michael Youngblood	2014			machine learning;natural language processing;artificial intelligence;computer science;nonverbal communication;cluster analysis;graph	AI	-40.73340988221953	-35.87255536325546	97031
5e738b7fce3ad438ebf9ff6246fe5483f07c6951	incorporating functionalities of expert medical critique dialogues in the design of a graphical interface	explanation;graphical interface;medical expert systems;wizard of oz;graphical user interfaces;natural language;diagnostic expert systems;expressivity expert medical critiquing dialogues graphical interface design medical diagnostic critiquing dialogues negotiation argumentation explanation wizard of oz dialogues human computer collaboration dialogue functionalities natural language;interactive systems medical expert systems diagnostic expert systems explanation graphical user interfaces;interactive systems;collaboration medical diagnosis natural languages knowledge based systems speech analysis medical diagnostic imaging expert systems production systems pediatrics pathology	Analysis of expert medical diagnostic critiquing dialogues shows that explanation, argumentation and negotiation are strongly interlinked. After presenting analyses of a corpus of Wizard of Oz dialogues in this domain, we describe a design for a graphical interface that enables human-computer collaboration for the same task. Many of the dialogues' functionalities can be transferred to the interface, whilst avoiding natural language interpretation problems and providing a comparable degree of expressivity for the user.	graphical user interface	Michael J. Baker;Laurent Charnay;Michelle Joab;Benoît Lemaire;Brigitte Safar;Daniel Schlienger	1996		10.1109/TAI.1996.560442	natural language processing;human–computer interaction;computer science;knowledge management;graphical user interface	HCI	-41.01901619758454	-27.599173451168667	97068
abbc79227f8a70dd11225d720885c4436608d6ac	grasping a virtual object with a bare hand	virtual fitting;real time garment simulation;machine learning;physically based simulation	近年,拡張現実感 (Augmented Reality:AR) に関する 研究が盛んに行われている.拡張現実感とは,実空間と仮 想空間を重ね合わせることで,実空間に情報を付加する技 術である.ユーザと仮想物体との自然なインタラクション を実現させるためには,正確にユーザの位置情報を取得し, 仮想空間を重ね合わせることが重要である.従来研究では, マーカやグローブを用いることで,ユーザの位置情報を取 得している.しかし,マーカやグローブの装着は,ユーザ に対して装着による違和感を与えると考えられる.そのた め,これらを装着せずにユーザの位置情報を取得すること で,装着による違和感を軽減することが必要とされる.そ こで先行研究では,マーカやグローブの装着なしにユーザ の位置情報を取得し,素手による仮想物体とのインタラク ションを可能とする AR システムを開発した.そして, 仮想物体の形状は変形しないという制限はあるが,仮想物 体に隠面処理や陰影を付加することで,視覚的な違和感を 軽減した. 特殊な機器を手に装着することなく力覚フィードバック を感じさせるためには,擬似触覚を用いる方法が挙げられ る.擬似触覚とは,視覚と触覚の間に矛盾が生じた場合に, 視覚情報に釣られて擬似的な触覚を錯覚する現象のことで ある.関連研究では,手に重畳表示された仮想物体と手と の接触を観察させ,それに関する評価を得た.その結果,		Sota Suzuki;Haruto Suzuki;Mie Sato	2014		10.1145/2614217.2630574	computer vision;simulation;computer science;virtual finite-state machine;computer graphics (images)	Visualization	-43.68334280406409	-35.58675326438416	97255
1ea6fff1f6a15bf221cb4a9868260da818869f30	novice-friendly authoring of plan-based interactive storyboards	storyboards;abl;reactive planning;interactive narrative;comics	Story Canvas is a visual authoring tool for the cre ation of interactive, generative stories. Aimed at authors w ithout a technical background in computational storytelling, our system takes an existing author goal-based narrativ e planning architecture and adds a highly visual auth oring and reading interface to the technology, using the lang u ge of storyboards and comics as a framework for both auth oring and interacting with the resulting narratives. In t his paper we describe Story Canvas and its evolution from our previous authoring work, including how our interfac e choices have been driven by our previous experience s with non-technical authors, and describe the details of translating the visual authoring constructs into story plans wi thin the story generator.	interaction;interactive storytelling;interactivity;nsicom cre-me;storyboard;user story	James Skorupski;Michael Mateas	2010			computer science;artificial intelligence;comics;multimedia;reactive planning;world wide web;abl;computer graphics (images)	HCI	-39.74043347477695	-32.774671690659105	97921
2bec0cb914d2c63cba781005d209d5ebe5d6a402	computer graphics for all	traitement texte;computer graphics;computer graphic;tratamiento textos;grafico computadora;infographie;word processing	Interactive computer graphics would rival word-processing and presentation programs for everyday communications.	computer graphics	Takeo Igarashi	2010	Commun. ACM	10.1145/1785414.1785436	human–computer interaction;computer science;graphics software;computer graphics	Graphics	-47.328027313708525	-31.49797189230807	97993
4824ba308624d117642880216e9a7c8f4cf39f7e	a platform to extract knowledge from graphic documents. application to an architectural sketch understanding scenario	document structure;graph theory;concepcion asistida;model based reasoning;interfase usuario;interfaz grafica;computer aided design;raisonnement base sur modele;analisis escena;teoria grafo;analyse scene;analisis datos;estructura documental;graphical interface;user interface;structure document;abstraction;document graphique;abstraccion;theorie graphe;scenario;data analysis;documento grafico;argumento;script;conception assistee;analyse donnee;interface utilisateur;interface graphique;graphic document;scene analysis;symbol recognition	This paper proposes a general architecture to extract knowledge from graphic documents. The architecture consists of three major components. First, a set of modules able to extract descriptors that, combined with domain-dependent knowledge and recognition strategies, allow to interpret a given graphical document. Second, a representation model based on a graph structure that allows to hierarchically represent the information of the document at different abstraction levels. Finally, the third component implements a calligraphic interface that allows the feedback between the user and the system. The second part of the paper describes an application scenario of the above platform. The scenario is a system for the interpretation of sketches of architectural plans. This is a tool to convert sketches to a CAD representation or to edit a given plan by a sketchy interface. The application scenario combines different symbol recognition algorithms stated in terms of document descriptors to extract building elements such as doors, windows, walls and furniture.	sketch	Gemma Sánchez;Ernest Valveny;Josep Lladós;Joan Mas Romeu;Narcís Lozano	2004		10.1007/978-3-540-28640-0_37	computer vision;simulation;computer science;artificial intelligence;scenario;graph theory;document structure description;operating system;model-based reasoning;graphical user interface;database;abstraction;data analysis;user interface;algorithm	AI	-34.01269390074167	-28.2106034191676	98026
57f6c0795dfd2775b58d23f032af792f46ccfe0c	taking advantage of contextualized interactions while users watch tv	remote control;digital tv;capture and access multimedia applications;digital interactive tv;proof of concept;interactive tv;interactive application;adaptive applications;middleware;user interaction;user media interaction	While watching TV, viewers use the remote control to turn the TV set on and off, change channel and volume, to adjust the image and audio settings, etc. Worldwide, research institutes collect information about audience measurement, which can also be used to provide personalization and recommendation services, among others. The interactive digital TV offers viewers the opportunity to interact with interactive applications associated with the broadcast program. Interactive TV infrastructure supports the capture of the user–TV interaction at fine-grained levels. In this paper we propose the capture of all the user interaction with a TV remote control—including short term and instant interactions: we argue that the corresponding captured information can be used to create content pervasively and automatically, and that this content can be used by a wide variety of services, such as audience measurement, personalization and recommendation services. The capture of fine grained data about instant and interval-based interactions also allows the underlying infrastructure to offer services at the same scale, such as annotation services and adaptative applications. We present the main modules of an infrastructure for TV-based services, along with a detailed example of a document used to record the user–remote control interaction. Our approach is evaluated by means of a proof-of-concept prototype which uses the Brazilian Digital TV System, the Ginga-NCL middleware.	interaction;middleware;nested context language;personalization;prototype;remote control;television set	César A. C. Teixeira;Erick Lazaro Melo;Renan G. Cattelan;Maria da Graça Campos Pimentel	2010	Multimedia Tools and Applications	10.1007/s11042-010-0481-7	telecommunications;computer science;operating system;middleware;multimedia;internet privacy;interactive television;proof of concept;world wide web;remote control	HCI	-40.24312521317237	-24.278943833265373	98117
3914e97441315d173b62d6acf8efde133554371a	user experiences with a virtual swimming interface exhibit	fondo marino;animacion por computador;fond marin;interfase usuario;realite virtuelle;realidad virtual;foire exposition;cubico;user interface;divertissement;virtual reality;onda oceanica;locomotion;jambe;exhibition;pierna;realite augmentee;realidad aumentada;cubique;senal video;signal video;user experience;onde oceanique;video signal;interface utilisateur;augmented reality;computer animation;parque exposicion;locomocion;ocean wave;ocean floors;entertainment;cubics;leg;animation par ordinateur;head mounted display	We created an exhibit based on a new locomotion interface for swimming in a virtual reality ocean environment as part of our Swimming Across the Pacific art project. In our exhibit we suspend the swimmer using a hand gliding and leg harness with pulleys and ropes in an 8ft-cubic swimming apparatus. The virtual reality ocean world has sky, sea waves, splashes, ocean floor and an avatar representing the swimmer who wears a tracked headmounted display so he can watch himself swim. The audience sees the swimmer hanging in the apparatus overlaid on a video projection of his ocean swimming avatar. The avatar mimics the real swimmer’s movements sensed by eight magnetic position trackers attached to the swimmer. Over 500 people tried swimming and thousands watched during two exhibitions. We report our observations of swimmers and audiences engaged in and enjoying the experience leading us to identify design strategies for interactive exhibitions.	answer to reset;avatar (computing);cubic function;electrical engineering;google art project;jones calculus;test harness;video projector;virtual reality;windows hardware certification kit	Sidney S. Fels;Steve Yohanan;Sachiyo Takahashi;Yuichiro Kinoshita;Kenji Funahashi;Yasufumi Takama;Tzu-Pei Grace Chen	2005		10.1007/11558651_42	wind wave;computer vision;augmented reality;entertainment;simulation;computer science;optical head-mounted display;exhibition;virtual reality;computer animation;user interface;computer graphics (images)	HCI	-47.121733279123845	-31.552054721771057	98200
0688b111c1bf6f54542f6d898c18dfbb30d052e2	massive: a distributed virtual reality system incorporating spatial trading	distributed virtual reality;streams;distributed system;naming services;spatial context;groupware;distributed processing;virtual reality;massive;aura manager;virtual reality context graphics communication system control streaming media context aware services;collision detection;attributes;naming services virtual reality remote procedure calls distributed processing groupware;remote procedure calls distributed virtual reality system massive user interaction communications architecture typed connections attributes streams spatial interface trading service aura manager;distributed virtual reality system;typed connections;spatial interface trading service;communications architecture;user interaction;remote procedure calls;spatial model	"""MASSIVE is a distributed virtual reality system. It provides rich facilities to support user interaction and cooperation via text, audio and graphics media, and interaction is controlled by a spatial model of interaction. The communications architecture is based on processes communicating via typed connections which have interfaces on both ends and which integrate RPCs, attributes and streams in a common context. A spatial interface trading service, the aura manager, has been developed to support interface trading in a spatial context. The concepts embodied in the aura manager can be useful in other interface trading situations, especially where notions of """" space """" and """" meeting """" may be applied. 1: Introduction Virtual reality as an application benefits from distribution in two main areas: providing increased performance by exploiting parallelism; and overcoming geographical constraints to allow cooperation between many (possibly dispersed) users. Examples of the first include MRToolkit [9] and WAVES [12] which address the high computational demands of simulating and rendering complicated virtual environments by cooperatively executing on a number of processors. Systems which concentrate on the second area include DIVE [7] and MASSIVE, which is the subject of this paper. MASSIVE is the """" Model, Architecture and System for Spatial Interaction in Virtual Environments """" , an experimental distributed virtual reality system intended to support collaborative activity. Virtual environments are a natural choice for supporting cooperative work because they can bring together a number of users in a single virtual place. This allows users to make use of their natural spatial skills, which they use and develop in their everyday interactions with the physical world, in the virtual world to manage interaction with the environment and with other users. For example, spatial factors such as position, orientation and gaze direction seem to be very important in turn-taking and conversation management. Peripheral awareness of other peoples' activity has also been shown to be important in some working contexts (e.g. [11]). The particular emphasis of our work (of which MASSIVE is a part) is on large-scale multiuser virtual environments , i.e. environments which might eventually support hundreds or thousands of simultaneous users. This has implications for the kind of facilities provided by the system and also for the architecture on which the system is based. The facilities provided by MASSIVE are based on the spatial model of interaction which is outlined in section 2 and discussed more fully in [3] and …"""	central processing unit;computation;graphics;interaction;massive (software);multi-user;parallel computing;peripheral;streams;simulation;spatial file manager;spatial navigation;virtual reality;virtual world	Chris Greenhalgh;Steve Benford	1995		10.1109/ICDCS.1995.499999	simulation;computer science;spatial contextual awareness;operating system;database;distributed computing;virtual reality;multimedia;streams;remote procedure call;world wide web;computer security;collision detection	Visualization	-44.207940670203214	-36.31587177504764	98489
d82e47320d9f99a1671062c9899b3c701979dddb	how planning becomes improvisation? - a constraint based approach for director agents in improvisational systems	lenguaje natural;multiagent system;architecture systeme;langage naturel;tratamiento lenguaje;behavioral animation;controle information;artificial intelligent;planificacion;control informacion;language processing;natural language;traitement langage;multi agent architecture;arquitectura sistema;planning;planification;system architecture;information control;sistema multiagente;everyday life;animated character;systeme multiagent	The aim of this paper is to explain how planning becomes improvisation for agents represented through animated characters that can interact with the user. Hayes-Roth and Doyle [10] proposed some changes in the view of intellectual skills traditionally studied as components of artificial intelligence. One of these changes is that planning becomes improvisation. They pointed out that like people in everyday life, animated characters rarely will have enough information, time, motivation, or control to plan and execute extended courses of behavior. Animated characters must improvise, engaging in flexible give-and-take interactions in the here-and-now. In this paper we present an approach to that change. We propose that planning can be understood as improvisation under external constraints. In order to show how this approach can be used, we present a multi-agent architecture for improvisational theater, focusing on the improvisational director's processes.		Márcia Cristina Moraes;Antônio Carlos da Rocha Costa	2002		10.1007/3-540-36127-8_10	planning;simulation;computer science;artificial intelligence;natural language;systems architecture	AI	-35.39891048232172	-26.681571904850635	98686
ce94350ff2553098cb28078a31c30231acb8b36f	a system for interactive acquisition and administration of geometric data for thematic map production	thematic maps;data processing;interactive system;information system	Most computer assisted information systems for planning purposes are designed to produce thematic maps as output. Graphic data processing, on the other hand, has not yet reached the degree of perfection already achieved in other fields of EDP, e.g. commercial and statistical applications. The author and his collegues are integrating cartographic data and presentation techniques gradually into an information system.Within the system being described, the geometric data base is considered as a line network. The edges of the network are represented by strings of orthogonal coordinates (Segment definitions). A unique identifier is assigned to each coordinate string, and the areal units are defined by a sequence of edge identifiers (Polygon definitions).In order to obtain error-free files, strict rules have to be observed and a high degree of accuracy has to be achieved.For this purpose, an interactive system - DIGNET - has been developed to fullfill these objectives: (1) guide the operator through the digitizing session, (2) ask for the appropriate input, (3) create the reference tables, (4) react to erroneous input and operation, (5) detect digitizer malfunctions, (6) allow immediate graphic replay of the digitized data.In order to facilitate the manipulation of the boundary network files, a number of utility programs have been added to the DIGNET-System; among them (1) DIGEDI - to prepare maps for digitzing, (2) DIGLIST - to list all results from digitizing sessions, (3) DIGMERGE - to merge two or more submaps into one final map.	cartography;computer graphics;database;electronic data processing;identifier;information system;interactivity;table (information);thematic map;unique key	Klaus Tuerke	1976		10.1145/563274.563304	computer vision;data processing;computer science;theoretical computer science;operating system;data mining;thematic map;information system;computer graphics (images)	DB	-35.81701156043629	-29.445514195908434	98831
0fe8ea7f5514e6893be8dc77f04deba54ee85941	drawing on air: input techniques for controlled 3d line illustration	trial and error algorithm;drawing on air;control systems;haptic interfaces computer graphics;optimisation;tape drawing;art;haptic aided input technique;image segmentation;performance evaluation;computer graphics;length preserved free boundary;user study;computational geometry;surface fitting;controlled 3d line illustration;user feedback;sweeping movement;two handed tape drawing;electrical equipment industry;indexing terms;haptics;control problem;visualization;feedback;artistic interface;3d model;industrial training;controlled 3d curves;haptic friction effect;one handed drag drawing;haptic interfaces large scale systems control systems performance evaluation friction feedback visualization art electrical equipment industry industrial training;haptic aided redrawing;quasidevelopable mesh segmentation;stretch free surface flattening;optimization;user feedback drawing on air controlled 3d line illustration haptic aided input technique controlled 3d curves sweeping movement one handed drag drawing two handed tape drawing haptic aided redrawing line weight adjustment freehand drawing haptic friction effect;haptic interfaces;air algorithms computer graphics computer peripherals image enhancement image interpretation computer assisted imaging three dimensional numerical analysis computer assisted paintings signal processing computer assisted touch user computer interface;friction;mesh generation;closed path theorem;modeling;freehand drawing;bimanual interaction artistic interface tape drawing haptics modeling;bimanual interaction;solid modelling;line weight adjustment;large scale systems	We present drawing on air, a haptic-aided input technique for drawing controlled 3D curves through space. Drawing on air addresses a control problem with current 3D modeling approaches based on sweeping movement of the hands through the air. Although artists praise the immediacy and intuitiveness of these systems, a lack of control makes it nearly impossible to create 3D forms beyond quick design sketches or gesture drawings. Drawing on air introduces two new strategies for more controlled 3D drawing: one-handed drag drawing and two-handed tape drawing. Both approaches have advantages for drawing certain types of curves. We describe a tangent preserving method for transitioning between the two techniques while drawing. Haptic-aided redrawing and line weight adjustment while drawing are also supported in both approaches. In a quantitative user study evaluation by illustrators, the one and two-handed techniques performed at roughly the same level and both significantly outperformed freehand drawing and freehand drawing augmented with a haptic friction effect. We present the design and results of this experiment, as well as user feedback from artists and 3D models created in a style of line illustration for challenging artistic and scientific subjects.	3d modeling;3dmark;addresses (publication format);adobe freehand;class;drawings (art);friction;gesture recognition;haptic device component;haptic technology;mcgurk effect;sweeping;usability testing;benefit	Daniel F. Keefe;Robert C. Zeleznik;David H. Laidlaw	2007	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2007.1060	mesh generation;computer vision;simulation;systems modeling;visualization;index term;computational geometry;computer science;friction;feedback;geometry;image segmentation;haptic technology;computer graphics;computer graphics (images)	HCI	-40.18679337927003	-37.610581441449824	98868
5ffa362034f4d5c71acff8fd3d0821a51ca58f36	authoring constraint-based tutoring systems	tutoring system	This tutorial covers both the theory and practice of Constraint-Based Modeling (CBM). The first part of the tutorial introduces constraints and CBM as a theoretical foundation for Intelligent Tutoring Systems (ITSs). The second part covers ASPIRE, our new authoring system for developing constraint-based ITSs, and gives participants the opportunity to experience developing a small ITS in ASPIRE. We will first introduce constraints as a way of representing domain knowledge. Currently, cognitive models typically cast declarative knowledge as consisting of propositions – knowledge units that encode assertions (which can be true or false) that support description, deduction and prediction. We have developed an alternative model of declarative knowledge that consists of constraints – units of knowledge that are more prescriptive than descriptive, and that primarily support evaluation and judgment. We present a formal representation of constraints and explain its conceptual rationale, and then introduce two applications of CBM. The first is the use of constraints as a basis for a machine learning algorithm that allows a heuristic search system to detect and correct its own errors. From this point of view, constraint-based learning is a form of adaptive search. This algorithm was originally developed as a hypothesis about how people learn from errors. We present the algorithm in some detail and briefly summarize applications to various problems in the psychology of cognitive skill acquisition. Next we develop in detail the application of CBM to the design and implementation of ITSs. The constraint-based knowledge representation provides a novel way to represent the target subject matter knowledge, which has the advantage of directly supporting one of the main functions of expert knowledge in an ITS: to detect student errors. More importantly, the constraint-based representation provides a theoretically sound and practical solution to the intractable problem of student modeling. Finally, constraints and the associated learning algorithm provide detailed implications for how to formulate individual tutoring messages. We present multiple systems that follow this blueprint, together with empirical evaluation data. In the second part of the tutorial we present our new authoring system (ASPIRE). The goal of ASPIRE is to make ITS authoring available to educators who have little technical knowledge of ITSs or programming in general. ASPIRE does this by providing extensive authoring support, such that the author, as far as possible, is always working at the domain knowledge level, not the programming level. We describe its architecture and functionality, as well as the …	algorithm;blueprint;cognitive model;computational complexity theory;design rationale;encode;heuristic;knowledge level;knowledge representation and reasoning;machine learning;natural deduction;subject matter expert turing test	Antonija Mitrovic;Stellan Ohlsson;Brent Martin;Pramuditha Suraweera	2007			computer science	AI	-37.31879841975523	-28.899823357082962	98990
995e59eee18c3a9a75a32934c9597cb144154ad0	the distributed musical rehearsal environment	groupware;rehearsal organization distributed musical rehearsal environment asynchronous transfer mode based environment immersive teleconference environment technical specifications installations studio setup;teleconferencing;groupware music distributed processing teleconferencing virtual reality;distributed processing;virtual reality;serveur institutionnel;archive institutionnelle;open access;archive ouverte unige;cybertheses;music;institutional repository;asynchronous transfer mode;conductors music rhythm space technology microphones instruments delay estimation information technology measurement delay effects	asynchronous transfer mode-based environment for distributed musical rehearsals in an immersive teleconference environment. This article describes the technical specifications of the installations and the organization and studio setup of these rehearsals. We present our implementation of the environment and give the results obtained from the organized distributed musical rehearsal trials. T he past few years have seen the rapid evolution and wide availability of high-quality communication networks and powerful, inexpensive computers. Consequently, new communication applications and ideas have appeared in different disciplines, ranging from networked medical applications to distributed music performances. A common denominator of these applications, and a communication system in high demand, is tele-conferencing. Currently, telephone operators offer videoconference services, and Internet users can communicate with Internet protocol (IP)-telephony and Webcams. However, the video and audio quality of these applications is low, and demanding users want more than small, low-resolution video images and telephone-quality audio. They need immersive systems like Tele-port 1 and Immersive High Quality Communication (IHQC) 2 that can accurately reproduce images and sounds, allowing observation of gestures and voice changes as if talking person-to-person. Musicians and composers were among the first artists interested in the new communication technologies and the artistic possibilities they offer. Various experiments and applications have addressed distributing a musical creation or a performance environment. Applications ranged from Musical Instrument Digital Interface (MIDI) synchronization over the Internet, like Distributed Music 3 and NetMusic, 4 to tools for interactive control of virtual instruments in virtual environments , like the Hypercello, 5 up to real-time, high-quality, interactive music networks over asynchronous transfer mode (ATM), like NetMuse. 6 Also, researchers in industry and acad-emia conducted a large number of music performance experiments using telephone and ISDN lines at various conferences and expositions, like the Lemma One performance. 7 To study problems and issues related to tele-conference applications in the framework of the Distributed Video Production (DVP) project of the European Union ACTS (Advanced Communication Technologies and Services) research program , 8 we designed and developed an immersive ATM based teleconference environment. A pilot application, called Distributed Musical Rehearsal (DMR), 9,10 tests and evaluates the DVP teleconfer-ence environment. The Distributed Musical Rehearsal aims to let small groups of geographically separated actors and musicians conduct rehearsals as if face-to-face in the same rehearsal room. We implemented a two-site setup with one installed at the The distributed musical rehearsal system used video walls, digital sound and video encoding, …	atm turbo;computer;data compression;dual modular redundancy;dynamic music;experiment;integrated services digital network;internet;midi;performance;real-time cmix;telecommunications network;television;video production;virtual reality;webcam	Dimitri Konstantas;Yann Orlarey;Olivier Carbonel;Simon Gibbs	1999	IEEE MultiMedia	10.1109/93.790611	simulation;human–computer interaction;telecommunications;computer science;asynchronous transfer mode;music;virtual reality;multimedia;world wide web	Networks	-48.2390622709729	-26.397296914745425	99077
bf989ee14e72e7c057914ae57f02a4e05adcdd95	post-processing npr effects for video games	video games;art;npr;art and science;perception	This paper describes different interactive, non photorealistic techniques which can be easily applied to video games. Based on a study of art and games, an emphasis is put on considering NPR tools as basic elements to provide different styles and moods, to convey different emotional and experiential representations of a game. We restrict ourselves to screen based effects, which permits any existing game to use our framework with practically no integration cost. This allows us not only to comply with user preferences in rendering style, but also the creation of multiple gaming experiences out of the same game. We show the resulting effects in an in-house videogame and in standard Unity demos, and show how the users can change the style of the videogame by means of a menu.	rendering (computer graphics);samegame;unity;user (computing);video post-processing	Milán Magdics;Catherine Sauvaget;Rubén Jesús García;Mateu Sbert	2013		10.1145/2534329.2534348	video game graphics;simulation;computer science;artificial intelligence;game mechanics;game art design;game developer;multimedia;perception;computer graphics (images)	HCI	-40.269004937390854	-34.99276865586267	99198
1f6173eba31ef8a6ac0e9c5566710b8ccf208fbd	development of an intercultural collaboration system with semantic information share function	representacion conocimientos;multimedia;semantics;intelligence artificielle;collaborative system;semantica;semantique;dominio trabajo;semantic information;internet;domaine travail;representation connaissance;artificial intelligence;workspace;inteligencia artificial;information system;knowledge representation;multilinguisme;video communication;shared workspace;systeme information;multilingualism;multilinguismo;sistema informacion	The Internet provides the opportunity to implement the real seamless communications over different geographic domains. However, when they cope with the network level challenges, they encounter the context and cultural level challenges. Therefore, we have developed Intercultural Collaboration System with Semantic Information Share Function. This system is a multilingual conferencing system. This system consists of three functions Spark2 for shared workspace, AnnoChat for chat communication tool and TalkGear2 for audio and video communication tool. The system has a semantic information share function and a translation function.		Kunikazu Fujii;Takashi Yoshino;Tomohiro Shigenobu;Jun Munemori	2005		10.1007/11552413_61	the internet;computer science;artificial intelligence;database;semantics;world wide web;information system;workspace	AI	-33.69767808077158	-27.033157710982348	99378
2ee043a839e2b33064231671c3d6e856d5d39a21	real virtuality: a multi-user immersive platform connecting real and virtual worlds	interaction;virtual reality;passive haptic;motion capture	Real Virtuality is a multi-user immersive platform combining motion capture with virtual reality (VR) headsets: users can freely move within the physical space while virtually visiting a virtual world and interacting with 3D objects or other users using the sense of touch.	headset (audio);interaction;motion capture;multi-user;virtual reality;virtual world;virtuality (gaming)	Sylvain Chagué;Caecilia Charbonnier	2016		10.1145/2927929.2927945	cave automatic virtual environment;simulation;engineering;mixed reality;multimedia;augmented virtuality;immersion;computer graphics (images)	Visualization	-43.93831699537543	-37.52994111353426	99419
a1d785a6242708fef40fd19a6061e6ba6eefe041	an immersive multi-agent system for interactive applications	path planning;crowd simulation;journal article;multi agent;interaction design	This paper presents an interactive multi-agent system based on a fully immersive virtual environment. A user can interact with the virtual characters in real time via an avatar by changing their moving behavior. Moreover, the user is allowed to select any character as the avatar to be controlled. A path planning algorithm is proposed to address the problem of dynamic navigation of individual and groups of characters in the multi-agent system. A natural interface is designed for the interaction between the user and the virtual characters, as well as the virtual environment, based on gesture recognition. To evaluate the efficiency of the dynamic navigation method, performance results are provided. The presented system has the potential to be used in the training and evaluation of emergency evacuation and other real-time applications of crowd simulation with interaction.	agent-based model;algorithm;automated planning and scheduling;autonomous agent;autonomous robot;avatar (computing);context (computing);crowd simulation;gesture recognition;immersion (virtual reality);kinect;mathematical model;motion planning;multi-agent system;multi-user;real-time transcription;speech recognition;user interface;virtual reality	Yanbin Wang;Rohit Dubey;Nadia Magnenat-Thalmann;Daniel Thalmann	2012	The Visual Computer	10.1007/s00371-012-0735-7	simulation;human–computer interaction;computer science;artificial intelligence;interaction design;crowd simulation;motion planning;multimedia;computer graphics (images)	Robotics	-38.120948040269	-37.99429946128355	99912
9dbe95a24c159b44337692481725a775fedbd38f	vision-based user interfaces for health applications: a survey	operating room;besoin de l utilisateur;interfase usuario;vision ordenador;algorithmique;image processing;image databank;user interface;surveillance;elderly;salud publica;exigence usager;vision based user interface;diagnostico;exigencia usuario;procesamiento imagen;personne âgee;hombre;necesidad usuario;zona dato;traitement image;anciano;computer vision;planificacion;image guided therapy;vigilancia;monitoring;algorithmics;user need;algoritmica;user requirement;banco imagen;chirurgie;banque image;data visualization;human;surgery;utilisabilite;user requirements;sante publique;interface utilisateur;planning;cirugia;vision ordinateur;monitorage;planification;usabilidad;data field;monitoreo;usability;diagnosis;zone donnee;algorithm design;public health;homme;diagnostic;health care	This paper proposes a survey of vision-based human computer interfaces for several key-fields in health care: data visualization for image-guided diagnosis, image-guided therapy planning and surgery, the operating room, assistance to motor-impaired patients, and monitoring and support of elderly. The emphasis is on the contribution of the underlying computer vision techniques to the usability and usefullness of interfaces for each specific domain. It is also shown that end-user requirements have a significant impact on the algorithmic design of the computer vision techniques embedded in the interfaces.	computer vision;data visualization;embedded system;human computer;requirement;usability;user requirements document	Alexandra Branzan Albu	2006		10.1007/11919476_77	computer vision;simulation;public health;human–computer interaction;image processing;computer science;user requirements document;algorithmics;data visualization	HCI	-34.78948582521548	-28.632926874131904	100043
2825b7dcae358168e2c6b4575e5aa1b08b7d4b01	efficient geometry-based sound reverberation	acoustic waves;reverberation;acoustic environment;beam tracing;geometric analysis;geometry-based sound reverberation;low-cost pc platforms;real-time sound;virtal reality applications;wave digital network	In this paper we propose a novel approach to sound reverberation based on the the geometric analysis of the acoustic environment, which allows the listener to freely move within it. The method is based on a combination of a tapped delay line for early reverberation, designed through beam tracing; and a wave digital network for late reverberation, designed through path tracing. The method is efficient enough to enable a real-time sound rendering for Virtal Reality applications, on low-cost PC platforms.	acoustic cryptanalysis;algorithm;algorithmic efficiency;augmented reality;beam tracing;covox speech thing;deployment environment;digital electronics;geometric analysis;path tracing;real-time clock;surround sound;virtual reality	Augusto Sarti;Stefano Tubaro	2002	2002 11th European Signal Processing Conference		electronic engineering;speech recognition;acoustics;engineering	Visualization	-45.55508753079213	-34.36398604796394	100053
7b3b11a569b5d05221a5a9a2fe7b29cb2e0b077d	enhancing enterprise virtual worlds with real-world event information	surveillance;client server systems;computer graphic;computer vision;rendering computer graphics avatars business data processing client server systems computer animation computer vision;visual surveillance;servers;streaming media;business data processing;avatars;video capture enterprise virtual world real world event information real life human activities visual surveillance system virtual world server virtual environment avatar animation rendering;humans;near real time;virtual environment;computer animation;rendering computer graphics;avatars servers rendering computer graphics humans streaming media cameras surveillance;human activity;cameras;virtual worlds	In this paper, we present a novel system for enhancing enterprise virtual worlds by supplying information about real-world events and activities at near real-time. With more enterprises realizing the commercial benefits of virtual worlds, the requirement for spontaneous replication of the real-world has become essential. In the proposed system, we detect and classify different real-life human activities using a visual surveillance system and then send corresponding relevant information as inputs to a virtual world server. The virtual world server uses this information to reproduce these activities in the virtual environment. We have used the office model of our lab and recreated human activities and movements with near real spontaneity.	emergence;real life;real-time computing;real-time transcription;server (computing);simulation;spontaneous order;user experience;virtual reality;virtual world	Surjeet Mishra;Atul Agarwal;Vinay Khemka;Geetika Sharma	2011	2011 Proceedings of 20th International Conference on Computer Communications and Networks (ICCCN)	10.1109/ICCCN.2011.6005771	simulation;computer science;virtual machine;operating system;computer animation;multimedia;server;computer graphics (images)	Visualization	-40.5326421492519	-36.58146290741885	100069
fd72ff6625400e43e407478ac593bda830efbbd2	specification and generation of variable, personalized graphical interfaces	metodologia;tool;graphical interface;specification;herramienta;methodologie;graphical system;interface;methodology;outil;systeme graphique	"""A top-down system design study, concerned with the complex issues that arise when humans interact with graphics systems, is presented. A number of desirable features are profiled and the mechanisms or """"agents"""" through which those features are incorporated into the interface are summarized. A formal, extensible method for specifying an interface, prior to implementation, is then demonstrated. The BNF-like specification is constructed from the language-based agents previously described and is used as a blueprint for the design of a software tool which allows an experimenter to generate variable, personalized graphical interfaces. Finally, a description of that tool, its capabilities, limitations, and some implementation issues are discussed."""	beta normal form;blueprint;graphical user interface;graphics;personalization;programming tool;systems design;top-down and bottom-up design	Richard Bournique;Siegfried Treu	1985	International Journal of Man-Machine Studies	10.1016/S0020-7373(85)80013-4	simulation;human–computer interaction;computer science;interface;methodology;graphical user interface;programming language;graphical user interface testing;specification	HCI	-40.62371033287188	-29.146761345607974	100071
d81f492849caa7c81452e5e7efedaeefd846c64c	pykaldi: a python wrapper for kaldi		We present PyKaldi, a free and open-source Python wrapper for the widely-used Kaldi speech recognition toolkit. PyKaldi is more than a collection of Python bindings into Kaldi libraries. It is an extensible scripting layer that allows users to work with Kaldi and OpenFst types interactively in Python. It tightly integrates Kaldi vector and matrix types with NumPy arrays. We believe Py Kaldi will significantly improve the user experience and simplify the integration of Kaldi into Python workflows. PyKaldi comes with extensive documentation and tests. It is released under the Apache License v2.0 with support for both Python 2.7 and 3.5=.	documentation;interactivity;kaldi;library (computing);numpy;open-source software;python;speech recognition;user experience	Dogan Can;Victor R. Martinez;Pavlos Papadopoulos;Shrikanth (Shri) Narayanan	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462463	programming language;documentation;numpy;feature extraction;mel-frequency cepstrum;python (programming language);artificial intelligence;user experience design;extensibility;pattern recognition;scripting language;computer science	Robotics	-43.809289059876505	-31.48611582630042	100347
509b268a28c6212725e2019f035fd2dd782d2b2e	feature-preserving 3d thumbnail creation via mesh decomposition and approximation	3d model;feature preservation	We propose an innovative approach to feature-preserving 3D thumbnail creation in this research. The 3D thumbnail system aims to help the user efficiently browse 3D models in a large 3D database. The user can browse multiple 3D models with pre-generated 3D thumbnails, and view them from different angles interactively. The 3D thumbnail is a simplified version of the original model and requires much less memory and time to render. In the proposed system, we separate the framework into offline and online two processes for improving the run-time performance. Two types of descriptors of each 3D model are first generated offline and, then, the thumbnail result can be quickly rendered online using these pre-generated descriptors. Preliminary experimental results are given to demonstrate the proposed methodology.	3d modeling;approximation;browsing;interactivity;online and offline;polygonal modeling;thumbnail	Pei-Ying Chiang;May-Chen Kuo;Teri M. Silva;Edgar Evangelista;Milton Rosenberg;C.-C. Jay Kuo	2010		10.1007/978-3-642-15702-8_2	computer science;multimedia;world wide web;computer graphics (images)	ML	-41.20729752463896	-32.55864228952101	100816
7e9d77c808961ff42f65f8bd5e0ade60bfe562d1	cardboardizer: creatively customize, articulate and fold 3d mesh models	foldable design;cardboard prototyping;articulated model	Computer-aided design of flat patterns allows designers to prototype foldable 3D objects made of heterogeneous sheets of material. We found origami designs are often characterized by pre-synthesized patterns and automated algorithms. Furthermore, augmenting articulated features to a desired model requires time-consuming synthesis of interconnected joints. This paper presents CardBoardiZer, a rapid cardboard based prototyping platform that allows everyday sculptural 3D models to be easily customized, articulated and folded. We develop a building platform to allow the designer to 1) import a desired 3D shape, 2) customize articulated partitions into planar or volumetric foldable patterns, and 3) define rotational movements between partitions. The system unfolds the model into 2D crease-cut-slot patterns ready for die-cutting and folding. In this paper, we developed interactive algorithms and validated the usability of CardBoardiZer using various 3D models. Furthermore, comparisons between CardBoardiZer and methods of Autodesk® 123D Make, demonstrated significantly shorter time-to-prototype and ease of fabrication.	123d;3d modeling;algorithm;computer-aided design;fold (higher-order function);prototype;rollable display;usability	Yunbo Zhang;Wei Gao;Luis Paredes;Karthik Ramani	2016		10.1145/2858036.2858362	simulation	HCI	-38.57158929537107	-33.6857878636124	100910
5fa3af339964e7b68e5849a22a72211e8a11aa47	a review of verbal and non-verbal human-robot interactive communication	human robot interaction;verbal;non verbal;survey;human robot communication	In this paper, an overview of human-robot interactive communication is presented, covering verbal as well as non-verbal aspects. Following a historical introduction,and motivation towards fluid human-robot communication, ten desiderata are proposed, which provide an organizational axis both of r ecent as well as of future research on human-robot communication. Then, the ten desiderata are examined in detail, culminatin g to a unifying discussion, and a forward-looking conclusion. I. I NTRODUCTION: HISTORICAL OVERVIEW While the first modern-day industrial robot, Unimate, began work on the General Motors assembly line in 1961, and was conceived in 1954 by George Devol [1], [2], the concept of a robot has a very long history, starting in mythology and folklore, and the first mechanical predecessors (automa ta) having been constructed in Ancient Times. For example, in Greek mythology, the God Hephaestus is reputed to have made mechanical servants from gold ([3] in p.114, and [4] verse 18.419). Furthermore, a rich tradition of designing a d building mechanical, pneumatic or hydraulic automata also exists: from the automata of Ancient Egyptian temples, to th e mechanical pigeon of the Pythagorean Archytas of Tarantum circa 400BC [5], to the accounts of earlier automata found in the Lie Zi text in China in 300BC [6], to the devices of Heron of Alexandria [7] in the 1st century. The Islamic world also plays an important role in the development of automata; AlJazari, an Arab inventor, designed and constructed numerou s automatic machines, and is even reputed to have devised the first programmable humanoid robot in 1206AD [8]. The word “robot”, a Slavic word meaning servitude, was first used in this context by the Czech author Karel Capek in 1921 [9]. However, regarding robots with natural-language conversa tional abilities, it wasnt until the 1990’s that the first pio neering systems started to appear. Despite the long history of mytho logy and automata, and the fact that even the mythological handmaidens of Hephaestus were reputed to have been given a voice [3], and despite the fact that the first general-purpo se electronic speech synthesizer was developed by Noriko Omed a in Japan in 1968 [10], it wasnt until the early 1990’s that conversational robots such as MAIA [11], RHINO [12], and AESOP [13] appeared. These robots cover a range of intended application domains; for example, MAIA was intended to carry objects and deliver them, while RHINO is a museum guide robot, and AESOP a surgical robot. In more detail, the early systems include Polly, a robotic guide that could give tours in offices [14], [15]. Polly had very simple interaction capacities; it could perceive huma n feet waving a “tour wanted” signal, and then it would just use pre-determined phrases during the tour itself. A slightly m ore advanced system was TJ [16]. TJ could verbally respond to simple commands, such as “go left”, albeit through a keyboar d. RHINO, on the other hand [12], could respond to tour-start commands, but then, again, just offered a pre-programmed to ur with fixed programmer-defined verbal descriptions. Regardi ng mobile assistant robots with conversational capabilities in the 1990s, a classic system is MAIA [11], [17], obeying simple commands, and carrying objects around places, as well as the mobile office assistant which could not only deliver parcels but guide visitors described in [18], and the similar in functio nality Japanese-language robot Jijo-2 [19], [20], [21]. Finally, an important book from the period is [22], which is characteristi c of the traditional natural-language semantics-inspired the oretical approaches to the problem of human-robot communication, and also of the great gap between the theoretical proposals and the actual implemented systems of this early decade. What is common to all the above early systems is that they share a number of limitations. First, all of them only accept a fixed and small number of simple canned commands , and they respond with a set of canned answers . Second, the only speech acts(in the sense of Searle [23]) that they can handle are requests. Third, the dialogue they support is cle arly ot flexibly mixed initiative; in most cases it is just humaninitiative. Four, they dont really support situated language , i.e. language about their physical situations and events th at are happening around them; except for a fixed number of canned location names in a few cases. Five, they are not able to handleaffective speech ; i.e. emotion-carrying prosody is either recognized nor generated. Six, their non-verbal communication[24] capabilities are almost non-existent; for example, gestures, gait, facial expressions, and head nods are neither recognized nor produced. And seventh, their dialog ue systems are usually effectively stimulus-response or stim ulusstate-response systems; i.e. no real speech planningor purposeful dialogue generation is taking place, and certainly not in conjunction with the motor planning subsystems of the robot. Last but quite importantly, no real learning, off-line or on-the-fly is taking place in these systems; verbal behavior s have to be prescribed. All of these shortcomings of the early systems of the 1990s, effectively have become desiderata for the next two decades of research: the 2000s and 2010s, which we are in at the moment. Thus, in this paper, we will start by providing a discussion giving motivation to the need for existence of interactive r obots with natural human-robot communication capabilities, and then we will enlist a number of desiderata for such systems, which have also effectively become areas of active research in the last decade. Then, we will examine these desiderata one by one, and discuss the research that has taken place towards their fulfillment. Special consideration will be given to th e socalled “symbol grounding problem” [25], which is central to most endeavors towards natural language communication wit h physically embodied agents, such as robots. Finally, after a discussion of the most important open problems for the futur e, we will provide a concise conclusion. II. M OTIVATION : INTERACTIVE ROBOTS WITH NATURAL LANGUAGE CAPABILITIES BUT WHY? There are at least two avenues towards answering this fundamental question, and both will be attempted here. The first avenue will attempt to start from first principles and derive a rationale towards equipping robots with natural language . The second, more traditional and safe avenue, will start fro m a concrete, yet partially transient, base: application dom ains existing or potential. In more detail: Traditionally, there used to be clear separation between design and deployment phases for robots. Application-spec ific robots (for example, manufacturing robots, such as [26]) were: (a) designed by expert designers, (b) possibly tailor programmed and occasionally reprogrammed by specialist engineers at their installation site, and (c) interacted wi th their environment as well as with specialized operators during ac tual operation. However, the phenomenal simplicity but also the accompanying inflexibility and cost of this traditional set ting is often changing nowadays. For example, one might want to have broader-domain and less application-specific robot s, necessitating more generic designs, as well as less effort b y the programmer-engineers on site, in order to cover the vari ous contexts of operation. Even better, one might want to rely less on specialized operators, and to have robots interact a nd collaborate with non-expert humans with little if any prior training. Ideally, even the actual traditional programmin g and re-programming might also be transferred over to non-exper t humans; and instead of programming in a technical language, to be replaced by intuitive tuition by demonstration, imita tion and explanation [27], [28], [29]. Learning by demonstratio n and imitation for robots already has quite some active resea ch; but most examples only cover motor and aspects of learning, and language and communication is not involved deeply. And this is exactly where natural language and other forms of fluid and natural human-robot communication enter the picture: Unspecialized non-expert humans are used to (and quite good at) teaching and interacting with other humans through a mixture of natural language as well as nonverbal signs. Thus, it makes sense to capitalize on this existing ab ility of non-expert humans by building robots that do not require humans to adapt to them in a special way, and which can fluidly collaborate with other humans, interacting with the m and being taught by them in a natural manner, almost as if they were other humans themselves. Thus, based on the above observations, the following is one classic line of motivation towards justifying efforts f or equipping robots with natural language capabilities: Why n ot build robots that can comprehend and generate human-like interactive behaviors, so that they can cooperate with and b e taught by non-expert humans, so that they can be applied in a wide range of contexts with ease? And of course, as natural language plays a very important role within these behaviors, why not build robots that can fluidly converse wit h humans in natural language, also supporting crucial non-ve rbal communication aspects, in order to maximize communication effectiveness, and enable their quick and effective applic ation? Thus, having presented the classical line of reasoning arriving towards the utility of equipping robots with natural language capabilities, and having discussed a space of possibilities regarding role assignment between human and rob ot, let us now move to the second, more concrete, albeit less general avenue towards justifying conversational robots: nam ely, specific applications, existing or potential. Such applica tions, where natural human-robot interaction capabilities with v erbal and non-verbal aspects would be desirabl	ancient unix;apache axis;automata theory;binary prefix;canned response;circa;design rationale;document object model;embodied agent;emoticon;heron;humanoid robot;humans;human–robot interaction;industrial robot;interactivity;jargon;karel the robot;line level;microsoft word for mac;natural language;non-functional requirement;north american mesoscale model;obedience (human behavior);office assistant;online and offline;polly (robot);programmer;re-order buffer;semantic prosody;situated;software deployment;spec#;speech synthesis;uniform memory access;unimate;verse protocol;dialog	Nikolaos Mavridis	2015	Robotics and Autonomous Systems	10.1016/j.robot.2014.09.031	human–robot interaction;nonverbal communication;computer science;artificial intelligence;multimedia	Robotics	-46.700548988927125	-36.13130594097847	100911
54130e832188cb3334f0a2e19463e9ccc36a4284	runtime user interface specification using direct manipulation			direct manipulation interface;user interface specification	Robert Tibbitt-Eggleton	1991				HCI	-42.199029764848554	-29.706224395883858	101172
647d31e3780a0978bcebe8fe5ac0d988b86158fe	interactive color perspective for 3d graphics applications: enhancing depth perception and the understanding of object relations		Perceiving depth and spatial relations between objects in virtual environments is challenging and can be facilitated by the rendering process in 3D graphics applications. Often the perspective projection is not sufficient to visualize all necessary information because the projected image can lead to position and orientation ambiguity. Therefore, additional indicators are needed to improve the visualization of information about spatial relationships and the structure of the scene. For this purpose, we introduce a toolbox that applies color as an interactive design tool. Within this toolbox, six algorithms can be used to dynamically modulate the coloring of single objects or the scene as a whole. For evaluation, we report a study that tested whether object coloring as implemented in the toolbox can change apparent depth.	3d computer graphics;3d projection;algorithm;color;depth perception;design tool;graph coloring;interactive design;virtual reality	Dietrich Kammer;Jan Wojdziak;Rainer Groh	2013		10.1007/978-3-642-39473-7_96	computer vision;computer science;multimedia;computer graphics (images)	Visualization	-36.25531072667817	-36.098285992039926	101418
7d65ac10f04a6a6f971d43726e42f13a0f7ce98d	coney island: combining jmax, spat and vss for acoustic integration of spatial and temporal models in a virtual reality installation		We present a case study of sound production and performance that optimize the interactivity of model-based VR systems. We analyze problems for audio presentation in VR architectures and we demonstrate solutions obtained by a model-based data-driven component architecture that supports interactive scheduling. Criteria and a protocol for coupling jMax and VSS software are described. We conclude with recommendations for diagnostic tools, sound authoring middleware, and further research on sound feedback methods to support a topology of interacting observers. 1. The VR Audio Problem Space We configure Virtual Reality to provide real-time interaction with geometric and temporal models. The dominant medium for displaying immersive VR is animated computer graphic images. While VR is frequently referred to as a “space” based upon 3D models, in most VR systems the primary physical space is constrained to a flat surface where an image is projected. A virtual camera view is generally a tetrahedron with its apex at the viewing position defining a viewing volume expanding symmetrically into a geometric 3-space. Stereo image computation generates depth cues by offsetting 2D images. For the most part visual “3-D” immersion results from 2D frontal image projection enhanced by interactive camera mobility. Introducing sound into a flat, frontal visual field often induces cinematic solutions: imitating space rather than simulating space. That is, fixed resonance characteristics and fixed timing of wave propagation, rather than spatio-temporal simulation. 3D audio has been touted as an important attribute of VR. However most VR systems presented at academic conferences and trade shows only provide rudimentary sound file playback, often coupled to MIDI-enabled devices. The significant limitation in this approach is the absence of information concerning an interactive simulation. Pre-recorded sounds and MIDI sequences can provide at best a rough approximation of the behavior of a simulated system. They are often observed to be mere imitations which cannot provide an accurate insight into the states of a real-time simulation. While they may confirm simple interaction, pre-determined sounds minimize the acoustic relevance of the degrees of freedom in a simulated system. If auditory feedback can improve the user’s spatial orientation and sense of real-time interaction, then why is VR audio typically limited to triggering pre-recorded sound file or MIDI file playback? A short answer is “there are no standard alternatives.” Unlike the comprehensive hardware solutions provided by proprietary graphics subsystems, there have been few vendor-supported efforts toward sound synthesis subsystems. Historically it has proven difficult to argue for the commercial profitability of sound synthesis on general computing platforms. There are several problem areas to establish and maintain a flexible audio development subsystem in VR. These include real-time scheduling and synchronization of sounds, graphics and control signals from interactive sensors. At a higher level of abstraction, grammars are needed for describing sound synthesis in relation to VR models and events. Traditional audio paradigms such as multi-track recording and sample-based or orchestra-scoreion, grammars are needed for describing sound synthesis in relation to VR models and events. Traditional audio paradigms such as multi-track recording and sample-based or orchestra-score descriptions of sound production, were developed in an era when virtual experiences could only be imitated. From our experience these paradigms are not compatible with the coherent spatial and temporal models central to VR. In this paper we discuss sound production and performance that optimize the interactivity of modelbased virtual systems. We present a case study of a VR installation called Coney Island. The functional software roles of the Coney Island architecture include (1) a VR authoring and rendering system, (2) Sound Synthesis engines, (3) Spatial Audio DSP, (4) Sound Authoring, and (5) Scheduling and Synchronization of sounds with graphics and simulations. Software from several research centers was combined to fill these roles: (1) A VR authoring environment called ScoreGraph (Choi 1998) was used to create the Coney Island simulations, graphics, message passing and interactive scheduling. ScoreGraph provides the context that determines the requirements for interoperability of sound production modules. (2) Sound synthesis was provided by VSS (Bargar 1994), including synthesis engines from the STK toolkit (Cook 1995), and by jMax. (3) Distance and directional cues for sound sources were generated using Spatialisatuer (Spat) (Jot 1995) running in jMax. (4) VSS performs VR data interpretation to generate synthesis parameter control messages. (5) These messages are exchanged and scheduled in real-time by ScoreGraph and VSS. Our discussion will proceed from the VR environment to the VR software architecture, and then to sound production. 2. The Coney Island Scenario Coney Island is a VR installation designed to explore and demonstrate advanced auditory display of spatial and temporal models. The installation provides an interactive tour of an archipelago of mechanized islands that comprise a fantastic carnival playground. The islands are driven by simulated mechanics and particle system dynamics, as well as advanced geometric, lighting and camera models for computer graphics. For each island MIDI-enabled drum pads allow up to ten observers to play simultaneously. Simulations provide a time-critical environment where players can impart forces and see and hear the resulting mechanical actions and particle system collisions. The tour continues underwater where the players can impart force to currents that activate sound-producing clusters of floating objects. In each case equations of motion convert the forces into motions of graphical objects and in parallel into sounds. Coney Island was designed as a case study for close coupling of audio signal processing to spatial and temporal VR paradigms. Multiple independent sound-producing events are determined by sensor data combined with simulated mechanics of rigid polygonal bodies and particle systems. Sensor, graphic and sound events must be scheduled to provide satisfactory temporal feedback. At the same time overlapping audio events must be rendered in a spatial model that allows each player the proper orientation with respect both to a view of the virtual world and a position in the real world adjacent to other players. Hardware Configuration IRCAM Studio 5 was arranged with a large-format video projection on the wall opposite its entrance. Graphics and simulations were rendered in ScoreGraph on an SGI Onyx and the image transmitted to the projector. Figure 1 shows ten MIDI-enabled drum pads positioned in the center of the installation facing the projection screen, with a solo joystick on the left and sound computation hardware on the right. Signals from players’ actions were input to ScoreGraph simulations, and the resulting movement events passed to graphics and sounds. Figure 2 shows a group of players at the IRCAM installation; Figure 3 shows the players’ view of a Coney Island scene. 1 Coney Island was presented at IRCAM during the June 1999 Portes Ouvertes. The sound system consisted of three multi-channel computer sound sources, a mixer and a 4-channel diffusion system with monitors positioned in the corners of the room. Audio software performed in realtime on linux, NT and Irix platforms. Data was transmitted from ScoreGraph to VSS and from VSS to jMax using udp. Audio sources included 2-channel VSS on Linux and NT PCs, 2-channel jMax on Linux PC and 4-channel jMax on an SGI Octane. 3. Coney Island: VR architecture and graphics Coney Island uses a software framework named ScoreGraph to organize its numerical simulations and interactive graphics, to manage input from a user interface, and to send audio control signals to VSS. ScoreGraph is a system for authoring and managing the presentation of interactive, real-time graphics and sound applications. ScoreGraph provides a scheduler and libraries for data computation and multi-threaded communication. A ScoreGraph application consists of reusable software modules written in C++ and a script that specifies the configuration and behavior of those modules at runtime. Application components are roughly divided into input devices, computational models, and graphics and sound displays. Individual components, called nodes, are organized into a directed graph, the edges of which represent control signal flow. When the application is run it is organized into parallel threads that manage the execution of its nodes. The service rates of the threads are independent of each Figure 1: Coney Island setup at IRCAM Figure 3: Players' view of Coney Island. Figure 2: IRCAM visitors interact with Coney Island simulations using MIDI drum pads. other (and, notably, independent of the graphics frame rate), and may in fact change as the real-time system evolves. Coney Island integrates user input from ten MIDI drum pads, physically-based mechanical simulations, and three-dimensional geometric models created with Alias|Wavefront’s Maya. The application runs on a four processor Silicon Graphics Onyx 2 with an Infinite Reality 2 graphics board. Graphics and Particle Simulations The visual space presented in Coney Island includes five islands floating on top of ocean waves, each of which contains a mechanical game. The games are similar to pinball: users apply forces to move particles toward some goal. Each island consists of a hierarchical geometric model created in Maya, and a physically based particle simulation to drive the animation. The particle systems model the forces applied by the user, particle collision against other particles and against three dimensional geometry, particle mass and radius, gravity, and friction. The	3d modeling;acoustic cryptanalysis;apex (geometry);approximation;audio signal processing;auditory display;autodesk maya;c++;coherence (physics);component-based software engineering;computation;computational model;computer graphics;computer simulation;depth perception;diff utility;directed graph;drum memory;experience;geometric modeling;graphical user interface;irix;immersion (virtual reality);input device;interaction;interactivity;iota and jot;joystick;library (computing);linux;midi;message passing;middleware;numerical analysis;parallel computing;particle filter;particle system;projection screen;real-time cmix;real-time clock;real-time computing;real-time locating system;real-time operating system;real-time transcription;relevance;rendering (computer graphics);requirement;resonance;run time (program lifecycle phase);sgi octane;sgi onyx;sgi onyx2;stk;scheduling (computing);sensor;sion's minimax theorem;software architecture;software framework;software propagation;sound blaster;sound card;structure of observed learning outcome;stunt island;system dynamics;thread (computing);video card;video projector;viewing frustum;virtual camera system;virtual reality;virtual world	Robin Bargar;François Déchelle;Insook Choi;Alex Betts;Camille Goudeseune;Norbert Schnell;Olivier Warusfel	2000			simulation;virtual reality;computer science	Graphics	-44.25001586165938	-35.14588578451129	101557
583de4079310f429eab4911c8dd8c00d0d22a0e5	using expert knowledge for distributed rendering optimization		The generation of virtual images, which do not differ from those taken from the real world, from an abstract description of a 3D scene is defined as Photorealistic Image Synthesis. Since achieving greater realism is the ultimate goal, the rendering of a single image may take hours or days even on powerful computers. To face this challenge, in this work we discuss the potential benefits of combining the use of expert knowledge and the adoption of a multi-agent architecture in order to optimize the rendering of complex 3D scenes. Within this context, we apply novel techniques based on the use of expert knowledge to distribute the different work units in which the input scene is divided in a balanced way, to automatically generate the rendering engine setting parameters, and to optimize the configuration of the rendering parameters given by the user. The conducted experiments demonstrate that our approach can drastically reduce the rendering time with unnoticeable quality loss.	parallel rendering;program optimization	Carlos González-Morcillo	2011			computer vision;knowledge management;multimedia	ML	-38.303487048758946	-33.779575191876326	101574
9eda84d1807f10118d55b8299f7aba1369c9e618	automatic hypermedia generation (abstract)	automatic hypermedia generation		hypermedia	Jean-Louis Vuldy	1996	SIGWEB Newsletter	10.1145/231738.232573	multimedia;programming language	NLP	-43.620884067835036	-27.52104678352163	101772
72d30ec346de4fdcc91466c0a989e551b64b4382	embeddedbuttons: documents as user interfaces	user interface	Recent electronic document editors and hypertext systems allow users to create customized user interfaces by adding user-pressable buttons to on-screen documents. Positioning these buttons is easy because users are already tlamlliar with the use of document editors. Unfortunately, the resulting user interfaces often ex!st only in stand-alone document systems, making it hard to integrate them with other applications. Furthermore, because buttons are usually treated as special document objects, they cannot take advan~age of document editor formatting and layout capabilities to create thenappearance This paper describes the EmbeddedButtons architecture, which makes ~asy to integrate buttons into documents and to use the resulting documents for a variety of user interface types. EmbeddedButtons allows arbitrary document elements to behave as buttons. Documents can be linked to application windows to serve as application control panels. Buttons can store and display application state to serve as mode indicators. New button classes, editors, and apphcations can be added dynamically	global positioning system;hypertext;microsoft windows;state (computer science);user interface	Eric A. Bier	1991		10.1145/120782.120787	user interface design;look and feel;user;10-foot user interface;interface metaphor;shell;natural language user interface;magic pushbutton;computer science;operating system;post-wimp;natural user interface;model–view–controller;user interface;multiple document interface	HCI	-42.687919627462016	-28.52852415954888	101777
0774a972aee4c780ba97c319bc530d429552d5a9	near-optimal character animation with continuous control	motion with constraints;constraint optimization;motion capture data;optimal control;obstacle avoidance;character animation;human animation	We present a new approach to realtime character animation with interactive control. Given a corpus of motion capture data and a desired task, we automatically compute near-optimal controllers using a low-dimensional basis representation. We show that these controllers produce motion that fluidly responds to several dimensions of user control and environmental constraints in realtime. Our results indicate that very few basis functions are required to create high-fidelity character controllers which permit complex user navigation and obstacle-avoidance tasks.		Adrien Treuille;Yongjoon Lee;Zoran Popovic	2007	ACM Trans. Graph.	10.1145/1276377.1276386	character animation;computer vision;mathematical optimization;constrained optimization;facial motion capture;simulation;optimal control;skeletal animation;computer science;mathematics;obstacle avoidance;computer graphics (images)	Graphics	-38.55846265097736	-36.24440432092874	101932
a61c172676cf044fdb3cbf8c91eff4b401e4aef1	an interactive drafting system based on two dimensional primitives	logic operator;special attention;dimensional primitive;cad;solid modeling;shape;graphics;visualization;automatic control;logic;design automation;geometry;user interface;technical drawing	This paper describes an interactive drafting system based on two dimensional primitives and logic operators.  A special attention is given to the ways of interaction derived from the analysis of the usual drafting procedures.	technical drawing	G. Cosmai;Umberto Cugini;Piero Mussio;Amri Napolitano	1982	19th Design Automation Conference	10.1145/800263.809254	computer vision;technical drawing;visualization;electronic design automation;shape;computer science;graphics;automatic control;cad;solid modeling;user interface;engineering drawing;logic;computer graphics (images)	EDA	-39.0845889473201	-31.29975104715602	101962
ecdbdd2d0e08a71194b4e99d1020b37e6e37b5e6	wbt content for geography and geology using vrml	e-leaming;geology;wbt;geography;vrml.;vrml	In this paper we report on WBT content for geography and geology using VRML. We also propose an idea of WBT content for local area study that has not yet been implemented and discuss its effect from the viewpoint of knowledge management.	knowledge management;vrml	Goro Akagi;Koichi Anada;Youzou Miyadera;Miyuki Shimizu;Kensei Tsuchida;Takeo Yaku;Maya Yasui	2006			geography;cartography;computer graphics (images)	HCI	-35.412064437726144	-31.760679858096484	102117
572911eefc0f895ba39fcbb42f880e61c005087d	editing and synthesizing two-character motions using a coupled inverted pendulum model	i 3 5 computer graphics physically based modeling;i 3 6 computer graphics interaction techniques;i 3 7 computer graphics three dimensnoal graphics and realism animation;categories and subject descriptors according to acm ccs	This study aims to develop a controller for use in the online simulation of two interacting characters. This controller is capable of generalizing two sets of interaction motions of the two characters based on the relationships between the characters. The controller can exhibit similar motions to a captured human motion while reacting in a natural way to the opponent character in real time. To achieve this, we propose a new type of physical model called a coupled inverted pendulum on carts that comprises two inverted pendulum on a cart models, one for each individual, which are coupled by a relationship model. The proposed framework is divided into two steps: motion analysis and motion synthesis. Motion analysis is an offline preprocessing step, which optimizes the control parameters to move the proposed model along a motion capture trajectory of two interacting humans. The optimization procedure generates a coupled pendulum trajectory which represents the relationship between two characters for each frame, and is used as a reference in the synthesis step. In the motion synthesis step, a new coupled pendulum trajectory is planned reflecting the effects of the physical interaction, and the captured reference motions are edited based on the planned trajectory produced by the coupled pendulum trajectory generator. To validate the proposed framework, we used a motion capture data set showing two people performing kickboxing. The proposed controller is able to generalize the behaviors of two humans to different situations such as different speeds and turning speeds in a realistic way in real time.	coexist (image);control system;experiment;fear, uncertainty and doubt;fundamental interaction;human–computer interaction;inverted pendulum;kinesiology;mathematical model;mathematical optimization;motion capture;online and offline;optical flow;preprocessor;ruby document format;web-based simulation	Jae-Pyung Hwang;Il Hong Suh;Taesoo Kwon	2014	Comput. Graph. Forum	10.1111/cgf.12470	computer vision;simulation;computer science;artificial intelligence;computer graphics (images)	Graphics	-38.51172879933383	-36.59300353440866	102194
6d80074603eaef2c9576d30ee24df2ca39916618	enhancing x3d for advanced mr appliances	image based lighting;camera model;real time;shader;x3d;differential rendering;image compositing;augmented reality;mixed reality	In this paper, we explore and discuss X3D as an application description language for advanced mixed reality environments. X3D has been established as an important platform for today's web-based visualization and VR applications. Yet, there are very few examples for augmented reality systems utilizing X3D beyond a simple geometric description format. In order to fulfill the image compositing and synthesis requests of today's augmented reality applications, we propose extensions to X3D, especially with a focus on lighting and realistic rendering.	augmented reality;compositing;global illumination;mixed reality;web application;x3d	Yvonne Jung;Tobias Alexander Franke;Patrick Dähne;Johannes Behr	2007		10.1145/1229390.1229394	augmented reality;simulation;computer science;multimedia;computer graphics (images)	Visualization	-41.11962925026065	-33.66266151241746	102210
3001047072b5a11cd565b87a9a7af4bf201833b9	codemetrpolis — a minecraft based collaboration tool for developers	groupware;source code visualisation;source code metrics;data visualization games software buildings collaboration visualization measurement;game engine;virtual reality;image texture;data visualisation;virtual reality computer games data models data visualisation groupware image texture program visualisation reverse engineering;minecraft;computer games;program visualisation;source code metrics source code visualisation game engine minecraft;reverse engineering;data models;codemetrpolis visual elements data model interactivity features game engine graphical primitives source code properties representation virtual minecraft world collaborative source code evaluation source code exploration virtual world long range 3d scene display photorealistic texture role playing game expressive power high quality graphics computer games source code structure visualisation graphical technique code representation namespace structure architectural metaphor visualisation tools code comprehension data visualisation minecraft based collaboration tool	Data visualisation with high expressive power plays an important role in code comprehension. Recent visualisation tools try to fulfill the expectations of the users and use various analogies. For example, in an architectural metaphor, each class is represented by a building. Buildings are grouped into districts according to the structure of the namespaces. We think that these unique ways of code representation have great potential, but in our opinion they use very simple graphical techniques (shapes, figures, low resolution) to visualise the structure of the source code. On the other hand, computer games use high quality graphic and have high expressive power. A good example is Minecraft, a popular role playing game that supports both high definition, photorealistic textures and long range 3D scene displaying. Additionally, it provides great extensibility and interactivity for third party software. In this paper, we introduce our mission to create a virtual world of source code in which developers and other stakeholders could explore and evaluate their project collaboratively in a virtual Minecraft world. Code properties are represented by graphical primitives offered by the game engine, and various interactivity features are planned. Besides challenges of the implementation there are some fundamental research issues considering the selection of a set of visual elements and mapping to source code properties. These elements have to be compatible not only with the visualisation and with the data model but also with the thinking of developers.	data model;display resolution;executable;expressive power (computer science);extensibility;game engine;graphical user interface;image resolution;interactivity;junit;minecraft;pc game;prototype;software development;software industry;software metric;third-party software component;virtual world	Gergõ Balogh;Árpád Beszédes	2013	2013 First IEEE Working Conference on Software Visualization (VISSOFT)	10.1109/VISSOFT.2013.6650528	image texture;data modeling;simulation;computer science;theoretical computer science;virtual reality;multimedia;programming language;data visualization;reverse engineering	SE	-35.288936036489595	-32.62393949609882	102514
df1694beda91747a28ae8528c1f85042c63b37d0	meeting capture in a media enriched conference room	description systeme;system description;systeme intelligent;architecture systeme;multimedia;congres international;equipo edificio;congreso internacional;sistema inteligente;experience;video conference;movie camera;microfono;multimedia application;international conference;sala reunion;camara;meeting room;capture;video recording;intelligent system;computer aid;registro video;building equipment;arquitectura sistema;asistencia ordenador;captura;descripcion sistema;experiencia;equipement bâtiment;enregistrement video;system architecture;salle reunion;assistance ordinateur;camera;microphone	We describe a media enriched conference room designed for capturing meetings. Our goal is to do this in a flexible, seamless, and unobtrusive manner in a public conference room that is used for everyday work. Room activity is captured by computer controllable video cameras, video conference cameras, and ceiling microphones. Presentation material displayed on a large screen rear video projector is captured by a smart video source management component that automatically locates the highest fidelity image source. Wireless pen-based notebook computers are used to take notes, which provide indexes to the captured meeting. Images can be interactively and automatically incorporated into the notes. Captured meetings may be browsed on the Web with links to recorded video.	computer;interactivity;laptop;microphone;seamless3d;unobtrusive javascript;video projector;world wide web	Patrick Chiu;Ashutosh Kapuskar;Lynn Wilcox;Sarah Reitmeier	1999		10.1007/10705432_8	simulation;computer science;operating system;video capture;multimedia;regulatory capture;videoconferencing;systems architecture;computer graphics (images)	Visualization	-39.94472769399424	-25.867333534923233	102670
3ee2ddd8f989c9f0568f815b7f1f3cab04fd120a	the third wave in computer graphics and interactive techniques	product development computer graphics dp industry interactive devices;interactive techniques;computer graphics;graphics and multimedia;dp industry;visualization;display technology;graphics and multimedia computer graphics visualization interactive techniques display technology;data visualization;rendering computer graphics;data visualization computer graphics rendering computer graphics interactive systems product development;graphics systems design computer graphics interactive techniques product development graphics industry display device advances interaction device advances;interactive systems;interactive devices;product development	Computer graphics advances driven by product development became mature in the late 1980s, and advances driven by arts and entertainment matured in the early 2000s. The graphics industry is at an innovation plateau and is ready for the next wave of innovation. This third wave won't be driven in response to a single industry. Rather, innovative researchers will respond to three drivers: the visual representations necessary to handle emerging application disciplines, display and interaction device advances, and graphics systems design and implementation.	computer graphics;new product development;systems design	David J. Kasik	2011	IEEE Computer Graphics and Applications	10.1109/MCG.2011.64	video game graphics;graphics pipeline;scientific visualization;visualization;interactive visualization;human–computer interaction;computer science;computer graphics lighting;real-time computer graphics;multimedia;graphics software;computer graphics;data visualization;new product development;statistics;software rendering;3d computer graphics;computer graphics (images)	Visualization	-48.03023768797413	-29.42352896824727	102830
33e6d0f096af0f2e28a2681b3137838816485917	acoustic simulation with dynamic mechanisms in virtual reality	real time;virtual reality	Although most investigators have realized the importance of acoustic simulation in sophisticated VR systems, large computational load involved in this process often contradicts the requirements of real-time interaction, which in return brings on applying the expensive hardware or VR-specific workstations to this area. In order to reduce the computational cost and try to realize the real-time acoustic simulation in software with (or even without) some low-cost hardware, this paper proposes some dynamic mechanisms which can be used as possible strategies embedded into acoustic simulation in VR. Preliminary implementation of those mechanisms has proved to be fairly effective.	acoustic cryptanalysis;algorithmic efficiency;embedded system;real-time transcription;requirement;simulation;virtual reality;workstation	Qiong Zhang;Jiaoying Shi	1998	Journal of Computer Science and Technology	10.1007/BF02943197	real-time computing;simulation;computer science;operating system;virtual reality;computer graphics (images)	Embedded	-38.90785545979001	-36.876741224763585	102832
9ca8eeb258cc5d93f2937208daf3b7182b29a74b	a knowledge based interface for message translation between shop floor devices	communication service;knowledge base	This paper addresses the use of a knowledge based interface to provide the communication service of message translation. The focus of this study is on the technical feasibility of using a knowledge based architecture to perform this service for programmable shop floor devices. Implementation of a software interface using a knowledge based architecture is unique compared with the procedural programming approach used for today's custom interfaces.		John M. Usher;Gerald Graves	1990	J. Intelligent Manufacturing	10.1007/BF01572635	knowledge base;simulation;computer science;knowledge management;artificial intelligence;multimedia	Robotics	-45.00034530712728	-26.291916402501748	102875
609103d73c594d58f2293f0c62be490cfd8fc3d9	designing multi-sensory displays for abstract data	data structures computer science;human computer interaction;abstract data types computer science;virtual reality;data mining;visualization;phd doctorate	2		Keith Nesbitt	2003			information visualization;human–computer interaction;computer science;data science;multimedia;information and computer science	HCI	-41.09234169903556	-28.25238464759511	103209
41cf686d7b91274f4b7d5be9ee5f801e0fcc8a4f	on-the-fly tracking of flame surfaces for the visual analysis of combustion processes: on-the-fly tracking of flame surfaces				Timo Oster;A. Abdelsamie;M. Motejat;Tim Gerrits;Christian Rössl;Dominique Thévenin;Holger Theisel	2018	Comput. Graph. Forum	10.1111/cgf.13331	computer vision;on the fly;computer science;artificial intelligence;combustion	Vision	-46.747171794362146	-31.170903129488917	103337
8b6f258a247ad32e3880164131ae5a2910e3e317	interactive painting and lighting in dynamic multi-projection mapping		Digital Art and Design is a major part of our culture. Creating assets in a completely digital environment is well established and understood. Currently we see a rise in mixed-reality applications that aim to combine the traditional real-world based way of working with their digital counterparts. In this paper we present a non-destructive, immersive, fully dynamic mixed-reality painting system for real-world objects that combines the workflow of a traditional airbrush artist with the power of digital media. With this system we can completely alter the appearance of any Lambertian object.		Vanessa Lange;Christian Siegl;Matteo Colaianni;Philipp Kurth;Marc Stamminger;Frank Bauer	2016		10.1007/978-3-319-40651-0_10	computer vision;immersion (virtual reality);digital media;artificial intelligence;digital art;computer science;projection mapping;architectural lighting design;workflow;painting	HCI	-40.464833487212054	-33.509353059368856	103547
e5b22b423e054e2fc2f1f6ba05842989ee996209	advances in computer graphics hardware iv (eurographics'89 workshop)	computer graphic		computer graphics;eurographics;graphics hardware		1991		10.2312/346	computing;computer graphics lighting;graphics software	Graphics	-47.296241459269226	-29.411376830586807	103827
21b7013d2349597541538529280907117f82893f	a framework of augmented reality for geotagged videos	gis;gis augmented reality system geotagged video prototype system mobile phone geocoordinate parameter orientation parameter v world open platform keyhole markup language kml geographic information systems;geotagged video;xml augmented reality geographic information systems mobile computing public domain software video signal processing;gis kml geotagged video v world;kml;v world	In this paper, we present a prototype system and a framework for the implementation of augmented reality system for a geotagged video. In the proposed system, a live video is captured by a mobile phone and the corresponding geo-coordinates and orientation parameters are recorded at the same time. Based on the parameters, the information about buildings around the center position of the mobile phone is obtained from a database called V-World (http://map.vworld.kr) that is an open platform built by Korean government and freely available through the online. More specifically, the keyhole markup language (KML) is used for the description of the parameters for a captured video, and a detailed description about a chosen building, e.g., the name and the height of the building are obtained from V-World and overlaid on the video. The proposed approach is based on a database freely available through the online and a new database is not required to be constructed for the service, which is very essential to provide augmented reality services.	augmented reality;geographic coordinate system;geotagging;keyhole markup language;mobile phone;open platform;prototype	Chung-Hyun Ahn;Tae-Hyun Hwang;Kyoung-Ho Choi	2015	2015 21st Korea-Japan Joint Workshop on Frontiers of Computer Vision (FCV)	10.1109/FCV.2015.7103710	augmented reality;simulation;computer science;multimedia;world wide web	Vision	-34.029361055007556	-36.04948504905104	104008
1603e2b2db81be23f6d20240e95f8f119d3d9f47	megaphone: a multimedia application based on object-oriented communiation	multimedia application;object oriented	‘MEGAPHONE’ is a pilot multimedia application combining the functions of a video phone with the features of a workstation. It is built upon an object oriented framework that provides workstation-based personal multimedia communication. The video phone functions are fully integrated into the user interface of the workstation used. In addition to the face-to-face meeting mode known from video conference studios, the application offers a shared workspace for discussion, presentation and exchange of digital documents. The integration of telecommunication services with computer conference applications shows useful insights into the architecture of upcoming multimedia applications and their demands on the underlying operating system and network software.		Daniel P. Ingold	1992		10.1007/3-540-57183-3_33	computer science;multimedia;object-oriented programming	PL	-46.949843301692844	-26.671122973032322	104096
d7194a5746fea81f92812e24c862f050c89ba575	treeannotator: versatile visual annotation of hierarchical text relations		We introduce TREEANNOTATOR, a graphical tool for annotating tree-like structures, in particular structures that jointly map dependency relations and inclusion hierarchies, as used by Rhetorical Structure Theory (RST). TREEANNOTATOR is browser-based, embedded within the UIMA framework and provides two visualization modes. TREEANNOTATOR’s interoperability exceeds similar tools, providing a wider range of formats, while annotation work can be completed more quickly due to a revised input method for RST dependency relations. TREEANNOTATOR offers a multiple window view, which allows users to inspect several annotations side by side. For storing and versioning annotations, the UIMA Database Interface (UIMA DI) was developed to save documents based on a pre-defined type system. These features not only connect TREEANNOTATOR annotations to modern technological and dialog theoretical work, but set it apart from related tools. The ease of use of TREEANNOTATOR and its newly designed user interface is evaluated in a user study consisting of annotating rhetorical relations with TREEANNOTATOR and the classic RSTTool.	apache uima;embedded system;input method;intel matrix raid;interoperability;type system;usability testing;user interface;web application;dialog	Philipp Helfrich;Elias Rieb;Giuseppe Abrami;Andy Lücking;Alexander Mehler	2018			artificial intelligence;natural language processing;speech recognition;computer science;annotation	HCI	-39.050600268682814	-28.970309482201237	104689
4d9cb265d33bb9cce790a1df85807b8503f60c6e	supporting mobile activities in a shared semantic ambient	modelizacion;distributed system;interfase usuario;reseau social;settore inf 01 informatica;systeme reparti;activite humaine;user interface;cultural heritage;semantics;semantica;semantique;modelisation;social network;patrimoine culturel;sistema repartido;internet;patrimonio cultural;interface utilisateur;actividad humana;modeling;cooperative learning;human activity;red social	We discuss an approach for modeling human activities in complex real environments, considering the delivery of services as a function of the semantic features of the environment and of the interaction between the users and their social networks. We propose an architecture supporting such a model, and discuss a case study about cooperative learning in a cultural heritage site.	blog;social network;user (computing)	Fabio Pittarello;Augusto Celentano	2006		10.1007/11915034_116	cooperative learning;the internet;systems modeling;human–computer interaction;computer science;cultural heritage;artificial intelligence;semantics;user interface;social network	AI	-36.08609780580792	-25.82099848888774	104700
5016f538ffe9618f626111f9e236a2aec57cdca0	running greenstone on an ipod	multimedia;digital library;conference contribution;computer science;mobile digital libraries;open source	The open source digital library software Greenstone is demonstrated running on an iPod. The standalone configuration supports browsing, searching and displaying documents in a range of media formats. Plugged in to a host computer (Mac, Linux, or Windows), the exact same facilities are made available to the world through a built-in web server.	digital library;host (network);linux;microsoft windows;open-source software;server (computing);web server;ipod	David Bainbridge;Steve Jones;Sam McIntosh;Matt Jones;Ian H. Witten	2008		10.1145/1378889.1378966	digital library;computer science;operating system;multimedia;world wide web	OS	-43.92266164638488	-25.793003660603073	104790
b02806d7a1780c937b7b7d375bb71b73af63667c	a multimodal artistic interface	design tool;art;haptics;tactile sensing;visual feedback;design tools	Traditional artists receive feedback from their tools that excites the visual, aural and tactile senses. The digital artist often only receives visual feedback. We propose a novel painting interface that incorporates the aural, tactile and visual senses, to enable users more accurate control over their painting tool. The multimodality of this system makes it unique amongst other existing digital art interfaces.	application programming interface;excited state;multimodal interaction	Nicola Quinn;Mikael Fernström	2005		10.1145/1186954.1186982	computer vision;computer science;artificial intelligence;multimedia;haptic technology	HCI	-45.491468680445834	-36.86711809026177	105000
9ca1e8f8e6c5747b0b9ffff4640e926d86664c4d	the design and building of the graphic user interface for the collaborative desktop	distributed system;iterative method;sra informations och kommunikationsteknik;interfase usuario;representation graphique;base donnee repartie;systeme reparti;design process;human computer interaction;distributed database;media and communication technology;manniska datorinteraktion interaktionsdesign;programming environment;user interface;computer graphics;implementation;representacion grafica;cooperation;sra ict;computer supported cooperative work;base repartida dato;conception;mode conversationnel;interactive mode;cooperacion;metodo iterativo;medio ambiente programacion;ejecucion;medieteknik;sistema repartido;design and implementation;methode iterative;modo conversacional;diseno;graphic user interface;design;interface utilisateur;user interface design;grafico computadora;interaction design;infographie;graphics;environnement programmation;graphic design	In this paper we describe design and implementation considerations for the graphic user interface for CoDesk (the Collaborative Desktop), which is an environment for CSCW (Computer Supported Cooperative Work). CoDesk is an attempt to make collaboration a natural part of the daily use of a computer. Our way to achieve this is to put the user in the centre of the computing in a similar way that applications and documents are defined and visualised in the desktop metaphor. The Collaborative Desktop is aimed to be a generic environment with a GUI (Graphic User Interface) that can be used by many persons with different backgrounds. We have developed it as an extension of a computer environment we know works for many users: the desktop metaphor that has made daily computing a lot easier and error tolerant. Here we give an overview of the design and implementation of the CoDesk user interface: design principles, design process, implementation techniques. In an iterative and interactive design process we have focused on creating a dialogue between the developers and potential users. The last chapter contains some conclusions and a vision how the traditional desktop metaphor could be extended using new ideas of the graphical design. Tollmar & Sundblad CoDesk 18 april 1995 2 11.24 The Design and Building of the Graphic User Interface for The Collaborative Desktop Konrad Tollmar and Yngve Sundblad IPLab (Interaction and Presentation Laboratory) NADA, KTH, S-100 44 Stockholm, Sweden Email: konrad / yngve @nada.kth.se	computer-supported cooperative work;desktop computer;desktop metaphor;distributed computing;distributed database;email;error-tolerant design;full scale;graphical user interface;interactive design;iterative method;library (computing);programming tool;prototype;user interface design;victor yngve	Konrad Tollmar;Yngve Sundblad	1995	Computers & Graphics	10.1016/0097-8493(94)00141-K	graphic design;user interface design;design;10-foot user interface;user experience design;simulation;desktop metaphor;design process;interface metaphor;shell;human–computer interaction;desktop management interface;computer science;graphics;artificial intelligence;operating system;interaction design;environmental graphic design;iterative method;multimedia;virtual desktop;computer graphics;user interface;implementation;distributed database;cooperation;computer graphics (images)	HCI	-38.3852691551992	-27.52974374738747	105028
e1c8275e97c519f1681a0e1f20434fb3b86fdd80	arweather — an augmented reality weather ystem	design and development;snow;wearable computers;virtual reality;i 6 8 types of simulation animation visual i 3 7 three dimensional graphics and realism virtual reality animation;simulated weather augmented reality weather system arweather simulation application weather types mobile augmented reality system tinmith system wearable computer system arweather system;computational modeling;wearable computers augmented reality mobile computing;solid modeling;rain;i 3 7 three dimensional graphics and realism virtual reality animation;wearable computer;atmospheric modeling;three dimensional graphics and realism;augmented reality;mobile computing;meteorology rain snow atmospheric modeling augmented reality solid modeling computational modeling;i 6 8 types of simulation animation visual;meteorology;mobile augmented reality	This paper presents the design and development of an ARWeather simulation application, which can simulate various types of precipitation: rain, snow, and hail. We analysed various real occurrences weather types and how they could be simulated in a mobile Augmented Reality system. The Tinmith system is wearable computer system for the development and deployment of the final ARWeather system that allows for autonomous and free movement for the user. The users can move freely inside the simulated weather without limitation.	augmented reality;autonomous robot;simulation;software deployment;wearable computer	Marko Heinrich;Bruce H. Thomas;Stefan Müller	2008	2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2008.4637359	computer vision;augmented reality;simulation;wearable computer;computer science;virtual reality;mobile computing;computer graphics (images)	EDA	-37.61054011392161	-35.70776654594327	105039
ce9be9f9dbc20d90e75531ff927fe3510bb7efbe	the mutantrumpet		Ben Neill will demonstrate the mutantrumpet, a hybrid electroacoustic instrument. The capabilities of the mutantrumpet are designed to erase the boundaries between acoustic and electronic musical creation and performance. It is both an expanded acoustic instrument and an electronic controller capable of interacting with audio and video simultaneously. The demonstration will explore the multi-faceted possibilities that are offered by the mutantrumpet in several brief, wide ranging musical examples composed and improvised by Neill. Interactive video performance techniques and collaborations will be integrated into the excerpts. The aesthetics of live intermedia performance will be discussed along with a technical overview of the interface and associated software applications Junxion and RoSa from STEIM, Amsterdam. Reflections on the development of a virtuosic performance technique with a hybrid instrument and influences from collaborators Robert Moog, David Behrman, Ralph Abraham, DJ Spooky and others will be included in the presentation.	acoustic cryptanalysis;amiga reflections;faceted classification;interaction;intermedia (hypertext);moog (code)	Ben Neill	2017				HCI	-47.34117978647777	-33.91631988638853	105181
3b9758d51b7dacd6132ee33adcb47ca1303dc290	biomixer: a web-based collaborative ontology visualization tool		Ontology    development    often    requires    the    participation    of    various    col-­‐ laborators.     Web-­‐based     ontology     editors,     such     as     WebProtégé,     have been     developed     to     provide     users     with     collaborative     support     such     as comments    and    discussions.    There     is     a     large    body    of    work    concerning ontology    visualization    techniques;    however,    less    research    attention    has been    placed    on    providing     the    necessary    support     for     collaborative    on-­‐ tology    visualization.    To    explore     this     research    gap,     the    web-­‐based     col-­‐ laborative     ontology     visualization     tool     BioMixer     is     presented     in     this paper.     In     order     to     assist     the     collaborative     visualization     process,     Bio-­‐ Mixer    provides    users    with    sharable    workspaces    and    embeddable    visu-­‐ alizations    that    can    be    seamlessly    inserted    into    external    websites.	workspace	Bo Fu;Lars Grammel;Margaret-Anne D. Storey	2012			owl-s;world wide web;ontology-based data integration;open biomedical ontologies;process ontology;data mining;web modeling;ontology (information science);visualization;information visualization;computer science	HCI	-42.01540160856925	-25.232134280226127	105187
dbb149be78c4835168b7f892ed711a049ed2da4b	remixing playware	software;instruments;sensors;materials;music;robot kinematics	In this paper, we describe the concept of remixing playware, which allows sampling and remixing of both physical and functional (e.g. music content) aspects of a system. Such remixing playware has a number of distinguished features which are explained in the paper: user-configurable modularity, which allows the user to interact and manipulate with samples; user-guided behavior-based system, which allows music compositions to emerge from the way performer interacts with the instruments that provide the primitive behaviours; intelligent sampling as the ability of creating samples that allow anybody to remix with the samples ensuring an engaging outcome. The paper exemplifies remixing playware with a variety of implementations in RoboMusic concerts, the virtual MusicTiles app, the physical MagicCubes, the physical dices in Peter Gabriel concerts, and the S'n'S system. These examples focus on music creation and performance, based upon the concept of RoboMusic, and it is argued that the concept of remixing playware extends to many other application areas of playware.	configurable modularity;playware;sampling (signal processing)	Henrik Hautop Lund;Patrizia Marti;Michele Tittarelli	2014	The 23rd IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2014.6926229	simulation;computer science;artificial intelligence;music;multimedia;robot kinematics	Robotics	-47.25026684569715	-35.57854767245784	105373
75ccbad3694e89181aee3dd317bc275e4b9001ac	broadening our collaboration with design	architectural design;groupware;art;modeling technique;cad;visual design;industrial design data visualization design collaboration computer graphics modeling tools image creation tool automatic modeling tools architectural design urban design;information visualization;procedural modeling;collaboration computer graphics data visualization displays collaborative tools internet biomedical imaging design automation collaborative work world wide web;solid modelling cad data visualisation groupware software tools;computer graphic;image creation tool;data visualisation;procedural modeling information visualization visual analytics visual design art graphic design urban design industrial design architecture;digital content;computer graphics modeling tools;automatic modeling tools;design collaboration;data visualization;next generation;software tools;visual analytics;urban design;architecture;graphic design;solid modelling;industrial design	Computer graphics researchers have been collaborating successfully with engineers, architects, and artists for decades, focusing on better tools for model and image creation. Graphics researchers have already developed a wide range of procedural (automatic) modeling techniques, but with few exceptions, these focus on modeling natural objects, such as plants, terrains, and water. The next generation of tools must automate modeling of the most common and complex elements of digital content: manmade artifacts such as cities, buildings, vehicles, and furniture. Creating these tools require a new and close collaboration with architects as well as urban and industrial designers	computer graphics;digital artifact;digital recording;drug vehicle;engineering;morphologic artifacts;physical object;procedural programming	Benjamin Watson	2006	IEEE Computer Graphics and Applications	10.1109/MCG.2006.99	graphic design;computer vision;human–computer interaction;computer science;architecture;data mining;cad;multimedia;procedural modeling;data visualization;computer graphics (images)	Graphics	-35.57754654720194	-31.668505226130296	105489
90d35a72eea61e32459c93d2632eb6acdec55d71	basic consideration of 3d digital traditional japanese crafting system	local industries;high speed network;distributed database;virtual reality languages;computer graphics;information retrieval;high speed networks;cad;virtual reality;industries;three dimensional;fitting;distributed database system;internet;industrial training;user friendly 3d cg presentation system;character generation;database systems;interior exterior rooms;fitting industrial training productivity character generation information retrieval distributed databases high speed networks database systems internet java;distributed databases;vrml;advanced technology;world wide web;virtual reality 3d digital traditional japanese crafting system advanced technology local industries productivity user friendly 3d cg presentation system distributed database interior exterior rooms high speed network computer graphics internet presentation space model vrml java world wide web www cad;productivity;3d digital traditional japanese crafting system;industry structure;www;presentation space model;java;virtual reality cad computer graphics virtual reality languages internet distributed databases industries	In contrast with modemizing industrial structure in recent years, traditional Japanese crafting industries have not always introduced advanced technology. So, in order to activate those industries 3s local industries, it is required to promote their activities, train up their successors and improve the productivity. In this paper, we propose a user-friendly three-dimensional CC presentation system for typical Japanese crafting industries. Using our proposed system, user can retrieve fittings from distributed database based on user’s perception. see fittings without spoiling the natural beauty and design and present a interiodexterior rooms using high speed network. In order to realize the presentation space from many three-dimensional fitting data stored in distributed database system on Intemet, we classified the presentation space model and implement our system using VRML and Java.	distributed database;java;usability;vrml	Kaoru Sugita;Akihiro Miyakawa;Koji Hashimoto;Tomoe Fukamachi;Yoshitaka Shibata	2001		10.1109/ICOIN.2001.905566	three-dimensional space;productivity;the internet;simulation;vrml;computer science;operating system;cad;multimedia;computer graphics;java;distributed database;computer security;computer graphics (images)	DB	-45.88361857689341	-28.65620213517159	105520
c9c0e98c2d13a649a403303540745a45abc1f5a2	the development of an augmented virtuality for interactive face makeup system		In this paper, we focus on developing an interactive face makeup system that allows the user to practice applying makeup on their face without using real makeup materials. In our system, feature points on the human face were tracked by Kinect and mapped to a 3D face model. Face textures were generated and mapped on the model by using UV mapping technique. The makeup tools were developed for providing tangible interactions to users. Users can perceive a realistic makeup feeling by using our makeup tools. When the user applies the makeup, the program will paint the color on a face model, which synchronized with the users’ movement in real-time. The system is evaluated by a subjective evaluation method. The result shows that our system can provide a new and attractive makeup experience to the users compared to other makeup applications.	interactivity;virtuality	Bantita Treepong;Panut Wibulpolprasert;Hironori Mitake;Shoichi Hasegawa	2017		10.1007/978-3-319-76270-8_43	multimedia;uv mapping;augmented virtuality;human–computer interaction;feeling;computer science;tracking system	HCI	-42.010783598530224	-37.99569855258781	105946
e4ea59d01bc4ac08de3bde58273aa879b074c457	making tracking technology accessible in a rapid prototyping environment	wizard of oz application tracking technology rapid prototyping environment rapid prototyping system mixed reality augmented reality tracking configuration data fusion multiple trackers;prototypes augmented reality application software hardware object oriented modeling switches educational institutions system testing computer languages cameras;software prototyping;data fusion;wizard of oz;rapid prototyping;sensor fusion;augmented reality;sensor fusion software prototyping augmented reality tracking;tracking	In this paper we present an approach for exposing tracking technology in an accessible and flexible way to users of a rapid prototyping system for mixed (MR) and augmented reality (AR). Our system provides a tracking framework that alleviates the need for a high level of expertise while also presenting a model of the technology that allows for flexible modification of tracking configurations, the ability to quickly change an application from one type of tracking technology to another, and the creation of synthetic trackers for playback of prerecorded data, data fusion from multiple trackers, and wizard-of-oz applications.	augmented reality;high-level programming language;rapid prototyping;synthetic data	Maribeth Gandy Coleman;Blair MacIntyre;Steven Dow	2004	Third IEEE and ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2004.39	computer vision;augmented reality;simulation;computer science;sensor fusion;computer graphics (images)	Visualization	-42.760485934621954	-35.4416887918832	106549
2fb277ad87cfcb17685c05bca56ec3c5587af850	a celestial map renderer based on 3d graphics system	bright stars 3d graphics system amateur astronomers ordinary persons celestial body night sky celestial map rendering program opengl based full 3d graphics features star catalogues messier list yale catalog;rendering computer graphics planets three dimensional displays educational institutions databases computer science;astronomical catalogues;stars astronomical catalogues astronomy computing feature extraction rendering computer graphics sky brightness;astronomy computing;feature extraction;rendering computer graphics;stars;sky brightness	It is not so easy for amateur astronomers or ordinary persons to find a specific celestial body on the night sky. We represent a celestial map rendering program which reproduces the night sky at the specific time and location. With the OpenGL-based full 3D-graphics features, our system brings a sense of spatial realism. For the more accurate celestial map rendering, we referred to a set of star catalogues including Messier list, Yale Catalog of Bright Stars, and others. Since it is developed on typical PC's, our system can provide accurate celestial maps in an interactive manner, without any special equipment.	3d computer graphics;celestial coordinate system;map;messier object;opengl;star catalogue	Young Chun Kwon;Nakhoon Baek	2013	2013 International Conference on IT Convergence and Security (ICITCS)	10.1109/ICITCS.2013.6717771	celestial cartography;astronomy;astrophysics;physics;computer graphics (images)	Robotics	-35.08008273157324	-29.690415709775856	106668
09e05b4cd31361684ff4dab5767686db7b4464f1	practical issues in graphical constraints		Use of constraint-based techniques in interactive graphics applications poses a variety of unique challenges to system implementors. This paper begins by describing how interface concerns create demands on interactive, constraint-based, graphical applications. We will discuss why such applications must be able to handle systems of non-linear constraints, and survey some of the techniques available to solve them. Employing these numerical algorithms in the contexts of interactive systems provides a set of challenges, including dynamically setting up the equations to be solved and achieving adequate performance and scalability. This paper will explore these issues and describe the methods we have used in our efforts to address them.	algorithm;graphical user interface;graphics;nonlinear system;numerical analysis;scalability	Michael Gleicher	1993			mathematical optimization;theoretical computer science;graphics;scalability;computer science	Graphics	-41.60761506468127	-32.7981431357226	106783
3bc1137a88e259e7b26e360b70da803bb4b933a6	mapping and interaction strategies for performing environmental sound	instruments;measurement;interactive systems haptic interfaces human computer interaction;computational modeling;hidden markov models;solid modeling;aerospace electronics;computational modeling hidden markov models measurement solid modeling instruments aerospace electronics context;phenomenologically meaningful sound qualities environmental sound performance interaction strategies squeaking door model mapping strategies generic touch based interface;context	While the design of computational audio models for real-time generation of sound has been gaining increasing attention in the field of virtual reality and games over the last few years, questions related to expressivity and human performability have remained largely unexplored. Unlike in the design of interactive sonic artefacts a performable model requires a different approach to parametrisation and interaction. A model of a squeaking door is presented along with three contrasting mapping strategies between a generic touch-based interface and parameters controlling phenomenologically meaningful sound qualities. Each of these mapping strategies is evaluated in a controlled study based around a set of four metrics proposed by the authors. Correlations between quantitative and qualitative data verify the evaluation procedure for each of these metrics.	real-time locating system;virtual reality	Christian Heinrichs;Andrew W McPherson	2014	2014 IEEE VR Workshop: Sonic Interaction in Virtual Environments (SIVE)	10.1109/SIVE.2014.7006286	simulation;sonic interaction design;engineering;artificial intelligence;communication	Visualization	-45.6003853610294	-35.096473692179	106800
265e57884f72b8c1f7644a829c650612accb0fad	civo: real-time visualization of social activities by cartoonized twitter	action script;cartoonization;web services;php;twitter	We propose a web system called CiVo (City Voice) that is capable of visualizing public conversations on the Internet. It monitors the posting on the Twitter with respect to a set of specific phrases and displays the correspondent movie clips with cartoons on the time line on CiVo when it finds them. It extends the Twitter to a visual social medium that allows people to share what happens in the world at a glance. The main loop of CiVo runs on PHP while monitoring the Twitter using its API and it returns a specific SWF file to the HTTP clients for each GET request. The SWF file always updates its sub-SWFs reflecting the phrases found on the Twitter.	real-time transcription	Mitsuru Nakamura;Yosuke Miyazawa;Yoshikazu Kidera;Tsuyoshi Moriyama;Motoyasu Tamaki	2011		10.1007/978-3-642-24500-8_39	computer science;multimedia;internet privacy;world wide web	HCI	-45.013672025650195	-27.743349108177444	106805
6f7e00dde61971af624e6d4285ab8e58044765ad	transforming discourse models to structural user interface models	user interface;computer and information science;user interface design;data och informationsvetenskap;interaction design	User-interface design is still a time consuming and expensive task to do, but recent advances allow generating them from interaction design models. We present a model-driven approach for generating user interfaces out of interaction design models. Our interaction design models are discourse models, more precisely models of classes of dialogues. They are based on theories of human communication and should, therefore, be more understandable to humans than programs implementing user interfaces. Our discourse models also contain enough semantics to transform them automatically into user interfaces for multiple devices and modalities. This paper presents a two-step transformation approach with an intermediate user interface model. By showing specific transformation rules, we concentrate on a major part of the first step, transforming discourse models to structural user interface models.	interaction design;model-driven architecture;theory;user interface design	Sevan Kavaldjian;Cristian Bogdan;Jürgen Falb;Hermann Kaindl	2007		10.1007/978-3-540-69073-3_9	user interface design;user;user experience design;human action cycle;simulation;user modeling;interface metaphor;human–computer interaction;computer science;interaction design;multimedia;post-wimp;natural user interface;interactivity;user interface	AI	-39.350833673017746	-28.69079003702758	107220
fdbd9aae2807792a61fde1aa8e04e3c0cd4b9519	interactive schema integration with sphinx	interfase usuario;base donnee;learning algorithm;navegacion informacion;analisis estadistico;data integrity;integracion numerica;user interface;sql;navigation information;integration information;heuristic method;interrogation base donnee;information browsing;database;catalogs;tipo dato;interrogacion base datos;base dato;metodo heuristico;schema integration;intelligence artificielle;algorithme apprentissage;data type;probabilistic approach;catalogue;information integration;metamodel;internet;statistical analysis;metamodele;metamodelo;numerical integration;enfoque probabilista;approche probabiliste;analyse statistique;integracion informacion;artificial intelligence;interface utilisateur;modele donnee;methode heuristique;inteligencia artificial;catalogo;type donnee;integration numerique;algoritmo aprendizaje;database query;meta model;data models	The Internet has instigated a critical need for automated tools that facilitate integrating countless databases. Since non-technical end users are often the ultimate repositories of the domain information required to distinguish differences in data types, we suppose an effective solution must integrate simple GUI based data browsing tools and automatic mapping methods that eliminate technical users from the solution. We develop a meta-model of data integration as the basis for absorbing feedback from an end-user. The schema integration algorithm draws examples from the data and learns integrating view definitions by asking a user simple yes or no questions. The meta-model enables a search mechanism that is guaranteed to converge to a correct integrating view definition without the user having to know a view definition language such as SQL or even having to inspect the final view definition. We show how data catalog statistics, normally used to optimize queries, can be exploited to parameterize the search heuristics and improve the convergence of the learning algorithm.	algorithm;boolean algebra;converge;data mining;data structure;graphical user interface;heterogeneous database system;heuristic (computer science);internet;learning rule;metamodeling;nest (neural simulation tool);point and click;population;reverse engineering;sql;spaces;sphinx;xml	François Barbançon;Daniel P. Miranker	2004		10.1007/978-3-540-25957-2_15	metamodeling;computer science;artificial intelligence;data mining;database	DB	-36.88050647681499	-24.968391924005775	107403
14e8a5b0f3104279397750ad9616bcc8c349028c	application frameworks: experience with macapp	application development;application framework;personal computer;object oriented programming;visual programming;design and implementation;object oriented;graphic user interface;interactive graphics	It is an unfortunate truth that as personal computers have become easier to use they also have become more difficult to program. Students who want to write programs that look and feel like real Macintosh applications, for example, are constrained by the time available and the sheer complexity of the application development process. We have found that several object-oriented programming systems have succeeded in abstracting out the commonalties of graphical user interfaces and can allow students to construct interactive, graphical applications at a lower cost. In this paper we describe our experiences in the use of MacApp, an application framework for the Apple Macintosh. The design and implementation of Prof, a prototype visual programming system for both educators and students developed by a final year student in a thirteen week project course, is used to illustrate the discussion. We describe the advantages and disadvantages of the object-oriented application framework approach in the expectation that our experiences will be useful for other educators who may be contemplating following this path.	admissible numbering;application framework;graphical user interface;look and feel;macapp;personal computer;prototype;visual programming language	John R. Pugh;Cafee Leung	1988		10.1145/52964.53000	simulation;human–computer interaction;computer science;operating system;software engineering;multimedia;programming language;object-oriented programming	HCI	-48.16873149753101	-28.03442360651383	107439
3abc78dc58dd18db7aaafeda270cbd454bf51e8e	virtual worlds as metaphors for web sites exploration: are they effective?	electrical capacitance tomography;hypermedia markup languages;information resources;information classification;vr interfaces;hypermedia markup languages virtual reality user interfaces online front ends information resources information retrieval;helium;information retrieval;prototypes;virtual reality prototypes layout electrical capacitance tomography helium world wide web data mining visualization xml organizing;virtual reality;web site structure;information visualization;layout;data mining;desktop virtual reality;online front ends;visualization;organizing;web navigation metaphor;xml;web site exploration;structured web sites;world wide web;dtd documents;user interfaces;web site structure virtual worlds web site exploration desktop virtual reality web navigation metaphor structured web sites information classification vr interfaces xml dtd documents;human computer interface;virtual worlds	This paper discusses the effectiveness of desktop virtual reality as a metaphor for navigating through information in structured Web sites. The work is based on a model for information classification and on an architecture for generating VR interfaces from XML and DTD documents describing the Web site structure. Some comments stemming from the evaluation of a prototype implementation are presented.	desktop computer;prototype;stemming;virtual reality;virtual world;world wide web;xml	Augusto Celentano	1999		10.1109/VL.1999.795903	web service;web modeling;human–computer interaction;web standards;computer science;web navigation;web page;multimedia;world wide web	Web+IR	-42.92828089866603	-24.873436805241106	107445
389320041e9838f723cbbaff5fe3eec8727b77be	a calligraphy training system based on skill acquisition through haptization		We present a virtual reality system targeted at building a calligraphy training environment. Our main goal is to construct a virtual space enabling learners to intuitively acquire hard-to-inherit skills through the development of a Japanese calligraphy training system. The proposed system provides a haptic interaction channel allowing the learners to intuitively master instructor’s fine motor skills through the sense of touch. We utilize a commercially available haptic device called PHANTOM for simulating a writing brush in a virtual training space. The system implements a function for recording and replaying instructor’s hand motions via the PHANTOM device. The instructor’s writing techniques such as brushstrokes and pen pressures are recorded when he/she is writing characters and they are effectively reproduced and presented to the learners via the PHANTOM device. The learners can master how to write the recorded characters by feeling the instructor’s style of handwritings. We implemented a simple yet powerful 3D brush model for real-time visualization of handwritten characters without compromising the quality and reality of the visualized characters. The learners can start training at any time and iterate training sessions without worrying about resource consumptions such as papers and ink as much as they like. We conducted two experiments to validate the effectiveness of the proposed system for learning calligraphy in a virtual environment.	experiment;haptic technology;imaging phantom;internet;iteration;modality (human–computer interaction);norm (social);phantom reference;real-time locating system;simulation;usability;virtual reality	Hiroaki Nishino;Kouta Murayama;Kazuya Shuto;Tsuneo Kagawa;Kouichi Utsumiya	2011	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-010-0042-y	embedded system;simulation;human–computer interaction;artificial intelligence;multimedia	HCI	-44.460107095385695	-37.702734792957614	107458
824dcfb1a6895c150c50780e38346447715bcb6c	using virtual reality to visualize scientific, engineering, and medical data	medical data;virtual environments;successful virtual reality application;virtual reality;computational fluid dynamic;technical data set;visualize scientific;liquid crystal displays;navigation;virtual environment;data engineering;application software;biomedical engineering;data visualization;scientific visualization	In this paper we briefly discuss the state of the art of Virtual Reality as applied to visualization of scientific and technical data sets. We describe the technologies and software for the creation of Virtual Environments. We also give an overview of some of the more significant and successful Virtual Reality applications in the fields of medicine, engineering, chemistry and computational fluid dynamics.	computation;computational fluid dynamics;gesture recognition;graphics pipeline;graphics processing unit;haptic technology;multimodal interaction;speech recognition;texture mapping;thrust;virtual reality;workstation	Marco Lanzagorta;Lawrence J. Rosenblum;Eddy Kuo;Robert Rosenberg	1997	Scientific Visualization Conference (dagstuhl '97)		information visualization;human–computer interaction;computer science;multimedia;computer graphics (images)	Visualization	-34.10279904192387	-30.448126222132238	107582
d75ddca8bcb18820b535f3491f23461c40b0c3cc	web-based information systems development - a user centered engineering approach	developpement logiciel;user needs;metodologia;red www;web information system;web engineering;separation of concern;web based information system;interface design;product model;methodologie;user profile;internet;levels of abstraction;desarrollo logicial;software development;world wide web;reseau www;process model;information system;methodology;user goal centered;systeme information;user model;sistema informacion	Web-based Information System (WIS) engineering is more complex than traditional Information System (IS) engineering in that it raises many new issues such as presentation issues, user profiling, navigation support etc...This paper presents a method - a set of product models along with process models for the development of WIS.This method adds the dimension of user modeling and customization to Web engineering. By capturing the user profiles, the designer is able to define user categories and to tune the presentation of the WIS content according to the specificity of the user. Besides, by capturing the user goals, he is able to define guidelines for navigating in the Hyperspace in order to optimize the satisfaction of the user needs.Finally, the proposed approach considers, as it does in much traditional software approaches, three different levels of abstraction (conceptual, logical and physical). It also clearly separates the management of potential users data, content design, navigational design and interface design. Such separation of concerns facilitates many maintenance tasks and leads to a higher degree of independence and flexibility.	information system	Christophe Gnaho	2001		10.1007/3-540-45144-7_11	user interface design;user;user experience design;the internet;user modeling;computer user satisfaction;human–computer interaction;user journey;separation of concerns;computer science;artificial intelligence;user requirements document;software development;interface design;operating system;process modeling;methodology;database;web engineering;user interface;world wide web;computer security;user story;information system	SE	-38.05441144521938	-26.33304583687321	107588
63dfe1cba6d62227bcc73e63d08bf5e02ec07e2e	optimized rendering for a three-dimensional videoconferencing system	computers;software;stereoscopic 3d representation rendering three dimensional videoconferencing system interactive streaming point cloud data;teleconferencing streaming media cameras communication industry three dimensional displays computer networks image reconstruction rendering computer graphics clustering algorithms;teleconferencing;video streaming teleconferencing video communication;video streaming;3d point cloud;real time;head tracking;three dimensional;low latency;long distance;three dimensional displays;3d representation;point cloud;video communication;rendering computer graphics;meteorology;high speed;high definition;cameras	Industry widely employs the two-dimensional videoconferencing system as a long distance communication tool, but current limitations such as its tendency to misrepresent eye contact prevent it from becoming more widely adopted. We are exploring the possibility of a three-dimensional videoconferencing system for future interactive streaming of point cloud data, and present the preliminary research results in this paper. We have tested thus far with one sender and one receiver, using pre-recorded data for the sender. The sender, encircled by high-definition cameras, stands and speaks in a room. A cluster of computers reconstructs each frame of the camera images into a 3D point cloud and streams it across a high-speed, low-latency network. On the receiving end, a splat-based renderer employs a new algorithm to efficiently resample the points in real-time, maintaining a user-specified frame rate. Parallel hardware projects onto multiple screens while head tracking equipment records the viewer's movements, allowing the receiver to view a stereoscopic 3D representation of the sender from multiple angles. We can combine these visuals with appropriate use of multiple audio channels to forge an unparalleled virtual experience. This next step towards immersive 3D videoconferencing brings us closer to empowering worldwide collaboration between research departments.	algorithm;computer;forge;hdmi;motion capture;point cloud;real-time clock;stereoscopic video game;stereoscopy;texture splatting	Rachel Chu;Daniel Tenedorio;Jürgen P. Schulze;Susumu Date;Seiki Kuwabara;Atsushi Nakazawa;Haruo Takemura;Fang-Pang Lin	2008	2008 IEEE Fourth International Conference on eScience	10.1109/eScience.2008.42	computer vision;computer science;multimedia;computer graphics (images)	Visualization	-35.19594097080286	-35.502922156003194	107634
8f99a749aa841facdc3c0203296ff33776a1a060	building user interfaces for database applications: the o2 experience	user interface	There is more to database application user interface than building a main window with a menu bar at the top and a push buttons column on the left. Users want to display, edit, browse, and manipulate database objects in a variety of ways. These objects can be large, multimedia, and complex. The interactive manipulation of a database object is a three step process: (i) displaying: read the state of the object and build a graphical presentation of it; (ii) modifying: use cut /copy/paste , editing functions as well as application functions to modify the state of the object; (iii) saving: write the new state of the object back into the database. To support user interface development, two kinds of tools are commercially available: toolkits and user interface editors. A toolkit consists of an extensible set of predefined graphical components or widgets such as menus, buttons, scrollbars, and text editors. Almost anything can be done with a graphical toolkit, but its usage requires an extensive learning of the system and a significant programming effort. A user interface (UI) editor is dedicated to a particular toolkit. It allows to specify graphically a user interface. For instance, one uses the mouse to pick a widget (a menu, a scrollbar, ...), to define its position on the screen as well as its graphical attributes (color, fonts, ...). When the specification is over, the UI editor automatically generates the toolkit code that produces this user interface. The programmer needs neither to learn the toolkit nor to write any code. User interface development is thus significantly speeded up. Unfortunately, there are many things that cannot be specified with a mouse. A programming language and direct, hard toolkit coding are often needed to do the job. In the rest of this paper, we describe two systems. The first system, O2Look [Altm'r 1990, 02 91, Plateau 90] is a graphical toolkit. Unlike traditional graphical toolkits, 02Look does not provide functions to manipulate widgets but database objects. The programmer does not create a scrollbar but a presentation for a city (see Figure 1). Given an object, 02Look queries the database to know about this object type and state, automatically combines and initializes widgets to represent the object on the screen. When the screen presentation is modified, it checks with the database that the modification is correct and then writes the new state in the database. This is possible because 02Look is fully integrated with the database. It should be clearly understood that 02Look is not better than Motif but an extension of Motif to meet database requirements. On one hand, displaying a complex object with 02Look is a simple function call when it would take thousands of lines with Motif and when it would be impossible with a user interface editor. On the other hand, O2Look is not the appropriate tool to build the	apl;browsing;graphical user interface;list of toolkits;motif;object type (object-oriented programming);paste;programmer;programming language;requirement;software widget;text editor	Patrick Borras;Jean-Claude Mamou;Didier Plateau;Bruno Poyet;Didier Tallot	1992	SIGMOD Record	10.1145/130868.130873	user interface design;user;10-foot user interface;user experience design;interface metaphor;shell;human–computer interaction;natural language user interface;magic pushbutton;computer science;event-driven programming;post-wimp;natural user interface;user interface;graphical user interface testing	HCI	-41.62366914360742	-30.49505230652651	107761
e79a9d697ea41fd2c6eeb9ed1d2b9c963f9d198a	knowledge-based architecture for intelligent query user interfaces	intelligent user interface;user interface;information retrieval;strategic interaction;user interface design;problem solving;knowledge base	The design of a user interface to intelligently intermediate between the user and a DB query system, based on a modular, knowledge-based generic architecture is to be discussed. The main principles concerning the user interface design are: identification of the essential (1) tasks of intelligent intermediation and (2) (meta-)knowledge as the basis of performing these tasks, but also (3) active role for the user in the strategic decisions of the tasks. The resulting user interface architecture is transparent, easily adaptable, and makes it possible to model strategic interaction with the user as well.	user interface design	Frances M. T. Brazier;Zsófia Ruttkay	1993		10.1145/259964.260155	user interface design;user;knowledge base;user experience design;human action cycle;user modeling;interface metaphor;shell;human–computer interaction;natural language user interface;magic pushbutton;computer science;knowledge management;natural user interface;interactivity;user interface;world wide web;graphical user interface testing;multiple document interface	AI	-41.55590783433497	-28.20239420721462	107815
fa30695765e8d60ac1d8ea787855848eaf125ce9	magictoon: a 2d-to-3d creative cartoon modeling system with mobile ar	color;two dimensional displays;three dimensional displays;solid modeling;mobile handsets;atmospheric modeling;augmented reality	We present MagicToon, an interactive modeling system with mobile augmented reality (AR) that allows children to build 3D cartoon scenes creatively from their own 2D cartoon drawings on paper. Our system consists of two major components: an automatic 2D-to-3D cartoon model creator and an interactive model editor to construct more complicated AR scenes. The model creator can generate textured 3D cartoon models according to 2D drawings automatically and overlay them on the real world, bringing life to flat cartoon drawings. With our interactive model editor, the user can perform several optional operations on 3D models such as copying and animating in AR context through a touchscreen of a handheld device. The user can also author more complicated AR scenes by placing multiple registered drawings simultaneously. The results of our user study have shown that our system is easier to use compared with traditional sketch-based modeling systems and can give more play to children's innovations compared with AR coloring books.	3d modeling;augmented reality;book;graph coloring;microsoft lumia;mobile device;sketch-based modeling;touchscreen;usability testing	Lele Feng;Xubo Yang;Shuangjiu Xiao	2017	2017 IEEE Virtual Reality (VR)	10.1109/VR.2017.7892247	computer vision;atmospheric model;augmented reality;computer science;artificial intelligence;multimedia;solid modeling;computer graphics (images);mechanical engineering	Visualization	-40.60680491588464	-35.049088421158	107905
219f0a4d23cf28b73ee699a7fab9a272cc4acbde	a java implementation of the myro api for using personal robots in cs1	cs1 curricula;personal robots;ipre;java	Myro is popular Python-based API for controlling personal robots used in many CS1 courses around the world. This paper describes the authoru0027s implementation of the Myro API in Java.	application programming interface;java;python;robot	Douglas E. Harms	2011		10.1145/1999747.1999850	real-time computing;jsr 94;computer science;programming language;java;world wide web;personal robot	Robotics	-48.229010890859335	-27.31657780982967	108162
b979c823dbe281515c86fd2c9fbc9d6479434db6	guest login - visitor-centred information design		Projection of 3D space onto a 2D surface relies on the computer graphics camera, which is designed like the camera obscura. The procedure follows the laws of perspective projection and does not explicitly consider the viewer of the rendered image. Our approach is to extend this camera model in order to involve human perception into the rendering of 3D scenes. The aim is to create a user-centred spatial impression by a 2D image. By integrating the user into this process, implicit interactions can be applied to provide interfaces for an efficient and coherent communication of information in virtual environments. This paper introduces user-specific and context-specific parameters that must be taken into account when designing presence-aware applications. To this end, we present a concept of an interactive exhibition based on implicit interaction and point out its applicability in information design of three-dimensional scenes.	3d projection;coherence (physics);computer graphics;information design;interaction;login;virtual reality	Jan Wojdziak;Martin Zavesky;Ingmar S. Franke;Christian Lambeck;Rainer Groh	2011				Graphics	-42.980196092145235	-37.71513266424627	108371
e72ea52a75c4c3d2918c35650855a5ded7f0e49d	location, location, location: using spatial memory in an integrated development environments to assist program code comprehension?			integrated development environment;list comprehension	Craig J. Sutherland;Andrew Luxton-Reilly;Beryl Plimmer	2016			information retrieval;comprehension;computer science	HCI	-41.8303817426395	-28.458370177285257	108384
eff51452bfa0dbe19a246e1697664edf9ea90ace	virtual environments for creative work in collaborative music-making	distributed system;systeme reparti;conceptualization;realite virtuelle;sistema temporizado;realidad virtual;musician;real time;timed system;acoustique musicale;virtual reality;conceptualizacion;musical acoustics;sistema repartido;acustica musical;temps reel;systeme temporise;tiempo real;virtual environment;musicien;virtual space;conceptualisation;musico	Virtual environments are beginning to allow musicians to perform collaboratively in real time at a distance, coordinating on timing and conceptualization. The development of virtual spaces for collaboration necessitates more clearly specified theorizing about the nature of physical copresence in music-making: how the available communicative cues are likely to affect the nature of visually mediated rehearsal and performance. Pilot data for a project carried out at the New School for Social Research demonstrate some important factors relevant to designing remote spaces for musical collaboration, and suggest that virtual environments for musical collaboration could actually enhance the feeling of being together that creative musical expression requires.	conceptualization (information science);virtual reality	Michael F. Schober	2006	Virtual Reality	10.1007/s10055-006-0049-z	conceptualization;new interfaces for musical expression;simulation;computer science;virtual machine;artificial intelligence;musical acoustics;virtual reality;multimedia	Visualization	-35.60776499166394	-26.517247543520956	108572
9648d991752d7f614ec5c65bae9698ffd3a25109	answer: a semantic approach to film direction	film direction;animated previsualizations;visualization engine;post production;scene rendering;film making;ontologies answer semantic approach film direction film production system architecture scene description intuitive gesture graphical user interfaces scene rendering animated previsualizations film making;layout;ontologies artificial intelligence;data visualisation;visualization;semantic model;scene description;graphical user interfaces;semantic approach;engines;humanities;rendering computer graphics computer animation data visualisation graphical user interfaces humanities ontologies artificial intelligence;answer;post production semantic web rdf ontologies reasoning visualization engine film production;games;animation;semantic description;semantic web;graphic user interface;production;ontologies;reasoning;computer animation;system architecture;service oriented architecture;rendering computer graphics;film production;intuitive gesture;context;ontologies production graphical user interfaces engines layout animation service oriented architecture games rendering computer graphics semantic web;rdf;films	In this paper we present ANSWER, an innovative approach to film direction. Here we describe a methodology to semantically model the film domain in a way which is coherent with the director’s intent during film production. To achieve this, we are developing a system architecture which will provide the director with the necessary tools and services to author a scene description through intuitive gesture based graphical user interfaces, which will in turn populate the underlying model with a rich set of semantic descriptions. These semantic descriptions will be used to render the scene graphically through animated pre-visualizations. A director using the ANSWER methodology will be able to understand and assert certain film making decisions before film production begins.	application domain;coherence (physics);graphical user interface;population;systems architecture	Ajay Chakravarthy;Richard Beales;Paul W. Walland;Angelos Yannopoulos	2009	2009 Fourth International Conference on Internet and Web Applications and Services	10.1109/ICIW.2009.103	human–computer interaction;filmmaking;computer science;artificial intelligence;graphical user interface;database;multimedia;world wide web;data visualization;systems architecture	Robotics	-39.094560902811246	-32.63709530960908	108693
b940bc7631c6d6debc73dc90ba97015be028be1c	mixview: a portable, graphics-based soundfile editor and processor			graphics	Douglas Scott	1990			computer architecture;computer hardware;computer graphics (images)	EDA	-47.09722484882279	-29.297394032822282	108707
e0b9e5f69cda1386580e563b51f3d0ba69c840c7	shadows no. 4: belly dance and interactive electroacoustic musical performance	wireless;sensors;music performance;wireless sensor network;music technology;musical performance;dance	Shadows no. 4 is a piece for a tribal-fusion belly dancer, wireless sensor network, and electronics. The movement vocabulary is derivative of Raqs al-Sharqi, commonly known as danse orientale (Middle Eastern dance). This dance form involves slow and languid movement and controlled isolations. The piece experiments with of notions of gesture (dance and musical) in the performance of electroacoustic music. During the performance, sensors translate the dancer's movements into subtle and salient variations of the sonic texture.	experiment;sensor;vocabulary	Aurie Y. Hsu;Steven T. Kemper	2010		10.1145/1753846.1753929	music technology;wireless sensor network;computer science;sensor;dance;multimedia;wireless	Mobile	-46.26050423173063	-35.258033751453986	108864
7bb186c47a09d9ac8aea3da05e1feddbec33abb2	displaying chemical structural formulae in epub format	vectorization;svg;e book conversion;chemical structure;epub;chemical structural formula	We describe one tool designed to enhance the visualization of chemical structural formulae in E-book readers. When dealing with small formulae, to avoid the pixelation effect with zoomed images, the formula is converted to a vectoral representation and then enlarged. On the opposite, large formulae are split in sub-images by cutting the image in suitable locations attempting to reduce the parts of the formula that are broken. In both cases the formulae are embedded in one ePub document that allows users to browse the chemical structure on most reading devices.	bitmap;browsing;e-book;epub;embedded system;pixelation	Simone Marinai;Stefano Quiriconi	2012		10.1145/2361354.2361382	computer science;scalable vector graphics;vectorization;chemical structure;algorithm	HCI	-39.94039830662612	-30.82010499653657	108970
24e7e3693f8c433434ae4ceb3487478ebf5de3fd	mri design review system: a mixed reality interactive design review system for architecture, serious games and engineering using game engines, standard software, a tablet computer and natural interfaces	web gl easy to use mixed reality interface mri user friendly virtual construction kit building information modeling bim digital prototyping 3d visualization serious games unity3d sketchup autodesk navisworks autodesk showcase unity3d trimble sketchup	Experience and control your design using natural interfaces! Most of todays conventional design review systems require special programming skills for preparation and high-capacity hardand software for demonstration. Interacting with 3D data sometimes can be complicated. Today we face five major problem fields using design review systems: Interaction with 3D data, navigation in 3D space, controlling design alternatives, design presentation using less extensive hardware, content development without special software and programming skills. Developments also targeting these issues by using different methods are presented e.g. by LANCELLE, SETTGAST and FELLNER (2008). They developed DAVE – Definitely Affordable Virtual Environment at Graz University of Technology. This immersive cage-based system today is used in evaluating the design of the new main railway station in Vienna, Austria. Also SHIRATUDDIN and THABET (2011) utilized the Torque 3D game engine to develop a Virtual Design Review System. Finally DUNSTON et. al. (2011) designed an Immersive Virtual Reality Mock-Up for Design Review of Hospital Patient Rooms. These and other research work was based on standard 3D game engines by using a conventional cave or power wall for presentation and physical immersion. The edddison MRI Design Review System is an easy to use mixed reality interface for design evaluation and presentation. It integrates a wide range of hardware input systems including a special 3D-printed tangible user interface, desktop computers, tablets and touch screens. On the software side it offers plug-ins for standard 3D software including Autodesk Navisworks and Showcase, Unity3D, Trimbles, SketchUp, Web GL and others. The edddison MRI Design Review System enables laymen to create their own interactive 3D content. It is a solution which makes the creation and presentation of interactive 3D applications as simple as preparing a powerpoint presentation. Without any programming skills you can easily manipulate 3D models within standard software applications. Control, change or adapt your design easily and interact with 3D models by natural interfaces and standard handheld devices. Navigate in 3D space using only your tablet computer. Complex buildings can be experienced by means of 2D floor plans and a touchscreen. System requirements are reduced by using standard software applications such as SketchUp or Unity3D. The edddison MRI Design Review System also makes it easy to present different design stages without extensive hardand software on all common mobile platforms. Actual application areas are Architectural Design, Digital Prototyping, Industrial Simulation, Serious Games and Product Presentation. Currently, the system has two major usecases: one setup will show the WebGL demo running on an iPad or an Android tablet computer. Using a WebGL/HTML5 cloud solution MRI Design Review System is able to reach the masses. The second demo is a SketchUp file controlled by optical tracking and 3D printed tangible objects also using a touchscreen or a handheld device. The edddison MRI Design Review System extends the range of existing design review systems with an easyto-use hardand software. Herein it simplifies the whole design process by an evolutionary, iterative approach, combined with a bunch of user-friendly intuitive interfaces. Demo Video URL: www.youtube.com/watch?v=CyC_TYdRSvo	3d computer graphics;3d floor plan;3d modeling;3d printing;android;desktop computer;game engine;html5;immersion (virtual reality);interactive design;iteration;mixed reality;mobile device;mock object;navisworks;requirement;simulation;sketchup;system requirements;tablet computer;tangible user interface;touchscreen;unity;usability;virtual reality;web content development;webgl;ipad	Andreas Behmel;Wolfgang Hohl;Thomas Kienzl	2014		10.1109/ISMAR.2014.6948472	simulation;human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-43.18230546834714	-32.88780148439813	109058
01c164577687ca4e5dc022f49e6613b73c306e46	xml-print. typesetting arbitrary xml documents in high quality			display resolution;xml	Lukas Georgieff;Marc Wilhelm Küster;Thomas Selig;Martin Sievers	2014			xml;database;computer science	DB	-43.74965956140848	-26.796421434475242	109079
268a7fc6961b698ff42751c3d8a8b94e9e88a8eb	varieties of computer graphics courses in computer science	computer graphic;graphics system;computer science education	The increased importance of graphics in computer systems has made computer graphics a more visible and important part of computer science education. This graphics education can take any of several forms. This panel describes four of these: the graphics service course for non-majors, the graphics systems course, the graphics concepts and algorithms course, and advanced or graduate courses in graphics. This panel is based on part of a workshop presented at SIGGRAPH '87.	algorithm;computer graphics;computer science;siggraph	Steve Cunningham;Judith R. Brown;Robert P. Burton;Mark Ohlson	1988		10.1145/52964.53041	computational science;scientific visualization;human–computer interaction;computer science;computer graphics lighting;graphics software;computer graphics;computer graphics (images)	Graphics	-47.758555116859156	-29.159043106453606	109178
7c4a7c56529e3e6768c95d05997fb6b1384233e9	techniques for interactive audience participation	control systems;onscreen game control;interactive audience participation;optical control;groupware;pointing device;competitive participation;beach ball batting;computer graphics;interactive entertainment system;interactive entertainment;testing;laser pointers;computer networks;computer vision;computer science education;optical tracking;shadow;cooperative participation;laser pointer;groupware computer vision entertainment optical tracking pointing systems computer graphics computer games;games;tracking computer vision control systems testing games optical control hardware computer networks computer science education buildings;shared entertainment experiences;computer games;pointing systems;off the shelf;entertainment;buildings;tracking;computer vision interactive entertainment system onscreen game control interactive audience participation competitive participation cooperative participation shared entertainment experiences beach ball batting shadow pointing device laser pointers;hardware	At SIGGRAPH in 1991, Loren and Rachel Carpenter unveiled an interactive entertainment system that allowed members of a large audience to control an onscreen game using red and green reflective paddles. In the spirit of this approach, we present a new set of techniques that enable members of an audience to participate, either cooperatively or competitively, in shared entertainment experiences. Our techniques allow audiences with hundreds of people to control onscreen activity by (1) leaning left and right in their seats, (2) batting a beach ball while its shadow is used as a pointing device, and (3) pointing laser pointers at the screen. All of these techniques can be implemented with inexpensive, off the shelf hardware. We have tested these techniques with a variety of audiences; in this paper we describe both the computer vision based implementation and the lessons we learned about designing effective content for interactive audience participation.	computer vision;experience;pointing device;siggraph	Dan Maynes-Aminzade;Randy F. Pausch;Steven M. Seitz	2002		10.1109/ICMI.2002.1166962	games;computer vision;shadow;entertainment;simulation;human–computer interaction;computer science;control system;tracking;multimedia;software testing;computer graphics;computer graphics (images);pointing device	HCI	-43.80263992203674	-35.494564566533796	109286
6fd70f797ab9bad027098af2b38b7188185237b5	overcoming the uncanny valley	motion pictures;humans motion pictures robots psychology eyes protection fuels automata animation cadaver;computer graphics;simulation;service robots;anthropometry computer graphics humans models anatomic models biological robotics user computer interface;robotics;visual artists;computer graphic;simulation animation computer graphics modeling robotics;human replicas;humanoid robots;roboticists;robots;animation;face;computer animation;human replicas computer graphics humanlike beings near human characters visual artists roboticists;modeling;tracking;near human characters;humanoid robots computer animation;humanlike beings	What makes some near-human characters scary while others are merely laughable? More important, why do some human and humanlike characters fail to arouse our sympathy? Visual artists and roboticists face these questions as they seek to alternately frighten and endear. Recent attempts to create accurate human replicas have brought these questions to the fore with increased urgency.	personality character;uncanny valley	Tom Geller	2008	IEEE Computer Graphics and Applications	10.1109/MCG.2008.79	robot;face;anime;computer vision;simulation;systems modeling;computer science;humanoid robot;artificial intelligence;computer animation;tracking;robotics;computer graphics;computer graphics (images)	Visualization	-39.67078321211006	-37.45045852267954	109465
6984176eee36d1603d35c3d398ca9bee8a7b361a	extended interface solutions for musical robotics	databases;software;groupware;instruments;hyperinstruments;real time control data feedback extended interface solutions musical robotics musical robots framework open ended control architecture mahadevibot esitar multiinput control interface bricktable open ended control interface multiuser interaction;real time control;multi user;tangible interface;esitar;bricktable;computer architecture;control architecture;robots;multimedia communication;mahadevibot;feedback control systems databases instruments robot control hardware fingers computer architecture software performance art;robots groupware music real time systems;music;tangible interfaces;multi touch interfaces;musical robotics;mahadevibot tangible interfaces multi touch interfaces hyperinstruments musical robotics esitar bricktable;real time systems	We present a framework for coupling musical robots with interfaces based on open-ended control architecture, allowing for new and expanded forms of expression. The MahaDeviBot allows for a single performer to simultaneously control up to 12 drums. The ESitar doubles both as a multi input control interface, and as a traditional sitar. Finally, the BricKTable represents a completely open-ended control interface possible of multi user interaction, and real-time control data feedback.	multi-user;nonlinear gameplay;real-time transcription;robot;robotics	Owen Vallis;Jordan Hochenbaum;Ajay Kapur	2008	2008 Tenth IEEE International Symposium on Multimedia	10.1109/ISM.2008.124	embedded system;human–computer interaction;computer science;artificial intelligence;music;multimedia	Robotics	-45.35308851496053	-35.831897144517065	109685
02996957ebf0f54dd877502b2843e910e113ca83	musicstrands™: a platform for discovering and exploring music		MusicStrandsTMhas launched a platform devoted to the discovery and exploration of music for the end-user. The services are freely accessible through our web site www. MusicStrands.com. The goal of this demonstration paper is to describe the main services offered by our proprietary technology which is based on Artificial Intelligence techniques to encode and predict musical users tastes. Our technology allows users to better enjoy music by facilitating the process of discovering and exploring new music.	artificial intelligence;encode;world wide web	Gunnar Holmberg;Marc Torrens	2005				AI	-47.13028560657122	-24.786395913847226	109727
b0ed28c9b3ce354d988e679b9f9d598de4d85f70	interactive music composition with a minimum of input states			interactivity	Justin R. Shuttleworth	1991			speech recognition;multimedia;communication	HCI	-47.78095094634336	-34.28604184591807	109867
a77f29a1071cc4a022811211ae456f7ab2e141fe	a platform to design and run dynamic virtual environments	virtual characters;natural scenes virtual reality computer animation;virtual reality;layout animation engines computational modeling multiagent systems character generation natural languages animals human robot interaction avatars;computer animation virtual scenes natural language virtual environment;natural language;virtual environment;computer animation;natural scenes;dynamic scenes	This paper is devoted to introducing, in a general fashion, a platform that allows users to generate virtual scenes. Such generation involves two phases: 1) description of the attributes, arrangement and intentions of the virtual characters participants throughout a natural language, and 2) a virtual representation of a dynamic scene during which the characters interact with each other in order to fulfill the specification provided by the modeler.	declarative programming;entity;extensible programming;genetic algorithm;high- and low-level;natural language;solver;virtual reality	H. Iván Piza;Fabiel Zúñiga;Félix F. Ramos Corchado	2004	2004 International Conference on Cyberworlds	10.1109/CW.2004.9	computer vision;computer science;virtual machine;instructional simulation;interactive skeleton-driven simulation;virtual reality;computer animation;multimedia;natural language;computer graphics (images)	Visualization	-38.926089366587306	-36.222781685241635	110095
476698a5d48153aa95e6f8aea4d5600121ef3bee	object-process based graphics recognition class library: principles and applications	null		graphics;library (computing)	Wenyin Liu;Dov Dori	1999	Softw., Pract. Exper.	10.1002/(SICI)1097-024X(19991225)29:15%3C1355::AID-SPE285%3E3.0.CO;2-B	vector graphics;2d computer graphics;computer graphics metafile;computer science;object-oriented design;real-time computer graphics;graphics software;programming language;computer graphics;object-oriented programming;3d computer graphics;computer graphics (images)	SE	-46.63070397007657	-29.521495423023286	110372
8f91f02392ff735c1965f23d101f6464327d72dc	head first web design - a brain-friendly guide	web design			Ethan Watrall;Jeff Siarto	2009			web service;web development;data web;web design;web navigation;web page;world wide web;web design program	ECom	-43.56329772236673	-24.007545182048986	110487
2421ff8beb40468f46cd36a4209156ecd04344cd	conversational user interfaces	remote control;service system;learning curve;user interface;language technology;web service;mobile phone;online help;facial expression	Self-service systems, online help systems, web services, mobile communication devices, remote control systems, and dashboard computers are providing ever more functionality. However, along with greater functionality, the user must also come to terms with the greater complexity and a steeper learning curve. This complexity is compounded by the sheer proliferation of different systems lacking a standard user interface. Conversational user interfaces allow various natural communication modes like speech, gestures and facial expressions for input as well as output and exploit the context in which an input is used to compute its meaning. The growing emphasis on conversational user interfaces is fundamentally inspired by the aim to support natural, flexible, efficient and powerfully expressive means of human-computer communication that are easy to learn and use. Advances in human language technology and intelligent user interfaces offer the promise of pervasive access to online information and web services. The development of conversational user interfaces allows the average person to interact with computers anytime and anywhere without special skills or training, using such common devices as a mobile phone. Advanced conversational user interfaces include the situated understanding of possibly imprecise, ambiguous or incomplete multimodal input and the generation of coordinated, cohesive, and coherent multimodal presentations. In conversational user interfaces the dialogue management is based on representing, reasoning, and exploiting models of the user, domain, task, context, and modalities. These systems are capable of real-time dialogue processing, including flexible multimodal turn-taking, backchanneling, and metacommunicative interaction. One important aspect of conversations is that the successive utterances of which it consists are often interconnected by cross references of various sorts. For instance, one utterance will use a pronoun to refer to something mentioned in the previous utterance. Computational models of discourse must be able to represent, compute and resolve such cross references. Conversational user interfaces differ in the degree with which the user or the system controls the conversation. In directed or menubased dialogues the system maintains tight control and the human is highly restricted in his dialogue behavior, whereas in free-form dialogue the human takes complete control and the system is totally passive. In mixed-initiative conversational user interfaces, the dialogue control moves back and forth between the system and the user like in most face-to-face conversations between humans. Four papers in this special issue deal with conversational user interfaces that use speech as the main mode of interaction. The paper by Helbig and Schindler discusses state-of-art component technologies and requirements for the successful deployment of conversational user interfaces in industrial environments such as logistics centers, assembly lines, and car inspection facilities. It shows that the speech recognition rate in such environments is still depending on the correct positioning and adjustment of the microphone and discusses the need for wireless microphones in most industrial applications of spoken dialogue systems. Block, Caspari and Schachtl describe an innovative dialogue engine for the Virtual Call Center Agent (ViCA), that provides access to product documentation. A multiframe based dialogue engine is introduced that supports natural conversations by allowing over-answering and free-order information input. The paper reports encouraging results from a usability test showing a high task completion rate. The paper by te Vrugt and Portele describes a tasked-oriented spoken dialogue system that allows the user to control a wide spectrum of infotainment applications, like a hard-disk recorder, an image browser, a music player, a TV set and an electronic program guide. The paper presents a flexible framework for such a multi-application dialogue system and an applicationindependent scheme for dialogue processing. Nöth et al. describe lessons learnt from the implementation of three commercially deployed conversational interfaces. The authors propose five guidelines, which they consider to be crucial, when building and operating telephone-based dialogue systems. One of the guidelines concerns the fact that a spoken dialogue system must react fast to any kind of user input, no matter	anytime algorithm;coherence (physics);computation;computational model;computer;control flow;control system;cross-reference;dos;dialog system;documentation;grace helbig;hard disk drive;image viewer;intelligent user interface;language technology;logistics;microphone;mobile phone;multimodal interaction;pervasive informatics;real-time locating system;remote control;requirement;situated;software deployment;speech recognition;spoken dialog systems;usability testing;web service	Wolfgang Wahlster	2004	it - Information Technology	10.1524/itit.46.6.289.54685	user interface design;web service;human–computer interaction;telecommunications;computer science;operating system;multimedia;user interface;language technology;learning curve;world wide web;facial expression;remote control;service system	HCI	-47.74137709382324	-37.33665071645799	110569
1363f692621ee81fd61548612c7068fec81474fb	processing.js: sketching with <canvas>	short term memory;web pages;image space;data visualization;gaze direction;web technology;open source	The Processing language [Casey Reas 2007], first introduced by Ben Fry and Casey Reas in 2001, is a simple, elegant language for data visualization that is already being used by artists, educators and commercial media groups to produce rich graphical content called sketches. Because Processing is implemented in Java, delivering Processing sketches via a web page requires the user to install a Java plug-in. Processing.js, in comparison, is an open source, cross browser JavaScript port of the Processing language; it translate Processing sketches into JavaScript using the <canvas> element for rendering. No additional plug-ins are required to view a Processing sketch delivered with Processing.js. Furthermore, Processing.js is much more than just a Processing parser written in JavaScript: it enables the embedding of other web technologies into Processing sketches and vice versa. Processing.js seamlessly integrates web technologies with the Processing language to provide an excellent framework for rich multimedia web applications.	canvas element;data visualization;graphical user interface;java;javascript;open-source software;plug-in (computing);processing.js;web application;web page	Andor Salga;Daniel Hodgin;Anna Sobiepanek;Scott Downe;Mickael Medel;Catherine Leung	2011		10.1145/2037826.2037846	computer vision;static web page;real-time computing;rich internet application;web-based simulation;computer science;artificial intelligence;processing;operating system;dynamic web page;web page;database;short-term memory;multimedia;programming language;world wide web;data visualization;computer graphics (images)	Web+IR	-43.7059836179035	-31.02874158486413	110709
a0e2a50469c12547c30caebd8d26f9d346b1f3b0	preparation for interactive live computer performance in collaboration with a symphony orchestra	point of view	This paper describes the design, implementation, and use of an interactive computer-based instrument in the context of a composition for orchestra and live electronics. We will begin with an overview of the piece from a compositional point of view, emphasizing the musical goals for the electronics aspect. Then we will discuss the extremely demanding technical requirements of integrating live electronics with a full orchestra. We will describe the interactive instrument in detail, emphasizing the novel mapping of performer's gestures to computer-generated sound events and the novel OpenSound Control-based structure of the software.	computer performance;computer-generated holography;interactivity;requirement;symphony	Timothy Madden;Ronald Bruce Sith;Matthew Wright;David Wessel	2001			simulation;human–computer interaction;computer science;multimedia	HCI	-46.827708219303716	-34.098430138270054	110916
5087129598ca79daf91bec5f81869d033085171b	visual programming and the blind: the challenge and the opportunity	blind programmers;visual programming;graphical user interfaces;visual basic;graphic user interface;scripting language	"""The proliferation of graphical user interfaces has had a dramatic impact on the ability to work as a programmer. It is particularly difficult for the blind to create forms for visual programming applications, such as Visual Basic. A scripting language is introduced that enables the blind to create Visual Basic forms without needing to specify a great deal of detail and without needing the """"point and click"""" approach that they cannot use. Related issues of accommodating the blind in a computer science course are also discussed."""	computer science;graphical user interface;point and click;programmer;scripting language;visual basic;visual programming language	Robert M. Siegfried	2006		10.1145/1121341.1121427	human–computer interaction;computer science;graphical user interface;multimedia;visual programming language;programming language	HCI	-42.596382408237545	-30.51771817838313	111283
308c95090a0ecb32536d5d58d064c290933b3e41	task-level assembly modeling in virtual environments	user interface;direct manipulation;real time;virtual reality;natural language;virtual environment;knowledge base	"""This contribution introduces a new framework for assembly modeling in Virtual Reality. Aiming at an easy instructability of the virtual environment, the framework provides a task-level interface which directly maps logical assembly commands to corresponding changes in the geometry scene. For example, the visual assembly of two parts is achieved given only a single command 'connect(a,b)'. This is in contrast to the assembly modeling style of conventional CAD systems which forces the designer to break down each conceptual assembly task into a series of lower-level subtasks. The proposed framework consists of two parts: (1) A knowledge-based model of connection-sensitive part features (\ports"""") and the connections between them; and, (2), a set of algorithms that dene the task-level interface for assembly, disassembly, and adjustment operations. All algorithms are computationally eÆcient and easily meet the real-time requirements of virtual environments. At the user interface, both direct manipulation and directive interfaces, e.g. based on natural language instructions are supported. A family of implemented VR-systems, including CAVE and Internet-based applications, demonstrates the feasibility of the approach."""	algorithm;assembly language;assembly modelling;computer-aided design;direct manipulation interface;directive (programming);disassembler;emoticon;internet;map;natural language;real-time clock;requirement;user interface;virtual reality	Bernhard Jung	2003		10.1007/3-540-44842-X_73	embedded system;knowledge base;simulation;human–computer interaction;computer science;virtual machine;artificial intelligence;operating system;virtual reality;natural language;user interface	Robotics	-38.94870675891057	-33.15067782550746	111298
5b66648271848574224b529c416589974933b87b	a broadband multimedia telelearning system	courseware user site;multimedia interactive courseware;real time interactive system;information systems;document model;communications;learning;courseware document models;database management systems;information retrieval;real time;code standards;multimedia systems;online facilitator site;atm networks;authoring systems;computer networks;media production center;multimedia computing;broadband multimedia telelearning system;telecommunication;multimedia communication;information processing;multimedia information processing;integrated services digital networks;b isdn;mheg based model;system architecture;university of ottawa;distributed data processing;courseware;interactive systems;real time systems b isdn multimedia communication multimedia computing courseware authoring systems database management systems asynchronous transfer mode interactive systems code standards;broadband communication;courseware database;multimedia information research laboratory;courseware document models broadband multimedia telelearning system multimedia information research laboratory university of ottawa multimedia information processing telecommunication media production center courseware author site courseware database courseware user site online facilitator site atm network multimedia interactive courseware mheg based model real time interactive system system architecture;atm network;asynchronous transfer mode;courseware author site;real time systems;mathematics computers information science management law miscellaneous	In this paper we discuss a broadband multimedia TeleLearning system under development in the Multimedia Information Research Laboratmy at the University of Ottawa. The system aims at providing a seamless environment for TeleLearning using the latest telecommunication and multimedia information processing technology. It basically consists of a media production center, a courseware author site, a courseware database, a courseware user site, and an on-line facilitator site. All these components are distributed over an ATM network and work together to offer a multimedia interactive courseware service. An MHEG-based model is exploited in designing the system architecture to achieve the real-time, interactive, and reusable information interchange through heterogeneous platforms. The system architecture, courseware processing strategies, courseware document models are presented	atm turbo;information processing;online and offline;real-time clock;seamless3d;systems architecture	Ruiping Wang;Ahmed Karmouch	1996		10.1109/HPDC.1996.546182	human–computer interaction;information processing;computer science;asynchronous transfer mode;multimedia;world wide web;information system	HPC	-47.020189009917836	-25.978574342921554	111359
cded7aa607e7b34bfa79f27e4914b6d5c494fd04	nvr: a system for networked virtual reality	virtual reality;client server networked virtual reality nvr client server network architecture software toolkit virtual reality;multimedia systems virtual reality software tools;multimedia systems;client server;networked virtual reality;software tools;network architecture;virtual worlds;virtual reality software tools multimedia computing	Describes the design, implementation, and analysis of the NVR system. NVR is a software toolkit that facilitates the creation of networked virtual worlds. It uses the client-server network architecture, operates on low-end computer hardware, and allows for networking over the Internet. Design choices are described and explained. Techniques used by NVR that address problems inherent to networked virtual reality are presented. Other features of NVR are also described, such as NVR's use of objects, server-moderated locks, local objects, and local changes. Observed performance of the system is then discussed. Finally, conclusions drawn from analysis of the system and suggestions for future work are presented. >	network video recorder;virtual reality	Joseph E. Berger;Loan T. Dinh;Michael F. Masiello;Jesse N. Schell	1994		10.1109/MMCS.1994.292431	network architecture;computer science;operating system;virtual reality;multimedia;world wide web;client–server model;computer network	Visualization	-46.45217465825823	-26.21229718399211	111632
b151a5b49f7922b7c12d348fdb74e1dfc60de177	the listearn experience	listearn experience	Abstract This document briefly describes the LISTEARN server, its file storage capabilities, mail distribution via DIST2 and the extended encoding specifications. Examples for each section are added for clarity.		Turgut Kalfaoglu	1990	Computer Networks and ISDN Systems	10.1016/0169-7552(90)90092-7	computer science;database;multimedia;world wide web	OS	-46.73546822508232	-25.212896786158023	111757
e2a944f0c18410355b15181dfdef3a0089ed0571	math-based introduction to 3d graphics			3d computer graphics		2003	IEEE Computer	10.1109/MC.2003.10056		Vision	-47.119973029611344	-29.638761312459216	111921
dcd1046874c309b8368e61d0a2f5bd976b48b153	chuck: a strongly timed computer music language		ChucK is a programming language designed for computer music. It aims to be expressive and straightforward to read and write with respect to time and concurrency, and to provide a platform for precise audio synthesis and analysis and for rapid experimentation in computer music. In particular, ChucK defines the notion of a strongly timed audio programming language, comprising a versatile time-based programming model that allows programmers to flexibly and precisely control the flow of time in code and use the keyword now as a time-aware control construct, and gives programmers the ability to use the timing mechanism to realize sample-accurate concurrent programming. Several case studies are presented that illustrate the workings, properties, and personality of the language. We also discuss applications of ChucK in laptop orchestras, computer music pedagogy, and mobile music instruments. Properties and affordances of the language and its future directions are outlined.	apl;chuck;concurrency (computer science);concurrent computing;control flow;experiment;laptop;list of audio programming languages;programmer;programming language;programming model	Ge Wang;Perry R. Cook;Spencer Salazar	2015	Computer Music Journal	10.1162/COMJ_a_00324	first-generation programming language;simulation;programming domain;computer science;operating system;programming language;world wide web;high-level programming language	PL	-46.40675505473982	-35.60323518909813	111980
a3feedb4f9189d6d7c3be7a7a7ec530a7567df55	unterstützung des kooperativen wissenserwerbs durch hypervideo-inhalte		This works covers the problem of supporting collaborative knowledge construction on the basis of hypervideo content. Thereby, hypervideo is defined as video based hypermedia that combines non-linear information structures with dynamic audio-visual information presentations. Video as the primary content type for hypervideo has become a media of increasing importance in past years. Thereby, videos can constitute an origin for communicating complex and dynamic visual information in an intuitive and effective fashion. This makes video an ideal content for knowledge construction within distributed environments. The concept of collaborative hypervideo enhanced the advantage of video in terms of group based discussion and/or conversation. With collaborative hypervideo group members can easily share their ideas and views with others and so establish a knowledge transfer among each other. This is achieved by the introduction of shared video annotations, in which users can accentuate objects out of the video context and combined these with further multimedia content or other video objects. In such a way a non-linear information structure will be created that can be easily applied for information access. The conceptual part of this work is based on an interaction model, a reference architecture and a data model for collaborative hypervideo. The model defines a user interface concept in terms of the hypervideo visualisation and interaction within a collaborative environment. The reference architecture provides a logical view of an abstract system. This architecture contributes to concrete system realisations within different application areas. The data model defines a core concept of how elements of a collaborative hypervideo structure like nodes, links, anchors and metadata sets, are organized in an effective manner. The general concept of collaborative hypervideo has been validated by means of four empirical field studies. The results gave great evidence for the conceptual work fulfilled and proofed that with collaborative hypervideo the construction of knowledge within a group based scenario is achieved.	data model;hypermedia;hypervideo;information access;nonlinear system;reference architecture;user interface	Matthias Finke	2005			hypervideo;multimedia;computer science	HCI	-45.50252762597521	-26.234736093053122	112077
8af3d769f5e5c3e24df909f7bf06c71860e81bd2	experiences from real-world deployment of context-aware technologies in a hospital environment	sensibilidad contexto;modelizacion;distributed system;context aware application;context aware computing;unfolding;interfaz grafica;pistage;systeme reparti;context aware;location tracking;informatique mobile;context information;deploiement;graphical interface;telephone portable;pervasive computing;localization;despliegue;rastreo;hospital environment;localizacion;mobile phone;informatica difusa;modelisation;telefono movil;sistema repartido;localisation;informatique diffuse;milieu hospitalier;medio hospitalario;ubiquitous computing;sensibilite contexte;mobile computing;modeling;interface graphique;tracking	Context-aware computing is a central concept in ubiquitous computing and many suggestions for context-aware technologies and applications have been proposed. There is, however, little evidence on how these concepts and technologies play out in a real-world setting. In this paper we describe and discuss our experiences from an ongoing deployment of a suite of context-aware technologies and applications in a hospital environment, including a context-awareness infrastructure, a location tracking system, and two context-aware applications running on interactive wall displays and mobile phones. Based on an analysis of the use of these systems, we observe that many of the ideas behind context-aware computing are valid, and that the context-aware applications are useful for clinicians in their work. By reflecting on the nature of the designed context-aware technologies, we present a model which states that the triggering of context-awareness actions depend upon the accuracy of the sensed context information, the degree to which you know which action to perform in a given situation, and the consequence of performing the action.	context awareness;mobile phone;privacy;software deployment;tracking system;ubiquitous computing	Jakob Eyvind Bardram;Thomas Riisgaard Hansen;Martin Mogensen;Mads Søgaard	2006		10.1007/11853565_22	embedded system;simulation;systems modeling;internationalization and localization;human–computer interaction;computer science;operating system;graphical user interface;tracking;computer security;ubiquitous computing	HCI	-35.65702874547921	-25.172544955355622	112211
854096d4a4d1647efc5b88ff9ac5a41cf38df182	real-time animated grass		We present a simple method to render fields of grass, animated in the wind, in real time. The technique employs vertex shaders to render displacement maps with Russian doll style transparent shells. Animation is achieved by translating the surface according to a local wind vector while preserving the length of the blades of grass. This technique achieves convincing results on current consumer graphics hardware and can be applied to other similar surfaces such as hair and fur. I.3.7Computer GraphicsThree-Dimensional Graphics and Realism	columbia (supercomputer);displacement mapping;global illumination;graphics hardware;library (computing);map;pc game;real-time transcription;shader;sorting;volume rendering	Brook Bakay;Paul Lalonde;Wolfgang Heidrich	2002		10.2312/egs.20021030		Graphics	-47.052034655299586	-29.583032405533096	112305
e8c8fe15373dcea6f293986c5d379aa07c74199a	digital fukuwarai: a new game concept using live video	user interface;video processing;game;high performance	We have developed a new game system called “Digital Fukuwarai” by using high performance video processing technology. It provides a very impressive experience and novel interactivity which has never been achieved by any existing games.	interactivity;video processing	Hiroshi Matoba	1998		10.1145/286498.286661	first playable demo;game design;game development tool;games;simulation;bink video;level design;computer science;video capture;game art design;multimedia;video processing;d-pad;game design document;user interface;video game development;game programming;computer graphics (images)	OS	-47.96952131314387	-30.208476230186626	112320
86ba98dd46d46e3fbe1f4f144c146db841c5fc41	an overview of oracle chart builder and map viewer	map viewer;oracle chart builder	Oracle Chart Builder is a real-time Java charting API. It enables more effective communication and analysis of information for business graphics and performance applications. MapViewer is a programmable tool for rendering maps using spatial data managed by Oracle Spatial. MapViewer provides tools that hide the complexity of spatial data queries and cartographic rendering, while providing customizable options for more advanced users. This paper presents an overview of data visualization features in the 9i Application Server product from Oracle.		Lory D. Molesky;Jayant Sharma	2002			data mining;information retrieval;computer graphics (images)	ML	-42.713993139551484	-27.834304179343683	112363
4083ae4e04a307b4df5b5dceb6c12e4cf5a6bf38	integrating multimedia technology, knowledge based system and speech processing for the diagnostic and treatment of developmental dyslexia	base donnee;architecture systeme;knowledge based system;multimedia;intelligent multimedia and hypermedia systems;case based reasoning systems;reconocimiento palabra;systeme multimedia;teaching strategies;speech processing;database;tratamiento palabra;traitement parole;base dato;base connaissance;multimedia systems;speech recognition;base conocimiento;arquitectura sistema;enseignement;reconnaissance parole;system architecture;teaching;knowledge base;developmental dyslexia;ensenanza	SICOLE is a research effort to develop a software environment to help tutors of dyslexic children with the diagnostic and treatment tasks. This paper describes the architecture of the package already implemented where three important elements interact: a multimedia interface, an inference module and a database. This architecture provides the system with the flexibility to support a large variety of tasks, dynamic presentations, and complex teaching strategies. The application of speech recognition technology is also researched as an important part of the evaluation of the dyslexic children improvements. This package is being used at the present moment in several Spanish Schools as part of its validation process.	knowledge-based systems;speech processing	Lorenzo Moreno;Carina S. González-González;Vanessa Muñoz;J. I. Estévez;Rosa María Aguilar;José L. Sánchez;José F. Sigut;José D. Piñeiro	2002		10.1007/3-540-47987-2_21	knowledge base;simulation;speech recognition;computer science;artificial intelligence;operating system;speech processing;database;programming language;algorithm	DB	-37.45589313251989	-26.086148910125022	112598
300d59d3a2540882f6c3c093735fb3370796cc40	kid's programming language (kpl)	computer program;programming language;graphical programming;integrated development environment;computer science education;game programming;games;beginning programming;2d graphics;is education;3d graphics;age groups;object model	"""In this paper, we introduce Kid's Programming Language, or KPL. KPL is an integrated development environment (IDE) and programming language which are similar to but greatly simplified from current mainstream IDEs and languages. KPL is educational freeware. KPL was initially targeted at the 10-14 age group, but has proven to be engaging and interesting to beginning programmers and hobbyists of all ages. KPL offers a highly leveraged object model which emphasizes graphics programming, including 2D and 3D graphics. KPL intends to address the problem of declining computer science interest and enrollment by 1) making it easy for beginners to get started with computer programming, 2) capturing and holding beginners' interest by emphasizing graphics and games programming and 3) enabling a smooth """"graduation"""" from KPL into mainstream languages and IDEs."""	3d computer graphics;computer programming;computer science;game programming;integrated development environment;programmer;programming language;s (programming language)	Jon Schwartz;Jonah Stagner;Walt Morrison	2006		10.1145/1179295.1179348	games;first-generation programming language;2d computer graphics;real-time computing;very high-level programming language;object model;programming domain;computer science;artificial intelligence;operating system;functional logic programming;programming paradigm;visual programming language;programming language;game programming;demographic profile;3d computer graphics;computer graphics (images)	PL	-48.120409541318274	-28.365437609498674	112633
db81395b3f0de6e96462434592c3d9e8d758f6ad	visualization of impulse reponse of virtual accoustic environments.				Adam J. Sporka;Pavel Slavík	2000			computer vision;simulation;computer graphics (images)	HPC	-46.92603766034426	-31.2816545892815	112715
556bca82121123651d5dfdf75c21bb27e4c3e932	videograph: a new tool for video mining and classification	video library;multimedia authoring;digital video editing;fast algorithm;silver;informedia	This paper introduces Videograph, a new tool for video mining and visu alizing the structure of the plot of a video sequence. The main idea is to &lquo;stitch&rquo; together similar scenes which are apart in time. We give a fast algorithm to do stitching and we show case studies, where our approach (a) gives good features for classification (91\% accuracy), and (b) results in Videographs which reveal the logical structure of the plot of the video clips.	algorithm;emoticon;image stitching;video clip	Jia-Yu Pan;Christos Faloutsos	2001		10.1145/379437.379462	video compression picture types;computer science;silver;video tracking;multimedia;video processing;world wide web;computer graphics (images);non-linear editing system	ML	-43.842309631422744	-32.26518879610357	112722
bcae179b37ed16b129c6a9cfcc20ed4fda715e20	development of speech design tool 'sesign99' to enhance synthesized speech		This paper introduces a new speech design tool (Sesign99) that can convert monotonous synthesized speech into a variety of speech styles. As multimedia services such as games, interactive movies and WWW home pages become more popular, more attention is being focused on the creation, management, and transmission of speech messages. Although speech synthesis-by-rule has improved with recent advances in TTS, the monotonous features of speech produced by synthesis-by-rule hamper the introduction of TTS to the application areas listed above. There is demand for software that allows even the novice to produce engaging and natural-sounding speech messages.	automatic sounding;design tool;speech synthesis;www	Hideyuki Mizuno;Masanobu Abe;Shin'ya Nakajima	1999			design tool;speech recognition;computer science	HCI	-46.942760198571825	-35.04828197224118	112750
c5e2488ab328d7648e23d8c2a0369e041105a8dc	a procedural geometry modeling api		This paper presents a solution for geometric manipulation in procedural modeling as an Application Programming Interface (API). This approach intends to enable a more powerful control over the geometric entities by performing selections based on their attributes, similar to picking features in graphical interfaces. This is achieved through the definition of a topological structure, which features a set of properties, such as scope, spatial localization and semantic information. The applicable modeling operations allow a more customized control, as well as successive tracking, which induce a greater, faster and more intuitive approach for geometry generation. This approach still constitutes ongoing work, but has already been successfully applied for the generation of large virtual urban environments.	application programming interface;blu-ray;entity;geometric modeling;graphical user interface;international symposium on fundamentals of computation theory;mass effect trilogy;procedural modeling;real-time clock;spatial–temporal reasoning;usability;wiki	Pedro Brandão Silva;António Coelho;Rui Amaral Rodrigues;António Augusto de Sousa	2012			computer vision;artificial intelligence;computer science;computer graphics (images)	Graphics	-37.701993518267614	-32.38085899530672	113006
e91932cea7a8ae2400fac27468c2360ef6e3c3f6	3d modeling and simulation of human activities in smart spaces	sensor phenomena and characterization;longitudinal simulation data 3d modeling human activities simulation smart spaces smart home researchers activity recognition algorithms sensory data persim 3d 3d graphical user interface spatial perception virtual character virtual environment;intelligent sensors solid modeling humans sensor phenomena and characterization data models context modeling;virtual reality;sensor based simulation;graphical user interfaces;solid modeling;virtual reality digital simulation graphical user interfaces home automation solid modelling ubiquitous computing;ubiquitous computing;simulation toolkit;humans;pervasive space;context modeling;intelligent sensors;digital simulation;solid modelling;data models;home automation;sensor based simulation ubiquitous computing pervasive space activity recognition simulation toolkit;activity recognition	For smart home researchers, it is essential to test activity recognition algorithms with various sets of sensory data. However, diverse sensory datasets are not always available due to several constraints, including limited budgets. Consequently, smart home simulators have recently grown in importance. However, there is still a need for realistic synthetic sensory data. This paper presents a simulator, 'Persim 3D', which relies on automatic scenario generation to create realistic sensory data. Persim 3D provides a 3D graphical user interface to help users' spatial perception, sensors that operate in real-time, similar to actual sensors, and a virtual character that lives in a virtual environment. These features allow users to generate longitudinal simulation data and eventually contribute to activity recognition research.	3d modeling;3d printing;activity recognition;algorithm;graphical user interface;home automation;real-time locating system;sensor;simulation;spaces;synthetic intelligence;virtual reality	Abdelsalam Helal;Kyungeun Cho;Wonsik Lee;Yunsick Sung;Jaewoong Lee;Eunju Kim	2012	2012 9th International Conference on Ubiquitous Intelligence and Computing and 9th International Conference on Autonomic and Trusted Computing	10.1109/UIC-ATC.2012.35	embedded system;data modeling;home automation;simulation;human–computer interaction;computer science;operating system;graphical user interface;virtual reality;context model;solid modeling;ubiquitous computing;intelligent sensor;activity recognition	Robotics	-35.648012828863	-37.50429607581761	113045
cde30c50db522f571a2fac6b1cf3044e5cdb356f	the clim prototyping environment (cpe)	clim prototyping environment	Introduction The CLIM Prototyping environment (CPE) is an interactive graphical object manipulation environment developed at the Institute for the Learning ‘sciences. The system functions as a user interface management system (UIMS) and can be used as a rnntime environment for arbitrary Common Lisp programs. In addition, interfaces may be created within CPE as part of a rmntime environment with minimal or no programming, This approach encourages a great deal of high level user interaction with the program and facilitates rapid prototyping. In addition end users are able to easily create multiple interfaces for a single program. The line between creating and using an interface has thereby been minimised. To date CPE has been used in a number of capacities including: . a stand-alone environment for schematic design, ● a module in the interactive graphic user interfaces for the Feedback Minilab (FML) [2], SIMGEN [4], the Qualitative Process Envisioned (QPE] [3], and the Structure Mapping Engine (SME)[l] ● a design environment for building stand-aIone user interfaces for the Simgen runtime system. CPE is written in Common Lisp with the Common Lisp Intefiace Manager (CLIM) as a base.	common lisp;constant phase element;face modeling language;graphical user interface;high-level programming language;minilab;prototype;rapid prototyping;runtime system;schematic;software prototyping;structure mapping engine;user interface management systems	Greg Siegle	1993		10.1145/259964.260041	human–computer interaction;computer science	HCI	-42.694182225479246	-32.42084635937867	113057
5193d5e20e0d403c6ae76ba43b274545ca9773c0	a modular architecture for an interactive real-time simulation and training environment for satellite on-orbit servicing	tele robotics;robot tele operation interactive real time simulation modular architecture real time interactive application on orbit servicing task programming virtual reality environment realistic dynamic behavior realistic kinematic behavior satellite components bimanual haptic interface;computer model;real time;training;virtual reality;maintenance engineering;real time simulation;physics simulation;haptics;tele robotics real time applications immersive virtual reality physics simulation haptics;computational modeling;interactive application;robotersysteme;virtual reality aerospace robotics artificial satellites haptic interfaces maintenance engineering real time systems telerobotics;satellites;solid modeling;aerospace robotics;robots;immersive virtual reality;mechatronische komponenten und systeme;artificial satellites;institut fur robotik und mechatronik ab 2013;telerobotics;virtual reality environment;real time applications;satellites robots haptic interfaces solid modeling real time systems computational modeling training;haptic interfaces;real time application;modular architecture;physical simulation;real time systems;haptic interface	This paper outlines the development of a real-time interactive application for the analysis, training and programming of on-orbit servicing tasks within a virtual reality environment. The main challenges put on the system are the real-time simulation of the realistic dynamic and kinematic behavior of satellite components and additionally integrate interaction through a bimanual haptic interface, as well as enable tele-operation of a robot. We give an overview of the application, describe the real-time challenges and outline our approach and proposed system structure.	haptic technology;real-time clock;real-time computing;real-time transcription;simulation;television;virtual reality	Robin Wolff;Carsten Preusche;Andreas Gerndt	2011	2011 IEEE/ACM 15th International Symposium on Distributed Simulation and Real Time Applications	10.1109/DS-RT.2011.23	simulation;human–computer interaction;engineering;computer graphics (images)	Embedded	-36.297140624458464	-37.90665181092721	113229
0bae4be1e197e7ca5286d8a02c1e4a81414779ce	exploring european cultural heritage using conversational agents		The semantic web and open data paradigms are gaining momentum in recent years and more information is being published online following the linked data principles. This enables easy access and processing of data by external services. An example of such services are intelligent conversational agents that provide to the users the ability to interact with a computer system in natural language. Such communication is much more intuitive and facilitates the use of complex services to less skilled users (e.g., elderly) or users with disabilities (e.g., visually impaired) thus providing to these groups access to the huge amount of information stored in the semantic web or specific online services. In this paper, we present a proof-of-concept conversational agent able to provide information about the European cultural heritage and display stored digital content from the Europeana database.		Octavian Machidon;Ales Tavcar	2018		10.1007/978-3-030-05819-7_14	open data;linked data;world wide web;natural language;semantic web;digital content;cultural heritage;computer science;dialog system	NLP	-42.08813600008357	-24.07536120550789	113248
da7fa7bbd842d082223237be86c2ac7fe9e70185	constraint-aware interior layout exploration for pre-cast concrete-based buildings	ucl;technology;interior layout;discovery;constraint aware form exploration;floorplans;theses;conference proceedings;software engineering;functional layouts;digital web resources;science technology;computational design;pre cast concrete;ucl discovery;open access;ucl library;computer science;book chapters;open access repository;ucl research	Creating desirable layouts of building interiors is a complex task as designers have to manually adhere to various local and global considerations arising from competing practical and design considerations. In this work, we present an interactive design tool to create desirable floorplans by computationally conforming to such design constraints. Specifically, we support three types of constraints: (i) functional constraints such as number of rooms, connectivity among the rooms, target room areas, etc., (ii) design considerations such as user modifications and preferences, and (iii) fabrication constraints such as cost and convenience of manufacturing. Based on user specifications, our system automatically generates multiple floor layouts with associated 3D geometry that all satisfy the design specifications and constraints, thus exposing only the desirable family of interior layouts to the user. In this work, we focus on pre-cast concrete-based constructions, which lead to interesting discrete and continuous optimization possibilities. We test our framework on a range of complex real-world specifications and demonstrate the control and expressiveness of the exposed design space relieving the users of the task of manually adhering to non-local functional and fabrication constraints.	constrained optimization;continuous optimization;design tool;interactive design;interactivity;mathematical optimization;software design;usability testing;user (computing)	Han Liu;Yong-Liang Yang;Sawsan AlHalawani;Niloy Jyoti Mitra	2013	The Visual Computer	10.1007/s00371-013-0825-1	simulation;computer science;artificial intelligence;algorithm;computer graphics (images);technology;mechanical engineering	HCI	-37.32386398305708	-34.1317458844209	113287
0cd441fac1ecc0eedd782e59420fb92d33e54cc6	strawbies: explorations in tangible programming	games;children;tangibles;programming;strawberries	In this demo we present Strawbies, a realtime tangible programming game designed for children ages 5 to 10. Strawbies is played by constructing physical programs out of wooden tiles in front of an iPad. This interaction is made possible with the use of an Osmo play system that includes a mirror to reflect images in front of the iPad through the front-facing camera. We combined this system with the TopCodes computer vision library for fast and reliable image recognition. Here we describe a set of principles that guided our iterative design process along with an overview of testing sessions with children that informed our most recent instantiation of Strawbies.	computer vision;iterative design;iterative method;osmo;programming game;universal instantiation;ipad	Felix Hu;Ariel Zekelman;Michael Horn;Frances Judd	2015		10.1145/2771839.2771866	simulation;computer science;multimedia;computer graphics (images)	HCI	-44.13790073473032	-36.026942517484066	113301
c9a94163e1ddf0cff4aa13b78dbe32aab940f6f7	mpeg analyzer a tool for visualizing mpeg encoding characteristics	video streaming;visualization;mpeg 2;encoding characteristics;open source	We present an open source tool called MPEG analyzer that is able to visualize various MPEG-2 encoding characteristics while playing a video stream. Our tool has been developed during a student lab and is intended to support teachers and lecturers in teaching the functioning of MPEG-2 encoding techniques.	alloy analyzer;mpeg-2;moving picture experts group;open-source software;streaming media	Matthias Dick;Sven Lahde;Lars C. Wolf	2008		10.1145/1496046.1496078	visualization;computer hardware;telecommunications;computer science;multimedia;mpeg-2;computer graphics (images)	SE	-44.239034180693906	-28.109753733198403	113309
70ead378c0e280b6aa3a6a1efd7970a85c58cd4d	uncovering the to-dos hidden in your in-box	graphical interface;unstructured information management architecture;machine learning;evaluation studies;rule based reasoning	D. M. Sow J. S. Davis II M. R. Ebling A. Misra L. Bergman In this paper we present SCOUT, an application that examines the machine-generated messages within the in-box of an e-mail application, extracts from these messages information regarding the tasks the recipient is asked to perform, and displays these messages in a graphical interface where they are grouped by context. The tool is intended for business managers who receive daily a large number of machinegenerated messages that require some action be taken. SCOUT uses the IBM Unstructured Information Management Architecture (UIMA) framework to apply rulebased reasoning for identification of tasks, and it uses contextual data to customize the presentation of task information to the user. SCOUT’s open, extensible architecture allows the use of alternate inference models (such as machine learning algorithms) as well as the integration of additional context sources and client interfaces. SCOUT was well received by the participants in a small evaluation study.	algorithm;client (computing);cluster analysis;email;graphical user interface;inbox by gmail;information management;logic programming;misra c;machine learning;software deployment;statement of work;thread (computing);usability testing	Daby M. Sow;John S. Davis;Maria Ebling;Archan Misra;Lawrence D. Bergman	2006	IBM Systems Journal	10.1147/sj.454.0739	rule-based system;simulation;computer science;operating system;data mining;graphical user interface;database;world wide web	HCI	-41.16691190854691	-24.580879787461683	113377
c70a9c12dc731f88c0bd5ceddb44ccc2cf830a75	multi-spectral imaging system (iwn) for the digitization and investigation of cultural heritage		This research focuses on the digitization and investigation of cultural heritage liaised with the practical requirements of conservators and museum curators. Different types of information are extracted about the physical characteristics of the artifacts, pigments preliminary identification and pigments distribution in addition to the colorimetric information. In this regard, a multi-spectral digitization system – named as “iwn” was developed to collect the required information from the cultural heritage objects. The system is portable, customizable, easy to use, in-situ, non-invasive and relatively not expensive. This paper will describe the specifications of the system showing its functions and capabilities through few case studies.		Ibrahim El-Rifai;Hend Mahgoub;Ari Ide-Ektessabi	2016		10.1007/978-3-319-48496-9_19	multimedia;cultural heritage;digitization;spectral imaging;engineering	Robotics	-35.6993532297159	-30.063999558180655	113410
3fb5a0eeb1f826083a54678ce85df4ae3780f4a6	the island metaphor	painting;mobile device;heterogeneous computing;foreground subtraction;video interface;mobile agent;virtual space	"""This paper presents an """"Island Metaphor"""" for interactions with systems of heterogeneous computational devices employing mobile agents. In this metaphor, stationary computers represent islands of virtual spaces, and mobile devices represent virtual rafts that allow agents to move through a sea of real space. The Island Metaphor provides several benefits over existing computational metaphors, and may be relevant to applications in education, entertainment, and social technologies."""	computer;interaction;mobile agent;mobile device;stationary process	Bill Tomlinson;Eric Ps Baumer;Man Lok Yau	2006		10.1145/1179622.1179719	simulation;painting;computer science;operating system;mobile agent;mobile device;multimedia;computer graphics (images)	HCI	-44.563717373908254	-36.504448537572195	113450
4260ae59fa02b658cca3d24e23154c84121a564f	ada -intelligent space: an artificial creature for the swiss expo.02	interactive systems human computer interaction real time systems;human computer interaction;sound effects ada intelligent space swiss expo 02 human computer interaction real time systems software architecture light effects;artificial intelligence space technology object oriented modeling hardware robot sensing systems orbital robotics organisms music research and development real time systems;interactive systems;real time systems	Ada is an entertainment exhibit that is able to interact with many people simultaneously, using a language of light and sound. “She” received 553,700 visitors over 5 months during the Swiss Expo.02 in 2002. In this paper we present the broad motivations, design and technologies behind Ada, and a first overview of the outcomes of the exhibit.	ada;switzerland	Kynan Eng;Andreas Bäbler;Ulysses Bernardet;Mark Blanchard;Márcio O. Costa;Tobi Delbrück;Rodney J. Douglas;Klaus Hepp;David Klein;Jônatas Manzolli;Matti Mintz;Fabian Roth;Ueli Rutishauser;Klaus Wassermann;Adrian M. Whatley;Aaron Wittmann;Reto Wyss;Paul F. M. J. Verschure	2003		10.1109/ROBOT.2003.1242236	simulation;human–computer interaction;computer science;engineering;computer graphics (images)	HCI	-45.3428966353054	-35.794935689521026	113597
bb35dd9e15d6dda9e0ca7387aba2dbfaa5e6a3ad	an attempt in modeling picasso's cubism style	generative art;layered approach;picasso cubism	This article presents our recent attempt in modeling Picasso's cubism style of paintings and generating similarly styled images. This extended abstract presents the results, with details of the texture implementation.		Guanyu Lian;Yanli Wang;Kang Zhang;Li Yao	2016		10.1145/3005274.3005306	computer graphics (images);picasso;computer science;painting	Vision	-39.476772362250145	-32.99581633904522	113606
bab683bc41e1712d484fa87bdfabd1d564e3658a	cg restoration of a historical noh stage and its use for edutainment	animacion por computador;texture;interfase usuario;estimation mouvement;multimedia;realite virtuelle;realidad virtual;atmosphere model;weather;tiempo meteorologico;user interface;cultural heritage;estimacion movimiento;texture mapping;temps meteorologique;virtual reality;peinture art;motion estimation;endommagement;motion capture data;deterioracion;paint;modelo atmosfera;motion capture;pintura arte;peinture;textura;character animation;distraccion;interface utilisateur;educacion;damaging;pintura;computer animation;modele atmosphere;distraction;painting art;animation par ordinateur	Japan's oldest Noh stage has been restored using CG. The painting of pine trees on a wall, which is severely damaged by the weather, has been restored and painted based on several historical and aesthetical investigations. The painting was texture-mapped onto the CG model giving the atmosphere of the stage when it was originally built. By using the CG Noh stage and motion capture data of a Noh play performed by a professional player, several kinds of edutaiment contents have been produced. Not only an ordinary character animation of a Noh play on the restored stage, but also walk-through animation of a scene which might be observed by the player during a play was created, which might be useful for introductory enlightenment.	circuit restoration;educational entertainment	Kohei Furukawa;Woong Choi;Kozaburo Hachimura;Kaori Araki	2006		10.1007/11890881_39	texture mapping;character animation;computer vision;atmospheric model;motion capture;computer science;cultural heritage;artificial intelligence;archaeology;motion estimation;virtual reality;computer animation;texture;user interface;computer graphics (images)	Vision	-47.058967809822406	-31.445876068521013	113730
c5a0d56f43c568cefd7eb198dc879211bdd168a3	computer vision framework for analyzing projections from video of lectures	computer vision	The overhead and computer projectors have become an essential element to the classroom and corporate settings. Users of web−based classes and conferences would like to have access to the projector tools. The bandwidth required to transmit these projections is too large to be useful. This paper proposes a framework for processing images displayed by overhead and computer projectors during presentations. In this way, the projectors can be easily rebroadcast over the internet without loss of quality due to compression. This process requires determining the areas of text, binarizing those areas, and performing Optical Character Recognition on the final image. Several examples are shown, including both successful and unsuccessful data sets. A discussion is also included which explains all of the results.	computer vision;data compression;internet;movie projector;optical character recognition;overhead (computing);video projector	Michael N. Wallick;Niels da Vitoria Lobo;Mubarak Shah	2000			optical character recognition;the internet;stereo cameras;projector;computer science;data set;computer vision;artificial intelligence	ML	-34.90071234258691	-35.4862303439135	113766
442f270257af9701717c31c4b464f807d264e3f8	korean text generation from database for homeshopping sites	relational database	This paper describes a text generation system, XExplainer, which can dynamically produce a description of commodities in Korean from a relational database for homeshopping sites. We focus on how to generate well-written texts through several generation stages in the marketing domain. The generated text was evaluated using several criteria, such as content completeness, structural coherence, conciseness of expression, and text layout.	algorithm;business object;computer engineering;domain-driven design;microsoft windows;natural language generation;relational database;user modeling;virtual community	Ji-Eun Roh;Sin-Jae Kang;Jong-Hyeok Lee	2001			computer science;database;data mining;relational database;information retrieval	NLP	-34.225922233329044	-27.534470967321386	113825
3a780a5871f1dde1b8c701ce40244e497e6b1acf	rectangle reasoning: a qualitative spatial reasoning with superposition		Qualitative spatial reasoning (QSR) is a method for treating images or figures qualitatively by extracting only the information required by a user for a specified purpose (Aliello et al. 2007). It is widely considered a promising method for reducing memory and workspace requirements for computations that do not involve strict data. However, few studies have developed practical applications. We propose rectangle reasoning as a framework for the application of QSR through an autonomic window placement system. A window can be considered as spatial data of rectangular shape, with changeable size and ratio of edges. Moreover, multiple windows can be displayed in a superposed manner. If we place multiple windows, such that the relevant parts of each are visible and the unnecessary parts are not visible, it provides a display in the most useful form to the user, effectively using a monitor of limited size. The rectangle reasoning proposed here addresses the relative positional relationships of rectangles with superposition. Each rectangle is represented symbolically using two simple objects, regions and lines, and their relationships, as well as visibility. In this study, we formalize rectangle reasoning and propose a reasoning algorithm. This algorithm determines whether a figure exists that satisfies all the given visibility requirements in a two-dimensional (2D) plane with foreground/background, and, if such a figure is determined to exist, derives the superposed locations of the rectangles. This paper is organized as follows. In section 2 we formalize rectangle reasoning; in section 3 we describe the reasoning algorithm; and in section 4 we show our conclusions.	algorithm;autonomic computing;computation;microsoft windows;quantum superposition;requirement;spatial–temporal reasoning;workspace	Shou Kumokawa;Kazuko Takahashi	2010			artificial intelligence;machine learning;spatial intelligence;superposition principle;rectangle;computer science	AI	-35.37794082708337	-36.92765021647738	113999
0f1f58ddecb39d290d1da2ea90f3273cb1f383c2	a novel interface for device diagnostics using speech recognition, augmented reality visualization, and 3d audio auralization	distributed system;natural language interfaces;tracking system;standard commercial off the shelf components novel interface device diagnostics speech recognition augmented reality visualization 3d audio auralization error diagnostics technical devices multimedia technology maintenance instructions diagnosis results multimedia techniques 3d rendered objects animations text annotations live video image movable camera device components real object auditory cues spatialized 3d audio computer vision based tracking system diagnostics system spoken annotations device modules distributed network;distributed networks;maintenance engineering;natural language interfaces fault diagnosis maintenance engineering speech recognition augmented reality computer animation multimedia computing computer vision;computer vision;multimedia computing;commercial off the shelf;speech recognition;speech recognition augmented reality visualization multimedia systems animation computer vision auditory displays world wide web human computer interaction maintenance;device diagnostics;augmented reality;computer animation;visual servoing;3d audio;fault diagnosis	Routine maintenance and error diagnostics of technical devices can be greatly enhanced by applying multimedia technology. The Rockwell Science Center is developing a system which can indicate maintenance instructions or diagnosis results for a device directly into the view of the user by utilizing Augmented Reality and multimedia techniques. The system can overlay 3D rendered objects, animations, and text annotations onto the live video image of a known object, captured by a movable camera. The status of device components can be queried by the user through a speech recognition system. The response is given as an animation of the relevant device module, overlaid onto the real object into the user’s view, and/or as auditory cues using spatialized 3D audio. The position of the user/camera relative to the device is tracked by a computer vision based tracking system. The diagnostics system also allows the user to leave spoken annotations attached to device modules for other users to retrieve. The system is implemented on a distributed network of PCs, utilizing standard commercial off-the-shelf (COTS) components.	ar (unix);augmented reality;computation;computer vision;distributed computing;human–computer interaction;multimodal interaction;personal computer;scalability;sonification;speech recognition;tracking system;wearable computer	Reinhold Behringer;Steven Chen;Venkataraman Sundareswaran;Kenneth Wang;Marius S. Vassiliou	1999		10.1109/MMCS.1999.779240	maintenance engineering;computer vision;augmented reality;tracking system;computer science;artificial intelligence;operating system;computer animation;multimedia;visual servoing;world wide web;computer graphics (images)	HCI	-44.21825289356698	-36.89634161869623	114184
71b288aa11867a7211982062fedcf8d3e70ab926	designing digital puppetry systems: guidelines and best practices	human computer interaction;digital puppetry;interaction design for children;computer vision;augmented virtuality;mixed reality	This instructive video presents guidelines and best practices for designers of digital puppetry systems by demonstrating four common setups and illustrating the benefits and limitations of each approach. Practical suggestions and humorous examples of green-screening techniques, digital composition, using rod puppets and using a Kinect camera are included to illustrate the possibilities and pitfalls of real-time animation for HCI designers interested in using computer vision to support creative expression with physical objects.	best practice;computer vision;digital puppetry;human–computer interaction;kinect;real-time locating system	Seth E. Hunter;Pattie Maes	2013		10.1145/2468356.2479529	human–computer interaction;computer science;digital puppetry;mixed reality;multimedia;augmented virtuality;computer graphics (images)	HCI	-42.32429665185945	-37.052264237307774	114230
7598366e914fe975588efb64d8bb7fa5958ed686	object caching and prefetching in distributed virtual walkthrough	model prefetching;virtual reality;computer graphic;simulation experiment;visual representation;distributed virtual walk through;user experience;multi resolution modeling;multi resolution caching;virtual environment;multi resolution;distributed virtual walkthrough;object model	We investigate a virtual reality application where a user experiences a walkthrough of a large virtual environment in the Internet, modeling a physical museum, library or any place of interest. Such technology provides users with the ability to view and explore a remote environment, without physical presence. To deliver good performance for such applications, we need to address several issues in different research disciplines. First, we must be able to model virtual objects effectively. The recently developed techniques for multi-resolution object modeling in computer graphics are useful in simplifying object models and reducing their rendering time. Second, with the low bandwidth of the Internet, caching suitable object models of high affinity will reduce the amount of data requested over the network for faster response. Prefetching object models by predicting those likely to be used in the near future and downloading them in advance will lead to a similar improvement. Third, the Internet often suffers from disconnection. A caching mechanism allowing objects to be cached with at least their minimum resolutions can provide at least a coarse view of the objects for an improved visual representation. In this paper, we propose amulti-resolution caching mechanism and investigate its effectiveness in supporting virtual walkthrough applications in the Internet environment. The caching mechanism is further complemented with several object prefetching mechanisms for predicting future accessed objects. The performance and feasibility of our proposed mechanisms are quantified via simulation experiments.	algorithm;approximation;cpu cache;cache (computing);client (computing);cognitive walkthrough;computation;computer graphics;database server;download;experiment;filter (signal processing);hit (internet);image noise;internet;link prefetching;motion capture;multi-user;point of interest;processor affinity;server (computing);simulation;software walkthrough;testbed;thread-local storage;transmitter;virtual reality;web cache	Rynson W. H. Lau;Jimmy H. P. Chim;Mark Green;Hong Va Leong;Antonio Si	2001	Real-Time Systems	10.1023/A:1011199405471	simulation;object model;computer science;virtual machine;operating system;virtual reality;multimedia;programming language;computer graphics (images)	Visualization	-41.48689944936852	-36.2849269935178	114327
0ebe25e80f0644889927f0b124dd37eb6dbe1243	3d virtual environment navigation aid techniques for novice users using topic map	sense of direction;concepcion asistida;3d virtual environment;computer aided design;diseno circuito;tecnologia electronica telecomunicaciones;realite virtuelle;red www;realidad virtual;information visuelle;topic maps;web semantique;circuit design;reseau web;virtual reality;semantics;semantica;semantique;informacion visual;guided navigation;object oriented;visual information;web semantica;semantic web;conception assistee;oriente objet;world wide web;conception circuit;topic map tour guide;virtual environment;analisis semantico;tecnologias;grupo a;analyse semantique;orientado objeto;semantic analysis	3D virtual environment provides a limited amount of information, mainly focusing on visual information. This is the main cause of users losing the sense of direction in the environment. Many researches for developing a navigation tools that address this problem have been carried out. In this study, a navigation tool is designed by applying topic map, one of the technologies for semantic web construction, to a 3D virtual environment. Topic map constructs a semantic link map by defining the connection relation between topics. According to an experiment done to evaluate the proposed navigation tool, the tool was more helpful in finding detailed object than highly represented objects. Also, it could be seen that providing the surrounding knowledge is effective for object selection by users when that target for searching is not defined. key words: guided navigation, topic map tour guide, virtual environment	link relation;semantic web;topic maps;virtual reality	Hak-Keun Kim;Teuk-Seob Song;Yoon-Chul Choy;Soon-Bum Lim	2006	IEICE Transactions	10.1093/ietisy/e89-d.8.2411	topic maps;simulation;computer science;virtual machine;artificial intelligence;circuit design;semantic web;semantics;virtual reality;multimedia;object-oriented programming;world wide web;mobile robot navigation	HCI	-38.46808702983047	-25.78113052078338	114537
95cf2428923529da3d06d99ed3cbd1e623da9b51	user interface design for virtual reality: a research tool for tracking navigation	vrnrt;spatial behavior;navigational decisions;human computer interaction;electronic mail;information systems;multimedia;user interfaces virtual reality navigation intelligent agent testing;user interface;3d mazes;internet user interface design virtual reality research tool multimedia intelligent agents web based navigation tool navigational decisions vr navigation research tool three dimensional worlds navigational tracking vrnrt 3d mazes spatial behavior;virtual reality;testing;three dimensional;navigation;intelligent agents;human factors;internet;vr navigation research tool;intelligent agent;web based navigation tool;virtual reality environment;user interface design;three dimensional worlds;navigational tracking;software tools;user interface management systems;computer science;research tool;interactive systems;user interfaces;microcomputers;virtual worlds	In the rapidly developing area of User Interface Des (UID), the existing knowledge and understanding of U serves as a basis for further research and study of fac affecting users as they interact with interfaces that uti the technology of multimedia, virtual reality an intelligent agents. Navigation is one of the factors t is being investigated to help designers develop u interfaces that are effective and efficient for the us This paper describes a web-based navigation developed to support research into factors that af navigational decisions in a virtual reality environmen The tool, VR Navigation Research Tool (VRNRT), all the design and exploration of three-dimensional wor and provides for navigational tracking. When usi VRNRT, the structure of the virtual worlds is confined 3D mazes. VRNRT enables researchers to custo experiments that allow for testing a subjec navigational and spatial behavior.	experiment;intelligent agent;item unique identification;user interface design;virtual reality;virtual world;web application	Joaquin Vila;Barbara Beccue;Greg Furness	1998		10.1109/HICSS.1998.654806	simulation;human–computer interaction;computer science;artificial intelligence;operating system;multimedia;user interface;intelligent agent	HCI	-34.26205514979305	-32.02194687968764	114562
ae6d6a99c2d972c5ac50c03b0d1538eec14bbdd2	adding rule-based reasoning to a demonstrational interface builder	interface builder;uimss;direct manipulation;programming by demonstration;interactive display;interface builders;rule based reasoning	This paper presents a demonstrational interface builder with improved reasoning capabilities. The system is comprised of two major components: an interactive display manager and a rule-based reasoner. The display manager provides facilities to draw the physical appearance of an interface and define interface behavior by graphical demonstration. The behavior is defined using a technique of stimulus-response demonstrations. With this technique, an interface developer first demonstrates a stimulus that represents an action that an end user will perform on the interface. After the stimulus, the developer demonstrates the response(s) that should result from the given stimulus. As the behavior is demonstrated, the reasoner observes the demonstrations and draws inferences to expedite behavior definition. The inferences entail generalizing from specific behavior demonstrations and identifying constraints that define the generalized behavior. Once behavior constraints are identified, the reasoner sends them to the display manager to complete the definition process. When the interface is executed by an end-user, the display manager uses the constraints to implement the run-time behavior of the interface.	graphical user interface;interface builder;logic programming;semantic reasoner	Gene L. Fisher;Dale E. Busse;David Wolber	1992		10.1145/142621.142632	rule-based system;real-time computing;simulation;human–computer interaction;computer science;artificial intelligence;operating system	HCI	-39.975183782307475	-29.639336324469593	115063
4cfd8383d1afc2509b1b64011a8f8ed733ed4bf0	interactive paper as a reading medium in digital libraries	digital documents;interactive paper;digital library	In digital libraries, much of the reading activity is still done on printed copies of documents. We show how digital pen and paper technologies can be used to support readers by automatically creating interactive paper versions of digital documents during the printing process that enable users to activate embedded hyperlinks to other documents and services from printed versions. The approach uses a special printer driver that allows information about hyperlinks to be extracted and stored at print time. Users can then activate hyperlinks in the printed document with a digital pen.	digital library;digital paper;digital pen;embedded system;hyperlink;library (computing);plug-in (computing);printed web;printer (computing);printer driver;printing;web page	Moira C. Norrie;Beat Signer;Nadir Weibel	2008		10.1007/978-3-540-87599-4_24	digital library;computer science;digital media;multimedia;world wide web;information retrieval	HCI	-43.84335343096621	-26.143022866313135	115714
7dde5e738481de1c0f7768a51f1ff96796b7dc83	made-to-measure technologies for an online clothing store	electronic commerce;virtual reality languages;vrml online clothing store web application virtual store clothing design pattern derivation clothing sizing 3d graphics three dimensional graphics;home shopping;clothing web server spatial databases layout application software animation information retrieval information geometry computer architecture delay;internet;design pattern;3d graphics;textile industry;home shopping electronic commerce internet virtual reality languages textile industry	The Internet along with the rapidly growing power of computing has emerged as a compelling channel for sale of garments. A number of initiatives have arisen recently across the world [1][2][3], revolving around the concepts of Made-to-Measure manufacturing and shopping via the Internet. These initiatives are fueled by the current Web technologies available, providing an exciting and aesthetically pleasing interface to the general public.	advanced transportation controller;autodesk 3ds max;bespoke;blackwell (series);common gateway interface;computational science;computer graphics international;curve fitting;e-commerce;eurographics;game developers conference;graphical user interface;html;humanoid animation;humans;interactive visualization;internet;jason;macy conferences;motion capture;numerical recipes;numerical linear algebra;plug-in (computing);real-time transcription;simulation;springer (tank);system integration;tom;web components;web page;world wide web	Frederic Cordier;Hyewon Seo;Nadia Magnenat-Thalmann	2003	IEEE Computer Graphics and Applications	10.1109/MCG.2003.1159612	e-commerce;the internet;simulation;textile industry;computer science;multimedia;design pattern;3d computer graphics;computer graphics (images)	Visualization	-46.43993722404261	-29.177826189952846	115763
37e528a58f6bf581ff94563739c59d4955046c14	bayesian user modeling for inferring the goals and needs of software users	user model	The Lumi ere Project centers on harnessing probability and utility to provide assistance to computer software users. We review work on Bayesian user models that can be employed to infer a user's needs by considering a user's background, actions, and queries. Several problems were tackled in Lumi ere research, including (1) the construction of Bayesian models for reasoning about the time-varying goals of computer users from their observed actions and queries, (2) gaining access to a stream of events from software applications, (3) developing a language for transforming system events into observational variables represented in Bayesian user models, (4) developing persistent pro les to capture changes in a user's expertise, and (5) the development of an overall architecture for an intelligent user interface. Lumi ere prototypes served as the basis for the O ce Assistant in the Microsoft O ce '97 suite of productivity applications.	approximation;autonomous robot;intelligent user interface;lumi masking;prototype;real-time transcription;sensor;user (computing);user modeling	Eric Horvitz;John S. Breese;David Heckerman;David Hovel;Koos Rommelse	1998			user interface design;user modeling;computer science;user requirements document;machine learning;data mining;database;world wide web	ML	-34.60465874898187	-25.718683661187274	116056
a8279aec711f40d1b901a322147a885d134a508f	holography map for home robot: an object-oriented approach	environment cognition;home holography map;object oriented;home service robot	The environment map plays an important role in robot service, so it should contain not only appearance information about the whole service environment, but also their profoundness. The key contribution of the paper is the presentation of a novel semantic map, namely, a holography map composed of robot, family persons, operable items, local environments, as well as locations and path sections for home service robot cognizing its surroundings and providing services. Inspired by the object-oriented approach, the holography map is divided into three hierarchies of item-room-home and in detail 13 classes of objects. The design and storage of the object-oriented holography map are described comprehensively, and construction of the map is introduced. The execution of robot service based on the object-oriented holography map is discussed briefly. Experiments on real service robot demonstrate that the object-oriented holography map is nearer to human thinking and applicable to indoor robot service tasks.		Peiliang Wu;Lingfu Kong;Shengnan Gao	2012	Intelligent Service Robotics	10.1007/s11370-012-0109-z	computer vision;simulation;computer science;social robot;object-oriented programming	Robotics	-40.12406902959197	-31.870674014729072	116081
3a44b112e90aa7b5ab39692cc719184af21159c6	application to the disaster data of an idea generation consistent support system	computers;groupware;disaster;time measurement;gungen spiral ii;disaster data;idea generation consistent support system;browsers;pictograph;pictograph stamp;public administration disasters internet;support system;servers;internet;iphone;iphone groupware pictograph idea generation support disaster;kj method;rain browsers servers time measurement writing cities and towns computers;image editing application;rain;pictograph stamp disaster data idea generation consistent support system image editing application quiccamera gungen spiral ii web based system kj method handwriting;writing;web based system;idea generation support;cities and towns;idea generation;quiccamera;disasters;handwriting;public administration	We have developed an image editing application, Quiccamera and an idea generation consistent support system, GUNGEN-SPIRAL II. Quiccamera was created for the idea collection part of GUNGEN-SPIRAL II. Quiccamera can upload data briefly. GUNGEN-SPIRAL II is a Web-based system, which can support shared ideas and the circulation of information for an idea generation method, the KJ method. We applied these two systems to disaster data. From the results of the experiment, we found that we could easily upload ideas using handwriting and a pictograph stamp for photographs with Quiccamera. The evaluation of the pictograph stamp in particular was high. The correspondence of anti-disaster measures created by GUNGEN-SPIRAL II was found to be sufficient.	basic stamp;image editing;pictogram;upload	Toshihiro Ajiki;Hiroshi Fukuda;Tomohiro Kokogawa;Junko Itou;Jun Munemori	2011	2011 IEEE Workshops of International Conference on Advanced Information Networking and Applications	10.1109/WAINA.2011.83	disaster;simulation;telecommunications;computer science;operating system;database;multimedia;law;world wide web;computer security	Robotics	-45.618657809261826	-31.651854869144515	116538
0c945b394e41ff78ac3cebe207efb88c6348cfd6	a framework for haptic broadcasting	skin;haptic interfaces multimedia communication broadcast technology digital multimedia broadcasting displays virtual environment streaming media layout mpeg 4 standard motion pictures;transform coding;haptics;prototype system;multimedia systems;proof of concept;media;mpeg 4haptics;multimedia systems haptic interfaces;streaming media;haptic multimedia broadcasting;prototype system haptic multimedia broadcasting;mpeg 4 haptics haptic media broadcasting;multimedia communication;mpeg 4;haptic media broadcasting;broadcasting;haptic interfaces	This article presents a comprehensive exploration of the issues underlying haptic multimedia broadcasting. It also describes the implementation of a prototype system as a proof of concept.	haptic technology;prototype	Jongeun Cha;Yo-Sung Ho;Yeongmi Kim;Jeha Ryu;Ian Oakley	2009	IEEE MultiMedia	10.1109/MMUL.2009.42	embedded system;transform coding;media;computer science;operating system;multimedia;skin;haptic technology;proof of concept;mpeg-4;broadcasting;computer graphics (images)	Visualization	-43.79629695702499	-34.07386874737082	116559
6e02aa916ff0c97c92705c692c1ef5b282d265ae	css - pocket reference: visual presentation for the web: covers css2 and css2.1 (2. ed)			cascading style sheets;handbook	Eric A. Meyer	2004			computer science;multimedia;world wide web;computer graphics (images)	Vision	-44.48839996954892	-26.801590918215737	116866
8622d3d2024c3b57be8c6caac627bb452cf67e4a	a prototype design tool for participants in graphical multiuser environments	design tool;urban planning;building block;multi user domains;constructionist environments;multi user;visual programming;design tools;virtual space	Users of this software construction kit can design layouts for virtual spaces. The elements of the software kit are based on Kevin Lynch's elements of the city image: districts, paths, edges, nodes, and landmarks (Lynch, 1960; Banerjee & Southworth, 1990).	design tool;graphical user interface;prototype;software construction	Carol Strohecker;Barbara Barros	1997		10.1145/1120212.1120374	simulation;human–computer interaction;computer science;urban planning;visual programming language	HCI	-42.21975968036345	-32.00396296519023	116946
5232d90c5f2472b009e362bb00e9094683a7f8a1	intelligent semantic oriented agent based search (i-soas)	agent based;language technology;graphical user interface;product data management;research paper;research and development;human machine interface;pdm;semantic web;graphic user interface;natural language processing	This research paper is the brief presentation of a PhD research and development conducted in the fields of Product Data Management (PDM). This research paper briefly presents targeted problems i.e. unfriendly graphical user interfaces and unintelligent search in PDM Systems, state of the art, proposed approach i.e., Intelligent Semantic Oriented Agent based Search, comparison of implemented prototype with existing systems and some future recommendation.	agent-based model;graphical user interface;prototype	Zeeshan Ahmed	2009		10.1145/1838002.1838065	human–computer interaction;natural language user interface;computer science;multimedia;world wide web	AI	-40.980090963264985	-26.991472764009572	116964
b8f4e4603adf7653ad30c502854b99ac93bffbc5	a magic lantern data projector		The Magic Lantern Data Projector allows a performer-participant to build sculptural glass forms that are projected, then algorithmically processed. The device re-envisions magic lantern slides as transparent, multifaceted three-dimensional glass boxes. The projected image is a palimpsest of coloured transparent glass forms (Figure 1). Servomotors position the boxes in front of a camera lens, which uses software to translate the projection into numerical data.	algorithm;level of measurement;magic lantern;video projector	Mark Hursty;Victoria Bradbury	2015			visual arts;engineering;optics;computer graphics (images)	HCI	-45.45461837446773	-29.7063894708924	117063
d165476c171b2d50ab6d7fe2b95b62a743100ae8	a generic development platform for asd therapy tools	proceedings international	We discuss the design and implementation of a generic development platform for 3D virtual environments for autism spectrum disorder therapy tools. The platform is intended to enable researchers and therapists alike to quickly develop individual therapy software without having to resort to extensive software development. The platform includes specific features requested by therapists, such as replays, data capturing and remote monitoring over a network. We also discuss the implementation of a number of different therapy tools, and their subsequent evaluation at local schools.	assistive technology;automatic identification and data capture;gnu;hierarchical editing language for macromolecules;john d. wiley;kerr effect;lua;middleware;object-oriented graphics rendering engine;open-source software;openal;registered jack;scripting language;software development;sourceforge;universal usability;virtual reality;winsock;x3d;gettext	Lynette van Zijl;Morné Chamberlain	2010			computer science	Visualization	-48.0930003079907	-27.355588032858307	117228
30bde3b0a279733d66d7cb992530229857321c68	constructive roofs from solid building primitives		The creation of building models has high importance, due to the demand for detailed buildings in virtual worlds, games, movies and geo information systems. Due to the high complexity of such models, especially in the urban context, their creation is often very demanding in resources. Procedural methods have been introduced to lessen these costs, and allow to specify a building (or a class of buildings) by a higher level approach, and leave the geometry generation to the system. While these systems allow to specify buildings in immense detail, roofs still pose a problem. Fully automatic roof generation algorithms might not yield desired results (especially for reconstruction purposes), and complete manual specification can get very tedious due to complex geometric configurations. We present a new method for an abstract building specification, that allows to specify complex buildings from simpler parts with an emphasis on assisting the blending of roofs.	3d modeling;algorithm;alpha compositing;cross-sectional data;information system;microsoft windows;pipeline (computing);polygonal modeling;procedural programming;rule-based system;stochastic context-free grammar;stochastic grammar;virtual world;xfig	Johannes Edelsbrunner;Ulrich Krispel;Sven Havemann;Alexei Sourin;Dieter W. Fellner	2016	Trans. Computational Science	10.1007/978-3-662-49247-5_2	discrete mathematics;topology;mathematics;geometry	Graphics	-37.404754772131135	-32.713181103922885	117254
04fd941e7040ac542a9fb3648bec65f99daecb62	strike on stage: a percussion and media performance	computer vision;conference paper;percussion;media performance	This paper describes Strike on Stage, an interface and corresponding audio-visual performance work developed and performed in 2010 by percussionists and media artists ChiHsia Lai and Charles Martin. The concept of Strike on Stage is to integrate computer visuals and sound into an improvised percussion performance. A large projection surface is positioned directly behind the performers, while a computer vision system tracks their movements. The setup allows computer visualisation and sonification to be directly responsive and unified with the performers’ gestures.	computer vision;sonification	Charles E. Martin;Chi-Hsia Lai	2011			visual arts;computer vision;simulation;computer science;percussion;computer graphics (images)	Vision	-44.58196801865447	-35.40354432756885	117269
3e0d69bde4f1fef9b6506eb163887ac674a68df2	implementations of the leap motion device in sound synthesis and interactive live performance	music expressivity;leap motion;gestural control	The Leap Motion device opens new doors for mapping various degrees of human motion for musical control and expression. In this paper, we explore implementations of the Leap Motion in sound synthesis and interactive live performance. The Leap Motion is a USB computer peripheral released in mid-2013 that uses IR cameras to track hand and finger location with unprecedented accuracy. Discussed implementations include a 5-grain granular synthesizer created in Max/MSP that connects to the Leap Motion, allowing users to trigger individual grains using finger and hand motion. The Leap Motion is also interfaced with Ableton Live, software synthesizers and a 6-channel hemispherical speaker. The benefits and limitations of these implementations are discussed in light of recent compositions and performances.	ableton live;kinesiology;max;performance;peripheral;usb	Lamtharn Hantrakul;Konrad Kaczmarek	2014		10.1145/2617995.2618020	simulation;computer science;communication;computer graphics (images)	Graphics	-46.067333803250754	-36.17566501051345	117326
42c7a8aeaecf51244fc672e00cd5e088d9b04277	mage -a platform for tangible speech synthesis		In this paper, we describe our pioneering work in developing speech synthesis beyond the Text-To-Speech paradigm. We introduce tangible speech synthesis as an alternate way of envisioning how artificial speech content can be produced. Tangible speech synthesis refers to the ability, for a given system, to provide some physicality and interactivity to important speech production parameters. We present MAGE, our new software platform for high-quality reactive speech synthesis, based on statistical parametric modeling and more particularly hidden Markov models. We also introduce a new HandSketch-based musical instrument. This instrument brings pen and posture based interaction on the top of MAGE, and demonstrates a first proof of concept.	hidden markov model;interactivity;markov chain;poor posture;programming paradigm;speech synthesis	Maria Astrinaki;Nicolas D'Alessandro;Thierry Dutoit	2012			human–computer interaction;interactivity;proof of concept;natural language processing;software;computer science;hidden markov model;artificial intelligence;speech synthesis;speech production	ML	-46.2030963592081	-35.40109740658794	117643
8919538f306b4716a011b6021aa544b03c3a14e4	study results: the use of virtual environments for product design	product design system testing virtual environment displays collaborative work product development collaboration process design virtual reality decision making;groupware;virtual reality;functional response;design environment;design review process virtual environments product design product development multi functional teams collaborative virtual design environments system selection visual displays;cad cam;interactive virtual environment;user interfaces virtual reality groupware product development cad cam;product design;virtual environment;user interfaces;product development	Today, many product development organizations are investigating the use of multi-functional teams coupled with emerging interactive virtual environment technologies to create more robust, collaborative virtual design environments. Within these environments design team mates could simultaneously enter a virtual product design, and jointly evaluate design issues, ideas, and parameters-each from their own experience, perspective, viewpoint and functional responsibility. However, the proliferation and variety of commercially available virtual environment systems has complicated system selection. This paper presents results of a study that investigated the use of four commercial visual displays on the design review process. Study results indicated that the sense of presence experienced while conducting a design review plays an important role in improving design review practices and that some devices are considerably better than others.	virtual reality	Grace M. Bochenek;James M. Ragusa	1998		10.1109/ICSMC.1998.728053	functional response;simulation;human–computer interaction;computer science;virtual machine;instructional simulation;virtual reality;design education;product design;design technology;user interface;new product development;computer-aided technologies;product engineering	Visualization	-48.073005988388736	-32.65469222144804	118012
db48dc2ab2bd963cb7ce3f4cf2c26a9994123463	design issues in human visual perception experiments on region warping	human visual perception;region warping.;priority rendering;distortions;perception;region;visual;virtual reality;human factors;3d graphics;warping;head mounted display;human perception;design;human visual system	Virtual reality systems are primarily concerned with the presentation of realistic 3D graphics to the user. In light of the fact that the human visual system can only perceive a finite amount of detail, it is therefore possible to reach a balance in the tradeoff between human perception of detail and computational load required for rendering. Priority rendering is a technique designed to reduce the overall rendering load for an address recalculation pipeline virtual reality system. This system was developed to reduce user perceived latency during head rotations in head mounted display virtual reality systems. Large object segmentation and region warping were methods introduced to priority rendering in order to further enhance the overall rendering load reductions and to deal with various visual artefacts. This paper discusses the issues and considerations involved in designing human factors experiments concerning human perception to region warping.	3d computer graphics;computer monitor;distortion;experiment;head-mounted display;human factors and ergonomics;human-based computation;image processing;virtual reality	Yang-Wai Chow;Ronald Pose;Matthew Regan	2005			rendering (computer graphics);latency (engineering);3d computer graphics;image warping;perception;virtual reality;human visual system model;computer vision;visual perception;artificial intelligence;computer science	Visualization	-36.58387029236372	-36.987354538811275	118129
7221aa178ddb8ab29b5b90d635dc85ce1b5d1cb0	virtual art volant	non photorealistic rendering;user interface;virtual reality;animation	"""We describe the creation of virtual """"Art Volant"""" (""""flying art""""), a virtual reality extension of kite art by Jackie Matisse. Past exhibitions of Matisse's kites have involved either static real-world kites, or pre-recorded films of the kites in motion. Through the use of VR, gallery patrons are able to fly kites themselves, enhancing their appreciation of the work."""	virtual art;virtual reality	David E. Pape;Jackie Matisse	2006		10.1145/1179622.1179806	anime;simulation;computer science;artificial intelligence;virtual reality;user interface;computer graphics (images)	HCI	-44.01159899664274	-34.83525339407045	118170
c007052f1ae69cf792a20444aa0d1d4c7442b67e	automatic real time simulation through embedding physical attributes	virtual reality	An automatic real time physical simulation method is presented through embedding physical attributes into collections of 3D models in Virtual Rapid Scene Building System called VR Scene Studio. In the physical simulation, dynamic equations of the objects with physical attributes are established by Lagrange method. The improved impulse-based paradigm is adopted to gain interactive simulating speed. To improve the performance, optimizing methods are investigated. The real time and physically plausible result can be achieved by the combined the above two methods. The experiment results show that the method is valid and practical.	simulation	Xiaohui Tan;Shanshan Li;Mingquan Zhou	2013	iJOE		computer vision;simulation;computer science;artificial intelligence;virtual reality;multimedia;computer graphics (images)	AI	-41.77226712106021	-33.50644746597216	118326
d63528b6be3c5c1bd5ac0c2150c7efe901c2bc37	proof animation: better animation for your simulation	proof animation;better animation;user interfaces;computer architecture;software design;computer languages;application software;user interface;complex system;programming language;animation;open architecture;geometry;software systems;hardware	The Proof AnimationTM family of animation software is designed to meet the animation needs for a vast array of applications. This product family runs on readily available, inexpensive PC hardware. Some of the overall features are general purpose architecture, ASCII filedriven, vector-based geometry with the ability to zoom in or out while maintaining crisp clear animations, post-processing for maximum performance, CAD-type drawing tools for ease of creating animations, and a unique presentation mode used for displaying the results of the study. Roof Animation is not tied to one specific simulation language. Because of its open architecture, Proof Animation can serve as the animation tool for models written in a wide variety of simulation and programming languages. Proof Animation's user interface is menu-based and easily navigated using either a mouse or keyboard. Its superior performance assures smooth, realistic motion whether the animations depict simple systems or complex systems with many moving parts.	complex systems;computer animation;computer-aided design;open architecture;programming language;simulation language;simulation software;systems design;user interface;video post-processing	Nancy J. Earle;James O. Henriksen	1993		10.1109/WSC.1993.718043	physically based animation;complex systems;simulation;computer facial animation;skeletal animation;computer science;theoretical computer science;interactive skeleton-driven simulation;computer animation;user interface;computer graphics (images)	Graphics	-41.89764203879214	-30.864818487948813	118348
8dd4f07454a642e70689401998e0234bfab63a54	toward an adaptive www: a case study in customized hypermedia	filtering;interfase usuario;filtrage;web based applications;object oriented methods;vector space model;user interface;information retrieval;web;filtrado;web interface;customization;personnalisation;systeme adaptatif;hypermedia;internet;adaptive system;sistema adaptativo;adaptive hypermedia system;world wide web;interface utilisateur;hipermedia;user model	Abstract The widespread and rapid growth of the World Wide Web (WWW) is a clear indicator of its success as an instrument for information delivery. At the same time, significant limitations in many current Web-based applications demonstrate the need for new research. The history of hypermedia usability research predating the WWW and the relatively new area of research in Adaptive Hypermedia Systems (AHS) offer promising possibilities for enhancing this new medium. The problem is to design an AHS which (1) provides the integration of information from heterogeneous sources into a unified interface, (2) provides a filtering mechanism so that users see and interact with a view that is customized to their needs, (3) delivers this information through a Web interface, and (4) supports the automatic creation and validation of links between related items to help with ongoing maintenance of the application. We present a design to address this problem which makes use of object-oriented methods, information retrieval...	hypermedia;www	Kathryn F. Gates;Pamela B. Lawhead;Dawn Wilkins	1998	The New Review of Hypermedia and Multimedia	10.1080/13614569808914697	human–computer interaction;computer science;adaptive system;web navigation;multimedia;user interface;world wide web	Web+IR	-38.36785894504577	-25.14797736737916	118615
37b531715ad951aaad3ea431cad98e7bc67d4bb3	an approach towards information systems with very large numbers of subscribers	computer industry;large numbers;computer control centre;federal ministry;hybrid information system;mini-computer multi-processor system;large number;two-way cable television;interactive system;important research aspect;approach towards information systems;control centre;laboratory model;motion pictures;information system	The Federal Ministry of Research and Technology (BMFT) took the initiative in starting a project concerning two-way cable television in 1976 at the Heinrich-Hertz-Institute. The two-way cable television provided a model for investigating the possibilities of new screen-orientated, interactive systems for large numbers of subscribers using alpha-numerics, graphics, motion-pictures, and sound. One of the most important research aspects proved to be the construction of a computer control centre which would cater for this type of hybrid information system. At present a laboratory model of such a control centre is being developed by the Heinrich-Hertz-Institute in collaboration with the Computer Industry. This model is based on a mini-computer multi-processor system.	information system	G. Creutz;F. Kiel	1980		10.1145/647003.711748	simulation;human–computer interaction;computer science;multimedia;world wide web;information system	DB	-46.983712249545036	-26.66269239298342	118670
bbdf9078e224b5559b34b28f893332827a44670d	towards a high efficient 360° video processing and streaming solution in a multiscreen environment		Immersive video has been around for some time but only recently the technology became more popular since affordable cameras with sufficient resolution as well as stitching software with reasonable quality became available to allow professionals and interested amateurs to create 360° movies. Networks became fast enough to allow end-users to stream 360° video content to their devices. Hardware on TVs, smartphones and tablets is sufficiently powerful to handle the content and react on view changes without noticeable delay. Most efforts in this area, however, have been aimed at the technical challenges creating and viewing of 360° content. As 360° video is starting to reach a wider audience, the need arises to pay attention to the use of such content in a realistic commercial environment. For this, two issues need to be addressed. The efficient distribution of 360° content and the added-value that it can bring content providers. We will address in this paper two main challenges a) the efficient streaming of high quality 360° video content using existing content delivery networks (CDNs) and without the need for additional bandwidth comparing to traditional video streaming and b) the playback of 360° content even on devices with limited processing resources and programmatic capabilities.	360-degree video;content delivery network;digital distribution;digital video;display resolution;emoticon;multi-screen video;smartphone;streaming media;tablet computer;video processing	Louay Bassbouss;Stephan Steglich;Sascha Braun	2017	2017 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2017.8026237	computer vision;artificial intelligence;computer science;software;image stitching;video processing;multimedia	Visualization	-39.89894746361452	-33.88588310204962	118752
ffbc1158138996efba9fcbd7eb3458abb0bd9af1	powerbookmarks: a system for personalizable web information organization, sharing, and management	modelizacion;site web;gestion informacion;base donnee;classification tree;bookmark;red www;metadata;reseau interconnecte;structure arborescente;signet;powerbookmarks;database;base dato;personalization;classification;query classification;information organization;modelisation;hypermedia;organizacion informacion;navigation;marcador;internet;estructura arborescente;information management;tree structure;metadonnee;query;organisation information;world wide web;management tool;reseau www;metadatos;gestion information;sitio web;red interconectada;modeling;interconnected power system;langage html;hipermedia;web site;html language;bookmarks	We extend the notion of bookmark management by introducing the functionalities of hypermedia databases. PowerBookmarks is a Web information organization, sharing, and management tool, which parses metadata from bookmarked URLs and uses it to index and classify the URLs. PowerBookmarks supports advanced query, classification, and navigation functionalities on collections of bookmarks. PowerBookmarks monitors and utilizes users' access patterns to provide many useful personalized services, such as automated URL bookmarking, document refreshing, and bookmark expiration. It also allows users to specify their preference in bookmark management, such as ranking schemes and classification tree structures. Subscription services for new or updated documents of users' interests are also supported.	knowledge organization	Wen-Syan Li;Quoc Vu;Divyakant Agrawal;Yoshinori Hara;Hajime Takano	1999	Computer Networks	10.1016/S1389-1286(99)00032-8	semantic url;navigation;web query classification;the internet;systems modeling;html;decision tree learning;biological classification;computer science;personalization;database;multimedia;tree structure;information management;metadata;world wide web	Web+IR	-38.618284620012936	-25.31617439126892	119203
3ce854a694265cd44b812933f81bb5b3efb12422	seamless interaction among heterogeneous devices in support for co-located collaboration	local mobility;groupware;movilidad;remote control;hospital work;affichage;informatique mobile;hospital;document analysis;aplicacion medical;service provider;visualizacion;mobility;echange document;pervasive computing;teclado;telecommande;mobilite;work environment;collaborative system;document exchange;remote operation;service utilisateur;informatica difusa;hopital;analyse documentaire;proximite;proximidad;display;intercambio documento;informatique diffuse;team work;teleaccion;user experience;proximity;application sharing;travail equipe;co located collaboration;trabajo equipo;proximity based application sharing;analisis documental;keyboard;medical application;control remoto;servicio usuario;heterogeneous devices;user service;mobile computing;collecticiel;clavier;teleoperation;application medicale	In some working environments users experience a high level of mobility while requiring collaborating and coordinating their activities with colleagues involving the exchange and analysis of documents distributed in space or time. Medical workers stand out among others by the demands imposed by hospital work. These new forms of interaction pose new challenges for the design of pervasive computing environments aimed at seamlessly integrating heterogeneous devices. Based on workplace studies conducted in a hospital, we designed and implemented a mobile collaborative system aimed at supporting co-located collaboration, proximity-based application-sharing, and the remote control of heterogeneous devices. The results of a preliminary evaluation show that users perceive the services provided by the application to be useful and efficient, even though the manipulation of the remote display through the PDA was less efficient than with the keyboard and mouse.	computer-supported cooperative work;cursor (databases);document;game controller;high-level programming language;interaction;mobile device;personal digital assistant;remote control;seamless3d;software deployment;ubiquitous computing;user experience	Antoine Markarian;Jesús Favela;Monica Tentori;Luís A. Castro	2006		10.1007/11853862_31	service provider;teleoperation;simulation;teamwork;human–computer interaction;computer science;operating system;database;mobile computing;distance;world wide web;computer security;remote control	HCI	-36.09380848446118	-25.657847883463358	119229
64ae95001a85c81effa86b1b5050b8a0e6d0dcb0	the whole internet for windows 95 - user's guide and catalog: covers internet explorer			internet explorer;windows 95	Ed Krol;Paula M. Ferguson	1995			active scripting;microsoft windows;the internet;windows internet name service;html application;engineering;software versioning;windows vista;database;internet privacy;jscript;world wide web;vbscript	Metrics	-44.00214506597983	-23.950849441953427	119634
e119e9a09da7efa172400766dd8c2f9c41f06d6f	mobile instruments made easy: creating musical mobile apps with libpd and ios, no experience necessary		The quirks of programming native iOS applications can be daunting, especially to someone not already familiar with other text based programming languages. Fortunately, recent developments in Apple’s Xcode IDE, along with the open-source Pure Data wrapper, LIBPD, enable the process of creating an iOS native, standalone, musical mobile app to be quite accessible. Basic implementation of LIBPD for iOS can be reduced to a series of 10 simple and accessible steps requiring no actual knowledge of objective-c or the LIBPD library itself, and requires no previous coding experience, text-based or otherwise. In addition, the dragand-drop feature of Xcode’s Storyboards allows the design and programming of a native user interface to also be reduced to simple and accessible step-by-step instructions. This is not just limited to buttons and sliders, but also includes the drag-and-drop creation of typical touch screen gestures. Presently, this process will be outlined and explained, and its current and potential uses described.	drag and drop;mobile app;objective-c;open-source software;programming language;pure data;storyboard;text-based (computing);touchscreen;user interface;xcode;ios	Danny Holmes	2014			computer hardware;computer science;multimedia;world wide web	HCI	-43.81713270019138	-30.746595541437312	119704
edcaef0f192146e316f38f91bf0772b6384c490b	context-based navigation in the web by means of dynamically generated guided tours	web navigation;information space;hypermedia;object oriented;system;environment;object orientation;design;guided tour;guided tours;hypertext	A key advantage of hypermedia systems such as the Web is that the user is able to navigate through the information space in a non-linear fashion. He can explore the interlinked documents according to his own interests and insights, instead of being confined to the rigid, linear structure of e.g. pages in a book. A downside, however, is that this navigational freedom entails the risk of disorientation, especially in a gigantic hypertext such as the Web. This paper presents a context-based navigation paradigm for the Web and reconciles navigational freedom with a measure of linear guidance to prevent the user from becoming disoriented. For that purpose, “conventional” navigation along static links is complemented by run-time generated guided tours, which are derived dynamically from the context of a user’s information requirements.	hypermedia;hypertext;nonlinear system;profile-guided optimization;programming paradigm;requirement;world wide web	Wilfried Lemahieu	2002	Computer Networks	10.1016/S1389-1286(02)00210-4	design;simulation;hypertext;computer science;web navigation;system;multimedia;world wide web	Web+IR	-39.102469064355255	-25.773828118147584	119781
c82317e40501e6caf03f8a16d7cb5fcc7eef3e7c	free flight in parameter space: a dynamic mapping strategy for expressive free impro	parameter space	The well-known difficulty of controlling many synthesis parameters in performance, for exploration and expression, is addressed. Inspired by interactive evolution, random vectors in parameter space are assigned to an array of pressure sensitive pads. Vectors are scaled with pressure and added to define the current point in parameter space. Vectors can be scaled globally, allowing exploration of the whole space or minute timberal expression. The vector origin can be shifted at any time, allowing exploration of subspaces. In essence, this amounts to mutation-based interactive evolution with continuous interpolation between population members. With a suitable sound engine, the system forms a surprisingly expressive performance instrument, used by the electronic free impro duo pantoMorf in concerts and recording sessions over the last year.		Palle Dahlstedt;Per Anders Nilsson	2008		10.1007/978-3-540-78761-7_51	mathematical optimization;real-time computing;simulation;mathematics	Robotics	-47.02698520345494	-35.31112869307458	119954
34c6767eafd09c0b396ba0372d9e22c7012a9035	holodesk: direct 3d interactions with a situated see-through display	3d interaction;input device;natural human grasping;3d virtual world;kinect;qualitative study;3d physics interactions;interactive system;see through display;augmented reality ar;augmented reality;3d graphics;quantitative evaluation;target acquisition;physical interaction	HoloDesk is an interactive system combining an optical see through display and Kinect camera to create the illusion that users are directly interacting with 3D graphics. A virtual image of a 3D scene is rendered through a half silvered mirror and spatially aligned with the real-world for the viewer. Users easily reach into an interaction volume displaying the virtual image. This allows the user to literally get their hands into the virtual display and to directly interact with an spatially aligned 3D virtual world, without the need for any specialized head-worn hardware or input device. We introduce a new technique for interpreting raw Kinect data to approximate and track rigid (e.g., books, cups) and non-rigid (e.g., hands, paper) physical objects and support a variety of physics-inspired interactions between virtual and real. In particular the algorithm models natural human grasping of virtual objects with more fidelity than previously demonstrated. A qualitative study highlights rich emergent 3D interactions, using hands and real-world objects. The implementation of HoloDesk is described in full, and example application scenarios explored. Finally, HoloDesk is quantitatively evaluated in a 3D target acquisition task, comparing the system with indirect and glasses-based variants.	3d computer graphics;approximation algorithm;book;cups;direct3d;emergence;input device;interaction;interactivity;kinect;see-through display;situated;virtual world	Otmar Hilliges;David Kim;Shahram Izadi;Malte Weiss;Andrew Wilson	2012		10.1145/2207676.2208405	computer vision;augmented reality;human–computer interaction;computer science;qualitative research;operating system;multimedia;input device;computer graphics (images)	HCI	-42.98828105677287	-37.67192924648031	120036
7a4fffd77122053b8a53ee8c7af7c9b9a80fc3b2	the infopad user interface	portable terminal;full motion video playback;keyboards;voice recognition infopad user interface portable terminal continuous network connectivity high bandwidth radio link pen input handwriting recognition audio input speech recognition full motion video playback synchronized audio text graphics display;handwriting recognition;text graphics display;user interface;user interfaces multimedia systems speech recognition handwriting recognition;prototypes;liquid crystal displays;auditory displays;multimedia systems;high bandwidth radio link;radio link;network connectivity;voice recognition;user interfaces speech recognition keyboards auditory displays workstations large screen displays liquid crystal displays computer interfaces prototypes radio link;workstations;pen input;speech recognition;user interface design;infopad user interface;continuous network connectivity;large screen displays;computer interfaces;synchronized audio;user interfaces;audio input	We have shown a prototype user interface for the InfoPad, a portable terminal with multi-modal input and multimedia output. The InfoPad's main features are: portability, continuous network connectivity using a high-bandwidth radio link, pen input with handwriting recognition, audio input with speech recognition, full-motion video playback with synchronized audio, text/graphics display. The InfoPad's unique input and output characteristics offer challenges and opportunities for user interface design. We have implemented an API for network access to audio, pen, graphics, and video data; we have also implemented speech and handwriting recognizers along with programming interfaces and toolkits. We are prototyping applications and user interfaces to explore how handwriting and voice recognition may best be used together. We believe that the lessons we will learn can be applied to other multi-modal platforms.	access network;application programming interface;finite-state machine;full motion video;graphics;handwriting recognition;input/output;list of toolkits;modal logic;prototype;software portability;speech recognition;user interface design	Andrew J. Burstein;Allan Christian Long;Shankar Narayanaswamy;Richard Han;Robert W. Brodersen	1995	Digest of Papers. COMPCON'95. Technologies for the Information Superhighway	10.1109/CMPCON.1995.512380	speech recognition;computer science;multimedia;computer graphics (images)	HCI	-45.779773722985944	-36.50056978959807	120069
2971b79210ece1c2ee658a51e8e4eeb426a0ddeb	a gesture-driven multimodal interactive dance system	motion capture;real time;real time system;multimodal interaction;leg;gesture recognition;torso;engines;humanities;real time systems;art;feature extraction;arm;space technology	In this paper, we report a real-time gesture driven interactive system with multimodal feedback for performing arts, especially dance. The system consists of two major parts., a gesture recognition engine and a multimodal feedback engine. The gesture recognition engine provides real-time recognition of the performer's gesture based on the 3D marker coordinates from a marker-based motion capture system. According to the recognition results, the multimodal feedback engine produces associated visual and audio feedback to the performer. This interactive system is simple to implement and robust to errors in 3D marker data. Satisfactory interactive dance performances have been successfully created and presented using the reported system	audio feedback;end-to-end principle;gesture recognition;hidden surface determination;interactivity;motion capture;multimodal interaction;performance;real-time locating system;real-time transcription	Gang Qian;Feng Guo;Todd Ingalls;Loren Olson;Jodi James;Thanassis Rikakis	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)		computer vision;motion capture;torso;feature extraction;computer science;multimodal interaction;gesture recognition;multimedia;space technology;arm architecture	Robotics	-39.30383774324297	-37.231895285412484	120147
e8a80c36119a999d4eae22bf02c68476d5eb1688	web-based information access	information resources;human computer interaction;information sources;internet information retrieval scalability remuneration electronic mail proposals read only memory artificial intelligence human computer interaction databases;database management systems;information retrieval;database;bibliographies;information access;online front ends;human factors;internet;user centered;artificial intelligence;easy to access information repository web based information access friendly environments global internet unstructured multimedia data record oriented data interaction techniques global information infrastructure interaction mechanisms information sources permanent novices web sites usable sites user centered db ai hci;bibliographies internet information resources information retrieval online front ends user interfaces interactive systems human factors knowledge based systems database management systems;effective interaction;interactive systems;user interfaces;knowledge based systems;interaction technique;www	The need of friendly environments for effective information access is further enforced by the growth of the global Internet, which is causing a dramatic change in both the kind of people who access the information and the types of information itself (ranging from unstructured multimedia data to traditional record-oriented data). To cope with these new demands, the interaction techniques traditional ly offered to the users have to evolve and eventually integrate in a powerful interface to the global information infrastru cture. The new interaction mechanisms must be especially friendly and easy-to-use, since, given the enormous quantity of information sources available on the Internet, most of the users remain “permanent novices” with respect to each one of the sources they have access to. This tutorial offers a survey of the main approaches adopted for letting the users effectively interact with the Web. Thus, it covers topics related with both extracting the information of interest spread over existing Web sites and building new, more usable, sites. Being mainly “usercentered”, the tutorial will analyze proposals coming from different areas, namely DB, AI, and HCI, which share the final goal of making the Web a huge, easy-to-access, information repository.	decibel;human–computer interaction;information access;information repository;interaction technique;internet;theory;user (computing);world wide web	Tiziana Catarci	1999		10.1109/COOPIS.1999.792149	the internet;human–computer interaction;computer science;artificial intelligence;human factors and ergonomics;operating system;knowledge-based systems;data mining;database;distributed computing;user interface;world wide web;interaction technique	Web+IR	-42.54435251135877	-25.31933102384995	120281
7bafd5e9cb002258c4ab25112eb477759b5b989d	collision avoidance between avatars of real and virtual individuals	real time;collision evasion;trajectory modification;real world integration;collision avoidance;virtual environment;domain specificity	One of the most difficult challenges of associating virtual environments and real-world events deals with integrating real-world, tracked individuals into a virtual environment: while virtual individuals may interact with the avatars of real-world individuals, the latter obviously do not respond to the behaviour of the virtual individuals. To prevent collisions or unrealistic behaviour of either, one needs to ’artificially’ modify the trajectory of the tracked individual. Moreover, after such modifications, one should end up returning that avatar back to the accurate representation of the real world. In this paper we propose a strategy for solving these problems based on generic control functions, which are flexible enough to be fine-tuned on a domaindependent basis. Control functions determine an offset from the tracked individual’s actual trajectory and, when carefully parameterized, guarantee a smooth and automatic recovery after each trajectory modification. We discuss this approach, illustrate it with a variety of Gaussian control functions, analyze the believability of the resulting trajectory modifications, and conclude that control functions provide a sound and powerful basis for improving strategies of collision-avoiding trajectory modification and recovery. Most examples of domain-specific strategies discussed here are taken from the upcoming application area of real-time racing games, which provides both easily recognizable and very attractive situations, and has been the original motivation for this research.	algorithm;avatar (computing);control function (econometrics);control theory;domain-specific language;fourier analysis;real-time clock;real-time computing;requirement;virtual reality	René van den Berg;Juan Manuel Rejen;Rafael Bidarra	2009		10.1007/978-3-642-10347-6_1	real-time computing;simulation;computer science;virtual machine;artificial intelligence;computer graphics (images)	HCI	-38.71673738708115	-37.782682432281476	120469
7566c142dc3176722feb2c3259ff2cb312fbee25	eukaryo: virtual reality simulation of a cell	game engine;virtual reality;biological simulation;eukaryotic cell;cell metabolism;machinery of life	Eukaryo is an interactive, 3-dimensional, simulated bio-molecular world that allows users to explore the complex environment within a biological cell. Eukaryo was developed using Unity, leveraging the capabilities and high performance of a commercial game engine. Through the use of MiddleVR, our tool can support a wide variety of interaction platforms including 3D virtual reality (VR) environments, such as head-mounted displays and large scale immersive visualization facilities.  Our model demonstrates key structures of a generic eukaryotic cell. Users are able to use multiple modes to explore the cell, its structural elements, its organelles, and some key metabolic processes. In contrast to textbook diagrams and even videos, Eukaryo immerses users directly in the biological environment giving a more effective demonstration of how cellular processes work, how compartmentalization affects cellular functions, and how the machineries of life operate.	british informatics olympiad;compartmentalization (information security);diagram;game engine;head-mounted display;simulated reality;simulation;unity;virtual reality	Douglas Yuen;Stephen Cartwright;Christian Jacob	2016		10.1145/2927929.2927931	simulation;human–computer interaction;engineering;multimedia	Visualization	-45.09136778451031	-37.7897996814299	120547
0e808690579e99e99d34fb383e716d70ffb817f6	3d user interfaces for general-purpose 3d animation	graphics architecture;3d animation;mice;motion control;3d user interface;motion pictures;three dimensions;graphical interface;three dimensional user interfaces;virtual reality;virtual reality 3d user interfaces three dimensional user interfaces general purpose 3d animation virtual studio professional platforms virtuality builder ii graphics architecture synthetic worlds graphical interface;synthetic worlds;virtual reality computer animation software packages graphical user interfaces human factors;graphical user interfaces;human factors;general purpose 3d animation;professional platforms;workstations;animation;3d user interfaces;computer animation;rendering computer graphics;virtuality builder ii;user interfaces;user interfaces animation mice graphics motion pictures motion control costs workstations rendering computer graphics timing;virtual studio;graphics;software packages;timing	Modern 3D animation systems allow a rapidly growing community to create and animate increasingly sophisticated worlds. Despite the inherent three-dimensionality of these tasks, the user interfaces of such systems are still predominantly two-dimensional, using 2D input devices and techniques which severely limit the range of tasks that can be accomplished interactively. In particular, these limitations make it very difficult to interactively input complex 3D movements. In this paper, we present Virtual Studio, a 3D animation environment where all the interaction is done directly in three dimensions. 3D devices allow the specification of complex 3D motion while virtual tools are visible mediators that provide interaction metaphors to control application objects. An underlying constraint solver, that automatically maintains multi-way relationships, provides the ability to tightly couple application and interface objects. The animation is defined by recording the effect of the user's manipulations on the models, taking into account the temporal aspect of the interaction. Data reduction techniques are applied to obtain editable representation of continuous parameters' evolution.	computer animation;general-purpose markup language;input device;interactivity;prototype;solver;user interface;virtual studio	Jean-Francis Balaguer;Enrico Gobbetti	1996	IEEE Computer	10.1109/2.532048	computer facial animation;human–computer interaction;computer science;operating system;graphical user interface;virtual reality;computer animation;multimedia;computer graphics (images)	HCI	-41.31384553994291	-34.384786593278044	120704
1a1abb85866f1f86ff172d31fb4ab96dca3dc91b	programmable design environments: integrating end-user programming with domain-oriented assistance	programmable design environment;domain-oriented assistance;programmable applications;programmable design environments;end-user programming;domain-oriented design environments;critics;conceptual framework	Programmable design environments (PDEs) are computational environments that integrate the conceptual frameworks and components of (a) design environments and (b) programmable applications. The integration of these two approaches provides elements (such as software “critics” and “query-able objects”) that assist users in learning both the application and its domain; in addition, an interactive “application-enriched” end-user programming environment stresses the values of expressiveness and modifiability. By way of illustration, we present a newly-developed programmable design environment, SchemeChart, for the domain of charting and information displays.	end-user development	Michael Eisenberg;Gerhard Fischer	1994		10.1145/259963.260432	simulation;human–computer interaction;computer science;operating system;conceptual framework	HCI	-41.60076724849568	-29.326529137986686	120786
8661e2b375fe701256e1f702081dde300f62265f	blindweb maps - an interactive web service for the selection and generation of personalized audio-tactile maps			map;web service	Timo Götzelmann;Laura Eichler	2016		10.1007/978-3-319-41267-2_19	web mapping;multimedia;world wide web;information retrieval	NLP	-42.89445706875879	-24.03758084341734	121010
e2ac0ccc4b7fc2875186f69e1ec8ef81019a7d4e	face-to-avatar: augmented face-to-face communication with aerotop telepresence system	creation;browsing;dynamic information;visualization technique;interactive system;animation;communication;face to face;comic	We have developed a face-to-avatar system that integrates a blimp with a virtual avatar for a unique telepresence system. Our aerotop telepresence system has two advantages comparing with conventional telepresence systems. One is to provide unique communication between user and physical blimp avatar. The blimp works as an avatar and contains several pieces of equipment, including a projector and a speaker. The user's presence is dramatically enhanced compared to using conventional virtual avatars (e.g., CG and images) because the avatar is a physical object that can move freely in the real world. The other is that the user's senses are augmented because the blimp detects dynamic information in the real world. For example, the camera provides the user with a special floating view, and the microphone catches a wide variety of sounds such as conversations and environmental noises. This paper describes our face-to-avatar concept and its implementation.	avatar (computing);cg (programming language);microphone;video projector	Hiroaki Tobita;Takuya Kuzi	2012		10.1145/2254556.2254605	anime;computer vision;human–computer interaction;computer science;multimedia;world wide web;computer graphics (images)	HCI	-44.69470219839543	-37.1493526444482	121184
4d67d33d0ca182a866a71585fb0d13fa32040061	a technique for generating graphical abstractions of program data structures	base donnee visuelle;interpretacion informacion;interpretation information;estructura datos;data visualization;visualisation donnee;structure donnee;information system;data structure;information interpretation;systeme information;sistema informacion;visual databases	Representing abstract data structures in a real programming language is a key step of algorithm implementation and often requires programmers to introduce language-dependent details irrelevant for both a high-level analysis of the code and algorithm comprehension. In this paper we present a logic-based technique for recovering from the loss of abstraction related to the implementation process in order to create intuitive high-level pictorial representations of data structures, useful for program debugging, research and educational purposes.	abstract data type;algorithm;computer graphics;data structure;debugging;high- and low-level;image;programmer;programming language;relevance	Camil Demetrescu;Irene Finocchi	1999		10.1007/3-540-48762-X_97	data structure;computer science;artificial intelligence;theoretical computer science;database;programming language;information system;data visualization;algorithm	PL	-34.051918260449604	-27.974653984945597	121392
dde21068d8b5966ab998a13e75c0ed70c0c5bdb2	querying by photographs: a vr metaphor for image retrieval	3d interface;virtual reality;virtual reality image retrieval layout humans painting navigation computer graphics virtual environment image databases design automation;multimedia databases;query by example;user interfaces content based retrieval visual databases multimedia databases virtual reality;content based retrieval;user interfaces;virtual worlds;image textures querying by photographs content based image retrieval interaction paradigms 3d interfaces virtual reality query by example image database three dimensional interfaces object editing image colors;visual databases;image retrieval	paradigms based on 3D interfaces and virtual reality offer new possibilities to overcome the limitations of query by example. We present a system that lets users navigate a 3D world where they can take photographs to query a database of images by content. Furthermore, users can interactively customize the virtual world by adding objects to the scene and editing object properties such as colors and textures. T he emergence of multimedia technology and the possibility of sharing and distributing image data through largebandwidth computer networks have contributed to an increase of visual data in the global information exchange. Significant advances have been made in the development of efficient compression techniques, but techniques that enable efficient retrieval by content of visual data remain an active research topic. Recently, researchers have developed new tools and interaction paradigms to search for visual information by referring directly to its content. Visual elements such as color, texture, shape, structure, and spatial relationships serve as clues for retrieving images with similar content. The most successful and commonly employed querying interfaces rely on query-by-example paradigms. These paradigms require users to sketch, either from scratch or using prototype images, the content of the image they’re looking for. Although it’s easy to create simple examples, producing significant examples of complex scenes remains a difficult task. The photographer’s metaphor presented in this article facilitates authoring complex examples. Actually, the user takes a photograph of a (customizable) 3D environment rendered by the system. Since the system renders the environment, the paradigm’s effectiveness doesn’t rely on the user’s painting abilities. Background The interaction paradigm of querying by 2D visual examples suits content-based retrieval well. This paradigm requires users to provide a prototype image as a reference example to express perceptual aspects of lowand intermediate-level features of visual content.1,2 Several types of querying by 2D examples have been proposed so far, including	color;emergence;image retrieval;information exchange;interactive media;programming paradigm;prototype;query by example;rendering (computer graphics);texture mapping;virtual reality;virtual world	Jürgen Assfalg;Pietro Pala	2000	IEEE MultiMedia	10.1109/93.839311	computer vision;visual word;image retrieval;computer science;query by example;virtual reality;multimedia;user interface;world wide web	Visualization	-39.45010941421888	-34.20851059570568	121413
f456bc939ce8d0fada39e2a4e012b59cffbce814	switching over to paper: a new web channel	interactive paper;interactive paper web channel web based information interactive information environments mobile client devices document pages information objects user dependent interaction context dependent interaction hypermedia interaction;authoring systems;hypermedia;internet;mobile communication;information systems switches conference management engineering management paper technology physics computing navigation books collaboration indexing;context dependent;mobile communication hypermedia internet authoring systems	We present a general web-based information infrastructure capable of supporting the rapid development of highlyinteractive information environments that cater for widely varying requirements across application domains and all forms of fixed and mobile client devices. In particular, we describe how this infrastructure has been extended to support digitally augmented paper through a special transformation component that can map active areas of document pages to information objects so that userand contextdependent interaction can be supported. Our infrastructure is sufficiently general and flexible to adapt to, not only emerging and even unanticipated technologies in the area of interactive paper, but also the rapidly expanding interaction sphere of hypermedia.	application domain;digital humanities;hypermedia;information management;information system;printing;requirement;usability testing;web application	Moira C. Norrie;Beat Signer	2003		10.1109/WISE.2003.1254484	the internet;mobile telephony;human–computer interaction;computer science;context-dependent memory;web navigation;database;multimedia;law;world wide web	HCI	-42.76077872304268	-26.21550203462917	121467
866774399d2c792993490c46197b09e9a4800a77	the vbow: development of a virtual violin bow controller haptic human-computer interface	human computer interface	This paper describes the development of a virtual violin bow haptic human-computer interface, which senses bow position with encoders, to drive bowed-string physical model synthesis, while engaging servomotors, to simulate the haptic feedback of a violin bow on a string. Construction of the hardware and programming of the software are discussed, as well as the motivation for building the instrument, and its planned uses.	encoder;haptic technology;human–computer interaction;simulation	Charles Nichols	2002			simulation;human–computer interaction;control theory;computer science;haptic technology;violin	HCI	-45.8806377243116	-36.468464028469334	121471
8a7c7df279136d28eb92c29444487841b1244d14	a web-based problem solving environment for solution of option pricing problems and comparison of methods	red www;bolsa valores;pricing;real time;reseau web;option pricing;bourse valeurs;stock exchange;internet;graphical representation;comparative method;world wide web;problem solving environment;fixation prix;scripting language	"""In this paper we present a Problem Solving Environment (PSE) for solving option pricing problems and comparing methods used for this purpose. An open underlying library of methods has been developed to support the functionality of the proposed PSE. PHP, a serverside, cross-platform, HTML embedded scripting language, is exploited to make the proposed environment available through the World Wide Web. The PSE is not addressed to expert users only. It is simple in use, fast and interactive, proposing dynamically selections depending on user's input. The output is returned in """"real time"""", in either simple or graphical representation."""		Minas D. Koulisianis;George K. Tsolis;Theodore S. Papatheodorou	2002		10.1007/3-540-46043-8_68	pricing;stock exchange;the internet;simulation;computer science;artificial intelligence;operating system;valuation of options;comparative method;database;distributed computing;scripting language;programming language;world wide web;computer security;algorithm	AI	-37.51855873579406	-24.77908717469638	121551
6a6fe60fcf416513fd408a594497b9f26a96f4b7	a system to convey the emotional content of text using a humanoid robot	color;light emitting diodes;speech;humanoid robots;artificial intelligence;conferences	Soon we will have anthropomorphic robots that will assist us at home and at work and will be able to recite our text-based communications to us in an animated and engaging manner. This paper describes some steps taken to explore this idea with a simple humanoid robot, the NAO Aldebaran, which is capable of speech of varying pitch, speed and loudness, and has evocative color LED lights around its eyes. To convey the emotional content of text, the proposed system maps ASCII text to these capabilities. The extraction of emotion makes use of the Natural Language Toolkit (NLTK), an open-source tool for computational linguistics using Python. A two-dimensional emotion space is used to map the extracted emotion to robot action. The effectiveness of the system is explored with a simple example of emotional text and a video of the results is provided along with a time-plot of the actions.	computational linguistics;humanoid robot;map;nao (robot);natural language toolkit;open-source software;python;text-based (computing)	Peter Sylvester;David Claveau	2016	2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI)	10.1109/ICTAI.2016.0162	computer vision;computer science;humanoid robot;speech;artificial intelligence;light-emitting diode	Robotics	-45.93450172629327	-36.709814519758254	121561
aa6948e1fbe9bd66595adeb229319480f31deedd	discovering petra: archaeological analysis in vr	cave archaeological analysis collaborative effort petra great temple data visualization virtual reality user interaction cave automatic virtual environment;virtual reality;archaeology;user interfaces virtual reality archaeology data visualisation;data visualisation;cave automatic virtual environment;user interfaces;interaction technique;virtual reality geographic information systems data visualization visual databases information analysis spatial databases failure analysis analytical models data analysis education	New tools give archaeologists access to formerly inaccessible parts of the archaeological record. The result is a demonstrably improved model for inquiry to pose, and answer, important research questions. We chronicle a collaborative effort (from 1997 to the present) with Petra Great Temple archaeologists to augment traditional analysis approaches. We introduce new archaeological analysis tools that combine novel visualization and interaction techniques within a Cave Automatic Virtual Environment (CAVE).		Eileen Vote;Daniel Acevedo Feliz;David H. Laidlaw;Martha Sharp Joukowsky	2002	IEEE Computer Graphics and Applications	10.1109/MCG.2002.1028725	cave automatic virtual environment;human–computer interaction;computer science;artificial intelligence;virtual reality;user interface;interaction technique;data visualization;computer graphics (images)	Visualization	-33.835980002888604	-31.25799239532634	121577
478f99dcb289652ebac2a05fbcc609e7849f82c5	speech-recognition interfaces for music information retrieval: 'speech completion' and 'speech spotter'		This paper describes music information retrieval (MIR) systems featuring automatic speech recognition. Although various interfaces for MIR have been proposed, speech-recognition interfaces suitable for retrieving musical pieces have not been studied. We propose two different speech-recognition interfaces for MIR, speech completion and speech spotter, and describe two MIR-based hands-free jukebox systems that enable a user to retrieve and play back a musical piece by saying its title or the artist’s name. The first is a music-retrieval system with the speech-completion interface that is suitable for music stores and car-driving situations. When a user can remember only part of the name of a musical piece or an artist and utters only a remembered fragment, the system helps the user recall and enter the name by completing the fragment. The second is a background-music playback system with the speech-spotter interface that can enrich human-human conversation. When a user is talking to another person, the system allows the user to enter voice commands for music-playback control by spotting a special voice-command utterance in face-to-face or telephone conversations. Our experimental results from use of these systems have demonstrated the effectiveness of the speech-completion and speech-spotter interfaces.	information retrieval;speech recognition;speech synthesis	Masataka Goto;Katunobu Itou;Koji Kitayama;Tetsunori Kobayashi	2004			speech recognition;conversation;voice command device;music information retrieval;multimedia;utterance;recall;computer science	Web+IR	-47.65409236997696	-37.20617023556194	121589
4ea2a71928ee3e49a5d55ae7b320a89bf83cc323	the augmented violin project: research, composition and performance report	augmented violin;bowing styles;hyper instrument;mapping	In this paper we present the augmented violin developed at IRCAM. This instrument is an acoustic violin with added sensing capabilities to measure the bow acceleration in realtime. We explain first the approach we developed to characterize bowing styles. Second, we describe the realtime implementation of the bowing style recognition system. Finally we describe an electro-acoustic music composition, Bogenlied, written for the augmented violin.	acoustic coupler;acoustic cryptanalysis;real-time computing	Frédéric Bevilacqua;Nicolas H. Rasamimanana;Emmanuel Fléty;Serge Lemouton;Florence Baschet	2006			simulation;speech recognition;acoustics	Robotics	-46.03575414059887	-36.29431975683061	121657
390b986084a76c56fbf726978682767ccf1def86	movie maps	computer vision;data visualisation;content based retrieval;image sequences	This paper presents methods for moving image sequence visualization and browsing based on algorithms from computer vision, information visualization and on a hierarchical model for content semantics. We introduce a new method, OM-Images, for the visualization of temporal changes in a moving image sequence. Together with interactive browsing techniques the visualization methods can be used for the exploration of a movie at different levels of abstraction. The proposed levels of abstraction are the physical, image, object or discourse level. The visualization is used to generate 1) static descriptions, which printed on paper yield a “movie book” and 2) interactive documents, e.g. web pages or special movie browsers. Finally, we give examples of a movie book of a feature length film.	algorithm;computer vision;hierarchical database model;information visualization;principle of abstraction;printing;web page	Heimo Müller;Ed Tan	1999		10.1109/IV.1999.781581	computer vision;information visualization;computer science;multimedia;computer graphics (images)	Visualization	-34.21043191672586	-34.30615343785147	121773
c08de98f4da391d33bbec4dc3ae08048a967b284	a time series based solution for the difference rate sampling between haptic rendering and visual display	haptic display;prediction method;time series computer displays differential equations haptic interfaces realistic images;real time;virtual reality;extrapolation;complex deformable objects;time series;deformable objects;virtual reality time series haptic rendering extrapolation deformable object;haptic rendering;real time haptic display;realistic visual display time series difference rate sampling haptic rendering system force display complex deformable objects real time haptic display;computer displays;force display;deformable object;realistic images;difference rate sampling;differential equations;realistic visual display;sampling methods haptic interfaces displays deformable models force sensors extrapolation minimally invasive surgery predictive models frequency rendering computer graphics;haptic interfaces;haptic rendering system;deformable model	In the haptic rendering system that allows the force display of complex deformable objects, the coherence remains difficult because of the different refresh frequencies necessary for real-time haptic display and realistic visual display. In this paper, a time series based predict method is proposed to extrapolate the force computed by the deformable model to go beyond interactively to haptic real-time. The principle of the time series method is introduced, and then a detailed analysis and experimental verification of the approach are described and illustrated. In the experiment, our method is compared with other two extrapolation methods and shows its feasibility.	extrapolation;haptic technology;interactivity;real-time clock;rendering (computer graphics);time series	Juan Wu;Aiguo Song;Jianqing Li	2006	2006 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2006.340267	computer vision;simulation;computer science;time series;virtual reality;extrapolation;differential equation;statistics;computer graphics (images)	Robotics	-39.615602300319736	-36.81709347082772	121784
27139752fc7d6379eb14faa04919cb37e6f6c86a	visualizing volume data using physical models	visualization;image segmentation;physical models;physical model;data visualisation	Visualization techniques enable scientists to interactively explore 3D data sets, segmenting and cutting them to reveal inner structure. While powerful, these techniques suffer from one serious flaw-the images they create are displayed on a flat piece of glass or paper. It is not really 3D-it can only be made to appear 3D. We describe the construction of 3D physical models from volumetric data. Using solid freeform fabrication equipment, these models are built as separate interlocking pieces that express in physical form the segmentation and cutting operations common in display-based visualization.	3d printing;computation;downstream (software development);flaw hypothesis methodology;graph drawing;ibm notes;interactivity;san diego supercomputer center;scalability;scene graph;scientific visualization;volume rendering	David R. Nadeau;Michael J. Bailey	2000	Proceedings Visualization 2000. VIS 2000 (Cat. No.00CH37145)	10.1145/375213.375314	computer vision;physical model;computer science;data visualization;computer graphics (images)	Visualization	-34.185455018388225	-31.250973229624595	121827
e2fd933a7279a4980e65bab3d9621f9100597961	new frontiers in music education through the ieee 1599 standard	settore inf 01 informatica	IEEE 1599 is an international standard conceived for a comprehensive description of music. Inside a unique XML document, all the data and meta-data related to a single music piece can be coded, ranging from catalogue to structural, from notational to performance, from audio to graphic information. This format is particularly fit for computer-supported music education, since it allows a number of applications such as evolved score following, real-time comparison among different performances, studies on score transcriptions, musicological analyses, etc. After describing the key features of the standard, a number of case studies will be presented in order to demonstrate its applicability to music education.	computer;encode;interoperability;online and offline;performance;real-time clock;xml	Adriano Baratè;Luca A. Ludovico	2012			telecommunications;computer science;software engineering	DB	-47.039361212600845	-24.285186269217437	121983
0d807c3410b930a83646fdd35ec74aecb77daa32	on the power of semantic partitioning of web documents.	web documents	A growing number of Web sites are maintained by content management software and thus a large number of Web pages are machine-generated via templates. Normally in such Web pages there is implicitly a fixed “schema” and what changes is the content. Informally a schema for a Web page represents concepts and relationships among them in a hierarchical fashion. For example, Figure 1 is a screen shot of the New York Times front page (see http://www.nytimes.com). Observe that this page includes: (i) a taxonomy of items such as “NEWS” (consisting of hyperlinks labeled with “International”, “National”, ...), “OPINION” (consisting of hyperlinks “Editorial/Op-Ed”, ...), etc.; (ii) several headlines of news articles where each article begins with a hyperlink labeled with the news headline (e.g., “Bush tells Nation ...”) followed by the author of the article (e.g., “By Richard W. Stevenson ...”), followed by a time-stamp and a text summary of the article (e.g., “President Bush portrayed ...”). The schema for this fragment of the New York Times front page therefore includes the taxonomy (which does not change) and the template for the news article. We should point out that the schema will also include several additional elements pertaining to other content appearing in the page.	basic stamp;database schema;hyperlink;screenshot;semantic analysis (compilers);space partitioning;taxonomy (general);the new york times;web content;web page	Guizhen Yang;Saikat Mukherjee;Wenfang Tan;I. V. Ramakrishnan;Hasan Davulcu	2003			web service;web mining;static web page;web development;web modeling;data web;web mapping;web design;web standards;computer science;semantic web;web navigation;social semantic web;web page;semantic web stack;web intelligence;web 2.0;world wide web;website parse template;semantic analytics;web server	Web+IR	-45.735550705189866	-24.17099485039907	122593
6fb156482a5e6f2220f5fe381292e422b65c0afe	framework for interpreting handwritten strokes using grammars	prototipificacion rapida;grammar;interfase usuario;caracter manuscrito;user interface;manuscript character;relacion hombre maquina;man machine relation;rapid prototyping;grammaire;pattern matching;aparato visual;interface utilisateur;appareil visuel;relation homme machine;concordance forme;caractere manuscrit;visual system;gramatica;prototypage rapide	To support the rapid development of pen-based structured diagram editors, we propose a framework for describing such editors. The framework uses grammar to describe the context, i.e., the positional relationship between handwritten strokes and other objects, which can be used to interpret ambiguous results of pattern matching, and to describe the syntax of the target diagrams. We implemented the framework by extending our visual system, which supports the rapid prototyping of structured diagram editors.	diagram;pattern matching;rapid prototyping	Buntarou Shizuki;Kazuhisa Iizuka;Jiro Tanaka	2004		10.1007/978-3-540-27795-8_41	natural language processing;visual system;computer science;artificial intelligence;operating system;pattern matching;grammar;programming language;user interface;algorithm	HCI	-38.3509920132618	-28.096507404418738	122850
c50b8bb26e432c897db91a306e15a264ee96f9ea	a state of the art report on research in multiple rgb-d sensor setups		That the Microsoft Kinect, an RGB-D sensor, transformed the gaming and end consumer sector has been anticipated by the developers. That it also impacted in rigorous computer vision research has probably been a surprise to the whole community. Shortly before the commercial deployment of its successor, Kinect One, the research literature fills with resumees and state-of-the art papers to summarize the development over the past 3 years. This particular report describes significant research projects which have built on sensoring setups that include two or more RGB-D sensors in one scene.	algorithm;computer vision;kinect;motion capture;needham–schroeder protocol;scientific literature;sensor;software deployment	Kai Berger	2013	CoRR		computer vision;simulation;multimedia	HCI	-33.77078943857841	-37.07989193999985	122928
5be4c35ea5513a34c3ca16caec20c7321249b88f	practical character physics for animators	musical interfaces;multi touch interaction;computer vision;character animation;animated character	Physical realism is an important aspect of producing convincing 3D character animation. This is particularly true for live-action visual effects where animated characters occupy the same scene as the live actors. In such a scenario, a virtual character's movements must visually match the behavior and movements of the live environment, else the discrepancy will be obvious to the viewer.	discrepancy function;visual effects	Ari Shapiro;Sung-Hee Lee	2009	IEEE Computer Graphics and Applications	10.1145/1597990.1598045	character animation;computer vision;skeletal animation;computer science;multimedia;computer graphics (images)	Graphics	-40.07796343248706	-35.87431971676308	122960
bb434a6df945270b69d74be7bd535bdff93851c0	dancedj: a 3d dance animation authoring system for live performance		Dance is an important component of live performance for expressing emotion and presenting visual context. Human dance performances typically require expert knowledge of dance choreography and professional rehearsal, which are too costly for casual entertainment venues and clubs. Recent advancements in character animation and motion synthesis have made it possible to synthesize virtual 3D dance characters in real-time. The major problem in existing systems is a lack of an intuitive interfaces to control the animation for real-time dance controls. We propose a new system called the DanceDJ to solve this problem. Our system consists of two parts. The first part is an underlying motion analysis system that evaluates motion features including dance features such as the postures and movement tempo, as well as audio features such as the music tempo and structure. As a pre-process, given a dancing motion database, our system evaluates the quality of possible timings to connect and switch different dancing motions. During run-time, we propose a control interface that provides visual guidance. We observe that disk jockeys (DJs) effectively control the mixing of music using the DJ controller, and therefore propose a DJ controller for controlling dancing characters. This allows DJs to transfer their skills from music control to dance control using a similar hardware setup. We map different motion control functions onto the DJ controller, and visualize the timing of natural connection points, such that the DJ can effectively govern the synthesized dance motion. We conducted two user experiments to evaluate the user experience and the quality of the dance character. Quantitative analysis shows that our system performs well in both motion control and simulation quality.	control function (econometrics);expectation propagation;experiment;kinesiology;online and offline;performance;poor posture;preprocessor;real-time locating system;real-time transcription;simulation;speech synthesis;user experience;user interface;viewtiful joe	Naoya Iwamoto;Takuya Kato;Hubert P. H. Shum;Ryo Kakitsuka;Kenta Hara;Shigeo Morishima	2017		10.1007/978-3-319-76270-8_46	computer animation;multimedia;human–computer interaction;computer science;computer facial animation;motion control;animation;user experience design;dance;choreography;character animation	Graphics	-40.50631446115567	-35.973438054334395	123284
4dbd03a2d2e71af82204544fb0ee621b3dfe1303	"""monalisa: """"see the sound , hear the image"""""""	installation;sound and image processing software;plug-in;image processing	"""Monalisa is a software platform that enables to """"see the sound, hear the image”. It consists of three software: Monalisa Application, Monalisa-Audio Unit, and Monalisa-Image Unit, and an installation: Monalisa “shadow of the sound”. In this paper, we describe the implementation of each software and installation with the explanation of the basic algorithms to treat the image data and the sound data transparently."""	algorithm	Kazuhiro Jo;Norihisa Nagano	2008			petroleum engineering;metalworking fluid;human–computer interaction;computer science;emulsion;acoustics	SE	-45.79235754972312	-34.429276335631315	123667
7fce669522ca3ec5124113158e6cea6ec68e1086	crowd simulation based on constrained and controlled group formation	crowd motion;3d simulation;artificial bee colony algorithm;freestyle formations	Freestyle formations appear widely in animation of groups. Most existing algorithms for generating special formations focus on the visualization performances of target formations, while social dynamics factors in the process of crowd motion are ignored. Thus, disregarding those factors will decrease the bionic features and fidelity of the crowd motion. According to this problem, a method based on bionic intelligence algorithm and self-adaptive evaluation to generate special formations is proposed in this paper. Simulation effect with good fluency and lively interaction is generated by means of user interaction, data analysis and crowd motion. In this method, 3D reconstruction is used to repaint characters, graphics or patterns in the 3D modeling system to build the basic virtual scene. Then, station points are generated through interlacing cross sampling. Based on the concentric circles model of fitness, each individual, self-adaptively, chooses a target station point which matches it aptly. Finally, the Artificial Bee Colony algorithm is used for path planing to generate the optimum route to the destination without collision. Visual simulation experiments are also made on the platforms of ACIS/HOOPS and Maya. The results show that this method can generate the optimum target formation with natural motion features and in accordance with users’ input. This method is also insensitive to the scale of crowd, exhibiting good performance when the number of individuals is large.	3d acis modeler;3d modeling;3d reconstruction;artificial bee colony algorithm;autodesk maya;computation;computational model;crowd simulation;distributed computing;experiment;fitness function;freestyle;hoops 3d graphics system;human–computer interaction;interactivity;interlacing (bitmaps);lively kernel;motion planning;parallel computing;performance;planning;sampling (signal processing);social dynamics;swarm intelligence	Peng Zhang;Hong Liu;Yanhui Ding	2013	The Visual Computer	10.1007/s00371-013-0900-7	computer vision;simulation;computer science;artificial intelligence;machine learning;crowd simulation;mathematics;artificial bee colony algorithm;computer graphics (images);mechanical engineering	Robotics	-39.61481338234661	-35.78135758019436	123772
6131a330ceb3b0f6d150e68b3208ffa9833ac69d	geometric differentiation for the intelligence of curves and surfaces		"""The regular type of help documentation is really a hard copy manual that's printed, nicely bound, and functional. Itoperates as a reference manual skim the TOC or index, get the page, and stick to the directions detail by detail.The challenge using these sorts of documents is the fact that user manuals can often become jumbled and hard tounderstand. And in order to fix this problem, writers can try and employ things i call """"go over here"""" ways tominimize the wordiness and simplify this content. I've found this approach to be extremely ineffective most of thetime. Why? Because geometric differentiation for the intelligence of curves and surfaces are considered unsuitable to get flippedthrough ten times for just one task. That is what online assistance is for."""	documentation;optical disc authoring;printing;smart common input method	Ian R. Porteous	1994				HCI	-45.724849551701475	-29.311996740735356	123946
b9ef007a220bc92f583b487941663a7e298fc880	combining gestures and graphical elements for collaboration using multi-touch surfaces	graphical interface;multi user gestures graphical interface collaboration innovation interactive;knowledge management;collaboration;text analysis;clipboard mechanism graphical elements multitouch surfaces gestures elements image representation text representation freehand drawings collaborative process fan like menu interactive surface knowledge management system hierarchical conceptual structures undo redo stack;multi user;interactive;graphical user interfaces;innovation;technological innovation collaboration surface treatment fingers graphical user interfaces organizations;gestures;data structures;image representation;text analysis data structures gesture recognition graphical user interfaces image representation knowledge management tactile sensors;knowledge management system;tactile sensors;gesture recognition	We propose a set of gestures and graphical elements as a means to introduce and manipulate ideas represented by images, text, and freehand drawings around a horizontal multi-touch surface in the context of collaborative processes. A fan-like menu that can be obtained from any point on the interactive surface is one of the key proposed graphical elements. We also propose a knowledge management system for this scenario, which allows for the organization of ideas into hierarchical conceptual structures. We present two innovative solutions to challenging issues related with the undo/redo stack and the clipboard mechanism for interactive surfaces.	graphical user interface;multi-touch	Yazmín Magallanes-Velázquez;Ariel Molina-Rueda;J. Alfredo Sánchez Huitrón	2012		10.1109/CONIELECOMP.2012.6189904	computer vision;human–computer interaction;computer science;gesture recognition;graphical user interface;multimedia	HCI	-35.77053508530397	-34.007397619364504	124005
7e34beb60917a24a9889f7a69cd77019177f803b	computational steering in the cave	3d interaction;computational steering;3d graphics and interaction;user interface;virtual environments;three dimensional;support system;on the fly;virtual environment;3d graphics	Scientists can gain much more insight from their simulations if they are enabled to change simulation parameters on the y while observing the results immediately. A crucial aspect of such computational steering is an intuitive user interface. We have developed an environment that enables researchers to construct such interfaces e ciently and e ectively for graphical workstations. In this paper we report on our next step towards more intuitive user-interfaces: We have modi ed our system for use in the CAVE. The CAVE is a projection-based virtual environment. Virtual environments are designed to provide the e ect of immersion in an interactive three-dimensional computer-generated environment. We show that the use of virtual environments for computational steering interfaces can improve interaction with the simulation and immersion in the computational process. We present our system, the methods we have developed for improved 3D interaction, and describe three applications.	3d interaction;computation;computational steering;computer-generated holography;graphical user interface;immersion (virtual reality);simulation;virtual reality;workstation	Jurriaan D. Mulder;Robert van Liere;Jarke J. van Wijk	1998	Future Generation Comp. Syst.	10.1016/S0167-739X(98)80023-X	three-dimensional space;simulation;human–computer interaction;computer science;virtual machine;operating system;user interface;3d computer graphics;computer graphics (images)	HPC	-41.891529105554085	-36.09395875560983	124132
85d1866db4e980ddcc4be348876d3ad60b4c7a2f	optimizing web-based virtual reality models	virtual reality;visualization;virtual reality internet user interfaces;computational modeling;internet;shape;three dimensional displays;image color analysis;user experience;web modelers;solid modeling;x3d;solid modeling optimization computational modeling three dimensional displays shape visualization image color analysis;structure tolerance;vrml;world wide web;appearance tolerance;optimization;web authors;virtual environment;web based modeling;appearance tolerance web based virtual reality model user experience web authors web modelers structure tolerance;vrml web based modeling virtual reality optimization x3d;user interfaces;web based virtual reality model	Optimizing the virtual reality model is a necessity to cope with the nature of the World Wide Web. Virtual reality scenes should load within an acceptable time for the user's experience and the sense of being immersed in the virtual environment is not to be affected. In this study, we propose to optimize web-based virtual reality models by removing redundant objects within the scenes, while keeping the model readable to web-authors and modelers. The achieved reductions in the model file size and in the model parsing time were further reduced by introducing structure and appearance tolerance.	human-readable medium;interpolation;mathematical optimization;optimizing compiler;parsing;virtual reality;web application;world wide web	Rana A. Kader El-Bahnasawy;Iman A. El-Azab;Hoda M. Onsi	2010	2010 10th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2010.5687060	the internet;vrml;visualization;shape;computer science;virtual machine;virtual reality;multimedia;x3d;solid modeling;user interface;computational model;world wide web;computer graphics (images)	Visualization	-37.90961626741557	-33.27347111414173	124142
f41a3772fce2e7294a9a2ed5288ac6f7d00af9e8	designing the interface designer's interface	user interface management system;user interface;direct manipulation;visual programming;user interface design	The concepts of a user interface management system (UIMS) and user interface designer have become well known in the user interface and graphics community. Most UIMSs developed so far have concentrated on the efficiency of generating the user interface; the user interface designer's interface has received relatively little attention. We identify the important features of the user interface designer's interface. A UIMS incorporating these features in its interface has been developed, and is described in this paper.	graphics;user interface management systems	Gurminder Singh;Mark Green	1988		10.1145/62402.62422	user interface design;user;interface description language;10-foot user interface;human action cycle;interface metaphor;shell;human–computer interaction;natural language user interface;computer science;console application;interface control document;multimedia;natural user interface;visual programming language;interactivity;user interface;world wide web;graphical user interface testing;multiple document interface	HCI	-42.42438948174374	-30.495319514719792	124152
22ed6b0b365243a0a4a1a1e0ea9707e883ab17a1	hip-storytelling : hand interactive projection for storytelling	hci;natural user interfaces;interactive storytelling;augmented reality	We have created an interactive storytelling system for public spaces that is collaborative, easy and entertaining to use, and allows for a natural interaction. The system consists of a table, a ceiling mounted projector that projects onto the table, and a 3D camera for tracking hands and for object recognition. The main feature of the system is the projection on top of the palm of the hand of the users; thus, the hand becomes also a viewing surface. This allows for very natural gestural interaction, such as holding and passing objects between users. For example, in the course of narrative, users can hand over a story character or object to each other. We also employ projection based augmented reality to animate real objects on the table. Apart from entertainment, the system shall be employed for concrete educational interactive storytelling applications in public spaces.	augmented reality;gesture recognition;interactive storytelling;outline of object recognition;video projector	Narciso Melo;Pedro Salgado;Ido Iurgel;Pedro Branco	2011		10.1007/978-3-642-25289-1_36	computer vision;augmented reality;human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-43.13907002059628	-37.91129193182898	124343
c7b4b03d1dbaa31471cac0b274a23cbadf545992	character workflow of final fantasy xv	character;work flow;hair creating	When creating the characters for Final Fantasy XV, we were required to create assets that were convincing and seemed like they could exist in the real world. In addition to PBR as a base technology we also used things like Linear Workflow, a skin shader with subsurface scattering effects, and an eye shader with a corneal refractive structure and two-layer reflective parameters. We will now explain the specific workflow we used to create attractive character assets with the underlying support of these technologies.	pbr theorem;shader;subsurface scattering	Kazutaka Kurosaka;Eitaro Iwabuchi	2016		10.1145/2897839.2927448	workflow;computer vision;simulation;computer science;artificial intelligence;operating system;character;computer graphics (images)	HCI	-40.38442546072181	-33.45697124246313	124698
3eda3ae053f6775e51e1366d459638f8c3c36af0	realizing seamless interaction: a cognitive agent architecture for virtual and smart environments	smart environment;agent architecture;human computer interface;virtual environment	We propose a cognitively motivated vertically layered two-pass agent architecture for realizing responsiveness, reactivity, and pro-activeness of smart objects, smart environments, virtual characters, and virtual place controllers, that is controllers for lighting and weather conditions. Being cognitively motivated, our approach aims to respect the cognitive demands of a human being, in order to deliver an adequate human-computer interface to ubiquitous and virtual environments. The vertically layered two-pass architecture allows to realize reaction within a certain time frame, as necessary for generating responsiveness and reactivity required for natural interaction, while at the same time providing a layer for further processing of information for creating pro-active, intelligent responses.	agent architecture;human–computer interaction;responsiveness;seamless3d;smart environment;smart objects;virtual reality	Youngho Lee;Hedda Rahel Schmidtke;Youngjung Suh;Woontack Woo	2007			human–computer interaction;computer science;architecture;virtual machine;smart environment;smart objects;cognition;agent architecture	AI	-47.030775478115665	-37.54258542266114	124820
c511a32a6e1e24e29db6c55c8e63ac73730d11c7	visual knowledge negotiation		We ask how users interact with ‘knowledge’ in the context of artificial intelligence systems. Four examples of visual interfaces demonstrate the need for such systems to allow room for negotiation between domain experts, automated statistical models, and the people who are involved in collecting and providing data.	artificial intelligence;statistical model	Alan F. Blackwell;Luke Church;Y Abhishek;Mariana Marasoiu	2018	2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)	10.1109/VLHCC.2018.8506553	human–computer interaction;visualization;ask price;data modeling;negotiation;statistical model;computer science	SE	-34.60906956400116	-25.710776197293992	124912
1759cc1fd060964905d8cd2986e8ab4be0252bf6	timbrefields: 3d interactive sound models for real-time audio	3d interaction;interaction point;high dimensionality;efficient algorithm;real time;virtual reality;real time simulation;space use;vector field;product space	We describe a methodology for virtual reality designers to capture and resynthesize the variations in sound made by objects when we interact with them through contact such as touch. The timbre of contact sounds can vary greatly, depending on both the listener’s location relative to the object, and the interaction point on the object itself. We believe that an accurate rendering of this variation greatly enhances the feeling of immersion in a simulation. To do this, we model the variation with an efficient algorithm based on modal synthesis. This model contains a vector field that is defined on the product space of contact locations and listening positions around the object. The modal data are sampled on this high dimensional space using an automated measuring platform. A parameter-fitting algorithm is presented that recovers the parameters from a large set of sound recordings around objects and creates a continuous timbre field by interpolation. The model is subsequently rendered in a real-time simulation with integrated haptic, graphic, and audio display. We describe our experience with an implementation of this system and an informal evaluation of the results.	algorithm;algorithmic efficiency;approximation;binaural beats;graphics;haptic technology;head-related transfer function;immersion (virtual reality);interpolation;modal logic;multimodal interaction;real-time cmix;real-time clock;remote control;robot;simulation;virtual reality	Richard Corbett;Kees van den Doel;John E. Lloyd;Wolfgang Heidrich	2007	PRESENCE: Teleoperators and Virtual Environments	10.1162/pres.16.6.643	computer vision;vector field;simulation;human–computer interaction;product topology;computer science;virtual reality;computer graphics (images)	Graphics	-45.25520482290455	-34.92736704460116	124956
0bc590bda15151d6f7fe03a2625e25fc0a8bc942	real-time animations of virtual fountains based on a particle system for visualizing the musical structure	musical structure;music computer animation data visualisation entertainment;visualization;visualization three dimensional displays instruments process control java animation games;animation;particle system;musical structure animation particle system visualization;generated animations musical structure visualization 3d particle system mapping algorithm real time animations virtual emitter fountains visual patterns musical compositions visual appreciation entertainment music perception music composition visual attributes	We present a 3D particle system and a mapping algorithm to generate real-time animations of virtual emitter fountains, choreographed by music. Adittionally, we included two different examples of use to demonstrate the main system's aspects, its functionality and feasibility. In particular, we show that is possible to identify different visual patterns that match both the themes of the musical compositions, as examples of how the system can be used not only for visual appreciation and entertainment, but also as a support tool for music perception and composition. Furthermore, we have conducted studies with users to evaluate the system. The results show a strong relationship between the perception of musical terms and visual attributes, besides providing positive and useful feedback on the effectiveness of the generated animations.	algorithm;lambert's cosine law;particle system;real-time clock;real-time transcription	Joyce Horn Fonteles;Maria Andréia F. Rodrigues;Victor Emanuel Dias Basso	2014	2014 XVI Symposium on Virtual and Augmented Reality	10.1109/SVR.2014.35	computer vision;computer science;multimedia;computer graphics (images)	HCI	-44.09067510727752	-34.25235356006965	125243
0ae92fcd6d8bc6d8c4fa5a35b372d2bd2cb62f03	video mosaic: laying out time in a physical space	storyboards;user interface;paper user interfaces;video editing;augmented reality	Paper video storyboards are still in use by even very experienced video producers with access to the most advanced video editing software. An analysis of the characteristics of paper and on-line editing provide an overlapping but distinct set of benefits (and problems). Paper provides the user with the ability to lay out various temporal sequences over a large spatial area and the ability to quickly sketch, annotate and rearrange the relevant video clips. On-line editing provides users with the ability to generate and store a variety of video arrangements. Video Mosaic provides users with the ability to combine the best of both worlds: elements of a paper video storyboard are used as input to an on-line video editing system to take advantage of the best aspects of each. We developed a Unix and a Macintosh version of Video Mosaic. This paper describes the design of Video Mosaic, compares alternative approaches to creating this type of application, and suggests directions for future work.	ncsa mosaic;online and offline;storyboard;unix;video clip	Wendy E. Mackay;Daniele Pagani	1994		10.1145/192593.192646	computer vision;augmented reality;post-production;computer science;operating system;video tracking;multimedia;video processing;smacker video;user interface;world wide web;computer graphics (images);non-linear editing system	HCI	-43.91752725387355	-32.63774543560009	125264
4c4968136cb64aeed5ad175a1ab8a3c57dca2ccc	rectangular cartograms: the game	game;cartograms	Raisz [3] introduced rectangular cartograms in 1934 as a way of visualizing spatial information, such as population or economic strength, of a set of regions like countries or states. Rectangular cartograms represent geographic regions by rectangles; the positioning and adjacencies of the rectangles are chosen according to their geographic locations, while their areas are proportional to the numeric values being communicated by the cartogram. Rectangles have the advantage that the sizes (area) of the regions can be estimated easily. On the other hand the rectangular shape is less recognizable and imposes limitations on possible layouts of the cartogram. In recent years several algorithms [2, 4, 5] were developed to efficiently compute rectangular cartograms. This note describes a game that was inspired by these algorithmic results. The game was initially developed as part of the outreach effort of TU Eindhoven, and it has been used at TU Eindhoven’s Open Day. The goal of the Open Day is to popularize the research activities of the various departments; the audience are often families with children in the age 6–15, or parents of (prospective) students. The goal of our game was thus to show in an entertaining way the difficulties one faces when developing algorithms for automatic cartogram construction. The game was quite popular, especially with children. It is an applet and can be found at http://www.win.tue.nl/~speckman/demos/game/. Following [4, 5] the cartograms used in the game are each based on a rectangular layout : a subdivision of a rectangle into finitely many interior-disjoint rectangles. Each region of the input map corresponds to a rectangle of the layout, in addition the layout may contain additional “sea rectangles”	algorithm;applet;prospective search;subdivision surface	Mark de Berg;Fred van Nijnatten;Bettina Speckmann;Kevin Verbeek	2009		10.1145/1542362.1542382	games;mathematics;cartogram	HCI	-36.212157579744435	-30.19224635945208	125268
2a6893f1c62625e6a8d8ce3ead9d210132c5a9a4	bumps: a program for animating projections	planar geometric projections;core graphics system;computer graphic;graphics system;animation;viewing transformations;interactive computer graphics	BUMPS (Brown University Multiple Projection System) is a program that illustrates the implementation of viewing transformations using animation. The program uses the viewing model defined in the Core Graphics System. BUMPS employs interactive computer graphics to demonstrate how planar geometric projections are generated, what the effects of different projections and projection parameters are on the projected object, and how the viewing functions of the Core Graphics System work. After presenting background material on projections, the features of BUMPS are described, followed by a pictorial user scenario of BUMPS in action. The paper concludes with a discussion of the merits of user controlled animation for teaching and possible improvements to the program.	computer graphics;graphical projection;human–computer interaction;image;quartz (graphics layer);scenario (computing)	Robert F. Gurwitz;Richard W. Thorne;Andries van Dam;Ingrid Carlbom	1980		10.1145/800250.807498	anime;computer vision;2d computer graphics;simulation;computer facial animation;computer science;real-time computer graphics;computer animation;graphics software;computer graphics;3d computer graphics;computer graphics (images)	Graphics	-40.06738488502541	-31.86588441487027	125423
012909234b7642b9a8c6619a45ea8e6c99fe8db4	jpeg2000 image adaptation for mpeg-21 digital items	mpeg 21 digital item adaptation;mobile device;personal digital assistant;image adaptation;universal multimedia access;adaptive system;jpeg2000;small screen device;visual attention;digital item adaptation	MPEG-21 user cases bring out a scenario of Universal Multimedia Access which is becoming the reality: people use different devices such as desktop PC, personal digital assistant as well as smartphone to access multimedia information. Viewing images on mobile devices is more and more popular than before. However, due to the screen size limitation, the experience of viewing large image on small screen devices is awkward. In this paper, an enhanced JPEG2000 image adaptation system is proposed for MPEG-21 digital item adaptation. The image is adapted considering both visual attentive region(s) of image and terminal screen size. Through the subjective testing, the system has been approved to be a solution of efficiently displaying large images in different devices.	attentive user interface;desktop computer;digital item;display size;interoperability;jpeg 2000;mpeg-21;mobile device;personal digital assistant;region-based memory management;smartphone;television;test set	Yiqun Hu;Liang-Tien Chia;Deepu Rajan	2004		10.1007/978-3-540-30541-5_58	computer vision;simulation;computer science;adaptive system;digital image processing;mobile device;jpeg 2000;multimedia;world wide web	HCI	-46.15036719081736	-31.806199515018605	125480
0717eb0a71bc7dbdfdb9e8a722ceccdb72a7f670	style control in the quill document editing systems	interfase usuario;text editor;edition electronique;user interface;document preparation systems;sistema informatico;computer system;sgml;wysiwyg;edicion electronica;traitement document;burotica;editor texto;controle style;interface utilisateur;editeur texte;systeme informatique;electronic publishing;document processing;structured documents;user interfaces;office automation;bureautique;tratamiento documento	A critical problem in the design of editors for structured documents is that of style control, i.e. mapping the logical elements of the documents to their physical appearance on pages. This paper presents a novel approach to style control, used in the Quill document editing system that has been prototyped at the IBM Almaden Research Center. In our approach, the style control mechanism is an integral part of the editing system and consistent with the overall system architecture, in both its inner structure and its user interface. Properties that specify the formatting process, together with action routines for specifying complex semantics, are the basic style control primitives in the proposed approach.		Yaron Wolfsthal	1991	Softw., Pract. Exper.	10.1002/spe.4380210606	style sheet language;computer science;operating system;multimedia;electronic publishing;user interface;world wide web;computer graphics (images)	SE	-38.56972471638156	-27.934881063735762	125597
7ee3bc26dc2f3f7961f221bedff011c8e5f85b9c	the pumapaint project	tecnologia industrial tecnologia mecanica;robotic art;grupo de excelencia;red green blue;online robotics;telerobotics;world wide web;tecnologias	The PumaPaint Project is an online robot that allows World Wide Web users to create original artwork. This paper describes the PumaPaint Project at two locations: the original site at Wilkes University and the new site at Roger Williams University. Each site allows control of a PUMA robot equipped with four paintbrushes, jars of red, green, blue and yellow paint and white paper attached to a vertical easel. A Java interface executing within a web browser allows interactive control of the robot. This interface contains two windows showing live camera views of the work site and various controls for connecting to the robot, viewing the task status and controlling the painting task. The original site operated from June 1998 to March 2000 with approximately 25,000 unique-addressed machines downloading the interface to produce about 500 canvases. The new site opened to the public on August 2002. This paper discusses the author’s experiences in operating the original site, and the motivation for and the challenges of reviving the site in its current location.	download;interface (java);java;microsoft windows;robot;world wide web	Matthew R. Stein	2003	Auton. Robots	10.1023/A:1026216520523	telerobotics;rgb color model;simulation;computer science;artificial intelligence	Robotics	-45.28491685998444	-27.999182312546257	125990
36995e73ec9ce9f4ced69cf64622766f623c9635	supporting multi-modal interfaces for adaptive tour planning	location service;multi agent system;user interface;multi modal interface;user adaptation;multi modal user interfaces;data type;mobile service;natural language;mobile gis;software component;tour planning;route planning;ogc openls	Typical examples for mobile GI services include tour planning and maps for navigation support (Reuter and Zipf 2004). We have developed several prototypes of mobile GI services for navigation tasks including user-adaptive tour planning as well as supporting multi-modal (i.e. graphical, natural language and gestures based) interfaces. As the development of GI services that adapt to user and context parameters as well as multi-modal interfaces for mobile navigation support are relative new research areas for GIScience it was necessary to specify interfaces for the software components that have been developed. These – relevant parts of the so-called Deep Map Objects (DMO), the message objects of the Deep Map multi agent system – will be introduced in this paper. Only recently the OGC Open Location Services Initiative (OpenLS) has published a specification for similar services in the area of LBS – but without a focus on adapatation or multi-modality. Both representations include relevant data types for route planning and map generation and are based on XML. Therefore we will compare these two representations and discuss their applicability for developing multi-modal map interaction for adaptive tour planning.	component-based software engineering;geographic information science;graphical user interface;location-based service;modal logic;modality (human–computer interaction);multi-agent system;natural language;warhammer 40,000: dark millennium;xml;zipf's law	Jochen Häußler;Alexander Zipf	2004	Annals of GIS	10.1080/10824000409480663	simulation;human–computer interaction;data type;computer science;component-based software engineering;multi-agent system;natural language;user interface;world wide web	HCI	-40.322747584134476	-26.74384823564818	126011
0a0e173731aadb8044766c25a548af0f52215252	integrated information manipulation systems (ims) - a cognitive view		The personal computer of the future will offer its owner an information manipulatio, 9~stem (IMS). It will be a totally integrated system being able to manipulate arbitrary information structures, eg programs, prose, graphical objects and sound. An IMS will be an important step towards achieving the goal that we can do all our work on-line -placing in computer store all of our specifications, plans, designs, programs, docummentation, reports, memos, bibliography and reference notes and doing all of our scratch work, planning, designing, debugging and most of our intercommunication via the consoles. We outline the basic principles underlying the design of an INS. We discuss the cognitive dimensions (specifically for text processing and programming systems) which should serve as the design criteria for systems whose goal is to reduce the cognitive burden and augment the capabilities of a human user.	cognitive dimensions of notations;data structure;debugging;graphical user interface;online and offline;personal computer	Gerhard Fischer	1980				HCI	-39.24107637248931	-29.770611800872437	126071
8846ed7cc71c7c60485fc9bbcfc040df00634bde	user interfaces for authoring systems with object stores	user interfaces authoring systems history image databases collaborative software object oriented modeling collaborative work software systems distributed databases writing;dynamism object stores authoring systems user interfaces textual scripts functionality direct manipulation interface;direct manipulation interface;user interface;direct manipulation;dynamism;object oriented programming;textual scripts;authoring systems;ease of use;object stores;object oriented programming authoring systems software tools user interfaces;authoring system;software tools;functionality;user interaction;user interfaces	In an authoring system with an object store, the user interacts with objects directly rather than through textual scripts. Although there is a history of tools and practices for using scripts, there are a number of authoring and user inter$ace problems that can be uniquely addressed by a system based on an object store. In this paper we discuss the advantages of authoring systems that have an object store. We then detail a set of functionality and a direct manipulation inte r$ace makes full use of these advantages. In so doing, we describe a system that achieves a much greater ease of use and new levels of dynamism.	direct manipulation interface;usability	Brian Roddy;Sidney Markowitz;Hernán Epelman-Wang	1996		10.1109/CMPCON.1996.501788	human–computer interaction;computer science;multimedia;world wide web	HCI	-42.53490864230222	-26.560812950407673	126334
d2684089dfd3a615417d874016649b62b6d6d7a1	opto-phono-kinesia (opk): designing motion-based interaction for expert performers		"""Opto-Phono-Kinesia (OPK) is an audio-visual performance piece in which all media elements are controlled by the body movements of a single performer. The title is a play on a possible synesthetic state involving connections between vision, sound and body motion. Theoretically, for a person who experiences this state, a specific colour could trigger both a sound and a body action. This synesthetic intersection is simulated in OPK by simultaneity of body movement, and audio-visual result.  Using the Gesture and Media System 3.0 motion-tracking system, the performer can dynamically manipulate an immersive environment using two small infrared trackers. The project employs a multipart interface design based on a formal model of increasing complexity in visual-sound-body mapping, and is therefore best performed by an expert performer with strong spatial memory and advanced musical ability. OPK utilizes the """"body as experience, instrument and interface"""" [1] for control of a large-scale environment."""	experience;gesture recognition;immersion (virtual reality);mathematical model;opto-isolator;rca connector;tracking system	Steve Gibson	2018		10.1145/3173225.3173295	human–computer interaction;computer science;interface design;multimedia;immersion (virtual reality);simultaneity;interaction design;gesture;match moving	HCI	-44.971873193226465	-35.99371020964907	126586
6e97584d2356ae8a7770d5adc01d66b209830b18	hypermedia exploration with interactive dynamic maps	query language;representation graphique;hipertexto;graphical language;information retrieval;representacion grafica;semantics;abstraction;mode conversationnel;interactive mode;abstraccion;semantica;semantique;lenguaje interrogacion;hypermedia;recherche documentaire;recherche information;modo conversacional;recuperacion documental;langage interrogation;document retrieval;recuperacion informacion;information system;hypertexte;systeme information;hypertext;graphics;lenguaje grafico;langage graphique;sistema informacion	Interactive Dynamic Maps (IDMs) help users interactively explore webs of hypermedia documents. IDMs provide automatically-generated abstract graphical views at different levels of granularity. Visual cues give users a better understanding of the content of the web, which results in better navigation control and more accurate and effective expression of queries. IDMs consist of: topic maps, which provide visual abstractions of the semantic content of a web of documents and document maps, which provide visual abstractions of subsets of documents. The major contributions of this work include 1) automatic techniques for building maps directly from a web of documents, including extraction of semantic content and use of a spatial metaphor for generating layout and filling space, 2) a direct manipulation interaction paradigm for exploring webs of documents, using maps and an integrated graphical query language, and 3) the ability to use the maps themselves as documents that can be customized, stored in a library and shared among users.	direct manipulation interface;graphical user interface;hypermedia;interactivity;map;programming paradigm;query language;topic maps	Mountaz Zizi;Michel Beaudouin-Lafon	1995	Int. J. Hum.-Comput. Stud.	10.1006/ijhc.1995.1053	document retrieval;hypertext;human–computer interaction;computer science;graphics;artificial intelligence;database;semantics;abstraction;world wide web;information retrieval;information system;query language	Web+IR	-34.124489759822886	-27.896520263683087	126894
9c16e7367922fb595b60f4fea6ce8d982482cb3c	exploring state diagram-based web browser programming	libraries;haptic interfaces rendering computer graphics hip force visualization containers solid modeling;combinatorial 3 manifolds;dynamic link library;collaborative work;implicit functions;hip;collaboration;geometry;computational geometry;virtual reality;tetrahedral mesh sets;shared virtual spaces haptic implicit functions;layout;set theory;shared virtual spaces;force;haptic interfaces layout solids space technology rendering computer graphics geometry collaborative work collaboration visualization libraries;visualization;complex geometry function based haptic interaction cyberworlds polygon based models point based models virtual reality 3d colors geometric textures collaborative virtual environments haptic interaction point rendering platform polygon meshes point sets voxel volumes x3d vrml;level of detail;solid modeling;state diagram;space technology;haptic interfaces;rendering computer graphics;haptic;mesh generation;tetrahedral mesh;data structure;containers;haptic interaction;solid modelling;virtual reality computational geometry haptic interfaces rendering computer graphics solid modelling;solids	Over the last few years a great deal of interest has focused on a major paradigm shift in World Wide Web-based services referred to as Web 2.0. In marked contrast to the earlier Web, in Web 2.0 sites become sources of information and functionality that enable users to create new content of their own. Consequently, users are now looking for more versatile browsers that will let them edit and display content based on their own preferences. This motivated us to develop a state diagram-based Web browser programming scheme that supports the close interaction between the end-user and Web content. Using state diagrams to represent browser behavior is easier for end-users with little or no programming experience to grasp than text-based programming systems.	programming paradigm;state diagram;text-based (computing);web 2.0;web content;world wide web	Yuka Obu;Mizuaki Yamamoto;Tatsuhiro Yonekura;Masaru Kamada;Shusuke Okamoto	2007	2007 International Conference on Cyberworlds (CW'07)	10.1109/CW.2007.35	layout;implicit function;mesh generation;computer vision;state diagram;simulation;visualization;data structure;computational geometry;computer science;level of detail;solid;geometry;virtual reality;space technology;solid modeling;haptic technology;programming language;force;set theory;computer graphics (images);collaboration	Web+IR	-35.278239663245834	-31.85435403934944	127028
556addffde04aa0c650c6010f1da2d1fa28cef46	the use of interpolating memories for music processing by microcomputer			interpolation;microcomputer	Andrew S. Noetzel	1985			interpolation;computer graphics (images);computer hardware;microcomputer;computer science	ML	-47.01412953452421	-29.474963763332564	127094
7ecb0aeb97d66355fd92582eea8f6ea162fd4f56	artist-driven crowd authoring tools		While crowd simulation frameworks can be very powerful for virtual crowd generation, in a VFX context they can also be unwieldy due to their chaotic nature. Small changes on the inputs can produce markedly different results, which can be problematic when attempting to adhere to a director's vision. Artist driven tools allow much more flexibility when constructing scenes, speed up turn-around time and can produce extremely dynamic crowd shots. To generate virtual crowds, Double Negative VFX (Dneg) has recently transitioned from an in-house standalone simulation-based solution to an artist-driven framework integrated into SideFX's Houdini.	crowd simulation;houdini;visual effects	Damien Maupu;Emanuele Goffredo;Nile Hylton;Mungo Pay;Martin Prazák	2017		10.1145/3084363.3085035	simulation;computer vision;computer graphics (images);artificial intelligence;chaotic;crowds;speedup;animation;crowd simulation;computer science;multimedia	HPC	-39.24357706227219	-34.50652521968711	127213
77dfa026f6d48deb25338316577fc0112f7c0540	live image analysis for computer assisted presentation	image processing equipment real time systems optical character recognition document image processing business graphics ccd image sensors video cameras intelligent control image resolution;image processing equipment;document analysis;image resolution;optical character recognition;business graphics;intelligent control;ccd image sensors;image analysis charge coupled devices multimedia systems cameras image resolution text analysis charge coupled image sensors intelligent systems engineering drawings instruments;video cameras;document image processing;image analysis;document analysis live image analysis computer assisted presentation intelligent presentation system documents drawings computer controlled camera zoom glance directions document image resolution scanned images mosaic;real time systems	We are developing an intelligent presentation system. This type of system can handle live images of documents and drawings. In the system, documents are input using a computer-controlled camera; it can actively decide zoom parameters and glance directions of the camera in order to capture the whole document image in sufficient resolution. Scanned images are mosaiced into a large image. Then, links between document data and electronic data can be constructed by document analysis. In this research, we describe techniques in order to realize this type of system.	image analysis	Masashi Toda;Takahito Yamaguchi;Tadakatsu Nakagawa;Toshio Kawashima;Yoshinao Aoki	1998		10.1109/ICIP.1998.723683	computer vision;feature detection;image analysis;image resolution;binary image;image processing;computer science;digital image processing;multimedia;microscope image processing;optical character recognition;sub-pixel resolution;automatic image annotation;digital image;intelligent control;computer graphics (images)	Vision	-40.911757166795816	-36.28919739127945	127216
02a34056d6160d203ee6d6acb108bdaea353cf9c	adobe illustrator plug-in to support brush selection using onomatopoeia utterance	adobe illustrator;design support;onomatopoeia	Illustration softwares can provide a variety of tools and parameters to make drawings. It isn't easy task for beginners to comprehend the softwares' specifications and functions. This paper proposed and developed an design supporting plug-in of Adobe Illustrator. Especially our study focused on selecting brushes of Adobe Illustrator. It is difficult for a user to select a proper brush corresponds to his/her image. Thus the plug-in can support the user to select the brush similar to his/her image. The user is able to specify an onomatopoeic word in the own voice to the plug-in while imaging the brush design the user wants to apply. This paper could demonstrate to show some output examples by using our proposed plug-in. As the result the plug-in worked without serious problems, but there exit some problems to be solved.	adobe illustrator;plug-in (computing);user experience	Gou Kayama;Masato Yoshiike;Yuri Yamada;Tsuyoshi Nakamura;Masayoshi Kanoh;Koji Yamada	2016	2016 Joint 8th International Conference on Soft Computing and Intelligent Systems (SCIS) and 17th International Symposium on Advanced Intelligent Systems (ISIS)	10.1109/SCIS-ISIS.2016.0142	simulation;onomatopoeia;computer graphics (images)	Robotics	-41.576074884582106	-30.933313968430415	127310
423116815222d06999cb4af2a6f4dd6308ee424d	towards plenoptic raumzeit reconstruction	free viewpoint video;dynamic scene reconstruction;image based rendering	The goal of image-based rendering is to evoke a visceral sense of presense in a scene using only photographs or videos. A huge variety of different approaches have been developed during the last decade. Examining the underlying models we find three different main categories: view interpolation based on geometry proxies, pure image interpolation techniques and complete scene flow reconstruction. In this paper we present three approaches for free-viewpoint video, one for each of these categories and discuss their individual benefits and drawbacks. We hope that studying the different approaches will help others in making important design decisions when planning a free-viewpoint video system.	interpolation	Martin Eisemann;Felix Klose;Marcus A. Magnor	2010		10.1007/978-3-642-24870-2_1	computer vision;image-based modeling and rendering;geography;rendering;multimedia;computer graphics (images)	Graphics	-36.485385441988335	-35.57183968172324	127432
067225a77865cdf8ed7dc93e8c59f5bd4965a429	semantic composition of 3d content behavior for explorable virtual reality applications		Virtual reality (VR) applications become increasingly popular in various application domains because of the possibilities of realistic immersive presentation and interaction with virtual objects as well as the diversity of advanced, relatively cheap devices. The semantic web, which is an important trend in the current web development, requires effective exploration of content distributed across different applications. Exploration of 3D content, which is the main part of VR applications, is a complex task, as the geometry, structure and appearance of the content may interactively evolve over time. Although a number of solutions are available for implementation of VR applications, the approaches have not been intended for on-demand exploration of behavior-rich 3D content in real time. In this paper, we present the development pipeline of explorable VR applications, which is based on semantic composition of 3D content activities into more complex behavior. The resulting applications are based on Prolog, which is a well-established knowledge representation language, and they can be queried for time-dependent 3D content features using domain-specific concepts. The approach has been implemented using the OpenStage 2 motion capture system and the Unity game engine. It can be used in different application domains and improve the integration of VR with the semantic web.		Jakub Flotynski;Marcin Krzyszkowski;Krzysztof Walczak	2017		10.1007/978-3-319-72323-5_1	web development;knowledge representation and reasoning;immersion (virtual reality);human–computer interaction;virtual reality;semantic web;event calculus;motion capture;prolog;computer science	Visualization	-41.586566141173655	-33.64285461253642	127681
71899c429383a0693d8dababd0c12f91523c764a	location-aware system based on a dynamic 3d model to help in live broadcasting of sport events	moving object;multiple camera management;real time;3d model;content selection in run time;location awareness;data retrieval	Broadcasting sport events in live is a challenging task because obtaining the best views requires taking into account many dynamic factors, such as: the location and movement of interesting objects, all the views provided by cameras in the scenario (some of them wireless, mobile, or attached to moving objects), possible occlusions, etc. Therefore, a technical director needs to manage a great amount of continuously changing information to quickly select the camera whose view should be broadcasted.  In this paper, we present a location-aware system that helps technical directors in the broadcasting task, using a 3D model updated continuously with real-time location data retrieved from the scenario. They can indicate in run-time their interest in certain moving objects and the system is in charge of selecting the cameras that provide the kind of views required.	3d modeling;location awareness;real-time clock	Roberto Yus;Eduardo Mena;Jorge Bernad;Sergio Ilarri;Arantza Illarramendi	2011		10.1145/2072298.2071924	computer vision;simulation;computer science;multimedia;world wide web;data retrieval	HCI	-34.65652952967501	-36.46898132198401	127938
889ab2228677ab8de87d1006efbb308d6b55e4a4	live performance tools: part ii	audio signal processing;music performance;real time;video processing;points;r tree;computer graphic;hierarchical spatial data structures;music analysis;spatial databases;quadtrees;r tree image processing;rectangles;lines;octrees;live performance	Many current digital art works operate in multiple media domains, for example computer graphics, video processing, audio signal processing, or arbitrary sensory input. Often, analysis and synthesis are cross-combined, such as applying real-time audio and music analysis results for computer graphics generation. In addition, digital art performances are typically interactive, and the computer is used as a performance instrument. From a human-centered viewpoint, and as in traditional music performance, this instrument must be controllable in a expressive and virtuous manner.	audio signal processing;computer graphics;human-centered computing;performance;real-time cmix;video processing	Stefan Müller Arisona	2007		10.1145/1281500.1281677	r-tree;computer vision;real-time computing;speech recognition;audio signal processing;computer science;operating system;line;real-time computer graphics;music theory;multimedia;video processing;computer graphics (images)	Graphics	-45.60836928718764	-33.293515835343335	127941
0b663e8453350a05545335c9cd70cdb9a648ec43	mimic: the microphone as a pencil	sonic interaction design;vocal sketching;gestures;augmented microphone	miMic, a sonic analogue of paper and pencil is proposed: An augmented microphone for vocal and gestural sonic sketching. Vocalizations are classified and interpreted as instances of sound models, which the user can play with by vocal and gestural control. The physical device is based on a modified microphone, with embedded inertial sensors and buttons. Sound models can be selected by vocal imitations that are automatically classified, and each model is mapped to vocal and gestural features for real-time control. With miMic, the sound designer can explore a vast sonic space and quickly produce expressive sonic sketches, which may be turned into sound prototypes by further adjustment of model parameters.	embedded system;mimic;microphone;peripheral;real-time locating system;sensor	Davide Rocchesso;Davide A. Mauro;Stefano Delle Monache	2016		10.1145/2839462.2839467	speech recognition;acoustics;engineering;communication	HCI	-46.42767894634736	-36.37240906412362	127947
e1de9c04b7a115cc70685495c429cee6df34bf90	computers and cartography	magnetic fields;minerals;earth;magnetometers;petroleum;geophysics computing;geology;civil engineering;cities and towns geophysics computing civil engineering minerals large scale systems petroleum geology earth magnetic fields magnetometers;cities and towns;large scale systems	Computer-aided plotting is turning the ancient art of cartography into an exact science. Today's geologist, civil engineer, city planner, highway engineer and government cartographer have turned to small computers and digital plotters for maps that in the past were produced entirely by hand. This movement to computer-aided cartography has yielded more accurate maps faster and at lower cost.	cartography;computer;map;plotter	Dennis L. Bress	1972	Computer	10.1109/C-M.1972.216996	magnetometer;magnetic field;earth;petroleum	EDA	-34.932785103724406	-30.641338008197458	128088
e38cc2ada87fb7bad1dbe17d7e013a5b304ee99b	effets des paramètres graphiques sur la perception visuelle: expérimentations sur la forme, la surface, l'orientation des objets et la définition des ecrans	verification;air traffic control;critical gui;object interaction;user interface;brightness;contrast;critical system;interactive system;pixels;visual perception;perception;object perception;experimentation;size	User interfaces of critical systems, such as air traffic control displays, use graphical objects to code for an ever increasing amount of information. This evolution brings forth concerns about the detection and identification of the displayed objects, in particular for small size objects. First, graphic properties of the interface should include some knowledge about the interplay between colour, shape and size interactions, and the visual perception. Second, the redesign of any interactive system should take into account the particularities of the evolving software and hardware display technology (pixel size and structure, for example) in order to preserve crucial aspects of the initial visual display. The two experiments described in this paper are aiming towards building a more systematic knowledge of graphic properties interactions per se, and their changes as a function of display technologies. More precisely, we examined the effect of the object size, shape and luminosity, as well as its contrast with the background. Results show that object perception is dependent upon its size, its contrast with the background and the overall luminosity of the background. Furthermore, for small size objects, interactions between pixel luminosities and pixel arrangement greatly influence their perception by the human eye.	color vision;display device;experiment;graphical user interface;interaction;interactivity;linear algebra;pixel	Gilles Tabart;Sylvie Athènes;Stéphane Conversy;Jean-Luc Vinot	2007		10.1145/1541436.1541442	computer vision;simulation;computer science;computer graphics (images)	HCI	-35.452398697739305	-35.842769436525565	128464
335174890d1d417fd109f8fffac7bba3b66edd75	adaptive educational hypermedia on the web	tecnologia electronica telecomunicaciones;computacion informatica;grupo de excelencia;ciencias basicas y experimentales;adaptive educational hypermedia;tecnologias	"""An adaptive hyper-media system tracks the user's browsing behavior in an attempt to determine what the user's background, experience , knowledge, and interests are. At the same time, these aspects of the user may change, at least in part due to the information the user is receiving from the system. Adaptive hypermedia techniques are used in many application areas, as exemplified by the articles in this special section. There is one dominating area however: educational applications. The domain of a learning application can be represented through (a hierarchy of) concepts. The user model in these systems is an overlay model, storing a knowledge value for each domain concept. When a user reads pages, the system assumes that he or she is gaining knowledge about concepts associated with these pages. (The knowledge gain can be verified through online tests.) Apart from having a hierarchy of concepts, adaptive educational applications also use a set of prerequisite relationships between concepts. The concept hierarchy, the knowledge values for concepts, and the prerequisite relationships together enable the system to decide whether a user is """" ready """" to study a new concept. For every link that appears on a Web page, the system can tell the user whether the link leads to interesting new information, to new information the user is not ready for, or to a page that provides no new knowledge. This idea is realized through the two most popular forms of adaptive navigation support: link annotation and link hiding. Figure 1 shows part of a possi-Adaptive Educational Hypermedia on the Web T T raditional textbooks carefully guide students through the topics of a course in a specific order the author and editor have deemed most appropriate. A reader of Web-based textbooks expects to be able to fully use the navigational freedom the author and editor have provided (through links) without running into difficulties such as the use of terms the reader does not (yet) know. Because of dependencies (or prerequisite relationships) between topics or course pages, it is not feasible to create a static Web-based textbook that can be read in any arbitrary sequence. Adaptive hypermedia methods and techniques can provide a solution: they make it possible to inform readers that certain links lead to material they are not ready for, or even to compensate for missing knowledge by adding explanations to the pages a reader visits. We illustrate the use of …"""	adaptive hypermedia;web page	Paul De Bra	2002	Commun. ACM	10.1145/506218.506247	human–computer interaction;multimedia;world wide web	Web+IR	-39.208054852014676	-26.22132482516155	128737
6a762246fbc4084def7d4745fce341cd5a90dad6	natural language generation from ontologies using grammatical framework		The paper addresses the problem of automatic generation of natural language descriptions for ontology-described artifacts. The motivation for the work is the challenge of providing textual descriptions of automatically generated scientific workflows (e.g., paragraphs that scientists can include in their publications). The extended abstract presents a system which generates descriptions of sets of atoms derived from a collection of ontologies. The system, called nlgPhylogeny, demonstrates the feasibility of the task in the Phylotastic project, that aims at providing evolutionary biologists with a platform for automatic generation of phylogenetic trees given some suitable inputs. nlgPhylogeny utilizes the fact that the Grammatical Framework (GF) is suitable for the natural language generation (NLG) task; the abstract shows how elements of the ontologies in Phylotastic, such as web services, inputs and outputs of web services, can be encoded in GF for the NLG task. 2012 ACM Subject Classification Computing methodologies → Logic programming and answer set programming, Information systems → Web services, Computing methodologies → Natural language generation	answer set programming;grammatical framework;logic programming;natural language generation;ontology (information science);phylogenetic tree;phylogenetics;stable model semantics;web service	Van-Duc Nguyen	2018		10.4230/OASIcs.ICLP.2018.22		NLP	-38.3493209857258	-29.28476783979293	128829
f386cd08150997deec89a7775a9963640ddc7c21	smart avatars in jackmoo	education and training;distance learning;web accessibility;object database;virtual reality;virtual human;semantic information;internet;avatars humans internet biological system modeling virtual prototyping lifting equipment communication system control facial animation electrical capacitance tomography layout;computer based training;realistic images;imperative sentences smart avatars jackmoo 3d multi user virtual worlds education training applications realism internet prototype system virtual human system lambdamoo interactive server persistent object database semantic information;3 dimensional;user interfaces;distance learning virtual reality user interfaces computer based training realistic images internet;virtual worlds	Creation of compelling 3-dimensional, multi-user virtual worlds for education and training applications requires a high degree of realism in the appearance, interaction, and behavior of avatars within the scene. Our goal is to develop and/or adapt existing 3-dimensional technologies to provide training scenarios across the Internet in a form as close as possible to the appearance and interaction expected of live situations with human participants. We have produced a prototype system, JackMOO, which combines Jack, a virtual human system, and LambdaMOO, a multiuser, network-accessible, programmable, interactive server. Jack provides the visual realization of avatars and other objects. LambdaMOO provides the web-accessible communication, programability, and persistent object database. The combined JackMOO allows us to store the richer semantic information necessitated by the scope and range of human actions that an avatar must portray, and to express those actions in the form of imperative sentences. This paper describes JackMOO, its components, and a prototype application with five virtual human agents. Comments Postprint version. Published in IEEE Proceedings of Virtual Reality 1999, March 1999, pages 156-163. Publisher URL: http://dx.doi.org/10.1109/VR.1999.756946 This working paper is available at ScholarlyCommons: http://repository.upenn.edu/hms/8 Smart Avatars in JackMOO Jianping Shi, Thomas J. Smith, John P. Granieri, and Norman I. Badler Center for Human Modeling and Simulation University of Pennsylvania Philadelphia, PA 19104-6389 215-898-5862	avatar (computing);imperative programming;internet;multi-user;prototype;server (computing);simulation;virtual actor;virtual reality;virtual world	Jianping Shi;Thomas James Smith;John P. Granieri;Norman I. Badler	1999		10.1109/VR.1999.756946	distance education;three-dimensional space;the internet;simulation;human–computer interaction;computer science;web accessibility;virtual reality;multimedia;user interface	Visualization	-42.67030334603487	-32.97181434688979	128856
c4a355089bbf49aa4aae8c8be1712fe42dad8c0d	computing swept volumes		The swept volume problem is practical, dif®cult and interesting enough to have received a great deal of attention over the years, and the literature contains much discussion of methods for computing swept volumes in many situations. The method presented here permits an arbitrary polyhedral object (given in a typical boundary representation) to be swept through an arbitrary trajectory. A polyhedral approximation to the volume swept by this moving object is computed and output in a typical boundary representation. A number of examples are presented demonstrating the practicality of this method. Copyright # 2000 John Wiley & Sons, Ltd.	3d modeling;algorithm;animat;approximation;automated planning and scheduling;boundary representation;columbia (supercomputer);computation;computational geometry;computer animation;computer science;computer vision;dynamic language runtime;gerd sommerhoff;graphical user interface;ibm notes;ibm research;john d. wiley;michael j. fischer;polyhedron;rapid prototyping;real-time clock;robot;robotic arm;s/pdif;systems architecture;terminator genisys;z-buffering	Steven Abrams;Peter K. Allen	2000	Journal of Visualization and Computer Animation	10.1002/1099-1778(200005)11:2%3C69::AID-VIS219%3E3.0.CO;2-7	computer science;artificial intelligence;theoretical computer science	Visualization	-46.34810357589342	-29.681867121770537	128899
74ef8fbb83d911f1b4fdc8e30e4bdfd3f63b6d4a	a tool kit system for the synthesis and the management of active media objects		Information management deals with documents that are represented in various forms. Formats of documents and shapes of deskwork tools afford us how to manipulate and manage them. However, we still lack a general paradigm that associates each object not only with its operations but also with its form of presentation. IntelligentPad is a tool kit for the storage and visual management of active media objects. It represents everything as a pad. It associates each pad with a function such as word processing, line drawing, tabulation, graph drawing etc. Different functions define different pads. Pasting of pads on another pad defines a new pad that has both an arbitrary layout of fields and a new function composed of the constituent pads. IntelligentPad provides four ways of managing a large amount of pads, i.e., visual catalogs of pads, hypermedia networks, form bases, and pad bases. A form base stores pads with the same format, while a pad base manages all types of pads in the system.		Yuzuru Tanaka	1989			computer hardware;engineering;world wide web;engineering drawing	EDA	-39.56908278796013	-30.94193475003452	129047
b4e77824d70d1810f63df2a4675ce617c1c07059	an application of template methodology: rapid prototyping of user interface management systems	user interface management system;software prototyping;user interface;prototypes user interfaces operating systems graphical user interfaces displays application software software prototyping computer science software quality resource management;user interface management systems human factors interactive systems software prototyping;underlying uims look and feel rapid prototyping user interface management system uims prototype template;rapid prototyping;human factors;user interface management systems;interactive systems	Probably the most important characteristic of a user interface is its ‘look and feel.’ Software consumers often select the programs they purchase based upon the program’s ease of use (i.e., quality of the user interface), rather than just the relative number of features each provides. Clearly, it is important to determine the appropriateness of a program’s interface early in the design phase. Unfortunately, textual descriptions do not fully capture ‘look and feel’, since this quality is a function of the way that users interact with the interface. We can model the ‘look and feel’ of u&r interfaces via rapid prototyping. However, the behavior of a particular user interface is greatly influenced by the particular user interface management system (UIMS) upon which it is built, so it is necessary to prototype an interface with regard to a particular user interface management system. Moving an application and its user interface to a different UIMS may result in a user interface that behaves in a substantially different manner. In this paper, we describe a methodology that may be used to develop a ViMS prototype using a template. This template will assist developers not only in developing new UIMSs, but in eidluating the behavior of user interfaces when the underlying UIMS changes.	norm (social);prototype;rapid prototyping;usability;user interface management systems	Deborah A. Frincke;Genc L. Fisher;Myla Archer;Karl N. Levitt	1991		10.1109/IWRSP.1991.218621	user interface design;look and feel;user;10-foot user interface;user modeling;interface metaphor;shell;human–computer interaction;natural language user interface;computer science;systems engineering;human factors and ergonomics;user requirements document;operating system;skin;natural user interface;user interface;graphical user interface testing;multiple document interface;computer engineering	HCI	-42.59333403964153	-30.470803084420133	129050
9afcbddc25668c410c2a93755a3e71f5439b3d45	visualisation techniques for using spatial augmented reality in the design process of a car	i 3 3 computer graphics picture image generation i 4 8 image processing and computer vision scene analysis i 4 9 image processing and computer vision applications;virtual and augmented reality;projector camera systems;ray tracing;radiometric compensation	Abstract#R##N##R##N#If spatial augmented reality is used in the design process of a car, then one of the most important issues is that the virtual content is projected with a very high visual quality onto the real object, because based on this projection design decisions are made. Especially, the visualised colours on the real object should not be distinguishable from corresponding real reference colours. In this paper, we introduce a new approach for the augmentation of real objects which is able to match the requirements of a design process. We present a new rendering method with ray tracing which increases the visual quality of the projection images in comparison to existing methods. The desired values of these images have further to be adjusted according to the material, the ambient light and the local orientation of the projector. For this purpose, we develop a physically based computation which exactly determines the corresponding projection intensities for these values by using three-dimensional lookup tables at every projector pixel. Since not all of the desired values can be represented with an intensity of the projector, an adjustment has to be computed for these values. Therefore, we conduct a user study with design experts who work in the automotive industry and use the results to propose a new adjustment method for such values. Finally, we compare our methods to existing procedures and conclude which ones are suitable for the design process of a car.	augmented reality	Christoffer Menk;Eduard Jundt;Reinhard Koch	2011	Comput. Graph. Forum	10.1111/j.1467-8659.2011.02066.x	ray tracing;computer vision;simulation;computer science;computer graphics (images)	HCI	-36.018530929605205	-36.665232630877995	129078
69a69a560cf38c968b52578a8c0dd740f39b388f	[poster] deformed reality: proof of concept and preliminary results		We introduce “Deformed Reality”, a new paradigm to interactively manipulate objects in a scene in a deformable manner. Using the core principle of augmented reality to estimate rigid pose over time, our method enables the user to deform the targeted object while it is being rendered with its natural texture, giving the sense of a real-time object editing in user environment. The presented results show that our method can open new ways of using augmented reality by not only augmenting the scene but also interacting with it in a non-rigid manner.	augmented reality;interaction;interactivity;muscle rigidity;physical object;programming paradigm;real-time clock;user interface	Nazim Haouchine;Antoine Petit;Frédérick Roy;Stephane Cotin	2017	2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)	10.1109/ISMAR-Adjunct.2017.56	computer science;proof of concept;computer vision;artificial intelligence;augmented reality;solid modeling	Visualization	-42.70968350485385	-37.331608459468505	129205
0d9e2a0314bb4222c9ab9829df284dbdde91030c	framestore system for map displays		Abstract   The map displays project was undertaken to investigate techniques for the storage and display of geographical maps and related information and to develop the hardware and software required for a complete working system. Initially most of the project was concerned with the design of graphics hardware to be interfaced to a DEC LSI-11/03 micro-computer which was the main supply of computing power for the project. The development of one of the pieces of specialized hardware, the framestore display, is described.		D. J. Watson;D. Economou;Richard L. Grimsdale	1983	Image Vision Comput.	10.1016/0262-8856(83)90068-9	embedded system;computer hardware;computer science;computer graphics (images)	Vision	-47.71111254310482	-28.14385688003263	129356
5df3be0ab3d595caa06ebc8f4f49433964e2b2e8	multimodal interaction in collaborative virtual environments	teleconferencing;programming environments;graphical user interfaces teleconferencing computer animation virtual reality programming environments;synthetic actor;real time;feature tracking;virtual reality;animator s expression multimodal interaction collaborative virtual environments computer graphics human interfaces total multi modal approach motion capture systems user controlled virtual environments visual analysis mpeg 4 standard parameters real time facial analysis system;computer graphic;information gathering;motion capture;graphics system;human interface;graphical user interfaces;visual analysis;facial animation;multimodal interaction;facial expression;virtual environment;computer animation;collaborative virtual environment;collaboration virtual environment mpeg 4 standard humans real time systems facial animation computer graphics face recognition performance analysis control systems	Human interfaces for computer graphics systems are now evolving towards a total multi-modal approach. Information gathered using visual, audio and motion capture systems are now becoming increasingly important within user-controlled virtual environments. This paper discusses real-time interaction through the visual analysis of human face feature. The underlying approach to recognize and analyze the facial movements of a real performance is described in detail. The output of the program is directly compatible with MPEG-4 standard parameters and therefore enhances the ability to use the available data in any other MPEG-4 compatible application. The real-time facial analysis system gives the user the ability to control the graphics system by means of facial expressions. This is used primarily with real-time facial animation systems, where the synthetic actor reproduces the animator’s expression. The MPEG4 standard mainly focuses on networking capabilities and it therefore offers interesting possibilities for teleconferencing, as the requirements for the network bandwidth are quite low.	collaborative virtual environment;computer animation;computer graphics;human interface device;modal logic;motion capture;multimodal interaction;real-time clock;real-time transcription;requirement;synthetic intelligence;virtual actor;virtual reality	Taro Goto;Marc Escher;Christian Zanardi;Nadia Magnenat-Thalmann	1999		10.1109/ICIP.1999.817057	computer vision;motion capture;teleconference;computer facial animation;human–computer interaction;computer science;virtual machine;multimodal interaction;graphical user interface;virtual reality;computer animation;multimedia;facial expression;human interface device;computer graphics (images)	Graphics	-40.22266837619143	-36.674823681381675	129402
d4ef27674481f452d946ff9b20775c7da753f29b	open framework facilitating automatic generation of cg animation from web site	text visualization;open framework facilitating automatic generation data visualization html data cga computer graphics animation web site;web sites computer animation data visualisation hypermedia markup languages;scripting langauge;scripting langauge animation text visualization media conversion;datorsystem;computer systems;media conversion;animation;datavetenskap;computer science;engines servers html blogs animation xml visualization	We have been studying and developing the system which enables to generate Computer Graphics Animation (CGA) automatically by processing HTML data of Web site. In this paper, we propose an open framework to facilitate this. The framework is functioning all at a server side, obtaining the HTML, converting it to a script describing the CGA story and updating the script. And at a client side, a user accesses the script on the server to visualize it by using real-time CG character with synthesized voice, camera work, superimposing, sound file playback etc. We have constructed the framework on the server and deployed the substantial engines to convert Web sites to CGAs. This paper describes the detail of the framework and also shows some example projects providing automatically generated News show, Talk show and personal Blog visualization.	blog;client-side;computer animation;computer graphics;html;real-time clock;server (computing);server-side;speech synthesis	Masaki Hayashi;Steven Bachelder;Masayuki Nakajima	2015	2015 International Conference on Cyberworlds (CW)	10.1109/CW.2015.11	anime;computer facial animation;computer science;artificial intelligence;operating system;computer animation;multimedia;world wide web;computer graphics (images)	Visualization	-42.5559088614365	-32.858430023738215	129592
1b09e7f355775e041a7f3cf4f6b04b4c006719ed	a psychophysical study of dominant texture detection	computer graphic;3d model;gaze control;pie menus;input devices;user interfaces	Images of everyday scenes are frequently used as input for texturing 3D models in computer graphics. Such images include both the texture desired and other extraneous information. In our previous work [Lu et al. 2009], we defined dominant texture as a large homogeneous region in an input sample image and proposed an automatic method to detect dominant textures based on diffusion distance manifolds. In this work, we explore the identification of cases where diffusion distance manifolds fail, and consider the best alternative method for such cases.	3d modeling;computer graphics;lu decomposition;texture mapping	Jianye Lu;Alexandra Garr-Schultz;Julie Dorsey;Holly E. Rushmeier	2009		10.1145/1620993.1621027	computer vision;computer science;operating system;multimedia;user interface;input device;computer graphics (images)	Vision	-40.62592456141856	-37.492836746316755	129611
d784854e9423b3054dafef7a491fb41244bc6f2f	issues and requirements for building a generic animation	computer animation;digital simulation;software engineering;software tools;fast;focused applicator simulator and trainer;animation requirements;communication tool;data-driven animation;generic animation building;generic simulation;nonprogramming data entry;object names;simulation model;simulation tools;visually accurate animation	Since the main function of animation is to serve as a communication tool (Law and McComas 1992), an animation needs to be visually accurate to a user. When presenting to managers, a visually accurate animation can increase the credibility of a simulation model and analysis. Currently it is feasible to build a generic simulation model in a reasonable time period that gives a user the capability of changing the system scope, the characteristics of the whole system or specific objects, and object names. The animation associated with the genetic simulation must be able to display all of the changes made by a user in the simulation. This paper presents the issues and requirements for developing generic animations and points out the deficiencies of currently available tools.	entry point;flash animation;generic programming;genetic algorithm;requirement;simulation	D. Michelle Benjamin;Barbara W. Mazziotti;F. Bradley Armstrong	1994			simulation;skeletal animation;computer science;technical report;interactive skeleton-driven simulation;simulation modeling;computer animation;multimedia;world wide web;computer graphics (images)	Graphics	-39.39122673111846	-31.940625674261877	129913
90cf29fafc0f48a20ab3c3c9879c0767322cbe16	geração de expressões de referência em ambientes virtuais	computacion informatica;filologias;linguistica;ciencias basicas y experimentales;geracao de lingua natural expressoes de referencia;natural language generation referring expressions	Instruction-giving systems for virtual interactive 3D worlds have a wide range of applications in education, games and others. This paper discusses the computational task of referring expression generation for systems of this kind, focusing on the use of spatial relations to describe domain objects.	business object;domain-driven design;referring expression generation	Diego dos Santos Silva;Ivandré Paraboni	2014	Linguamática		art;performance art;cartography	NLP	-39.70413527237489	-26.822571423637385	130223
b20ce299c410f4b70275b007c4f81ad05e7696af	a japanese calligraphy trainer based on skill acquisition through haptization	real time visualization;motor skills;virtual reality system;iterate training;application software;computer graphics;haptic device;training;skill acquisition;haptization;virtual reality;virtual learning;virtual learning space;writing skills;text analysis;force;computer graphic;3d brush model;visualization;imaging phantoms;virtual environment japanese calligraphy trainer skill acquisition haptization virtual reality system handwritten characters learning writing skills motor skills haptic device virtual learning space writing characters 3d brush model real time visualization iterate training calligraphy learning;e learning;virtual reality writing haptic interfaces brushes computer graphics imaging phantoms education application software computer science intelligent systems;intelligent systems;writing;computer science;virtual environment;learning artificial intelligence;japanese calligraphy trainer;haptic interfaces;rendering computer graphics;calligraphy haptic interface e learning virtual reality computer graphics;calligraphy;writing characters;virtual reality handwritten character recognition learning artificial intelligence text analysis;handwritten character recognition;calligraphy learning;handwritten characters learning;haptic interface;brushes	We present an approach for implementing a virtual reality system targeted at learning handwritten characters. We especially aim at enabling learners to acquire important writing skills required to be a good writer in Japanese calligraphy. The proposed system provides a haptic channel allowing the learners to intuitively master an instructor’s fine motor skills through the sense of touch. We utilize a commercially available haptic device called PHANTOMTM for simulating a writing brush in a virtual learning space. The system implements a function for recording and replaying the instructor’s hand motions via the PAHNTOM device. The instructor’s writing techniques such as brush-strokes and pen pressures he/she performs/adds when writing characters are effectively presented to the learners via the PHANTOM device. Then, they can master how to write the recorded characters with feeling the instructor’s style of handwritings. We also invented a simple yet powerful 3D brush model for real-time visualization of handwritten characters without compromising the quality and reality of the characters. The users can start learning at any time and iterate training without worrying about resource consumptions such as papers and ink as much as they like. We conducted an experiment to validate the effectiveness of the proposed system for learning calligraphy in a virtual environment.	haptic technology;imaging phantom;internet;iteration;norm (social);real-time locating system;simulation;virtual reality	Hiroaki Nishino;Kouta Murayama;Tsuneo Kagawa;Kouichi Utsumiya	2010	2010 24th IEEE International Conference on Advanced Information Networking and Applications	10.1109/AINA.2010.112	simulation;computer science;artificial intelligence;virtual reality;multimedia;haptic technology;computer graphics (images)	Visualization	-44.399764925597445	-37.6470146136805	130351
bd4e7d626b8137686cb65ec9c42f4fa343a34748	a cave/desktop collaborative virtual environment for offshore oil platform training	cave;immersive user environment cave desktop collaborative virtual environment offshore oil platform training cve training simulations 3d projections;training;avatars training computational modeling virtual environments three dimensional displays visualization collaboration;virtual reality computer based training gas industry petroleum industry production engineering computing;collaboration;platform;virtual environments;visualization;computational modeling;three dimensional displays;avatars;training cave collaborative platform;collaborative	Collaborative Virtual Environments (CVEs) enable engaging training simulations in several areas, such as medicine, military, engineering as well as in the oil and gas industry. CVEs allow multiple users to interact with one another through the simulation. A CAVE system is composed of a number of walls, floor (and eventually ceiling as well) which receive 3D projections, surrounding its user. Such setup provides an increased imersiveness experience to the user, which makes a given simulation to become more realistic, which is desirable if a training application is to provide useful experience to a given user. This work introduces a CVE which enables the communication between an immersive user environment (in a CAVE setup) with a trainer interface which runs in a desktop setup. The application focuses on training in an offshore oil platform.	collaborative virtual environment;common vulnerabilities and exposures;desktop computer;multi-user;simulation;user interface	Leonardo Cardia da Cruz;Jauvane Cavalcante de Oliveira	2016	2016 XVIII Symposium on Virtual and Augmented Reality (SVR)	10.1109/SVR.2016.38	simulation;human–computer interaction;engineering;multimedia	Visualization	-43.73265261456897	-37.49359545243927	130422
2590bd1206e688336862dfa3b6eb7a47ca29d60d	collaborative virtual geographic environment: concepts, features and construction	distributed vge;cvge;vegetation mapping;ve;groupware;geographic information visualization method;collaborative work;forestry;dvge;vge;geographic information;virtual forest environment;ai;virtual reality languages;behavior modeling;computer supported cooperative work;geographic problem;virtual reality;perception model;collaboration collaborative work virtual environment computer networks virtual reality content addressable storage application software visualization ip networks artificial intelligence;multi user;information services;computer network development;construction graphics;computer networks;vfe;computer network;virtual geographic workspace;data visualisation;internet technology;agent;visualization;multi agent systems;collaborative virtual geographic environment;internet;spatial relation;computer supported collaborative work;geographic information systems;system design;decision support systems;vrml;virtual reality modelling language;artificial intelligence;cscw;group work;group work collaborative virtual geographic environment cvge geove geographic information visualization method computer network development internet technology distributed vge dvge multiusers support system virtual geographic workspace geographic problem computer supported collaborative work cscw ai artificial intelligence construction graphics perception model virtual forest environment vfe virtual reality modelling language vrml agent electronic whiteboard;geovisualization;virtual environment;geocollaboration;multiusers support system;electronic whiteboard;virtual reality languages artificial intelligence computer networks data visualisation forestry geographic information systems groupware internet multi agent systems vegetation mapping;geove	Virtual Geographic Environment (VGE), or Geographic Virtual Environment (GeoVE) is considered to have the potential to extend the power of geographic information visualization methods. As the increasing development of computer network and Internet technologies, Distributed VGE (DVGE) has been developed to support multi-users to participate and share the same virtual geographic workspace. Nowadays, more and more tasks require group work, and many of them are spatial related, although DVGE extends the scope for participants to handle geographic problems, it still lacks efficient mechanism to make peoples' collaborative work more efficient. In this regard, we integrate CSCW and AI with CVGE and bring forward Collaborative VGE (CVGE) to support collaborative work in VGE. This paper illustrates the seven distinguished features of CVGE as a criterion for CVGE system design and assess; then analyses the construction of CVGE on the aspects of Individual, System and Society; the construction graphic shows that the perception model, behavior model and collaboration model are the key ingredients of a CVGE system. As an application of CVGE, a collaborative Virtual Forest Environment (VFE) is introduced in which VRML, Agent, Electronic Whiteboard and other technologies are applied	behavior model;computer-supported cooperative work;geographic information system;information visualization;interactive whiteboard;systems design;vrml;workspace	Mingyao Qi;Tianhe Chi;Xin Zhang;Jingxiong Huang	2004	IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2004.1370253	geovisualization;simulation;human–computer interaction;computer science;computer-supported cooperative work;virtual reality;multimedia	Visualization	-46.3005575976414	-27.271364473301926	130675
a90830b88cb4e40859bd4b634a358aa39851f75f	dynamic mapping strategies for expressive synthesis performance and improvisation	dynamic mapping strategies;minute timberal control;expressive performance instrument;sound engine;control parameter;synthesis space;control space;random many-to-many mapping;expressive synthesis performance;sound editing;random vector;random point	Realtime musical expression through synthesis is notoriously di cult. The complete potential of a sound engine is traditionally available only at design time. In this paper two mapping strategies are presented, adressing this problem. Based on dynamic and random many-tomany mappings between control space and synthesis space, they erase the line between sound editing and synthesizer performance, with an emphasis on free improvisation. One strategy is based on the addition of random vectors in synthesis space, weighted by control parameters. The other is based on a gravity analogy, interpolating between random points in synthesis space, with the gravities controlled by player interaction. Vectors and point sets can be scaled and shifted during performance, allowing dynamic exploration of vast soundspaces or minute timberal control. The mappings have been adopted to a wide range of musical interfaces. Together with suitable sound engines, surprisingly expressive performance instruments have been created, and used in regular rehearsals, concerts and recording sessions over the last two years.	acoustic cryptanalysis;interpolation;open road tolling;pitch (music);registry recon;repeatability;stepwise regression	Palle Dahlstedt	2008		10.1007/978-3-642-02518-1_16	simulation;computer science;artificial intelligence;machine learning;mathematics	PL	-46.909616447308814	-35.22981100771007	130954
4a97030281452bd1c210e1b0161095251deb0068	dream & team: a tool and a notation supporting exploration of options and traceability of choices for safety critical interactive systems	air traffic control;design process;certificate authority;development process;interactive system;graphical representation;design rationale;interaction technique	Justification of choices made throughout the design process of systems is a recurrent desire and quite often a formal request from certification authorities in the safety critical domain. However, even though some work has already been done in the early phases of the development processes, justifying choices in the later phases such as detailed design or implementation remain a cumbersome activity left (without any support) in the hands of the developers. This paper presents a notation called TEAM (Traceability, Exploration and Analysis Model) and its associated tool called DREAM (Design Rationale Environment for Argumentation and Modelling). The paper presents first the notation and its specificities with respect to other Design Rationale notations. Both the notation and the tools are presented on a case study showing how they can support design of interaction techniques for Air Traffic Control workstations. We also present the rationale that we have gathered while designing the graphical representation of the notation.	interactivity;traceability	Xavier Lacaze;Philippe A. Palanque	2007		10.1007/978-3-540-74800-7_48	simulation;design process;human–computer interaction;idef6;computer science;artificial intelligence;air traffic control;management science;management;certificate authority;software development process;design rationale;interaction technique	HCI	-39.35382169916663	-27.459439517014935	131060
2267e8d3db29808cf40e7d7b4cb242830901f9d1	navegaçao conceitual em ambientes de gestao do conhecimento usando navcon	web pages;knowledge management;knowledge structure;conceptual navigation;semantic web;world wide web;context based navigation;ontology;open hypertext system	This article presents conceptual navigation and NavCon, an architecture that implements this navigation in World-Wide Web pages. NavCon architecture makes use of ontology as metadata to contextualize user's search for information. Conceptual navigation is a technique to browse Web sites within a context. Context filters relevant retrieved information. Context also drives user's navigation through paths that meet his needs. Based on ontologies, NavCon automatically inserts conceptual links in Web pages. By using these links, users may navigate in a graph representing ontology concepts and their relationships. Browsing this graph, it is possible to reach documents associated with user's desired ontology concept. An enterprise may implement conceptual navigation to improve user's search for information in a knowledge management environment. Using a ontology to conduct navigation in a Intranet, it is possible to the user have a better understanding about enterprise knowledge structure.	browsing;intranet;knowledge management;ontology (information science);web page;world wide web	José Renato Villela Dantas;Pedro Porfírio Muniz Farias	2008		10.1145/1666091.1666132	computer science;knowledge management;ontology;semantic web;web navigation;ontology;web page;data mining;database;world wide web	Web+IR	-38.88862855308669	-25.29441540834834	131264
742801c51a3a6a419e387af5230ee544eb429904	collapsible user interfaces for information retrieval agents	human computer interaction;user interface;information retrieval;software agent;user interface software;information retrieval model;input output;software agents;graphical user interfaces;graphic user interface;model based user interfaces	This paper presents an architecture for information retrieval agents in which each agent declaratively describes its domain, input, output, and user interface. A mediating piece of software can then assemble software agents for a given information retrieval task, and produce a single, unified user interface for that task from the individual agents’ descriptions.	declarative programming;information retrieval;on the fly;software agent;user interface	Martin R. Frank;Pedro A. Szekely	1998		10.1145/291080.291085	user interface design;look and feel;user;10-foot user interface;user experience design;user modeling;clickable;interface metaphor;shell;human–computer interaction;natural language user interface;magic pushbutton;computer science;artificial intelligence;software agent;operating system;graphical user interface;multimedia;post-wimp;natural user interface;interactivity;model–view–controller;user interface;world wide web;graphical user interface testing;multiple document interface	AI	-41.95496704166932	-28.69050242035568	131276
090ebf6a018322120bc3ce4c2b567d4fa1733e3a	visualizing and manipulating automatic document orientation methods using vector fields	tabletop;orientation;design space;vector fields;design framework;vector field	We introduce and illustrate a design framework whereby tabletop documents are oriented according to vector fields that can be visualized and altered by end users. We explore and illustrate the design space using interactive 2D mockups and show how this approach can potentially combine the advantages of the fully manual and fully automatic document orientation methods previously proposed in the literature.		Pierre Dragicevic;Yuanchun Shi	2009		10.1145/1731903.1731918	computer vision;computer science;theoretical computer science;computer graphics (images)	HCI	-36.14635389982668	-34.65299286113607	131604
65fa93bdcc981f6cca06d929df155a5ceafaf25a	multimedia retrieval: fundamental techniques and principles of adaptivity				Korinna Bade;Ernesto William De Luca;Andreas Nürnberger	2004	KI		multimedia;computer science	Theory	-45.538988950817874	-24.788514795593745	131737
4299a7df6705ccb3541a76dc70810801e60cf2dc	organization of an integrated information base thanks to hypermedia systems	hypermedia systems;integrated information base			Frédérique Biennier;Joël Favrel	1993			human–computer interaction;computer science;knowledge management;multimedia	DB	-45.32282047109544	-25.99675987135886	131814
d5f2c0b475d5b6b569026e2ec3049a3b846e3931	darstellungs- und interaktionstechniken zur effizienten nutzung grafischer oberflächen durch blinde und sehbehinderte	svg;graphics editor	Personal computers become more and more important in people's lives. Digital information and user interfaces with high-quality graphical representations facilitate education as well as daily work and entertainment. Also for blind and visually impaired persons, the computer is important to improve their quality of live. It gives them independence and at the same time enhances their integration into society. Due to the increasing use of digital documents, the accessibility of information is raised steadily. By using assistive technologies such as magnification software and screen readers, the presentation of digital documents can be adapted to each user's needs. For printed or handwritten documents, this is not easily possible. However, current technologies still need improvements. Screen readers, for example, present graphical user interfaces only by textual information including the visible textual content as well as some semantic information like the type of a user interface element or its current state. Information on the graphical representation itself is provided only rarely, although this is also useful for the blind. Visually enabled people often use graphical information like color or positioning as means of information or communication. Indeed, for users of magnification software graphical information is available. However, the way of presenting graphical user interfaces often not assists users in an optimal way. For instance, while working with documents to read and edit texts horizontal scrolling is often necessary which significantly stresses the user. Further, through the enlargement of the screen content to multiple screen sizes important parts of the graphical user interfaces are often not visible to the user. Therefore, there is a high risk of missing important information. This thesis contributes to improving the access of blind and visually impaired people to graphical representations. Therefore, state-of-the-art technologies and new methods for improving the adaption of graphical user interfaces and documents as well as new approaches for interactive use and creation of graphics by blind and visually impaired persons themselves were examined. It could be shown that the requirements for presenting graphics to visually impaired persons and blind ones, who use graphical-tactile displays, are basically similar. Based on this finding a framework was developed which can be used to realize tac-tile representations as well as a newly developed screen magnification concept for partially sighted computer users. With respect to the steadily growing group of people with weaker and mostly age-related visual impairment another concept and prototype for screen magnification was developed which requires …	accessibility;assistive technology;color;computer;graphical user interface;graphics;printing;prototype;requirement;scrolling;user (computing)	Christiane Taras	2011			visual arts;engineering;electrical engineering;performance art	HCI	-44.55143259215918	-30.87279595391591	131861
8c97dab3a9b154eb10e7aa1681dc141ea07bf210	an open modular architecture for effective integration of virtual worlds in the web	virtual reality internet;cascading style sheets;virtual reality;browsers;open architecture;computer architecture;servers;world wide web open modular architecture virtual world;internet;three dimensional displays;animation;animation three dimensional displays servers browsers computer architecture cascading style sheets;the web;open architecture virtual worlds the web;virtual worlds	The Web and virtual worlds are currently crossing their ways, and although there are some efforts made to integrate them into each other, those typically rely on technologies that are rather esoteric to most web-developers. In this paper, we present a new open architecture that combines several emerging and established technologies to provide convenient tools for developing virtual worlds in the Web. These technologies are easy to learn and understand by the web community and allow for quick prototyping. Overall the modular architecture allows virtual worlds to be developed more quickly and more widely deployed.	open architecture;prototype;virtual world;world wide web	Sergiy Byelozyorov;Vincent Pegoraro;Philipp Slusallek	2011	2011 International Conference on Cyberworlds	10.1109/CW.2011.17	anime;the internet;simulation;open architecture;computer science;applications architecture;metaverse;web page;virtual reality;multimedia;cascading style sheets;world wide web;server	Visualization	-43.14448022518694	-26.09380797628525	131942
9e2a3f9e50be1b80f5be6a5954fa3069ed029747	automatic testing of natural user interfaces	articulo;kinect test case generation system testing natural user interfaces;automatic testing of natural user interfaces;natural user interfaces;kinect;statistical analysis automatic test software gesture recognition graphical user interfaces program testing;skeleton markov processes browsers testing vectors graphical user interfaces;test case generation;system testing;gesture controlled kinect web browser application automatic testing natural user interfaces automated test generation graphical user interfaces motion detection gesture detection software testing automatically tests kinect based applications statistical model	Automated test generation can effectively explore programs through their programmer interfaces and traditional graphical user interfaces, but the recent advent of natural user interfaces (NUI) based on motion and gesture detection, for example the Microsoft Kinect, has outrun software testing research. This leaves a rapidly growing domain of software ranging from entertainment to medical applications without suitable test automation techniques. To address this issue, we propose a technique that automatically tests Kinect-based applications by synthesising realistic sequences of skeletal movement. The novel test cases are generated by a statistical model, which is trained on a corpus of common gestures. Evaluation on a gesture-controlled Kinect web browser application demonstrates that our approach achieves significantly higher code coverage than random test inputs.	bitbucket;code coverage;command-line interface;correctness (computer science);design by contract;experiment;graphical user interface;human–computer interaction;interpolation;kinect;multilevel security;natural user interface;oracle (software testing);out run;programmer;prototype;random testing;relevance;robustness testing;simulation;smartphone;smoothing;software bug;software testing;statistical model;system under test;test automation;test case;text corpus;widget (gui)	Chris J. Hunt;Guy J. Brown;Gordon Fraser	2014	2014 IEEE Seventh International Conference on Software Testing, Verification and Validation	10.1109/ICST.2014.25	human–computer interaction;computer science;multimedia;post-wimp;natural user interface;user interface;system testing;world wide web;graphical user interface testing	SE	-45.55339558567015	-30.468152057150885	132058
b11e13552020707c1f3fa15387af9204d3329c9c	a programmable particle system framework for shape modeling	libraries;nonphotorealistic surface illustration;surface mesh algorithm visualization programmable particle system framework shape modeling information visualization surface visualization nonphotorealistic surface illustration;surface mesh algorithm visualization;application software;computer graphics;surface fitting;information visualization;surface texture;data mining;visual programming;data visualisation;visualization;shape;computer displays;particle system;shape modeling;particle tracking;surface visualization;fires;mesh generation;mesh generation data visualisation solid modelling surface fitting visual programming;shape visualization surface texture application software computer graphics data mining computer displays libraries fires particle tracking;programmable particle system framework;solid modelling	Particle systems are an effective tool for visualizing information in a variety of contexts. This paper focuses on the use of surface-constrained particles to visualize information about the surface. We have designed a particle system programming framework consisting of behaviors, attributes and shaders that allows users to rapidly create, debug, and deploy particle systems for sensing and extracting specific surface information and displaying this information in an visually effective manner. We also introduce a simple particle system “little language” to facilitate the articulation of these particle programs. We demonstrate the flexibility and power of this framework for surface visualization with the applications of singularity detection and display, nonphotorealistic surface illustration, and surface mesh algorithm visualization.	algorithm;artistic rendering;biconnected component;c++;debugger;debugging;domain-specific language;extensibility;graphics hardware;implicit surface;iterative method;iterative refinement;particle system;pixel;programming language;real-time clock;real-time computing;refinement (computing);run time (program lifecycle phase);shader;shading;simulation;system programming	Wen Y. Su;John C. Hart	2005		10.1109/SMI.2005.2	computer vision;computer science;theoretical computer science;computer graphics (images)	Visualization	-36.102891337561836	-31.05938572616812	132073
bf1fe9b0e3c11984be5ece946dc2e99ba05a6aa3	open multimedia environment to retrieve and organise documents: an adaptive web-based ir system in the field of textile and clothing industry	industrie textile;interfase usuario;sistema experto;multimedia;red www;information retrieval system;user interface;information retrieval;base connaissance;hypermedia;internet;recherche information;world wide web;base conocimiento;industria textil;interface utilisateur;reseau www;recuperacion informacion;systeme expert;information seeking;hipermedia;textile industry;problem solving;knowledge base;expert system	Computer based information repositories are becoming larger and more diverse. In this context the need for an effective information retrieval system is related not only to the efficiency of the retrieval process but also to its compatibility to support information seekers during a typical problem solving activity (Marchionini 1995). OMERO project (national research project, n. 41902) perspective considers information seeking as a problem solving activity (Rumiati 1990) that depends on communication acts.		Cristiano Chesi;Francesca Rizzo	2000		10.1007/3-540-44595-1_56	knowledge base;the internet;simulation;textile industry;computer science;artificial intelligence;database;multimedia;user interface;world wide web;computer security;expert system	Web+IR	-37.82803463679984	-25.00715251475964	132137
32cfd41c5337f54a7b6e6de4ea3a9be5fa36892a	movies from music: visualizing musical compositions	color animation;video display;raster graphics;computer graphics;computer graphic;music;computer art	A theory of music visualization proposed by Nancy Herman postulates an association between colors and pitches of musical scales. A color raster graphics display is used to generate images of notes, chords, and chord progressions based on this theory. Temporal adjacency of notes or chords is mapped to spatial adjacency of colors, usually in a concentric pattern of squares or circles. By varying certain image parameters, different “brush stroke” effects may be obtained. Illustrations of several computer generated “musical paintings” are included, along with some original paintings for comparison.	color;herman ring;music visualization;percussion mallet;raster graphics	J. B. Mitroo;Nancy Herman;Norman I. Badler	1979		10.1145/800249.807447	computer vision;raster graphics;computer science;music;multimedia;computer graphics;computer art;3d computer graphics;computer graphics (images)	Graphics	-33.806134359984995	-34.12810619298758	132204
29fe3023ee4602ec85b687bcbad58b2f927dc7e5	interaction volume management in a multi-scale virtual environment	direct manipulation;spatial relation;large displays;virtual environment;stereoscopic display;interaction technique	This book chapter explores issues of interaction and stereoscopic display in multi-scale virtual environments. When interaction, especially direct manipulation, is combined with stereoscopic display, there are trade-offs that must be considered when optimizing both. The chapter addresses the design issues for different types of large displays and then concentrates on the virtual workbench for an implementation that balances interaction and stereoscopic display needs. The general importance of recognizing and using specifically defined geometric areas of user interest is demonstrated. A multi-scale application is then developed, and the implementation is evaluated. When good and stable stereoscopic display is present, users gain much in their ability to perceive shape, depth, and spatial relations. This makes interaction techniques more powerful and suggests new techniques.	data descriptor;direct manipulation interface;interaction technique;programming paradigm;scale space;stereoscopy;usability testing;virtual reality;workbench	Zachary Wartell;Ernst Houtgast;Onno Pfeiffer;Chris Shaw;William Ribarsky;Frits H. Post	2009		10.1007/978-3-642-04141-9_16	computer vision;human–computer interaction;computer graphics (images)	HCI	-35.949147034923534	-35.68762799733762	132545
a66cd387c6060591b2d9dc1e1046c69becf3bf16	talking heads: which matching between faces and synthetic voices?	speech synthesis;speech synthesis synthetic faces text to speech voice synthesis talking heads man machine interfaces face animation communicative interface agents user acceptability qualitative criteria dialogue systems;interface agent;speech synthesis facial animation humans animals research and development face user interfaces internet telephony automatic control application software;human factors;platforms and tools;distributed multimodal interfaces;distance collaboration;text to speech;human factors speech synthesis computer animation user interfaces;talking head;man machine interface;computer animation;group perceptual interfaces;user interfaces	"""The integration of synthetic faces and text-to-speech voice synthesis (what we call """"talking heads"""") allows new applications in the area of man-machine interfaces. In a close future, talking heads might be useful communicative interface agents. But before making an extensive use of talking heads, several issues have to be checked according to their acceptability by users. An important issue is to make sure that the used synthetic voices match to their faces. The scope of this paper is to study the coherence that might exist between synthetic voices and faces. Twenty-four subjects rated the coherence of all the combinations between ten faces and six voices. The main results of this paper show that not all associations between faces and voices are relevant and that some associations are better rated than others according to qualitative criteria."""	speech synthesis;synthetic intelligence;talking angela	Marc Mersiol;Noël Chateau;Valérie Maffiolo	2002		10.1109/ICMI.2002.1166971	human–machine interface;computer vision;speech recognition;human–computer interaction;computer science;computer animation;multimedia;user interface;speech synthesis	AI	-47.07670768798213	-34.99765793624978	132576
00d3c00758b89a2d1bcf7a90fbf3e2b9259d8c44	smart learning management system framework		Thanks to modern networking technologies and advancement of social networks, people in the modern society need more and more information just to be in the game. With such environment, the importance of learning and information sharing cannot be overemphasized. Even though plethora of information is available on various sources such as the web, libraries, and any learning material repositories, if it is not readily available and meets the needs of the user, it may not be utilized. For that, we need a system that can help provide customized information – matches with user’s level and interest to the user. Such system should understand what the user’s interests are, what level the user belongs for the topic, and so on. In this paper, we are proposing a framework for smart learning management system (SLMS) that utilizes user profiles and semantically organized learning objects so only the relevant information can be delivered to the user. The SLMS maintains user profiles – continuously updating whenever there is a change – and learning objects that are organized by building ontology. Upon user’s request, the system fetches relevant learning materials based on the user’s profile. The delivered learning materials are suitable for the user’s topic and the level for the requested topic sorted by relevancy ranking.	academy;agent-based model;apache nutch;information management system (ims);library (computing);relevance;semantic search;social network;software repository;user profile;world wide web	Yeong-Tae Song;Yuanqiong Wang;Sungchul Hong;Yongik Yoon	2012			internet of things	AI	-39.74314201468693	-24.05934226762327	132675
59028569b3532a2ac29cd8c2d63caac1aaa433b4	robodanza: live performances of a creative dancing humanoid		The paper describes the artistic performances obtained with a creative system based on a cognitive architecture. The performances are executed by a humanoid robot whose creative behaviour is strongly influenced both by the interaction with human dancers and by internal and external evaluation mechanisms. The complexity of such a task requires the development of robust and fast algorithms in order to effectively perceive and process musical inputs, and the generation of coherent movements in order to realize an amusing and original choreography. A basic sketch of the choreography has been conceived and set-up in cooperation with professional dancers. The sketch takes into account both robot capabilities and limitations. Three live performances are discussed in detail, reporting their impact on the audience, the environmental conditions, and the adopted solutions to satisfy safety requirements, and achieve aesthetic pleasantness.	cognitive architecture;coherence (physics);computational creativity;experiment;fitness function;genetic algorithm;humanoid robot;interactive evolutionary computation;performance;requirement;robustness (computer science);time complexity	Ignazio Infantino;Agnese Augello;Adriano Manfré;Giovanni Pilato;Filippo Vella	2016			simulation;art	Robotics	-47.71171936343721	-34.856185746632896	133027
b01a28deafd67e61c933c1706dcedca18fcb4386	electronic trombone: an interactive tool to promote musical learning and performance creativity	musical toy;musical instruments;musical instrument;edutainment	A lot of musical toys (e.g., toy guitars, toy drums and percussion) can provide a good introduction to musical instruments and give children high entertainment. Via playing an instrument toy, children can sense and practice what kinds of physical movements (e.g., fingers and/or mouths) they need to play its corresponding actual instrument to produce sounds. In this paper, we present an electronic trombone with three different playing modes for children. The research mission is to offer a simple and joyful approach to engaging children in learning to play a trombone.	toys	Mu-Chun Su;Wen-Yi Lee;Sherry Y. Chen	2011		10.1145/2087756.2087870	new interfaces for musical expression;simulation;multimedia	HCI	-47.51589065129702	-36.24154529885324	133166
13e52b0150e46c6ae95f4b82b3175374264d8564	live computer animation (panel)	abstract images;particle systems;painting;non photorealistic rendering;painterly rendering;computer animation	computer aided design, scientific computation, visual simulation, and film special effects have become so sophisticated in their ability to generate high-quality, real-time computer animation that they can be used for live creation of graphics and effects for television broadcast. Although the field typically is dominated by special-purpose video processing hardware, these machines are beginning to show up in a variety of innovative applications.	augmented reality;computation;computational science;computer animation;computer-aided design;graphics;real-time locating system;simulation;video processing	Tim Heidmann	1996		10.1145/237170.237305	computer vision;image-based modeling and rendering;computer facial animation;3d rendering;rendering;painting;computer science;non-photorealistic rendering;computer animation;multimedia;real-time rendering;software rendering;computer graphics (images)	Graphics	-47.28016460125187	-29.946639192901888	133247
a395ff2b13db8795c09b4e7f004742fecb0da448	a novel online textual/graphical domain separation approach for sketch-based interfaces		Multimodal interfaces can be profitably used to manage the increasingly complex applications and services which support human activities in everyday life. In particular, sketch-based interfaces enable users to effortless and powerful communication way to represent concepts and/or commands on different devices. This kind of information can be expressed by users performing two types of object: freehand drawing (graphical domain) and/or handwriting (textual domain). Usually, current frameworks require that users, somehow, indicate whether they are performing one or the other object. In this way, the frameworks can adopt the suitable recognition process to interpret as expressed by users. Moreover, more complex situations can occur when users perform, on a same schema, both types of object. This paper describes a novel intelligent framework able to automatically distinguish, in online way, freehand drawing from handwriting. The proposed approach works taking into account only the mathematical features belonging to the sketch performed by the user during interaction activity. Moreover, the approach can be used on schemata made up by heterogeneous objects which can also be overlapped.		Danilo Avola;Andrea Del Buono;Pierluigi Del Nostro;Rui Wang	2009		10.1007/978-3-642-02937-0_15	human–computer interaction;computer science;multimedia;communication	HCI	-38.278990154999086	-33.06467090895908	133264
0650ae4e51b4b8360e071bd89414873085ff3d4f	anipaint: interactive painterly animation from video	painting;interactive video processing;video signal processing;color;interactive video;nonphotorealistic rendering;video sequences;spectrum;integrated optics;animation image color analysis painting rendering computer graphics video sequences color integrated optics;computer graphic;objective function;automatic synthesis algorithm anipaint interactive painterly animation video sequences automatic stroke synthesis interaction spectrum stroke synthesis keyframed control strokes;interactive system;video signal processing computer animation image sequences;image color analysis;animation;animal imaging;painterly animation;interactive video processing nonphotorealistic rendering painterly animation;temporal coherence;computer animation;vector field;rendering computer graphics;image sequences	This paper presents an interactive system for creating painterly animation from video sequences. Previous approaches to painterly animation typically emphasize either purely automatic stroke synthesis or purely manual stroke key framing. Our system supports a spectrum of interaction between these two approaches which allows the user more direct control over stroke synthesis. We introduce an approach for controlling the results of painterly animation: keyframed Control Strokes can affect automatic stroke's placement, orientation, movement, and color. Furthermore, we introduce a new automatic synthesis algorithm that traces strokes through a video sequence in a greedy manner, but, instead of a vector field, uses an objective function to guide placement. This allows the method to capture fine details, respect region boundaries, and achieve greater temporal coherence than previous methods. All editing is performed with a WYSIWYG interface where the user can directly refine the animation. We demonstrate a variety of examples using both automatic and user-guided results, with a variety of styles and source videos.	animation;automatic differentiation;cerebrovascular accident;coherence (physics);digital artifact;direct manipulation interface;framing (world wide web);greedy algorithm;interactivity;interface device component;key frame;loss function;mathematical optimization;morphologic artifacts;optical flow;optimization problem;specification;speech synthesis;tracing (software);wysiwyg;mixture;videocassette	Peter O'Donovan;Aaron Hertzmann	2012	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2011.51	spectrum;computer vision;vector field;painting;computer science;mathematics;computer animation;multimedia;computer graphics (images)	Graphics	-38.07535291126113	-35.34952929154923	133325
5c0d579fca7bebcb5d429ec0b3c6dea31e04ae67	promenade - an interactive graphics pattern-recognition system	pattern recognition		graphics;pattern recognition	David J. Hall;G. H. Ball;Daniel E. Wolf;J. W. Eusebio	1968			computer graphics;real-time computer graphics;computer graphics (images);computer graphics lighting;3d computer graphics;graphics;graphics software;computer science	Graphics	-46.91677260735842	-29.69168612160846	133388
44b2513e79b59f7867335d3ed00b527def323270	a mobile and adaptive language learning environment based on linked data	mobile;web based;adaptive;technology and engineering;itec;language learning	The possibilities within e-learning environments increased dramatically the last couple of years. They are more and more deployed on the Web, allow various types of tasks and fine-grained feedback, and they can make use of audiovisual material. On the other hand, we are confronted with an increasing heterogeneity in terms of end-user devices (smartphones, tablet PCs, etc.) that are able to render advanced Web-based applications and consume multimedia content. Therefore, the major contribution of this paper is an adaptive, Web-based e-learning environment that is able to provide rich, personalized e-learning experiences to a wide range of devices. We discuss the global architecture and data models, as well as how the integration with media delivery can be realized. Further, we give a detailed description of a reasoner, which is responsible for the adaptive selection of learning items, based on the usage environment and the user profile.	data model;domain model;linked data;personalization;semantic web;semantic reasoner;smartphone;tablet computer;tag cloud;user profile;web application;world wide web	Davy Van Deursen;Igor Jacques;Stefan De Wannemacker;Steven Torrelle;Wim Van Lancker;Maribel Montero Perez;Erik Mannens;Rik Van de Walle	2011			natural language processing;computer science;multimedia;world wide web	Web+IR	-40.972200274329985	-25.35111120023792	133486
fc7df733dacd0cc7dfd15d20e372404e0bb994bb	sketching with chinese calligraphy	vdp teknologi 500 informasjons og kommunikasjonsteknologi 550;user interfaces	Characters Resembling User Interface Components Several of the basic shapes found in Chinese characters resemble common user interface elements. Here we outline some of these shapes and describe the techniques used to draw them correctly. Figure 2 illustrates these shapes, and Figure 3 shows an example of what they might look like in practice. Rectangles: The rectangle is a versatile shape that can represent buttons, text fields, pull-down menus, user interface views, a portion of a view or image, etc. The kou character (mouth) resembles a rectangle. To render it: • Draw the left vertical line in one downward stroke. • Draw the top horizontal line and duced as stripped-down versions of the traditional characters when the Communists took power in China, whereas the traditional characters are still used in Taiwan and Hong Kong. Here we focus mainly on traditional characters, as they contain details that resemble user interface components. Chinese calligraphy requires techniques, acquired through practice, that slavishly follow specific rules. These give Chinese characters their unique visual style. In fact, most Westerners are able to produce reasonably good Chinese characters with relatively little training—all that is needed is to learn the basic rules and to have some practice drawing the basic shapes, called radicals, that are used in the construction of more complex characters. The rules of constructing Chinese characters can be summarized as follows: • Rule 1: Horizontal stroke first, then vertical stroke. • Rule 2: Left falling stroke first, then right falling stroke. • Rule 3: From left to right. • Rule 4: From top to bottom. • Rule 5: From outer to inner. • Rule 6: Outer sealing first, then the inner sealing. Sketches and hand-drawn paper prototypes are popular tools—they are quick to make, inexpensive, and cannot be mistaken for the final product. When drawings take little effort, it is easier to discard them and replace them with new and improved versions. Moreover, during ideation it is important to be able to quickly capture ideas on paper, while they are flowing. Unfortunately, as one’s drawing speed increases, the sketches may become unclear and hard to interpret. Here we illustrate how Chinese calligraphy can serve as an inspiration to speed up sketching while maintaining a clear, consistent, and attractive style. After introducing the basic principles of Chinese calligraphy, we showcase character elements that resemble user interface components and demonstrate how calligraphy techniques can be used to proportion the overall user interface structure. Written Chinese is the common glue that bridges many spoken languages, including the many Chinese dialects, Japanese, Korean, and old Vietnamese. There are two systems in use, traditional characters and simplified characters. Simplified characters were introSketching with Chinese Calligraphy	cjk characters;line level;paper prototyping;record sealing;user interface;vertical bar	Frode Eika Sandnes;Hua-Li Jian	2012	Interactions	10.1145/2377783.2377796	human–computer interaction;computer science;engineering;multimedia;user interface;world wide web	AI	-38.23719304592846	-32.32984031411763	133543
773470e1b1f2bd4cd74ad1f4a1b0621231a4a3b6	binding external interactivity to x3d	interaction;3d model;midi;x3d;external sensors	The VRML and X3D Standards have achieved success as a method for not only 3D Model Interchange, but also for creation of complex synthesized 3D worlds. Shortcomings in VRML and X3D exist in the areas of manipulation of aural soundscapes, and interaction via intuitive devices.  Cognitive and Computer Scientists at the Communications Research Centre, Canada, have embarked on a process of exploration to resolve these shortcomings by binding leading edge audio control software to VRML/X3D, thus using de facto standards to extend I/O control and audio data manipulation. This paper will outline the direction of these experiments.	computer scientist;experiment;input/output;interactivity;polygonal modeling;vrml;x3d	John A. Stewart;Sarah J. Dumoulin;Sylvie Noël	2007		10.1145/1229390.1229409	simulation;computer science;multimedia;world wide web	HCI	-47.38436189009573	-32.83331364135673	133627
b8924741e4918e701b0b9279bf9fecf8d51f9cdb	template based approach for augmenting image descriptions		With the increasing focus on digital learning, it has become extremely important that digital content is available with ease. However, a lot of this digital content is not generated keeping Universal Access in mind. Most of such content available is either completely inaccessible or only partially accessible to the print disabled people. One of the major gaps in accessibility of digital content, especially electronic books is the lack of alternative texts for diagrams and ineffective descriptions in cases they are present. The paper discusses the design of a template, which can help in augmenting descriptions for textbook diagrams. The template consists of various components, which are populated using the information present in the diagram or from the text surrounding the diagram in the textbook. This template provides means for generation of comprehensible diagram descriptions, which not only help the user to visualize the diagram but also create a mental model of the layout of the diagram. Observations made during the user study validate the effectiveness of these augmented descriptions.		Akshansh Chahal;Manshul Belani;Akashdeep Bansal;Neha Jadhav;Meenakshi Balakrishnan	2018		10.1007/978-3-319-94277-3_19	information retrieval;diagram;digital learning;digital content;computer science	AI	-40.319932416797926	-31.19659852045837	133730
b4a34374d10225997680012fb6ab2d5f71834771	generating educational tourism narratives from wikipedia		We present a narrative theory-based approach to data mining that generates cohesive stories from a Wikipedia corpus. This approach is based on a data mining-friendly view of narrative derived from narratology, and uses a prototype mining algorithm that implements this view. Our initial test case and focus is that of field-based educational tour narrative generation, for which we have successfully implemented a proof-of-concept system called Minotour. This system operates on a client-server model, in which the server mines a Wikipedia database dump to generate narratives between any two spatial features that have associated Wikipedia articles. The server then delivers those narratives to mobile device clients.	algorithm;client–server model;data mining;database dump;mobile device;prototype;server (computing);test case;wikipedia	Brent J. Hecht;Nicole Starosielski;Drew Dara-Abrams	2007			knowledge management;multimedia	AI	-41.19487928822962	-24.287163969757184	133818
134d3815b967e713880ba12a4c1ee362270891ed	the personalized, collaborative digital library environment cyclades and its collections management	documento electronico;distributed system;intercambio informacion;informacion numerica;systeme reparti;collaborative work;digital library;interrogation base donnee;interrogacion base datos;recommandation;information space;satisfiability;archive;materialized view;document electronique;digital information;biblioteca electronica;sharing;sistema repartido;particion;automatic detection;archivo;echange information;information exchange;comportement utilisateur;recomendacion;recommendation;information numerique;electronic library;user behavior;materialized views;information system;information need;partage;system management;database query;systeme information;comportamiento usuario;bibliotheque electronique;electronic document;sistema informacion;vue materialisee	Usually, a Digital Library (DL) is an information resource where users may submit queries to satisfy their daily information need. The CYCLADES system envisages a DL additionally as a personalized collaborative working and meeting space of people sharing common interests, where users (i) may organize the information space according to their own subjective view; (ii) may build communities, (iii) may become aware of each other, (iv) may exchange information and knowledge with other users, and (v) may get recommendations based on preference patterns of users. In this paper, we describe the CYCLADES system, show how users may define their own collections of records in terms of un-materialized views over the information space and how the system manages them. In particular, we show how the system automatically detects the archives where to search in, which are relevant to each user defined collection.	archive;digital library;emoticon;information needs;library (computing);materialized view;personalization;programming paradigm;world wide web	Leonardo Candela;Umberto Straccia	2003		10.1007/978-3-540-24610-7_12	materialized view;digital library;computer science;data mining;database;world wide web	HCI	-37.701722148784036	-24.704756607182404	133995
04cf7e538ae5ce3ded54a0d69f2108d74cf85309	context-aware metadata creation in a heterogeneous mobile environment	context aware;context information;mobile device;metadata;heterogeneous environment;data management;image classification;mobile environment;digital image management;heterogeneous wireless network;heterogeneous wireless networks;digital image;mobile agent	With an exponentially-growing amount of digital information, data management is becoming increasingly burdensome for an average user. We propose an enhancement to the existing media-management techniques which utilizes the context available in the surrounding environment around the time a media file is created. This context information is associated with the file to provide enhanced data categorization and searching capabilities. A typical scenario considered in our design involves a heterogeneous environment where users capture digital images using camera-enabled mobile devices, such as iPAQs. Concurrently, these mobile agents also collect environmental data from a variety of sensors and other surrounding wirelessly-enabled devices. At the time of image capture, mobile devices collect and process environmental information, which is then associated with the digital image in the form of metadata. Association of the context with the image makes it possible to build user applications with context-based image classification and query facilities. Our system includes such a GUI application, which provides an image query mechanism based on metadata attributes collected at the time of a picture. Our preliminary evaluation of the system validates successful metadata creation and association with the media files and demonstrates the enhanced searching and classification capabilities using a GUI application.	bluetooth;categorization;computer vision;digital camera;digital data;digital image;digital media;graphical user interface;mobile agent;mobile device;sensor	Olga Volgin;Wanda Hung;Chris Vakili;Jason Flinn;Kang G. Shin	2005		10.1145/1065983.1066002	contextual image classification;mobile search;data management;computer science;mobile agent;mobile device;database;internet privacy;metadata;automatic image annotation;world wide web;digital image;metadata repository	HCI	-33.787627891487205	-35.87079825267624	134292
98c159ac31b6f5e0c72509dda376fa9e87a745f0	isml: an interface specification meta-language	sistema interactivo;interfase usuario;interfaz grafica;graphical interface;user interface;semantics;lenguaje especializado;semantica;semantique;specification language;metalangage;systeme conversationnel;metafora;metalanguage;interactive system;indexation;comportement utilisateur;graphic user interface;interface utilisateur;user interface design;lenguaje especificacion;user behavior;langage specialise;metaphor;special purpose language;langage specification;interface graphique;comportamiento usuario;metaphore;metalenguaje	In this paper we present an abstract metaphor model situated within a model-based user interface framework. The inclusion of metaphors in graphical user interfaces is a well established, but mostly craft-based strategy to design. A substantial body of notations and tools can be found within the model-based user interface design literature, however an explicit treatment of metaphor and its mappings to other design views has yet to be addressed. We introduce the Interface Specification Meta-Language (ISML) framework and demonstrate its use in comparing the semantic and syntactic features of an interactive system. Challenges facing this research are outlined and further work proposed.	executable;fundamental interaction;graphical user interface;interactivity;machine translation;semiconductor industry;situated;user interface design;xml schema;xslt	Simon Crowle;Linda Hole	2003		10.1007/978-3-540-39929-2_25	interface metaphor;human–computer interaction;computer science;graphical user interface;database;semantics;multimedia;programming language;user interface	HCI	-38.74911067538641	-28.005653060902908	134298
b9d40ac3926c793db689336e05f4f4e92e260bea	visualisation multi-échelles pour photos personnelles	pictures;visualization;management	The number of pictures taken every year increases and it is more and more difficult to manage all the pictures we own. In this paper, we present a prototype to manage interactively ours pictures. This prototype allows creating and browsing subgroups of pictures. These subgroups have the property to contain pictures taken in a time interval defined by the user.	image;interactivity;prototype	Emmanuel Nars	2005		10.1145/1148550.1148591	computer vision;computer science;multimedia;computer graphics (images)	DB	-46.05473455723657	-30.810781255260434	134340
4a9b082854f7235009a4b48e8610b5fb492be3a9	visuostm - strategies and user interface concepts for next generation knowledge work systems			user interface;work systems	Clemens Lango	2008	Intelligent Decision Technologies			Robotics	-45.28006580868822	-26.057117685645927	134746
a9c828e619e3b4f75d2d974950c3f15f2a9134f1	special section: point-based graphics: guest editorial: special section on the symposium on point-based graphics 2007	guest editorial;special section;point-based graphics	As point-based graphics primitives and geometry processing techniques have established themselves as strong alternatives to polygonal modeling and rendering, the Symposium on Point-Based Graphics 2007 (PBG 2007) has demonstrated a number of advanced and mature techniques on sample-based object representation and display methods. The fourth symposium in this series was co-located with the Volume Graphics Symposium, as in previous years, as well as with Eurographics, which offered a lot of synergistic opportunities for the organizers and attendees. This special section of Computers & Graphics is composed of the five best and comprehensive papers presented at PBG 2007. Together with the input from the program committee, the papers and conference chairs selected these five papers for invitation to submit an extended journal article version. The authors submitted revised and extended versions of their original work, which were subsequently subjected to a second round of peer reviews based on Computers & Graphics guidelines. The revised and finally accepted papers are briefly introduced below. The first paper, ‘‘Efficient Image Reconstruction for Point-Based and Line-Based Rendering’’ by Ricardo Marroquim, Martin Kraus, and Paulo Roma Cavalcanti targets the fundamental image synthesis problem of generating a smooth and hole-free rendering from a discrete sampling of points. Alternatively to the common splatting approaches, the presented image-space reconstruction solution offers fast rendering of large and adaptively sampled point models. Moreover, the approach extends well to lines and polygonal graphics primitives. Since raw point clouds rarely come as clean and readily usable data sets, efficient processing of very large unstructured point sets is a critical area of development. The paper ‘‘Processing and Interactive Editing of Huge Point Clouds from 3D Scanners’’ by Michael Wand, Alexander Berner, Martin Bokeloh, Philipp Jenke, Arno Fleck, Mark Hoffmann, Benjamin Maier, Dirk Staneker, Andreas Schilling, and Hans-Peter Seidel approaches this problem with a novel multiresolution out-of-core data structure and allows for a number of basic editing operations on large point sets.	3d scanner;computer graphics;core data;data structure;eurographics;geometry processing;h2o (software);hans-peter seidel;iterative reconstruction;out-of-core algorithm;point cloud;polygonal modeling;rendering (computer graphics);representation oligonucleotide microarray analysis;sampling (signal processing);synergy	Mario Botsch;Renato Pajarola	2008	Computers & Graphics	10.1016/j.cag.2008.02.001	computer vision;artificial intelligence;computer science;helicopter rotor;mold;graphics;blanket;overlay;strips;mechanical engineering	Graphics	-34.110028372440325	-29.767680977636182	134821
15465ebc55bb482213e84dd307f66ee425b7d47c	interfaces for interactive audio-visual media browsing	continuous time;multimedia browsing;data stream;audio interfaces;time synchronization;audio skimming;video recording;audio visual;audio interface	In this demo, we present new interfaces for interactive navigation in continuous, time-based multimedia files, such as video recordings. In contrast to common techniques for multimedia skimming, our approaches enable users to interactively navigate audio files along the timeline and support time-synchronized browsing in both audio as well as visual data streams.	interactive media;timeline	Wolfgang Hürst;Tobias Lauer;Robert Kaschuba	2006		10.1145/1180639.1180819	computer science;multimedia;internet privacy;world wide web	HCI	-45.20649367368502	-33.107673558932596	134845
3e39539b3f781193282906eb1a138e7c945ee09d	exploration and implementation of a next generation telepresence system	humanoid robot;context awareness;teleconferencing;context aware;qox;slam;user perception;qualitative analysis;collective intelligence on cloud;affective interfaces;user experience ux experiential telepresence cognition augmented reality context awareness humanoid robot affective interfaces tele operation collective intelligence on cloud slam cloud robotics quality of experience qoe qox quality of service qos;tele operation;video conferencing next generation telepresence system human communication feedback 2 way audio transmission 2 way video transmission nonverbal information experiential telepresence system cognitive intelligence humanoid robot cognitive collective intelligence platform;quality of experience;humanoid robots;video conferencing;user experience;cognition;performance analysis;video transmission;next generation;quality of experience qoe;telecontrol;quality of service qos;navigation cameras face recognition sensors humanoid robots emotion recognition;collective intelligence;facial expression;augmented reality;quality of service;experiential telepresence;video communication humanoid robots next generation networks teleconferencing telecontrol;video communication;next generation networks;cloud robotics;user experience ux	Human communication includes not only spoken language but also non-verbal cues such as hand and body gestures, facial expressions, etc., to communicate our thoughts and feelings and gather feedback. Telepresence systems of today use a 2-way audio and video transmission to transmit this non-verbal information. In this paper, we introduce a novel Experiential Telepresence System, which possesses cognitive intelligence and is also context-ware i.e., it is aware of the multiple components of communication and ambience in which it communicates — both verbal and non-verbal, making the telepresence experience far more immersive when compared to its peers. This is achieved using a 3-tier architecture comprising of a Humanoid Robot, a Cognitive Collective Intelligence Platform on Cloud and an Experience Centre. Towards the end, a performance analysis coupled with a qualitative analysis of user perception which in otherwords is to measure the Quality of Experience of the system — shows the acceptability and user experience of our system is far higher when compared to with traditional telepresence and video conferencing.	collective intelligence;humanoid robot;multitier architecture;profiling (computer programming);symmetric multiprocessing;user experience;warez	Ramachandra Budihal;Navaneeth Mohanan;Sahil A. Anand;Saish Satish Kamat	2011	2011 Fifth IEEE International Conference on Advanced Telecommunication Systems and Networks (ANTS)	10.1109/ANTS.2011.6163637	computer vision;simulation;engineering;multimedia	Robotics	-46.7502737826282	-37.92877338351386	134856
5a089d55850a4b1ea2c806b38587dcc301979efa	course: rapid advanced multimodal multi-device interactive application prototyping with max/jitter, processing, and opengl	software prototyping computer aided instruction computer graphics;processing;processing illimitable space system issv2 opengl real time human computer interfaces interaction kinect jitter max;human computer interfaces;interaction;real time;jitter max;issv2 rapid advanced multimodal multidevice interactive application prototyping maxjitter processing opengl interactive graphical application entertainment computing cgi los angeles siggraph asia 2015 kobe japan illimitable space system v2;issv2;kinect;asia real time systems visualization jitter art streaming media;opengl;illimitable space system	We explore rapid prototyping of interactive graphical applications using Jitter/Max and Processing with OpenGL, and connectivity with various devices such as, Kinect, Wii, and iDevice-based controls. Such rapid prototyping environment is ideal for entertainment computing, as well as for artists and real-time performances that use interactive graphics. As a case study, we share our expertise in connecting realtime graphics with on-stage performances with the Illimitable Space System (ISS) v2. This course complements a full paper and a demo at GEM'15 as well as our demo at the CGI in Asia SIGGRAPH International event at SIGGRAPH'15 in Los Angeles. A related course was accepted for presentation at SIGGRAPH Asia 2015 in Kobe, Japan.	common gateway interface;graphical user interface;kinect;multimodal interaction;opengl;performance;rapid prototyping;real-time computer graphics;real-time transcription;siggraph;wii	Miao Song;Serguei A. Mokhov;Sudhir P. Mudur	2015	2015 IEEE Games Entertainment Media Conference (GEM)	10.1109/GEM.2015.7377246	embedded system;simulation;computer science;computer graphics (images)	Visualization	-44.573337241487685	-34.06002787225967	135005
de73c224f77ec17120744942555a5eb089c590f3	dhtml accessibility: solving the javascript accessibility problem	user agent;web pages;user interface;application program interface;dhtml;html;accessibility;javascript	This project demonstrates fully keyboard accessible components on a web page working with a screen reader. By adding the appropriate semantic data to web components and having user agents translate this to the platform accessibility application programming interfaces, the user interface of a web site can be made fully accessible to keyboard only and vision impaired users. In addition, the web component interface will operate in the same manner as client application components.	accessibility;application programming interface;client (computing);component-based software engineering;dynamic html;javascript;user agent;user interface;web components;web page	Becky Gibson;Richard S. Schwerdtfeger	2005		10.1145/1090785.1090830	web service;user agent;static web page;web application;web modeling;html;web design;human–computer interaction;application programming interface;web accessibility initiative;web standards;computer science;unobtrusive javascript;accessibility;web api;dynamic web page;web navigation;web page;database;dynamic html;javascript;user interface;web 2.0;world wide web	HCI	-42.56399301874306	-28.488782694391183	135093
cdc498e566a3068868bb3c31982fc07e3f21682f	inversion of a physical model of a trumpet		In this paper, we deal with the inversion of a physical model of a trumpet, i.e. how should the player control the model in order to obtain a given sound ? After having shown that the inversion is a ill-posed problem, we add a physically based constraint which leads to a physically pertinent solution .	relevance;well-posed problem	Thomas Hélie;Christophe Vergez;Jean Lévine;Xavier Rodet	1999			physical modelling synthesis;inversion;speech recognition;physical model;musical acoustics	AI	-45.64824582299204	-34.55473246339793	135409
1245c7d3d950f7a214e6cb288e3a02da95c06fc2	embarking on multimodal interface design	speech gesture co analysis;continuous gesture recognition;multimodal interface;software prototyping;human computer interaction multimodal interface design heterogeneous devices multimodal input professional multimodal interaction designers design sketches prototypes wizard of oz techniques interactive application;interface design;wizard of oz;human factors;interactive application;multimodal interaction;real time system;human factors software prototyping user interfaces;visual tracking;user interfaces;user interfaces prototypes handheld computers layout portable computers speech recognition handwriting recognition computer vision process design navigation;multimodal hci;work practice	Designers are increasingly faced with the challenge of targeting multimodal applications, those that span heterogeneous devices and use multimodal input, but do not have tools to support them. We studied the early stage work practices of professional multimodal interaction designers. We noted the variety of different artifacts produced, such as design sketches and paper prototypes. Additionally, we observed Wizard of Oz techniques that are sometimes used to simulate an interactive application from these sketches. These studies have led to our development of a technique for interface designers to consider as they embark on creating multimodal applications.	multimodal interaction;paper prototyping;simulation;user interface design;wizard (software)	Anoop K. Sinha;James A. Landay	2002		10.1109/ICMI.2002.1167021	real-time operating system;speech recognition;human–computer interaction;eye tracking;computer science;human factors and ergonomics;interface design;operating system;multimodal interaction;multimedia;user interface;computer graphics (images)	HCI	-46.63218277474586	-34.877693866890134	135712
efa91dc0986980bd16c22a561955d5d4fcf9e066	connecting strangers at a train station	interactive music;mapping strategies;public installation;virtual instrument;motion tracking;multiple participants music interfaces	In this paper we describe a virtual instrument or a performance space, placed at Høje Tåstrup train station in Denmark, which is meant to establish communicative connections between strangers, by letting users of the system create soundscapes together across the rails. We discuss mapping strategies and complexity and suggest a possible solution for a final instance of our interactive musical performance system.	virtual instrumentation	Ole Gregersen;Lars Pellarin;Jakob Olsen;Niels Böttcher;Michel Guglielmi;Stefania Serafin	2005			computer vision;match moving;simulation;human–computer interaction;computer science;multimedia	HCI	-47.09955957941582	-37.33668745084825	135723
166671d326cd4d3d1708ae20089dcbca8e39b1b3	a dynamic graphics system for simulating virtual buildings	bsp tree;real time;graphics system;rapid prototyping;interaction model;building model;virtual worlds	As part of our graphics research into virtual worlds, we are building a tool for an architect and his client to use for rapid prototyping of buildings by visually “walking through” them in order to refine specifications. Our first prototype simulated the new UNC Computer Science building with some 8000 polygons. BSP-tree software on the Adage Ikonas gave a colored, shaded perspective view every 3-5 seconds while the user moved a cursor in real-time over floorplans shown on the Vector-General 3300. The current (third) version uses Pixel-Planes to generate 9 updates/second, view images shown 4' x 6' by projector. Active short- and long-term research questions include speed-up, stereo, a 6-DoF interface with eye-level defaults, and an interactive model-building, model-changing system.	binary space partitioning;cognitive walkthrough;computer science;cursor (databases);graphics;pixel;prototype;rapid prototyping;real-time transcription;shading;simulation;video projector;virtual world	Frederick P. Brooks	1986		10.1145/319120.319122	binary space partitioning;embedded system;real-time computing;simulation;computer science;artificial intelligence;operating system;computer graphics (images)	Graphics	-38.972800411444105	-34.877408416290365	135867
1be5551ce3878994c032d6290498865bdc3f8e8c	generation as a solution to its own problem.	input device;natural language	Natural language generation technology is now ripe for commercial exploitation, but one of the remaining bottlenecks is that of providing NLG •systems with user-friendly interfaces for Specifying the content of documents to be generated. We present here a new technique we have developed for providing such interfaces: WYSIWYM editing. WYSIWYM (What You See Is What You Meant) makes novel use of the system's generator to provide a natural language input device which requires no NL interpretation	input device;nl (complexity);natural language generation;usability;wysiwym	Donia Scott;Richard Power;Roger Evans	1998			natural language processing;input device;natural language;artificial intelligence;natural language generation;computer science	HCI	-38.84413225287876	-29.073088871401282	135960
a0cf09b68acbc8d263cb02b9b8fbbf436a079e38	swimmy: a framework of multi-agent instruction system for children	visual tool interactive programmed instruction system swimmy squeak etoys multiagent architecture software agents;software agent;computer aided instruction;software agents;multi agent systems;user interfaces computer aided instruction interactive systems interactive programming multi agent systems software agents;educational technology computer architecture software agents application software programming profession displays mice positron emission tomography painting timing;multi agent architecture;interactive systems;user interfaces;interactive programming	The author has developed a framework for an interactive programmed instruction system, named Swimmy, in the Squeak eToys. This system is based on a multi-agent architecture for adaptability and open-endedness. Therefore, Swimmy is flexible for complex situations in classrooms. Moreover, Swimmy aims to be fun and easy to use for children. Children interact with software agents just like interacting with pets. Children can program the agents with the aid of a visual tool just like painting.	agent architecture;etoys;interaction;multi-agent system;software agent;squeak	Koji Yokokawa	2005	Third International Conference on Creating, Connecting and Collaborating through Computing (C5'05)	10.1109/C5.2005.26	agent architecture;simulation;human–computer interaction;computer science;artificial intelligence;software framework;software development;software agent;operating system;multi-agent system;multimedia;resource-oriented architecture;software system	Robotics	-42.76619419933717	-32.27809543809837	136010
7703250a014cbfc66190918a9b666845ef885d9e	user interaction styles in museum hypermedia		"""This paper will analyse a variety of interaction styles that can be designed in hypermedia applications, will point out the need of using them consistently, and will analyze the possible design trade-offs between richness of interaction and easiness of use. Examples will be taken from a museum hypermedia we are currently developing in co-operation with the Poldi Pezzoli Museum in Milano. Multimedia and hypermedia can have a potentially enormous exploitation in museums, addressing different categories of users and serving for a variety of purposes. Just to mention few examples, we may think of hypermedia interfaces to museum databases to support the activity of museum operators, or hypermedia information points, catalogues, or exhibitions locally installed for museum visitors, distributed on CD-ROM's [19, 221 or on the Internet for education or research use. Whatever the intended use of a museum hypermedia, its ultimate goal is comprehension and usability. In other words, it must allow users to understandthe application content, the application goals and """"message"""", and to use the system in a easy and natural way. Comprehension and usability both depend on a number of factors: the quality of contents (the clearness of written texts, the incisiveness of multimedia elements), the elegance of the lay-out, and, last but not least, the quality of user interaction. The term """"interaction"""" refers to how users operate on the various information structures and functionalities of a hypermedia application by acting on lay-out elements. In this paper, we will focus Q Archives & Museum Informatics, 1995 User Interaction Styles in Museum Hypermedia on interaction issues and will analyse a variety of interaction styles that can be provided in hypermedia applications. Our goal is to provide designers with a taxonomy of """"hypermedia interaction styles,"""" among which they can choose the most appropriate ones for their applications and the requirements of their users. In hypermedia systems, the user interacts with the application at various levels, from operating on single information pieces (e.g., video elements) to acting on larger structures such as sequences or trees of nodes, in order to access information. Interaction is strictly related to both the nature of the application content and ofthe information structures used to organize the content itself. Interacting with media such as video and animation, for example, that are intrinsically """"active"""" and """"reactive,"""" is quite different from interacting with media such as images or text, which are basically passive and can be at most scrolled and zoomed. Similarly, accessing information organized in linear structures (such as Guided Tours) can be much simpler than interacting with topologically more complex structures such as trees or arbitrary networks. In order to make a precise analysis on hypermedia interaction styles, we will need an unambiguous terminology to describe data types and structural elements of hypermedia applications. For this purpose, we will adopt the Hypermedia Design Model HDM [4, 6, 8, 9, 161, which provides a rich set of primitives that are appropriate to this purpose. The discussion on the various interaction styles will be exemplified by analysing a museum hypermedia we are currently developing in co-operation with the Poldi Pezzoli Museum in Milano. This application concerns the so-called """"Agostinian Polyptych"""" by Piero della Francesca one of the most mysterious and fascinating masterpieces of this artist and, more generally, Italian Polyptychs in the Renaissance. The studies on the """"Agostinian Polyptych"""", by a team of the researchers at the Poldi Pezzoli Museum, have involved the analysis of other works by Piero dellaFrancesca and by other artists of the same period, as well as the analysis of many ancient and modern documents, in order to track the vicissitudes of the """"Agostinian Polyptych"""" along the centuries (e.g., its possessors, its various restorations, etc.). In addition, the researchers have investigated a number of artistic and cultural events related to sculpture, texture, fashion, jewellery, every-day life, religious life in the Renaissance, that together can contribute to a deeper understanding of Piero's work and, more generally, of the Italian Polyptychs in the Renaissance period: The results of these three-years of research will be presented during a """"traditional exhibition"""" that is under organisation, and will be held at the Poldi Pezzoli Museum in Winter 1995. In parallel, we are developing a hypermedia version, that will provide a multimedia presentation of all the relevant material that is related to the """"Agostinian Polyptych"""" research and have been investigated or discovered in these years. The hypermedia will allow the museum visitors to interactively explore both the material presented at the exhibition and a number of additional artworks, documents, research results, that will not be physically shown within the exhibition. A CD-ROM version is planned for the mid of 1996, to make the application available to a larger public for education or research. O Archives & Museum Informatics, 1995 2 18 Multimedia Computing and Museums ICHIM '94 MCN '95 The rest of this paper is organised as follows. Section 2 provides a short summary of the HDM model; it will define the terminology on which we will found the analysis of various styles of hypermedia interaction, described in Section 3 . This section will also discuss some criteria of usage of the various interaction styles. Section 4 will draw the conclusions. Hypermedia Structural Modelling with HDM The Hypermedia Design Model HDM provides a set of primitives to describe the structures of an existing or to-be-developed hypermedia application, in a concise way, abstracting from implementation issues. This section is a short synthesis of main features of HDM. The interested reader is referred to the bibliography [4, 6, 8, 9, 161 for a more complete presentation of the model. HDM primitives allow descriptions of hypermedia applications according to two levels: in-the-large, where large granules of information are considered, and in-the-small, where small granules of information are taken into consideration. We will describe first the features for modelling in-the-small, and later the features for modelling in-the-large. HDM primitives in-the-small Slot, 9ame and node are the structural primitives in-the-small of the HDM model. A slot represents an atomic piece of information. It can be of a simple type, such as an """"integer"""" (e.g., representing an historical period, say, 1400), """"text"""" (e.g., describing a painting), """"image"""" (e.g., showing a painting), or of a complex type, such as, for example, a video synchronised with a sound track. Structurally speaking, a frame is an aggregate of slots, put together in order to """"present"""" them in a co-ordinated fashion. All the slots related to a given painting, for example, can be put together in a frame. A node is a navigational unit. In HDM, a node is always associated to a frame that represents the node content. A node is interconnected to other nodes, but the organisation of several interconnected nodes belongs to the in-the-large realm, discussed in the following section. HDM primitives in-the-large Entity, component, entity type, collection, link, and link type, are the structural primitives in-the-large 1 that we consider . O Archives & Museum Informatics, 1995 219 User Interaction Styles in Museum Hypermedia Entity An entity is a representation structure that corresponds to some real-world object. A Painter or an Art Period, are all example of entities. An entity is made of a set of components. A component groups together a number of nodes, in a granule that corresponds to a constituent of a real world object. For example, the various life periods of a given artist can be modelled as component of a Painter entity. The arrangement of the nodes, within a component, and of components within an entity can vary, according to the topology of the entity itself: they can be arranged in a sequence, in a set, in a tree, etc. Sometimes, the various nodes within a component denote differentperspectives, i.e., different ways to look at the same piece of information. Entities that correspond to domain objects of the same class are grouped in the same entity type, and all share the same topology. Example The content of the Poldi Pezzoli hypermedia (shortly presented in section 1) has been organized in twelve entity types: Restoration, Reconstruction, Renaissance Fashion, Texture, Polyptych, Jewellery, Sculpture, Archive Documents, Bibliography, Painter Life, Glossary, and General Comment. For some entity types, entities are single-component (i.e. they are represented by one component only); in other cases, entities have a linear structure, a tree-shape structure, or a lattice structure. Usually, each component is made of several nodes; one node, that we call """"main node of the component,"""" summarizes the content associated to the component itself. The other nodes are called """"details nodes"""" and contain a deeper discussion of some specific issue, e.g., details on a specific portion of a Painting presented in the main node. In most cases, each component also contain a node called """"visual presentation"""" which provides a visual perspective of the main node, and only stores a large image concerning the component subject, an image caption, and a short sound comment. As an example, let us analyse the entity type """"Polyptych."""" Each entity of this type is made of various components (arranged as a tree-shaped structure). The (single) node of the root component has the purpose of providing an introduction to a Polyptych. The frame associated to a root node contains the 1 Entities, components, and collections are examples of composites, in the sense defined by the Dexter Hypertext Reference model [ 12, 131 Q Archives & Museum Informatics, 1995 220 Multimedia Computing and Museums ICHI"""	aggregate data;archive;archives & museum informatics;cd-rom;circuit restoration;crystal structure;database;dexter (malware);document;domain-driven design;entity;glossary;granule (oracle dbms);human–computer interaction;hypermedia;hypertext;interactivity;link relation;list comprehension;painter's algorithm;reference model;renaissance;requirement;taxonomy (general);tree (data structure);usability;video	Franca Garzotto;Luca Mainetti;Paolo Paolini	1995			human–computer interaction;interaction styles;multimedia;hypermedia;computer science	Web+IR	-46.72066535907658	-25.692144228325535	136099
5519f4b3205cef439e0f28172404efe411121960	pesce: a visual generator for software understanding	user interface;automatic generation;presentation generation;knowledge based user interfaces;knowledge base;software visualization	We present a short overview of PESCE, a system that addresses the problem of automatically generating consistent visual explanations of software.		Rogelio Adobbati;W. Lewis Johnson;Stacy Marsella	1998		10.1145/291080.291120	user interface design;software visualization;knowledge base;human–computer interaction;computer science;artificial intelligence;operating system;programming language;user interface;world wide web	SE	-40.6656007497881	-29.029000028493392	136179
4b1a3ccd3219d0d4f4918584307d49bb9fa89199	augmented reality for board games	tangible interface;video game;computer vision;graphical user interfaces augmented reality computer games computer vision;graphical user interfaces;tangible interface augmented reality webcam computer vision board game;games augmented reality three dimensional displays computers detectors computer vision cameras;augmented reality;computer games	We introduce a new type of Augmented Reality games: By using a simple webcam and Computer Vision techniques, we turn a standard real game board pawns into an AR game. We use these objects as a tangible interface, and augment them with visual effects. The game logic can be performed automatically by the computer. This results in a better immersion compared to the original board game alone and provides a different experience than a video game. We demonstrate our approach on Monopoly− [1], but it is very generic and could easily be adapted to any other board game.	augmented reality;computer vision;immersion (virtual reality);tangible user interface;turned a;visual effects;webcam	Eray Molla;Vincent Lepetit	2010	2010 IEEE International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2010.5643593	game design;game development tool;video game graphics;computer vision;augmented reality;computer-mediated reality;simulation;level design;human–computer interaction;computer science;game mechanics;game art design;graphical user interface;mixed reality;multimedia;video game development;game programming	Visualization	-42.63801163634887	-37.786315872206345	136309
1dc1fb642b0337eb7edd34bc7242e2d62f42957d	feeling your way around a cave-like reconfigurable vr system		Virtual reality (VR) systems can generate environments that do not exist or are difficult to access. State-of-the-art VR is rapidly evolving and has resulted in enhanced user experience, which leads to a completely immersive experience. Advances in high-resolution displays and highly powerful computer graphics hardware drive the most substantial advancement in VR, which is the introduction of low-cost consumer-grade head-mounted displays (HMD). Despite being inexpensive and providing a high-quality VR experience, commonly used HMDs have a limited field of view, and giving multiple people access to the same virtual environment is inherently challenging. CAVE™ Automated Virtual Environments (CAVE) have benefitted from the same advances in computer graphics hardware and from improvements to binocular (stereo) projection technology, which has reduced the cost and complexity of such systems and increased the visual display quality (resolution, colour, frame rate, etc.). Unlike in HMDs, in a CAVE tracking system, interaction technologies and audio are distinct sub-systems that need to be designed to achieve the desired purpose. A designer needs to consider applications to be used in CAVE and optimise performance. In this paper, we present the design specifications of a reconfigurable CAVE-like VR system that incorporates 6-degree-of-freedom haptic interaction and 3D ambisonic audio. The system was designed for the Centre for Advanced Design in Engineering Training, VR Lab, at Deakin University. Future directions and different use cases along with a comparison matrix are presented to highlight the advantages of the presented system over other existing VR technologies.	binocular vision;cave story;cave automatic virtual environment;computer graphics;graphics hardware;head-mounted display;image resolution;tracking system;user experience;virtual reality	Ben Horan;Mehdi Sevedmahmoudian;Michael Mortimer;Gokul Sidarth Thirunavukkarasu;Stephen Smilevski;Alex Stojcevski	2018	2018 11th International Conference on Human System Interaction (HSI)	10.1109/HSI.2018.8431365	simulation;haptic technology;user experience design;immersion (virtual reality);tracking system;frame rate;virtual machine;virtual reality;computer graphics;computer science	Visualization	-42.3781869343699	-36.62282002997643	136341
9054a19f59296ae837fd1c8822b73ca838695328	development and evaluation of no-record chat system against screen capture	ojo;salida;eye;affichage;ophthalmology;visualizacion;ingenierie connaissances;image databank;defecto;informacion incompleta;frase;information technology;conversacion;technologie information;intelligence artificielle;fuite;vida privada;incomplete information;sentence;private life;display;telecomunicacion;banco imagen;defect;telecommunication;banque image;information incomplete;leak;conversation;defaut;artificial intelligence;vie privee;phrase;inteligencia artificial;tecnologia informacion;private information;oeil;oftalmologia;ophtalmologie;knowledge engineering	To prevent leakage of private information becomes serious issues in the field of information technology. The security of the telecommunication line is almost enough. However, the environment that does a secret conversation is insufficient through the network. We found that it can be embarrassed that the record of the conversation remains though a person wants to communicate privately with another. In this paper, we proposes the chat system that can take communications to solve the above-mentioned problem. We have developed a chat system named ‘you-me Chat.' They cannot capture the screen of the conversation though users can read the conversation sentences in you-me Chat while chatting. In you-me Chat, the character is resolved to imperfect parts as image data. They are displayed continuously. They are imperfect characters even if a person preserved them on a PC as data. A person can read them for image lag of eyes. We present the effectiveness of the system from the experiments.		Takashi Yoshino;Takuma Matsuno	2006		10.1007/11893011_23	speech recognition;private information retrieval;computer science;artificial intelligence;knowledge engineering;database;information technology;computer security;complete information;algorithm	HCI	-34.995163991389084	-26.099312489446273	136559
61229b8a9de85285ef4b99ccbdf0de51a4c1f7a6	asking the right questions: task hierarchy predictive traversal mechanisms for mixed initiative dialog management		This paper describes an approach for building conversational applications that dynamically adjust to the user’s level of expertise based on the user’s responses. In our method, the Dialog Manager interacts with the application user through a mechanism that adjusts the prompts presented to the user based on a hierarchical model of the domain, the recent interaction history, and the known complexity of the domain itself. The goal is to present a conversational modality for experienced or confident users, and a simpler Directed Dialog experience for the more inexperienced users, and to dynamically identify these levels of expertise from the user’s utterances. Our method uses a task hierarchy as a representation of the domain and follows a feedback control system framework to traverse of this tree. We illustrate these mechanisms with a simple sample domain based on a car rental application	control system;dialog manager;feedback;hierarchical database model;modality (human–computer interaction);traverse;user experience	Juan M. Huerta	2004			simulation;computer science;knowledge management;data mining	HCI	-36.81986587521754	-28.583106481250685	136807
36a738f59e701d801150221e4a23b651c3a13f31	virtual prints: augmenting virtual environments with interactive personal marks	developpement logiciel;navegacion;sistema interactivo;interfase usuario;realite virtuelle;realidad virtual;user interface;virtual reality;systeme conversationnel;augmented virtuality;navigation;implementation logiciel;interactive system;desarrollo logicial;concept impression virtuelle;software development;iterative design;interface utilisateur;virtual environment;exploratory study;user interaction	This paper introduces the concept of Virtual Prints (ViPs) as an intuitive metaphor for supporting interaction and navigation, as well as a number of additional tasks in virtual environments (VEs). Three types of ViPs are described: Virtual Footprints, which are used for tracking user navigation (position, orientation and movement), Virtual Handprints, which are used for tracing user interaction with the VE, and Virtual Markers, which are ‘special’ marks (usually coupled with information) that can be created upon user request. In a VE, the ViPs concept is instantiated and supported through a software mechanism (the ViPs mechanism) that allows users to create, manage and interact with their personal ViPs, as well as other users’ ViPs. The paper presents the background and related work upon which the suggested concept builds, as well as the distinctive properties that differentiate ViPs from other related efforts. An account of how users can interact with ViPs is provided and related issues and challenges are discussed along with techniques and methods for addressing them. The paper also describes the process followed towards defining and experimenting with the concept of ViPs by means of iterative design and evaluation of an interactive prototype. This process involved exploratory studies, as well as several inspections and formal tests with both experts and potential end-users, in order to assess the usefulness of the concept and identify possible shortcomings, and also to evaluate and improve the usability of the proposed designs and software prototypes. In general, the findings of the studies reinforce the initial hypothesis that ViPs are an intuitive and powerful concept, and show that the related software is easy to learn and use. Overall, the results of the studies support strong evidence that an appropriately designed and implemented, fully functional ViPs mechanism can significantly increase the usability of VEs. r 2005 Elsevier Ltd. All rights reserved.	application domain;augmented reality;compiler;experiment;handy board;humans;input device;interface metaphor;iteration;iterative design;norm (social);prototype;real life;rendering (computer graphics);requirement;software inspection;software prototyping;text corpus;universal instantiation;usability;user requirements document;utility;virtual reality	Dimitris Grammenos;Alexandros Mourouzis;Constantine Stephanidis	2006	International Journal of Man-Machine Studies	10.1016/j.ijhcs.2005.08.011	iterative design;navigation;simulation;human–computer interaction;computer science;virtual machine;artificial intelligence;software development;operating system;virtual reality;multimedia;augmented virtuality;user interface;exploratory research	HCI	-37.97323600210516	-27.16819753428176	136848
43416fd8a509d813882d2b344160d60e7543edbd	open domain collaborative storytelling with say anything	3d virtual environment;information retrieval;satisfiability;interactive media;interactive storytelling;interaction model	In this demonstration we present Say Anything, an open domain interactive storytelling application where an author's original story sentences are used to select subsequent sentences from a corpus of millions of stories extracted from Internet weblogs. Demonstration: Say Anything Interactive storytelling has become a rich area of research for computer science, interactive-media and the learning sciences. In this demonstration we will present our system, Say Anything (Swanson and Gordon 2008). This application uses a different approach from most interactive storytelling systems in that it takes advantage of massive amounts of weblog text to enable completely open domain story generation. Say Anything breaks from recent interactive storytelling systems in its interaction model as well as its underlying architecture. Unlike most current systems, which use rich immersive 3D virtual environments, the interaction with our system is purely textual. Although 3D virtual environments offer many advantages and possibilities that were not possible when research in this area first began, there are still many reasons that textual environments have significant value (Montfort 2007). In our system, a human and computer interact by taking turns contributing individual sentences of an emerging story. The human user begins the story with the first sentence, which is continued by the computer’s response. This process continues cyclically until the user is satisfied (or sufficiently dissatisfied) with their story. The computer performs three major operations to generate a sentence for the developing story. First, it analyzes lexical and grammatical features of the user’s story, including the most recently contributed sentence. This analysis is then used to retrieve the most similar sentence from a database of millions of stories collected from Internet weblogs (Gordon et al. 2007). The measure of similarity is based on standard information retrieval techniques implemented in the Apache Lucene search Copyright © 2009, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. engine (Gospodnetic and Hatcher 2004). The system then selects the next sentence in the story where the retrieved sentence appears. In addition to this sentence, the user is also presented with nine other sentences that follow from the next most similar sentences, which they can choose from using a drop-down menu. Alternatively, if none of the sentences are satisfactory to the user, they may opt to remove it completely and continue the story by writing another sentence themselves. On the other hand if the user is pleased with the direction the computer is taking the story they can allow the computer to continue generating sentences, one at a time. See Figure 1 for an example story written by one of our users. Our system is collaborative in the sense that the user and computer are working together to create a single story. It is also collaborative in the sense that our database consists of stories from hundreds of thousands of weblog authors who contribute a small piece to each new story created by our system. Keeping with this collaborative spirit we chose to use a web interface for our system in hopes of spurring a community that can write, share and rate each other’s stories. See Figure 2 for an illustration of the main writing interface page of our system. In the demonstration of our system we will allow users to author their own original stories in real time. In addition, the users will be able to explore the features we have implemented to try to foster a web-based community around this type of simple storytelling game.	artificial intelligence;blog;box counting;computer science;context-free grammar;database;inferring horizontal gene transfer;information retrieval;interactive storytelling;lexicon;real-time computing;user interface;virtual reality;web application	Reid Swanson;Andrew S. Gordon	2009			human–computer interaction;computer science;multimedia;interactive media;world wide web;satisfiability	AI	-44.28137650377407	-31.752993608982127	137208
75b479c4127bb3a322424ff7540f8a74edbc1b19	salto - a versatile multi-level annotation tool		In this paper, we describe the SALTO tool. It was originally developed for the annotation of semantic roles in the frame semantics paradigm, but can be used for graphical annotation of treebanks with general relational information in a simple drag-and-drop fashion. The tool additionally supports corpus management and quality control.	drag and drop;graphical user interface;programming paradigm;treebank	Aljoscha Burchardt;Katrin Erk;Anette Frank;Andrea Kowalski;Sebastian Padó	2006			artificial intelligence;natural language processing;computer science;annotation	NLP	-38.83095063252724	-29.074603674908403	137258
dcf96b82fca1184fe63db928a4aae59d4d959340	everyday encounters with context-aware computing in a campus environment	sensibilidad contexto;distributed system;context aware computing;movilidad;systeme reparti;context aware;informatique mobile;mobility;pervasive computing;localization;localizacion;mobilite;informatica difusa;sistema repartido;localisation;informatique diffuse;contexto;contexte;institutional arrangement;ubiquitous computing;actitud;sensibilite contexte;mobile computing;attitude;context	As ubiquitous computing technologies mature, they must move out of laboratory settings and into the everyday world. In the process, they will increasingly be used by heterogeneous groups, made up of individuals with different attitudes and social roles. We have been studying an example of this in a campus setting. Our field work highlights the complex relationships between technology use and institutional arrangements – the roles, relationships, and responsibilities that characterize social settings. In heterogeneous groups, concerns such as location, infrastructure, access, and mobility can take on quite different forms, with very different implications for technology design and use.	embedded system;emergence;experience;field research;high-level programming language;social structure;software deployment;ubiquitous computing;usability	Louise Barkhuus;Paul Dourish	2004		10.1007/978-3-540-30119-6_14	attitude;simulation;human–computer interaction;computer science;operating system;mobile computing;ubiquitous computing	HCI	-35.77932410227216	-25.333191849982665	137471
7b38cd34c81e2ac09794e82d85468edeee042ba0	model-based development of interactive multimedia system		The increase of the number of flights and connections has led to new demands in terms of passengers' media experience and consumption of In-Flight Entertainment (IFE) systems. For this purpose, we have designed and developed an IFE multimedia system architecture using the Model- Based Development (MBD) approach. Our IFE system has been implemented at both software and hardware levels and includes an LCD screen and Arduino microcontrollers. Hence, our embedded, real-time system allows users to interactively and easily access multimedia content through a low-cost IFE system which has been successfully tested to a standard required by certification authorities.	arduino;certificate authority;coherence (physics);embedded system;interactive media;microcontroller;model-based definition;real-time clock;real-time computing;requirement;systems architecture	Scott Leonard;Joanna Isabelle Olszewska	2017	2017 3rd IEEE International Conference on Cybernetics (CYBCON)	10.1109/CYBConf.2017.7985791	interactive media;model-based design;microcontroller;embedded system;certification;systems architecture;arduino;software;engineering	Robotics	-43.274444096576175	-33.0072315412862	137482
519f964312cdd39305459c4dd0c793ad1dcab731	mcml: motion capture markup language for integration of heterogeneous motion capture data	extensible markup language;mcml;motion capture data;motion capture;human body;xml;markup language;motion capture file format	Motion capture technology is widely used for manufacturing animation since it produces high-quality character motion similar to the actual motion of the human body. However, motion capture has a significant weakness due to the lack of an industry-wide standard for archiving and exchanging motion capture data. It is difficult for animators to reuse and exchange motion capture data with each other. In this paper, we propose a standard format for integrating different motion capture file formats. Our standard format is called Motion Capture Markup Language (MCML). It is a markup language based on eXtensible Markup Language (XML). The purpose of MCML is not only to facilitate the conversion or integration of different formats, but also to allow for greater reusability of motion capture data, through the construction of a motion database storing the MCML documents.	markup language;motion capture	Hyun-Sook Chung;Yillbyung Lee	2004	Computer Standards & Interfaces	10.1016/S0920-5489(03)00071-0	xml;computer science;database;multimedia;programming language;computer graphics (images)	DB	-41.63422234556032	-33.2263888824862	137540
ec425f970f32dc73245d4b7a7d46eccb99144f20	interacción en tiempo real para un sistema de escultura virtual	categories and subject descriptors according to acm ccs i 3 6 computer graphics methodology and techniques interaction techniques			Alejandro León;Francisco Velasco;Francisco Ortin	2008		10.2312/LocalChapterEvents/CEIG/CEIG08/105-111	computer science;algorithm;computer graphics (images)	Vision	-46.9273704992641	-31.01514929032162	137601
d2d1e08f60de082d7283dabaf5fca49ccc04432a	illustrative shadows: integrating 3d and 2d information displays	interactive 3d graphics;semantic network;information space;information visualization;multiple views;spatial relation;spreading activation;user interaction	Many exploration and manipulation tasks benefit from a coherent integration of multiple views onto complex information spaces. This paper proposes the concept of Illustrative Shadows for a tight integration of interactive 3D graphics and schematic depictions using the shadow metaphor. The shadow metaphor provides an intuitive visual link between 3D and 2D visualizations integrating the different displays into one combined information display. Users interactively explore spatial relations in realistic shaded virtual models while functional correlations and additional textual information are presented on additional projection layers using a semantic network approach. Manipulations of one visualization immediately influence the others, resulting in an in-formationally and perceptibly coherent presentation	3d computer graphics;coherence (physics);display device;interactivity;schematic;semantic network;shading	Felix Ritter;Henry Sonnet;Knut Hartmann;Thomas Strothotte	2003		10.1145/604045.604072	spatial relation;computer vision;information visualization;computer science;linguistics;multimedia;spreading activation;semantic network;computer graphics (images)	HCI	-35.8865871618508	-34.051114068565916	137886
f91826ca57061ae33ad559eac6323078c4883366	"""interaction based on function of a table in real world with """"kage no sekai"""""""	tangible user interface;interactive animation;shadow;physical action;tabletop display	"""Our research is about an interaction based on the function of a table in real world. A tabletop type display is very popular these days, and most of it focuses on the process of presenting information and the interaction with people. These, however, are forgetting the function as a table. The function of a table is related to various purposes in using tabletop such as studying, and it is necessary for our daily life. Therefore in this research, we looked at the characteristics of the table. The content """"Kage no Sekai"""" (Figure1) provides information through shadows that are not distracting while work. This enables the users to keep working without being so much conscious of the digital world. Moreover, it is fully possible for this content to be placed in daily use such as information desks, for it consists of only light and shadow."""	table (database)	Yu Uchida;Mami Naito;Shiho Hirayama;Masa Inakage	2007		10.1145/1306813.1306859	shadow;human–computer interaction;multimedia;computer graphics (images)	HCI	-44.38687024429658	-37.51310159380367	137910
ff58422377ff013776c1c168fde4f069cc837031	personal and temporary hyper bridges: 2-d interface for undefined topics	vocabulaire;red www;information retrieval;vocabulary;hyper bridge;vocabulario;internet;recherche information;interest expression;world wide web;interface utilisateur;vocabulary of user;reseau www;new topics;user interfaces;personal and temporal hyper bridges	It is not always obvious for people how important new topics, which appear and disappear dynamically on the WWW, are for themselves because one does not have concrete or enough vocabulary for new topics. This paper presents an interface Personal and Temporal Hyper Bridges (PTHB) for a user vaguely interested in new topics, but can not express one's own interest for oneself. PTHB supplies a Web-site reading plan, which leads the user to concrete comprehension of the topics.	undefined behavior	Yukio Ohsawa;Kenki Matsuda;Masahiko Yachida	1998	Computer Networks	10.1016/S0169-7552(98)00032-4	the internet;human–computer interaction;computer science;multimedia;user interface;world wide web	HCI	-38.7673812923056	-25.74680170618945	138045
d500daec1e195793771d57f73f89bf4665dec554	a novel real-time video transmission approach for remote laboratory development		Remote laboratories are an inevitable necessity for Internet enabled education in Science, Technology, Engineering, and Math (STEM) fields due to their effectiveness, flexibility, cost savings, and the fact that they provide many benefits to students, instructors and researchers. Accordingly, real-time experiment live video streaming is an essential part of remote experimentation operation. Nevertheless, in the development of real-time experiment video transmission, it is a key and difficult issue that the video is transferred across the network firewall in most of the current remote laboratory solutions. To address the network firewall issue, we developed a complete novel solution via HTTP Live Streaming (HLS) protocol and FFMPEG that is a powerful cross-platform command line video transcode/encoding software package on the server side. In this paper, a novel, real-time video streaming transmission approach based on HLS for the remote laboratory development is presented. With this new solution, the terminal users can view the real-time experiment live video streaming on any portable device without any firewall issues or the need for a third party plug-in. This new solution also significantly benefits remote laboratory development in the future.	command-line interface;encoder;experiment;ffmpeg;firewall (computing);http live streaming;high-level synthesis;hypertext transfer protocol;internet;mobile device;plug-in (computing);real-time clock;real-time transcription;server (computing);server-side;streaming media	Ning Wang;Xuemin Chen;Gangbing Song;Hamid R. Parsaei	2015	iJOE		embedded system;simulation;telecommunications;computer science;electrical engineering;operating system;multimedia;world wide web	Embedded	-43.44827704729393	-33.00210226462898	138138
660c831ebf88f857888371c3da25e3feebcb8b02	introduction to the icme2010 special issue	special issues and sections;multimedia systems;multimedia computing;special issues and sections meetings multimedia systems user interfaces multimedia computing;meetings;user interfaces	The 15 papers in this special issue are extended versions of papers presented at the 2010 IEEE International Conference on Multimedia and Expo (ICME), held in Singapore on July 19-23, 2010. These papers cover a wide range of topics in multimedia including user interface, content understanding, mobility, 3-D processing, storage, and forensics.		Zicheng Liu;Ming-Ting Sun;Chia-Wen Lin;Zhengyou Zhang;Zhu Liu;Homer H. Chen;Yap-Peng Tan;Oscar C. Au	2011	IEEE Trans. Multimedia	10.1109/TMM.2011.2136510	human–computer interaction;computer science;operating system;multimedia;user interface;world wide web	Visualization	-48.1309904807022	-25.039553465472753	138308
0b83ad2d3719bd392cd0c7219f9176b70199182d	examining virtual busts: are photogrammetrically generated head models effective for person identification?	3d visualization	We examined the effectiveness of using 3D, visual, digital representations of human heads and faces (i.e., virtual busts) for person identification. In a series of 11 studies, participants learned a number of human faces from analog photographs. We then crafted virtual busts from those analog photographs, and compared recognition of photographs of the virtual busts to the original analog photographs. We demonstrated that the accuracy of person identification using photographs of virtual busts is high in an absolute sense, but not as high as using the original analog photographs. We present a paradigm for comparing the similarity, both structural (objectively similar in shape) and subjective (subjectively in the eyes of a viewer) of virtual busts to analog photographs, with the goal of beginning the discussion of a uniform standard for assessing the fidelity of digital models of human faces.	photogrammetry;programming paradigm	Jeremy N. Bailenson;Andrew C. Beall;Jim Blascovich;Christopher Rex	2004	Presence: Teleoperators & Virtual Environments	10.1162/1054746041944858	computer vision;simulation;visualization;computer science;computer graphics (images)	Visualization	-36.909219434109765	-36.15230999093522	138319
9b8d39900a18e842d6a5185c79c5cfc226ca08de	introduction to the special section on the fifth international workshop on multimedia information systems	information systems;special issues and sections;multimedia information system;multimedia systems;indexing;special issues and sections multimedia systems multimedia databases information systems bandwidth multimedia communication virtual environment conferences animation indexing;multimedia communication;animation;multimedia databases;bandwidth;virtual environment;conferences	MULTIMEDIA systems play a major role in a number of applications, including educational applications, entertainment technology, e-commerce, medical, database, and library information systems. Designing and developing multimedia information systems involves a multitude of aspects including: acquisition, compression, storage, access, presentation, and communication. The main characteristics of multimedia applications are: 1) large bandwidth and storage requirements, 2) low communication latency requirements, and 3) synchronization requirements of various multimedia sources, all of which are often coupled with real-time constraints. Furthermore, in designing and building large high-performance multimedia information and communication systems, one must consider a whole spectrum of applications, from relatively low bandwidth, high throughput, and ajust-intimeo delivery of video-on-demand servers to very high bandwidth, relatively low volume, and aASAPo delivery of supercomputing/scientific applications. Thus, such systems must be able to accommodate the various storage, performance, and reliability requirements of the different types of media and applications. Papers presented in the special section address some of these problems. This special section is based on the Fifth International Workshop on Multimedia Information Systems (MIS '99), held on 21-23 October 1999 at the Miramonte Resort in Indian Wells, Palm Springs Desert, California. MIS '99 is the fifth in a series of workshops which started in 1995 with the aim of fostering interdisciplinary discussions and research in all aspects of multimedia information systems, including databases, networks, operating systems, graphics and visualization, artificial intelligence, real-time systems, theory, and algorithms. Authors were invited to submit extended abstracts. Selections were based on originality and contribution to the field. In addition to the workshop proceedings, four papers were selected, based on program committee recommendations, as full papers for this special issue. A brief introduction to each paper follows: The first paper in this issue, aScheduling Algorithms for the Broadcast Delivery of Digital Products,o is by Joel L. Wolf, Mark S. Squillante, John J. Turek, Philip S. Yu, and Jay Sethuraman. The focus of this work is on a broadcast-based electronic delivery service for purchasing digital products. Examples of such services include e-commerce sites on the World Wide Web with products including a variety of multimedia objects ranging from DVDs to software to electronic books. In this work, the authors focus on scheduling algorithms which attempt to maximize the profits of such a service. The second paper, aMinimizing Bandwidth Requirements for On-Demand Data Deliveryo by Derek Eager, Mary Vernon, and John Zahorjan, concentrates on ondemand multicast (or broadcast) delivery of streaming media where the goal is to achieve significant client stream sharing. This, in turn, would lead to considerable savings in server and network bandwidth. The third paper, aIndexing Animated Objects Using Spatiotemporal Access Methodso by George Kollios, Vassilis J. Tsotras, Dimitrios Gunopulos, Alex Delis, and Marios Hadjieleftheriou, describes an approach for indexing animated objects so as to efficiently answer queries about object positions in time and space. This is a novel indexing problem given that objects are allowed to move as well as change their extent continuously between frames. Finally, the paper by John C.S. Lui on aConstructing Communication Subgraphs and Deriving an Optimal Synchronization Interval for Distributed Virtual Environment Systemso examines the construction of communication subgraphs for a distributed virtual environment system which allows clients located in different parts of the network to concurrently explore and interact with each other in a high resolution, 3D, graphical virtual environment. Design and implementation of multimedia systems presents and will continue to present great challenges to computer scientists and engineers. We expect that, in the coming years, there will be a tremendous growth in the use of multimedia applications and, thus, the design of efficient and cost-effective multimedia information and communication systems will be of central importance to the growth of a multitude of applications. We offer this collection of papers IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 13, NO. 5, SEPTEMBER/OCTOBER 2001 719	algorithm;artificial intelligence;book;computer scientist;data compression;database;e-book;e-commerce;graphical user interface;graphics;information systems;information system;john c.s. lui;multicast;operating system;purchasing;real-time computing;real-time locating system;real-time web;requirement;scheduling (computing);server (computing);streaming media;supercomputer;systems theory;throughput;virtual reality;world wide web	Leana Golubchik;Satish K. Tripathi;Vassilis J. Tsotras	2001	IEEE Trans. Knowl. Data Eng.	10.1109/TKDE.2001.956095	anime;search engine indexing;synchronized multimedia integration language;computer science;virtual machine;multimedia;world wide web;information retrieval;information system;bandwidth	DB	-48.105929452389894	-24.900204376635347	138389
8174813ba2abe715617f8527842357e8a3e88b11	interactive design of multi-perspective images for visualizing urban landscapes	city block;town and country planning;cross slits image;interpolation;interpolation data visualisation town and country planning graphical user interfaces cameras ray tracing image segmentation;image segmentation;multi perspective image;visualization cameras computer graphics cities and towns image generation layout fabrics art application software optical distortion;data visualisation;graphical user interfaces;interactive system;ray tracing;city block interactive design multiperspective images urban landscape visualization cross slits images interpolation scheme;city block cross slits image multi perspective image;interaction design;cameras	Multi-perspective images are a useful way to visualize extended, roughly planar scenes such as landscapes or city blocks. However, constructing effective multi-perspective images is something of an art. In this paper, we describe an interactive system for creating multi-perspective images composed of serially blended cross-slits images. Beginning with a sideways-looking video of the scene as might be captured from a moving vehicle, we allow the user to interactively specify a set of cross-slits cameras, possibly with gaps between them. In each camera, one of the slits is defined to be the camera path, which is typically horizontal, and the user is left to choose the second slit, which is typically vertical. The system then generates intermediate views between these cameras using a novel interpolation scheme, thereby producing a multi-perspective image with no seams. The user can also choose the picture surface in space onto which viewing rays are projected, thereby establishing a parameterization for the image. We show how the choice of this surface can be used to create interesting visual effects. We demonstrate our system by constructing multi-perspective images that summarize city blocks, including corners, blocks with deep plazas and other challenging urban situations.	interactive design;interactive media;interactivity;interpolation;visual effects	Augusto Román;Gaurav Garg;Marc Levoy	2004	IEEE Visualization 2004	10.1109/VISUAL.2004.50	city block;ray tracing;computer vision;interpolation;computer science;interaction design;graphical user interface;multimedia;image segmentation;data visualization;statistics;computer graphics (images)	Vision	-34.97646532103511	-32.539474055531144	138487
373e9895cef2788a1ff58a393a8259bcf1e97652	intermedia: the concept and the construction of a seamless information environment	user interfaces electronic publishing software tools;object oriented framework;education joining processes;software tools;electronic publishing;user interfaces;object oriented framework hypermedia seamless information environment intermedia tool teaching research university environment multiapplication hypermedia system linking capabilities desktop user environment hypertext	A description is given of Intermedia, a tool designed to support both teaching and research in a university environment. This multiapplication hypermedia system provides linking capabilities integrated into a desktop user environment. Hypermedia is simply an extension of hypertext that incorporates other media in addition to text. To promote consistency, the applications were built with an object-oriented framework. A sample Intermedia session is presented.<<ETX>>	desktop computer;hypermedia;hypertext;intermedia (hypertext);seamless3d;user interface	Nicole Yankelovich;Bernard J. Haan;Norman K. Meyrowitz;Steven M. Drucker	1988	Computer	10.1109/2.222120	human–computer interaction;computer science;operating system;multimedia;electronic publishing;user interface;world wide web	Web+IR	-42.66437757593243	-26.459235364746903	138649
e201baee82bfb7bef73e1c3695af0768ebd8b0c1	cloudlet-screen computing: a client-server architecture with top graphics performance	pervasive cloud mobile computing;multimedia;display latency;client server architecture;cloudlet screen computing;pervasive computing;screen content coding;cloudlets;graphics performance;performance analysis;ubiquitous computing;video;mobile computing;cloud computing	This paper presents a three-decoupling-boundary theory and classification for performance analysis of client-server architectures. Based on the analysis, a novel client-server architecture with top graphics performance is proposed. The client is only a display screen plus a few Human-Interface-Devices such as a mouse-keyboard-set or touchscreen with an optional mobile storage. The client is connected to a remote or nearby server cloudlet by a low latency link transferring primarily post-GPU display screen and HID data. By following exactly the same most-efficient CPU-GPU-screen graphics rendering pipeline from initial graphics primitives to final pixels as that in the traditional computer graphics architecture developed and optimised in the past decades, the architecture has higher graphics and multimedia performance than any other client-server and cloud-mobile computing architectures, e.g. VNC, Remote Desktop, VDI, Zero-client, and PCoIP. No read back of frame buffers as virtualised screen is necessary, so no CPU/GPU overhead occurs and display latency is minimised.	client–server model;cloudlet;graphics;server (computing)	Tao Lin;Kailun Zhou;Shuhui Wang	2013	IJAHUC	10.1504/IJAHUC.2013.054174	embedded system;simulation;video;human–computer interaction;cloud computing;computer hardware;computer science;operating system;real-time computer graphics;ubiquitous computing;general-purpose computing on graphics processing units;client–server model;computer graphics (images)	Arch	-45.738339171176186	-31.36968296282995	138682
1297f73669447921695d6b4ba69496d9c13b9a35	an audio processing library for game development in flash	libraries;rhythm;audio signal processing;instruments;c compilation;real time;music audio signal processing computer games internet;video game;game development;game music interaction;games music signal processing algorithms graphics software libraries performance analysis toy industry programming profession animation user interfaces;dynamic audio control;game music interaction audio processing library music centric video game adobe flash web based platform flash scripting dynamic audio control c compilation;internet;feature extraction;games;music centric video game;flash scripting;adobe flash;web based platform;computer games;audio processing library;music;real time systems	In recent years, there has been sharp rise in the number of games on web-based platforms, which are ideal for rapid game development and easy deployment. In a parallel but unrelated trend, music-centric video games that incorporate well-known popular music directly into the gameplay (e.g., Guitar Hero and Rock Band) have attained widespread popularity on console platforms. The limitations of such web-based platforms as Adobe Flash, however, have made it difficult for developers to utilize complex sound and music interaction within web games. Furthermore, the real-time audio processing and synchronization required in music-centric games demands significant computational power and specialized audio algorithms, which have been difficult or impossible to implement using Flash scripting. Taking advantage of features recently added to the platform, including dynamic audio control and C-compilation for near-native performance, we have developed the Audio processing Library for Flash (ALF), providing developers with a library of common audio processing routines and affording web games with a degree of sound interaction previously available only on console or native PC platforms. We also present several audio-intensive games that incorporate ALF to demonstrate its utility. One example performs real-time analysis of songs in a user's music library to drive the gameplay, providing a novel form of game-music interaction.	actionscript;adobe flash;algorithm;chorusos;documentation;graphics;kinetic data structure;modulation;open-source software;palette (computing);real-time cmix;software deployment;usability;user-generated content;video game development;web application	Raymond Migneco;Travis M. Doll;Jeffrey J. Scott;Christian M. Hahn;Paul J. Diefenbach;Youngmoo E. Kim	2009	2009 International IEEE Consumer Electronics Society's Games Innovations Conference	10.1109/ICEGIC.2009.5293603	simulation;computer science;multimedia;computer graphics (images)	DB	-42.603381786479154	-33.5872956620712	138691
6b5e9f15c57389dd8810a653a1abfa1dd7a1e49c	symmetric model of remote collaborative mr using tangible replicas	groupware;virtual reality groupware;collaborative interaction;virtual reality;usability mixed reality remote collaboration collaborative interaction;collaboration virtual reality collaborative work feedback virtual environment haptic interfaces biological system modeling environmental factors augmented reality displays;virtual reality remote collaborative mixed reality augmented reality tangible replicas;immersive virtual reality;augmented reality;usability;mixed reality;tangible replicas;remote collaboration;remote collaborative mixed reality	"""Research into collaborative mixed reality (MR) or augmented reality has recently been active. Previous studies showed that MR was preferred for collocated collaboration while immersive virtual reality was preferred for remote collaboration. The main reason for this preference is that the physical object in remote space cannot be handled directly. However, MR using tangible objects is still attractive for remote collaborative systems, because MR enables seamless interaction with real objects enhanced by virtual information with the sense of touch. Here we introduce """"tangible replicas""""(dual objects that have the same shape, size, and surface), and propose a symmetrical model for remote collaborative MR. The result of experiments shows that pointing and drawing functions on the tangible replica work well despite limited shared information."""	augmented reality;existential quantification;experiment;immersion (virtual reality);mixed reality;seamless3d;virtual reality	Shun Yamamoto;Hidekazu Tamaki;Yuta Okajima;Ken-ichi Okada;Yuichi Bannai	2008	2008 IEEE Virtual Reality Conference	10.1109/VR.2008.4480753	augmented reality;computer-mediated reality;simulation;usability;human–computer interaction;computer science;artificial intelligence;virtual reality;mixed reality;multimedia;immersion	Visualization	-44.06607729003769	-37.81105213419586	138887
1e24608202794176c4fcd94be7bc15f4615abea2	personalized guided routes in an adaptive evolutionary hypermedia system	metodo adaptativo;hipertexto;evolutionary model;itineraire;itinerario;user adaptation;semantics;methode adaptative;intelligence artificielle;semantica;semantique;systeme adaptatif;hypermedia;route;adaptive method;adaptive system;preferencia;sistema adaptativo;adaptive hypermedia system;artificial intelligence;algorithme evolutionniste;algoritmo evolucionista;preference;inteligencia artificial;evolutionary algorithm;hypertexte;hipermedia;hypertext;user model	In this paper we describe an adaptation method for adaptive hypermedia systems, consisting in personalized guided routes for the SEM-HP model. SEM-HP is a layered, systemic, semantic and evolutionary model for the development of adaptive hypermedia systems, which adapt to the particular features and interests of each user. For evolution it uses a Metasystem, which offers to the author a set of evolutionary actions that permit the hypermedia system to evolve in a flexible and consistent way. In SEM-HP a hypermedia system is composed by four subsystems, each of which offers a different functionality to the user and to other subsystems. User adaptation is carried out by the learning subsystem, which keeps and updates an user model, which includes the user knowledge about the informational elements offered by the system, his preferences and his goal, which is to reach a certain degree of knowledge. Guided routes direct the user through the hypermedia system, so the user goal can be reached in an optimal way.	adaptive hypermedia;meta-system;models of dna evolution;personalization;platoon (automobile);prototype;semantic network;software engineering;systemics;user (computing)	Nuria Medina-Medina;Fernando Molina-Ortiz;Lina García-Cabrera;José Parets-Llorca	2003		10.1007/978-3-540-45210-2_19	route;simulation;user modeling;hypertext;computer science;artificial intelligence;adaptive system;machine learning;evolutionary algorithm;semantics;world wide web	Web+IR	-38.03943883494574	-25.739184916988044	139002
a9a03590e2cec13dfadf99bc0ef30d3b81854842	providing end-user facilities to simplify ontology-driven web application authoring	web based applications;intelligent user interfaces;human computer interaction;intelligent user interface;programming by example;end user development;user interface;user study;professional development;ease of use;semantic web;model based user interfaces;article	Generally speaking, emerging web-based technologies are mostly intended for professional developers. They pay poor attention to users who have no programming abilities but need to customize software applications. At some point, such needs force end-users to act as designers in various aspects of software authoring and development. Every day, more new computing-related professionals attempt to create and modify existing applications in order to customize web-based artifacts that will help them carry out their daily tasks. In general they are domain experts rather than skilled software designers, and new authoring mechanisms are needed in order that they can accomplish their tasks properly. The work we present is an effort to supply end-users with easy mechanisms for authoring web-based applications. To complement this effort, we present a user study showing that it is possible to carry out a trade-off between expressiveness and ease of use in order to provide end-users with authoring facilities.	application domain;artifact (software development);cut, copy, and paste;debugging;desktop computer;drag and drop;dynamic web page;end-user development;html element;insertion sort;maximal set;on the fly;pegasus;programmer;programming by example;programming paradigm;s-box;semantic web;server (computing);subject-matter expert;usability testing;user interface;wimp (computing);wysiwyg;web application;web content;web development tools;world wide web	José A. Macías;Pablo Castells	2007	Interacting with Computers	10.1016/j.intcom.2007.01.006	user interface design;professional development;web service;web application;web modeling;usability;web design;human–computer interaction;computer science;operating system;software engineering;semantic web;multimedia;user interface;world wide web	HCI	-42.26229621960193	-30.648768089209643	139068
300fc55c157d6619aa225d20ba3e4b0a678cb8ea	graphical representation of models and results in the finite element system cosar	modelizacion;concepcion asistida;computer aided design;representation graphique;cosar;methode element fini;metodo elemento finito;computer graphics;representacion grafica;finite element method;finite element;modelisation;graphical representation;conception assistee;modeling;grafico computadora;infographie;graphics	Abstract   The finite element method has become one of the most-used calculation procedures in various fields of engineering in recent years. Due to progress in hardware architecture and computer graphics, the efficiency of its application could increase decisively. Notably, the solution of complex problems would be unthinkable today without the availability of powerful graphic processors.  In connection with the development of the finite element system COSAR at the Technical University Magdeburg, various graphic processors were created. In this paper, these processors are introduced. Information is given about interactive techniques for the generation of finite element models, about checking procedures for complex 3D structures by graphical means, and about possibilities for result presentation with colored pictures. Requirements for hardware architecture and software tools in the field of graphical input and output are discussed.	finite element method	H. Berger;J. Altenbach	1990	Computers & Graphics	10.1016/0097-8493(90)90060-B	simulation;computer science;computer aided design;finite element method;algorithm;computer graphics (images)	HPC	-37.43083613720726	-30.16971116612195	139167
34403b47d2f17f04dfc54b7fb531369c68baf51c	icrux: an artificially intelligent virtual screen technology	artificial;input device;draw;technology;real time;screen;video processing;character;platform;icrux;artificial intelligent;virtual screening;recognition;face recognition;operating system;click;package;face;intelligence;virtual;character recognition;realtime;open source	The paper aims at the development of an open-source, platform independent and artificially intelligent virtual screen technology -- 'iCrux'. It proposes state-of-the-art architecture of a novel virtual screen technology, capable of turning any computer powered screen into an artificially intelligent virtual screen. iCrux Technology enables a user to interact with the operating system and operate the computer using fingers, thus making the need for any hardware input device obsolete. The fingers are virtually linked to the mouse pointer on the screen and fully capable of performing any mouse operation. The paper describes the impeccable design of iCrux Technology designed to operate in unknown, random, non-plain, changing environment. A comprehensive artificial intelligence module built into the technology constantly monitors the changing environment and can respond and adapt intuitively, making the system highly robust and suitable for seamless deployment into any computer system. The proposed technology has been implemented and tested by our research team using real-time video processing and a single camera. In extension to this technology, the researchers have engineered several new artificially intelligent applications including character recognition in virtual screen technology, real-time face recognition system with dynamic training and enhanced multi-algorithm face recognition along with many more incredible features bundled into a platform independent package -- ' iCrux '.	algorithm;artificial intelligence;computer;facial recognition system;input device;open-source software;operating system;optical character recognition;pointer (computer programming);pointer (user interface);real-time clock;real-time locating system;seamless3d;software deployment;video processing;virtual desktop	S. Prasad;A. Sawant;R. Shettigar;R. Khokale;S. Sinha	2011		10.1145/1980022.1980162	simulation;engineering;multimedia;computer graphics (images)	Graphics	-43.231149958697095	-33.55789877849711	139190
e724bfcb3b8dbe27044aaf949bbc802aed4bf35a	a model representation for solid modelling in a virtual reality environment	real-time visualization;solid model;virtual reality system;precise object definition;model representation;virtual reality;hierarchically structured constraint-based data model;computational geometry;3d computer graphics;hierarchical geometry abstractions;high-levelconstraint-based model;complex objects;data visualisation;data models;data model;solid modellingin;solid modelling;brep hybrid solid model;low-level polygon model;csg;real-time visualizationand interaction;real-time systems;virtual reality environment;hierarchicallystructured constraint-based data model;geometry;data visualization;solid modeling;computer aided manufacturing;design automation;shape;real time systems	With today's virtual reality systems, it is difficult to directly and precisely create and modify complex objects in a virtual reality environment. One of the most important reasons is the absence of a suitable model representation that can efficiently support solid modelling in a virtual reality environment. A hierarchically structured constraint-based data model for solid modelling in the virtual reality environment is presented in this paper. The data model integrates a high-level constraint-based model for precise object definition, a mid-level CSG/B rep hybrid solid model for supporting hierarchical geometry abstractions and object creation, and a low-level polygon model for real-time visualization and interaction in the virtual reality environment. Constraints are embedded in the solid model and are organized at different levels to reflect the process of solid modelling. This data model not only provides precise object definition, but also supports real-time visualization and interaction in the virtual reality environment	solid modeling;virtual reality	Yongmin Zhong;Wolfgang Müller-Wittig;Weiyin Ma	2002		10.1109/SMA.2002.1003544	computer vision;simulation;computer science;computer graphics (images)	Visualization	-36.69372743201997	-31.883735645874886	139258
5e8227a625e5f9a10614e26d9faedfd81284fad1	a development of integrated learning system for visual impaired		Integrated Learning System (ILS) is a system that integrates several functions of the multimedia elements such as audio, text and slide show presentation to support teaching/learning process. This paper describes a development of ILS model for visually impaired students. In this context, one of the most unique functions of the system is the Text to Voice feature which is able to “read” texts from e-slides written in Bahasa Malaysia for learning a topic in History. Waterfall Model has been chosen as the methodology for developing the system and a prototype of the ILS was introduced. Delphi programming, one of the rapid application programming tools that support object-oriented design is used to develop the system. Microsoft Access is used to manage the Database of audio files containing all the voices in Bahasa Malaysia. The prototype will benefit the visually impaired to enjoy the benefits of computer technology in using ILS.	computer;microsoft access;programming tool;prototype;waterfall model	Wan Fatimah Wan Ahmad;Rustam Asnawi;Sufia Ruhayani Binti Zulkefli	2011		10.1007/978-3-642-22170-5_10	multimedia;waterfall model;computer science;delphi	HCI	-47.667446949797906	-26.747949336803735	139978
bc681fb21e66dcf0f124707ab5fa150ec507eae6	the fairlight cmi and its uses in advanced laser graphics			computer memories inc.;graphics	Barton McLean	1981			computer graphics (images);computer science;laser;graphics	Robotics	-47.270986962507294	-29.08946290422781	139992
eb804c0362b07ff9a4516536f85475a2b67bf4e7	surfacestreams: a content-agnostic streaming toolkit for interactive surfaces		We present SurfaceStreams, an open-source toolkit for recording and sharing visual content among multiple heterogeneous display-camera systems. SurfaceStreams clients support on-the-fly background removal and rectification on a range of different capture devices (Kinect & RealSense depth cameras, SUR40 sensor, plain webcam). After preprocessing, the raw data is compressed and sent to the SurfaceStreams server, which can dynamically receive streams from multiple clients, overlay them using the removed background as mask, and deliver the merged result back to the clients for display. We discuss an exemplary usage scenario (3-way shared interactive tabletop surface) and present results from a preliminary performance evaluation.	image rectification;kinect;open-source software;performance evaluation;preprocessor;server (computing);webcam	Florian Echtler	2018		10.1145/3266037.3266085	raw data;multimedia;human–computer interaction;preprocessor;overlay;computer science	HCI	-35.277989155273865	-34.74963133924418	140036
4e81c30b9c14cf6f87040b46600495a442d9c2a8	context in 3d planar navigation	keyboards;3d visualisation;useability issues;mice;3d environments;performance;distortion viewing 3d planar navigation 3d environments partial cylinder useability issues performance 3d graphics navigation techniques;virtual reality;navigation informatics graphics data visualization engine cylinders mice keyboards humans eyes motion planning;engine cylinders;data visualisation;navigation;eyes;3d environment;virtual reality data visualisation;navigation techniques;data visualization;motion planning;informatics;humans;partial cylinder;three dimensional graphics;article;3d graphics;graphics;3d planar navigation;distortion viewing	One of the most frustrating barriers to the widespread use of 30 visualisation is the additional complexity in navigating 30 data. This paper details a new approach to improving navigation in 3 0 environments where the navigation is mainly planar. Data at a distance fi-om the viewpoint is distorted as if projected onto a partial cylinder to approximate a plan view, thereby exposing information that may have been obscured. Previous approaches are compared with this new technique and screenshots presented. Implementation details of the technique are discussed as well as possible performance and useability issues.	approximation algorithm;cylinder seal;planar (computer graphics);screenshot;usability	Scott Vallance	2001		10.1109/AUIC.2001.906282	computer vision;navigation;simulation;human–computer interaction;performance;computer science;graphics;motion planning;informatics;data visualization;computer graphics (images)	DB	-38.80965193592492	-35.09006143479703	140083
6e2fa1e1a557821090440d40a0f5cf40b93e25dc	individualized route planning and guidance based on map content transformations		We have created a system of rule-based map content transformations that allows to create maps that are better fit for specific purposes and user groups than the base material. In this paper we demonstrate the application of the map content transformations in route planning and route guidance of a navigation system for specific user groups. We show that it is possible to create maps that are better suited to these tasks than the material on which they are based.	map	Bernhard Schmitz;Thomas Ertl	2014		10.1007/978-3-319-08599-9_19	navigation system;computer vision;artificial intelligence;computer science	ML	-40.22054850953362	-25.86815760410432	140228
1280fc697c361959cf24500130aa23ddfb4a1fbb	a case study of a virtual audience in a reconstruction of an ancient roman odeon in aphrodisias	isotropic meshing;real time;cultural heritage;virtual human;sizing field;slivers;delaunay mesh;visual quality	The benefits of including virtual humans into cultural heritage reconstructions are twofold: the realism of architectural models is increased by populating them; and, as well, it allows to preserve the intangible heritage describing how people in historical times behaved. We present a case study, where we create an interactive real-time scenario of a virtual audience in an ancient Roman odeon in Aphrodisias. Based on historical sources, we reconstruct both the building and the people. Inhabited virtual heritage applications require careful balancing of computational resources between the visualization of environment and the visualization of people. We describe several techniques that allow us to achieve high visual quality for a large number of virtual humans rendered together with a complex architectural model while still keeping interactive framerates. An important part of the heritage recontruction is the authoring, we propose a comprehensive framework for authoring of crowd scenarios.	computational resource;population;real-time clock;real-time computing;virtual heritage	Kevin Cain;Yiorgos Chrysanthou;F. Silberman	2004		10.1145/1198555.1198674	computer vision;simulation;computer science;cultural heritage;multimedia;computer graphics (images)	Visualization	-38.5668023561841	-33.83600053489823	140239
aef0f82a13cdd54466259c72d9f690f49f1047de	creating choreography with interactive evolutionary algorithms	parametric model;evolutionary design;interaction techniques;group behavior;animation;evolutionary algorithm;choreography;behavioral systems;interaction technique	Directing a group behavior towards interesting and complex motion can and should be intuitive, iterative, and often participatory. Toward this end, we present a choreographic system that enables designers to explore a motion space based on a parametric model of behaviors. Designers may work with the system by moving back and forth through two complementary stages: first, using an evolutionary algorithm to traverse the space of behavior possibilities, allowing designers to emphasize desired kinds of motion while leaving room for an element of the unexpected, and second, using selected behaviors to direct the group motion of simple performing creatures. In the second stage, evolved group motion behaviors from the first stage are used alongside existing high-level parametric rules for local articulated motion.	continuous design;dos;evolutionary algorithm;high- and low-level;interactivity;iterative method;parametric model;prototype;traverse	Jonathan Eisenmann;Benjamin Schroeder;Matthew R. Lewis;Rick Parent	2011		10.1007/978-3-642-20520-0_30	simulation;engineering;artificial intelligence;communication	HCI	-37.22869411662867	-33.184866655081045	140335
dabaedc17bdba4e553fa6f387ff2ae8f4252ea5f	knowledge-based dialog structuring for graphics interaction	description logic;knowledge base	structure of the interface and the appearance of graphical objects be based on conceptual information about domain objects and user actions because, at development time, concrete objects are not available. This paper presents a new approach to model dialog structuring knowledge for interactive infer-faces to realize dialog structuring on the basis of a Description Logic knowledge base.	abox;bottom-up parsing;business object;code generation (compiler);computational resource;description logic;dialog system;distributed object;domain-driven design;graphical user interface;graphics;knowledge base;run time (program lifecycle phase);top-down and bottom-up design;user interface design;visual instruction set	Ralf Möller	1996			computer science;casing;mathematical optimization;structuring;cylinder;computer vision;acoustics;graphics;artificial intelligence	AI	-39.27360969885507	-28.678649168701494	140652
557fce07a55aaf7976b2bd493fff4c5a445af877	application of low-cost commercial off-the-shelf (cots) products in the development of human-robot interactions		In the effort of developing sensible ways for interaction between humans and automated equipment the use of commercial off-the-shelf (COTS) products is shown to be fruitful in the learning process. The development in the field of consumer electronics has lead to increasingly more elaborate facilities for interaction with the human user. Modern cell phones and game technology are typical representatives of this trend. In this work such equipment has been explored in the aim to achieve easy and natural interfaces between humans and	documentation;experiment;human–robot interaction;mobile device;mobile phone;remote control;reverse engineering;virtual community;wii remote plus	Ottar L. Osen;Helge Kristiansen;Webjørn Rekdalsbakken	2010		10.7148/2010-0110-0116	embedded system;commercial off-the-shelf;human–robot interaction;computer science	HCI	-48.08192254313302	-37.654255861444305	140998
abfb955b581290abe6ff88814d984a3748d93363	projection-based augmented reality in disney theme parks	interactive programming augmented reality humanities;handheld computer;interaction techniques;humanities;interactive system;handheld computers cameras augmented reality entertainment calibration interactive systems;camera calibration;augmented reality;interactive space;interactive systems;entertainment;calibration;cameras;interactive programming;handheld computers;interaction technique;magical world projection based augmented reality disney theme park projector camera toolbox spatially augmented 3d object interactive space;interaction techniques augmented reality camera calibration	Walt Disney Imagineering and Disney Research Zürich are building a projector- camera toolbox to help create spatially augmented 3D objects and dynamic, interactive spaces that enhance the theme park experience by immersing guests in magical worlds. A related video can be seen here: http://youtu.be/wjrylXl0tTk. It shows examples of projection-based augmented reality techniques being employed in Disney theme parks.	augmented reality;theme park	Mark R. Mine;Jeroen van Baar;Anselm Grundhöfer;David Rose;Bei Yang	2012	Computer	10.1109/MC.2012.154	augmented reality;entertainment;calibration;camera resectioning;simulation;computer science;digital puppetry;multimedia;interaction technique;computer graphics (images)	HCI	-47.3801361302479	-33.0053856671979	141191
43ef86b57c52bc11bb18ad46c13813300c3e04ac	rdfa live browser extension: faceted presentation and tooltip navigation over linked data on the web		Several efforts in research and development of technologies have been spent to publish data in open standard formats. The main project in this regard is the Linking Open Data, which goal is to create an open and semantic Web of Data, enabling processing and understanding the data by software agents. However, not only the machines can take advantage of the explicit semantics of data. People can take advantage from the semantic of the data to explore unknown concepts, new relationships and to obtain personalized access to relevant resources and services. However, it is not trivial for a user without experience with Web of Data, to satisfactorily explore and use these data. This paper presents the RDFa Live Browser Extension, a prototype to support the non-technical users in presentation and navigation over Linked Data on the Web.	browser extension;faceted classification;linked data;rdfa;tooltip;world wide web	Andre Carlomagno Rocha;Cássio V. S. Prazeres	2015		10.1007/978-3-319-26762-3_25	computer science;database;internet privacy;world wide web	AI	-42.54458458616501	-24.313393118061168	141241
0a9b83cf947091ec69930a06a8a2eeb731b208f5	interactive reconfiguration techniques of reference frame hierarchy in the multi-viewport interface	information science;prototypes;virtual reality;layout;navigation;layout virtual environment navigation large scale systems information science usability virtual reality real time systems rendering computer graphics prototypes;virtual environment;rendering computer graphics;usability;large scale systems;real time systems	This paper proposes interaction techniques for interactive reconfiguration of the transformation hierarchy among coordinate systems in the multi-viewport interface, which we have proposed for versatile three-dimensional (3-D) operations. The multi-viewport interface provides an arbitrary number of secondary views in window frames placed in a virtual environment, each showing the same or different virtual scenes from different perspectives. Using the multi-viewport interface, the user can seamlessly perform a variety of object manipulation and user navigation operations between multiple virtual scenes. The relationships among the reference frames of a window frame, the primary view outside the frame, the secondary view inside the frame, and the user define the characteristics and usability of a virtual environment; however, our former framework supports limited types of relationships and does not support interactive reconfiguration of the hierarchy. This paper extends our former framework with deeper analysis of combinations of the transformation hierarchy and a set of new interaction techniques for interactive reconfiguration of the hierarchy.	glossary of computer graphics;interaction technique;interactivity;jumbo frame;real-time clock;real-time computing;reference frame (video);tree structure;usability;viewport;virtual reality;window function	Kouichi Hirose;Takefumi Ogawa;Kiyoshi Kiyokawa;Haruo Takemura	2006	3D User Interfaces (3DUI'06)	10.1109/VR.2006.89	human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-35.71380790041499	-34.31342991929815	141319
426505a106536f3d6c1b8844d0d3e3227c64bae0	object-based production: a personalised interactive cooking application		We present the Cook-Along Kitchen Experience (CAKE), a prototype that illustrates an interactive, personalised and responsive audio-visual experience created using Object-Based Media concepts and techniques. CAKE combines existing technologies in novel ways to create a distinctly new experience - a cookery programme that dynamically adapts in real-time as you cook with it. We demonstrate the novelty of the user experience - people can interact with the application and see a visualization of how CAKE responds to live input, using a data model and scheduling algorithm.	algorithm;data model;object-based language;prototype;real-time transcription;scheduling (computing);user experience	Jasmine Cox;Rhianne Jones;Chris Northwood;Jonathan Tutcher;Ben Robinson	2017		10.1145/3084289.3089912	simulation;novelty;visualization;interactive television;scheduling (computing);data model;user experience design;multimedia;computer science	HCI	-44.47636806455907	-33.26282898837045	141459
f96d1a086ee7ad86aa4a9bc92e13b4d5a76317a4	development of a versatile interactive performance system		This paper describes the development of an interactive performance system begun in 2001 and built using Max/MSP. Initially created for performance of a multimedia theater work, the system has been further developed since that time for use as an instrument for a variety of chamber music and improvisational situations.		Douglas Geers;Maja Cerar	2006			multimedia;computer science	Robotics	-47.28705257185125	-34.1496379911275	141678
ac6d3c4f9d86b383dd7a00ced1e370c511a42cf6	phonet: telephone call database 3d exploration applet	web based interaction;navigation;graphical user interfaces;web based services	2. INTRODUCTION PhoNct is a tool for the interactive exploration of a L&phone call database in three dimensions. It has been implcmcnted as a Java applet and uses VRML [7] to draw lhe 3D scene. Therefore, it can be easily embedded in an HTML document. The information displayed comes from a real telephone call database of the United States and contains 452 telephone numbers and 768 calls between lhem.	embedded system;html;java applet;telephone number;vrml	Christian Ghezzi	1997		10.1145/275519.275526	web service;web modeling;human–computer interaction;computer science;web navigation;web page;multimedia;world wide web	DB	-43.08353793503318	-27.697087467696765	141819
3f844b2da5c414c32ef4a95b8ad5ab36dfb0dd22	the graphics demands of virtual medicine	medical imagery;representation graphique;formation image tridimensionnelle;affichage;edicion;visualizacion;3d imaging;representacion grafica;extraction forme;edition;publishing;virtual reality;resonancia magnetica;deformation;display;extraccion forma;imagen virtual;magnetic resonance;image virtuelle;imagerie medicale;imageneria medical;formacion imagen tridimensional;imagen color;resonance magnetique;pattern extraction;virtual image;image couleur;deformacion;graphics;color image	Abstract   Any technology for virtual reality limits what can be presented to the user. Games are designed within such limits, but medicine has categorical requirements, more easily met in “reach-in” than in “immersive” VR. We discuss the effects of the needs of medicine on interface and graphics technology, and the achievement of precise, dextrous control for investigation and editing of 3-D medical data.		Tim Poston;Luis Serra;Meiyappan Solaiyappan;Pheng-Ann Heng	1996	Computers & Graphics	10.1016/0097-8493(95)00093-3	stereoscopy;computer vision;virtual image;color image;computer science;graphics;publishing;virtual reality;deformation;computer graphics (images)	Visualization	-47.20091028927054	-30.98621819692538	142076
a8621386ed98526cc9aed6b27f724c40f28ef5a7	code: an interactive musical performance software using keywords	human computer interaction;computer graphics and computer aided design;interactive music;virtual environments;moviemaps;camera tracking;computer vision and pattern recognition	∗ e-mail: fuctracker@hotmail.com † e-mail: info@samb.jp “Code” is a real-time, audio/visual performance application that is based on typing keywords. Other similar systems are already commercially available, but “Code” is unique because it is based on inputting keywords. It uses links between words that correspond to audio/visual components to easily convert a conceptual stream into a visual performance.	email;real-time cmix;real-time computing	Sadam Fujioka;Osamu Sambuichi;Shigenobu Nakamura	2006		10.1145/1179849.1179962	computer vision;computing;interactive systems engineering;computer science;computer graphics lighting;real-time computer graphics;multimedia;graphics software;computer graphics;computer network programming;3d computer graphics;computer graphics (images)	HCI	-44.07652175673451	-33.34592668808719	142077
b617a76f374f0fefd94526e0f60311f6bb30ebb4	new challenges in web accessibility		The Web is a foundational infrastructure for our society. Web access is now an imperative part of our education, work and everyday life. That is why we cannot stop ensuring and improving access to services on the Web for every member of our society. Web for All (W4A) is an annual international conference to share and discuss the latest technologies and practices for improving Web accessibility for persons with disabilities. This conference is decidedly cross-disciplinary in nature and brings together users, accessibility experts, graphic designers, and technologists from academia and industry to discuss how accessibility can be supported. W4A 2012 was held at Lyon, France receiving 26 papers and 71 participants. Every year the conference poses a main theme to capture the latest trends and challenges in the area, this year’s theme being ‘‘the Web of Data’’. Quote, ‘‘Laying beneath the surface of the Web there are a number of phenomena such as trends and patterns in information structure and in user behavior that do shape the way we communicate, consume and browse. As far as accessibility is concerned, Web content plays a central role in an ecosystem where user agents, authoring tools, crowdsourcing frameworks and testing tools determine how accessible is the Web’’. Among the presented papers, two papers were selected and thoroughly reviewed and revised for journal format, to be published in the UAIS journal. The first paper ‘‘Acceptance Tests for validating ARIA requirements in Widgets’’ approaches the data from a programmatic perspective by investigating data-oriented Web programming methodologies such as dynamically changing pages based on dynamic data transactions. Along with the evolution of high-performance script runtimes inside browsers and with the evolution of data exchange methods (e.g., JSON), dynamic Web applications are becoming the dominant Web user interface. The W3C WAI-ARIA is the metadata standard to make dynamic applications accessible by overlaying accessibility of the graphical user interface (GUI) as a layer for browsers to treat dynamic Web applications as GUI applications. However, testing can still be a challenge which ‘‘Acceptance Tests for validating ARIA requirements in Widgets’’ tries to address. The paper proposes a behavior-driven testing approach for checking accessibility issues of dynamic Web content, applies it to web pages containing dynamic content and compares the results with those produced by static accessibility checking tools and Tab Panel widget libraries, to check the conformance required by ARIA by comparing the results with manual evaluation. The second paper ‘‘How to Present more Readable Text for People with Dyslexia’’ investigates the effects of visual text presentation on the ease-of-reading for people with dyslexia. The paper introduces results from a set of experimental tests looking at various types of improvement methods for presenting text to people with dyslexia by using eye-tracking systems. Larger text and character spacing, it seems, improves reading speed for all. This is critically important knowledge when investigating large data sets or analyzing results on which critical decisions will be made. The importance of data is increasingly becoming more prevalent. Focus has grown since 2012 and by 2015 big & Hironobu Takagi takagih@jp.ibm.com	acceptance testing;browsing;conformance testing;crowdsourcing;dynamic data;dynamic web page;ecosystem;eye tracking;graphical user interface;human-readable medium;imperative programming;json;library (computing);requirement;runtime system;tracking system;user agent;wai-aria;web accessibility initiative;web application;web content;world wide web	Hironobu Takagi;Chieko Asakawa	2015	Universal Access in the Information Society	10.1007/s10209-015-0436-x	web accessibility initiative;web standards;web accessibility	Web+IR	-44.830327755763896	-24.837185133444454	142252
fe5f1f27a4dd563a0ad4ee607bcc040a31d3a54b	emulation of adobe cid resources by cjk truetype fonts. practical mappings from adobe cids to truetype glyphids			cjk characters	Suzuki Toshiya;Taiji Yamada;Masatake Yamato;Hideyuki Suzuki	2006	Document Numérique			Security	-44.93335482321611	-30.317338371966443	142431
3b7e7760957c1b37003d9da3cc1ad3639e41215e	using logic programming to detect activities in pervasive healthcare	historia clinica;informatique mobile;medical record;aplicacion medical;pervasive computing;hospital environment;logical programming;sistema complejo;systeme complexe;dossier medical;programmation logique;complex system;milieu hospitalier;medio hospitalario;medical application;logic programs;mobile computing;programacion logica;medical record systems;application medicale;expert system	In this experience paper we present a case study in using logic programming in a pervasive computing project in the healthcare domain. An expert system is used to detect healthcare activities in a pervasive hospital environment where positions of people and things are tracked. Based on detected activities an activity-driven computing infrastructure provides computational assistance to healthcare staff on mobileand pervasive computing equipment. Assistance range from simple activities like fast log-in into the electronic patient medical record system to complex activities like signing for medicine given to specific patients. We describe the role of logic programming in the infrastructure and discuss the benefits and problems of using logic programming in a pervasive context.	centralized computing;data quality;declarative programming;epr paradox;expert system;heuristic (computer science);imperative programming;jess;knowledge base;linear algebra;location awareness;logic programming;login;pervasive informatics;santa claus machine;scalability;server (computing);single point of failure;ubiquitous computing;user interface	Henrik Bærbak Christensen	2002		10.1007/3-540-45619-8_29	context-aware pervasive systems;simulation;computer science;artificial intelligence;mobile computing;computer security;expert system;medical record	HCI	-35.5258476390743	-24.969276221246982	142448
396a8310baa0756979f463718120d191b3fe98f3	integrated natural language generation systems	natural language generation	Many existing natural language generation systems can be characterized according to their modularization as either pipelined or interleaved. In these separated systems, the generator is divided into several modules (e.g., planning and realization), with control and information passing between themodules during the generation process. This paper proposes a third type of generator, which we call integrated, that unifies the modules into a single mechanism. The mechanism uses a small set of orthogonal basic operations to produce planned and grammatical language output. Integrated systems are conceptually attractive and may support generation of pragmatic effects more effectively than other systems. After discussing the advantages of the integrated approach, we summarize GLINDA, an integrated generator currently under development at Carnegie Mellon. GLINDA is the generator used for narration and intercharacter communication in the Oz Interactive Fiction and Virtual Reality Project.	automated planning and scheduling;drum memory;interactivity;lumped element model;modal logic;natural language generation;pipeline (computing);piper alpha;programming paradigm;software propagation;virtual reality	Mark Kantrowitz;Joseph Bates	1992		10.1007/3-540-55399-1_2	natural language processing;computer science;artificial intelligence;algorithm	AI	-38.61335418000593	-29.077381487675073	142473
3b14f85c9b7e3fc0d69f6431eb1e4a77078a6c35	an application based on spatial-relationship to basketball defensive strategies	distributed system;base donnee;systeme reparti;calculateur embarque;pervasive computing;videojuego;database;base dato;video game;jeu video;informatica difusa;sistema repartido;senal video;signal video;informatique diffuse;educational aid;video recording;boarded computer;video signal;enseignement;spatial relationships;calculador embarque;teaching;ensenanza	This paper aims to develop a simulated system used for teaching and training basketball defensive strategies. Respectively, defensive strategies can be described within one method by editing video recorded from basketball games into desired clips for analysis and storing them into the database. In this paper, we used Spatial-Temporal Relationships to describe the local defensive movements by the basketball players in a game. The system will automatically capture tracks of defensive movements by the basketball players in the video clips, from which basketball coaches and players can learn various defensive strategies within the shortest period of time. The simulated system is expected to become a computerized educational aid to basketball teaching and training and to replace the unscientific and stereotyped system of basketball teaching and training.		Su-Li Chin;Chun-Hong Huang;Chia-Tong Tang;Jason C. Hung	2005		10.1007/11596042_19	spatial relation;simulation;computer science;artificial intelligence;database;multimedia;computer security;ubiquitous computing;computer graphics (images)	Robotics	-35.67400435207496	-26.91518011572672	142612
145f83c9842acf27404df87b8f0f021d8b2dd7b3	software development for real-time online interactive applications on clouds			real-time transcription;software development	Dominik Meiländer;Alexander Ploss;Frank Glinka;Sergei Gorlatch	2011		10.3233/978-1-60750-831-1-81	human–computer interaction;computer science;software development	Embedded	-47.342002623367904	-31.953243527002645	142775
af1181e06b766ec0166db366a5580d71847a4f09	exact time and near-realtime visualization for the experimentalist	video analysis;scientific visualization;animation;realtime	This paper discusses the requirements for visualization as a tool for the experimentalist in steering experiments and describes work in progress in the implementation of applicable visualization technlques for a particular experimental system. Exact time visualization refers to the accurate representation of timing relationships in order to display temporal patterns. Near-realtime refers to visualizalions that are performed quickly enough to provide feedback. The results must be integrated into a global mosaic in order for the experimcntalist to track relationships during a survey of parameter space. CR Cntcgorles and Subject Descriptors: 1.3.3 [ComputerGraphits]: Picture/Image Generation Viewing Algorithms; 1.3.6 [Computer Graphics]: Methodology and Techniques Interation Techniques. Additional	adobe photoshop;computer graphics;experiment;experimental system;ncsa mosaic;requirement	Kay A. Robbins	1997		10.1145/275519.275534	simulation;information visualization;visualization;computer science;multimedia;computer graphics (images)	Visualization	-41.11939497371781	-28.5700229782909	143076
3e2572531b6f6f2b3dd773f50febe308f7f9ebb8	effectv: a real-time software video effect processor for entertainment	interfase usuario;estimation mouvement;detecteur image;image processing;user interface;juguete;real time;estimacion movimiento;divertissement;procesamiento imagen;motion estimation;traitement image;detection mouvement;senal video;signal video;jouet;temps reel;logiciel libre;toy;deteccion movimiento;video signal;tiempo real;image processing techniques;software libre;interface utilisateur;detector imagen;jeu ordinateur;computer games;motion detection;entertainment;image sensor;open source software;open source	EffecTV is a real-time software video effect processor based on motion detection and image processing techniques. EffecTV was released in 2001 as an open source software, and has been growing the number of features and effects through contributions from the open source community. EffecTV has been used for various purpose desktop toy applications, by visual jockeys (VJs), in theatrical plays and other stage performances. In this paper, we describe the implementation of EffecTV and some case studies.	desktop computer;image processing;open-source software;performance;real-time computing;real-time transcription	Kentaro Fukuchi;Sam Mertens;Ed Tannenbaum	2004		10.1007/978-3-540-28643-1_80	embedded system;computer vision;entertainment;simulation;image processing;computer science;motion estimation;image sensor;user interface;computer graphics (images)	SE	-46.823795604966186	-31.171970956465547	143098
95125f08329779f833396dc9c391068f64b0958b	slave: a score-lyrics-audio-video-explorer		We introduce the music exploration system S LAVE , which is based upon previous developments of our group. S LAVE manages multimedia music collections and allows for multimodal navigation, playback, and visualization in an efficient and user-friendly manner. 1 While previously the focus of our system development has been the simultaneous exploration of digitized sheet music and audio, with SLAVE we enhance the functionalities by video and lyrics to achieve a more comprehensive music interaction. In this paper, we concentrate on two aspects. Firstly, we integrate video documents into our framework. Secondly, we introduce a graphical user interface for semi-automatic feature extraction, indexing, and synchronization of heterog neous music collections. The output of this GUI is used by SLAVE to offer both high quality audio and video playback with time-synchronous display of digitized sheet music and content-based search.	display resolution;feature extraction;graphical user interface;master/slave (technology);multimodal interaction;semiconductor industry;sound card;usability	Verena Thomas;Christian Fremerey;David Damm;Michael Clausen	2009			multimedia;world wide web;computer graphics (images)	HCI	-44.819078416634895	-33.39803406865274	143133
dd98fecef48037d79f0177c15ad764f2d2f2e2a6	evitae: an event-based electronic chronicle	multiple perspectives;characteristic;base donnee;heart;multimedia;cardiology;interrogation base donnee;database;interrogacion base datos;base dato;hombre;caracteristica;information organization;coeur;sistema reactivo;organizacion informacion;corazon;cardiologie;indexation;human;reactive system;organisation information;systeme reactif;caracteristique;cardiologia;database query;event based system;homme	We present an event based system for storing, managing, and presenting personal multimedia history. At the heart of our approach is information organization using the concept of an event. Events allow modeling of the data in a manner that is independent of media. To store events, a novel database called EventBase is developed which is indexed by events. The unique characteristics of events make multidimensional querying and multiple perspective explorations of personal history information feasible. In this demo we present the major functions of eVitae.	knowledge organization	Bin Wu;Rahul Singh;Punit Gupta;Ramesh Jain	2004		10.1007/978-3-540-24741-8_51	reactive system;computer science;artificial intelligence;data mining;database;characteristic;heart	DB	-34.18442341708968	-27.923434947703733	143141
833240f30cd1bd3b402ea38aac585e5fef9b6122	a multi-perspective user interface for music signal analysis		In view of the exploding distribution of digitized audio material, computer-based methods have become indispensable for processing and analyzing the content of music signals. To evaluate analysis results obtained by automated methods, one requires manually generated highquality labeled data and the feedback by music experts. In this paper, we introduce various novel functionalities for a user interface that opens up new possibilities for viewing, comparing, interacting, and evaluating analysis results within a multi-perspective framework and bridges the gap between signal processing and music sciences. Here, we exploit the fact that a given piece of music may have multiple, closely-related sources of information including different audio recordings and score-like MIDI representations. Our interface then allows a user to interactively generate unifying views of the analysis results across the available music representations. Disclosing musically relevant consistencies and inconsistencies, these views not only afford new evaluation and navigation possibilities but also deepen a user’s understanding of the underlying musical material.	interaction;interactivity;midi;signal processing;user interface	Meinard Müller;Verena Konz;Nanzhu Jiang;Zhe Zuo	2011			labeled data;multimedia;signal processing;midi;exploit;computer science;user interface	Web+IR	-45.10446577285195	-32.505598793143626	143216
36fbc55bcab60f0213820c90694b41d2419fe580	computer animation used as a tool in teaching computer science.	computer animation			F. Robert A. Hopgood	1974			computer facial animation;non-photorealistic rendering;computer animation	Logic	-46.99321414610026	-30.361382261638738	143250
1829545acc46f2b1de1187a5eb3dc7fe0a3e3868	social navigation support through annotation-based group modeling	social navigation;modelizacion;anotacion;distributed system;metodo adaptativo;navigation maritime;adaptive hypermedia;systeme reparti;hipertexto;closed systems;navegacion informacion;systeme interaction;sistema cerrado;guidage;navigation information;information browsing;customization;personnalisation;annotation;classroom;methode adaptative;guiado;sistema interaccion;navigational aid;user assistance;modelisation;hypermedia;sea navigation;sistema repartido;assistance utilisateur;adaptive method;comportement utilisateur;asistencia usuario;personalizacion;aula clase;aide navigation;guidance;user behavior;ayuda navegacion;salle cours;modeling;interaction system;hypertexte;hipermedia;hypertext;comportamiento usuario;systeme ferme;navegacion maritima	Closed corpus AH systems demonstrate what is possible to achieve with adaptive hypermedia technologies; however they are impractical for dealing with the large volume of open corpus resources. Our Knowledge Sea project explores social navigation support, an approach for providing open corpus personalized guidance that is based on past learners’ interaction with the system. This paper presents the most recent stage of our project that focuses on using annotations for social navigation support. We present most recent version of Knowledge Sea that implements annotation-based social navigation support and reports the results of several classroom studies evaluating this technology.	adaptive hypermedia;information overload;java annotation;personalization	Rosta Farzan;Peter Brusilovsky	2005		10.1007/11527886_64	navigation;simulation;systems modeling;hypertext;computer science;closed system	NLP	-36.621255168411224	-25.532773820323264	143322
d9a4f4252ba2836737a1bcb9f2d9f240a4302bf7	ers: a system to facilitate emotion recognition in movies	video analysis;sentiment analysis;algorithms;design;experimentation;open source	We present eRS, an open-source system whose purpose is to facilitate the workflow of emotion recognition in movies, released under the MIT license. The system consists of a Django project and an AngularJS web application. It allows to easily create emotional video datasets, process the videos, extract the features and model the emotion. All data is exposed by a REST API, making it available not only to the eRS web application, but also to other applications. All visualizations are interactive and linked to the playing video, allowing researchers to easily analyze the results of their algorithms. The system currently runs on Linux and OS X. eRS can be extended, to integrate new features and algorithms needed in the different steps of emotion recognition in movies.	algorithm;angularjs;application programming interface;django;emotion recognition;linker (computing);linux;open-source software;operating system;representational state transfer;web application	Joël Dumoulin;Diana Affi;Elena Mugellini;Omar Abou Khaled	2015		10.1145/2733373.2807409	computer vision;design;simulation;computer science;multimedia;world wide web;sentiment analysis	HCI	-43.74764579277058	-31.472497188009584	143331
7f53efd1c1995a2920792168483d4fa32e633488	a user-centered method for the development of data-intensive dialogue systems: an object-oriented approach	user-centered method;object-oriented approach;data-intensive dialogue system	Reactive information systems for highly qualified users require their participation in the development process in order to ensure optimal support of their work. In this paper we present a user-centered development method for data-intensive dialogue systems. In this case the system itself must offer a high grade of flexibility in the use of dialogues.	data-intensive computing;database;dialog system;dialog tree;entity;information system;referential transparency;rendering (computer graphics);requirement;software system;user-centered design	Bettina Schewe;Klaus-Dieter Schewe	1995			computer science;knowledge management;artificial intelligence;communication	DB	-41.32391130080028	-27.717097709319113	143353
d96dda6e397e1e9a18c546520711a92a5f5d6e62	freqtric drums	tactual illusion;tactile display;shape presentation	Today, telecommunications penetrate our daily life, like the telephone, e-mail, chat on the Internet, etc. Certainly it is convenient and necessary to use these electronic ways of communication, which offer a certain proximity in distance. However, face-to-face, moreover, body-to-body communication is also necessary for human life to recover its inherent sensitivity. Especially, skin contact and sounds have an original power of communication. This kind of primordial communication is characterized by the corporal presence of each other and an interaction of bodies through skin contact, auditory contact like voice, singing, music, and body’s movement in presence like gestures or dance etc., which are absent from a virtual space communication. Freqtric Drums is a new musical corporalelectronic instrument, which enables us not only to recover the faceto-face communication, but also to enhance the possibility of our body-to-body communication, so that our self-consciousness based on our closed proper body can receive significant change, being open to another body and becoming a part of another body. This electronic device is also a sort of toy providing a place for the encounter and the play of bodies known or unknown to each other, and for the discovery of the materiality of our existence in relationships with each other.	drum memory;email;internet;materiality (digital text);self-consciousness;virtual reality	Tetsuaki Baba;Kiyoshi Tomimatsu	2006		10.1145/1179133.1179146	computer vision;multimedia	HCI	-48.11690365249438	-36.61721894034683	143463
7a837b60bc2a3ab83945683229148efbeb84f95b	a philosphy of teaching java (poster)		There are two possible approaches to presenting Java as a teaching language. The first is immediately attractive: start at point 3; go straight into GUI, multimedia, applets and the whole network environment, thereby catching the students’ interest and showing them how modem and different Java really is. The second approach is to start at the beginning, laying a sound foundation of basic concepts, and then moving on to the newer aspects.	applet;graphical user interface;java;modem	Judith Bishop	1997		10.1145/266057.266169	computer science;multimedia;java	PL	-48.161220930257905	-27.93449543398586	143887
769dbfd54ea8ac6738f0fb3ba89e71853d3aefb9	packaging videogames for long-term preservation: integrating frbr and the oais reference model	record format;video games;interactive fiction;format enregistrement;reference model;frbr functional requirements for bibliographic records;digital object preservation;functional requirements for bibliographic records owl;preservation;formato grabacion;preservacion;knowledge modeling;computer game	The Preserving Virtual Worlds project has been investigating the preservation of computer games and interactive fiction. The preservation of games benefits from simultaneous application of the data models from the Functional Requirements for Bibliographic Records report, and the Open Archival Information System reference model. The article described efforts to integrate these two data models within a single Web ontology language for application with multiple XML‐based packaging formats. © 2011 Wiley Periodicals, Inc.	functional requirements for bibliographic records;open archival information system;reference model	Jerome McDonough	2011	JASIST	10.1002/asi.21412	reference model;computer science;data mining;database;multimedia;world wide web;preservation;information retrieval	Vision	-42.49312515489608	-26.281370958730548	143901
6077a64d5059a22d4b7c2963690b5a4c40c0202f	book review: user interfaces for all: concepts, methods and tools	user interface		user interface	Bonnie A. Nardi	2002	User Modeling and User-Adapted Interaction	10.1023/A:1021203518213	user interface design;user;user experience design;user modeling;interface metaphor;shell;human–computer interaction;magic pushbutton;computer science;user requirements document;post-wimp;natural user interface;user interface;graphical user interface testing;multiple document interface	HCI	-42.710508128288076	-29.567550266879095	144174
85a5a3a404d337832f4381b4ab175016cf59954b	making music with images: interactive audiovisual performance systems for the deaf		This paper describes the technical and aesthetic approach utilised for the development of an interactive audiovisual performance system designed specifically for use by children with multiple learning difficulties, including deafness and autism. Sound is transformed in real-time through the implementation of a Fast Fourier Transform (FFT) and translated into a moving image. This image is adapted so that relevant information can be understood and manipulated visually in real-time. Finally, the image is turned back into sound with only minimal delay. The translation process is based on research in computer music, neuroscience, perception and abstract film studies, supported by the Arts and Humanities Research Council. The system has been developed through collaboration with the Sonic Arts Network, Whitefields Special Needs School, and the South Bank Centre, specifically for a project led by Duncan Chapman with the London Philharmonic Orchestra. The system has now been made available for free by the Sonic Arts Network.	fast fourier transform;machine perception;real-time locating system;real-time transcription	Mick Grierson	2008		10.1515/ijdhd.2011.009	speech recognition;multimedia;communication	Graphics	-47.033457789203	-33.771000358291154	144178
a823bb90fdaa40ac9c59c9f731c3b899cb37d761	recording and reproducing high order surround auditory scenes for mixed and augmented reality	audio signal processing;computer graphics;virtual reality;computer graphic;computer vision;computer graphics augmented reality computer vision audio signal processing;visual impairment;augmented reality;layout augmented reality microphone arrays loudspeakers virtual reality computer graphics computational modeling computer displays robustness laboratories;loudspeaker array high order surround auditory scene mixed reality augmented reality virtual reality system computer graphics vision technology microphone array	Virtual reality systems are largely based on computer graphics and vision technologies. However, sound also plays an important role in human's interaction with the surrounding environment, especially for the visually impaired people. In this paper, we develop the theory of recording and reproducing real-world surround auditory scenes in high orders using specially designed microphone and loudspeaker arrays. It is complementary to vision-based technologies in creating mixed and augmented realities. Design examples and simulations are presented.	augmented reality;computer graphics;loudspeaker;microphone;simulation;virtual reality	Zhiyun Li;Ramani Duraiswami;Larry S. Davis	2004	Third IEEE and ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2004.51	3d reconstruction;computer vision;augmented reality;computer-mediated reality;audio signal processing;computer science;virtual reality;multimedia;computer graphics;3d computer graphics;computer graphics (images)	Visualization	-45.1863370823826	-34.61633058189858	144227
a1ca620046d777aca61f876e36f52c039fc8b8c7	multi-modal navigation for interactive wheelchair	interfaz multimodal;navegacion;robot movil;multimodal interface;ultrason;habla;ultrasound;multimodal navigation;neuro fuzzy network;speech;silla de ruedas;multimodal integration;systeme conversationnel;information sharing;navigation;robot mobile;ultrasonido;interactive system;intelligent wheelchair;neuro fuzzy;wheel chair;sistema conversacional;parole;reseau neuronal;vision;red neuronal;moving robot;interface multimodale;neural network;fauteuil roulant	Typical examples for mobile GI services include tour planning and maps for navigation support (Reuter and Zipf 2004). We have developed several prototypes of mobile GI services for navigation tasks including user-adaptive tour planning as well as supporting multi-modal (i.e. graphical, natural language and gestures based) interfaces. As the development of GI services that adapt to user and context parameters as well as multi-modal interfaces for mobile navigation support are relative new research areas for GIScience it was necessary to specify interfaces for the software components that have been developed. These – relevant parts of the so-called Deep Map Objects (DMO), the message objects of the Deep Map multi agent system – will be introduced in this paper. Only recently the OGC Open Location Services Initiative (OpenLS) has published a specification for similar services in the area of LBS – but without a focus on adapatation or multi-modality. Both representations include relevant data types for route planning and map generation and are based on XML. Therefore we will compare these two representations and discuss their applicability for developing multi-modal map interaction for adaptive tour planning.	component-based software engineering;geographic information science;graphical user interface;location-based service;modal logic;modality (human–computer interaction);multi-agent system;natural language;warhammer 40,000: dark millennium;xml;zipf's law	Xueen Li;Tieniu Tan;Xiaojian Zhao	2000		10.1007/3-540-40063-X_77	vision;computer vision;navigation;simulation;computer science;speech;artificial intelligence;neuro-fuzzy;machine learning;ultrasound;linguistics;artificial neural network	HCI	-40.22985785675231	-26.73038510074795	144251
5be57ae846b8a1598f34b236f10e1e792422b808	soft printing with fabric		3-D printed objects made of fabric could be flexible and deformable, bringing possibilities to new sensors and actuators.	3d printing;as-interface;sensor	Huaishu Peng;Scott E. Hudson;Jennifer Mankoff;James McCann	2016	ACM Crossroads	10.1145/2893499	world wide web;computer engineering;computer science	HCI	-45.03104772508102	-37.864977203965495	144281
0a0acadab84e7b5bc0884ca0f5aab71bc6ef1583	immersive full-surround multi-user system design	multi user;vr systems;immersion;display technology;multimodal interaction	This paper describes our research in full-surround, multimodal, multi-user, immersive instrument design in a large VR instrument. The three-story instrument, designed for large-scale, multimodal representation of complex and potentially high-dimensional information, specifically focuses on multi-user participation by facilitating interdisciplinary teams of co-located researchers in exploring complex information through interactive visual and aural displays in a full-surround, immersive environment. We recently achieved several milestones in the instrument0s design that improves multi-user participation when exploring complex data representations and scientific simulations. These milestones include affordances for “ensemble-style” interaction allowing groups of participants to see, hear, and explore data as a team using our multi-user tracking and interaction systems; separate visual display modes for rectangular legacy content and for seamless surround-view stereoscopic projection using 4 highresolution, high-lumen projectors with hardware warping and blending integrated with 22 smallfootprint projectors placed above and below the instrument0s walkway; and a 3D spatial audio system enabling a variety of sound spatialization techniques. These facilities can be accessed and controlled by a multimodal framework for authoring applications integrating visual, audio, and interactive elements. We report on the achieved instrument design. Published by Elsevier Ltd.	alpha compositing;computer display standard;immersion (virtual reality);movie projector;multi-user;multimodal interaction;seamless3d;simulation;stereoscopy;surround sound;systems design	JoAnn Kuchera-Morin;Matthew Wright;Graham Wakefield;Charles Roberts;Dennis Adderton;Behzad Sajadi;Tobias Höllerer;Aditi Majumder	2014	Computers & Graphics	10.1016/j.cag.2013.12.004	computer vision;simulation;human–computer interaction;computer science;multimodal interaction;multimedia;immersion;computer graphics (images)	HCI	-47.506923647119855	-33.994709925062104	144415
368acbcd2a7be9f8b048a954e448648daf93ae64	teallach's presentation model	user interface;presentation models;model based systems;user interface development environments;model based user interface development;user interface development environment;qa76 computer software	This short paper describes the presentation model used by the Teallach model-based user-interface development environment. Teallach's presentation model provides both abstract and concrete interactors, which are first-class objects that may be freely intermixed when building a user-interface. An example is provided showing this approach in use.	user interface	Peter J. Barclay;Jessie B. Kennedy	2000		10.1145/345513.345295	simulation;human–computer interaction;computer science;operating system;multimedia;user interface	SE	-41.89972302478307	-29.699434800677984	144447
154e2ac22698bb180ca2b295ed65795271835584	talk roila to your robot	human robot interaction;speech recognition	"""In our research we present a speech recognition friendly artificial language that is specially designed and implemented for humans to talk to robots. We call this language Robot Interaction Language (ROILA). In this paper, we describe our current work with ROILA that utilizes the Nao humanoid robot. Our current demo implementation will allow users to interact with the Nao robot without the usage of any external laptops or microphones. Therefore the purpose of our demo is two-fold: 1) to demonstrate ``live"""" that ROILA has improved recognition accuracy over English and 2) to demonstrate that users can interact with the Nao robot in ROILA without the use of any external devices."""	humanoid robot;humans;laptop;microphone;nao (robot);robot interaction language;speech recognition;talk box	Omar Mubin;Joshua Henderson;Christoph Bartneck	2013		10.1145/2522848.2531752	human–robot interaction;simulation;speech recognition;computer science;artificial intelligence	Robotics	-45.95607702880698	-36.64786247776597	144514
0ea5cf711a3e62f7c4c62997ac9c4fbcf0f3e026	graphical techniques in a spreadsheet for specifying user interfaces	programming language;user interface;interface design;automatic generation;graphical programming;development tool;user interface development environment	underlying programming language. However, it is significantly easier to use, and provides many of the adMany modern user interface development environments vantages for graphics programming that financial spreaduse constraints to connect graphical objects. Constraints sheets provide for business. are relationships that are declared once and then maintained by the system. Often, systems provide graphical, iconic, or C32 is different from previous spreadsheet systems for user demonstrational techniques for specifying some coninterface construction because it uses a wide array of visual straints, but these are incapable of expressing all desired and inferencing techniques so the user does not have to relationships, and it is always necessary to allow the user write the entire constraint by hand. In particular: interface designer to write code to specify complex constraints. The spreadsheet interface described here, called • C32 automatically generates appropriate references to C32, provides the programmer with the full power of writgraphical objects when the user clicks on the object in a ing constraint code in the underlying programming lanuser interface window. guage, but it is significantly easier to use. Unlike other • It uses demonstrational techniques to guess which spreadsheets tools for graphics, C32 automatically properties of objects should be used, generates appropriate object references from mouse clicks • It guesses how to parameterize constraints when they are in graphics windows and uses inferencing and demonstracopied from one place to another or generalized into tional techniques to make constructing and copying conprocedures, so abstract and reusable constraints can be straints easier. In addition, C32 also supports monitoring constructed by example. and debugging interfaces by watching values in the spread• It incorporates graphical techniques to help trace and sheet while the user interface is running. debug constraints. • It is integrated with an existing prototype-instance sys	debugging;graphical user interface;graphics;microsoft windows;programmer;programming language;prototype;spreadsheet	Brad A. Myers	1991		10.1145/108844.108903	user interface design;look and feel;user;interface description language;10-foot user interface;user experience design;interface metaphor;shell;human–computer interaction;natural language user interface;magic pushbutton;functional reactive programming;computer science;interface design;event-driven programming;post-wimp;natural user interface;visual programming language;programming language;user interface;graphical user interface testing;multiple document interface	HCI	-41.43190420755516	-30.466113487672455	145196
1082c39f6f0b1544645b8da407068d6520fb0d54	archaeological models: pretty pictures or research tools?	computer aided instruction;virtual reality;archaeology;computer graphic;internet;computer aided instruction archaeology architectural cad virtual reality internet;computer aided instruction archaeological models research tools architectural reconstructions computer graphics undergraduates cad tools virtual reality internet athenian acropolis stonehenge;architectural cad;virtual reality application software image restoration slabs painting paints physics computing data visualization costs guidelines	Are architectural reconstructions in computer graphics a helpful research tool, or just pretty pictures to allow undergraduates a chance to get the look and feel of a forgotten place? The paper discusses the application of CAD tools, virtual reality and the Internet to architectural reconstructions such as the Athenian Acropolis and Stonehenge.		Dave Sims	1997	IEEE Computer Graphics and Applications	10.1109/38.576850	the internet;computer science;virtual reality;multimedia;computer graphics (images)	Visualization	-47.58851872013382	-30.07634863515802	145371
6f3b01c38fbc258d25d6c5591a281d1fd6338b06	duet musical companion: improvisational interfaces for children	context awareness;musical improvisation;context aware;toy interface agent;interface agent;sensor doll	We present a sensor-doll interface as a musical outlet for personal expression. A doll serves the dual role of being both an expressive agent and a playmate by allowing solo and accompanied performance. An internal computer and sensor system allow the doll to receive input from the user and its surroundings, and then respond accordingly with musical feedback. Sets of musical timbres and melodies may be changed by presenting the doll with a series of themed cloth hats, each suggesting a different style of play. The doll may perform by itself and play a number of melodies, or it may collaborate with the user when its limbs are squeezed or bent. Shared play is further encouraged by a basic set of aural tones mimicking conversation.	structure of observed learning outcome	David Ventura;Kenji Mase	2003			visual arts;simulation;artificial intelligence;multimedia	HCI	-48.200278382025665	-36.502743903603616	145378
b6a257eef317434c96c474dceee20f8a6e475a36	specification of the social force pedestrian model by evolutionary adjustment to video tracking data	urban environment;pedestrian interaction;interactive video;pedestrian simulation;video tracking;data analysis;large scale;evacuation scenarios;social force model;video recording;evolutionary optimization	Based on suitable video recordings of interactive pedestrian motion and improved tracking software, we apply an evolutionary optimization algorithm to determine optimal parameter specifications for the social force model. The calibrated model is then used for large-scale pedestrian simulations of evacuation scenarios, pilgrimage, and urban environments.	social force model;video tracking	Anders Johansson;Dirk Helbing;Pradyumn K. Shukla	2007	Advances in Complex Systems	10.1142/S0219525907001355	computer vision;simulation;computer science;video tracking;data analysis;statistics;computer graphics (images)	AI	-38.16017805275332	-36.72391273440167	145492
64405b589cb2779da59eb6ad96d0be0fe17f9c65	how to evaluate and shop for computer graphics hardware	microcomputer;computer graphic;operating systems	Anyone shopping for graphics peripherals to attach to mainframes, mini, and workstation computer systems. Course attendees will typically be either developing their own applications or running existing applications targeted to one or more classes of display devices. The course assumes that an attendee has some familiarity with the different graphics display device technologies (e.g., raster terminals, plotters, display list devices, film recorders, etc.).	computer graphics;display device;display list;film recorder;graphics hardware;mainframe computer;peripheral;plotter;workstation	James R. Warner	1985		10.1145/320435.320495	hardware compatibility list;computer graphics lighting;graphics software;3d computer graphics	Graphics	-47.566347228324844	-29.270389416190348	145696
ce7d8139fb252e4418db0ee1d867cc591d70f859	a system for real-time multimodal analysis of nonverbal affective social interaction in user-centric media	interfaz multimodal;systeme temps reel;multimodal analysis of nonverbal affective social interaction;semiologia;interfase usuario;reseau social;real time applications of multimodal affective social interaction;nonverbal communication;multimodal interface;social interaction;interaction sociale;metadata;real time multimodal analysis;user interface;etude experimentale;experimental test bed;sociologia;real time;cultural heritage;musica;divertissement;computing quantitative measures;semiologie;affective sound and music processing;sistema complejo;violon;synchronisation;media;social sciences computing interactive systems real time systems;user centric media affective social behavior of small groups of users affective sound and music processing multimodal analysis of nonverbal affective social interaction real time applications of multimodal affective social interaction;social network;indexes;therapy;musique;real world application;musical instrument;instrumento musical;intercambio electronico de datos;interaccion social;systeme complexe;emotion emotionality;complex system;social sciences computing;lead;social behavior;violin;synchronization;instrument corde;feature extraction;eyesweb xmi platform;user centric media;instrumento cuerda;temps reel;comportement utilisateur;active music listening real time multimodal analysis user centric media nonverbal affective social interaction computing quantitative measures interactive social activity affective nonverbal communication eyesweb xmi platform;instrument musique;metadonnee;semiology;terapia;tiempo real;multimodal system;emotion emotivite;affective social behavior of small groups of users;interface utilisateur;audition;real time system;juegos de computadora;sistema tiempo real;echange donnee informatise;audicion;sincronizacion;emocion emotividad;metadatos;therapie;jeu ordinateur;user behavior;sociologie;educacion;active music listening;computer games;nonverbal affective social interaction;communication non verbale	This paper presents a multimodal system for real-time analysis of nonverbal affective social interaction in small groups of users. The focus is on two major aspects of affective social interaction: the synchronization of the affective behavior within a small group and the emergence of functional roles, such as leadership. A small group of users is modeled as a complex system consisting of single interacting components that can auto-organize and show global properties. Techniques are developed for computing quantitative measures of both synchronization and leadership. Music is selected as experimental test-bed since it is a clear example of interactive and social activity, where affective nonverbal communication plays a fundamental role. The system has been implemented as software modules for the EyesWeb XMI platform (http://www.eyesweb.org). It has been used in experimental frameworks (a violin duo and a string quartet) and in real-world applications (in user-centric applications for active music listening). Further application scenarios include entertainment, edutainment, therapy and rehabilitation, cultural heritage, and museum applications. Research has been carried out in the framework of the EU-ICT FP7 Project SAME (http://www.sameproject.eu).	affective computing;algorithm;brainwave entrainment;complex system;educational entertainment;emergence;emergentism;high- and low-level;interactive media;modality (human–computer interaction);multimodal interaction;norton's theorem;principle of abstraction;real-time clock;real-time transcription;social network;synchronization (computer science);testbed;xml metadata interchange	Giovanna Varni;Gualtiero Volpe;Antonio Camurri	2010	IEEE Transactions on Multimedia	10.1109/TMM.2010.2052592	nonverbal communication;social relation;synchronization;complex systems;real-time operating system;computer science;multimedia	Visualization	-35.83638491773858	-26.17162119564101	145765
b2d15340dc6750d68749bcfb4a02646712448933	cogmap: a visual description language for spreadsheets		Abstract Understanding a program is easier when the visual representation of the program provides a u0027cognitive mapu0027 conveying information about the programu0027s structure and function. Although spreadsheets have been justly praised for improving on the presentation of conventional languages, programmers have much u0027metacodisticu0027 knowledge about the program that cannot be expressed using the resources of a conventional spreadsheet. Rather than build a complex knowledge-based interface, we have sought a solution that is computationally simple but cognitively effective, and we describe u0027CogMapu0027, a tool for the visual expression of simple assertions about structure, function or any other propositions. As a preliminary test of efficacy, we interviewed 10 experienced spreadsheet users and obtained detailed descriptions of their spreadsheets, which were then transcribed into CogMap representations. We argue that the simple principles of CogMap allow it to be extended to many other visual programming environments.	spreadsheet	David G. Hendry;Thomas R. G. Green	1993	J. Vis. Lang. Comput.	10.1006/jvlc.1993.1003	natural language processing	Robotics	-37.61355582912544	-28.90809337009263	146009
160359dad02dc1f395871b2cc7743fa31332a08d	adaptive hypermedia: from systems to framework	adaptive hypermedia;navigation adaptative presentation;hypermedia;adaptation;adaptive hypermedia system;adaptive navigation support;user model	The navigational freedom in conventional hypermedia applications leads to comprehension and orientation problems [Nielsen 1990]. Adaptive hypermedia attempts to overcome these problems by adapting the presentation of information and the overall link structure, based on a user model. This paper introduces a framework for adaptive hypermedia systems (AHS). It briefly describes some popular methods and techniques for adaptation. Examples and evaluations of existing AHS are used to illustrate the potential benefits of using adaptation in hypermedia applications.	adaptive hypermedia;platoon (automobile)	Paul De Bra;Peter Brusilovsky;Geert-Jan Houben	1999	ACM Comput. Surv.	10.1145/345966.345996	simulation;user modeling;human–computer interaction;computer science;multimedia;adaptation	Web+IR	-39.150226820641194	-26.25018373926922	146401
2a17b3c0f15ae4b545fd053fb3d893586f3285e5	sound synthesis from real-time video images	sound synthesis;real time;light intensity;tactile interface;digital video	Digital video offers an interesting source of control information for musical applications. A novel synthesis technique is introduced where digital video controls sound spectra in real time. Light intensity modulates the amplitudes of 32 harmonics in each of several synthesized “voices.” Problems addressed include how to map from video to sound, dealing with global variations in light level, dealing with low frame rates of video relative to high sample rates of audio, and overall system implementation. In one application, images of light reflected from a shallow pool of water are used to control sound, offering a rich tactile interface to sound synthesis.	digital video;modulation;real-time transcription	Roger B. Dannenberg;Thomas P. Neuendorffer	2003			computer vision;video;acoustics;computer graphics (images)	Graphics	-46.151036458460716	-33.74743298791415	146881
885500225381e90aac495c8394d35751792f4a00	template based authoring for ar based service scenarios	service personnel;maintenance;computer graphics;service manuals template based authoring augmented reality product maintenance manufactured products cars planes large machinery service personnel repair tasks context sensitive virtual information;virtual reality;maintenance engineering;virtual reality augmented reality documentation personnel manufactured products machinery context aware services chromium multimedia systems computer graphics;multimedia systems;repair tasks;product maintenance;engineering information systems;personnel;chromium;field of view;large machinery;authoring;engineering information systems augmented reality maintenance engineering;manufactured products;augmented reality;machinery;service manuals;context sensitive virtual information;template based authoring;cars;documentation;context aware services;planes	As of today one of the major application domains for augmented reality is the service and maintenance of manufactured products, e.g. cars, planes, large machinery. In this area, augmented reality is used to support service personnel in carrying out their repair tasks by displaying context-sensitive, additional, virtual information in their field of view. One major problem is the creation of such information. In this paper the authors describe a concept, which heavily simplifies the creation of AR based manuals and even allows people without special AR and IT skills to carry out this task.	ar (unix);augmented reality;context-sensitive grammar	Christian Knöpfle;Jens Weidenhausen;Laurent Chauvigné;Ingo Stock	2005	IEEE Proceedings. VR 2005. Virtual Reality, 2005.	10.1109/VR.2005.75	maintenance engineering;augmented reality;chromium;simulation;field of view;human–computer interaction;documentation;computer science;operating system;virtual reality;multimedia;computer graphics	Visualization	-42.5201390413623	-35.30474480517047	146964
5d28a4ba92dc2fcaa495895bf0c2753b941cbccc	real-time implementation of a general model for spatial processing of sounds		In 1982, one of the authors proposed a general model for spatial processing of sounds and included a partial implementation of it in the cmusic sound synthesis program[7]. This model is designed to simulate the most perceptible physical characteristics of a real or imaginary space relating to the localization of sound sources. The original model defines the acoustic space in two dimensions as an outer closed space (the illusory acoustic space) and an inner listening room (the intended performance space) with “openings” along its perimeter in the location of the speakers. The spatial impression is produced by simulating the direct radiation, early echoes, and global reverberation of a sound source as heard through each opening. This paper discusses our modifications of the original cmusic implementation: first, the algorithm runs in real-time, and second, we made additions that more satisfactorily realize the general model. Our implementations of the algorithm in Pd and Max/MSP are presented.	acoustic cryptanalysis;algorithm;cmusic;central processing unit;covox speech thing;imaginary time;internationalization and localization;interpolation;linux;mac os 9;max;operating system;overhead (computing);perimeter;real-time clock;real-time computing;real-time transcription;simulation;unit generator	Shahrokh Yadegari;F. Richard Moore;Harry D. Castle;Anthony Burr;Ted Apel	2002			speech recognition;acoustics;communication	Graphics	-45.80048654237069	-34.41669969918344	147015
6ee1f3ccc0cf01d92fb80fd356a85cb8ac1f8ebd	a question-answering system for the french yellow pages	lenguaje natural;interfase usuario;representacion conocimientos;concepcion sistema;user interface;cuestion respuesta;langage naturel;intelligence artificielle;systeme conversationnel;question answering system;interactive system;system design;natural language;sistema conversacional;analizador sintaxico;artificial intelligence;question reponse;interface utilisateur;parser;inteligencia artificial;knowledge representation;representation connaissances;analyseur syntaxique;conception systeme;question answering	This paper describes a dialogue-based system which is intended as an intelligent natural language interface to the French Yellow Pages. We do not assume that the user knows how the Yellow Pages are organized, and we paraphrase his request, if necessary, so as to better search for the desired information. We do, however, assume that the reason the user is on line is to find an address and phone number for some supplier.#R##N##R##N##R##N##R##N#There are three basic modules used in our system: parser, dialogue manager, and generator. The first two exist (and are constantly being extended); the generation module is still only a set of functional specifications which will be outlined later in this article.#R##N##R##N##R##N##R##N#Cet article decrit un systeme a base de dialogues concu comme une interface intelligente en langue naturelle des Pages jaunes francaises. Nous ne presumons pas que l'utilisateur sait comment les Pages jaunes sont structurees et nous paraphrasons sa demande, si necessaire, afin de faciliter la recherche du renseignement desire. Cependant, nous presumons que la raison pour laquelle l'utilisateur est en ligne est qu'il recherche l'adresse et le numero de telephone d'un fournisseur.#R##N##R##N##R##N##R##N#Trois modules de base sont utilises dans notre systeme: analyseur, gestionnaire de dialogues et generateur. Les deux premiers existent (et font constamment l'objet d'une extension); le module de generation n'est qu'un ensemble de specifications fonctionnelles qui seront decrites plus loin dans cet article.		P. Herman;Gérard Sabah;Anne Vilnat	1988	Computational Intelligence	10.1111/j.1467-8640.1988.tb00122.x	question answering;computer science;artificial intelligence;natural language;user interface;algorithm;systems design	NLP	-37.220363880068554	-26.307698671856905	147363
0b43bf09e5e70a6bb52374f0c3fe9281366d04de	easysp: nueva aplicación para la enseñanza de procesado de señal		— The present article presents a software application, developed by the authors, in the digital signal processing field, with educative purposes. This tool offers a graphic interface to perform signal processing presentations via plugins. The program allows users to add plugins designed in both XML format and Java. Both kind of plugins allow the incorporation of Octave/Matlab functions in order to implement signal processing algorithms. In one hand, this application allows the student to interact with presentations changing their parameters and, in the other hand, offers the possibility of implementing new signal processing plugins easily	algorithm;digital signal processing;gnu octave;graphical user interface;java;linear algebra;matlab;naruto shippuden: clash of ninja revolution 3;plug-in (computing);power-on reset;unique name assumption;urban dictionary;xml	Javier Vicente Sáez;Begoña García Zapirain;Ibon Ruiz;Amaia Méndez Zorrilla;Oscar Lage	2007	IEEE-RITA		simulation;human–computer interaction;telecommunications;computer science;engineering;electrical engineering;artificial intelligence;operating system;world wide web;computer graphics (images);mechanical engineering	Graphics	-44.01460158602573	-28.665079741563996	147388
b3a38e7afe6d103af045cc031af5b93fd0852912	"""previsualization for """"starship troopers"""" managing complexity in motion control"""	motion control	"""film productions find it difficult to effectively manage production of extremely complex visual effects. In producing these scenes, directors and cinematographers often lack the creative control they would prefer to have, especially when the scenes involve numerous models and extensive motion-control photography. Complex visual effects have also proven to be hard to plan, both in terms of their cost and in the time it takes to execute them. 3D animation can aid in this process. Although exchanging data between computers and motion control systems is a fairly well-established practice, the process only works well with relatively uncomplicated shoots involving, for example, a single setup and a single element. It tends to break down in more complex situations involving numerous rigs, camera systems, and photographic elements. For the recent film """"Starship Troopers,"""" Pixel Liberation Front, working with Sony Pictures Imageworks, developed a new process that enabled it to create and manage motion-control files used to shoot hundreds of miniature spacecraft elements. Extremely complicated space battle sequences were designed in 3D and analyzed to determine the requirements for shooting each spacecraft. This allowed SPI's motion-control unit to shoot the individual spacecraft with maximum efficiency. Once filming of the models was complete, PLF integrated the component elements of each shot into 3D templates that SPI used to complete the final composites. PLF's team began by previsualizing each space battle sequence. The previz was then edited into animatics so that they could be viewed and approved by the filmmakers in a cinematic context. Once the animatics were approved, PLF created a 3D """"virtual stage"""" for use in testing various stage configurations and in planning camera movement, lighting, and the position of each model in precise detail. PLF supplied SPI's motion control unit with custom move files for each spacecraft, detailed diagrams of the staging, and reference videos for each shot. This reduced the time required to configure setups and program rigs, and virtually eliminated the need to design moves or try different configurations on stage. With decisions about shot composition and motion out of the way, the film crews were able to focus on maximizing photographic quality. After the elements were shot, PLF prepared """"slap comps"""" of telecined motion-control elements, creating the first accurate views of the fully assembled shots weeks before they were available at film resolution. In addition, PLF imported the camera data back from the motion-control units and used it to …"""	computer animation;control system;control unit;diagram;disk staging;image resolution;killzone: liberation;pixel;previsualization;requirement;visual effects	Colin Green	1998		10.1145/280953.282418	motion control;computer science	Graphics	-38.87251436387543	-34.05219109817381	147644
fbef6234bde1845c0198d94297adb62e62050e83	rtmix: a real-time interactive electroacoustic music performance, composition and coaching interface		With the technological advancements in computer technology, the making of the real-time computer-aided interactive music has become a reality. However, due to lack of comprehensive software, such form of artistic expression still proves to be a daunting task. RTMix is an open-source software application that has been designed for the highly stable and scalable Linux platform and whose function is to provide a transparent real-time and userfriendly performance interface, as well as the universally portable and easily editable/reproducible “scorefile” format for the live and interactive multimedia works. RTMix’s implementation addresses the problem of lack of comprehensive and accurate interactive electroacoustic music computer interface by providing an all-in-one solution: it can be used for composition, coaching, and most importantly performance of complex interactive works that would otherwise be very hard to execute with currently available software tools. Its abstractness and extreme flexibility enables user to simultaneously control a variety of independent applications, such as RTCmix, Csound, aplay, sox, mpg123, as well as any other process that can be triggered via UNIX shell. Key-Words: Computer, Music, Interactive, RTMix, Linux, Electroacoustic, Open-source, Art, Computer-aided.	aplay;computer;csound;dynamic music;linux;multi-function printer;open-source software;real-time cmix;real-time computing;real-time transcription;scalability;unix shell;mpg123	Ivica Ico Bukvic	2002			human–computer interaction;multimedia;communication	HCI	-43.51002233848506	-33.243055344288486	147723
b0c593da7ac4079fd39731bda4917be295d9e670	modeling virtual worlds in databases	databases;modelizacion;base donnee;procesamiento informacion;realite virtuelle;realidad virtual;database;virtual reality;base dato;modelisation;data structures;information processing;traitement information;modeling;data structure;virtual worlds	A method of modeling virtual worlds in databases is presented. The virtual world model is conceptually divided into several distinct elements, which are separately represented in a database. The model permits to dynamically generate virtual scenes.	database;virtual world	Krzysztof Walczak;Wojciech Cellary	2003	Inf. Process. Lett.	10.1016/S0020-0190(03)00381-8	simulation;systems modeling;data structure;computer science;database;programming language;world wide web	DB	-34.735696351265894	-27.42256924321091	147739
d56eb59e6097a5c31c0942bbe70f8f456eed03a6	efficiently planning coherent visual discourse	design process;building block;user interface;top down;visual representation;smooth transition;computational efficiency;knowledge base;partial order	Abstract   A visual discourse is a series of connected visual displays. A coherent visual discourse is characterized by smooth transitions between displays, consistent design within and across displays, and successful integration of new information into existing displays. We use a topdown, hierarchical-decomposition, partial order planner to efficiently construct a visual discourse from scratch, taking advantage of parametrized primitive visual objects that serve as building blocks in the design process. Visual representations are modeled as visual objects, graphical techniques are employed as planning operators, and design policies are encoded as constraints. This approach not only improves computational efficiency compared to search-based approaches, but also facilitates knowledge encoding, and ensures global coherency.		Michelle X. Zhou;Steven K. Feiner	1998	Knowl.-Based Syst.	10.1016/S0950-7051(97)00036-1	partially ordered set;computer vision;knowledge base;design process;computer science;artificial intelligence;theoretical computer science;top-down and bottom-up design;human visual system model;user interface	Robotics	-38.01444760112963	-29.312882843365188	147874
906b5bf722ec236971d8b2dff562ef4fa849e731	integrated virtual human interface system with portable virtual reality capability	integrated virtual human interface;portable virtual reality;virtual human;face detection;liquid crystal displays;virtual reality;information processing;face recognition	We demonstrate a photo-realistic, interactive virtual human agent application, called the Virtual Human Interface that employs virtual people to provide digital media users with information, learning services and entertainment in a highly personalized, visually rich virtual reality environment. The virtual digital human is capable of seeing, detecting and recognizing one or multiple people in front of the display and internally model, adapt to, and modulate the user’s mood and emotional state via advanced facial information processing techniques. Additional real-time modules include a portable head mounted VR system to enhance the experience and live imagery captured from a video source to support augmented reality applications.	augmented reality;digital media;human–computer interaction;information processing;personalization;real-time clock;sensor;virtual actor;virtual reality	Bernadette Kiss;Barnabás Takács;Gábor Szijártó	2003		10.1109/VR.2003.1191189	computer vision;augmented reality;computer-mediated reality;face detection;human–computer interaction;information processing;computer science;liquid-crystal display;virtual reality;multimedia	Visualization	-44.868471240560694	-36.88485906228041	147932
0b0f3b3c6edad57d09bfd05267ab5d5fab9475b4	activemath: an intelligent tutoring system for mathematics	metodo adaptativo;representacion conocimientos;systeme tutoriel intelligent;red www;intelligent tutoring system;reseau web;methode adaptative;adaptive behavior;internet;adaptive method;intelligent tutoring systems;world wide web;enseignement;knowledge representation;representation connaissances;teaching;ensenanza	ActiveMath is a web-based intelligent tutoring system for mathematics. This article presents the technical and pedagogical goals of ActiveMath, its principles of design and architecture, its knowledge representation, and its adaptive behavior. In particular, we concentrate on those features that rely on AI-techniques.	adaptive behavior;interaction technique;knowledge representation and reasoning;web application	Erica Melis;Jörg H. Siekmann	2004		10.1007/978-3-540-24844-6_12	knowledge representation and reasoning;the internet;simulation;computer science;artificial intelligence;adaptive behavior;multimedia	AI	-37.297008852564396	-25.821907014849305	148267
84b3c3e586e65cd6dda31f662641d2fdac71d701	a task definition language for virtual agents	intelligent virtual environment;user interface;real time;virtual reality;intelligent agent;dynamic characteristic;virtual environment;high level language;virtual agent	The use of Virtual Environments as a user interface can be important for certain types of applications, especially in the fields of education and entertainment. These synthetic worlds are even more attractive for the user when they exhibit dynamic characteristics and are populated by virtual agents. There is, however, a lack of generalpurpose tools for designing and implementing intelligent virtual environments, and especially in the case of defining virtual agents’ tasks, where there is a strong dependence between the task execution and the context. In this paper, we present our approach towards a context -independent definition of tasks using a high-level language. With the proposed task definition language, one can combine numerous built-in functions and commands to describe complex tasks as a combination of parallel, sequential and conditional execution of actions. It can be used to program complicated virtual agent interactions with the environment without going into much detail on how these tasks are implemented and how parallelism is achieved. The main advantage of the proposed language is that it enables tasks to be easily constructed and reused by different agents and in different environments. Our approach has been based on SimHuman, a platform for rendering and animating Virtual Agents in real-time.	educational entertainment;high- and low-level;high-level programming language;intelligent agent;interaction;parallel computing;population;real-time transcription;synthetic intelligence;user interface;virtual reality	Spyros Vosinakis;Themis Panayiotopoulos	2003			real-time computing;simulation;computer science;virtual machine;artificial intelligence;instructional simulation;kernel virtual address space;operating system;virtual reality;multimedia;user interface;high-level programming language;intelligent agent;virtual finite-state machine	Visualization	-35.854517964060186	-28.9063541015056	148865
6ea24be84aa55fe0dbc77ba44c533152905aa1f2	dynamic modelling and visualization on the internet	common gateway interface;client server;hydrologic model;java applet;dynamic modelling;internet gis;visual processing;analytical model;environmental modeling	There is a growing requirement for GIS to incorporate dynamic analytic models. At the same time, there is a need to distribute results of dynamic GIS using the Internet. Therefore, this paper sets out to explore the implementation of dynamic environmental models using Internet-based geocomputation techniques. An overview discusses shortcomings of current Internet GIS techniques for dynamic modelling based on the idea that bidirectional and sustained communication is required between the client and the server sides. Thus an applet-servlet approach is explored to demonstrate the modelling process of a chosen hydrological model, TOPMODEL, which requires frequent and efficient client-server interactions. This approach overcomes the inherent shortcomings of the current Common Gateway Interface (CGI) and more primitive Java applet techniques. We present an effective and generic way to implement dynamic modelling and visualization processes in an Internet environment. This allows users to benefit from Internet-based geocomputation techniques to gain insights into computation and representation of dynamic spatial phenomena.	internet	Bo Huang;Michael F. Worboys	2001	Trans. GIS	10.1111/1467-9671.00072	simulation;computer science;database;common gateway interface;world wide web;client–server model;java applet	HCI	-42.692792982657565	-27.308089785653586	148939
bac00b4d33994a224562d6e704d6d9e4c5f8ed84	iamhear: a tabletop interface with smart mobile devices using acoustic location	smart mobile device;interface design;tabletop interface;acoustic location;tapir	"""IAMHear is a novel tabletop interface for music performance and sound making, in which smart mobile devices are used as on-table objects for interaction. Thanks to the advanced features of smart mobile devices, IAMHear is by nature multi-modal and highly interactive.  The system also allows for acoustic location mechanism using virtually inaudible sound without any special sensors, making itself simpler in structure and easier to implement. In addition, use of """"everyday objects"""" also evokes interaction by intuitive gestures such as placement, movement, and rotation.  As a music sequencer, IAMHear enables the user to make music by placing objects on table; inspired by the idea of spectrographic mapping with virtual scan line, pitch and timbre of sounds are determined by the location/orientation of tabletop objects as well as ambient noise.  We present IAMHear as a simple and novel alternative to interactive tabletop interface for music and various multimedia applications as well."""	acoustic cryptanalysis;microsequencer;mobile device;modal logic;scan line;sensor;smart device	Seunghun Kim;Bongjun Kim;Woon Seung Yeo	2013		10.1145/2468356.2468628	simulation;human–computer interaction;interface design;acoustic location;multimedia	HCI	-46.155805293525596	-37.04106147325869	149097
1b1552dd28f1b33285a022caeb8883644e623b57	learning controls for blend shape based realistic facial animation	human expression;key idiosyncracies;motion-capture animation;keyframe facial animation;facial expression;blend shape model;blend shape;physically-motivated segmentation;key facial expression;realistic facial animation;blend shape animation	Blend shape animation is the method of choice for keyframe facial animation: a set of blend shapes (key facial expressions) are used to define a linear space of facial expressions. However, in order to capture a significant range of complexity of human expressions, blend shapes need to be segmented into smaller regions where key idiosyncracies of the face being animated are present. Performing this segmentation by hand requires skill and a lot of time. In this paper, we propose an automatic, physically-motivated segmentation that learns the controls and parameters directly from the set of blend shapes. We show the usefulness and efficiency of this technique for both, motion-capture animation and keyframing. We also provide a rendering algorithm to enhance the visual realism of a blend shape model.		Pushkar Joshi;Wen C. Tien;Mathieu Desbrun;Frédéric H. Pighin	2005		10.1145/1198555.1198588	computer vision;motion capture;computer facial animation;computer science;gesture recognition;optical flow;virtual reality;multimedia;haptic technology;perception;facial expression;linear space;interpersonal communication;computer graphics (images)	Graphics	-39.36099975718668	-36.158107092480556	149242
d9f8a023e68e100309a8f2c6fe1d998776a90b3b	exploring the use of a multi-touch surface to support collaborative information retrieval	computer supported cooperative work;multi touch interaction techniques;collaborative information retrieval	Collaborative Information Retrieval (CIR) is the process by which people search for and retrieve information, typically using documents as data sources. Shared computing surfaces that utilise multi-touch interaction can allow multiple users to interact around a shared display. This paper describes the design and evaluation of a prototype system to support CIR, named Co-IMBRA. Co-IMBRA was developed to allow multiple users to retrieve information, using the Internet as a shared information space. Documents are represented as visual objects that can be manipulated on a multi-touch surface, annotated, rated and added to folders. Co-IMBRA was evaluated using a user study to determine whether the multi-touch interaction techniques effectively supported CIR. The results showed that Co-IMBRA can be used to effectively support and encourage CIR.	commitment ordering;committed information rate;grid computing;information retrieval;interaction technique;internet;multi-touch;multi-user;prototype;usability testing;visual objects;web search engine	Janet Wesson;Dieter Vogts;Ivan Sams	2012		10.1145/2389836.2389870	computer science;data mining;world wide web;information retrieval	HCI	-42.1387774480593	-24.83334814194553	149439
07f2b2e72bfec60d75185e671910da0f1830f63a	interactive physically-based manipulation of discrete/continuous models	interactive techniques;physics based modeling;satisfiability;design space;grammars;discrete model;interaction model;mechanism design;geometric constraints;user interaction;interaction technique;physically based modeling	Physically-based modeling has been used in the past to support a variety of interactive modeling tasks including free-form surface design, mechanism design, constrained drawing, and interactive camera control. In these systems, the user interacts with the model by exerting virtual forces, to which the system responds subject to the active constraints. In the past, this kind of interaction has been applicable only to models that are governed by continuous parameters. In this paper we present an extension to mixed continuous/discrete models, emphasizing constrained layout problems that arise in architecture and other domains. When the object being dragged is blocked from further motion by geometric constraints, a local discrete search is triggered, during which transformations such as swapping of adjacent objects may be performed. The result of the search is a “nearby” state in which the target object has been moved in the indicated direction and in which all constraints are satisfied. The transition to this state is portrayed using simple but effective animated visual effects. Following the transition, continuous dragging is resumed. The resulting seamless transitions between discrete and continuous manipulation allow the user to easily explore the mixed design space just by dragging objects. We demonstrate the method in application to architectural floor plan design, circuit board layout, art analysis, and page layout. Keywords—Interactive techniques, physically-based modeling, grammars.	active set method;drag and drop;paging;printed circuit board;seamless3d;visual effects	Mikako Harada;Andrew P. Witkin;David Baraff	1995		10.1145/218380.218443	mechanism design;computer vision;simulation;computer science;theoretical computer science;geometry;interaction technique;satisfiability;computer graphics (images)	HCI	-37.69213269723284	-34.296622394275616	149557
839bc687b3e2b585f7f61c7a6d7305c9b96887bc	graphics implementation and conceptualization at augmentation research center			conceptualization (information science);graphics	Charles H. Irby	1971	RFC	10.17487/RFC0191	human–computer interaction;computer science;computer graphics (images)	Robotics	-48.032265068661786	-32.188094318831716	149665
abe241dad497eb4987a0e09e6b7bfd900e5af69a	enhancing interaction design on the semantic web: a case study	personal computing;hypermedia markup languages;semantics ontologies semantic web html web pages user interfaces;human computer interaction;web pages;resource allocation;information retrieval;knowledge management;semantics;user centered design;ontologies artificial intelligence;html;user centred design human computer interaction hypermedia markup languages information retrieval interactive systems knowledge management ontologies artificial intelligence personal computing resource allocation semantic web;human computer interaction interaction design semantic web resource description framework based ontologies knowledge repositories knowledge sharing automatic processing problem solving interactive technologies semantic environment semantic interaction end user oriented paradigms semantic information access demonstration approach html data presentation semantic models interaction evaluation;user interfaces human computer interaction hci semantic web user centered design;semantic web;human computer interaction hci;ontologies;user centred design;interactive systems;user interfaces	The use of resource description framework-based ontologies as knowledge repositories has become increasingly popular over the past few years. The semantic web has rapidly spread, appearing as a new challenge for knowledge sharing and automatic processing. However, the reality is that the power of the semantic web is still barely used. This is mostly because of the fact that the semantic web is a powerful but complex technology that most end users cannot afford to use for their common problem-solving activities. This has probably made the semantic web to stay in the background of interactive technologies, unlike other new end-user-oriented paradigms (e.g., the so-called Web 2.0 and later approaches) that have very much increased along these years. Nevertheless, the semantic web can be considered as a highly valuable paradigm that has not been, conveniently, exploited yet. In this paper, we propose a semantic environment to exploit semantic interaction by end users in order to help them access semantic information easily. We follow a programming by demonstration approach, where the user navigates and modifies HTML presentation of data and the system, automatically, infers changes to the underlying semantic models. Furthermore, we provide an evaluation of the interaction, including the most important results obtained for the proposed approach.	interaction design;semantic web	José Antonio Macías	2012	IEEE Trans. Systems, Man, and Cybernetics, Part C	10.1109/TSMCC.2012.2187052	semantic interoperability;semantic computing;user-centered design;web modeling;semantic integration;html;semantic search;semantic grid;web standards;resource allocation;computer science;ontology;semantic web;social semantic web;web page;linked data;database;semantics;web intelligence;user interface;world wide web;owl-s;information retrieval;semantic analytics	Embedded	-42.39014237220023	-25.26499558223125	149745
62aaa821810965aa711c15ee7e801aaa7b514566	from urban terrain models to visible cities	town and country planning;emergency response 3d city semiautomated system visually navigable models urban models virtual environments urban planning;virtual reality;terrain modeling;emergency services virtual reality town and country planning;cities and towns image databases virtual reality buildings filling system testing urban areas libraries visualization visual databases;emergency services	returning from his travels, encounters Kublai Khan. It’s the end of the day, near the end of the great Khan’s life, and perhaps at the end of his empire. Marco Polo tells of the cities he visited while traveling the far-flung empire. Each one is fantastic and is described in a short fable, so the whole story takes on the quality of a folktale constructed around a series of adventures. The reader can’t be sure whether these cities actually exist or whether they are an illusion in Marco Polo’s or the Khan’s mind. We are now faced with the possibility and, in some cases, the results of acquiring accurate digital representations of our cities. But these cities will be just as invisible as Marco Polo’s unless we meet some fundamental challenges. The first challenge is to take data from multiple sources, which are often accurate but incomplete, and weave them together into comprehensive models. Because of the size and extent of the data that we can now obtain, this modeling task is daunting and must be accomplished in a semiautomated manner. Once we have comprehensive models, and especially if we can build them rapidly and extend them at will, the next question is what to do with them. Thus, the second challenge is making the models visible. In particular, they must be made interactively visible so users can explore, inspect, and analyze them. In this article, we discuss the nature of the acquired data and how we’re beginning to meet these challenges and produce visually navigable models. We’ve developed 3D City, a semiautomated system that supports human intervention at key points to meet the challenge of constructing complete and extended urban models from several data sources. We’ve already built virtual environments (VEs) for urban planning and emergency response using 3D City. Once we’ve met the challenges of urban construction and visualization, possible applications include education, urban planning, emergency response, tourism and entertainment, military operations, traffic management, construction (especially large-scale projects), various geolocated and mobile services, citizen–government relations (when complex civic projects are vetted), and games based on real locations. For example, large-scale urban projects now require coordinated efforts by affected neighborhood groups; the business community; and city, state, and federal governments. Collecting data and modeling a city as it is creates a foundation for inserting new structures, bridges, and roads. We can provide different constituencies with interactive visualizations of different model designs. Vehicle and pedestrian traffic models can be applied to the street layouts and then visualized in the city environment, helping with overall planning and with meeting government requirements (such as those from the US Environmental Protection Agency). The results will be faster, less expensive, and better planning and construction. With the appropriate apparatus for collecting and then inserting new data into existing data collections, we’ll be able to always keep an urban database up to date. Any change in a building facade, move of a lamppost, or removal of a tree can be recorded.	empire;interactive visualization;interactivity;mind;requirement;virtual reality	William Ribarsky;Tony Wasilewski;Nickolas Faust	2002	IEEE Computer Graphics and Applications	10.1109/MCG.2002.1016692	simulation;computer science;artificial intelligence;virtual reality;computer graphics (images)	HCI	-35.03701050912181	-32.48577799617054	149943
58f41b4f94278bb0ca918bb535fe46f460379a2b	generating annotated graphs using the nlg pipeline architecture		The Arria NLG Engine has been extended to generate annotated graphs: data graphs that contain computer-generated textual annotations to explain phenomena in those graphs. These graphs are generated alongside text-only data summaries.	computer-generated holography;experience;graph (discrete mathematics);graphics;multimodal interaction;natural language generation;pipeline (computing);text-based user interface	Saad Mahamood;William Bradshaw;Ehud Reiter	2014			computer science;theoretical computer science;database;world wide web	NLP	-38.492626550076935	-29.340846655155456	150223
949d779fbcafe1e0456cbbdd6c847153962108c9	turning pages of 3d electronic books	electronic;3d book;software libraries;turning;user interface;computer graphics;digital library;text processing;visual design;publishing;books;user interface 3d book 3d workspace digital library electronic publishing visual design;chromium;animation;3d workspace;electronic publishing;scalability;user interfaces;turning electronic publishing books software libraries animation user interfaces scalability chromium text processing computer graphics	Taking the form of physical books, virtual 3D books can be used as basic components of e-book systems, information workspaces, and digital libraries. This paper describes the page turning design of 3Book, a 3D book system that we recently developed. Our design aims to find a sensible balance among important factors such as visual realism, readability, interactivity, and scalability. To convey the impression of reading or viewing an actual physical book, we model all the faces of the book and synchronize the movements of various portions of the book during page turning. Our design delivers a seamless transition between two states of the book (i.e., when it is lying still and when it is turning pages). In addition, we deform the turning pages around an imaginary cone of changing sizes to produce realistically-looking curved pages.	book;digital library;e-book;imaginary time;interactivity;library (computing);scalability;seamless3d;workspace	Lichan Hong;Stuart K. Card;Jindong Chen	2006	3D User Interfaces (3DUI'06)	10.1109/VR.2006.135	human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-41.33940590286973	-32.323481893834526	150227
b8943d56ad8f5379dca1851a6b8b6fe4539f0fb1	computer-aided staging	staging	This paper describes the genesis and main functionalities of a real-time animation system for designing multimedia scenes. Users can create a system of synchronized events and give a structured shape to the various parts of a performance. Much work has been done on the production of realistic synthetic movies, but little to support planning and pre-production. This system addresses the requirements of those who have to plan, preview and evaluate the spatial and temporal arrangements of human figures with other media before their practical realization in a real or virtual environment. To define a user staging process which is general and valid for different kinds of users and different applications, we have analysed current staging methodologies used in the theatre, cinema and TV. Since these fields lack standard procedures, obtaining a good degree of generality and completeness entailed an iterative work of specification, prototyping and testing with the help of professionals from such fields. Unlike traditional systems which treat media in separate software, e.g. human body motion alone or speech alone, we consider the elements in the scene altogether according to the theatrical paradigm which is the basic framework for describing the design of a performance. Copyright ? 1999 John Wiley & Sons, Ltd.	cinema 4d;disk staging;genesis;iterative method;john d. wiley;programming paradigm;real-time transcription;requirement;software prototyping;standard operating procedure;synthetic intelligence;television;virtual reality	Patrizia Palamidese	1999	Journal of Visualization and Computer Animation	10.1002/(SICI)1099-1778(199901/03)10:1%3C3::AID-VIS191%3E3.0.CO;2-Z	computer vision;simulation;stage;computer science;artificial intelligence;multimedia;computer graphics (images)	Visualization	-39.0653212722466	-35.66730865219136	150381
976f7f2e9a938fc0df063091196f07c279fc29ff	"""""""move the couch where?"""" : developing an augmented reality multimodal interface"""	multimodal interface;multimodal fusion;hit;technology;semantic integration;lab;interaction style;virtual reality;multimedia information system;speech based user interfaces;h 5 2 user interfaces;hitlab;natural language;interface;human;semantic technique;nz;graphic user interface;speech recognition;multimodal interaction;paddle gesture;augmented reality;augmented reality multimodal interface;time based technique;gesture recognition;speech gesture	This paper describes an augmented reality (AR) multimodal interface that uses speech and paddle gestures for interaction. The application allows users to intuitively arrange virtual furniture in a virtual room using a combination of speech and gestures from a real paddle. Unlike other multimodal AR applications, the multimodal fusion is based on the combination of time-based and semantic techniques to disambiguate a users speech and gesture input. We describe our AR multimodal interface architecture and discuss how the multimodal inputs are semantically integrated into a single interpretation by considering the input time stamps, the object properties, and the user context.	augmented reality;habbo;multimodal interaction;oracle fusion architecture	Sylvia Irawati;Scott A. Green;Mark Billinghurst;Andreas Dünser;Heedong Ko	2006	2006 IEEE/ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2006.297812	augmented reality;semantic integration;speech recognition;computer science;multimodal interaction;interface;gesture recognition;virtual reality;multimedia;natural language;technology	Visualization	-47.489892499154784	-37.24165255518408	150624
fbf5e0864c2f69927f0a037555609cd364a73f7a	f.gaze — focus on gaze animation for autonomous virtual human characters	image motion analysis;image motion analysis computer animation;computer animation;motion capture animation gaze animation autonomous virtual human character interactive realtime animation interactive virtual character modified inverse kinematics scheme;bones animation games torso bismuth visualization joints	The advances in interactive realtime animation do not match the realistic visuals synthesized by modern hardware. We investigate if the liveliness of interactive virtual characters may be enhanced by a more detailed gaze animation compared to current games. We propose a modified inverse kinematics scheme, that we use to adapt motion capture animation to a specific situation.	autonomous robot;inverse kinematics;motion capture;virtual actor	Christian Bode;Bernhard Dubbick;Thomas Bremer;David Strippgen;Simone Strippgen	2013	2013 IEEE Third International Conference on Consumer Electronics ¿ Berlin (ICCE-Berlin)	10.1109/ICCE-Berlin.2013.6698047	physically based animation;computer vision;facial motion capture;motion capture;computer facial animation;skeletal animation;computer science;interactive skeleton-driven simulation;non-photorealistic rendering;computer animation;multimedia;computer graphics (images)	Visualization	-39.10486632792825	-36.80798960064918	150727
71336345f2ee940c18e9b162189ce64f747e4edb	design and evaluation of a visual formalism for real time logics	logica temporal;formal specification;visualizacion;temporal logic;heuristic method;higher education;metodo heuristico;ingenieria logiciel;software engineering;specification formelle;usability engineering;especificacion formal;visualization;visualisation;visual representation;graphical representation;genie logiciel;real time logic;methode heuristique;logique temporelle	A visual formalism for the presentation of a real time logic is introduced, motivated, and evaluated. The visual formalism has been designed following a user-centered usability engineering process, targeted to the students of higher education courses in software engineering. On the one hand, heuristic design was applied to maximize consistency, i.e. to minimize the complexity of the visual metaphor mapping textual sentences to the visual representation. On the other hand, individual metaphoric assumptions were defined by prototyping and exposing alternative graphical representations to a representative sample of the target community of expected users. The resulting notation has been implemented within a syntax-directed interactive editor which integrates the visual presentation with the conventional textual notation. The editor has been used to carry out a competitive user-based evaluation of the usability of textual and visual representations, by carrying out a readability test on a larger sample of representative end-users.	semantics (computer science)	M. Lusini;Enrico Vicario	1998		10.1007/BFb0053504	computer vision;visualization;computer science;artificial intelligence;theoretical computer science;software engineering;machine learning;usability engineering;database;mathematics;distributed computing;programming language;algorithm	Vision	-36.76529747618023	-28.01126910286955	150805
bd0376d1132daa576aef8de4500c885ef754a95f	agentsheets: a tool for building domain-oriented visual programming environments	video structure;image recognition;multimedia;spatial reasoning;visual programming;visualization;video editing;authoring;motion picture	Visual programming systems are supposed to simplify programming by capitalizing on innate human spatial reasoning skills. I argue that: (i) good visual programming environments should be oriented toward their application domains, and (ii) tools to build domain-oriented environments are needed because building such environments from scratch is very difficult. The demonstration illustrates how the visual programming system builder called Agentsheets addresses these issues and demonstrates several applications built using Agentsheets.	admissible numbering;agentsheets;integrated development environment;spatial–temporal reasoning;visual programming language	Alexander Repenning	1993		10.1145/169059.169119	computer vision;visualization;functional reactive programming;computer science;multimedia;programming paradigm;spatial intelligence;inductive programming;visual programming language;computer graphics (images)	HCI	-39.842970453141106	-31.913392958659017	150837
82227943ceb302ef7eebd450df48b3ca1cf7cd76	configuring graphics systems components	iso;computer graphics;standardisation computer graphics;standardisation;graphics system;computer graphics reference model;computer graphics reference model graphics systems components iso standardisation;graphics systems components;computer graphics standardization	Computer graphics systems have traditionally been described in terms of a conceptual model of the so-called ‘graphics processing pipeline’. This model explains the relationship between graphics information defined by an application and the realisation of that information on a display in terms of a sequence of transformation stages. Although adequate for giving an outline of a single graphics system, the model lacks flexibility and detail when placed in the sphere of many different graphics systems designs, as in the ‘family of graphics systems’ under ISO standardisation at the current time. An alternative approach is needed which provides a sufficient level of detail and flexibility to describe both existing graphics systems and possible extensions to these, which at the same time permits the comparison of graphics systems designs in a well defined framework. The computer graphics reference model presented in this paper meets many of these objectives.		David B. Arnold;Graham J. Reynolds	1988	Software Engineering Journal	10.1049/sej.1988.0032	graphics pipeline;scientific visualization;computer graphics metafile;iso image;computer hardware;computer science;real-time computer graphics;graphics software;computer graphics;standardization;software rendering;3d computer graphics;computer graphics (images)	SE	-37.695627501745946	-30.43792105105398	150840
5718aad983d24f5b51ee6fc33ecc449d7cd67edb	interactive reduce	interactive mode;interactive reduce;paper deal;reduce-2 computer algebra system;new version;extended interactive capability;brief description;distinctive characteristic	This paper deals with some distinctive characteristics of the REDUCE-2 computer algebra system operating in interactive mode. A brief description of a new version of the system with extended interactive capabilities is presented.	computer algebra system;reduce	Alexey P Kryukov;A. Ya. Rodionov	1985	ACM SIGSAM Bulletin	10.1145/1089411.1089416	simulation;computer science;multimedia;computer graphics (images)	Graphics	-45.556337340159494	-30.200180955726225	150906
2c306df73587960cdc08d052ad03b4f8d9aa1609	an introduction to 3d spatial interaction with video game motion controllers	human movement;3d virtual world;spatial interaction;video game;augmented reality	3D spatial interfaces [Bowman et al. 2004] give users the ability to spatially interact with 3D virtual worlds because they provide natural mappings from human movement to interface controls. These interfaces, common in virtual and augmented reality applications, give users, rich, immersive, and interactive experiences that can mimic the real world or provide magical, larger than life interaction metaphors [Katzourin et al. 2006].	augmented reality;experience;motion controller;virtual world	Joseph J. LaViola;Richard Marks	2010		10.1145/1837101.1837103	augmented reality;simulation;human–computer interaction;computer science;mixed reality;multimedia	HCI	-44.622762560845096	-37.34607459885291	151251
0a1f6eb46228ef035ec2233e96221bcda48a39a5	semantic annotations for digital video	semantic web;indexation	"""This paper describes a functioning system that associates semantic annotations (in the form of a table of triples of identifiers) with digital video. The first part describes a subsystem (in existence since 2002, desktop version 1995) of adding annotations to digital video. The second part builds on that subsystem, creating additional semantic annotations that enable semantic-web retrieval. The key component of the second subsystem is a glossary for the text being indexed, created by the user in interaction with Princeton's WordNet. The resulting glossary is a microformat within an XHTML document, which we validate using our microformat validator. Additional techniques for harvesting metadata from the domain expert are described. 1 Multimedia Annotator XML (MannX) MannX consists of two programs, MannX-author and MannX-player. MannX-author creates MannX applications that are accessible via MannX-player. The main functionality of MannX-author is to establish a segment-by-segment correspondence between time-segments of video and space-segments of associated text (which may or may not be the transcript of the video). This is done in the point-and-click fashion; an hour of video can be synchronized with its text in about 90 minutes. MannX-player provides the following functionaliity: Navigation by text and by video: the ability to find a video segment corresponding to a selected stretch of text and vice versa. We have, in effect, random access to time-based video based on its content. In other words, text segments serve as indices of an associative array whose values are video segments. The converse is also true: given a segment of video (showing, e.g., a particularly tricky step in the bypass operation) the user can click to see the corresponding segment of the expanatory text. Segment replay: the ability to replay a segment of video while viewing the corresponding text and commentary. Hypermedia annotation of text and video: the ability to attach arbitrary annotations to a segment of text (and therefore to the corresponding video segment), attach more than one set of annotations to the text, and easily switch between them. (For instance, in language-learning contexts one may want to have grammatical, lexical, stylistic and cultural commentaries.) The reason we can have multiple annotations is that we do NOT attach them directly to the video file, as in MPEG 7, but via a text file with matching segments. Dictionary lookup: typically, a MannX application has a glossary (bilingual or monolingual) of associated terms. The glossary can be associated with a single application or a group of applications that together form a course or a manual. This functionality is relatively straightfoward, but the glossary is an important component of the system of semantic annotations described in this paper. The screenshot below shows a typical MannX screen with four frames: a video recording of a lecture; its transcript; an English-Pashto glossary (Pashto is one of two official languages of Afghanistan); and a commentary that in this case consists of translations of transcript segments into Pashto and slides accompanying the lecture. Fig. 1. The MannX Screen The first version of MannX (Nakhimovsky, 1997) was a desktop application for Windows and Mac OS, written in C++, in which text segments were indicated by byte offsets, and adding functionality was difficult. The current version (Nakhimovsky and Myers, 2003) is written in Java, JavaScript and XSLT, so text segments and glossary entries are indicated by XHTML """"microformats"""" (see, e.g., Dubinko 2005), and the system is open to the entire array of XML and Semantic Web technologies, including those that move effortlessly between sets of triples and RDF, RDFS and OWL. We test on Red Hat Linux, Windows XP and OSX. In this paper we describe an extension of the MannX system that adds knowledge-based indexing and retrieval of digital video and associated text, using an enriched glossary and a database of triples that represent semantic relations between items in the glossary and the text. 2 Microformats and Validation In this section we present and illustrate with examples the new or revised components of the MannX system. They include: Glossary, revised as an XHTML microformat ntGloss; Triples table, also a microformat ntTriples. We use the prefix nt, as in ntopus.com, as a lightweight substitute for a namespace. XhtmlDecoder, a general JavaScript class for building and validating microformat objects. Both ntGloss and ntTriples are subclasses of XhtmlDecoder. (See Nakhimovsky and Myers 1998 for OOP in JavaScript.) The alpha.html utility for building glossaries from text. It has been greatly revised from the earlier version to include semantic information retrieved from WordNet (http://wordnet.princeton.edu/perl/webwn). The code sample below shows the ntGloss and ntTriples microformats: <table id=""""glossTable"""" class=""""ntGloss""""> <tr id=""""sound_n1"""" class=""""ntDef""""> <td class=""""ntTerm"""">sound</td><!-the lemma of the entry--> <td class=""""ntPos"""">noun</td> <!-Part of Speech --> <td class=""""ntDesc""""> <!-description of meaning--> the particular auditory effect produced by a given cause </td> <td class=""""ntExList""""><!-List of examples --> <span class=""""ntEx"""">the sound of rain on the roof</span> <span class=""""ntEx"""">the beautiful sound of music </span></td> <td class=""""ntPatList""""> <span class=""""ntPat"""">sound</span> </td> </tr></table> <table id=""""triplesTable"""" class=""""ntTriples""""> <tr class=""""ntRel""""> <td class=""""ntRelX"""">sound_n1</td> <td class=""""ntRelR"""">subClass</td> <td class=""""ntRelY"""">perception_n3</td> </tr></table> Our microformat implementation depends on using CSS class attributes which are also the names of JavaScript classes. A microformat structure is defined by a JavaScript object (associative array), which serves the function of a DTD or a grammar. Consider the definitions below: var ntGlossParts = {ntGloss:{ntDef:""""*""""}, ntDef:{ntTerm:1,ntPos:1,ntDesc:1,ntExList:""""?"""",ntPatList:""""?""""}, ntTerm:{}, ntPos:{}, ntDesc:{}, ntExList:{ntEx:""""*""""}, ntPatList:{ntPat:""""*""""}, ntEx:{}, ntPat:{} } var ntTriplesParts = {ntTriples:{ntRel:""""*""""}, ntRel:{ntRelX:1,ntRelR:1,ntRelY:1}, ntRelX:{}, ntRelR:{}, ntRelY:{} }; BuildDecoderClassesFor(ntGlossParts); BuildDecoderClassesFor(ntTriplesParts); These are reasonably readable: for instance, ntGloss consists of 0 or more ntDef’s, where an ntDef consists of a term, Part of Speech, description, an optional list of examples, and an optional list of Regular Expression patterns to match when looking for instances of this term in the text. (In the example, the list of patterns consists of just the literal identical to the term.) The function BuildDecoderClassesFor() uses the class definitions to build subclasses of XhtmlDecoder that conform to those definitions. Being a subclass of XhtmlDecoder is good because you can use the builder() method of that class to create an object of the specific subclass from XHTML data, and to validate it in the process. If the data does not conform to the microformat, an exception will be thrown: function doIt(x){ try{ builder(top.document.getElementById(x),ntTriplesParts); }catch(ex){alert(""""ERROR in building """"+x+""""; """"+ex); } The builder() method recursively traverses the DOM subtree specified by the first argument, and checks all its elements against the class definition provided by the second element. The code of XhtmlDecoder (close to 200 lines without comments) forms a self-standing module that can be used in many microformat applications, providing the benefit of validation, providing the benefits of validation, standardized data access and some automatic processing. For example, the first ntDef definition within an ntGloss object can be accessed within an ntGloss method as this.xdParts.ntDef[0] This is because each non-leaf class is constructed with an xdParts object which contains its part-objects, as well as a reference to the XHTML DOM element from which each object came; in this case this.xdParts.ntDef[0].xdDomElement If we provide an """"onclick"""" method for the ntDef class in Javascript, then the glossary initialization will automatically pass that onclick to each XHTML element which is recognized and processed as an ntDef. 3 The Glossary and the Triples In this section we show simple examples of the Glossary and the Triples structures, and step through code that shows them in operation. The examples are coming from a simple XHTML page that contains a glossary table, a triples table, and the buttons to invoke the builder() method on them (see Figure 2). First consider an example of Glossary, with definitions coming from WordNet: <table border=""""1"""" id=""""glossTable"""" class=""""ntGloss""""> <tr id=""""sound_n01"""" class=""""ntDef""""> <td class=""""ntTerm"""">sound</td> <td class=""""ntPos"""">n</td> <td class=""""ntDesc""""> the particular auditory effect produced by a given cause </td> <td class=""""ntExList""""> <span class=""""ntEx"""">the sound of rain on the roof</span> <span class=""""ntEx"""">the beautiful sound of music</span> </td> <td class=""""ntPatList""""><span class=""""ntPat"""">sound</span></td> </tr></table> A click on the buildGloss button invokes the doIt() function of the preceding section, with “glossTable” as its argument. Similarly, a click on the buildTriples button invokes the doIt() function on the Triples table, whose first line goes like this: <table border=""""1"""" id=""""triplesTable"""" class=""""ntTriples""""> The result of these invocations of doIt() is that two JavaScript objects are created, structured as described in Section 2. At this point, one detail of the structure of the Glossary becomes important. As you recall, ntGloss consists of 0 or more ntDef’s, where the ntDef structure is defined as follows: ntDef:{ntTerm:1,ntPos:1,ntDesc:1,ntExList:""""?"""",ntPatList:""""?""""} In addition to static fields, an ntDef object also has the onclick() method defined in the code sample below. Note that while onclick() is defined as an ntDef method, it is actually executed through a click on the associated DOM element, and so the """	apache jena semantic web framework;byte;c++;cascading style sheets;data access;desktop computer;dictionary;digital video;document object model;entity;glossary;html element;human-readable medium;hypermedia;identifier;inferring horizontal gene transfer;java;java applet;javascript;leaf class (computer programming);linux;literal (mathematical logic);lookup table;mpeg-7;markup language;memory segmentation;microformat;microsoft windows;moving picture experts group;ontology (information science);operating system;point and click;programmer;rdf schema;random access;recursion;regular expression;resource description framework;screenshot;semantic web;span and div;subject-matter expert;text simplification;tree (data structure);user interface;validator;web ontology language;wordnet;xhtml;xml;xslt;xfig;macos	Alexander Nakhimovsky;Chris Hellmuth;Tom Myers	2005			voltage;world wide web;information retrieval;computer science;volt;transistor;light-emitting diode;relay;current limiting;shunt (electrical);thermal conduction	Web+IR	-43.89595780200723	-25.6126523828824	151520
1db7d2618140d7e77f57d4ca90aa93f755e68b16	a gui editor that generates tutoring agents	software agent;task model;software agents;model based gui design	Tutoring agents can provide a dynamic and engaging way to help users understand an application. However, integrating tutoring agents into applications is difficult. It requires the expertise to create the tutoring agent, and also an understanding of the inner workings of the application itself. This demo presents a task-based GUI editor that produces a software agent tutor for free. The designer need only create a task model, and then use the editor to produce the GUI. A tutoring agent will automatically be included in the new application.	graphical user interface;software agent	Jacob Eisenstein;Charles Rich	2002		10.1145/502716.502775	simulation;computer science;artificial intelligence;software agent;multimedia;world wide web	HCI	-42.59550419402259	-31.750800294603685	151546
38a460a86bfb6a27b0f66b928f00e1035c8ede02	state of the art in global illumination for interactive applications and high-quality animations	global illumination;interactive application	Global illumination algorithms are regarded as computationally intensive. This cost is a practical problem when producing animations or when interactions with complex models are required. Several algorithms have been proposed to address this issue. Roughly, two families of methods can be distinguished. The first one aims at providing interactive feedback for lighting design applications. The second one gives higher priority to the quality of results, and therefore relies on offline computations. Recently, impressive advances have been made in both categories. In this report, we present a survey and classification of the most up-to-date of these methods. ACM CSS: I.3.7 Computer Graphics—Three-Dimensional Graphics and Realism	algorithm;cascading style sheets;computation;computer graphics;global illumination;interaction;online and offline;quality of results	Cyrille Damez;Kirill Dmitriev;Karol Myszkowski	2003	Comput. Graph. Forum	10.1111/1467-8659.t01-1-00646	computer vision;computer science;artificial intelligence;multimedia;global illumination;algorithm;computer graphics (images)	Graphics	-39.61684709628942	-33.94282074172845	151631
9adda294babba13cb6b7ef831ff97d2834868eb0	learning angularjs - a guide to angularjs development		With AngularJS, you can quickly build client-side applications that run well on any desktop or mobile platform, using REST web services for backend processes. You may have heard that the learning curve for this JavaScript MVC framework is too steep, but that’s not the case. This practical guide provides a hands-on approach to learning AngularJS that will have you building high-quality applications and websites in no time. Along with a conceptual understanding of the framework, you’ll also gain direct experience with AngularJS by building a sample application throughout the book. If you’re familiar with JavaScript, web development, and software design concepts and patterns, this book is the perfect way to get started.	asp.net mvc;angularjs;client-side;desktop computer;hands-on computing;javascript;mobile operating system;software design;web development;web service	Ken Williamson	2015				SE	-44.302950450531974	-30.67637786099213	151842
05dc6ff0034a11161b01d4ab93a0f9b98ed3d185	spoken-word direction of computer program synthesis	computer program synthesis;voice-directed programming;spoken computer commands;constructive composition of functional programs;incremental verbal software specification;talking with computers;human- computer interface;voice computing;chatting in context.	Prototype software is being designed to orchestrate speech-directed synthesis of customizable computer programs. The problems encountered are considered from a perspective that assumes the notation, syntax and function structure of APL. Program synthesis is to be completed with spoken-word dialogs between humans and computers. The computer is to assist in constructing programs with minimal or zero need for mechanical contact between mobile users and computer hardware. During synthesis, the system is to respond audibly and, only when necessary, visually. Spoken commands that invoke functions must be easily recognized in a limited vocabulary in a given context for interactively completing specification of each program. Experimentation with prototype system is expected to facilitate the replacement of conventional text-entry programming systems by that a practical one for speech-directed program synthesis and development.	apl;artificial intelligence;autonomous robot;computer hardware;computer program;distributed computing;high- and low-level;interactivity;iteration;network switch;pervasive informatics;program synthesis;prototype;semiconductor industry;speaker recognition;user (computing);vocabulary;voice command device;dialog	Alvin J. Surkan	2000		10.1145/570475.570504	program analysis;computer literacy;software requirements specification;computing;speech recognition;computer science;artificial intelligence;theoretical computer science;operating system;programming language;computer network programming;functional programming	PL	-46.529160127013846	-37.41603125137128	152066
498c8f0b151bc64e2ca0586a7b68ca7338222379	coupled mode synthesis	arte	A new way of doing modal-style synthesis has been developed which allows easy control over decay rate profile, natural coupling effects like two stage decay, and much complex timbres for very little additional compute cost, resulting in high quality, yet practical, physical modeling of percussion sounds.		Scott A. Van Duyne	1997			computer science	Graphics	-45.784418803057264	-34.52921579107204	152262
fca59309cf0b7a9a8a17b8e7a8157573514d9724	from documenting design to design by documenting	case base reasoning;scenario based design;case based reasoning;design memories	User-centered approach to Information Systems (IS) design requires documenting user interfaces in conjunction with the other design documents. The lack of this documentation increases the cost of the user-centered specifications when producing a new version of the user requirements or passing from a system to an analogous one, although in principle it is possible to take advantage from former experience. To facilitate both versioning and reuse of the IS specifications, the paper presents a new organization of the design documentation based on a story-telling theory (SIT) previously proposed by the authors. SIT-based specifications consist of a set of use stories, each constituted by a sequence of episodes. Within this framework, the paper proposes to structure the IS design documentation as a set of use episodes, each referred to a multimedia document, called scene, illustrating how the episode is enacted by its main character in collaboration with other actors of the story. Scenes are traced to system interface and structure, thus enabling the designer to see how episodes influence the implementation. Moreover, linking the scenes of a project to the analogous ones of former projects results in a collaboratively built design memory appropriate for a reasonable documentation of the design process that facilitates versioning and reuse.	information system;requirement;software documentation;user interface;user requirements document;user-centered design	Alberto Faro;Daniela Giordano	1997		10.1145/263367.263376	case-based reasoning;computer science;knowledge management	SE	-40.07826859377924	-27.32626268301518	152332
5ff460cc07ebabb912bf0bedf2e723edb6655e9f	taichi: an open-source computer graphics library		An ideal so ware system in computer graphics should be a combination of innovative ideas, solid so ware engineering and rapid development. However, in reality these requirements are seldom met simultaneously. In this paper, we present early results on an open-source library named Taichi (http://taichi.graphics) which alleviates this practical issue by providing an accessible, portable, extensible, and high-performance infrastructure that is reusable and tailored for computer graphics. As a case study, we share our experience in building a novel physical simulation system using Taichi.		Yuanming Hu	2018	CoRR		computer engineering;computer vision;computer science;software system;artificial intelligence;extensibility;computer graphics	Visualization	-41.083248446739084	-32.491477466750936	152374
fdbb51c2d2b68b7b4a49c0af31fb0ab0b4a8583d	tüist: transformable über interface for stardom	transformable;music performance;gesture;nime;motor mimesis;multi track;toy;one man band	This project is an emotionally driven interface taking on our inner desires and fantasies of instantaneously becoming superstars (and momentarily living under the skin of our idols) and on our instinctive imitation of the musical performers gestures as expressions of sounds.  Tuist is a multi-instrument interface based on a single object with minimal gesture input surfaces and various modes of use through different positioning and orientation towards the user's body. Multi-track gesture recording and playback/loop features also enable for multi-arrangement and composition by allowing the user to record and interact with his own performances in time. Its objective was the development of an intuitive and playful interface for novices, capable of providing explorative interactions in an enjoyable experience inspired by our significant mimicking gestures of 'real' musicians and our private ambitions of creating music.	interaction;performance;under the skin	Rui Pereira	2008		10.1145/1501750.1501880	simulation;computer science;multimedia;communication	HCI	-47.512358584812524	-36.37130697976493	152443
a238d15d4cc671b17133342ade7498f8b8db804e	semi autonomous camera control in dynamic virtual environments	script interpreter;pc and xbox;camera control	We present a system for controlling the camera movement in an interactive dynamic virtual world. The camera control is scripted in a specialized scripting language. A portable script interpreter has been implemented, allowing to run the scripts both on standard PCs and XBOX 360 systems.	autonomous robot;semiconductor industry;virtual reality	Marcel Klomann;Jan-Torsten Milde	2011		10.1007/978-3-642-22024-1_40	embedded system;computer science;operating system;computer graphics (images)	Robotics	-42.02318058164913	-34.46375300047174	152455
d7ed177749ad590635e4477085e67486ecb132a0	user interface agents in a public information system	interfase usuario;multiagent system;architecture systeme;user interface;information retrieval;intelligence artificielle;apprentissage moteur;recherche information;agent intelligent;intelligent agent;artificial intelligence;arquitectura sistema;interface utilisateur;agente inteligente;inteligencia artificial;recuperacion informacion;information system;motor learning;system architecture;sistema multiagente;public information;systeme information;aprendizaje motor;systeme multiagent;sistema informacion		information system;user interface	Nestor Pridun;Peter Purgathofer	1997			user interface design;user;simulation;motor learning;computer science;engineering;artificial intelligence;user interface;intelligent agent;information system	ECom	-36.96222263765215	-25.7562135078916	152469
d32403a440c9224c7cd5eba2128fba761d48d130	fishnet: finding and maintaining information on the net		This short paper presents a tool for keeping a hotlist or homepage up to date. It combines two existing tools: MOMspider [Fielding 1994] is a tool to verify whether links are still valid and whether documents they point to have been modified or moved. Fish-Search [De Bra & Post 1994b] is a search tool for finding new interesting documents in the neighborhood of a given set of (addresses of) documents. FishNet keeps track of the evolution of a domain of interest by periodically running MOMspider and FishSearch and presenting the user with newly found documents. The user can put documents in the hotlist or in a reject list. This positive and negative feedback are constantly used to improve the precision of the search. 1. Overview and Motivation Beginning World Wide Web users start collecting addresses of interesting documents they find, by storing them in the browser’s bookmark list. Later they may also move this information to their home page to share their findings with the world. Keeping the list consistent and adding addresses of new interesting documents to ensure that the list remains a valuable resource can quickly become a full-time job. Existing large search engines such as Alta Vista, Excite or Lycos do not offer a solution to this problem, because no small set of keywords is sufficiently discriminating to perform a search without returning a high rate of non-relevant documents. Browsing through the answers of these engines, in search of some new interesting documents, often takes much more time than it’s worth. The FishNet toolkit offers a platform for automating hotlist maintenance. It offers the following features: Verification of link consistency and of updates to documents through the standard MOMspider package [Fielding 1994] (developed by Roy Fielding, not by us). Multi-threaded Fish-Search navigation engine [De Bra & Post 1994a, De Bra & Post 1994b] for finding new documents. This engine can be extended by means of external filters for determining relevance of documents. FishNet contains a set of such filters. History of documents previously marked as relevant or non-relevant, to improve the selection of new documents. (HTML) Report generator through which the bookmark list or home page can be updated. By means of FishNet the user can ensure that the list or home page always contains valid links, that the descriptions of these documents remain accurate, and that new documents on the topics of interest are found and added to the list. Using FishNet can reduce the full-time information discovery job to just a few minutes a day. For use with FishNet the Fish-Search tool has been improved significantly since its original development back in 1994. The most important new features of Fish-Search are: Fish-Search used to be integrated into a Web-browser. The new version is a stand-alone program that can be activated as a CGI-script. 1Paul De Bra is also affiliated to the University of Antwerp, and with the “Centrum voor Wiskunde en Informatica” in Amsterdam. Use of multi-threading (through the standard W3C library) to load documents from different servers in parallel. Fish-Search now obeys the Robot-Exclusion protocol [Koster 1994]. External filters can be used in addition to the built-in keywordregular-expression and approximate maching algorithms. These filters must reside in a special directory to avoid abuse.	approximation algorithm;browsing;built-in self-test;common gateway interface;directory (computing);document;excite;html;home page;information discovery;multithreading (computer architecture);negative feedback;relevance;report generator;robots exclusion standard;thread (computing);web search engine;world wide web	Paul De Bra;Pim Lemmens	1997			discrete mathematics;mathematics	Web+IR	-45.76167292031741	-24.5690035973423	152500
3fd7badc352d6661916d74e564f60b0545cd3e0b	location-based device ensemble architecture for a spatial reality enhancement of distributed multimedia presentation	sensibilidad contexto;interfase usuario;affichage;context aware;multimedia;realite virtuelle;visualizacion;realidad virtual;distributed multimedia presentation;machine unique;user interface;signal audio;analyse fonctionnelle;musica;localization;audio signal;distributed multimedia;virtual reality;terminal;localizacion;location;lenguaje marcacion;spatial reality;scenario;realite augmentee;realidad aumentada;musique;single machine;maquina unica;localisation;display;functional analysis;modulo funcional;module fonctionnel;interface utilisateur;sensibilite contexte;functional module;augmented reality;multimedia presentation;speaker;locutor;markup language;music;set top box;locuteur;senal audio;langage marquage;analisis funcional	Usual multimedia contents are represented as scenarios with markup language such as SMIL, and then delivered to be played back in terminal machines, e.g., PC or STB(Set-Top Box). However, these multimedia presentations are likely limited within single machine presentation where only the speakers and display devices attached to same computer system are used, so that the spatial reality is necessarily degraded consequently. In order to enhance even audio scenario presentation with such spatial reality, each audio clip for each audio actor, dubbing artist or background music should be assigned a separate speaker mostly close to the location where the author intended. In this paper, we propose a new architecture called L-DEAR(Location-based Device Ensemble Architecture) to support the spatial reality using location information of each node in a network environment. The core of the L-DEAR is IBEE(Immediate-But-Ephemeral device Ensemble) which is a set of nodes called out for playing back at least one assigned clip at a given time during the whole presentation. We will present the functional modules of L-DEAR and procedures for building IBEE, as well as functional requirements for L-DEAR such as extending the SMIL and using a global clock.		Doo-Hyun Kim;Lila Kim;Hwasun Kwon;Dongwoon Jeon;Songah Chae	2006		10.1007/11941354_38	functional analysis;augmented reality;simulation;computer science;artificial intelligence;scenario;operating system;audio signal;music;database;virtual reality;multimedia;markup language;programming language;location;user interface;computer security	HCI	-35.65072281808118	-26.82053916958842	152835
1a1c50d8dda2ab088ba6625b51955dc8de35f31d	a computational steering system for studying microwave interactions with missile bodies	simulation model;engineering graphics;microwaves;network flow;cave;scientific visualization;interface;modeling and simulation;virtual reality;parameter space	This paper describes a computer modeling and simulation system that supportscomputational steering , which is an effort to make the typical simulation workflow more efficient. Our system provides an interface that allows scientists to perform all of the steps in the simulation process in parallel and online. It uses a standard network flow visualization package, which has been extended to display graphical output in an immersive virtual environment such as a CAVE. Our system allows scientists to interactively manipulate simulation parameters and observe the results. It also supports inverse steering , where the user specifies the desired simulation result, and the system searches for the simulation parameters that achieve this result. Taken together, these capabilities allow scientists to more efficiently and effectively understand model behavior, as well as to search through simulation parameter space. This paper is also a case study of applying our system to the problem of simulating microwave interactions with missile bodies. Because these interactions are difficult to study experimentally, and have important effects on missile electronics, there is a strong desire to develop and validate simulation models of this phenomena.	computational steering;computer simulation;experiment;flow network;graphical user interface;interaction;interactivity;microwave;virtual reality	J. Edward Swan;Marco Lanzagorta;Doug Maxwell;Eddy Kuo;Jeffrey K. Uhlmann;Wendell Anderson;Haw-Jye Shyu;William Smith	2000		10.1145/375213.375291	computer vision;scientific visualization;flow network;simulation;microwave;computer science;simulation modeling;interface;modeling and simulation;virtual reality;parameter space;cave;programming language;statistics;computer graphics (images)	HPC	-34.54807141127684	-29.426230386148998	153170
1a8dcfcf771876848755a089de9831b813c30399	state estimation techniques for 3d visualizations of web-based tele-operated mobile robots.	3d visualization;mobile robot;real time;web interface;state estimation;internet use;dynamic environment;world wide web	The Internet provides a unique opportunity to tele-operate and monitor mobile robots. Web-controlled mobile robots can give people all over the world the ability to become telepresent at distant places. Additionally, Internet-based tele-experimentation systems for mobile robots give distributed research groups located at distant places the ability to carry out joint experiments. This way, they can share expensive robot platforms and furthermore save travel expenses. Finally, the Internet can be used for on-line demonstrations with mobile robots, for example during the presentation of research results at conferences. All these applications of Web interfaces for mobile robots require accurate visualization techniques. This includes a high level of detail as well as high update rates. Unfortunately, the Internet does only provide a restricted bandwidth. Therefore, video streams cannot be transferred at appropriate resolutions and with a frame rate required for smooth visualizations. Moreover, static monitoring cameras, which are frequently used on the Internet, have the disadvantage that they only provide a reduced field of view and that important details are occluded in certain perspectives. In this paper we present state estimation techniques for a system which provides accurate and smooth real-time 3D visualizations of the movements of an autonomous mobile robot over the Internet. The system has a client/server architecture. The server is directly connected to the robot control system and transfers changes of the state of the robot and changes of the environment to all its clients which provide the 3D visualization at high frame rates. The system uses a 3D model of the robot and its environment. To cope with changes in the environment it uses probabilistic techniques to continuously estimate the states of different types of non-static objects such as people, doors, tables, etc. based on the information acquired by the robot’s sensors. The changes of the environment are transfered to all the clients connected over the Internet which can instantly update the visualization. 2 Related Work	autonomous robot;client–server model;control system;experiment;high-level programming language;internet;level of detail;mobile robot;online and offline;real-time locating system;robot control;sensor;server (computing);streaming media;television;volume rendering	Dirk Schulz;Wolfram Burgard;Armin B. Cremers	2000	KI		web service;sensor web;web modeling;mobile search;mobile web;web mapping;web-based simulation;web design;human–computer interaction;computer science;web navigation;multimedia;world wide web;web server	Robotics	-34.752846158016006	-36.50798399550725	153191
ad1d2606e773a6ec1ea98d7bc45a3c1f42572328	colorplate: the mobile animator: interactive character animation in collaborative virtual environments	collaborative virtual environments;mobile animator;interactive character animation;real time control;proof of concept;virtual reality;handheld device;character animation	Abstract We     have     designed     a     mobile  - PDA - based - inter - face  for   real - time  control  of   virtual  characters  in   mul - tiuser    semi - immersive    Virtual    Environments  - using    a large   rear - projection   screen The   proof - of - concept   im - plementation  we  present  shows  the  potential  of  handheld devices   as   powerful   interfaces   to   Virtual   Reality   appli - cations This   technique   eliminates   the   display   of   float - ing  menus  and  other  widgets  over  the  simulation  screen A    brief    discussion    on    the    advantages    and    disadvan - tages  of  using  a  handheld  for  3D  interaction  is  presented as well	collaborative virtual environment	Mario Gutiérrez;Frédéric Vexo;Daniel Thalmann	2004		10.1109/VR.2004.10034	character animation;cave automatic virtual environment;real-time control system;human–computer interaction;computer science;artificial intelligence;instructional simulation;operating system;mobile device;virtual reality;multimedia;proof of concept;immersion;computer graphics (images)	HCI	-43.40701893519107	-36.709381053322524	153329
fd92b63861998e6e615fdfe703405fbe99e5644f	spatial audio approaches for embedded sound art installations with loudspeaker line arrays		The concept of embedded acoustic systems for diffusing spatial audio is considered. This paradigm is enabled by advancements in floating-point hardware on inexpensive embedded Linux systems. Examples are presented using line array configurations for electroacoustic music and for making interactive kiosk and poster systems.	acoustic fingerprint;embedded system;interactive kiosk;linux on embedded systems;loudspeaker;programming paradigm;surround sound	Edgar Berdahl;Matthew Blessing;Matthew Williams;Pacco Tan;Brygg Ullmer;Jesse T. Allison	2017			loudspeaker;multimedia;sound art;computer science	EDA	-45.79249184276953	-34.194233709121875	153459
f321283218d7dffe449889d512fc6cd14baee71d	automatically stereoscopic camera control for 3d animation production	attention tracking;stereoscopic 3d;animation;visual comfort;disparity control	This paper proposes a novel approach for automatically controlling stereoscopic camera parameters that specifically addresses challenges in stereo 3D animation production process.Our proposed camera control method produces stereo contents with preferable depth perception and guarantees visual comfort by optimization of camera parameters. We introduce an attention tracking method to calculate convergence plane, avoiding window violation and minimizing visual conflict. Moreover, we derive an smoothing function on convergence plane that reduces depth jump over time. Then, we calculate the inter-axial separation using a perceived depth mapping. We describe how to implement our method on the Maya plug-in and test the stereo effect using professional stereo 3D animation scenes. The experimental results, including a user study, show that our method enhances the stereo effect. Our controller provides automatic camera control that can be helpful in creating comfortable and faster stereo 3D animations.	autodesk maya;computer animation;depth perception;mathematical optimization;plug-in (computing);smoothing;stereo camera;stereoscopy;usability testing	Dawei Lu;Huadong Ma;Zeyu Wang;Liang Liu;Huiyuan Fu	2015		10.1145/2733373.2806369	computer stereo vision;anime;stereoscopy;stereo cameras;stereo camera;computer vision;simulation;computer science;artificial intelligence;computer graphics (images)	HCI	-40.74430501930356	-37.58891880075227	153466
c086fccde785d803ee2fab1979f70fd6f313d621	aurio: audio processing, analysis and retrieval	processing;retrieval;features;fingerprinting;audio;synchronization;dynamic time warping;open source software	Aurio is an open source software library written for audio-based processing, analysis, and retrieval of audio and video recordings. The novelty of this library is the implementation of a number of fingerprinting and time warping algorithms to retrieve, match and synchronize media streams, which no other library currently offers. It is designed with simplicity, performance and versatility in mind, can be easily integrated into .NET applications, and offers a collection of many basic signal processing methods. It can read many file formats, offers multiple export abilities for further processing, and contains various UI widgets for graphical applications. Built upon the Aurio library, AudioAlign is an additionally released open source application for the (semi-)automatic synchronization of media recordings.	algorithm;fingerprint (computing);graphical user interface;library (computing);open-source software;signal processing	Mario Guggenberger	2015		10.1145/2733373.2807408	synchronization;fingerprint;speech recognition;aes11;computer science;processing;operating system;machine learning;dynamic time warping;multimedia;world wide web	OS	-44.045062539180364	-31.76752750727581	153497
c0a208ee7fabf745ea23c81ac14afc2d9ce445da	robot technology ontology targeting robot technology services in kukanchi — “interactive human-space design and intelligence”	kukanchi;structured information environment;ontology			Ken Ukai;Yoshinobu Ando;Makoto Mizukawa	2009	JRM	10.20965/jrm.2009.p0489	computer science;knowledge management;data mining;multimedia	Robotics	-45.03178152883355	-26.04172378441404	153597
9330c1a2a804c1df34e0b7ae0544a6450d34af5b	poster: the neteyes collaborative, augmented reality, digital paper system	3d interface;input devices and strategies;groupware;neteyes;vision based tracking component;groupware augmented reality data visualisation;vision based tracking component neteyes augmented reality digital paper system natural language capability head mounted display seamless collaboration tangible paper map collaborative annotation;digital paper;vision based tracking;h 5 2 user interfaces input devices and strategies tangible interfaces collaborative interfaces 3d interfaces digital paper natural language interaction augmented reality vision based tracking h5 3 group and organization interfaces collaborative computing synchronous interaction;tangible interface;natural language interaction;h5 3 group and organization interfaces collaborative computing;3d interfaces;data visualisation;tangible paper map;h 5 2 user interfaces;seamless collaboration;h 5 2 user interfaces input devices and strategies;natural language;collaborative interfaces;natural language capability;augmented reality;digital paper system;collaborative annotation;collaborative computing;synchronous interaction;tangible interfaces;collaboration augmented reality three dimensional displays visualization collaborative work natural languages monitoring user interfaces tracking computer interfaces;head mounted display	NetEyes is a system that allows remote and co-located partners to collaborate by annotating maps printed on digital paper. It combines natural language capabilities, in particular the interpretation of sketched symbols, with the display of three-dimensional representations of recognized objects, allowing users to jointly visualize a planned or evolving situation in detail. Visualization can take place either on a conventional monitor or through optical see-though, head-mounted displays. Seamless collaboration is promoted via the use of tangible paper maps, which makes it possible for multiple parties sitting around a table to place annotations as they would using regular paper and pen. To account for movements and rotation of the maps that are common during collaborative annotation sessions, a vision-based tracking component is used. This component recovers the location of the paper map in the real-world, and scales and rotates the digitally displayed objects so that they keep aligned with the map.	augmented reality;digital paper;head-mounted display;map;natural language;printing;seamless3d	David McGee;Xiao Huang;Paulo Barthelmess;Philip R. Cohen	2008	2008 IEEE Symposium on 3D User Interfaces	10.1109/3DUI.2008.4476609	human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-43.41077958000589	-36.671581789546565	153727
548c6f563599f51bd40d665c055b45ae4cf42554	capture and express behavior environment (cebe) for realizing enculturating human-agent interaction	woz system;human agent interaction;measuring system;robot operation interface	We are studying how Embodied Conversational Agents (ECAs) express communication behavior with cultural background. The objective of this study is the proposition of the modified Capture and Express Behavior Environment (CEBE) in which a person can interact with ECAs controlled by the captured behavior of another person with cultural background. In this paper, we discuss modifications and concepts of CEBE to apply CEBE for investigations to realize an ECA with cultural background. The prototype system could capture basic human behavior, such as head direction, posture of the upper body, and 3D angles of arms, when each part of the body, such as head, hands, arms and trunk. In addition, the system could control a robot or a virtual agent based on the detected data. We have to develop some implementations to interact with people with cultural background.		Yoshimasa Ohmoto;Akihiro Takahashi;Hiroki Ohashi;Toyoaki Nishida	2010		10.1007/978-3-642-17184-0_4	simulation;human–computer interaction;engineering;communication	HCI	-44.8874758782721	-36.04829370677543	153795
64a2bd783cc2897a3c6d54415a347b686d6f1cde	an intelligent design interface for dancers to teach robots		Dancers are human Expressive Motion experts and could theoretically help robots communicate their state to people, e.g., rushed, confused, curious. The problem is twofold: first, dancers are trained in human-motion whereas many robots are non-anthropomorphic, and second, most dancers are not programmers. This is where the present interface is useful: the robot demos a batch of motions, in person, and the dancer, who knows expressive motion when she sees it, rates each pathu0027s success at communicating a particular state. Using an evolutionary algorithm, the interface — where feedback is recorded on the robotu0027s screen and motion is demonstrated via the robot — calculates a new batch of motions that explore variations of the top-rated paths from the previous generation. This approach addresses the challenges of visualizing the expressive potential of non-anthropomorphic robots, while also ensuring path characteristics are reproducible via the robotu0027s motion controller. The purpose of the interface is to help a non-expert negotiate a high-dimensional space of robot motion expression. Thus, it also has interactive functionality enabling users to freeze a feature value they like, or reset all features to begin again. To illustrate the system, this paper includes the results of two dancers designing motions for an omni-directional mobile robot, showing convergence with every generation. In reality, motion designers may have many authoring styles — exploring multiple solutions before honing in, or being satisfied easily versus getting each detail exactly right. By combining human-in-the-loop machine learning with direct authoring, we create a kinetic conversation between the robot and the dancer, and gain the ability to model knowledge from complementary fields.	actor model;autonomous car;cognitive robotics;computation;evolutionary algorithm;expressive power (computer science);feature toggle;feedback;machine learning;mobile robot;motion controller;motion planning;out there;programmer;smoothing	Heather Knight;Reid G. Simmons	2017	2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2017.8172479	conversation;simulation;evolutionary computation;computer science;intelligent design;robot;mobile robot;motion controller;evolutionary algorithm;convergence (routing)	Robotics	-37.78034792912488	-37.93676061375156	153859
024d6b4d946c3eeed2560a9f71df88e80aed3104	an architectural framework for interactive music systems	interactive music system;music software;software architecture;interactive system;interactive systems	This report introduces the Software Architecture for Immersipresence (SAI) framework to the computer music community. SAI is a software architecture model for designing, analyzing and implementing applications that perform distributed, asynchronous parallel processing of generic data streams. The most significant innovation of SAI is its ability to handle real-time DSP, interactive control, and datacentered representations in a unified model. This generality facilitates the design and implementation of complex interactive systems that combine music analysis, synthesis and on-line control. Two examples illustrate the use of SAI in the design and implementation of interactive music systems: MuSA.RT, a system for real-time analysis and interactive visualization of tonal patterns in music, and ESP, a driving interface (wheel, pedals and display) for creating expressive performances from expressionless music files.	digital signal processor;dynamic music;esp game;interactive visualization;online and offline;parallel computing;performance;real-time cmix;real-time locating system;software architecture;unified model	Alexandre R. J. François;Elaine Chew	2006			software architecture;real-time computing;simulation;human–computer interaction;computer science;operating system;multimedia;computer graphics (images)	Graphics	-45.11107339312913	-33.256996743896266	153905
ef63d30e7ce66a85066412974da11d94cf664ae6	concepts for generating multi-user interfaces including graphical editors		In this paper concepts for generating multi-user interfaces including graphical editors will be discussed. These concepts are applied in the runtime architecture of multi-user interfaces generated automatically by the generator EMUGEN. The main contribution of this paper is the integrative application of the concepts which are mostly already established in computer science.	graphical user interface;multi-user	Alfons Brandl	2002			human–computer interaction;computer science;theoretical computer science	HCI	-41.99915186441	-29.418944343870262	154056
39b4c99bc28ff07beddb82c9c7b6504d3294b2a4	performance-based biped control using a consumer depth camera		We present a technique for controlling physically simulated characters using user inputs from an off-the-shelf depth camera. Our controller takes a real-time stream of user poses as input, and simulates a stream of target poses of a biped based on it. The simulated biped mimics the user's actions while moving forward at a modest speed and maintaining balance. The controller is parameterized over a set of modulated reference motions that aims to cover the range of possible user actions. For real-time simulation, the best set of control parameters for the current input pose is chosen from the parameterized sets of pre-computed control parameters via a regression method. By applying the chosen parameters at each moment, the simulated biped can imitate a range of user actions while walking in various interactive scenarios.		Yoonsang Lee;Taesoo Kwon	2017	Comput. Graph. Forum	10.1111/cgf.13134	computer vision;simulation;multimedia	HCI	-38.86929532179091	-37.91716823248531	154109
b015fb37076a53335510c86a5ef78ba614785d91	foral lp - making pointed queries with a light pen.			light pen	Michael E. Senko	1977			light pen;computer hardware;computer science	Crypto	-46.07766604334332	-30.016312004608285	154161
8c2b492bd8015ed7e330db5214a02752d3134f6c	a model to aggregate heterogeneous learning objects repositories		Technology has increasingly permeated the educational context making the use of learning objects (LOs) very popular. LOs need to be stored and cataloged so they can be located and retrieved in an efficient manner by both students and teachers. Furthermore, the objects collected in different repositories must be examined individually in order to select which ones best match the user’s needs. The main goal of this work is to propose a model that, prepared to the Web 3.0, enables LO to be used, reused or adapted from heterogeneous environments. To reach this goal the creation of a LO federation is proposed in order to organize various repositories in a hierarchical system. We describe how these objects can be harvested, indexed and retrieved by users from different standard repositories and how to present them to users in different devices. The system was able to index more than 100 thousand Los.	aggregate function	Luiz Henrique Longhi Rossi;Marcos Freitas Nunes;Paulo Schreiner;Rosa Maria Vicari	2017		10.1007/978-3-319-60285-1_32	world wide web;database;federated architecture;multi-agent system;computer science;hierarchical control system	ML	-39.481108634206585	-24.176687774167554	154164
5966fc83fe6940f5fc5d7cbdb388589877b8bb73	novative rendering and physics engines to apprehend special relativity	categories and subject descriptors according to acm ccs i 2 3 computer graphics physically based modeling i 2 3 computer graphics modeling and simulation i 2 4 computer graphics realtime rendering i 2 3 computer graphics collision detection i 2 4 computer graphics edutainment	Relativity, as introduced by Einstein, is regarded as one of the most important revolutions in the history of physics. Nevertheless, the observation of direct outcomes of this theory on mundane objects is impossible because they can only be witnessed when relative velocities close the speed of light are involved. These effects are so counterintuitive and contradicting with our daily understanding of space and time that physics students find it hard to learn Special Relativity beyond mathematical equations and to understand the deep implications of the theory.#R##N##R##N#Although we cannot travel at the speed of light for real, Virtual Reality makes it possible to experiment the effects of relativity in a 3D immersive environment. Our project is a framework designed to merge advanced 3D graphics with Virtual Reality interfaces in order to create an appropriate environment to study and learn relativity as well as to develop some intuition of the relativistic effects and the quadri-dimensional reality of space-time.#R##N##R##N#In this paper, we focus on designing and implementing an easy-to-use game-like application : a carom billiard. Our implementation includes relativistic effects in an innovative graphical rendering engine and a non-Newtonian physics engine to treat the collisions.#R##N##R##N#The innovation of our approach lies in the ability i) to render in real-time several relativistic objects, each moving with a different velocity vector (contrary to what was achieved in previous works), ii) to allow for interactions between objects, and iii) to enable the user to interact with the objects and modify the scene.#R##N##R##N#To achieve this, we implement the 4D nature of space-time directly at the heart of the rendering engine, and develop an algorithm allowing to access non-simultaneous past events that are visible to the observers at their specific locations and at a given instant of their proper time. We explain how to retrieve the collision event between the pucks and the cushions of the billiard game and we show several counterintuitive results for very fast pucks. The effectiveness of the approach is demonstrated with snapshots of videos where several independent objects travel at velocities close to the speed of light, c.	numerical relativity;physics engine	Tony Doat;Etienne Parizot;Jean-Marc Vézien	2011		10.2312/EGVE/JVRC11/009-018	simulation;computer science;multimedia;real-time rendering;computer graphics (images)	ML	-41.78737526784238	-35.464608801241354	154212
70c29922a2e1a77eef6fa082dd3c59d34d41d8d0	feasibility of the living canvas: restricting projection to a performer on stage	art design;performance;spectrum;computer graphic;projection systems;near infrared;machine vision;augmented reality	The Living Canvas initiative aims to use a performer on stage as a dynamic projection surface. Using machine vision in the near-infrared spectrum enables the system to follow and adapt to the performer, restricting projection to the silhouette. Ultimately, the system aims to create the illusion of a completely dynamic costume. This paper introduces the concept and presents an implementation and analysis of the performance-critical stages of the projection pipeline, proving the feasibility of the idea as well as analysing the limitations introduced by current digital projection technology.  Bringing together the research from computer graphics and machine vision with the artistic vision and guidance from Cryptic, the initiative aims to create and explore a new expressive medium by taking projection systems on stage to a highly interactive level and providing a powerful new tool for live video artists.	computer graphics;machine vision	Martin Näf;Cathie Boyd	2008		10.1145/1459359.1459438	near-infrared spectroscopy;spectrum;computer vision;augmented reality;simulation;machine vision;performance;computer science;world wide web;computer graphics (images)	HCI	-41.80905651740178	-36.67831381652706	154538
a383cceb505afd5b327bb9639730dcc67ea95f82	user interface development tools for pen computer applications			pen computing;user interface	Isamu Iwai;Yoshiyuki Miura	1993			10-foot user interface;user interface design;natural user interface;human–computer interaction;shell (computing);active pen;computer science;pen computing;user interface;computer applications	HCI	-42.92133391231007	-30.042609729387642	154543
8977d159b0076215b44e27ae2c97a2bcf33de6fb	tabulamagica: an integrated approach to manage complex tables	integrated approach;structural model;table processing;tabular legacies;separation of structure and presentation;wysiwyg editor	"""Tables are a special part of documents and specific means have been developed to manage them. Step by step, the underlying models to edit and format tables have been improved or supplemented by new ones. These models led to a wide variety of table formats and produced """"tabular legacies"""", making it difficult to edit, use, or modify tables in varying formats. It is even more time-consuming to convert them for various media or to unify or compare tabular information. Our approach to tackle these problems is to integrate different formats. To do so, we recognize the table structure, model the structure and the presentational form and combine both. This way, one can modify the structure, the topology, and the layout of tables simultaneously. Table manipulations may be very complex and hard to understand for the user. In addition, users are accustomed to WYSIWYG environments and want to be able to track their operations by optical control. Therefore, we have developed our WYSIWYG-GUI to work on tables, which we present here, discussing the advantages, limitations and further work to do."""	document;graphical user interface;table (information);wysiwyg	Horst Silberhorn	2001		10.1145/502187.502198	computer science;artificial intelligence;table;database;world wide web;algorithm	HCI	-39.45900084360284	-30.612179894631275	154609
85b06e1e5fcc50dffbff348689f37d355157a128	scatar: a mobile augmented reality application that uses scattering delay networks for room acoustic synthesis		We present an augmented reality (AR) audio application where scattering delay networks efficiently generate and organize a reverberator, based on room geometry scanned by an AR device. The application allows for real-time processing and updating of reflection path geometry. It provides a proof-of-concept for plausible audio-spatial registration of a virtual object in a real environment, but further tests are needed in perceptual evaluation.	acoustic cryptanalysis;augmented reality;real-time clock	Alex Baldwin;Stefania Serafin;Cumhur Erkut	2017		10.1145/3139131.3141201	computer vision;simulation;virtual image;artificial intelligence;real-time rendering;computer science;augmented reality;scattering	Visualization	-45.44433100582578	-34.52729034839278	154615
e3851f5b95344fc6ceed1df7756bf6ba1bf5784a	a look at performer/machine interaction using real-time systems	digital signal processing;arte;computer music;real time systems	This paper examines the utilization of the computer as a musical tool. In particular, the relationship between per- formers and interactive computer music systems involving real-time digital signal processing is explored. The ques- tion is posed: as a musical tool, does the computer have a unique functionality?	real-time transcription	Cort Lippe	1996			speech recognition;computer science;multimedia;computer graphics (images)	Embedded	-46.09604774288204	-33.27941235390274	154626
c43af38ad60aa699c72cbcc201eb141a3d39270d	crowd-z: the user-friendly framework for crowd simulation on an architectural floor plan	pedestrian dynamics;design support;agent based modeling;digitized floor plan	This paper introduces Crowd-Z (CZ): a framework that provides a user-friendly platform where architects can perform simple crowd simulations on floor plans. A simple but robust and flexible agent-based system is used for modeling of the crowd dynamics. Such simulations can be performed at any stage of design – from rough sketches to the final blueprints. CZ allows acquiring the layouts for the simulations in a number of ways: freehand sketches, importing already prepared images and appropriating preprocessed images from commercially available Computer Aided Design programs. These three methods are illustrated with practical examples, followed by a number of simulations compared with the literature or other commercially available programs. 2013 Elsevier B.V. All rights reserved.	adobe freehand;agent-based model;blueprint;computer-aided design;crowd simulation;haplogroup cz (mtdna);interactivity;performance;rough set;smoothing;usability	Machi Zawidzki;Mohcine Chraibi;Katsuhiro Nishinari	2014	Pattern Recognition Letters	10.1016/j.patrec.2013.10.025	computer vision;simulation;computer science;machine learning;computer graphics (images)	Vision	-38.571662763887915	-31.994845447144446	154732
8f27ec39db92c64e2d29836dad08259ded8c9691	a graph manipulation visual interface for construction of e-learning systems	graph manipulation visual interface;graph manipulation interface;graph theory;e learning systems graph manipulation interface;graphical user interfaces computer aided instruction data visualisation graph theory;e learning systems;computer aided instruction;e learning system;data visualisation;graphical user interfaces;management education;educational courses graph manipulation visual interface e learning system;educational courses;visual interfaces	In this paper, we propose a visual interface which assist managing educational courses in e-Learning systems. Educational courses in e-Learning systems can be represented as graphs. We design a simple interface which specializes in manipulation of educational courses only by movements of mouse. The feature of the interface is that we can manipulate graphs without pull down menus and buttons. We specify operations by drawing lines in the canvas window. For example, we can add an edge to a graph by drawing an arrow, and delete nodes and edges by drawing crossing lines on them. Since this is an application-dependent small system, we carefully select the operations and design the simple interface. We show which operations can be used in the interface and how we detect them from mouse movements.	graph (discrete mathematics)	Mariko Sasakura;Susumu Yamasaki	2008	2008 12th International Conference Information Visualisation	10.1109/IV.2008.82	human–computer interaction;computer science;theoretical computer science;multimedia	Robotics	-39.81087439121758	-30.998112497531746	154759
67f9809f39834b4f9e24b61e166a59e6ec9db261	changing characters' point of view in interactive storytelling	meetings and proceedings;virtual characters;book chapter;virtual reality;interactive narrative;virtual actors;planning;interactive storytelling;point of view;virtual actor;narrative modelling;ai planning	Virtual characters are at the epicentre of Interactive Storytelling systems and in recent years multiple AI planning approaches have been described to specify their autonomous behaviour. This demonstrator provides an overview of our novel approach to the definition of virtual characters aimed at achieving a balance between character autonomy and global plot structure which proposes the notion of a character's Point of View. Additionally, the demonstrator offers the active spectator the ability to discover the story described from the perspective of a number of different characters. We present our fully-implemented Interactive Narrative based on Shakespeare's Merchant of Venice. The system, which features a novel AI planning approach to story generation, can generate very different stories depending on the Point of View adopted and support dynamic modification of the story world which results in different story consequences.	automated planning and scheduling;autonomy;interactive storytelling;interactivity;point of view (computer hardware company)	Fred Charles;Julie Porteous;Marc Cavazza	2010		10.1145/1873951.1874321	planning;computer vision;simulation;virtual actor;computer science;artificial intelligence;virtual reality;multimedia;world wide web	AI	-47.64850497841605	-34.96881321060617	154796
3cb285be6387181d07a832ab65960ae4f46e63d6	emic: developing works for vocal performance using a modified, sensor based microphone stand	emic;body;performance;voice;vocal;interface;mapping;choreography	The eMic (Extended Microphone Stand Interface Controller) is a Human Computer Interface specifically designed for contemporary vocal performance. The designer has been performing with the interface for the past decade and has explored a range of different approaches to composing and developing performance works for this unique interface. The design of the interface itself draws upon stylized gestural language of contemporary vocalists and the performances explore gesture in the context of vocal performance.	human computer;human–computer interaction;microphone;performance;pointing device gesture	Donna Hewitt	2013		10.1145/2468356.2479580	simulation;speech recognition;emic and etic;performance;interface;choreography;voice	HCI	-46.107576893139424	-36.27926915892417	155046
f2082e9746ddaacb6edc88a2ffff08470aadfe75	scene depth reconstruction on the gpu: a post processing technique for layered fog	real time;general solution;human visual system;virtual environment;visual system;line of sight	Realism is a key goal of most VR applications. As graphics computing power increases, new techniques are being developed to simulate important aspects of the human visual system, increasing the sense of 'immersion' of a participant in a virtual environment. One aspect of the human visual system, depth cueing, requires accurate scene depth information in order to simulate. Yet many of the techniques for producing these effects require a trade-off between accuracy and performance, often resulting in specialized implementations that do not consider the need to integrate with other techniques or existing visualization systems. Our objective is to develop a new technique for generating depth based effects in real time as a post processing step performed on the GPU, and to provide a generic solution for integrating multiple depth dependent effects to enhance realism of the synthesized scenes. Using layered fog as an example, our new technique performs per pixel scene depth reconstruction accurately for the evaluation of fog integrals along line-of-sight. Requiring only the depth buffer from the rendering processing as input, our technique makes it easy to integrate into existing applications and uses the full power of the GPU to achieve real time frame rates.		Tianshu Zhou;Jim X. Chen;Peter Smith	2007		10.1007/978-3-540-73335-5_19	computer vision;simulation;computer science;computer graphics (images)	Graphics	-36.46995463092768	-36.898832389556624	155417
e3d49366dc06484ec2cc8ed2b15da92c7af46570	camouflage as an adaptive control system: applications for multimedia and interactive composition	sensor systems;shape;image color analysis;mathematical model;pattern recognition;context	We first make some considerations on the biological phenomenon of camouflage as a biological control adaptive process and discuss possible approaches for its mathematical modeling. We then focus on applications to interactive media using a simplified approach in which an L-system is imitated by means of a Markov chain. This model is implemented with computational vision and multichannel spatialization techniques as a tool for computer assisted composition.	computer vision;control system;interactive media;l-system;markov chain;mathematical model	Pablo Padilla Longoria;Jaime Alonso Lobato Cardoso	2016	2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)	10.1109/ISMAR-Adjunct.2016.0105	computer vision;simulation;shape;computer science;artificial intelligence;mathematical model	Embedded	-38.60130805543865	-36.46841011654916	155460
db1abe8f1d2842a94c1e7c8c4f22acf305f4c4cb	causal interactions	causal reasoning;psychology;gui;design guidelines;interaction design;graphical interfaces	In this paper we present two design guidelines, causal order and continuity, to be used as rules of thumb for designing intuitive interactions based on principles of causal reasoning. We propose that designing interactions to behave like real-world systems of cause and effect makes them more intuitive. Using these basic principles avoids the limitations inherent to specific metaphors. In three experiments, participants solved puzzles using variations of a novel graphical interface. Participants using interfaces that were consistent with the causal guidelines consistently solved the puzzle faster than participants using inconsistent interfaces. We also discuss common interactions already consistent with the causal guidelines as well as areas where the guidelines are likely to apply successfully. The causal order guidelines provide specific utility while also demonstrating how principles of causal psychology can be applied to help interface designers better convey the functionality of their interfaces.	causal filter;causal system;causality;experiment;graphical user interface;interaction design;scott continuity;software design pattern;world-system	Adam Darlow;Gideon Goldin;Steven Sloman	2014		10.1145/2556288.2557216	causal reasoning;human–computer interaction;computer science;artificial intelligence;interaction design;graphical user interface	HCI	-39.829211399803015	-29.81426670235642	155576
b82707f4e641ae7477e51ceb117ed6a381ecd434	ecce toolkit: prototyping ubicomp device ecologies	prototyping;ubiquitous computing;device ecologies	The tremendous amount of different input devices and interactive environments envisioned by researchers produces a severe challenge for the development of ubiquitous interaction. Toolkits that support the rapid setup of ubiquitous environments reduce the effort in arranging the technological medium and have the potential to lower prerequisite knowledge and automate low-level programming tasks. In this paper, we present our work-in-progress approach: a toolkit that combines physical and digital components into a unique environment to allow the rapid setup of device ecologies.	ecology;extensible computational chemistry environment (ecce);high- and low-level;input device;low-level programming language;ubiquitous computing	Andrea Bellucci;Ignacio Aedo;Paloma Díaz	2014		10.1145/2598153.2600035	simulation;human–computer interaction;computer science;prototype;ubiquitous computing	HCI	-43.430309832974025	-31.801531255681677	155777
51e24bc761d32368e0955b54730c3a8760e79273	2k-reality: an acoustic sports entertainment augmentation for pickup basketball play spaces		In this paper we describe 2K-Reality; an acoustic sports entertainment augmentation designed to enhance the enjoyment of playing and watching the cultural practice of pickup basketball. 2K-Reality is an interactive digital artefact for pickup basketball play spaces that recontextualises sounds appropriated from a National Basketball Association (NBA) videogame to create interactive sonic experiences for players and spectators. We discuss how the design blends NBA videogames and real basketball play spaces using broadcast-style commentary, stadium-style crowd sound effects and contemporary music break beats activated by spectators interacting with a touchscreen interface connected to a public address (PA) system. Using an ethnographic approach, we analyse the different ways spectators orchestrate the different sounds, and the subsequent effects 2K-Reality soundscapes had on social interactions and the experiences of playing and watching pickup basketball. We conclude from our study that 2K-Reality is a demonstration of a compliant sports augmentation, a term we use to describe a digital enhancement of playing and watching grassroots sports without modifying existing spatial, temporal and cultural practices or the standards codified by a sport's governing body.	acoustic cryptanalysis;apevia;crowd simulation;digital millennium copyright act;digital artifact;emulator;experience;interaction;reincarnation;spaces;the australian;touchscreen;virtual reality	Timothy P. Ryan;Jonathan Duckworth	2017		10.1145/3123514.3123529	entertainment;soundscape;public address system;multimedia;pickup;touchscreen;basketball;political science;grassroots	HCI	-48.272377337698465	-35.312419808828544	156417
9d739388c096798810a2d5f43c025d966b6db5a9	a cost-effective usability evaluation progression for novel interactive systems	military computing augmented reality user interfaces statistical analysis;user interface development;user needs;usability evaluation;user interface;usability interactive systems bars laboratories user interfaces augmented reality application software virtual reality costs knowledge engineering;information presentation;situational awareness;statistical evaluation;usability engineering;symposia;man computer interface;test and evaluation;user based formative evaluation usability evaluation progression interactive system user interface battlefield augmented reality system information presentation situation awareness urban war fighting setting usability engineering user based statistical evaluation;statistical analysis;interactive system;computerized simulation;situation awareness;cost effectiveness;user interface design;augmented reality;user interfaces;military computing;urban warfare;formative evaluation	This paper reports on user interface design and evaluation for a mobile, outdoor, augmented reality (AR) application. This novel system, called the battlefield augmented reality system (BARS), supports information presentation and entry for situation awareness in an urban war fighting setting. To our knowledge, this is the first time extensive use of usability engineering has been systematically applied to development of a real-world AR system. Our BARS team has applied a cost-effective progression of usability engineering activities from the very beginning of BARS development. We discuss how we first applied cycles of structured expert evaluations to BARS user interface development, employing user interface mockups representing occluded (non-visible) objects. Then we discuss how results of these evaluations informed our subsequent user-based statistical evaluations and formative evaluations, and present these evaluations and their outcomes. Finally, we discuss how and why this sequence of types of evaluation is cost-effective.	augmented reality;bmc remedy action request system;color gradient;usability engineering;user interface design	Deborah Hix;Joseph L. Gabbard;J. Edward Swan;Mark A. Livingston;Tobias Höllerer;Simon J. Julier;Yohan Baillot;Dennis G. Brown	2004	37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings of the	10.1109/HICSS.2004.1265653	situation awareness;augmented reality;simulation;human–computer interaction;computer science;usability engineering;multimedia;user interface;heuristic evaluation;world wide web	HCI	-34.23780496641579	-32.23266706493779	156455
e0adb4ab93932f2b864208e39014ede970d760f9	accommodating both expert users and novice users in one interface by utilizing multi-layer interface in complex function products	multi layer interface;universal usability;consistent interface	Rapid development of electronic technology promotes the complex function of a product. Users have to adapt themselves to interface with diverse mental model setting. To improve usability, both resolutions of consistent interface and wizard interface were frequently applied to help novice. However, both of them require as much time for those who are familiar with the system, i.e. expert users. Shneiderman (2000) remarked that Universal Usability requires that software systems accommodate a diverse set of users. Multi-Layer interface might be a solution for complex user interface and satisfy both novice and expert users. The idea of multi-layer interface has been applied to hardware and software interface design. Cases were discussed in which the improved Multi-Layer Interface was analyzed and conclusions made.	layer (electronics);mental model;software system;universal usability;user interface;wizard (software)	T. K. Philip Hwang;Horng-Yi Yu	2011		10.1007/978-3-642-21660-2_18	interface description language;interface metaphor;human–computer interaction;computer science;adapter pattern;multimedia;natural user interface;heuristic evaluation;world wide web;graphical user interface testing;multiple document interface;usability inspection	HCI	-42.485975477814286	-30.409971089333666	156594
b2e7775204e22f6950de16dbe6be6d864a539842	an approach to intelligent automated window management	window manager	Abstract   The CUBRICON Intelligent Window Manager (CIWM) is a knowledge-based system that automates windowing operations. The CIWM is a component of CUBRICON, a prototype knowledge-based multi-media human-computer interface. CUBRICON accepts inputs and generates outputs using integrated multiple media/modalities including speech, printed/typed natural language, tables, forms, maps, graphics, and pointing gestures. The CIWM automatically performs window management functions on CUBRICON's color and monochrome screens. These functions include window creation, sizing, placement, removal, and organization. These operations are accomplished by the CIWM without direct human inputs, although the system provides for user override of the CIWM decisions.  The motivation for automated window management is based on the premise that, by freeing the user's cognitive and temporal resources from the task of managing the human-computer interface, more of these resources are available for the user's application domain activities. As the problems and tasks confronting computer users become more complex and information intensive, the potential of this approach for improving overall performance is enhanced. Recent research discussed in this paper indicates that, for some database management tasks, a significant portion of the user's time is spent in managing the window-based interface. If these findings are representative of the larger range of computer-based tasks that use windowing systems, the concept of automated window management offers great potential for enhancing human performance on these computer-based tasks.  This paper provides a brief overview of the CUBRICON system and describes the CIWM and its underlying design principles and premises. The following important CIWM features are discussed: the hybrid tiled and overlapped approach to window layout; an algorithm for determining the importance of a window based on its contents, relation to the ongoing dialogue, time of creation, frequency of use, and recency of use; and an approach to determining window size based on clutter and object resolution requirements. Actual interactive examples are provided to illustrate the CIWM functionality. Results of an evaluation of CUBRICON support the design. Those results which pertain specifically to the CIWM are presented. Limitations and applicability of this research are also discussed.		Douglas J. Funke;Jeannette G. Neal;Rajendra D. Paul	1993	International Journal of Man-Machine Studies	10.1006/imms.1993.1044	simulation;human–computer interaction;computer science;artificial intelligence;operating system;data mining	Robotics	-40.641857188685385	-30.20778281302019	156675
2fb0a915b35166ffcae6b4489558dea69c075b89	compiling a conversation policy's implementation from its validated specification model				Jean-Luc Koning	2000			conversation;computer science;programming language	Logic	-39.208753038468906	-27.306254707424483	156721
62603256bc375820fe8f24e1da78d360f9747f6c	a multimodal reference resolution approach in virtual environment	teoria cognitiva;interfase usuario;metodo paso a paso;step by step method;multimedia;realite virtuelle;realidad virtual;user interface;temporal constraint;virtual reality;semantics;cognitive theory;probabilistic approach;matching function;semantica;semantique;theorie cognitive;user assistance;temporal constraints;assistance utilisateur;enfoque probabilista;approche probabiliste;asistencia usuario;methode pas a pas;constrenimiento temporal;ambiguity;interface utilisateur;virtual environment;ambiguedad;contrainte temporelle;ambiguite;immersive virtual environment	This paper presents a multimodal reference resolution approach in virtual environment, which is called RRVE. Based on the relationship between cognitive status and reference, RRVE divides the objects into four status hierarchies including pointing, in focus, activated, extinct, and step by step it processes multimodal reference resolution according to current status hierarchy. Also it defines a match function to compute the match probability of referring expression and potential referent, and describes the semantic signification and temporal constraints. Finally, sense shape is used to deal with the pointing ambiguity, which helps the user to interact precisely in immersive virtual environment.		Xiaowu Chen;Nan Xu	2006		10.1007/11890881_3	simulation;computer science;virtual machine;artificial intelligence;operating system;semantics;virtual reality;user interface	HPC	-34.985084606575825	-27.05584820168527	156906
c419397204608501de518454645521a1841caa27	virtual world interaction	human computer interaction;player;character;interactivity;kismet and interaction;avatar;virtual world;udk	The research paper describes the development of a user friendly 3D virtual world. For the research, we modeled building number 3 of the Technological University of Panama, Victor Levi Sasso Campus.  For the virtual world development we consider the use of an avatar, which becomes the user's virtual representation that interacts with certain objects in the virtual world.  The tools and resources used, because they facilitate the interoperability, were the following: Computer Aided Design software (CAD), Computer Aided Architectural Design (CAAD), Virtual Reality Technology, Videogame technology and 3D game engine.	avatar (computing);computer-aided architectural design;computer-aided design;game engine;house numbering;interoperability;usability;virtual reality;virtual world	G. NadiaE.Lee;Gisela Torres de Clunie	2014		10.1145/2590651.2590683	simulation;human–computer interaction;computer science;instructional simulation;multimedia;interactivity;character;world wide web	HCI	-42.476818928323056	-32.71646212265059	157032
92d92a7eabf1922b072d07e13c19c0659cb4a251	automatic camera navigation for time-variant objects	art;haptics;design tools	Effectively visualizing scenes in which the data varies by location over time is a difficult task using standard 2D input devices (mouse, keyboard, etc...). The objective of the algorithm introduced is to completely eliminate the human-computer interface element and automatically visualize a scene that will contain optimal views of the scene at any given time, as well as produce smooth camera animation for the entire visualization.	algorithm;computer mouse;human–computer interaction;input device	Matt S. Berger;Lijun Yin	2005		10.1145/1186954.1186987	computer vision;computer science;artificial intelligence;multimedia;haptic technology;computer graphics (images)	Visualization	-41.08896964431507	-37.16301214146864	157129
6b7d884ec1f00e1876b721bcd1aa1b11e860da75	kernel for a responsive and graphical user interface	user interface;display tree;interactivity;graphic user interface;programmer productivity;user friendliness;graphics	Abstract#R##N##R##N#A kernel that facilitates building graphical and responsive user interfaces for application programs has been constructed. A display tree representing the structure of the 2-dimen-sional screen image is defined. Each node contains an input procedure, an output procedure and links to their arguments. This display tree is the only interface between the user and the application program. For viewing purposes, the display tree can be ‘painted’ onto any viewport. The output routines attached to the nodes are invoked to produce the image. The arguments found in the ancestor nodes can be thought of as shared graphical attributes. Upon each input event, the display tree is traversed to determine which node is touched by the cursor. The input procedure attached to that node is then invoked. The use of a library with standard attached procedures (e.g. for screen editing and error checking) leads to a system that behaves uniformly across applications. Some demonstration programs, based on this kernel, show the very dynamic screen communication that can be achieved.	graphical user interface;kernel (operating system)	Hugo J. Strubbe	1983	Softw., Pract. Exper.	10.1002/spe.4380131108	10-foot user interface;human–computer interaction;computer hardware;computer science;graphics;operating system;graphical user interface;programming language;interactivity;user interface;world wide web	HCI	-41.51489224555687	-30.56678859482358	157203
47eeb2462a0395c7c2eb370ae4c16b6e0630df28	smart objects for attentive agents	bottom up;object interaction;behavioural animation;top down;real time;animation system;automatic generation;conference paper;smart objects;smart object;virtual environment	We present an extended framework for modelling agent-object interactions in virtual environments. Our framework is based on the concept of Smart Objects and provides agents with pre-programmed interaction information for the automatic generation of animations. The ability to generate such animations without human intervention is vital when constructing plausible, real-time agents. Unlike previous approaches, our model also contains information for directing the attention of agents when interacting with objects. Such information is useful for driving gaze behaviours, for example when grasping objects. Our framework supports both bottom-up (attention capture) and top-down, task driven, simulation of behavioural animation on a per-object basis. It also provides support for the management of the interactions of multiple agents with a single object. We show how objects are designed and provide a concrete example of using the modelling approach with a gaze controller in an animation system.	bottom-up proteomics;interaction information;real-time clock;simulation;smart objects;software agent;top-down and bottom-up design;virtual reality	Christopher E. Peters;Simon Dobbyn;Brian MacNamee;Carol O'Sullivan	2003			computer vision;real-time computing;simulation;computer science;operating system;top-down and bottom-up design;multimedia	AI	-37.558408038524924	-37.152926592443265	157459
0b2e3c7df31106bdc9cbff753f49564f50ef7a20	grenade: a grid enabled desktop environment	printing;silicon;grid enabled desktop environment;isolation technology;human computer interaction;user friendly interface;grid technology;printers;user interface;resource allocation;prototypes;grenade prototype;resource allocation grid computing graphical user interfaces human computer interaction;graphical user interfaces;prototypes printers printing graphical user interfaces silicon graphics problem solving isolation technology grid computing utility programs;utility programs;grid computing;graphics;grenade prototype grid enabled desktop environment grid technology user friendly interface;problem solving	Making grid technology available to a wider community requires moving away from the complex command line tools commonly in use today towards a more transparent and user-friendly interface. We seek to address this challenge by presenting the notion of a Grid Enabled Desktop Environment (GRENADE), seamlessly extending the familiar desktop user-interface paradigm to interactions with grid resources. This paper describes our initial implementation.	command-line interface;desktop computer;interaction;programming paradigm;usability;user interface	James Marsh;Steve Pettifer;Daniel Hanlon;Stephen Pickles;Jon MacLaren;Martyn Foster	2004	13th IEEE International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises	10.1109/ENABL.2004.33	simulation;human–computer interaction;computer science;operating system;database;virtual desktop;world wide web;grid computing;computer graphics (images)	HPC	-43.08084085463568	-30.187690558549296	157582
bc3f283ac823a20eedf49dc9e5825f1e7440ff59	elements of a three-dimensional graphical user interface	three-dimensional graphical user interface;graphic user interface	The graphical user interface (GUI) is now firmly established as the preferred user interface for end users in most situations. Just as decreasing hardware prices and increasing hardware capabilities made two-and-a-hall dimensional (2!D) GUIs affordable in the early eighties and widespread in the ninetees, we believe declining hardware prices and increasing hardware capabilities will make three-dimensional (3D) GUIs possible and affordable in the near future. Three-dimensional GUIs raise many issues of design, metaphor and usability. In this paper we discuss elements of a prototype 3D GUI we are developing.	graphical user interface;interface metaphor;prototype;usability	Geoff Leach;Ghassan al-Qaimari;Mark Grieve;Noel Jinks;Cameron McKay	1997			user interface design;look and feel;user;10-foot user interface;clickable;shell;magic pushbutton;computer science;z-order;operating system;console application;graphical user interface;skin;natural user interface;user interface;graphical user interface testing;multiple document interface	HCI	-43.12404167568467	-30.339117885558508	157774
fefedeccb68d609c68213455ce89a81b62809fff	hypermedia-based structured modeling (abstract)	structural model		hypermedia	Yongming Tang;Hua Hua	1996	SIGWEB Newsletter	10.1145/231738.232585	simulation;human–computer interaction;computer science;multimedia	Logic	-41.83583859706595	-28.94764829817199	158109
790d4ec886631f5a9618c380f0aa90c2d0abbe8c	the song remains the same: lossless conversion and streaming of midi to rdf and back		In this demo, we explore the potential of RDF as a representation format for digital music. Digital music is broadly used today in many professional music production environments. For decades, MIDI (Musical Instrument Digital Interface) has been the standard for digital music exchange between musicians and devices, albeit not in a Web friendly way. We show the potential of expressing digital music as Linked Data, using our midi2rdf suite of tools to convert and stream digital music in MIDI format to RDF. The conversion allows for lossless round tripping: we can reconstruct a MIDI file identical to the original using its RDF representation. The streaming uses an existing, novel generative audio matching algorithm that we use to broadcast, with very low latency, RDF triples of MIDI events coming from arbitrary analog instruments.	algorithm;linked data;lossless compression;midi;resource description framework	Albert Meroño-Peñuela;Rinke Hoekstra	2016		10.1007/978-3-319-47602-5_38	speech recognition;computer science;operating system;database;multimedia;world wide web	Web+IR	-44.69584025991925	-32.05033521106911	158388
c3bf89c7154a97b4d6119e7c7bc149269f05a9c8	customizable physical interfaces for interacting with conventional applications	dynamic document layout;repositioning annotations;annotation;physical interface;freeform digital ink	When using today's productivity applications, people rely heavily on graphical controls (GUI widgets) as the way to invoke application functions and to obtain feedback. Yet we all know that certain controls can be difficult or tedious to find and use. As an alternative, a customizable physical interface lets an end-user easily bind a modest number of physical controls to similar graphical counterparts. The user can then use the physical control to invoke the corresponding graphical control's function, or to display its graphical state in a physical form. To show how customizable physical interfaces work, we present examples that illustrate how our combined phidgets® and widget tap packages are used to link existing application widgets to physical controls. While promising, our implementation prompts a number of issues relevant to others pursuing interface customization.	graphical user interface;interaction;phidget;widget (gui)	Saul Greenberg;Michael Boyle	2002		10.1145/571985.571991	human–computer interaction;computer science;operating system;multimedia;programming language;world wide web;java annotation	HCI	-42.380318654591505	-30.730705759424637	158516
2692dca30487098068f821c15e6b64d6a0970436	the dynamic screen - beyond the limits of traditional graphic user interfaces	high resolution;software systems;graphic user interface;user interface	 IntroductionThe small screen area is one essential limit for the amount of displayable informationand for the amount of user receptable information in computer assistedvisualisation and animation of complex information. During the last few yearsthis limit has been shifted with the advent of high resolution graphic displays andwith the introduction of the window oriented user interfaces. But it is reachedagain by actual or even more by future complex software systems like hypertext or... 	computer graphics;user interface	Peter Lüders;Rolf Ernst	1994			shell (computing);user interface design;computer hardware;10-foot user interface;natural user interface;human–computer interaction;look and feel;graphical user interface testing;user interface;computer science;interface metaphor	Graphics	-43.10973917147438	-30.2598960818293	158583
69e10795fdff4229967601be0b2f53bf5c052bf4	three-dimensional computation visualization	program diagnostics;program visualization;software understanding;3d computer graphics;computer graphics;algorithm information;visual programming computer graphics program diagnostics software tools;information visualization;three dimensional;visual programming;virtual environment 3d graphics animation 3d imagery 3d computation visualizations toolkit;software tools;technical report;virtual environment;data visualization animation biology computing computer graphics three dimensional displays usability educational institutions virtual environment packaging focusing	Systems supporting the visualization and animation of algorithms, programs, and computations have focused primarily on two-dimensional graphics to date. In this paper we identify the benefits and the drawbacks of using three-dimensional graphics in these types of systems, and we describe how 3 0 imagery can be used for visualizing computations in interesting new ways. We also present examples of 3 0 computation visualizations created with a new toolkit that we have developed. m e toolkit has been extended to run in a virtual environment and we describe our early interactions with it.	3d computer graphics;algorithm;computation;interaction;virtual reality	John T. Stasko;Joseph F. Wehrli	1993		10.1109/VL.1993.269585	computational science;scientific visualization;information visualization;visualization;computer facial animation;human–computer interaction;computer science;real-time computer graphics;non-photorealistic rendering;computer animation;graphics software;computer graphics;data visualization;3d computer graphics;computer graphics (images)	Visualization	-34.18600736870726	-30.64601367811902	158642
2c38870cd8667f6d670a8085c42c3dc1241a0151	interactive computer graphics for the generation of phase plane portraits	computer graphic	Abstract   This paper describes the use of computer graphics facilities for generating phase plane portraits by truly interactive means. The final portrait produced is completely governed by the user who chooses the starting points for any particular future trajectories using only the graphical information currently available to him on the screen.	computer graphics	Graham F. Raggett	1979	Computers & Graphics	10.1016/0097-8493(79)90015-3	human–computer interaction;computer science;real-time computer graphics;multimedia;computer graphics (images)	Graphics	-41.70412904403206	-35.19073864055968	158847
d030c56308f572563c44c73fd033288430985ff5	enabling closed-source applications for virtual reality via opengl intercept-based techniques	matlab geometry mathematical model tracking cameras rendering computer graphics;virtual reality computer animation helmet mounted displays mathematics computing middleware software packages;geometry;mathematical model;matlab closed source application virtual reality vr opengl intercept based technique software package middleware motionbuilder motion capture animation oculus rift head mounted display hmd;rendering computer graphics;matlab virtual reality opengl motionbuilder;matlab;cameras;tracking	Everyday, people use numerous high-quality commercial software packages on desktop systems. Many times, these software packages are not able to access specialized virtual reality (VR) display and input devices, which can enhance interaction and visualization. To address this limitation, we have been using the well-known OpenGL intercept concept to insert middleware at runtime between the application and the graphics card. In this paper, we motivate the use of OpenGL intercept techniques and present three intercept-based techniques that enable closed-source applications to be used with VR systems. To demonstrate the usefulness of these intercept-based techniques, we describe two case studies. In the first case study, we enabled MotionBuilder, a commercial motion capture and animation software, to work with the Oculus Rift, a consumer-level head-mounted display (HMD). In the second case study, we enabled MATLAB, a commercial mathematics and simulation software, to run in the Duke immersive Virtual Environment (DiVE), six-sided CAVE-like system. In both cases, display and interaction are successfully handled by intercept-based techniques.	cave automatic virtual environment;commercial software;computer animation;desktop computer;distortion;graphics;head-mounted display;input device;interaction;matlab;middleware;motion capture;motionbuilder;oculus rift;opengl;run time (program lifecycle phase);shader;simulation software;video card;virtual reality;visual intercept;whole earth 'lectronic link	David J. Zielinski;Ryan P. McMahan;Solaiman Shokur;Edgard Morya;Regis Kopper	2014	2014 IEEE 7th Workshop on Software Engineering and Architectures for Realtime Interactive Systems (SEARIS)	10.1109/SEARIS.2014.7152802	simulation;computer hardware;computer science;computer graphics (images)	Visualization	-41.36814214048497	-34.579073531083715	158850
470bbe5add37f4d8c251650df127d86116948359	classification and overview of research in real-time imaging	image communication;virtual reality;medical image;remote sensing;real time imaging	e Abstract. Real-time imaging has application in areas such as multimedia, virtual reality, medical imaging, and remote sensing and control. Recently, the imaging community has witnessed a tremendous growth in research and new ideas in these areas. To lend structure to this growth, we outline a classification scheme and provide an overview of current research in real-time imaging. For convenience, we have categorized references by research area and application. © 1996 SPIE and IS&T.	categorization;comparison and contrast of classification schemes in linguistics and metadata;medical imaging;real-time clock;real-time locating system;real-time transcription;virtual reality	Purnendu Sinha;Sergey Gorinsky;Phillip A. Laplante;Alexander D. Stoyen;Thomas J. Marlowe	1996	J. Electronic Imaging	10.1117/12.245842	computer vision;dynamic imaging;computer science;imaging science;virtual reality;multimedia;computer graphics (images)	Visualization	-35.50439896834724	-29.96035138613491	158960
a35c8a070f3006cf58b855d5c63026868cab7781	multimodal split view tabletop interaction over existing applications	speech based user interfaces gesture recognition groupware;tabletop interaction;groupware;large digital workspaces;true groupware;multimodal gesture interaction capability multimodal split view tabletop interaction digital tables operating systems true groupware large digital workspaces multimodal speech interaction capability;speech based user interfaces;collaborative work collaborative software application software mice computer displays operating systems speech enhancement conferences laboratories couplings;operating system;multimodal gesture interaction capability;digital tables;multimodal split view tabletop interaction;computer science;gesture recognition;multimodal speech interaction capability;operating systems	While digital tables can be used with existing applications, they are typically limited by the one user per computer assumption of current operating systems. In this paper, we explore multimodal split view interaction - a tabletop whose surface is split into two adjacent projected views - that leverages how people can interact with three types of existing applications in this setting. Independent applications let people see and work on separate systems. Shared screens let people see a twinned view of a single user application. True groupware lets people work in parallel over large digital workspaces. Atop these, we add multimodal speech and gesture interaction capability to enhance interpersonal awareness during loosely coupled work.	multimodal interaction	Edward Tse;Chia Shen;John Barnwell;Sam Shipman;Darren Leigh;Saul Greenberg	2007		10.1109/TABLETOP.2007.21	human–computer interaction;computer science;multimedia;communication	HCI	-44.11301578951133	-37.77535585558095	159147
1a625a12f1bbb0fb0000d394692027579fd66529	using processing.org in an introductory computer graphics course			computer graphics	Jordi Linares-Pellicer;Jordi Santonja-Blanes;Pau Micó;David Cuesta-Frau	2009		10.2312/eged.20091014	computer graphics (images);processing;computer graphics;computer science	Visualization	-47.580870029321694	-29.344480196896242	159172
142e63be31e1645fba0730b61db81ce3476f450f	immersive remote grasping: realtime gripper control by a heterogenous robot control system	virtual reality;human robot interaction;tele operation	Current developments in the field of user interface (UI) technologies as well as robotic systems provide enormous potential to reshape the future of human-robot interaction (HRI) and collaboration. However, the design of reliable, intuitive and comfortable user interfaces is a challenging task. In this paper, we focus on one important aspect of such interfaces, i.e., teleoperation. We explain how to setup a heterogeneous, extendible and immersive system for controlling a distant robotic system via the network. Therefore, we exploit current technologies from the area of virtual reality (VR) and the Unity3D game engine in order to provide natural user interfaces for teleoperation. Regarding robot control, we use the well-known robot operating system (ROS) and apply its freely available modular components. The contribution of this work lies in the implementation of a flexible immersive grasping control system using a network layer (ROSbridge) between Unity3D and ROS for arbitary robotic hardware.	control system;extensibility;game engine;heterogeneous computing;human–robot interaction;natural user interface;robot operating system;robot control;robot end effector;unity;virtual reality;whole earth 'lectronic link	Dennis Krupke;Lasse Einig;Eike Langbehn;Jianwei Zhang;Frank Steinicke	2016		10.1145/2993369.2996345	embedded system;computer vision;simulation;computer science;artificial intelligence;virtual reality	Robotics	-43.30650898431105	-36.150161493843804	159312
8abe085c56005695cf642145104cc6fd008cb0f8	multimodal interfaces - a generic design approach	gaze;developpement logiciel;interfaz multimodal;sistema interactivo;interfase usuario;telematics;entrada salida;human computer interaction;universal access;multimodal interface;aplicacion medical;design and development;componente logicial;user interface;salud publica;gesture;telematique;best practice;composant logiciel;programmation generique;information access;mirada;context of use;multimodal user interface;systeme conversationnel;input output;regard;societe information;interactive system;telematica;desarrollo logicial;sensibilidad tactil;software development;software component;acces information;sociedad informacion;information society;sante publique;interactive environment;interface utilisateur;user interface design;acceso informacion;medical application;multimodal systems;information system;reusable component;programacion generica;geste;sensibilite tactile;systeme information;tactile sensitivity;public health;interface multimodale;generic programming;gesto;entree sortie;application medicale;design methodology;sistema informacion	Integrating new input-output modalities, such as speech, gaze, gestures, haptics, etc., in user interfaces is currently considered as a significant potential contribution to implementing the concept of Universal Access (UA) in the Information Society; see (Oviatt, 2003), for instance. UA in this context means providing everybody, including handicapped users, with easy human-computer interaction in any context of use, especially mobility. However, the cost of developing an appropriate specific multimodal user interface for each interactive software is prohibitive. A generic design methodology and generic reusable components are needed to master the complexity of the design and development of interfaces that allow flexible use of alternative modalities and meaningful combinations of modalities, according to the constraints in the interaction environment or the user's motor and perceptual capabilities. We present a design approach meant to facilitate the development of generic multimodal user interfaces, and based on best practice in software and user interface design and architecture. At present, specific on the shelf components are available that can process and interpret data from a wide range of input devices reliably. However, these components are monomodal in the sense that they are dedicated to a specific medium and modality. There is not yet, outside research laboratory prototypes, any software platform capable of interpreting multimodal input data. Symmetrically, software on the market is available for the generation of monomodal output messages conveyed through various media, whereas the generation of multimodal presentations is not yet supported. In the next section, we present a design approach that makes it possible to: − Interpret users' multimodal commands or manipulations/actions using partial monomodal interpretations, each interpretation being elaborated by a dedicated component which processes the specific input data stream transmitted through one of the available media. This treatment may be viewed as a fusion process of events or data. − Match these global interpretations with appropriate functions in the current application software, or translate them into appropriate commands (i.e., execution calls of the appropriate functions in the kernel of the considered software). − As regards system multimodal outputs, break up the information content of system messages into chunks, assign the resulting data chunks to appropriate modalities, and input each of them into the relevant monomodal generation/presentation component. This treatment is often viewed as a data fission process in contrast with multimodal input fusion. In most current applications, stereotyped system messages only need to be implemented in order to achieve efficient …	best practice;component-based software engineering;haptic technology;human–computer interaction;input device;iterative and incremental development;metamodeling;modality (human–computer interaction);multimodal interaction;norm (social);plug-in (computing);requirement;self-information;singlet fission;software architecture;software design;television;user agent;user interface design	Noëlle Carbonell	2005		10.1007/11424628_17	user interface design;input/output;user;user experience design;simulation;universal design;public health;design methods;human–computer interaction;computer science;component-based software engineering;software development;multimedia;telematics;programming language;user interface;generic programming;gesture;information system;best practice	HCI	-37.08860039354892	-26.624455970131518	159690
11ce58f926d5ec5ac198df3f82c1956ad74f78e3	media features for display, print, and fax		"""Status of this Memo This document specifies an Internet standards track protocol for the Internet community, and requests discussion and suggestions for improvements. Please refer to the current edition of the """"Internet Official Protocol Standards"""" (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Abstract This specification defines some common media features for describing image resolution, size, color, and image representation methods that are common to web browsing, printing, and facsimile applications. These features are registered for use within the framework of [REG]."""	fax;image resolution;internet;printing;std bus	Larry Masinter;Dan Wing;Andrew H. Mutz;Koen Holtman	1999	RFC	10.17487/RFC2534	computer vision;computer science;multimedia;computer graphics (images)	Web+IR	-46.40307159370942	-25.951574389104778	159817
f51ee7745a0b1272b846ce78a2ea9d55ca397da0	a new music database describing deviation information of performance expressions	music perception	We introduce the CrestMuse Performance Expression Database (CrestMusePEDB), a music database that describes music performance expression and is available for academic research. While music databases are being provided as MIR technologies continue to progress, few databases deal with performance expression. We constructed a music expression database, CrestMusePEDB. It may be utilized in the research fields of music informatics, music perception and cognition, and musicology. It will contain music expression information on virtuosis’ expressive performances, including those of 3 to 10 players at a time, on about 100 pieces of classical Western music. The latest version of the database, CrestMusePEDB Ver. 2.0, is available. The paper gives an overview of CrestMusePEDB.	cognition;database;informatics;list of online music databases;performance;ver (command)	Mitsuyo Hashida;Toshie Matsui;Haruhiro Katayose	2008			programming;speech recognition;music and artificial intelligence;computer science;multimedia;pop music automation	DB	-47.766215979083604	-26.041738722533246	160132
32438ce64b4fe104ce0a092d8caec855ca955f6e	virtual exploration of underwater archaeological sites: visualization and interaction in mixed reality environments	categories and subject descriptors according to acm ccs j 2 computer applications physical sciences and engineering;mixed reality	This paper describes the ongoing developments in Photogrammetry and Mixed Reality for the Venus European project (Virtual ExploratioN of Underwater Sites, http://www.venus-project.eu). The main goal of the project is to provide archaeologists and the general public with virtual and augmented reality tools for exploring and studying deep underwater archaeological sites out of reach of divers. These sites have to be reconstructed in terms of environment (seabed) and content (artifacts) by performing bathymetric and photogrammetric surveys on the real site and matching points between geolocalized pictures. The base idea behind using Mixed Reality techniques is to offer archaeologists and general public new insights on the reconstructed archaeological sites allowing archaeologists to study directly from within the virtual site and allowing the general public to immersively explore a realistic reconstruction of the sites. Both activities are based on the same VR engine but drastically differ in the way they present information. General public activities emphasize the visually and auditory realistic aspect of the reconstruction while archaeologists activities emphasize functional aspects focused on the cargo study rather than realism which leads to the development of two parallel VR demonstrators. This paper will focus on several key points developed for the reconstruction process as well as both VR demonstrators (archaeological and general public) issues. The first developed key point concerns the densification of seabed points obtained through photogrammetry in order to obtain high quality terrain reproduction. The second point concerns the development of the Virtual and Augmented Reality (VR/AR) demonstrators for archaeologists designed to exploit the results of the photogrammetric reconstruction. And the third point concerns the development of the VR demonstrator for general public aimed at creating awareness of both the artifacts that were found and of the process with which they were discovered by recreating the dive process from ship to seabed.	augmented reality;bathymetry;display resolution;mixed reality;photogrammetry	Mahmoud Haydar;Madjid Maidi;David Roussel;Malik Mallem;Pierre Drap;Kim Bale;Paul Chapman	2008		10.2312/VAST/VAST08/141-148	simulation;computer science;mixed reality;multimedia;computer graphics (images)	HCI	-36.12833584202111	-32.855417552493314	160305
1a47c665641ae745c208e73a795cb4c05685c311	the brass project, from physical models to virtual musical instruments: playability issues	automatic control;busqueda informacion;developpement logiciel;modelizacion;securite;information retrieval;real time;musica;commande automatique;acoustique musicale;teclado;musical instruments;musical acoustics;modelisation;musique;musical instrument;instrumento musical;acustica musical;recherche information;desarrollo logicial;temps reel;software development;safety;instrument musique;physical modelling;tiempo real;keyboard;control automatico;physical model;seguridad;modeling;music;audio acoustics;clavier;acoustique audio	The Brass project aims to deliver software virtual musical instruments (trumpet, trombone, tenor saxophone) based on physical modelling. This requires to work on some aspects of the playability of the models so that they can be played in real time through a simple keyboard : better control of the attacks, automatic tuning, humanization.	international computer music conference;lester the unlikely;mpeg-7;personal computer;variable frame rate	Christophe Vergez;Patrice Tisserand	2005		10.1007/11751069_2	visual arts;simulation;speech recognition;acoustics;physical model;computer science;software development;musical acoustics;automatic control;music	SE	-46.74329521173866	-31.66980231637587	160486
6a9dab51f0f274473090ae27af046e808b809413	distributing web components in a display ecosystem using proxywork	proxywork;web;distributed user interfaces	In order to carry out this task, Web browsers connected to the Proxywork proxy receive a modified version of Web pages they have requested to the proxy. This modification attaches a menu on each UI component to display, hide, copy, or distribute the UI component to the rest of the browsers that are connected to the Proxywork proxy. Therefore, the Proxywork proxy is also in charge of orchestrating how UI components are displayed on devices running on different platforms.		Pedro González Villanueva;Ricardo Tesoriero;José A. Gallud	2013			user interface design;web service;static web page;web development;web modeling;data web;web analytics;web mapping;web-based simulation;web design;human–computer interaction;computer science;web api;web navigation;web page;database;client-side scripting;web 2.0;world wide web;web server;mashup	Security	-42.673573620642095	-28.179776812960345	160800
2118823d110190864d1ff9d129d2ad21ed3aa436	navigating through sparse views	image features;user interface;concurrent programming;computer supported cooperative work;texture mapping;virtual reality;scene graph;3d model;distributed virtual environment;distributed graphics	This paper presents an image-based walkthrough technique where reference images are sparsely sampled along a path. The technique relies on a simple user interface for rapid modeling. Simple meshes are drawn to model and represent the underlying scene in each of the reference images. The meshes, consisting of only few polygons for each image are then registered by drawing a single line on each image, called model registration line, to form an aligned 3D model. To synthesize a novel view, two nearby reference images are mapped back onto their models by projective texture-mapping. Since the simple meshes are a crude approximation to the real model in the scene, image feature lines are drawn and used as aligning anchors to further register and blend two views together and form a final novel view. The simplicity of the technique yields rapid, “home-made” image-based walkthroughs. We have produced walkthroughs from a set of photographs to show the effectiveness of the technique.	3d modeling;approximation;cognitive walkthrough;feature (computer vision);html element;projective texture mapping;software walkthrough;sparse matrix;user interface	Shachar Fleishman;Baoquan Chen;Arie E. Kaufman;Daniel Cohen-Or	1999		10.1145/323663.323677	texture mapping;computer vision;human–computer interaction;computer science;artificial intelligence;operating system;computer-supported cooperative work;virtual reality;multimedia;scene graph;user interface;feature;computer graphics (images)	Graphics	-36.10029826395514	-34.14252280381577	160862
92b8b4709d522c1e1034237791199a93ec9e39f1	graphic visualization of probabilistic traffic/trajectory predictions in mobile applications. a first prototype and evaluations for general aviation purposes		The present work describes the interactive prototype and the preliminary evaluation results of a tool dedicated to the light General Aviation pilot’s community. The tool’s interface has been developed through an Android tablet application and aims at supporting the pilots in the task of staying “well-clear” from the surrounding traffic by presenting them the long-term prediction of the flights. The initial results and the approach of a heuristic evaluation conducted with five experts coming from the fields of user-experience, aviation and automotive are discussed along with the improvements in the design of the user-interface focusing on the trajectory depictions.	prototype	Giuseppe Frau;Francesca De Crescenzio;Damiano Taurino	2015		10.1007/978-3-319-21006-3_16	computer vision;simulation;human–computer interaction	Mobile	-34.21911692877322	-32.5187072335141	160953
258a9759f07432eee336fdfb638827283d719a48	3d timeline: reverse engineering of a part-based provenance from consecutive 3d models	ucl;discovery;theses;conference proceedings;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;ucl research	We present a novel tool for reverse engineering of modeling histories from consecutive 3D files based on a timeline abstraction. Although a timeline interface is commonly used in 3D modeling packages for animations, it has not been used on geometry manipulation before. Unlike previous visualization methods that require instrumentation of editing software, our approach does not rely on pre-recorded editing instructions. Instead, each stand-alone 3D file is treated as a keyframe of a construction flow from which the editing provenance is reverse engineered. We evaluate this tool on six complex 3D sequences created in a variety of modeling tools by different professional artists and conclude that it provides useful means of visualizing and understanding the editing history. A comparative user study suggests the tool is well suited for this purpose.	3d modeling;autoregressive integrated moving average;case preservation;computer graphics;computer-aided design;expectation propagation;file spanning;heat map;jules;key frame;legacy system;reverse engineering;systems architecture;timeline;usability testing;user interface	Jozef Dobos;Niloy Jyoti Mitra;Anthony Steed	2014	Comput. Graph. Forum	10.1111/cgf.12311	computer science;operating system;multimedia;world wide web;computer graphics (images)	HCI	-40.696502404453724	-31.378931330874416	160957
f6c396220784a1e8d051c760b9fc26685e791595	take three snapshots - a tool for fast freehand acquisition of 3d objects	user interface;interaction techniques;visualisation techniques;3d model;object acquisition;object reconstruction;virtual environment;point of view;usability;user interfaces	We introduce a tool which allows an untrained user to take three images of an object freehand with a simple consumer camera. From these images a 3d model of the visible parts of the object is reconstructed within seconds and visualized realistically. From a research point of view we propose solutions for three weaknesses of the state-of-the-art reconstruction pipeline. These contributions allow for a more robust and a considerably faster reconstruction process than before, which can be used, e.g., to create new types of interfaces or to assist in creating virtual environments.	3d modeling;adobe freehand;virtual reality	Gabriele Peters;Klaus Häming	2009		10.1007/978-3-642-03658-3_91	computer vision;computer science;operating system;multimedia;user interface;computer graphics (images)	Vision	-38.85657679019346	-34.75486162631761	160973
a69fd2ad66791ad9fa8722a3b2916092d0f37967	interactive example-based urban layout synthesis	structure-based synthesis;complete urban layout;example-based;real-world urban area;interactive example-based urban layout;image-based synthesis;procedural modeling;urban layout;synthesis algorithm;new urban layout;content-aware image editing;texture and image synthesis;example urban layout fragment;complex layout;example fragment	We present an interactive system for synthesizing urban layouts by example. Our method simultaneously performs both a structure-based synthesis and an image-based synthesis to generate a complete urban layout with a plausible street network and with aerial-view imagery. Our approach uses the structure and image data of real-world urban areas and a synthesis algorithm to provide several high-level operations to easily and interactively generate complex layouts by example. The user can create new urban layouts by a sequence of operations such as join, expand, and blend without being concerned about low-level structural details. Further, the ability to blend example urban layout fragments provides a powerful way to generate new synthetic content. We demonstrate our system by creating urban layouts using example fragments from several real-world cities, each ranging from hundreds to thousands of city blocks and parcels.	aerial photography;algorithm;high- and low-level;interactivity;synthetic intelligence	Daniel G. Aliaga;Carlos A. Vanegas;Bedrich Benes	2008	ACM Trans. Graph.	10.1145/1457515.1409113	computer vision;simulation;computer science;procedural modeling;computer graphics (images)	Graphics	-37.6617944771629	-32.77237158320487	161053
529ee9708de323699ddd9a54da6ac05bb4de44dd	digital preservation of interactive multimedia performances.	interactive multimedia;digital preservation	Interactive multimedia technologies and digital media are now usual tools and resources for contemporary performing arts. From the perspective of recreating a performance and/or later analysis, these technologies and media have added much complexity to preservation issues. Preserving an interactive multimedia performance involves keeping human interactions with multimedia systems, the multimedia contents generated (usually music and graphical animation) and the artistic, cultural and technical knowledge embedded in the performance material so that a recreation of the performance is possible in the future. This paper introduces an approach to such a challenging preservation by using 3D motion data for coding human interactions and ontologies for knowledge preservation together with the preservation framework developed in the CASPAR EC IST project, which is based on the standardised Open Archival Information System (OAIS) reference model. The work reported is part of the contemporary arts testbed of CASPAR.		Kia Ng;Tran Vu Pham;Bee Ong;Jérôme Barthélemy;Alain Bonardi;David L. Giaretta	2007	ERCIM News		computer science;multimedia;interactive media	HCI	-47.057577540131305	-33.426451546111466	161111
804b0dce17a4a7275ddb71add3a54a976f1410f1	devicecycle: rapid and reusable prototyping of gestural interfaces, applied to audio browsing by similarity	gestural interface	This paper presents the development of rapid and reusable gestural interface prototypes for navigation by similarity in an audio database and for sound manipulation, using the AudioCycle application. For this purpose, we propose and follow guidelines for rapid prototyping that we apply using the PureData visual programming environment. We have mainly developed three prototypes of manual control: one combining a 3D mouse and a jog wheel, a second featuring a force-feedback 3D mouse, and a third taking advantage of the multitouch trackpad. We discuss benefits and shortcomings we experienced while prototyping using this approach.	browsing;gesture recognition;haptic technology;integrated development environment;jog dial;multi-touch;prototype;pure data;rapid prototyping;touchpad;visual programming language	Christian Frisson;Benoit M. Macq;Stéphane Dupont;Xavier Siebert;Damien Tardieu;Thierry Dutoit	2010			simulation;human–computer interaction;computer science;multimedia	HCI	-45.39973277246069	-37.28724316568804	161174
40f25b91d0dfe3f722211893667d1965f5f4fb6b	about 23 million documents match your query	information access;searching;relevance feedback	A simple search of a large information space, such as the World Wide Web, often leaves the user to scan millions of hits. Relevance feedback is an information retrieval technique that can be used to make a search query more specific, so that its results are more manageable and useful. However, users tend not to take advantage of relevance feedback when systems provide it. I believe that this is because the process is badly represented at the search interface, and my thesis work to date has been aimed at facilitating relevance feedback by providing a visual representation of the user’s search context. An initial Java interface has been implemented, which will evolve in future to become a user-centred information workspace, supporting the search of a multimedia document collection.	archive;information retrieval;interface (java);java;relevance feedback;workspace;world wide web	Kerry Rodden	1998		10.1145/286498.286531	query expansion;ranking;computer science;data mining;world wide web;information retrieval	Web+IR	-42.98192042204715	-25.34925263344602	161356
a018480eb5c28b0049b588f29b55bc885820f311	a prototype of a chat system using message driven and interactive actions character	ojo;gaze;animacion por computador;mimica;eye;embodiment;affichage;psychologie sociale;ophthalmology;visualizacion;facies;ingenierie connaissances;mimique;gesture;conversacion;intelligence artificielle;mirada;regard;system evaluation;display;envoi message;psicologia social;conversation;message passing;artificial intelligence;social psychology;encarnacion;inteligencia artificial;facial expression;incarnation;computer animation;face to face;geste;oeil;eye gaze;oftalmologia;gesto;ophtalmologie;animation par ordinateur;knowledge engineering	In this article, we present a chat system in which embodied characters behave as agents of users and automatically act on messages of the users and the other character's action. We display and exchange nonverbal expressions including gestures, eye-gazes, noddings, and facial expressions in daily conversation. Nonverbal expressions convey various kinds of information that is essential to make our face-to-face communication successful. In the previous work on social psychology, it is known that there are interdependences among nonverbal expressions between those from different persons in conversation with each other. We apply this knowledge to the chat between embodied characters, so that 3D characters interactively act by user's messages. The system evaluation results demonstrated higher validity than the system that the user explicitly indicates the character's actions.	prototype	Junko Itou;Kenji Hoshio;Jun Munemori	2006		10.1007/11893011_27	computer vision;message passing;facies;eye tracking;computer science;artificial intelligence;knowledge engineering;computer animation;multimedia;programming language;gesture;facial expression	Graphics	-35.458473084411445	-26.428213862079048	161386
214e0a7c4267e4b9e4ee9deb00c04f557a9a5480	content-based access to multimedia educational libraries	servers;materials	The availability of huge amounts of digital multimedia information introduces new challenges for the efficient representation of the data and the access to it. This document describes a system designed to allow such an access for a concrete type of information (educational video material) with a defined architecture (a client-server model over standard IP networks).	client–server model;internet protocol suite;library (computing);server (computing)	Paulo Villegas;Elena Nistal;Isidro Aguado;Miguel Roser;Narciso N. García;Guillermo Cisneros	2000	2000 10th European Signal Processing Conference		computer science;theoretical computer science;multimedia;world wide web	DB	-46.098691430821304	-25.496780679767152	161567
5d2ee9fb0a19d52579c1d452118f03028045880d	method of user interface design based on semantic approach		Eliminate the reason and achieve the preservation of the meaning of user actions in the domain, and fully meet the user requirements in the interface of the information system is possible if to create a methodological framework (a general methodological description) of user interfaces development that will be a common sign form of models organization and will be understandable to all participants of the interface development process, including the user. It is proposed to represent the user actions in the domain in the form of mechanisms of action and on the bases of them to build a process of interface design. The paper presents the method of user interface design based on semantic approach and demonstrates a short example of its usage.	user interface design	Svetlana A. Belikova;Yury I. Rogozov;Alexandr S. Sviridov	2018		10.1007/978-3-319-91186-1_32	user interface design;interface design;information system;human–computer interaction;user requirements document;user interface;computer science	SE	-40.113788681913746	-27.326242646820507	162214
d410bb647ae91f5b8572cea5052a30f78abf5835	perl in a nutshell - covers perl 5.8: a desktop quick reference (2. ed.)			desktop computer;perl	Nathan Patwardhan;Ellen Siever;Stephen Spainhour	2002			computer science;programming language;computer graphics (images)	NLP	-45.075888712393095	-28.250580662974638	162444
2cb9f8685912a82b5e53bfb7fb9a41af1a77dd3e	content based automatic zooming: viewing documents on small displays	document image analysis;small displays;document viewing;mobile phone;content analysis;image analysis;automatic zooming	We present an automatic zooming technique that leverages content analysis for viewing a document page on a small display such as a mobile phone or PDA. The page can come from a scanned document (bitmap image) or an electronic document (text and graphics data plus metadata). The page with text and graphics is segmented into regions. For each region, a scale-distortion function is constructed based on image analysis of the signal distortion that occurs at different scales. During interactive viewing of the document, as the user navigates by moving the viewport around the page, the zoom factor is automatically adjusted by optimizing the scale-distortion functions of the regions visible in the viewport.	bitmap;distortion;graphics;image analysis;mobile phone;personal digital assistant;viewport	Patrick Chiu;Koichi Fujii;Qiong Liu	2008		10.1145/1459359.1459495	computer vision;image analysis;content analysis;computer science;multimedia;information retrieval	HCI	-34.14803763974274	-35.08753575011025	163307
bd1a833852aa40fda375e8bc7e0037310255386f	"""optimisation par """" hommilière """" de chemins pédagogiques pour un logiciel d'e-learning"""	invertebrata;optimisation;swarm intelligence;ant colony optimisation;teleenseignement;intelligence en essaim;camino grafo;optimizacion;arthropoda;insecto social;graph path;internet;metafora;aculeata;insecta;e learning;chemin graphe;man hill;optimization;teleensenanza;hymenoptera;enseignement;insecte social;social insect;remote teaching;formicoidea;metaphor;inteligencia de enjambre;teaching;metaphore;ensenanza	This paper describes experiments aimed at adapting Ant Colo ny Optimisation (ACO) techniques to an e-learning environment, thanks to the fact that the available online material can be organised in a graph by means of hyperlinks between edu cational topics. The idea is to find paths in the graph making it easier for students to im prove. ACO is based on an ant-hill metaphor. In this case, however, the agents that mo ve on the graph are students who unconsciously leave pheromones in the environment. Tests s howed that humans did not behave as ants, meaning that the ACO paradigm had to be modified so tha t it could work with human agents. A new word has been coined to describe the new paradig m: “man-hill” optimization. MOTS-CLÉS :Hommilière, E-Learning, Optimisation par Colonie de Fourm is.	ant colony;colocation centre;experiment;hill climbing;humans;hyperlink;mathematical optimization;programming paradigm;protologism	Grégory Valigiani;Evelyne Lutton;Cyril Fonlupt;Pierre Collet	2007	Technique et Science Informatiques	10.3166/tsi.26.1245-1267	the internet;simulation;swarm intelligence;computer science;artificial intelligence	Web+IR	-36.49718286641681	-24.28144101702003	163552
2f11214aec8c30d05fff27db6a2da039843a4adb	modeling naturalistic argumentation in research literatures: representation and interaction design issues	argument structure;semantic annotation;intelligent user interface;user interface;digital library;domain modelling;conceptual graph;semantic web;electronic publishing;interaction design	This paper characterises key weaknesses in the ability of current digital libraries to support scholarly inquiry, and as a way to address these, proposes computational services grounded in semiformal models of the naturalistic argumentation commonly found in research literatures. It is argued that a design priority is to balance formal expressiveness with usability, making it critical to co-evolve the modelling scheme with appropriate user interfaces for argument construction and analysis. We specify the requirements for an argument modelling scheme for use by untrained researchers, describe the resulting ontology, contrasting it with other domain modelling and semantic web approaches, before discussing passive and intelligent user interfaces designed to support analysts in the construction, navigation and analysis of scholarly argument structures in a Web-based environment.	digital library;intelligent user interface;interaction design;library (computing);requirement;semantic web;usability	Simon Buckingham Shum;Victoria S. Uren;Gangmin Li;Bertrand Sereno;Clara Mancini	2007	Int. J. Intell. Syst.	10.1002/int.20188	conceptual graph;digital library;human–computer interaction;computer science;knowledge management;artificial intelligence;machine learning;interaction design;semantic web;electronic publishing;user interface;world wide web	HCI	-40.42088923056328	-26.377786645492815	163668
944cb939d9e35de87284d7bd5558bd0f95d9aa61	exploiting visualization and direct manipulation to make parallel tools more communicative	interfase usuario;user interface;computer graphics;direct manipulation;sistema informatico;relacion hombre maquina;man machine relation;computer system;systeme conversationnel;level of detail;interactive system;visual search;sistema conversacional;interface utilisateur;systeme informatique;relation homme machine;systeme parallele;parallel system;user interaction;grafico computadora;infographie;sistema paralelo	Parallel tools rely on graphical techniques to improve the quality of user interaction In this paper we explore how visualization and direct manipulation can be exploited in parallel tools in order to improve the naturalness with which the user interacts with a parallel tool Examples from recent tool research demonstrate that tool displays can be made more communicative and more intuitive to use Visualization methods can be used to organize complex performance data into lay ers and perspectives that exploit the user s visual searching capabilities Direct manipulation techniques allow the user to focus on key elements and then transition smoothly to further levels of detail or interrelated aspects of program behavior Heuristics derived from studies with paral lel users are proposed for when and how the techniques can be applied more e ectively	direct manipulation interface;graphical user interface;heuristic;smoothing	Cherri M. Pancake	1998		10.1007/BFb0095363	simulation;visual search;human–computer interaction;computer science;artificial intelligence;operating system;level of detail;computer graphics;user interface;computer graphics (images)	HCI	-36.15131358052452	-28.51287155659558	163838
226ea4479fc64c2f0b702ca7b08291e68fc818f3	perceptual principles and computer graphics	bottom up;blobtree;real time;cognitive theory;computer graphic;metamorphosis;implicit surfaces;feature extraction;animation;change blindness;visual perception;retinal imaging;warping	Now that technology allows us to present photorealistic animations of scenically lit objects acting in realtime, the problem of computer graphics has changed from making displays recognisable, to ensuring that users notice what they are intended to see, without being distracted by irrelevant information. Worse than that, the use of veridical displays that are intended to be lifelike runs the risk of introducing unpredictable sources of information, that can lead users to infer all sorts of unwanted details. Traditional visual theory, based upon bottom-up models of feature extraction from the retinal image, cannot inform us about these aspects of perception. Broader based cognitive theories are required that integrate visual perception with attention, memory, emotion and inference. Theories such as Barnard’s Interacting Cognitive Subsystems enable phenomena such as change blindness and the craft principles of film editing to be interpreted within a common framework, supporting extrapolation to computer graphics.	cognitive science;computer graphics;extrapolation;feature extraction;interaction;relevance;theory	Jon May	2000	Comput. Graph. Forum	10.1111/1467-8659.00463	image warping;anime;computer vision;change blindness;visual perception;feature extraction;computer science;artificial intelligence;top-down and bottom-up design;metamorphosis;algorithm;computer graphics (images)	Graphics	-39.986190937084714	-35.83731468382051	163849
405cd7dd58080526992a3db1ed230e05a779cd07	turning function calls into animations		Animated transitions are an integral part of modern interaction frameworks. With the increasing number of animation scenarios, they have grown in range of animatable features. Yet not all transitions can be smoothed: programming systems limit the flexibility of frameworks for animating new things, and force them to expose low-level details to programmers. We present an ongoing work to provide system-wide animation of objects, by introducing a delay operator. This operator turns setter function calls into animations. It offers a coherent way to express animations across frameworks, and facilitates the animation of new properties.	coherence (physics);floor and ceiling functions;haptic technology;high- and low-level;microcontroller;programmer;prototype;smoothing;software framework	Thibault Raffaillac;Stéphane Huot;Stéphane Ducasse	2017		10.1145/3102113.3102134	operator (computer programming);user interface design;human–computer interaction;simulation;computer animation;multimedia;animation;computer science	HCI	-43.315161305208036	-34.26982151342127	163902
2c5e76e3836d74da2184061f31c2b932f5eaa842	immersed gaming in minecraft	head tracking;gestures;minecraft;position tracking	This demonstration will showcase mixed reality technologies that we developed for a series of public art performances in Vienna in October 2015 in a collaboration of performance artists and researchers. The focus of the demonstration is on natural interaction techniques that can be used intuitively to control an avatar in a virtual 3D world. We combine virtual reality devices with optical location tracking, hand gesture recognition and smart devices. Conference attendees will be able to walk around in a Minecraft world by physically moving in the real world and to perform actions on virtual world items using hand gestures. They can also test our initial system for shared avatar control, in which a user in the real world cooperates with a user in the virtual world. Finally, attendees will have the opportunity to give us feedback about their experience with our system.	gesture recognition;haptic technology;high-level programming language;immersion (virtual reality);interaction technique;minecraft;mixed reality;performance;scrolling;smart device;user experience;virtual reality;virtual world	Milan Loviska;Otto Krause;Herman Arnold Engelbrecht;Jason B. Nel;Gregor Schiele;Alwyn Burger;Stephan Schmeißer;Christopher Cichiwskyj;Lilian Calvet;Carsten Griwodz;Pål Halvorsen	2016		10.1145/2910017.2910632	simulation;multimedia;gesture;world wide web;computer graphics (images)	HCI	-44.323434114214926	-37.98947280829154	164060
3e53f6d077a8390c6d8eb551b2226b27cdba4fd5	integration of simultaneous searching and reference linking across bibliographic resources on the web	reference linking;searchbots;digital library;digital object identifier;openurl;university of illinois at urbana champaign;software design;proxy server;asynchronous simultaneous search	Libraries and information providers are actively developing customized portals and gateway software designed to integrate secondary information resources such as A & I services, online catalogs, and publishers full-text repositories. This paper reports on a project carried out at the Grainger Engineering Library at the University of Illinois at Urbana-Champaign to provide web-based asynchronous simultaneous searching of multiple secondary information resources and integrated reference linking between bibliographic resources.The project has tested two different approaches to simultaneous broadcast searching. One approach utilizes custom distributed searchbots and shared blackboard databases. The other approach uses event-driven asynchronous HTTP queries within a single web script.The reference linking implementation is built around the application of OpenURL and Digital Object Identifier (DOI) technologies and the CrossRef metadata database within a proxy server environment.	database;event-driven programming;hypertext transfer protocol;identifier;library;openurl;portals;proxy server;server (computing);web application	William H. Mischo;Thomas G. Habing;Timothy W. Cole	2002		10.1145/544220.544244	digital library;computer science;software design;database;world wide web;information retrieval	Web+IR	-46.24847447177531	-24.132675414853686	164106
09836f9a0e92423b89c87828e85fa2f68f72daa9	a spatial approach to organizing and locating digital libraries and their content	map;information retrieval;digital library;image;reference material;authoring system;graphical;geographical;spatial	Spatial Approach to Organizing and Locating Digital Libraries and Their Content Jason Orendo~ Charles Kacmar Department of Computer Science Florida State University 203 Love Building Ta&dtass&, Florida 32306-4019 {orendorfJcacmar] @cs.fsu.edu phone: (904) 6$4-9661 frtx: (904) 644-0058 Explosive growth of world-wi& web (WWW) sites combined with the lack of an overalf and consistent organizational structure is making it increasingly difficult for researchers and users to locate relevant materials. This paper proposes a spatial method of structuring digital libraries and their content in which users navigate geographicrdly to locate and access information. A prototype based on a spatial methodology was implemented to further study this organizational shucttrre. The system, SDLS, is a hypermedia-based digital library browser, authoring system, and document viewer in which users navigate using geographical (map) displays to locate and retrieve information. This method of access provides a natural means of information retrieval for geographically-based repositories and reference materials.	computer science;digital library;hypermedia;information retrieval;jason;library (computing);organizing (structure);prototype;www;world wide web	Jason Orendorf;Charles J. Kacmar	1996		10.1145/226931.226949	computer science;multimedia;world wide web;information retrieval	Web+IR	-40.496756128300426	-25.392211620353468	164187
a808e83bbcb6df84dbf9bc892a5fd1713111321d	the lime music editor: a diagram editor involving complex transformations	representation;interfase usuario;translating;wysiwyg editors;notacion;user interface;musica;editor;conception;diagramme;traduction;diagram;musique;wysiwyg;design and implementation;music notation;diseno;publisher;design;interface utilisateur;traduccion;diagram editing;editeur;user interfaces;music;notation;representacion;diagrama	Lime is a music editor, oriented to the production of printed music. A user manipulates the information content of the music notation, while viewing the physical appearance of the notation. To implement this, Lime must perform a complex translation from an abstract musical source representation to a graphical product. Techniques are discussed for providing automation without loss of customizability, for implementing WYSIWYG editing, for supporting multiple products (e.g. orchestral score and parts), and for providing good response time despite a slow translator. Similar design issues are encountered by editors for other types of diagrams, such as schematics, chemical formulas, and maps.	bespoke;diagram;map;printing;response time (technology);schematic;self-information;wysiwyg;lime	Dorothea Blostein;Lippold Haken	1994	Softw., Pract. Exper.	10.1002/spe.4380240304	human–computer interaction;computer science;engineering;operating system;multimedia;user interface	HCI	-39.65456086745035	-29.216511638537277	164315
a284703fc28a406cad5e5d3ec5f1b7e954101394	a two-view vr shooting theater system	human computer interaction;shooting game;two view projecting system;simulation gun;virtual world	In traditional shooting theater system, only one single scene image can be presented to all the players, and the interaction based on simulation guns is limited. In this paper, we describe a novel shooting theater system for freely moving players, which is equipped with two-view projecting system, individual surround-stereo earphone and user-customized simulation gun. To provide friendly user interaction, a new-designed strategy of simulation gun is introduced. In the end, a tennis game system is given to show the extensibility and practicability of our system.	extensibility;headphones;simulation	Hong-Da Yu;Hui-Yu Li;Wei-Si Sun;Wei Gai;Tingting Cui;Chu-Tian Wang;Dong-Dong Guan;Yi-Jun Yang;Chenglei Yang;Wei Zeng	2014		10.1145/2670473.2670497	simulation;human–computer interaction;computer science;multimedia;computer graphics (images)	Robotics	-42.96423966704938	-37.80916718993328	164316
c2ddb8298e16289bfe6bbd97fff91a67176a69c7	conceptual, collaborative building design through shared graphics	groupware;building;engineering graphics;collaboration buildings graphics collaborative work design engineering design automation data mining teamwork process design software tools;intelligent design assistants;interdisciplinary communication medium computer environment shared graphic modeling environment network based services architecture engineering construction team conceptual collaborative building design;architecture engineering construction;graphical model;architectural cad;building design;open systems;intelligent design assistants groupware building architectural cad engineering graphics modelling open systems	The Interdisciplinary Communication Medium computer environment integrates a shared graphic modeling environment with network-based services to accommodate many perspectives in an architecture/engineering/construction team.		Renate Fruchter	1996	IEEE Expert	10.1109/64.506752	human–computer interaction;computer science;environmental graphic design;architectural technology;building design;graphical model;building;open system	Visualization	-46.342569924751174	-27.255586641644935	164433
14cc15fb72845dbcd15816fe5ec55b4f61f5bb57	converting 3d furniture models to fabricatable parts and connectors	grammar;fabrication;3d modeling;procedural modeling;automatic generation;3d model;formal grammar;assembly instructions;lexical analysis;exploded view illustrations;structure analysis	Although there is an abundance of 3D models available, most of them exist only in virtual simulation and are not immediately usable as physical objects in the real world. We solve the problem of taking as input a 3D model of a man-made object, and automatically generating the parts and connectors needed to build the corresponding physical object. We focus on furniture models, and we define formal grammars for IKEA cabinets and tables. We perform lexical analysis to identify the primitive parts of the 3D model. Structural analysis then gives structural information to these parts, and generates the connectors (i.e. nails, screws) needed to attach the parts together. We demonstrate our approach with arbitrary 3D models of cabinets and tables available online.		Manfred Lau;Akira Ohgawara;Jun Mitani;Takeo Igarashi	2011	ACM Trans. Graph.	10.1145/2010324.1964980	natural language processing;lexical analysis;computer science;machine learning;grammar;structural analysis;formal grammar;fabrication;programming language;procedural modeling;algorithm;computer graphics (images)	Graphics	-38.498974026044934	-32.19356276649875	164696
18e9bc8357627bee7208099f519317a446404bef	computer drafting of stones, wood, plant and ground materials	computer graphics;plant representations;stone walls and pavements;computer graphic;intelligent drafting;ground representations;architectural drafting;graphical representation;wood patterns	Architectural presentation drawings frequently require the drafting of stones, wood patterns, plants and ground materials, which, contrary to the majority of drafting tasks, cannot rely on repetitious procedures. This paper discusses and illustrates computer implemented algorithms which generate graphic representations of the above materials.  Simulating a process known to be applied in practice is certainly a sound approach, and one such algorithm, applicable for the derivation of stone walls, is presented, But the majority of the algorithms discussed are based on a technique which introduces randomly generated disturbances on initially regular patterns. The latter algorithms in particular have produced highly satisfactory graphic representations, which include frequently hand-made qualities.	algorithm;computer-aided design;procedural generation;regular expression	Chris I. Yessios	1979		10.1145/800249.807443	architectural drawing;simulation;computer science;computer graphics;computer graphics (images)	Graphics	-38.364286826980006	-31.66318367272829	164769
82df1af7629a90c945bb3dd021b12dc68fade9c9	autocomplete textures for 3d printing		Texture is an essential property of physical objects that affects aesthetics, usability, and functionality. However, designing and applying textures to 3D objects with existing tools remains difficult and time-consuming; it requires proficient 3D modeling skills. To address this, we investigated an auto-completion approach for efficient texture creation that automates the tedious, repetitive process of applying texture while allowing flexible customization. We developed techniques for users to select a target surface, sketch and manipulate a texture with 2D drawings, and then generate 3D printable textures onto an arbitrary curved surface. In a controlled experiment our tool sped texture creation by 80% over conventional tools, a performance gain that is higher with more complex target surfaces. This result confirms that auto-completion is powerful for creating 3D textures.	3d film;3d modeling;3d printing;essence;usability	Ryo Suzuki;Tom Yeh;Koji Yatani;Mark D. Gross	2017	CoRR		computer science;personalization;3d printing;usability;computer graphics (images);3d modeling;sketch;computer vision;texture atlas;autocomplete;artificial intelligence	HCI	-38.91735628707272	-34.632998496433096	164859
31edf4e5779a5dd50cfa2d8857496ca9fdfb26ca	a personalized collaborative digital library environment: a model and an application	community;travail cooperatif;collaborative work;digital library;cooperation;collaboration;customization;personnalisation;information space;personalization;satisfiability;trabajo cooperativo;cooperacion;modelo;biblioteca electronica;personalizacion;comunidad;modele;electronic library;electronic publishing;information need;models;bibliotheque electronique;communaute;cooperative work	The Web, and consequently the information contained in it, is growing rapidly. Every day a huge amount of newly created information is electronically published in Digital Libraries, whose aim is to satisfy the users’ information needs. In this paper, we envisage a Digital Library not only as an information resource where users may submit queries to satisfy their daily information need, but also as a collaborative working and meeting space of people sharing common interests. Indeed, we will present a personalized collaborative Digital Library environment, where users may organize the information space according to their own subjective view, may build communities, may become aware of each other, exchange information and knowledge with other users, and may get recommendations based on preference patterns of users.	digital library;experience;information needs;library (computing);personalization;world wide web	M. Elena Renda;Umberto Straccia	2005	Inf. Process. Manage.	10.1016/j.ipm.2004.04.007	digital library;human–computer interaction;computer science;personalization;multimedia;electronic publishing;world wide web;information retrieval	HCI	-37.71185969260746	-24.77507001554839	164927
1e78e906a25b864c8a4ef63c79cf75af79548bb1	visually interpreting the motion of objects in space	motion pictures;humans cameras psychology visual system cathode ray tubes computer applications orbital robotics robot vision systems motion pictures three dimensional displays;psychology;orbital robotics;computer applications;three dimensional displays;humans;visual system;robot vision systems;cathode ray tubes;cameras	The human visual systemu0027s ability to extract three-dimensional structure from a two-dimensional source is the key to automatic interpretation of structure from motion.	structure from motion	Jon A. Webb;Jake K. Aggarwal	1981	Computer	10.1109/C-M.1981.220561	cathode ray tube;computer vision;structure from motion;simulation;visual system;computer science;computer applications;computer graphics (images)	Graphics	-39.62008991020097	-37.62388656478658	164984
03f95f429aa22a6a25c4a64d812507c6b3a43b95	tool demonstration abstract: openmodelica graphical editor and debugger	run time debugging;modeling and simulation;graphic editor;algorithmic code;connection diagrams	This paper demonstrates the OpenModelica graphic editor for easy-to-use graphic modeling of Modelica models and the Modelica debugger. The graphic editor aims at providing a user friendly open source Modelica modeling graphical user interface since most of the already existing open source tools were either textual or not so user friendly. The target audiences for the tool are the Modelica users who want easy-to-use model creation, library browsing, connection editing, simulation of models, plotting results and visualization of components. Modeling errors and problems are often hard to find because of the high abstraction level of languages like Modelica. Models containing functions with huge algorithm sections increase the need for run-time debugging. The OpenModelica debugger provides a debugging of such models. The debugger currently supports debugging of algorithmic code. The debugger uses the Gnu low-level C-language debugger (GDB) for low-level manipulation and control of the executing program during debugging.	abstraction layer;algorithm;debugging;gnu debugger;graphical user interface;high- and low-level;image editing;open-source software;openmodelica;simulation;usability	Adeel Asghar;Peter Fritzson	2013			parallel computing;computer science;operating system;programming language	HCI	-40.757754372166815	-29.887116338662292	165229
e32ad5b323fca5eac380af12eea830f9f0a8bb5c	gaze-orchestrated dynamic windows	graphical interface;real time;videodisc;human factors;man machine interfaces;technology integration;man machine interface;eye tracking;non real time	Consider a large-format display before the user, bearing a multiplicity of “windows,” like little movies, the majority dynamic and in color. There are upwards of 20 windows, say, more than a person can ordinarily absorb at once. Some of the windows come and go, reflecting their nature as direct TV linkages into real-time, real-world events. Others are non-real-time, some dynamic, others static but capable of jumping into motion.  Such an ensemble of information inputs reflects the managerial world of the top-level executive of the not too distant electronic future: a world of brevity, fragmentation, variety, above all one of an overwhelming onslaught of events.  The multiplicity and simultaneity of such a display situation ordinarily would make coping with it untenable. The intent of the reported research is to introduce order and control, through the creation of a dynamic, gaze-interactive interface.  Making the behavior and reactivity of the “windows” contingent upon measured eyemovements - the point-of-regard of the observer - aims both to help the observer to cope with the onslaught of events on the one hand, yet enable on the other hand continuing close contact with that everchanging ensemble.  A simulation of such a world is described and demonstrated in the composite medium of computer, videodisc, and video special effects. Eye-tracking technology, integrated with speech and manual inputs, controls the display's visual dynamics, and orchestrates its sound accompaniments. All elements are combined to form a testbed for the conception generally, and to explore the associated human factors and stagecraft.	bus mastering;contingency (philosophy);eric a. meyer;eye tracking;fragmentation (computing);graphics;human factors and ergonomics;interactivity;microsoft windows;onslaught;our world;output device;real-time computing;real-time locating system;simulation;testbed;user interface	Richard A. Bolt	1981		10.1145/800224.806796	human–machine interface;computer vision;real-time computing;simulation;eye tracking;computer science;human factors and ergonomics;operating system;graphical user interface;multimedia;computer graphics (images)	HCI	-47.3873854642998	-34.383778392457735	165262
6b778443216c22480ac86d23f5a5da57334bcfe1	the calder toolkit: wired and wireless components for rapidly prototyping interactive devices	physical user interfaces;design process;user interface;user centered design;development environment;rapid prototyping;design and implementation;graphic user interface;product design;interaction and product design;interaction design;physical interaction;toolkits	Toolkits and other tools have dramatically reduced the time and technical expertise needed to design and implement graphical user interfaces (GUIs) allowing high-quality, iterative, user-centered design to become a common practice. Unfortunately the generation of functioning prototypes for physical interactive devices as not had similar support -- it still requires substantial time and effort by individuals with highly specialized skills and tools. This creates a divide between a designers' ability to explore form and interactivity of product designs and the ability to iterate on the basis of high fidelity interactive experiences with a functioning prototype. To help overcome this difficulty we have developed the Calder hardware toolkit. Calder is a development environment for rapidly exploring and prototyping functional physical interactive devices. Calder provides a set of reusable small input and output components, and integration into existing interface prototyping environments. These components communicate with a computer using wired and wireless connections. Calder is a tool targeted toward product and interaction designers to aid them in their early design process. In this paper we describe the process of gaining an understanding of the needs and workflow habits of our target users to generate a collection of requirements for such a toolkit. We describe technical challenges imposed by these needs, and the specifics of design and implementation of the toolkit to meet these challenges.	graphical user interface;input/output;interaction design;interactivity;iteration;prototype;rapid application development;requirement;user-centered design	Johnny C. Lee;Daniel Avrahami;Scott E. Hudson;Jodi Forlizzi;Paul H. Dietz;Darren Leigh	2004		10.1145/1013115.1013139	user interface design;user experience design;user-centered design;simulation;design process;human–computer interaction;computer science;systems engineering;operating system;interaction design;graphical user interface;development environment;product design;user interface	HCI	-42.56940739308376	-30.887688458898616	165280
6fbe2bec96d584fe10451537a90f8967c95ff792	build-it: a computer vision-based interaction technique of a planning tool for construction and design	computer vision;interaction technique	It is time to go beyond the established approaches in human-computer interaction. With the Augmented Reality (AR) design strategy humans are able to behave as much as possible in a natural way: behavior of humans in the real world with other humans and/or real world objects. Following the fundamental constraints of natural way of interacting we derive a set of recommendations for the next generation of user interfaces: the Natural User Interface (NUI). The concept of NUI is presented in form of a runnable demonstrator: a computer vision-based interaction technique for a planning tool for construction and design tasks.	augmented reality;computer vision;human–computer interaction;interaction technique;natural user interface	Matthias Rauterberg;Martin Bichsel;Ulf Leonhardt;Markus Meier	1997			computer vision;simulation;interactive systems engineering;human–computer interaction;computer science;interaction technique	HCI	-38.207444328816244	-37.81262217896919	165464
2c82ae113a47b9499aad22e32906885c8244c1a1	nalix: a generic natural language search environment for xml data	grammar;sistema interactivo;iterative method;lenguaje natural;query language;xquery;computacion informatica;frase;xml language;user study;langage naturel;interrogation base donnee;database;xml database;interrogacion base datos;base dato;lenguaje interrogacion;systeme conversationnel;metodo iterativo;sentence;natural language parsing;proximite;milieu naturel;proximidad;interactive system;ciencias basicas y experimentales;methode iterative;natural environment;grammaire;iterative search;natural language;system;proximity;xml;base de donnees;algorithms;natural language interface;design;langage interrogation;phrase;medio natural;grupo a;experimentation;gramatica;database query;large classes;langage xml;lenguaje xml;dialog system	We describe the construction of a generic natural language query interface to an XML database. Our interface can accept a large class of English sentences as a query, which can be quite complex and include aggregation, nesting, and value joins, among other things. This query is translated, potentially after reformulation, into an XQuery expression. The translation is based on mapping grammatical proximity of natural language parsed tokens in the parse tree of the query sentence to proximity of corresponding elements in the XML data to be retrieved. Iterative search in the form of followup queries is also supported. Our experimental assessment, through a user study, demonstrates that this type of natural language interface is good enough to be usable now, with no restrictions on the application domain.	application domain;information retrieval;natural language user interface;parse tree;parsing;principle of good enough;usability testing;xml database;xquery	Yunyao Li;Huahai Yang;H. V. Jagadish	2007	ACM Trans. Database Syst.	10.1145/1292609.1292620	natural language processing;query optimization;query expansion;xml;natural language user interface;data control language;computer science;database;dialog system;rdf query language;programming language;web search query;query language	DB	-37.81969251358812	-27.718778671987494	165609
22dbbf7ebb86ec9d6bf1189de32c82327c36aa98	assessment of a multimodal interaction and rendering system against established design principles	design principle;user interface;model transformation;proof of concept;rendering system;model based development;multimodal interaction	To provide user interfaces for a rich set of devices and interaction modalities, we follow a model-based development methodology. We devised an architecture which deploys user interfaces specified as dialogue models with abstract interaction objects and allows context-based adaptations by means of an external transcoding process. For the validation of the applicability of this methodology for developing usable multimodal multi-device systems, we present two case studies based on proof-of-concept implementations and assessed them with a large set of established design principles and different types of modality cooperation.	feedback;input device;modality (human–computer interaction);model-driven engineering;multimodal interaction;rendering (computer graphics);requirement;server-side;specification language;state (computer science);uiml;usability;user interface	Robbie Schaefer;Wolfgang Müller	2008	Journal on Multimodal User Interfaces	10.1007/s12193-008-0003-3	simulation;speech recognition;interactive systems engineering;human–computer interaction;computer science;operating system;multimodal interaction;multimedia;user interface;proof of concept;model-based design	HCI	-44.00381366101635	-34.2301163108196	165659
87636874adc7fde45ff0361544186b0fc7ccecc7	views of mathematical programming models and their instances	structural model;management system;computer assisted analysis;natural language discourse;modeling languages;large scale system;structured modeling;modeling language;large scale;cognitive skills;mathematical programming;natural language;linear programming;mathematical model;linear program;problem behavior;graphics;large scale systems	Large-scale mathematical models are built, managed and applied by people with different cognitive skills. This poses a challenge for the design of a multi-view architecture of a system that accommodates these differences. A primary objective of mathematical modeling is providing insights into problem behavior, and there are many constituencies who require different views for different questions. One constituency is composed of modellers who have different views of basic model components. Another constituency is composed of problem owners for whom models are built. These two constituencies , which are not exhaustive, have significantly different needs and skills. This paper addresses this issue of multi-view architecture by presenting a formal framework for the design of a view creation and management system. Specific views we consider include algebraic, block schematic, graphic, and textual. Both form and content are relevant to view creation, and the merits of views are determined by their value in aiding comprehension and insight. The need for a central, formal structure to create and manage views is demonstrated by the inadequacy of direct mappings from any of the popular systems that are typically designed to support only one view of linear programming models and their instances .	linear algebra;linear programming;list comprehension;mathematical model;mathematical optimization;schematic	Harvey J. Greenberg;Frederic H. Murphy	1995	Decision Support Systems	10.1016/0167-9236(93)E0029-D	natural language processing;simulation;system model;computer science;linear programming;artificial intelligence;machine learning;data mining;database;modeling language	AI	-37.01017953467508	-29.64482303247769	165733
36e2e9c4049de6769cfb18ea81177ae2bb659933	jamioki-purejoy: a game engine and instrument for electronically-mediated musical improvisation	jamioki;purejoy;structured improvisation;game engine;found sound;collaborative performance;electronically mediated performance	JamiOki-PureJoy is a novel electronically mediated musical performance system. PureJoy is a musical instrument; A highly flexible looper, sampler, effects processor and sound manipulation interface based on Pure Data, with input from a joystick controller and headset microphone. PureJoy allows the player to essentially sculpt their voice with their hands. JamiOki is an engine for running group-player musical game pieces. JamiOki helps each player by 'whispering instructions' in their ear. Players track and control their progress through the game using a graphical display and a touch-sensitive footpad. JamiOki is an architecture for bringing groups of players together to express themselves musically in a way that is both spontaneous and formally satisfying. The flexibility of the PureJoy instrument offers to JamiOki the ability for any player to play any requested role in the music at any time. The musical structure provided by JamiOki helps PureJoy players create more complex pieces of music on the fly with spontaneous sounds, silences, themes, recapitulation, tight transitions, structural hierarchy, interesting interactions, and even friendly competition. As a combined system JamiOki-PureJoy is exciting and fun to play.	game engine;graphical user interface;headset (audio);infographic;interaction;joystick;microphone;on the fly;pure data;sampling (signal processing);spontaneous order;theme (computing);touchscreen	Benjamin Vigoda;David Merrill	2007		10.1145/1279740.1279810	real-time computing;simulation;human–computer interaction;artificial intelligence;multimedia	HCI	-46.75521861823035	-35.67113048555996	165825
5a0b817319132a8916f47340e42e44793988d0a2	the role of built-in knowledge in adaptive interface systems	adaptive user interfaces;user modeling;adaptive interface;task oriented interfaces;pattern recognition;user variation;adaptive user interface;user model	Built-in Knowledge in Adaptive Interface Systems Daniel Crow and Barbara Smith School of Computer Studies, University of Leeds, LS29JT, United Kingdom daniel@dcs.leeds. ac.uk We discuss the construction of task-oriented interfaces and argue that they must adapt themselves to each individual user’s behaviour. Because of the variation between users, it is impracticable to attempt to build irt a priori assumptions about the user. We present an adaptive interface system, DB_Habits, which shows that it is possible, however, to incorporate knowledge about the underlying system, and which uses this to recognise repeated sequences of commands issued by the user which represent the user’s higher-level tasks. DB_Habits uses the command sequences found, to collaborate with the user in achieving the user’s tasks and is malleable i.e. can be easily adapted by the user to support their tasks. We present results showing the effect of incorporating simple command syntax knowledge on the performance of the system.	canonical account;computer science	Daniel Crow;Barbara Smith	1993		10.1145/169891.169919	user interface design;user;human action cycle;user modeling;shell;human–computer interaction;natural language user interface;computer science;multimedia;natural user interface;user interface;graphical user interface testing;multiple document interface	HCI	-39.29913997174547	-27.679691624101714	165865
99b056372079ae938bc867a753ab8d9650940157	the musicdb: a music database query system for recombinance-based composition in max/msp		We propose a design and implementation for a music information database and query system, the MusicDB, which can be used for data-driven algorithmic composition. Inspired by David Copeu0027s ideas surrounding composition by “music recombinance”, the MusicDB is implemented as a Java package, and is loaded in MaxMSP using the mxj external. The MusicDB contains a music analysis module, capable of extracting musical information from standard MIDI files, and a search engine. The search engine accepts queries in the form of a simple six-part syntax, and can return a variety of different types of musical information, drawing on the encoded knowledge of musical form stored in the database.	max	James B. Maxwell;Arne Eigenfeldt	2008			musical;syntax;java package;midi;algorithmic composition;search engine;database;music theory;musical form;computer science	DB	-47.15240521455988	-25.509217085018303	165995
7351103fb82087249c6b671e270a1e0b82d1073b	signal processing, acoustics, and psychoacoustics for high quality desktop audio	large scale system;signal processing;dynamic adaptation	Integrated media workstations are increasingly being used for creating, editing, and monitoring sound that is associated with video or computer-generated images. While the requirements for high quality reproduction in large-scale systems are well understood, these have not yet been adequately translated to the workstation environment. In this paper we discuss several factors that pertain to high quality sound reproduction at the desktop including acoustical and psychoacoustical considerations, signal processing requirements, and the importance of dynamically adapting the reproduced sound as the listener’s head moves. We present a desktop audio system that incorporates several novel design requirements and integrates vision-based listener-tracking for accurate spatial sound reproduction. We conclude with a discussion of the role the pinnae play in immersive (3D) audio reproduction and present a method of pinna classification that allows users to select a set of parameters that closely match their individual listening characteristics.	computer-generated holography;desktop computer;display resolution;psychoacoustics;requirement;signal processing;workstation	Chris Kyriakakis;Tomlinson Holman;Jong-soong Lim;Hai Hong;Hartmut Neven	1998	J. Visual Communication and Image Representation	10.1006/jvci.1998.0379	computer vision;speech recognition;computer science;signal processing;multimedia	Graphics	-46.29457057805996	-34.09655681887194	166063
674c7009aa2d705240ac1f033dc246f4e613a517	interactive vr-based visualization for material flow simulations		The conventional way of visualizing the material flow in a production system is to use simulation tools and their integrated symbols and pictograms. By going this way, the reference to the real production system is very limited since conventional material flow models provide only an abstract view and are not very comprehensive for the user. This paper introduces a procedure which enables a Virtual Design Review of the planned process layout on a large-screen visualization facility. This enables production planners to conduct a virtual inspection of alternative concepts for a planned production system including the visualized material flow. As a result, planning certainty and system comprehension of all parties involved increase significantly, so that the presented procedure serves as a valuable decision support. This paper describes the steps to be taken from production data to an optimized material flow being verified by a Virtual Design Review.	computer simulation;material flow	Jan Berssenbrügge;Jörg Stöcklein;Daniel Köchling	2016		10.1007/978-3-319-39907-2_56	computer graphics (images)	HPC	-38.13406764323753	-31.53092283499462	166121
8b93d103d9af626ed0515ece66efbc2e688b6e99	software platform for real-time room acoustic visualization	software platform;3d visualization;real time;real time audio beam tracing;room acoustic;room acoustics	This paper presents a novel platform for interactive virtual room acoustics simulation and visualization. The platform is based on two autonomous modules: EVERTims for acoustic simulation and Audio2graphical for graphical rendering and interaction management. The system is a tool for acousticians and architects to understand better the acoustic properties of a closed space. Beside visualizing sound reflections, it allows navigation as well as geometrical properties modification of a given sound source and a listener.	acoustic cryptanalysis;autonomous robot;covox speech thing;graphical user interface;real-time transcription;reflection (computer graphics);simulation	Rami Ajaj;Lauri Savioja;Christian Jacquemin	2008		10.1145/1450579.1450636	simulation;visualization;room acoustics;computer science;computer graphics (images)	Visualization	-45.75010533962473	-34.54755972422595	166174
4ec89c30cc59e9052cca68a9c7bdd8d20a3e6a6a	wish: a web information sharing system accessible via the www	architecture systeme;navegacion informacion;red www;navigation information;information browsing;information sharing;world wide web;arquitectura sistema;reseau www;information system;system architecture;systeme information;sistema informacion	The desktop conferencing system described here is called WISH (for Web Information SHare) and enables users to conduct a conference on the Internet. Users can participate in the conference by using WWW browsers and can share HTML documents with all participants in realtime. When one participant accesses WWW pages and browses them, the same pages are automatically displayed on all other participants'browsers. Participants can also annotate directly on the pages and share the annotations in realtime.	www	Hiromi Mizuno;Hideyuki Fukuoka	1997		10.1007/3-540-63343-X_42	human–computer interaction;computer science;database;multimedia;world wide web;information system;systems architecture	ECom	-41.24482737981566	-24.637811038752346	166374
71e2761f73aa0cab5ee77c97244f1d4bcc70cb5e	the wui-toolkit: a model-driven ui development framework for wearable user interfaces	wearable user interface toolkit;envisioned user interface;context aware;user interface;wearable computers;monocular head mounted display;graphical wearable user interface wearable user interface toolkit model driven user interface development framework wearable computing system envisioned user interface monocular head mounted display;graphical user interfaces;wearable computing system;industrial application;wearable computers graphical user interfaces helmet mounted displays user interface management systems;wearable computer;model driven user interface development framework;user interface management systems;user interfaces;graphical wearable user interface;helmet mounted displays	"""We introduce the """"WUI-Toolkit"""" as a framework to support and ease the development of wearable user interfaces (WUIs). The toolkit presents a first step towards a model-driven UI design approach in wearable computing that allows even non UI experts the generation of usable and context-aware WUIs. Based on an abstract model of an envisioned user interface that is independent of any concrete representation, the toolkit is able to generate a device- and context- specific UI for a given wearable computing system at runtime. The toolkit features the ability to use available context sources and can automatically adapt generated interfaces to maintain their usability. We present the current architecture including the capabilities of the abstract model and introduce a renderer developed to generate graphical WUIs suitable for monocular head-mounted displays in industrial applications."""	context awareness;graphical user interface;head-mounted display;mobile device;model-driven architecture;model-driven integration;programming paradigm;refinement (computing);run time (program lifecycle phase);usability;usability testing;user interface design;wearable computer	Hendrik Witt;Tom Nicolai;Holger Kenn	2007	27th International Conference on Distributed Computing Systems Workshops (ICDCSW'07)	10.1109/ICDCSW.2007.80	embedded system;wearable computer;human–computer interaction;computer science;operating system;multimedia;user interface	HCI	-43.68214181245686	-34.326616243427914	166405
47abe37ecca579b39ff106bf7f203a5cd7cc82bd	teleinvivotm: towards collaborative volume visualization environments	application development;image tridimensionnelle;interfase usuario;systeme unix;architecture systeme;protocole tvp ip;protocole transmission;aplicacion medical;image processing;red www;user interface;ultrasound;implementation;unix system;teleinformatica;procesamiento imagen;traitement image;computer graphic;ejecucion;teleinformatique;protocolo transmision;telecomunicacion;systeme windows;telecommunication;data visualization;volume visualization;tridimensional image;world wide web;arquitectura sistema;interface utilisateur;medical application;reseau www;sistema unix;system architecture;remote data processing;langage html;imagen tridimensional;application medicale;health care;transmission protocol	Abstract   Converging technologies in the areas of telecommunications, volume visualization, and computer hardware and peripherals have made possible in recent years the development of new tools for collaboration that extend the reach of health care professionals and other consumers of volumetric data around the world. We describe a recent development at the Center for Research in Computer Graphics in Providence, RI, that makes a significant contribution to this area. TeleInViVo ™  is an application that supports collaborative volumetric data visualization and exploration. It is an extension and partial reworking of InViVo ™ , a volume visualization application developed at the Fraunhofer IGD, in Darmstadt, Germany. InViVo, which is largely focused around the medical community and with an emphasis on diagnostic ultrasound, has been augmented with new modes of interaction, an intuitive collaboration mechanism, and an architectural modification to support future developments in this area.	scientific visualization	John Coleman;Ammo Goettsch;Andrei Savchenko;Hendrik Kollmann;Wang Kui;Edwin Klement;Peter Bono	1996	Computers & Graphics	10.1016/S0097-8493(96)00053-2	simulation;information visualization;image processing;computer science;operating system;ultrasound;user interface;implementation;rapid application development;data visualization;health care;computer graphics (images)	Visualization	-35.068611970493805	-28.706878602874692	166544
0f4bdc4ca73e28ca2747704b35f308c306062bc8	a universal assistive technology with multimodal input and multimedia output interfaces		In this paper, we present a universal assistive technology with multimodal input and multimedia output interfaces. The conceptual model and the software-hardware architecture with levels and components of the universal assistive technology are described. The architecture includes five main interconnected levels: computer hardware, system software, application software of digital signal processing, application software of human-computer interfaces, software of assistive information technologies. The universal assistive technology proposes several multimodal systems and interfaces to the people with disabilities: audio-visual Russian speech recognition system (AVSR), “Talking head” synthesis system (text-to-audiovisual speech), “Signing avatar” synthesis system (sign language visual synthesis), ICANDO multimodal system (hands-free PC control system), and the control system of an assistive smart space.	assistive technology;multimodal interaction	Alexey Karpov;Andrey Ronzhin	2014		10.1007/978-3-319-07437-5_35	simulation;human–computer interaction;computer science;multimodal interaction;multimedia	HCI	-46.40440931158828	-36.80149500736573	167026
2174cf05b9b27dc418f1319a8105d35814b0116f	augmented robot agent: enhancing co-presence of the remote participant	electronic mail;h 5 1 information interfaces and presentation multimedia information systems artificial;virtual reality;user interfaces augmented reality telerobotics;indexing terms;tangible interface;multimedia information system;augmented;visualization;tele tutoring augmented robot agent tele meeting system 3d volume video augmentation tangible interface tele marketing;three dimensional displays;h 5 1 information interfaces and presentation multimedia information systems artificial augmented and virtual realities;robots;and virtual realities;telerobotics;agent systems;augmented reality;robots three dimensional displays visualization cameras robot vision systems real time systems electronic mail;information interfaces and presentation;user interfaces;robot vision systems;cameras;real time systems	In this paper, we present a tele-meeting system which uses an augmented robot agent as the representation of the remote participant. In this system, we augment a 3D volume video of the remote participant over the on-site robot. The robot agent in this system represents a remote user with camera, microphone, and mobility. Using this robot agent, the remote user and the local user can show their appearances and interact with tangible interfaces. This robot agent system can be applied to various tele-meeting applications such as tele-marketing and tele-tutoring. For example, we implemented a tele-marketing system to show the feasibility of the suggested system.	microphone;robot;television	Jane Hwang;Sang Yup Lee;Sang Chul Ahn;Hyoung-Gon Kim	2008	2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2008.4637346	telerobotics;robot;augmented reality;simulation;visualization;index term;human–computer interaction;computer science;artificial intelligence;social robot;virtual reality;multimedia;user interface	Robotics	-46.62681651817133	-37.685440809719275	167125
618ba3eeb0020ae7148ee903e159d764246ec234	human-computer interface development tools: a methodology for their evaluation	developpement logiciel;evaluation performance;interfase usuario;metodologia;performance evaluation;user interface;evaluacion prestacion;relacion hombre maquina;man machine relation;ingenieria logiciel;software engineering;methodologie;development tool;desarrollo logicial;software development;genie logiciel;array;interface utilisateur;relation homme machine;methodology;human computer interface	A comprehensive checklist-based methodology produces quantifiable criteria for evaluating and comparing human-computer interface development tools along two dimensions: functionality and usability. An empirical evaluation shows that the methodology which is in use in several corporate interface development environments, produces reliable (consistent) results	coherence (physics);human–computer interaction;programming tool;taxonomy (general);universal instantiation;usability	Deborah Hix;Robert S. Schulman	1991	Commun. ACM	10.1145/102868.102873	human–computer interaction;computer science;software development;software engineering;methodology;user interface	HCI	-37.88495932204568	-27.212202382637273	167218
bf3b2b04bde99631744941e36a73172f01d3c805	aspects of display technology	data processing;interactive graphics	In providing background information on display technology, this paper discusses cathode-ray tube characteristics and interactive devices as they affect the user.#R##N##R##N#Described are the display functions of the IBM 2250 display console, which is used in many applications.#R##N##R##N#Some elementary aspects of image generation are presented, and the current and potential capability of displays is discussed.		Arthur Appel;Thaddeus P. Dankowski;Richard L. Dougherty	1968	IBM Systems Journal	10.1147/sj.73.0176	data processing;human–computer interaction;computer science;real-time computer graphics;database;multimedia;computer graphics (images)	EDA	-47.364997237878065	-29.361388860028658	167333
1060d282e3fac021fe86587839cfaed2fbf45eb5	remote access to 3d models of cultural heritage	digital cultural heritage distance learning real time 3d transmissions virtual reality;virtual reality data visualisation distance learning history solid modelling;digitized cultural heritage remote access stereoscopic visualization immersive visualisation long distance transmission real time interaction remote 3d model historical architecture distance research distance education;three dimensional displays solid modeling computational modeling rendering computer graphics cultural differences data visualization data models	Stereoscopic and immersive visualisations are used in various scientific and engineering fields. We describe our experience with long-distance transmissions enabling real-time interaction with a remote 3D model of historical architecture over a long-distance transmission. We illustrate the potential of such technology for distance research, education and research using models of historical architecture and other digitized cultural heritage.	3d modeling;real-time transcription;remote desktop software;stereoscopy	Sven Ubik;Jirí Navrátil;Zdeněk Trávníček;Jiri Melnikov	2015	2015 Digital Heritage	10.1109/DigitalHeritage.2015.7419622	computer vision;computer science;multimedia;computer graphics (images)	HCI	-35.65865785859003	-31.814869676340816	167445
95758c1fab8727e4b1283e7d4e23a3c2fe35fa97	spam: a sparql analysis and manipulation tool	sparql query;manipulation tool;main feature;dynamic analysis;graphical interface;sparql analysis;elaborate tool;system demo;sql developer;spam tool;various function	SQL developers are used to having elaborate tools which help them in writing queries. In contrast, the creation of tools to assist users in the development of SPARQL queries is still in its infancy. In this system demo, we present the SPARQL Analysis and Manipulation (SPAM) tool, which provides help for the development of SPARQL queries. The main features of the SPAM tool comprise an editor with both text and graphical interface, as well as various functions for the static and dynamic analysis of SPARQL queries.	graphical user interface;sparql;sql	Andrés Letelier;Jorge Pérez;Reinhard Pichler;Sebastian Skritek	2012	PVLDB	10.14778/2367502.2367547	named graph;computer science;sparql;database;world wide web;information retrieval	DB	-42.069115296560554	-24.58341534463699	167543
e90575bfb9c1faf73398b969261bb8e6e9b8d161	rapid interactive real-time application prototyping for media arts and stage performance	processing;human computer interfaces;interaction;real time;jitter max;kinect;computer graphics education;opengl	We explore a rapid prototyping of interactive graphical applications using Jitter/Max and Processing with OpenGL, shaders, and featuring connectivity with various devices such as, Kinect, Wii, iDevice-based controls, and others. Such rapid prototyping environment is ideal for entertainment computing, as well as for artists and live performances using real-time interactive graphics. We share the expertise we developed in connecting the real-time graphics with on-stage performance with the Illimitable Space System (ISS) v2.  We include various presentation options ranging from half-day to 1-hour style with the default of half-day and making the course delivery more flexible; topics can be modularly added or subtracted per the syllabi that follow.	graphical user interface;graphics;kinect;modular programming;opengl;performance;rapid prototyping;real-time computing;real-time locating system;real-time transcription;shader;wii	Miao Song;Serguei A. Mokhov;Sudhir P. Mudur;Peter Grogono	2015		10.1145/2818143.2818148	computer vision;interaction;real-time computing;simulation;computer science;artificial intelligence;processing;operating system;multimedia;programming language;computer graphics (images)	Graphics	-44.39217563281886	-34.039025907135525	167694
41dfa9e2c9b779c345e11bb6b403ae9bcc8ceb0f	3d-printed prosthetics for the developing world	gaze;alternative splicing;graphic novel;branching;attention;belief states;interactive;fixation;eye tracking	The growing availability of 3D printing has made it possible for end-users to manufacture prosthetic devices tailored to their individual needs. For example, Project e-Nable (www.enablingthefuture.org) provides parametric 3D-printable prosthetic hand designs. However, the e-Nable hand is an assembly of standardized parts, customized via rigid-body transformations. For cases of trans-tibial and trans-femoral leg amputation, the required prosthetic must blend mechanical parts with a socket that conforms to the shape of the residual limb. The socket design also plays a critical role in minimizing pain by distributing the significant mechanical stresses to appropriate anatomical locations. As a result, design customization is much more challenging.	3d printing;source-to-source compiler	Ryan Schmidt;Ginger Coons;Vincent Chin-Hung Chen;Timotheius Gmeiner;Matt Ratto	2015		10.1145/2785585.2792535	fixation;simulation;attention;eye tracking;branching;computer science;alternative splicing;interactivity	HCI	-35.36529903312126	-37.68513146610103	167720
f6ba0d7bdee5881ef67af062345635deefa94a7a	making linear equations accessible for visually impaired students using 3d printing	printing;special education;three dimensional printing authoring systems computer aided instruction graph theory handicapped aids mathematics computing;visually impaired students;printers;3d printing;content accessibility;special education 3d printing mathmatics visually impaired students content accessibility braille linear equation;mathmatics;mathematical model three dimensional displays solid modeling printing education printers;three dimensional displays;solid modeling;mathematical model;braille captioning linear equations visually impaired students 3d printing math education authoring tool math teachers accessible learning materials 3d models linear equation graphs;braille;linear equation	Due to the limitation of the available tools that enhance math education for visually impaired students, this paper proposes the design of an authoring tool that helps math teachers produce accessible learning materials for linear equations in the form of 3D models. The proposed tool will focus on producing 3D models of linear equation graphs printed in predefined plates with proper Braille captioning.	3d modeling;3d printing;linear equation	Noha Al-Rajhi;Amal Al-Abdulkarim;Hend Suliman Al-Khalifa;Hind M. Al-Otaibi	2015	2015 IEEE 15th International Conference on Advanced Learning Technologies	10.1109/ICALT.2015.46	speech recognition;special education;3d printing;computer science;mathematical model;multimedia;linear equation;solid modeling;computer graphics (images)	Robotics	-41.45650951217408	-37.21529665190557	167748
9d1380f974bdcfa6863280663f01e190ca2133d7	three dimensional software modeling	software systems;qa 76 software;object oriented programming;system modelling;software engineering;three dimensional;formal method;computer programming;3d model;object oriented;case tool;object oriented software development;object oriented modeling software systems power system modeling solid modeling workstations computer aided software engineering software tools visualization flowcharts unified modeling language;rendering computer graphics;3d graphics;object oriented programming rendering computer graphics software engineering;2d stereoscopic projections three dimensional software modelling graphical notations structured systems modelling object oriented modelling topological graph metaphor three dimensional rendering z co ordinate	Traditionally, diagrams used in software systems modelling have been two dimensional (2D). This is probably because graphical notations, such as those used in object-oriented and structured systems modelling, draw upon the topological graph metaphor, which, at its basic form, receives little benefit from three dimensional (3D) rendering. This paper presents a series of 3D graphical notations demonstrating effective use of the third dimension in modelling. This is done by e.g., connecting several graphs together, or in using the Z co-ordinate to show special kinds of edges. Each notation combines several familiar 2D diagrams, which can be reproduced from 2D projections of the 3D model. 3D models are useful even in the absence of a powerful graphical workstation: even 2D stereoscopic projections can expose more information than a plain planar diagram.	2d to 3d conversion;3d modeling;3d rendering;graphical user interface;modeling language;polygonal modeling;software system;stereoscopy;systems modeling;topological graph;voronoi diagram;workstation	Joseph Gil;Stuart Kent	1998		10.1109/ICSE.1998.671107	formal methods;computer science;theoretical computer science;software engineering;programming language;object-oriented programming;3d computer graphics;computer graphics (images)	SE	-37.17488165441411	-30.283847126996182	167761
2eccd62201182f6538ef81471bc0d7cb175d40d4	an interactive fire animation on a mobile environment	interactive physically based fluid simulation interactive fire animation mobile environment computer graphics fluid like motion mobile phone mobile 3d games;physics based modeling;application software;computer graphics;mobile platform fire simulation mobile game;fluid model;computer graphic;mobile phone;mobile environment;computational modeling;mobile service;3d environment;mobile platform;fluid like motion;interactive fire animation;animation;mobile communication;fires animation computational modeling computer graphics computer simulation rendering computer graphics mobile computing mobile communication application software mobile handsets;mobile handsets;mobile game;interactive physically based fluid simulation;fire simulation;fluid simulation;mobile games;computer animation;computer games;rendering computer graphics;mobile computing;mobile 3d games;fires;computer simulation;mobile computing computer animation computer games	In computer graphics, fluid-like motion is an important and challenging problem with many applications. Currently, the graphical performance and resolution of the mobile phone is highly improved; therefore, mobile 3D games are more and more popular items in the field of mobile services. Yet interactive physically-based fluid simulation is still challenging for mobile platforms. Stable and fast fluid simulation methods are well developed in PC and console games, but fluid simulation and interactive fluid models still have many problems in the mobile environment. We studied and implemented physically- based models for fluids like fire and smoke effects on a Mobile 3D environment.	application programming interface;computational fluid dynamics;computer graphics;console game;fluid animation;graphical user interface;mobile 3d graphics api;mobile device;mobile game;mobile operating system;mobile phone;simulation	DongGyu Park;SangHyuk Woo;MiRiNa Jo;DoHoon Lee	2008	2008 International Conference on Multimedia and Ubiquitous Engineering (mue 2008)	10.1109/MUE.2008.85	computer simulation;fluid simulation;anime;physically based animation;application software;mobile search;simulation;mobile telephony;computer science;operating system;mobile technology;computer animation;multimedia;computer graphics;mobile computing;computational model;computer graphics (images)	Robotics	-41.38831476541934	-34.79551192728386	167818
a26a997f8c0695f0f121b8d1fc176f18e509f7b2	towards syntax-aware editors for visual languages	syntax aware editing;error recovery;visual language parsing;error handling;visual language;profitability	Editors for visual languages should provide a user-friendly environment supporting end users in the composition of visual sentences in an effective way. Syntax-aware editors are a class of editors that prompt users into writing syntactically correct programs by exploiting information on the visual language syntax. In particular, they do not constrain users to enter only correct syntactic states in a visual sentence. They merely inform the user when visual objects are syntactically correct. This means detecting both syntax and potential semantic errors as early as possible and providing feedback on such errors in a non-intrusive way during editing. As a consequence, error handling strategies are an essential part of such editing style of visual sentences.In this work, we develop a strategy for the construction of syntax-aware visual language editors by integrating incremental subsentence parsers into free-hand editors. The parser combines the LR-based techniques for parsing visual languages with the more general incremental Generalized LR parsing techniques developed for string languages. Such approach has been profitably exploited for introducing a noncorrecting error recovery strategy, and for prompting during the editing the continuation of what the user is drawing.		Gennaro Costagliola;Vincenzo Deufemia;Giuseppe Polese	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.08.050	exception handling;natural language processing;speech recognition;computer science;syntax;programming language;profitability index	HCI	-38.2756339563904	-30.275442457187577	168144
09badc2cba2568d64cfb70ff6353ad007b3f8af5	fast, accurate creation of data validation formats by end-user developers	design tool;web macros;end user development;data type;web applications;data validation;regular expression	Inputs to web forms often contain typos or other errors. However, existing web form design tools require end-user developers to write regular expressions (“regexps”) or even scripts to validate inputs, which is slow and errorprone because of the poor match between common data types and the regexp notation. We present a new technique enabling end-user developers to describe data as a series of constrained parts, and we have incorporated our technique into a prototype tool. Using this tool, end-user developers can create validation code more quickly and accurately than with existing techniques, finding 90% of invalid inputs in a lab study. This study and our evaluation of the technique’s generality have motivated several tool improvements, which we have implemented and now evaluate using the Cognitive Dimensions framework.	algorithm;browsing;centrality;cognitive dimensions of notations;cognitive science;consortium;data validation;form (html);general-purpose language;general-purpose modeling;global variable;ibm notes;javascript;problem domain;prototype;regular expression;software developer;string (computer science);tweaking;usability testing	Christopher Scaffidi;Brad A. Myers;Mary Shaw	2009		10.1007/978-3-642-00427-8_14	computer science;data mining;database;world wide web	SE	-40.53282423036561	-29.878843473807688	168146
0556822b00730b8deb22e08766d69c2593b77a74	visartico: a visualization tool for articulatory data	vocal tract;speech production;visualization	In this paper, we present VisArtico, a visualization tool for articulatory data acquired using the AG500 3D electromagnetic articulograph (EMA). This software allows displaying the positions of the EMA coils that are simultaneously animated with playback of the acoustic speech signal. It is also possible to display contours for the tongue and lips. The software helps to find the midsagittal plane of the speaker and offers data-based palate shape discovery. In addition, VisArtico allows labeling the articulatory data into phonetic segments. Our main goal is to provide an efficient, easy-to-use tool to visualize articulatory data for researchers working in the field of speech production.	acoustic fingerprint	Slim Ouni;Loic Mangeonjean;Ingmar Steiner	2012			speech recognition;visualization;computer science;software;electromagnetic articulography;speech production	Visualization	-44.82641200498312	-33.34332958938112	168154
1278348e149e7cf281e75b0785f75c877e0eecb6	a matlab-based interactive simulator for mobile robotics	educational courses matlab based interactive simulator open source matlab based interactive software tool mobile robotics teaching modeling subject path planning subject motion control subject graphical user interface gui;path planning computer aided instruction control engineering education educational courses graphical user interfaces interactive systems mobile robots motion control;mobile robots trajectory wheels matlab	This paper presents an open-source Matlab-based interactive software tool for teaching mobile robotics in introductory courses. In particular, it deals with the subjects: modeling, path planning, and motion control. This simulator offers the advantage of testing different aspects related to these fundamental topics and instantly seeing the result in a Graphical User Interface (GUI). This fact leads to a high realism in interactivity, while no previous knowledge in Matlab or programming is required. Additionally, it constitutes an easily scalable tool, the GUI has been properly designed with just one single window with straightforward and intuitive buttons, icons, and figures. Some illustrative examples demonstrate the benefits of the proposed simulator.	algorithm;graphical user interface;interactivity;matlab;mobile robot;motion controller;motion planning;obstacle avoidance;open-source software;parsing;programming tool;region of interest;robotics;scalability;simulation;velocity (software development);voronoi diagram	R. Gilberto Gonzalez;Cristian Mahulea;Marius Kloetzer	2015	2015 IEEE International Conference on Automation Science and Engineering (CASE)	10.1109/CoASE.2015.7294097	simulation;human–computer interaction;computer science;multimedia	Robotics	-37.36486835375332	-37.33144501775585	168165
ccae1f3961cd33acec0a742530d7d5e3b06d5422	ten more unsolved problems in computer graphics	computer graphics facial animation hair arithmetic computer science education conference proceedings optical reflection production systems skin tree graphs;computer graphics;technical issues computer graphics unsolved problems novelty education systems integration simplicity pixel arithmetic theory legacy compatibility arithmetic sloppiness antialiasing modelling rendering animation real time 3d graphics cheap fast solutions sociological issues marketing issues	"""The tradition of posing unsolved problems in computer graphics goes back to Ivan Sutherland's article """"Ten unsolved problems in computer graphics"""", Datamation, vol. 12, no. 5, pp. 22-7 (1966). In this article, the author presents his own personal top 10: (1) novelty; (2) education; (3) systems integration; (4) simplicity; (5) better pixel arithmetic theory; (6) legacy compatibility; (7) arithmetic sloppiness; (8) antialiasing; (9) a modelling/rendering/animation challenge; and (10) finding a use for real-time 3D. Some of the problems I decry have, indeed, been solved in the theoretical sense. The problems remain unsolved in the practical sense, though, because cheap and fast solutions remain elusive. Many of the problems are more sociological and marketing issues than technical. Also, many of them have multiple parts with much overlap."""	computer graphics	James F. Blinn	1998	IEEE Computer Graphics and Applications	10.1109/38.708564	computer vision;computer science;artificial intelligence;theoretical computer science;mathematics;geometry;programming language;computer graphics;algorithm;computer graphics (images);mechanical engineering	Visualization	-48.17534936322734	-29.179902505236573	168212
b5d6ef2f0e1fe8163bb61fa63c6696869668f4a0	experiments toward reverse linking on the web	user evaluation;log files;web pages;world wide web;open hypermedia systems;bi directional linking;user satisfaction;hypertext;inbound links	Multi-headed reverse linking (incoming links) is a fundamental concept of Open Hypermedia Systems. However, this bi-directionality has been lost in the move to the World Wide Web (Web). Here, we suggest a Web based solution for rediscovering these reverse links, and develop a series of experiments to demonstrate our approach. Simply our algorithm involves parsing a Web server's log file, identifying each Web page viewed and saving an ordered list of referrers within a 'name-matched' XML file. This file is then used as a link point within a standard XHTML Web-page using a freely available Javascript library. While we have not performed any comprehensive user evaluation initial qualitative results suggest users are positive regarding our additions and that widespread adoption would increase user satisfaction due to constancy of the browsing experience.	algorithm;backlink;computer user satisfaction;experiment;hypermedia;javascript library;parsing;server (computing);web page;web server;world wide web;xhtml;xml	Yeliz Yesilada;Darren Lunn;Simon Harper	2007		10.1145/1286240.1286244	web service;backlink;deep linking;static web page;web development;web modeling;data web;web mapping;web-based simulation;hypertext;html;web design;web standards;computer science;dynamic web page;web navigation;web log analysis software;web page;database;multimedia;client-side scripting;web 2.0;world wide web;web server	Web+IR	-43.2809675692633	-25.49952722177689	168383
7b63b94ed7d03ade744554fce21eafa7a7fb7709	multi-console intelligent satellite graphics	engineering design;satisfiability;computer graphic;graphics system	A variety of needs, in combination, make existing configurations for interactive computer graphics unsatisfactory for our purposes. Attempts to simultaneously satisfy all of these needs, lead to a graphics system configuration that is under development for potential utilization in a variety of circumstances within the Department of the Army. It is part of an overall project, Multi-discipline Engineering Design, Evaluation and Analysis (MEDEA), to provide an integrated capability to in-house, government scientists and engineers.	computer graphics;engineering design process;human–computer interaction;system configuration	J. C. Jervert;R. M. Dunn	1974		10.1145/563182.563192	simulation;computer science;operating system;programming language;engineering design process;satisfiability;computer graphics (images)	Graphics	-46.76563319156686	-28.215335268950387	168549
a60995cc4a966c998ffa8e02bbd2da609ffa9ef3	perceptually-motivated stereoscopic film grain	viewing algorithms;i 3 3 computer graphics picture image generation display algorithms viewing algorithms;i 3 3 computer graphics picture image generation display algorithms;categories and subject descriptors according to acm ccs	Independent management of film grain in each view of a stereoscopic video can lead to visual discomfort. The existing alternative is to project the grain onto the scene geometry. Such grain, however, looks unnatural, changes object perception, and emphasizes inaccuracies in depth arising during 2D-to-3D conversion. We propose an advanced method of grain positioning that scatters the grain in the scene space. In a series of perceptual experiments, we estimate the optimal parameter values for the proposed method, analyze the user preference distribution among the proposed and the two existing methods, and show influence of the method on the object perception.	2d to 3d conversion;3d film;experiment;stereoscopy	Krzysztof Templin;Piotr Didyk;Karol Myszkowski;Hans-Peter Seidel	2014	Comput. Graph. Forum	10.1111/cgf.12503	computer vision;computer science;multimedia;computer graphics (images)	Visualization	-35.966265428557264	-36.35823287138959	168621
bf07dfed6c9588f7237543549bdb0fd47d2ca8fa	sis-prueba a tool for rapid prototyping and testing of speech recognition user interfaces in telefónica móviles españa	service provider;user interface;usability testing;experimental design;life cycle;mobile telecommunication;use case;speech recognition;unified process;development process;information retrieval	"""SIS PRUEBA is a software tool to integrate usability and user-centred design principles in the development process of services within Telefónica Móviles España (TME), the largest mobile telecommunications operator in Spain. The successful deployment of complex services, from both users' and developer's points of view, require of a high degree of integration of a User Centered methodology with the design, development and usability tests in all the phases of the life cycle of services. An important example is Telefónica Móviles Portal de Voz (Voice Portal), a service providing information retrieval on a large number of issues using the voice or DTMF commands. With this objective, the purpose of SIS PRUEBA is to provide a single tool to integrate the characteristics of the unified processes of development (UP) with techniques of rapid prototyping like part of the usability methodology. Main benefits of SIS PRUEBA are: • Definition and management of assigned components to usability projects (services, functionalities, use cases, types of users and use contexts). • Creation of measurement libraries (questions, formats of answers and questionnaires). • Allows for the quick production of batteries of usability trials by means of different experimental designs. • Management of user panels for trials • Administration of usability trials in supports of """"paper and pencil"""" and/or remote (Internet). • Emulation of voice interfaces on flow charts in design time."""	chart;design of experiments;dual-tone multi-frequency signaling;emulator;flowchart;information retrieval;library (computing);programming tool;rapid prototyping;software deployment;speech recognition;usability;user interface;user-centered design	Pedro Concejero Cerezo;Juan José Rodríguez Soler;Daniel Tapias Merino	2004			software deployment;unified process;the internet;user interface design;human–computer interaction;usability;software;use case;computer science;user interface	HCI	-43.53137945229872	-29.416455817504428	168728
225ff20680dbce1274e5bf9ded6cc1f3f2035ec7	extensible immersive virtual environments for large tiled video walls		The intent of the work is to present an Immersive Virtual Environment (IVE) as a new abstracted management layer on top of the Scalable Adaptive Graphics Environment (SAGE) system, allowing the simplified management and linear scaling of multiple SAGE-driven video walls, and the creation and control of simple interactive scenarios. The framework exposes evolved Application Programming Interfaces (APIs) that are detached from the underlying system and can be used by mobile clients as well. Primitives offered to developers and content creators allow the definition of immersive cinematographic experiences using basic commands, which are synchronized on the whole IVE environment. A complete implementation of the system is described and then evaluated with the specific case of one physical installation.	application programming interface;client (computing);coherence (physics);computer cluster;data visualization;embedded system;experience;front and back ends;graphics;high- and low-level;home automation;image scaling;interaction;knx (standard);low-power broadcasting;mobile app;prototype;scientific visualization;server (computing);virtual reality	Lorenz Cuno Klopfenstein;Brendan Paolini;Gioele Luchetti;Alessandro Bogliolo	2016		10.5220/0005727803210328	application programming interface;computer science;scalability;computer graphics (images);simulation;graphics;immersion (virtual reality);human–computer interaction;virtual machine;extensibility	Visualization	-43.72855222799651	-34.8309886150667	168986
dcd03e4e9ce7a1d1820114875003a09e9f6d8783	separating hypertext content from structure in trellis			hypertext;trellis quantization	Richard Furuta;P. David Stotts	1989			multimedia;world wide web;computer science;hypertext	Web+IR	-45.25532489045337	-24.879062029196085	169097
1b8cd1c6bcba02bd0c4a197d0d05065b91a87967	designing user interfaces for a variety of users: possible contributions from model-based development schemes	modelo dinamico;developpement logiciel;sistema interactivo;model based reasoning;interfase usuario;raisonnement base sur modele;user interface;interaction;interaction style;dynamic model;model based approach;systeme conversationnel;interactive system;desarrollo logicial;contexto;modele dynamique;comportement utilisateur;software development;contexte;model based development;interface utilisateur;interaccion;interaction model;user behavior;context;comportamiento usuario;user model	As User Interfaces for All penetrate software applications, multi-dimensional design concepts become increasingly important. Both, for structured and user-oriented interface development, model-based approaches have turned out to be beneficial. However, most of these approaches remain vague with respect to the explicit representation of information about users and different modalities of interaction, as well as the structural and dynamic interfacing of user models to context and interaction models. However, these interfaces are required to provide different access possibilities for a functional core, and to allow switching between different modalities of interaction when serving a variety of users. In this paper we structure the requirements and evaluate existing model-based representation schemes against the structured set of requirements. The results reveal that model-based representation schemes should be enhanced through dedicated relationships and interface-management capabilities to mutually tune the models representing users, tasks, application-domain data, interaction styles and interactive devices.	user interface	Christian Stary	2002		10.1007/3-540-36572-9_7	interaction;simulation;user modeling;human–computer interaction;computer science;artificial intelligence;software development;model-based reasoning;machine learning;user interface;model-based design	HCI	-37.827526939244414	-26.54351908695768	169138
6dceefe3bee847acc16f9a7da823a7b49a93bdef	interactive interiors: preliminary study of integrating textile embellishment techniques and polymeric photonic fibers for interior textiles	embellishment;compact space;three dimensional;interactive;design;furnishings	Interactive interiors which are customizable to the design preferences and functional purposes of individual users can help create flexible interiors within fixed spaces. They are particularly relevant to densely populated cities where most people live in compact spaces. This proposed research explores interactive interior textile surfaces as an adaptive media which can transform interiors via the change of colors, luminescence and surface design. This is done by investigating the integration of textile base material via embellishment techniques, such as embroidery and felting, with polymeric photonic fibers (POF) which enhance tactile quality without compromising on the technological functionality. Their flexible application methods mean that, it is adaptable to the rigid nature of polymeric photonic fibers. In addition, embroidery has the unique ability to arrange and combine threads together with various materials in non-rigid formations and multiple directions to create stable two-dimensional and three-dimensional forms [1]. Different materials can be combined to explore aesthetically pleasing surface designs, textures and achieve positive tactile quality.	color;optical fiber;population;reliability engineering;texture mapping	Jeanne Tan;Zi-qian Bai;Xiao-ming Tao	2011		10.1145/2347504.2347542	structural engineering;design;engineering;compact space;management;engineering drawing	HCI	-37.106606596672876	-33.65380478132282	169221
36ec766e982311a318a77dcc3b611d558fa520a3	interactive view-dependent rendering with culling for articulated models in crowd scenes	volume matting;auto rotoscoping;crowd simulation;data capture;large scale;object tracking;user interaction	Owing to advances of data capture and modeling technologies, detailed articulated models are easily generated and widely used in many different applications. Moreover, various crowd simulation techniques have been designed and a high number of articulated models are frequently used in large-scale crowd scenes. In typical crowd scenes with lots of articulated characters, the rendering performance is one of the main bottlenecks in providing an interactive experience to users.	crowd simulation;interactivity	Dohyeong Kim;Pio Claudio;Tae-Joon Kim;Sung-Eui Yoon	2010		10.1145/1900354.1900414	computer vision;simulation;computer science;artificial intelligence;video tracking;crowd simulation;automatic identification and data capture;computer graphics (images)	Graphics	-38.906005437622994	-35.37398467906269	169269
f4ac87d9182bb1654139286d6ed568f9b37b0f09	adaptive generation of graphical information presentations in a heterogeneous telecooperation environment	information presentation			Thomas Rist;Matthias Hüther	1999			human–computer interaction;computer science;theoretical computer science;multimedia	NLP	-47.570763282093395	-32.33689646804452	169270
f93b30f9e60b969f808cd2dc3cc6cf5aafe37f96	automatic generation of user interface layouts for alternative screen orientations		Creating multiple layout alternatives for graphical user interfaces to accommodate different screen orientations for mobile devices is labor intensive. Here, we investigate how such layout alternatives can be generated automatically from an initial layout. Providing good layout alternatives can inspire developers in their design work and support them to create adaptive layouts. We performed an analysis of layout alternatives in existing apps and identified common realworld layout transformation patterns. Based on these patterns we developed a prototype that generates landscape and portrait layout alternatives for an initial layout. In general, there is a very large number of possibilities of how widgets can be rearranged. For this reason we developed a classification method to identify and evaluate “good” layout alternatives automatically. From this set of “good” layout alternatives, designers can choose suitable layouts for their applications. In a questionnaire study we verified that our method generates layout alternatives that appear well structured and are easy to use.		Clemens Zeidler;Gerald Weber;Wolfgang Stuerzlinger;Christof Lutteroth	2017		10.1007/978-3-319-67744-6_2	computer science;human–computer interaction;device independence;graphical user interface;mobile device;page layout;user interface	HCI	-40.537398955326374	-31.16925789073267	169782
e1f2f88290d4580980bf8452137478c0a6b05c92	introducing haptic interactions in web application modeling	web navigation;mice;graphical formalism;web pages;vibrations;web page haptic interaction tactile feedback user interaction web application web navigation graphical formalism;haptic device;force;graphical user interfaces;internet;web application;tactile feedback;haptic interfaces force rendering computer graphics three dimensional displays web pages mice vibrations;three dimensional displays;web page;haptic interfaces;rendering computer graphics;user interaction;internet graphical user interfaces haptic interfaces;haptic interaction;haptic interface	Haptic devices, providing tactile feedback to the user, by applying forces, vibrations, and/or motions, are becoming a common way of user interaction in several fields of applications, from gaming, to mobile, automotive, etc. This innovative technology could be thought as suitable also for Web navigation in the near future, with haptic devices replacing mice and offering to the user a more immersive way of interaction. In this work we first provide an overview of the characteristics of current haptic technologies. Then, we propose a high-level graphical formalism for the specification of haptic behaviors that can be associated to the objects/widgets of a Web page. Some examples of haptic interaction on the Web and issues in the implementation of an haptic interface for the Web are also provided.	document object model;embedded system;failure mode and effects analysis;graphical model;graphical user interface;haptic technology;high- and low-level;interaction;javascript;mcgurk effect;model-driven engineering;parsing;personalization;plug-in (computing);pointer (computer programming);pointer (user interface);rich internet application;semantics (computer science);usability;visual programming language;web 2.0;web application;web navigation;web page;webml;world wide web	Sara Comai;Davide Mazza	2010	2010 12th IEEE International Symposium on Web Systems Evolution (WSE)	10.1109/WSE.2010.5623571	web modeling;human–computer interaction;computer science;web page;multimedia;haptic technology;world wide web	Visualization	-42.80849189412424	-29.639743245261638	169852
00686a31db24cb6f96b06c4aeefadc8572ee4a13	a systems architecture for ubiquitous video	three dimensions;network mobility;user requirements;ubiquitous computing;physical environment;system architecture	Realityflythrough is a telepresence/tele-reality system that works in the dynamic, uncalibrated environments typically associated with ubiquitous computing. By harnessing networked mobile video cameras, it allows a user to remotely and immersively explore a physical space. RealityFlythrough creates the illusion of complete live camera coverage in a physical environment. This paper describes the architecture of RealityFlythrough, and evaluates it along three dimensions: (1) its support of the abstractions for infinite camera coverage, (2) its scalability, and (3) its robustness to changing user requirements.	algorithm;alpha compositing;automated planning and scheduling;dec alpha;requirement;scalability;systems architecture;television;ubiquitous computing;user requirements document;virtual camera system	Neil J. McCurdy;William G. Griswold	2005		10.1145/1067170.1067172	embedded system;three-dimensional space;real-time computing;simulation;computer science;user requirements document;operating system;ubiquitous computing	Mobile	-43.35591876455247	-34.59577605660038	170191
327c52393b0063bf546a0065711d70a3150b54d5	walk to here: a voice driven animation system	natural animation interface;equivalent capability;voice-driven system;voice-user interface;animation system;novel interface;button-driven animation interface;equivalent graphical user interface;motion capture data;informal user study;character animation;finite elements;graphic user interface;subdivision surfaces;polar decomposition;computer animation	We present a novel interface for directing the actions of computer animated characters and camera movements. Our system takes spoken input in combination with mouse pointing to generate desired character animation based on motion capture data. The aim is to achieve a more natural animation interface by supporting the types of dialogue and pointing that might be used when one person is explaining a desired motion to another person. We compare our voice-driven system with a button-driven animation interface that has equivalent capabilities. An informal user study indicates that for the test scenarios, the voice-user interface (VUI) is faster than an equivalent graphical user interface (GUI). Potential applications include storyboarding for film or theatre, directing characters in video games, and scene reconstruction.	computer animation;graphical user interface;motion capture;scene graph;storyboard;usability testing;voice user interface	Zengfu Wang;Michiel van de Panne	2006			character animation;computer vision;facial motion capture;simulation;computer facial animation;polar decomposition;skeletal animation;computer science;interactive skeleton-driven simulation;finite element method;graphical user interface;computer animation;multimedia;natural user interface;subdivision surface;computer graphics (images)	HCI	-43.68771353800014	-37.119938873141656	170222
00848003ffb1704262d6060e30c777cbe579d4d7	telida: a package for manipulation and visualization of timed linguistic data	language processing	TEDview, the viewing component, has some unique features such as  a film view that proceeds smoothly during play,  ability to process and display incremental data, visualizing the dynamics of speech processing and interpretation as it happens,  ability of online-visualization for speech processing applications (e.g. spoken dialogue systems), helping to debug and understand temporal aspects of system behaviour,  fine-grained control of visualization for intuitive use.	dialog system;smoothing;speech processing	Titus Malsburg;Timo Baumann;David Schlangen	2009		10.3115/1708376.1708419	natural language processing;computer science;database;programming language	Visualization	-40.094566113991704	-29.343623793600003	170255
bd8b3fe1bfc67f91ebf595e4c3812e148862f1a6	design of training platform for manned submersible vehicle based on virtual reality technology		Aiming at the problems of long training time, high cost and high risk existing in the deep working oceanauts, this paper, based on virtual reality technology, designed and developed the simulation system of diving and underwater operation process of Jiaolong which possesses multiple functions and good interactivity. Through the research on the motion model of A-frame swing, use Unity3D engine to develop the interactive simulation of diving and underwater operation process of Jiaolong after the 3D model of Jiaolong and mother ship was built by 3DMax. On the basis of giving full consideration to user experience, the real situation of diving and underwater operation process of Jiaolong was simulated, and the interactive manipulation function was realized.	autodesk 3ds max;interactivity;mathematical model;requirement;simulation;unity;user experience;virtual reality	Xiaoxi Zhang;Yong Yin	2018		10.1145/3205326.3205364	linguistics;interactivity;simulation;user experience design;underwater;computer science;virtual reality	Visualization	-42.92418190815715	-36.861878545439225	170406
f47133767f619657c6c321a00f50853dae1d1d61	low-overhead 3d items drawing engine for communicating situated knowledge	virtual instrument;augmented reality;knowledge representation;3d structure	This paper presents a low-overhead 3D items drawing engine for a situated knowledge medium aiming at improving the knowledge communication between experts and end-users of scientific instruments. The knowledge representation is based on the concept of Spatial Knowledge Quantum that associates a 3D geometry structure with knowledge. Our engine attempts to provide an effective means for creating those 3D structures. It consists of a hand-held Augmented Reality (AR) system and allows to directly draw, in the context of a subject instruments, free 3D lines. A line is recorded as a set of sample points. From these points a volume can optionally be interpolated by performing a Delaunay tetrahedralization. During the drawing operations, a physically concrete version of the instrument is not required. The AR interface shows the real-world combined, not only with the created 3D items, but also with a virtual representation of the instrument. A virtualized instrument offers many advantages such as availability, mobility, scalability, spatial freedom (e.g., we can draw through it) and rendering options.	situated	Loic Merckel;Toyoaki Nishida	2009		10.1007/978-3-642-04875-3_9	computer vision;augmented reality;simulation;computer science;artificial intelligence;machine learning;data mining;database;multimedia;world wide web	HCI	-35.9607956074145	-33.22752117224007	170438
17a6cc39762927e0f89e0a84806ab34476af5570	the look of the link - concepts for the user interface of extended hyperlinks	user interface;web;link marker;xlink;distributed hypertext	The design of hypertext systems has been subject to intense research. Apparently, one topic was mostly neglected: how to visualize and interact with link markers. This paper presents an overview of pragmatic historical approaches, and discusses problems evolving from sophisticated hypertext linking features. Blending the potential of an XLink-enhanced Web with old ideas and recent GUI techniques, a vision for browser link interfaces of the future is being developed. We hope to stimulate the development of a standard for hyperlink marker interfaces, which is easy-to-use, feasible for extended linking features, and more consistent than current approaches.	alpha compositing;graphical user interface;hyperlink;hypertext;marker interface pattern;xlink	Harald Weinreich;Hartmut Obendorf;Winfried Lamersdorf	2001		10.1145/504216.504225	computer science;web navigation;data mining;hyperlink;multimedia;user interface;world wide web	Web+IR	-43.396966534294236	-28.141253397384048	170481
775755aad0fb4ef4f8c7ecdc3a2e55541cf32243	"""erratum to """"a cognitive map simulation approach to adjusting the design factors of the electronic commerce web sites"""" [expert systems with applications 24 (2003) 1-11]"""	cognitive map;electronic commerce;expert system		cognitive map;e-commerce;expert system;simulation	Kun Chang Lee;Sangjae Lee	2003	Expert Syst. Appl.	10.1016/S0957-4174(03)00013-7	human–computer interaction;cognitive map;computer science;knowledge management;artificial intelligence;world wide web;expert system	AI	-45.1213814792769	-25.85167121729376	170633
9a03903ce449d8736ca058d7e6ebab7944083f8e	y-notes: unobtrusive devices for hypermedia annotation	navigation aids;hypertext annotation;world wide web;collaborative annotation	This paper describes y-notes , a light-weight, unobtrusive system that allows world wide web users to add persistent annotations to web-based hypermedia.	hypermedia;web application;world wide web	Saturnino Luz	2001		10.1145/504216.504227	image retrieval;computer science;unobtrusive javascript;semantic web;web navigation;web page;multimedia;world wide web;information retrieval	Web+IR	-43.00200682709543	-24.550796182616274	170760
f4fc6feae87e1da4dcdef4ccb302482c74b4230b	itheater puppets tangible interactions for storytelling	young children;integrable system;tangible interface;interactive environment;computer animation;tangible interaction	In this paper we present preliminary work on iTheater, an interactive integrated system for story-creation and storytelling, dedicated to young children. Based on the analogy with hand puppets’ theatre, the system aims to create an interactive environment where children will be able to give life to their imaginary characters by creating, editing and recording computer animations in a simple and exciting way, through the movement and tactile manipulation of traditional hand puppets. The system merges the familiarity of use of physical objects with the engaging richness of expression of sounds, images and animations. The iTheater is conceived as a creative flexible toolkit to create and tell stories, taking advantage of the new opportunities based on the multimedia and interactive technologies.	imaginary time;interaction	Oscar Mayora-Ibarra;Cristina Costa;Andrei Papliatseyeu	2009		10.1007/978-3-642-02315-6_11	integrable system;human–computer interaction;computer science;computer animation;multimedia;computer graphics (images)	HCI	-46.92523552494728	-35.83337531048216	170932
d372ac77219b864d4fbaf232fe8acc2a6e7c329d	an emotion generation model based on random graphs	modelizacion;animacion por computador;graphic method;mimica;random graph;emotion modelling;interfase usuario;dominance;humeur;realite virtuelle;realidad virtual;generic model;facies;personality;3d virtual humans;user interface;emotion generation;mimique;grafo aleatorio;personnalite;virtual reality;pad emotion space;intelligence artificielle;graphe aleatoire;probabilistic approach;grafismo;graphisme;3d facial expressions;man machine system;modelisation;aspect humain;methode graphique;dominancia;emotion emotionality;expression animation;enfoque probabilista;approche probabiliste;human aspect;personalidad;stochastic graphical models;humor;graphism;metodo grafico;3d facial expression animation;sistema hombre maquina;artificial intelligence;emotion emotivite;interface utilisateur;emocion emotividad;inteligencia artificial;human emotions;facial expression;aspecto humano;computer animation;human head;modeling;mood;systeme homme machine;animation par ordinateur	Emotions are an indispensable aspect in harmonious human–computer interaction and artificial intelligence. In this paper, we present an algorithm to generate emotions and show them with 3D facial expression animation. The Pleasure-Arousal-Dominance (PAD) emotion space is used to define the affective elements of exterior stimuli and interior emotional states. A stochastic graphical model is designed to represent the relationships between the emotion, the mood and the personality. Finally, we build a mapping from the emotion space to the facial expression space with a competitive learning network. The facial expression is vividly shown by a 3D virtual human head. The experimental results demonstrate that our emotion generation model works effectively and meets the basic principle of human emotion generation.	algorithm;artificial intelligence;competitive learning;computer vision;graphical model;human–computer interaction;machine learning;queueing theory;random graph;real-time transcription;regular expression;virtual actor	Chao-gang Wan;Jie-yu Zhao;Yuan-yuan Zhang	2010	IJCAT	10.1504/IJCAT.2010.034159	random graph;simulation;systems modeling;emotion;facies;computer science;artificial intelligence;affective computing;virtual reality;computer animation;dominance;personality;user interface;facial expression	AI	-35.18637980161661	-26.876509732388783	171001
1a789ddb30abe008a3a62995f78c1b56b3e29109	automatic stylistic manga layout	generative probabilistic model;stylistic layout;manga	Manga layout is a core component in manga production, characterized by its unique styles. However, stylistic manga layouts are difficult for novices to produce as it requires hands-on experience and domain knowledge. In this paper, we propose an approach to automatically generate a stylistic manga layout from a set of input artworks with user-specified semantics, thus allowing less-experienced users to create high-quality manga layouts with minimal efforts. We first introduce three parametric style models that encode the unique stylistic aspects of manga layouts, including layout structure, panel importance, and panel shape. Next, we propose a two-stage approach to generate a manga layout: 1) an initial layout is created that best fits the input artworks and layout structure model, according to a generative probabilistic framework; 2) the layout and artwork geometries are jointly refined using an efficient optimization procedure, resulting in a professional-looking manga layout. Through a user study, we demonstrate that our approach enables novice users to easily and quickly produce higher-quality layouts that exhibit realistic manga styles, when compared to a commercially-available manual layout tool.	encode;experience;fits;hands-on computing;mathematical optimization;usability testing	Ying Cao;Antoni B. Chan;Rynson W. H. Lau	2012	ACM Trans. Graph.	10.1145/2366145.2366160	multimedia;computer graphics (images)	Graphics	-37.39621949137528	-34.28017866832586	171112
b74e7c968e23275e1c1c2c03bb1fb401288a0eb8	moori: interactive audience participatory audio-visual performance	live audiovisual performance;story telling;algorithmic sound and graphics;generic algorithm;real time;text input;dynamic narrative;audio visual;audience participatory performance;collaborative	Moori is a hybrid form of an audience participatory dynamic narrative and an audio-visual performance system. By incorporating a smart phone's dynamic interface and SMS, users share their thoughts to proposed questions by a performer. Through text inputs, buttons, and multi-touch pads, user-data is processed to generate algorithmic audio and visuals. The result is a collaboration between performer and audience members, a real-time audio-visual composition and a dramatic narrative. As authors, viewers transform the content in a collective narrative space.	multi-touch;real-time cmix;smartphone	Haeyoung Kim	2011		10.1145/2069618.2069737	simulation;genetic algorithm;computer science;multimedia	HCI	-47.94278462644206	-34.54361289270667	171222
ccaced4b433cbd2ca900b9c7164a15c0cf411fdb	design of a flexible hardware interface for multiple remote electronic practical experiments of virtual laboratory	ajax;web interface;vlan	The objective of this work is to present a new design of a Flexible Hardware Interface (FHI) based on PID control techniques to use in a virtual laboratory. This flexible hardware interface allows the easy implementation of different and multiple remote electronic practical experiments for undergraduate engineering classes. This interface can be viewed as opened hardware architecture to easily develop simple or complex remote experiments in the electronic domain. The philosophy of the use of this interface can also be expanded to many other domains as optic experiments for instance. It is also demonstrated that software can be developed to enable remote measurements of electronic circuits or systems using only Web site Interface. Using standard browsers (such as Internet explorer, Firefox, Chrome or Safari), different students can have a remote access to different practical experiments at a time.	electrical connector	Said Farah;Abdelhalim Benachenhou;Guillaume Neveux;Denis Barataud	2012	iJOE		ajax;embedded system;human–computer interaction;computer science;operating system;virtual lan;user interface;world wide web	Arch	-44.05386447209233	-28.645350132074114	171267
c2c42b1002ff8c0035a81f82105f29302a460a1b	the new standard ieee 1599, introduction and examples	symbolic representation of music;performance indicator;standards for music;music layers;indexing terms;music encoding;ieee;graphical representation;xml;computer application;historical data;symbolic representation	IEEE 1599 is a new standard to encode music with XML symbols. It offers two important original characteristics compared to existing standards of the worlds of music, musicology and computer applications to this art and science. On one side, the encoding is in the form of symbols that can be read both by machines and humans. On the other, it allows the realization of applications in which all aspects of music, such as audio and sound, graphical representation, historical data, performance indications, represented thanks to the new concept of layers, are fully integrated, synchronized and can be accessed both individually and as parts of a whole. The article will give a brief description of the standard and of applications that have been built to show its power.	computer;encode;graphical user interface;humans;symbol (formal);xml	Denis L. Baggi;Goffredo Haus	2009	Journal of Multimedia	10.4304/jmm.4.1.3-8	xml;speech recognition;index term;computer science;theoretical computer science;performance indicator;machine learning;multimedia;pop music automation;world wide web	Visualization	-46.70511183445393	-25.821300861167355	171271
78ebb6ef30fad8ea103f21b0ad08a48408f1d692	the architectural evolution of a high-performance graphics terminal	high performance graphics terminal;design engineering;application software;tektronix 4115b;computer graphics;computer graphic equipment;special purpose hardware;thumb;microcoded picture processor;interactive terminals;architectural evolution;displays;displays computer graphics product design design engineering job design velocity measurement productivity vacuum technology application software thumb;vacuum technology;job design;productivity;product design;velocity measurement	By using a microcoded picture processor, Tektronix engineers were able to meet higher performance requirements within the tight constraints set by an existing family of terminals.	computer terminal;graphics;microcode;requirement	Douglas J. Doornink;John C. Dalrymple	1984	IEEE Computer Graphics and Applications	10.1109/MCG.1984.276144	productivity;application software;simulation;job design;computer science;operating system;product design;computer graphics;computer graphics (images)	Visualization	-47.7088886020063	-29.061062609867864	171563
9048992fc7ff5e9821a20b66d501eb16c12b0bac	design of multimedia semantic presentation templates: options, problems and criteria of use	multimedia templates;user tasks;automatic generation;model based user interface design;user interface design;multimedia presentation;query result presentation	This paper presents and discusses the use of Semantic Multimedia Presentation Templates which capture presentation structures that are suitable for communicating semantic data relationships occurring across many applications. The identification of the space of the possible presentation templates along with criteria to evaluate them that depend on the current task, data cardinality and other aspects, is very useful for supporting the automatic generation of effective query result multimedia presentations.		Nicola Aloia;Tullio Bendini;Fabio Paternò;Carmen Santoro	1998		10.1145/948496.948524	user interface design;human–computer interaction;computer science;database;multimedia;world wide web	SE	-40.78180088736608	-27.579615274161352	171700
f86848485da3863f7b8d5a1a91de34330f0bdda8	3d design and simulation of men garments	particle-based model.;clothing design and simulation;industrial application;3d graphics	This paper outlines a 3D graphic environment to design and simulate men garments according to fabric properties and manufacturing processes. The aim is to permit the design in 3D of men base garments, in particular jackets, together with evaluation of their styles and automatic generation of 2D patterns from the 3D representation. 3D garment design has been based on the use of MAYA? (Alias/Wavefront) Deformers, while simulation relies on particle-based approach. Main modules of the system are described as well as methodologies and techniques adopted. The prototype has been experimented by end-user; results and final considerations are reported.	3d computer graphics;alias systems corporation;autodesk maya;prototype;simulation	Umberto Cugini;Caterina Rizzi	2002			computer graphics (images);artificial intelligence;computer vision;3d computer graphics;clothing;simulation;computer science;alias	EDA	-40.478627128851926	-32.32448474340358	171701
48e1ac8d8f6b3fe9b2f1e739b754bc86ee5969e9	incorporating 3d sound in different virtual worlds	computer graphics;3d sound virtual world;computer aided instruction;3d sound;virtual reality;virtual environments navigation interference educational institutions portable computers visualization;computer science education;virtual reality computer aided instruction computer graphics computer science education;virtual world;computer engineering incorporating 3d sound different virtual worlds virtual tools computer human interaction computer graphics courses 3d graphics	"""The use of virtual tools in a university level results in a series of benefits. A virtual world allows the students to interact with modern technology, to learn with different tools, etc. This project was developed to be used in the Interaction Computer-Human and Computer Graphics courses, due to different manipulations kind, and the combination 3D graphics and sound, at the Bachelor in Computer Engineering at the Autonomous University of Tlaxcala. Specifically, this paper focuses on the behavior of sound on combining: """"Navigation, 3D objects and 3D sound"""", in different virtual worlds. Our project allows building different virtual environment multi-screen, with a full navigation system, four ways of manipulating: keyboard, head tracker, remote control and input files with different predefined tours. Furthermore, with this project we can also build a Wheat stone-type stereoscope. We have implemented different virtual worlds for the different physic environments, which use different sound sources."""	3d computer graphics;autonomous robot;computer engineering;interference (communication);regular expression;remote control;stereoscope;surround sound;virtual reality;virtual world;wheatstone bridge	Marva Angélica Mora Lumbreras;Leticia Flores-Pulido;Brian Manuel González-Contreras;Edgar Alberto Portilla-Flores	2012	2012 Eighth Latin American Web Congress	10.1109/LA-WEB.2012.17	human–computer interaction;computer science;artificial intelligence;instructional simulation;virtual reality;multimedia;computer graphics;3d computer graphics;computer graphics (images)	Visualization	-44.177749616847414	-35.161401824417176	171846
65c81e3a27bc0795885f7aa4eef73ff7bcd50533	screen-area coherence for interactive scanline display algorithms	software;concepcion asistida;coherence application software user interfaces graphics solid modeling computer displays aerospace simulation;computer aided design;affichage graphique;logiciel;application software;user interface;mode conversationnel;interactive mode;aerospace simulation;graphic display;algorithme;algorithm;hidden surface removal;algorritmo;boolean operation;elimination surface cachee;modo conversacional;surface model;solid modeling;computer displays;constructive solid geometry;conception assistee;facility layout;coherence;logicial;visualizacion grafica;user interaction;user interfaces;graphics	This article proposes and demonstrates a technique enabling polygon-based scanline hidden-surface algorithms to be used in applications that require a moderate degree of user interaction. Interactive speeds have been achieved through the use of screen-area coherence,a derivative of frame-to-frame coherence and object coherence. This coherence takes advantage of the face that most of the area of the screen does not change from one frame to the next in applications that have constant viewing positions for a number of frames and in which a majority of the image remains the same. One such application, the user interface of constructive solid geometry (CSG) based modelers, allows a user to modify a model by adding, deleting, repositioning, and performing volumetric Boolean operations on solid geometric primitives. Other possible applications include robot simulation, NC verification, facility layout, surface modeling, and some types of animation. In this article, screen-area coherence is used as the rationale for recalculating only those portions of an image that correspond to a geometric change. More specifically, this article describes a scanline hidden-surface removal procedure that uses screen-area coherence to achieve interactive speeds. A display algorithm using screen-area coherence within a CSG-based scanline hidden-surface algorithm was implemented and tested. Screen-area coherence reduced the average frame update time to about one quarter of the original time for three test sequences of CSG modeling operations.	algorithm;central processing unit;constructive solid geometry;deployment environment;design rationale;display device;freeform surface modelling;hidden surface determination;microcomputer;scan line;scanline rendering;shading;simulation;user interface;wire-frame model;workstation	Gary A. Crocker	1987	IEEE Computer Graphics and Applications	10.1109/MCG.1987.277050	computer vision;simulation;computer science;artificial intelligence;operating system;geometry;user interface;constructive solid geometry;algorithm;computer graphics (images)	Graphics	-37.557502322747624	-30.73604027894386	171973
0f7f92e8c46f872622c1bbea4800fff392661519	computer animation: a new application for image-based visual servoing	virtual reality;video game;computer graphic;computer vision;image sequences computer animation virtual reality;animation application software visual servoing robot vision systems computer graphics cameras automatic control virtual environment computer vision layout;animation;asservissement;humanoid avatars image based visual servoing virtual camera virtual environment highly reactive applications video games;robotique;virtual environment;computer animation;visual servoing;image sequences	This paper presents a new application for image-based visual servoing: computer graphics animation. Indeed, the control of a virtual camera in virtual environment is not a trivial problem and usually required skilled operators. Visual servoing, a now well known technique in robotics and computer vision, consists in positioning a camera according to the informations perceived in the images. Using this method within computer graphics context leads to a very intuitive approach of animation. Furthermore, in that case a full knowledge about the scene is available. It allows to easily introduce constraints within the control law in order to react automatically to modifications of the environment. In this paper, we apply this approach in two different contexts: highly reactive applications (virtual reality, video games) and the control of humanoid avatars.	avatar (computing);computer animation;computer graphics;computer vision;optimal control;robotics;virtual camera system;virtual reality;visual servoing	Nicolas Courty;Éric Marchand	2001		10.1109/ROBOT.2001.932557	3d reconstruction;anime;computer vision;computer-generated imagery;computer facial animation;skeletal animation;computer science;virtual machine;artificial intelligence;interactive skeleton-driven simulation;virtual reality;computer animation;multimedia;color cycling;visual servoing;computer graphics (images)	Graphics	-39.188233793298366	-36.88550745591762	172085
0bd3c416ef1c678c1b6cb05bdb19a0c8d1576318	dorothy: enhancing bidirectional communication between a 3d programming interface and mobile robots	computational thinking;robotics;artificial intelligence;educational tool	Dorothy is an integrated 3D/robotics educational tool created by augmenting the Alice programming environment for teaching core computing skills to students without prior programming experience. The tool provides a drag and drop interface to create graphical routines in virtual worlds; these routines are automatically translated into code to provide a real-time or offline enactment on mobile robots in the real world. This paper summarizes the key capabilities of Dorothy, and describes the contributions made to: (a) enhance the bidirectional communication between the virtual interface and robots; and (b) support multirobot collaboration. Specifically, we describe the ability to automatically revise the virtual world based on sensor data obtained from robots, creating or deleting objects in the virtual world based on their observed presence or absence in the real world. Furthermore, we describe the use of visually observed behavior of teammates for collaboration between robots when they cannot communicate with each other. Dorothy thus helps illustrate sophisticated algorithms for fundamental challenges in robotics and AI to teach advanced computing concepts, and to emphasize the importance of computing in real world applications, to beginning programmers.	algorithm;drag and drop;emoticon;graphical user interface;integrated development environment;mobile robot;online and offline;programmer;real-time locating system;robotics;sensor;virtual world	Emilie Featherston;Mohan Sridharan;Susan Darling Urban;Joseph E. Urban	2014			simulation;human–computer interaction;computer science;artificial intelligence;machine learning;robotics	HCI	-43.29593232576895	-33.69420215019107	172264
236f592df88187ce31bc82d39a2d9ff31f3c51ef	using emotion transmission to simulate spectator behaviors	pad space;heat conduction;crowd simulation;emotion transmission;affective computing	In this paper, we introduced a novel method to simulate moving spectator crowd behaviours. Different from the common methods, we taken the emotional affections between spectators into consideration and calculated these affections by heat conduction model. The generation loops can be divided into three steps. We firstly exploited PAD space to build spectators' emotional states. Then we updated spectators' positions and generated their emotional affections by emotion transmission equations which were formulated from heat conduction equations. We called this step dynamic emotion transmission. At last, we mapped their emotions to actions. Using this algorithm, we can simulate more realistic crowd behaviours.	algorithm;simulation	Nan Xiang;Zhigeng Pan;Mingmin Zhang	2012		10.1145/2407516.2407531	simulation;computer science;artificial intelligence;crowd simulation;affective computing;thermal conduction	AI	-37.73569562332925	-37.03372835813766	172275
9cbaf9ccd65b3bcafaca5000ca79a3fd9ca90038	automated interior design using a genetic algorithm		In this paper, we present a system that automatically populates indoor virtual scenes with furniture objects and optimizes their positions and orientations with respect to aesthetic, ergonomic and functional rules called interior design guidelines. These guidelines are represented as mathematical expressions which form the cost function. Our system optimizes the set of multiple interior designs by minimizing the cost function using a genetic algorithm. Moreover, we extend the optimization to transdimensional space by enabling automatic selection of furniture objects. Finally, we optimize the assignment of materials to the furniture objects to achieve a unified design and harmonious color distribution. We investigate the capability of our system to generate sensible and livable interior designs in a perceptual study.	fundamental fysiks group;genetic algorithm;human factors and ergonomics;loss function;mathematical optimization;population;sandy bridge	Peter Kán;Hannes Kaufmann	2017		10.1145/3139131.3139135	genetic algorithm;simulation;computer science;expression (mathematics);interior design	Graphics	-37.124947947818775	-34.101730210630244	172557
3947d452dd430f97d5dbae11806c81da8be010e4	from technological investigation and software emulation to music analysis: an integrated approach to barry truax's riverrun		This paper presents an approach to studying Barry Truax’s Riverrun as it is being carried out within the TaCEM project (Technology and Creativity in Electroacoustic Music), a collaboration between the Universities of Huddersfield and Durham funded for 30 months (2012-2015) by the Arts and Humanities Research Council in the United Kingdom. This approach aims at realising an Interactive Aural Analysis with which the user can explore the creative and technological environment used by the composer to build his oeuvre, as well as navigate aurally through the results of the musicological study. It involves an important technological investigation of Truax’s GSX program for digital granular synthesis, leading to the implementation, in the Max environment, of emulation software allowing for the live recreation of each of Riverrun’s sequences, along with further tools dedicated to the musical analysis of the piece. This paper presents the technological investigation and its issues, the pieces of software for the Interactive Aural Analysis of the work, and musicological observations drawn from		Michael Clarke;Frédéric Dufeu;Peter D. Manning	2014				SE	-47.49482990638087	-33.41406467650779	172584
4bbb2ffda5bce2176dd9cc1a8f464511503bd246	exploring melody space in a live context using declarative functional programming	declarative programming;communicating sequential processes;osc;assisted composition;clojure	This paper introduces Composer, a system offering composition capabilities for live performance, requiring no prior experience with composition and programming. Current research in computer assisted composition is focused on offline composition. A composer is seen as a person that composes pieces of music which are then performed at a later date, either by the composer or an artist. There has been work done in computer assisted live performance, but the focus in that field has mainly been on the live generation of synthesizers and novel, virtual instruments and musical interfaces. Unlike existing systems, Composer is intended to be used in a live context for the composition of novel melodies. The system makes no assumptions about the user's existing experience as a composer or a programmer. Instead of giving the user unbounded freedom, the system only allows the user to manipulate key properties of the desired melodies. The constraints the user can put on the melodies are the scale or mode in which the melody is set; the tonic note of the scale or mode; the cadence of the melody; the tempo of the melody; and the relative gap-size between notes in the melody. These rules are modelled using a declarative programming model that also supports automatic enumeration of the space of valid melodies. As complete enumeration of this search space is infeasible in a live context, experiments have been performed and their results are presented, to limit the size of the enumerated space while still yielding sufficient variation in the composed pieces. Furthermore, the general system design is presented and it is discussed how choices concerning the inter-communication between components in the system helps the system to be responsive and usable in a live composition context.	composer;declarative programming;experiment;functional programming;online and offline;programmer;programming model;systems design	Thomas Greve Kristensen	2014		10.1145/2633638.2633646	computer science;artificial intelligence;communication;algorithm	PL	-46.64007919581362	-35.74088791281705	172629
387fc4666ba5b307bdd81003f13b34aaeaf0f326	the virtual raft project: a mobile interface for interacting with communities of autonomous characters	mobile device;virtual characters;autonomous characters;tangible interface;interactive animation;mobile interfaces;intuitive interfaces;virtual environment;virtual space;tangible interfaces;mobile devices;animated character	This paper presents a novel and intuitive paradigm for interacting with autonomous animated characters. This paradigm utilizes a mobile device to allow people to transport characters among different virtual environments. The central metaphor in this paradigm is that virtual space is like land and real space is like water for virtual characters. The tangible interface described here serves as a virtual raft with which people may carry characters across a sea of real space from one virtual island to another. By increasing participants' physical engagement with the autonomous characters, this interaction paradigm contributes to the believability of those characters.	autonomous robot;computer animation;interaction;mobile device;programming paradigm;tangible user interface;virtual reality	Bill Tomlinson;Man Lok Yau;Jessica O'Connell;Ksatria Williams;So Yamaoka	2005		10.1145/1056808.1056857	human–computer interaction;computer science;operating system;mobile device;multimedia;computer graphics (images)	Visualization	-44.72136357631938	-36.84778714201764	172941
554a357b08f746a40b008e127095e7e87665ad15	in-place 3d sketching for authoring and augmenting mechanical systems	three dimensional virtual scenes;in place augmented reality;mechanical systems psychology breast educational institutions assembly visualization education object detection output feedback virtual reality;in place 3d sketching;hit;technology;lab;hand sketching;sketch interaction in place 3d sketching mechanical systems authoring mechanical systems augmenting three dimensional virtual scenes augmented reality hand sketching trihedral solid models;sketch interaction;geometry;computational geometry;dual perception;3d content authoring;three dimensional;trihedral solid models;visualization;visual languages;hitlab;dual perception in place augmented reality free hand sketching augmented reality 3d content authoring physical simulation interaction by sketching visual language;three dimensional displays;image reconstruction;pixel;visual languages augmented reality computational geometry image reconstruction;solid modeling;interface;human;free hand sketching;visual language;nz;mechanical systems augmenting;augmented reality;interaction by sketching;mechanical systems;physical simulation;mechanical systems authoring	We present a framework for authoring three-dimensional virtual scenes for Augmented Reality (AR) which is based on hand sketching. Sketches consisting of multiple components are used to construct a 3D virtual scene augmented on top of the real drawing. Model structure and properties can be modified by editing the sketch itself and printed content can be combined with hand sketches to form a single scene. Authoring by sketching opens up new forms of interaction that have not been previously explored in Augmented Reality. To demonstrate the technology, we implemented an application that constructs 3D AR scenes of mechanical systems from freehand sketches, and animates the scenes using a physics engine. We provide examples of scenes composed from trihedral solid models, forces, and springs. Finally, we describe how sketch interaction can be used to author complicated physics experiments in a natural way.	3d modeling;adobe freehand;augmented reality;central processing unit;diagram;digital camera;dynamical simulation;experiment;interaction technique;interpretation (logic);mobile phone;physics engine;printing;seamless3d	Oriel Bergig;Nate Hagbi;Jihad El-Sana;Mark Billinghurst	2009	2009 8th IEEE International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2009.5336490	computer vision;augmented reality;visualization;computational geometry;computer science;interface;multimedia;solid modeling;pixel;computer graphics (images);technology	Visualization	-41.68148903347756	-37.504805483459776	173063
a7f06ec22692b9d1929136de17fc920774fd4981	an independent motion controlling method towards virtual 3d scene	computational mechanics;motion control;virtual reality;virtual reality computer animation solid modelling;virtual 3d scene analysis independent motion controlling method path search 3d model delta skeletal animation motion control general computing mechanism;animation sequence artificial intelligence virtual reality;animation motion control three dimensional displays real time systems legged locomotion mobile communication solid modeling;animation sequence;artificial intelligent;3d model;artificial intelligence;3d scene analysis;computer animation;high efficiency;solid modelling	In view of the 3D virtual scene for the current self-motion method in the general lack of high-efficiency issues, an independent motion controlling method towards virtual 3D scene is proposed. In motion control, under the guidance of path from path search, combining with the 3D model are Delta skeletal animation to generate a moving process with smooth and vivid action. Compared with the traditional methods, this method does not depend on the motion control template, realizes motion control general computing mechanism completely based on virtual 3D scene analysis. Experimental results show that the proposed method is effective.	real-time locating system;skeletal animation;virtual reality	Yanjun Li;Yongwei Mao	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023939	motion control;computer vision;simulation;computer facial animation;skeletal animation;computer science;artificial intelligence;computational mechanics;interactive skeleton-driven simulation;virtual reality;computer animation;computer graphics (images)	Robotics	-39.04168994447686	-36.9302889509608	173083
b1a14253e03b68f1ff0b18e5113fca2e284cba4d	audiovisual analysis of music performances: overview of an emerging field		In the physical sciences and engineering domains, music has traditionally been considered an acoustic phenomenon. From a perceptual viewpoint, music is naturally associated with hearing, i.e., the audio modality. Moreover, for a long time, the majority of music recordings were distributed through audio-only media, such as vinyl records, cassettes, compact discs, and mp3 files. As a consequence, existing automated music analysis approaches predominantly focus on audio signals that represent information from the acoustic rendering of music.		Zhiyao Duan;Slim Essid;Cynthia C.S. Liem;Gael Richard;Gaurav Sharma	2019	IEEE Signal Processing Magazine	10.1109/MSP.2018.2875511	artificial intelligence;computer vision;rendering (computer graphics);task analysis;computer science;signal processing;visualization;perception;music theory;audio signal;phenomenon	Web+IR	-46.45174990991921	-34.00623090591954	173337
125840a670b59bbe022678da97a52b57ff9bbe9b	virtual pottery: an interactive audio-visual installation	audio visual	Virtual Pottery is an interactive audiovisual piece that uses hand gesture to create 3D pottery objects and sound shape. Using the OptiTrack motion capture (Rigid Body) system at TransLab in UCSB, performers can take a glove with attached trackers, move the hand in x, y, and z axis and create their own sound pieces. Performers can also manipulate their pottery pieces in real time and change arrangement on the musical score interface in order to create a continuous musical composition. In this paper we address the relationship between body, sound and 3D shapes. We also describe the origin of Virtual Pottery, its design process, discuss its aesthetic value and musical sound synthesis system, and evaluate the overall experience.	apache axis;interactivity;motion capture;real-time computing	Yoon Chung Han;Byeong-jun Han	2012			visual arts;engineering;multimedia;computer graphics (images)	HCI	-44.57516045758748	-35.4241848492757	173456
630cd9c041e86f4d751955af9dd20a0d3aeb1300	gcubik+i virtual 3d aquarium: interfacing a graspable 3d display with a tabletop display		We propose gCubik+i as a new interactive platform that naturally interfaces a 3D display with a tabletop display. The proposed platform is suitable for group collaboration and it introduces two novel interaction paradigms to existing tabletop display applications: 1) natural switching between the shared working spaces of the table and the users’ hands; and 2) transforming static 2D images into interactive 3D images that can be viewed and manipulated as if holding a real object. This paper describes the conceptual design and prototype implementation of the gCubik+i platform along with a description of its 3D virtual aquarium application.	prototype;stereo display	Roberto Lopez-Gulliver;Shunsuke Yoshida;Mao Makino;Sumio Yano;Hiroshi Ando	2010		10.2312/egsh.20101044	human–computer interaction;engineering;multimedia;computer graphics (images)	HCI	-42.64504476020086	-36.89603531017999	173755
4b9e6d9c22cc45fb59250293c4f103a1ff9ef6a1	virtual environments with four or more spatial dimensions	high dimensionality;three dimensions;degree of freedom;real time;texture mapping;head tracking;visual quality;three dimensional;computer graphic;interactive display;user experience;higher dimensions;cross section;virtual environment;head mounted display	We describe methods for displaying complex, texturemapped environments with four or more spatial dimensions that allow for real-time interaction. At any one moment in time, a three-dimensional cross section of the high-dimensional environment is rendered using techniques that have been implemented in OpenGL. The position and orientation of the user within the environment determine the 3-D cross section. A variety of interfaces can be used to control position and orientation in 4-D, including a mouse freelook interface for use with a computer monitor display, and an interface that uses a head-tracking system with three degrees of freedom and PINCH gloves in combination with a head-mounted display. The methods avoid the use of projections that require depth buffering in greater than three dimensions and can be used in conjunction with either 2-D or 3-D texture mapping. A computer graphic engine that displays 4-D virtual environments interactively uses these methods, as does a level editor and modeling program that can be used to create 4-D environments.	3d computer graphics;computer monitor;cross section (geometry);free look;head-mounted display;interactivity;level editor;opengl;real-time clock;texture mapping;tracking system;virtual reality;z-buffering	Michael D'Zmura;Philippe Colantoni;Gregory Seyranian	2000	Presence: Teleoperators & Virtual Environments	10.1162/105474600300040411	three-dimensional space;computer vision;user experience design;simulation;human–computer interaction;computer science;computer graphics (images)	Graphics	-40.947380151575125	-37.574330804007595	174002
e340d29ab651ce18ed10277744726127e7d2ea5d	an intelligent user interface with motion planning for 3d navigation	user interfaces navigation virtual reality graphics layout mice computer science us department of transportation java acceleration;intelligent user interface;3d navigation;personal computer;virtual reality languages;path planning;maze like environment intelligent user interface motion planning 3d navigation control graphics hardware interactive 3d graphics desktop personal computers mouse path planner frame rate architectural walkthrough applications probabilistic roadmap unnecessary maneuvers collisions java3d implementation vrml browser checkpoint sequence traversal;3d navigation control;graphical user interfaces;graphics hardware;randomized roadmap;rapid evolution;architecture path planning virtual reality languages computerised navigation interactive devices graphical user interfaces artificial intelligence;motion planning;vrml;artificial intelligence;user equipment;architecture;3d graphics;interactive devices;computerised navigation	Due to the rapid evolution of graphics hardware, interactive 3D graphics is becoming popular on desktop personal computers. However, it remains a challenging task for a novice user equipped with a 2D mouse to navigate in an architectural environment efficiently. We think the problem is partly due to the fact that precise navigation control is difficult to achieve with low frame rates. In this paper, we propose a novel approach to improve the effectiveness and efficiency of 3D navigation for architectural walkthrough applications. We adopt a path planner with probabilistic roadmap to help users avoid unnecessary maneuvers due to collisions with the environment. We modify a Java3D implementation of VRML browser to incorporate the path planner into the user interface. Experiments show that our implementation of path planner is very efficient and can be seamlessly incorporated into the navigation control loop. The overall navigation time for traversing a sequence of checkpoints in a maze-like environment can be improved by about a factor of two if the intelligent user interface is used.	3d computer graphics;cognitive walkthrough;control system;desktop computer;direct manipulation interface;experiment;graphics hardware;high- and low-level;intelligent user interface;java 3d;motion planning;personal computer;probabilistic roadmap;randomized algorithm;software walkthrough;vrml;workspace	Tsai-Yen Li;Hung-Kai Ting	2000		10.1109/VR.2000.840496	turn-by-turn navigation;computer vision;simulation;human–computer interaction;computer science;artificial intelligence;operating system;motion planning;mobile robot navigation;3d computer graphics;computer graphics (images)	Robotics	-38.90143002322751	-35.32510879080047	174138
0803d97fd7b04e388e726fe4edd8b6d674454796	centenas de razões para achar o jems difícil	user interface description languages;user interface specification methods and languages;user interface components;xml based user interface languages;model based user interface development;browser based user interfaces			Eduardo Hideki Tanaka;Ariel Vargas;André Constantino Silva;Heloisa Vieira da Rocha	2006		10.1145/1298023.1298075	user interface design;user;shell;human–computer interaction;natural language user interface;natural user interface;programming language;user interface;world wide web;graphical user interface testing;multiple document interface	Vision	-42.45467022413587	-29.331530739988562	174199
96909930c35b865a2e5f2839ea2d46708a2c9110	a simplified model for generating 3d realistic sound in the multimedia and virtual reality systems	sound localization;virtual reality	It is a key feature to embed 3D realistic sound effect in the future multimedia and virtual reality systems. Recent research on acoustics and psychoacoustics reveals the important cues for sound localization and sound perception. One promising approach to generate 3D realistic sound effect uses two earphones by simulating the sound waveforms from sound source to eardrum. This paper summarizes two methods for generating 3D realistic sound and points out their inherent drawbacks. To overcome these drawbacks we propose a simplified model to generate 3D realistic sound at any positions in the horizontal plane based on the results of sound perception and localization. Experimental results show that the model is correct and efficient.	3d computer graphics;covox speech thing;headphones;internationalization and localization;psychoacoustics;simulation;virtual reality	Yu Zhao;Qiong Zhang;Hui Xiang;Jiaoying Shi;Zhijun He	1996	Journal of Computer Science and Technology	10.1007/BF02947213	physical modelling synthesis;simulation;sound localization;computer science;directional sound;virtual reality	Graphics	-45.5619361265508	-34.828351008437345	174358
6f09001e1ccc16b2fa8383886c5128f48959c238	forms of expression for designing visual languages for animation	universal access;programming language;animation system;animation sketching systems visual language design creative expression;computer animation visual programming visual languages;animation taxonomy user interfaces software tools batteries chemistry;software engineering;visual programming;visual languages;visual language;pen input;computer animation;field study	We present further steps in our research into visual languages for animation. Animation is a rich mode of communication that is currently accessible to few, because animation systems are complex. Some systems try to make animation simple but put severe limits on users’ creative expression. Our field studies are demonstrating that would-be animators need to express animation in a wide variety of ways. We are developing a taxonomy of forms of expression for animation that will help the designers of visual languages for animation to determine which expressive forms to support. Our end goal is to build animation sketching systems that use pen input to make animation universally accessible.	computer accessibility;taxonomy (general)	Richard C. Davis;James A. Landay	2005		10.1109/VLHCC.2005.39	computer facial animation;skeletal animation;computer science;interactive skeleton-driven simulation;non-photorealistic rendering;computer animation;multimedia;programming language;computer graphics (images)	Graphics	-41.98269114143023	-31.709419425361734	174569
0cad2e33df309328be6c8f45a4b06e4148b13002	comparison of 3d measurement techniques in cultural heritage application: user point of view	3d laser scanner;manuals;3d touch probe;measurement techniques;computer graphics;optical scanners;3d photographer;cultural heritage;photography;laser ranging surveying humanities photography optical scanners computer graphics;heritage related object;testing;shape measurement;laser ranging;three dimensional;probes;metalworking machines;measurement techniques cultural differences shape measurement testing probes manuals costs metalworking machines tiles buildings;photographic texturing;3d measurement techniques;humanities;surveying;3d surveying;manual survey;planning;tiles;point of view;3d measurement techniques three dimensional surveying 3d surveying manual survey photographic texturing 3d touch probe 3d laser scanner heritage related object architect 3d photographer planning;survey methods;three dimensional surveying;3d measurement;architect;buildings;cultural differences	This paper summarizes the results from a PhD thesis concerned with the comparison between traditional and new three-dimensional (3D) surveying techniques from a user point of view. Three types of 3D surveying methods were tested. The three methods are manual survey combined with photographic texturing, 3D touch probe and 3D laser scanner. A heritage-related object was picked according to a given method. For a user point of view, e.g. an architect or a 3D photographer, the knowledge of these different methods is fundamental in order to match a given object to be surveyed with the optimal survey procedure that will take full advantage of the data that the survey supplies to that user. Obviously the knowledge of the different methods is essential but so is the purpose of the project itself and the planning phase.	3d computer graphics;3d printing;list comprehension;point of view (computer hardware company);texture mapping	Francesca Pozzi	2002		10.1109/TDPVT.2002.1024156	visual arts;computer vision;engineering;computer graphics (images)	AI	-36.54149395480505	-35.08479779594688	174642
b364e2d49e46f10ee6dc9b5ea6156c3da79122cb	soundanism: blurred by breathing		Soundanism is an interactive sound installation that explores possible relationships between breathing, feedback, masturbation, music and sex. The soundanist wears a uniquely designed hatphone, equipped with loudspeakers and breathes into a mask connected to a microphone. A computer analyses his breathing in real time, and this input data are used as parameters for the selection of sounds and their real time manipulation. The work could also be considered a musical (hyper)instrument, but this categorization fosters problems that are extensively argued through the paper. Some of these issues are closely related to the vague and ambiguous nature of breathing, which is discussed in the last section.		Miguel Álvarez-Fernández;Asia Piascik;Stefan Kersten	2007			anesthesia;breathing;medicine	HCI	-47.84113355694682	-35.81104881373767	174672
18d344014e0a7aab07f572eb4ae442ae24673915	boosting mechanical design with the c++ ooml and open source 3d printers	object oriented programming paradigm;mechanical engineering computing;probability;number systems;printers;cad;mechanical design;3d objects;geometry;semantics;object oriented programming;public domain software;three dimensional printing;data analysis;c language;three dimensional displays printers geometry semantics wheels solid modeling;wygiwym paradigm;algebra;object oriented;three dimensional displays;solid modeling;c object oriented mechanics library;object oriented programming paradigm mechanical design c ooml open source 3d printers c object oriented mechanics library 3d objects wygiwym paradigm number systems geometry algebra data analysis probability trigonometry stl files;open source 3d printers;trigonometry;mechanism design;c ooml;three dimensional printing c language cad mechanical engineering computing object oriented programming public domain software;stl files;wheels;open source	In this paper we are presenting the C++ Object Oriented Mechanics Library (OOML). The OOML is a WYGIWYM (what you get is what you mean) tool that allows the fast development of 3D objects for fabrication on a 3D Printer. Designing with a WYGIWYM paradigm help students to communicate and reason geometrically, by applying geometry in real-world settings, and by solving problems through the integrated study of number systems, geometry, algebra, data analysis, probability, and trigonometry. These designed objects can be converted to STL files. The STL file can be used in 3D printers for fast prototyping or with traditional mechanization processes. We have designed and evaluated with students the OOML. Results show that the OOML plus the 3D Printer boosts their creativity. As a non searched result, we have observed that OOML also helps students to understand better the Object Oriented Programming Paradigm.	3d printing;boosting (machine learning);c++;open-source software;programming paradigm;stl (file format)	Alberto Valero-Gomez;Juan González-Gómez;Mario Almagro;Miguel A. Salichs	2012	Proceedings of the 2012 IEEE Global Engineering Education Conference (EDUCON)	10.1109/EDUCON.2012.6201114	computer science;theoretical computer science;engineering drawing	SE	-46.29422129597102	-29.584630619475593	174836
